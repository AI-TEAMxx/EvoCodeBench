{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return system(cmd)"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Compute the inverse of the contract function for the input vector\n  z_inv = z / (1 + np.linalg.norm(z)**2)\n\n  # Scale the input vector to ensure the output vector has a magnitude that correctly inverses the operation of the contract function within a certain domain\n  z_inv = z_inv * np.sqrt(2)\n\n  return z_inv"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        import sqlite3\n\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n            c.execute(\n                f\"CREATE TABLE IF NOT EXISTS {func_name} (args TEXT, kwargs TEXT, result TEXT)\"\n            )\n            conn.commit()\n            c.execute(\n                f\"SELECT * FROM {func_name} WHERE args = ? AND kwargs = ?\",\n                (str(args), str(kwargs)),\n            )\n            result = c.fetchone()\n            if result:\n                return eval(result[2])\n            else:\n                result = func(*args, **kwargs)\n                c.execute(\n                    f\"INSERT INTO {func_name} VALUES (?, ?, ?)\",\n                    (str(args), str(kwargs), str(result)),\n                )\n                conn.commit()\n                return result\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min ({values['x_min']}) must be less than x_max ({values['x_max']})\")\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min ({values['y_min']}) must be less than y_max ({values['y_max']})\")\n    return values\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the squared norms of the vectors\n  norm0 = np.sum(mat0 ** 2, axis=0)\n  norm1 = np.sum(mat1 ** 2, axis=0)\n\n  # Compute the dot product of the vectors\n  dot_prod = np.dot(mat0.T, mat1)\n\n  # Compute the squared distance matrix\n  dist_mat = norm0[:, np.newaxis] + norm1[np.newaxis, :] - 2 * dot_prod\n\n  # Set negative distances to zero\n  dist_mat[dist_mat < 0] = 0\n\n  return dist_mat"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"http://\") or path.startswith(\"https://\"):\n        return True\n    return False\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"When {name} is a dictionary, assets_names must be provided.\"\n            )\n        if len(items) != n_assets:\n            raise ValueError(\n                f\"When {name} is a dictionary, the number of items must match the number of assets.\"\n            )\n        if dim == 1:\n            array = np.array([items.get(asset, fill_value) for asset in assets_names])\n        elif dim == 2:\n            array = np.array(\n                [\n                    [items.get(asset, fill_value) for asset in assets_names]\n                    for _ in range(n_assets)\n                ]\n            )\n        else:\n            raise ValueError(\n                f\"Invalid dimension {dim} for {name}. Only 1 or 2 are allowed.\"\n            )\n    elif isinstance(items, np.ndarray):\n        if dim == 1:\n            if items.ndim != 1:\n                raise ValueError(\n                    f\"When {name} is a numpy array, it must be 1-dimensional.\"\n                )\n            if items.shape[0] != n_assets:\n                raise ValueError(\n                    f\"When {name} is a numpy array, the number of elements must match the number of assets.\"\n                )\n            array = items\n        elif dim == 2:\n            if items.ndim != 2:\n                raise ValueError(\n                    f\"When {name} is a numpy array, it must be 2-dimensional.\"\n                )\n            if items.shape[0] != n_assets or items.shape[1] != n_assets:\n                raise ValueError(\n                    f\"When {name} is a numpy array, the shape must be (n_assets, n_assets).\"\n                )\n            array = items\n        else:\n            raise ValueError(\n                f\"Invalid dimension {dim} for {name}. Only 1 or 2 are allowed.\"\n            )\n    else:\n        raise ValueError(\n            f\""}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        agent = MicroAgent(agent_lifecycle, openai_wrapper)\n        agent.name = data.get(\"name\", \"\")\n        agent.description = data.get(\"description\", \"\")\n        agent.version = data.get(\"version\", \"\")\n        agent.language = data.get(\"language\", \"\")\n        agent.language_version = data.get(\"language_version\", \"\")\n        agent.agent_type = data.get(\"agent_type\", \"\")\n        agent.agent_subtype = data.get(\"agent_subtype\", \"\")\n        agent.agent_id = data.get(\"agent_id\", \"\")\n        agent.agent_key = data.get(\"agent_key\", \"\")\n        agent.agent_secret = data.get(\"agent_secret\", \"\")\n        agent.agent_endpoint = data.get(\"agent_endpoint\", \"\")\n        agent.agent_owner = data.get(\"agent_owner\", \"\")\n        agent.agent_email = data.get(\"agent_email\", \"\")\n        agent.agent_url = data.get(\"agent_url\", \"\")\n        agent.agent_location = data.get(\"agent_location\", \"\")\n        agent.agent_country = data.get(\"agent_country\", \"\")\n        agent.agent_region = data.get(\"agent_region\", \"\")\n        agent.agent_city = data.get(\"agent_city\", \"\")\n        agent.agent_latitude = data.get(\"agent_latitude\", \"\")\n        agent.agent_longitude = data.get(\"agent_longitude\", \"\")\n        agent.agent_timezone = data.get(\"agent_timezone\", \"\")\n        agent.agent_currency = data.get(\"agent_currency\", \"\")\n        agent.agent_locale = data.get(\"agent_locale\", \"\")\n        agent.agent_locale_language = data.get(\"agent_locale_language\", \"\")\n        agent.agent_locale_country = data.get(\"agent_locale_country\", \"\")\n        agent.agent_locale_variant = data.get(\"agent_locale_variant\", \"\")\n        agent.agent_locale"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = jnp.finfo(jnp.float32).eps\n\n  # Define the piecewise function for sRGB to linear conversion\n  def srgb_to_linear_piecewise(x):\n    return xnp.where(\n        x <= 0.04045,\n        x / 12.92,\n        xnp.power((x + 0.055) / 1.055, 2.4)\n    )\n\n  # Convert sRGB values to linear space\n  linear = srgb_to_linear_piecewise(srgb)\n\n  return linear\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Check if the input and output times are sorted\n  if not np.all(np.diff(t_input) >= 0):\n    raise ValueError(\"Input times must be sorted.\")\n  if not np.all(np.diff(t_output) >= 0):\n    raise ValueError(\"Output times must be sorted.\")\n\n  # Check if the input and output times are within the range of the signal\n  if t_input[0] > t_output[0] or t_input[-1] < t_output[-1]:\n    raise ValueError(\"Output times must be within the range of the input times.\")\n\n  # Check if the input and output times are not too close together\n  if np.min(np.diff(t_input)) < 1e-6:\n    raise ValueError(\"Input times must be at least 1e-6 apart.\")\n  if np.min(np.diff(t_output)) < 1e-6:\n    raise ValueError(\"Output times must be at least 1e-6 apart.\")\n\n  # Check if the input and output times are not too far apart\n  if np.max(np.diff(t_input)) > 1000:\n    raise ValueError(\"Input times must be at most 1000 apart.\")\n  if np.max(np.diff(t_output)) > 1000:\n    raise ValueError(\"Output times must be at most 1000 apart.\")\n\n  # Check if the input and output times are not too close to the edges of the signal\n  if t_input[0] - t_output[0] < 1e-6:\n    raise ValueError(\"Output times must be at least 1e-6 apart from the first input time.\")\n  if t_input[-1] - t_output[-1] < 1e-6:\n    raise ValueError(\"Output times must be at least 1e-6 apart from the last input time.\")\n\n  # Check if the input and output times are not too far from the edges of the signal\n  if t_input[0] - t_output[0] > 1000:\n    raise ValueError(\"Output times must be at"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    elif word[0].isupper() and word[1].isupper():\n        return word.capitalize()\n    else:\n        return word.lower()\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}.{field.name} must be binary, but found {v.dtype}\")\n    return v\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  import numpy as np\n\n  # Calculate the norm of the input array\n  norm = np.linalg.norm(x)\n\n  # Perform the isotropic scaling operation\n  x_scaled = x / norm\n\n  return x_scaled"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    # Load the summary file into a DataFrame\n    summary_df = pd.read_csv(summary_path)\n\n    # Convert the specified columns into dictionaries\n    if dict_columns is None:\n        dict_columns = ['module_params']\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "coord.isotropize", "completion": "  import numpy as np\n\n  if mode == 'fast':\n    # Compute the isotropic covariance matrix using the determinant directly\n    cov_iso = np.diag(np.diag(cov))\n  elif mode == 'accurate':\n    # Compute the isotropic covariance matrix using the logarithm of the determinant for stability\n    cov_iso = np.diag(np.diag(cov)) * np.exp(0.5 * np.log(np.linalg.det(cov)) / cov.shape[0])\n  else:\n    raise ValueError(\"Invalid mode. Must be either 'fast' or 'accurate'.\")\n\n  return cov_iso"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Run a task.')\n    parser.add_argument('--task', type=str, required=True, help='The task description, specifying what task should be performed.')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload, allowing multiple files to be specified.')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use.')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs.')\n    parser.add_argument('--mode', type=str, default='auto', choices=['auto', 'manual'], help='Operational mode, which can be \\'auto\\' or \\'manual\\', specifying how the task should be executed.')\n    parser.add_argument('--quiet', action='store_true', help='If set, the program runs in quiet mode with minimal output.')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be.')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution.')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans.')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be.')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.')\n    parser.add_argument('--max-retry-times', type=int, help"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}.{field.name} must be a list of 2D points\")\n    return v\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set\n    char_set = \"_abcdefghijklmnopqrstuvwxyz0123456789\"\n\n    # Calculate the number of digits in the integer\n    num_digits = len(str(n))\n\n    # Calculate the index of the character in the character set\n    char_index = n % len(char_set)\n\n    # Return the encoded character\n    return char_set[char_index]\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.maximum(x, value_at_zero))"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_indexes = {}\n    for worker_index, intervals in workers_intervals.items():\n        if worker_index not in indexes:\n            indexes[worker_index] = 0\n        if worker_index not in chunk_indexes:\n            chunk_indexes[worker_index] = 0\n\n        if indexes[worker_index] >= len(intervals):\n            indexes[worker_index] = 0\n            chunk_indexes[worker_index] += 1\n\n    return chunk_indexes, indexes\n\n"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(f'Invalid datastructure: {datastructure}')\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check if the tessellation factor is valid\n  if v < 1:\n    raise ValueError(\"Tessellation factor must be greater than or equal to 1.\")\n\n  # Compute the barycentric weights for each vertex of the triangle\n  weights = np.zeros((3, v, v))\n  for i in range(3):\n    for j in range(v):\n      for k in range(v):\n        weights[i, j, k] = (j + 1) * (k + 1) / (v * v)\n\n  # Normalize the weights to get the barycentric coordinates\n  weights /= np.sum(weights, axis=0)\n\n  return weights"}
{"namespace": "linspline.query", "completion": "  # Check that the input arrays are the same length\n  if len(t) != len(v):\n    raise ValueError('The time and value arrays must be the same length.')\n\n  # Check that the time points are in ascending order\n  if not np.all(np.diff(t) >= 0):\n    raise ValueError('The time points must be in ascending order.')\n\n  # Check that the query points are within the range of the time points\n  if np.any(tq < t[0]) or np.any(tq > t[-1]):\n    raise ValueError('The query points must be within the range of the time points.')\n\n  # Interpolate the values at the query points\n  vq = np.interp(tq, t, v)\n\n  # Set extrapolated values to 0\n  vq[tq < t[0]] = 0\n  vq[tq > t[-1]] = 0\n\n  return vq"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if not all(x > 0 for x in v):\n        raise ValueError(f\"{cls.__name__}.{field.name} must be positive\")\n    return v\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane\n  origins = origins + directions * near\n\n  # Calculate ray directions in NDC\n  directions = xnp.einsum('ij,nj->ni', pixtocam, directions)\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  return origins, directions"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = dir1 / np.linalg.norm(dir1)\n  dir2 = dir2 / np.linalg.norm(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = np.dot(dir1, dir2)\n\n  # Check if the dot product is close to 1 or -1, considering a small epsilon\n  return np.isclose(dot_product, 1.0) or np.isclose(dot_product, -1.0)\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Calculate n-gram overlaps\n    ngram_overlaps = []\n    for n in range(1, 5):\n        continuation_ngrams = get_ngrams(continuation_tokens, n)\n        reference_ngrams = get_ngrams(reference_tokens, n)\n        overlap = len(continuation_ngrams.intersection(reference_ngrams))\n        ngram_overlaps.append(overlap)\n\n    # Calculate brevity penalty\n    if with_penalty:\n        brevity_penalty = min(1, len(reference_tokens) / len(continuation_tokens)) ** 0.25\n    else:\n        brevity_penalty = 1\n\n    # Calculate BLEU-4 score\n    bleu_score = brevity_penalty * np.exp(np.sum(np.log(ngram_overlaps)))\n\n    return bleu_score\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, jnp.sqrt(value_at_zero), jnp.sqrt(x))"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  return w / np.diff(t)\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    import os\n    import time\n\n    total_size = 0\n    start_time = time.time()\n\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            try:\n                fp = os.path.join(dirpath, f)\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n\n    print(f\"Total size of {path} is {total_size} bytes\")\n    print(f\"Time taken: {time.time() - start_time} seconds\")\n\n    return total_size\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if agent.purpose_embedding is not None:\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n        }\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"Number of bins must be positive\")\n\n    # Check that the number of items and weights are equal\n    if len(items) != len(weights):\n        raise ValueError(\"Number of items and weights must be equal\")\n\n    # Check that all weights are positive\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"All weights must be positive\")\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and their weights\n    bins = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        min_bin = min(bin_weights, key=bin_weights.get)\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Convert the function name, arguments, and keyword arguments to a string representation\n        data = func_name + str(args) + str(kwargs)\n\n        # Compute the SHA-256 hash of the data\n        hash_obj = hashlib.sha256()\n        hash_obj.update(data.encode())\n        return hash_obj.hexdigest()\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    length = 0\n    for i in range(polygon.shape[0] - 1):\n        distance = np.linalg.norm(polygon[i + 1] - polygon[i])\n        if distance <= max_point_distance:\n            length += distance\n    return length\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    if rel_tr < 0 or abs_tr < 0:\n        raise ValueError(\"Relative and absolute thresholds must be non-negative.\")\n\n    if rel_tr == 0 and abs_tr == 0:\n        raise ValueError(\"At least one threshold must be non-zero.\")\n\n    largest_area = max(polygon.area for polygon in polygons)\n    filtered_polygons = [polygon for polygon in polygons if polygon.area >= rel_tr * largest_area or polygon.area >= abs_tr]\n\n    return filtered_polygons"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_remaining = num_samples_yielded % num_workers\n\n    num_samples_per_worker_dict = {worker_idx: num_samples_per_worker for worker_idx in range(num_workers)}\n\n    for worker_idx in range(num_samples_remaining):\n        num_samples_per_worker_dict[worker_idx] += 1\n\n    return num_samples_per_worker_dict\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    return filtered_results, filtered_metadatas"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\n            f\"The input array must have shape (_, 2), where _ can be any number of points. The input array has shape {array.shape}.\"\n        )\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Check if the input tensors are sorted\n    if not torch.all(torch.diff(a) >= 0):\n        raise ValueError(\"Input tensor a must be sorted.\")\n\n    # Check if the input tensors have the same shape except for the last dimension\n    if a.shape[:-1] != v.shape[:-1]:\n        raise ValueError(\"Input tensors must have the same shape except for the last dimension.\")\n\n    # Check if the last dimension of v is smaller than the last dimension of a\n    if v.shape[-1] > a.shape[-1]:\n        raise ValueError(\"The last dimension of v must be smaller than or equal to the last dimension of a.\")\n\n    # Create a new tensor to store the indices\n    idx_lo = torch.zeros(v.shape[:-1], dtype=torch.long, device=a.device)\n    idx_hi = torch.zeros(v.shape[:-1], dtype=torch.long, device=a.device)\n\n    # Iterate over the last dimension of v\n    for i in range(v.shape[-1]):\n        # Find the indices where the element should be inserted into a\n        idx_lo[..., i], idx_hi[..., i] = torch.searchsorted(a, v[..., i], side=\"left\")\n\n    return idx_lo, idx_hi\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n    [fx, 0, cx],\n    [0, fy, cy],\n    [0, 0, 1],\n  ])\n"}
{"namespace": "coord.contract", "completion": "  return x * (1 - np.linalg.norm(x, axis=-1, keepdims=True) ** 2)\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0 B\"\n\n    suffixes = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    i = 0\n    while num_bytes >= 1000 and i < len(suffixes) - 1:\n        num_bytes /= 1000.0\n        i += 1\n\n    return f\"{num_bytes:.2f} {suffixes[i]}\"\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"{field.name} must be a {nb_dimensions}D array, but got {v.ndim}D array\"\n            )\n        return v\n\n    return validator\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Convert the input array to a NumPy array\n  cartesian_vector = onp.array(cartesian_vector)\n\n  # Extract the x, y, and z coordinates from the input array\n  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  # Calculate the radius (r) using the Pythagorean theorem\n  r = onp.sqrt(x**2 + y**2 + z**2)\n\n  # Calculate the inclination (theta) using the arccosine function\n  theta = onp.arccos(z / (r + eps))\n\n  # Calculate the azimuth (phi) using the arctangent function\n  phi = onp.arctan2(y, x)\n\n  # Return the spherical coordinates as a tuple\n  return r, theta, phi\n\n"}
{"namespace": "common.rougeL_score", "completion": "    # Tokenize the continuation and reference texts using a custom tokenizer function based on jieba\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Compute the ROUGE-L score using the tokenized texts\n    rouge_l_score = rouge_l(continuation_tokens, reference_tokens)\n\n    return rouge_l_score\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return locate_object(name)\n    except Exception as e:\n        return locate_object_fallback(name)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the module to an in-memory buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Load the module from the in-memory buffer\n    buffer.seek(0)\n    reloaded_module = torch.jit.load(buffer)\n\n    return reloaded_module"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check that the lengths of the ids and scores tuples match\n    if len(ids) != len(scores):\n        raise ValueError(\"The lengths of the ids and scores tuples must match\")\n\n    # Check that the length of the weights tuple matches the length of the ids and scores tuples\n    if len(weights) != len(ids):\n        raise ValueError(\n            \"The length of the weights tuple must match the length of the ids and scores tuples\")\n\n    # Check that the sum of the weights is equal to 1\n    if not math.isclose(sum(weights), 1):\n        raise ValueError(\"The sum of the weights must equal 1\")\n\n    # Check that the top_k parameter is a positive integer\n    if not isinstance(top_k, int) or top_k <= 0:\n        raise ValueError(\"The top_k parameter must be a positive integer\")\n\n    # Normalize the scores of each retrieval result\n    normalized_scores = []\n    for i in range(len(ids)):\n        normalized_scores.append(\n            [score / sum(scores[i]) for score in scores[i]])\n\n    # Combine the scores of each retrieval result using a convex combination method\n    fused_scores = []\n    for i in range(len(ids)):\n        fused_scores.append(\n            [sum(weights[i] * score for score in scores[i]) for score in normalized_scores[i]])\n\n    # Select the top_k results based on the fused scores\n    fused_ids = []\n    fused_scores = []\n    for i in range(len(ids)):\n        top_k_indices = sorted(\n            range(len(scores[i])), key=lambda x: scores[i][x], reverse=True)[:top_k]\n        fused_ids.append([ids[i][index] for index in top_k_indices])\n        fused_scores.append(\n            [fused_scores[i][index] for index in"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if x != x:\n        return str(x)\n\n    if percent:\n        x *= 100\n\n    if x < 1e-3:\n        return f\"{x:.2e}\"\n    elif x < 1e-1:\n        return f\"{x:.3f}\"\n    elif x < 1:\n        return f\"{x:.4f}\"\n    elif x < 10:\n        return f\"{x:.5f}\"\n    elif x < 100:\n        return f\"{x:.6f}\"\n    elif x < 1000:\n        return f\"{x:.7f}\"\n    elif x < 10000:\n        return f\"{x:.8f}\"\n    elif x < 100000:\n        return f\"{x:.9f}\"\n    elif x < 1000000:\n        return f\"{x:.10f}\"\n    elif x < 10000000:\n        return f\"{x:.11f}\"\n    elif x < 100000000:\n        return f\"{x:.12f}\"\n    elif x < 1000000000:\n        return f\"{x:.13f}\"\n    elif x < 10000000000:\n        return f\"{x:.14f}\"\n    elif x < 100000000000:\n        return f\"{x:.15f}\"\n    elif x < 1000000000000:\n        return f\"{x:.16f}\"\n    elif x < 10000000000000:\n        return f\"{x:.17f}\"\n    elif x < 100000000000000:\n        return f\"{x:.18f}\"\n    elif x < 1000000000000000:\n        return f\"{x:.19f}\"\n    elif x < 1000"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import os\n    import time\n\n    while True:\n        free_space = os.statvfs(input_dir).f_bavail * os.statvfs(input_dir).f_frsize\n        free_space_in_gb = free_space / (1024 ** 3)\n        if free_space_in_gb > threshold_in_gb:\n            break\n        time.sleep(sleep_time)\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  return p * np.diff(t)\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = line_text.split(\",\")\n\n    return tokens\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zeros cannot exceed the total number of weights.\")\n\n    weights = np.random.uniform(size=n)\n    weights /= weights.sum()\n\n    if zeros > 0:\n        zero_indices = np.random.choice(n, zeros, replace=False)\n        weights[zero_indices] = 0\n\n    return weights\n\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module = cls(module_type=module_type, **module_dict)\n        return module\n\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the bounding box coordinates and mode of the instance\n    bbox = instance[\"bbox\"]\n    mode = instance[\"mode\"]\n\n    # Calculate the center of the bounding box\n    bbox_center = [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2]\n\n    # Calculate the crop size based on the desired crop size and the mode\n    if mode == \"whole\":\n        crop_size = [crop_size[0], crop_size[1]]\n    elif mode == \"half\":\n        crop_size = [crop_size[0] // 2, crop_size[1] // 2]\n    elif mode == \"quarter\":\n        crop_size = [crop_size[0] // 4, crop_size[1] // 4]\n    else:\n        raise ValueError(\"Invalid mode: {}\".format(mode))\n\n    # Calculate the top-left corner of the crop based on the center of the bounding box and the crop size\n    top_left = [\n        max(0, bbox_center[0] - crop_size[0] // 2),\n        max(0, bbox_center[1] - crop_size[1] // 2),\n    ]\n\n    # Adjust the top-left corner to ensure the crop fits within the image boundaries\n    top_left[0] = min(top_left[0], image_size[1] - crop_size[0])\n    top_left[1] = min(top_left[1], image_size[0] - crop_size[1])\n\n    # Create a CropTransform object with the calculated parameters\n    crop_transform = CropTransform(\n        crop_size=crop_size,\n        top_left=top_left,\n    )\n\n    return crop_transform\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  norm = jnp.sqrt(jnp.sum(x * x, axis=-1, keepdims=True))\n  norm_clamped = jnp.maximum(norm, grad_eps)\n  return x / norm_clamped\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split('Use Agent[')[1].split(']')[0]\n        agent_name = agent_info.split(':')[0]\n        if len(agent_info.split(':')) > 1:\n            agent_input = agent_info.split(':')[1]\n        else:\n            agent_input = ''\n        return agent_name, agent_input\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    # The following keys are used in the annotations\n    if \"segmentation\" in annos[0]:\n        keys = [\"iscrowd\", \"category_id\", \"bbox\", \"area\", \"image_id\", \"id\", \"segmentation\"]\n    else:\n        keys = [\"iscrowd\", \"category_id\", \"bbox\", \"area\", \"image_id\", \"id\"]\n\n    # The annotations are organized into a list of dictionaries, where each dictionary contains information about one instance in an image.\n    # The following code iterates through each instance in the annotations and extracts the relevant information.\n    instances = []\n    for anno in annos:\n        if not anno.get(\"iscrowd\", 0):\n            if \"segmentation\" in anno:\n                segm = [\n                    convert_polygon_to_coco_mask(\n                        [anno[\"segmentation\"]], anno[\"bbox\"], image_size\n                    )\n                ]\n            else:\n                segm = None\n\n            instances.append(\n                {\n                    \"gt_bboxes\": Boxes(anno[\"bbox\"]),\n                    \"gt_classes\": anno[\"category_id\"],\n                    \"gt_masks\": segm,\n                    \"gt_keypoints\": None,\n                }\n            )\n\n    return instances\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n    return str(data_home)"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure the input is a 2D array\n    if not isinstance(cov, np.ndarray) or len(cov.shape) != 2:\n        raise ValueError(\"Input must be a 2D numpy array\")\n\n    # Calculate the standard deviations for each variable\n    std_devs = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(std_devs, std_devs)\n\n    return corr, std_devs\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    for submodule in model.modules():\n        submodule.__class__.__setattr__(\"training\", lambda self: False)\n\n    return model\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def are_shapes_equal_validator(cls, values: Dict) -> None:\n        \"\"\"\n        This function is a validator that checks if the shapes of two fields within a Pydantic model match.\n\n        Input-Output Arguments\n        :param cls: The class of the Pydantic model being validated.\n        :param values: Dict, A dictionary containing the values of the fields being validated.\n        :return: None, If the shapes of the two fields match, the function does not return anything. If the shapes do not match, it raises a ValueError indicating the mismatch.\n        \"\"\"\n        field1_value = values.get(field1)\n        field2_value = values.get(field2)\n\n        if field1_value is None or field2_value is None:\n            return\n\n        if field1_value.shape != field2_value.shape:\n            raise ValueError(\n                f\"The shapes of {field1} and {field2} do not match: {field1_value.shape} vs {field2_value.shape}\"\n            )\n\n    return are_shapes_equal_validator"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list):\n        if isinstance(metrics[0], str):\n            return metrics, [{} for _ in metrics]\n        elif isinstance(metrics[0], dict):\n            return [metric[\"name\"] for metric in metrics], metrics\n        else:\n            raise ValueError(\"Metrics must be either a list of strings or a list of dictionaries.\")\n    else:\n        raise ValueError(\"Metrics must be a list.\")\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    fn_inv = {\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda t: t,\n      lambda"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  import numpy as np\n\n  x = r * np.sin(theta) * np.cos(phi)\n  y = r * np.sin(theta) * np.sin(phi)\n  z = r * np.cos(theta)\n\n  return np.array([x, y, z])"}
{"namespace": "linspline.integrate", "completion": "  if len(t) != len(w):\n    raise ValueError(\"The length of t and w must be the same.\")\n  if len(t) < 2:\n    raise ValueError(\"The length of t and w must be at least 2.\")\n  if t[0] != 0:\n    raise ValueError(\"The first element of t must be 0.\")\n  if t[-1] != 1:\n    raise ValueError(\"The last element of t must be 1.\")\n  if not np.all(np.diff(t) > 0):\n    raise ValueError(\"The elements of t must be strictly increasing.\")\n  if not np.all(np.diff(w) > 0):\n    raise ValueError(\"The elements of w must be strictly increasing.\")\n  return np.trapz(w, t)\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Initialize an empty list to store the weighted scores for each ID\n    weighted_scores = []\n\n    # Iterate over the IDs and scores in parallel\n    for id_list, score_list in zip(ids, scores):\n\n        # Iterate over the IDs and scores in the current category or group\n        for id, score in zip(id_list, score_list):\n\n            # Calculate the weighted score for the current ID\n            weighted_score = score * weights[0]\n\n            # Append the weighted score to the list of weighted scores\n            weighted_scores.append((id, weighted_score))\n\n    # Sort the list of weighted scores in descending order based on the score\n    weighted_scores.sort(key=lambda x: x[1], reverse=True)\n\n    # Extract the top K IDs and their corresponding scores\n    top_ids, top_scores = zip(*weighted_scores[:top_k])\n\n    # Return the top K IDs and their corresponding scores\n    return list(top_ids), list(top_scores)"}
{"namespace": "coord.track_linearize", "completion": "  # Get the shape of the mean and covariance tensors\n  shape = mean.shape\n\n  # Reshape the mean and covariance tensors to 2D\n  mean = tf.reshape(mean, [-1, shape[-1]])\n  cov = tf.reshape(cov, [-1, shape[-1], shape[-1]])\n\n  # Apply the function to the mean\n  fn_mean = fn(mean)\n\n  # Compute the Jacobian of the function\n  jacobian = tf.stack([tf.gradients(fn(mean)[:, i], mean)[0] for i in range(shape[-1])], axis=-1)\n\n  # Compute the linearized covariance\n  fn_cov = tf.einsum('...ij,...jk,...lk->...il', jacobian, cov, jacobian)\n\n  # Reshape the mean and covariance tensors back to their original shape\n  fn_mean = tf.reshape(fn_mean, shape)\n  fn_cov = tf.reshape(fn_cov, shape[:-1] + [shape[-1], shape[-1]])\n\n  return fn_mean, fn_cov"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[:len(i) // 2], i[len(i) // 2:]]\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = np.power(2, np.arange(min_deg, max_deg + 1))\n  x_scaled = x[..., None] * scales[None, :]\n  x_enc = np.concatenate([np.sin(x_scaled), np.cos(x_scaled)], axis=-1)\n  if append_identity:\n    x_enc = np.concatenate([x[..., None], x_enc], axis=-1)\n  return x_enc\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def are_all_shapes_equal_validator(cls, values: Dict) -> Dict:\n        \"\"\"\n        This function is a Pydantic validator that checks if two lists of numpy arrays (specified by field names) have the same length and if each corresponding pair of arrays within these lists has the same shape.\n\n        Input-Output Arguments\n        :param cls: Class type. The class type of the Pydantic model that the validator is being applied to.\n        :param values: Dict. A dictionary of values to be validated. The keys of this dictionary should be the names of the fields to be validated.\n        :return: Dict. The validated values if the check passes.\n\n        \"\"\"\n        field1_values = values.get(field1)\n        field2_values = values.get(field2)\n\n        if len(field1_values) != len(field2_values):\n            raise ValueError(\n                f\"The length of {field1} and {field2} must be the same.\"\n            )\n\n        for i, (arr1, arr2) in enumerate(zip(field1_values, field2_values)):\n            if arr1.shape != arr2.shape:\n                raise ValueError(\n                    f\"The shape of {field1}[{i}] and {field2}[{i}] must be the same.\"\n                )\n\n        return values\n\n    return are_all_shapes_equal_validator"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.width, camera.height)\n        self.render(camera)"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object\n    nomic_config = NomicBertConfig()\n\n    # Set the model_type attribute to \"nomic\"\n    nomic_config.model_type = \"nomic\"\n\n    # Set the num_hidden_layers attribute to the value of the num_hidden_layers attribute in the BertConfig\n    nomic_config.num_hidden_layers = bert_config.num_hidden_layers\n\n    # Set the num_attention_heads attribute to the value of the num_attention_heads attribute in the BertConfig\n    nomic_config.num_attention_heads = bert_config.num_attention_heads\n\n    # Set the intermediate_size attribute to the value of the intermediate_size attribute in the BertConfig\n    nomic_config.intermediate_size = bert_config.intermediate_size\n\n    # Set the hidden_act attribute to the value of the hidden_act attribute in the BertConfig\n    nomic_config.hidden_act = bert_config.hidden_act\n\n    # Set the hidden_dropout_prob attribute to the value of the hidden_dropout_prob attribute in the BertConfig\n    nomic_config.hidden_dropout_prob = bert_config.hidden_dropout_prob\n\n    # Set the attention_probs_dropout_prob attribute to the value of the attention_probs_dropout_prob attribute in the BertConfig\n    nomic_config.attention_probs_dropout_prob = bert_config.attention_probs_dropout_prob\n\n    # Set the max_position_embeddings attribute to the value of the max_position_embeddings attribute in the BertConfig\n    nomic_config.max_position_embeddings = bert_config.max_position_embeddings\n\n    # Set the type_vocab_size attribute to the value of the type_vocab_size attribute in the BertConfig\n    nomic_config.type_vocab_size = bert_config.type_vocab_size\n\n    # Set the vocab_size attribute to the value of"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == RenderType.POINTS:\n            self.program.use()\n            self.upload_gl_uniforms(camera)\n            gl.glBindVertexArray(self.vao)\n            gl.glDrawArrays(gl.GL_POINTS, 0, self.vertices.shape[0])\n            gl.glBindVertexArray(0)\n        elif self.render_type == RenderType.LINES:\n            self.program.use()\n            self.upload_gl_uniforms(camera)\n            gl.glBindVertexArray(self.vao)\n            if self.ebo is not None:\n                gl.glDrawElements(gl.GL_LINES, self.faces.shape[0], gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(gl.GL_LINES, 0, self.vertices.shape[0])\n            gl.glBindVertexArray(0)\n        elif self.render_type == RenderType.TRIANGLES:\n            self.program.use()\n            self.upload_gl_uniforms(camera)\n            gl.glBindVertexArray(self.vao)\n            if self.ebo is not None:\n                gl.glDrawElements(gl.GL_TRIANGLES, self.faces.shape[0], gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(gl.GL_TRIANGLES, 0, self.vertices.shape[0])\n            gl.glBindVertexArray(0)\n        elif self.render_type == RenderType.QUADS:\n            self.program.use()\n            self.upload_gl_uniforms(camera)\n            gl.glBindVertexArray(self.vao)\n            if self.ebo is not None:\n                gl.glDrawElements(gl.GL_QUADS, self.faces.shape[0], gl.GL_UNSIGNED_INT, None)\n            else"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.numpy()\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n        self.texture.upload(ptr, x, y, w, h)\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched\n    R = R.unsqueeze(0) if R.ndim == 2 else R\n    tvec = tvec.unsqueeze(0) if tvec.ndim == 1 else tvec\n    camera_matrix = (\n        camera_matrix.unsqueeze(0) if camera_matrix.ndim == 2 else camera_matrix\n    )\n    image_size = image_size.unsqueeze(0) if image_size.ndim == 1 else image_size\n\n    # Validate input shapes and values\n    assert R.ndim == 3 and R.shape[-1] == 3 and R.shape[-2] == 3\n    assert tvec.ndim == 2 and tvec.shape[-1] == 3\n    assert camera_matrix.ndim == 3 and camera_matrix.shape[-1] == 3 and camera_matrix.shape[-2] == 3\n    assert image_size.ndim == 2 and image_size.shape[-1] == 2\n    assert znear > 0\n\n    # Compute camera position\n    camera_position = -R.transpose(-1, -2) @ tvec\n\n    # Compute camera rotation\n    camera_rotation = R.transpose(-1, -2)\n\n    # Compute focal length\n    focal_length = camera_matrix[:, 0, 0] / (\n        camera_matrix[:, 0, 2] * image_size[:, 0]\n    )\n\n    # Compute principal point offsets\n    principal_point_offsets = torch.stack(\n        [\n            camera_matrix[:, 0, 2] / image_size[:, 0],\n            camera_matrix[:, 1, 2] / image_size[:, 1],\n        ],\n        dim=-1,\n    )\n\n    # Compute sensor width\n    sensor_width = camera_matrix[:, 0, 0] / (\n        camera_matrix[:, 0, 2] * image_size[:, 0]\n    )\n\n    # Compute camera parameters\n    camera_params = tor"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        glViewport(x, y, w, h)\n        glScissor(x, y, w, h)\n        self.quad_program.use()\n        self.tex.bind()\n        self.vao.bind()\n        glDrawArrays(GL_TRIANGLE_STRIP, 0, 4)\n        self.vao.unbind()\n        glViewport(0, 0, self.W, self.H)\n        glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    # Extract camera parameters\n    R = batch.R\n    T = batch.T\n    K = batch.K\n    H = batch.H\n    W = batch.W\n\n    # Adjust rotation matrix\n    R = R.transpose(0, 1)\n\n    # Adjust translation vector\n    T = -R @ T\n\n    # Recalculate intrinsic matrix for NDC\n    K[0, 0] = K[0, 0] / W\n    K[1, 1] = K[1, 1] / H\n    K[0, 2] = K[0, 2] / W + 0.5\n    K[1, 2] = K[1, 2] / H + 0.5\n\n    # Compute camera center\n    C = -R.transpose(0, 1) @ T\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        glBindFramebuffer(GL_READ_FRAMEBUFFER, self.FBO)\n        glBlitFramebuffer(x, y, w or self.W, h or self.H, x, y, w or self.W, h or self.H, GL_COLOR_BUFFER_BIT, GL_NEAREST)\n        glBindFramebuffer(GL_READ_FRAMEBUFFER, 0)\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Construct the inner measure\n    inner = t0.unsqueeze(1) - t1.unsqueeze(0)\n    inner = inner.clamp(min=0)\n    inner = inner.sum(dim=1)\n\n    # Construct the outer measure\n    outer = t0.unsqueeze(1) - t1.unsqueeze(0)\n    outer = outer.clamp(max=0)\n    outer = outer.sum(dim=1)\n\n    return inner, outer"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the upper envelope weights\n    w_env_upper = torch.maximum(w_env, w)\n\n    # calculate the difference between target weights and the upper envelope weights\n    diff = w - w_env_upper\n\n    # calculate the scaled half-quadratic loss\n    loss = torch.square(diff) / (2 * torch.square(t) + eps)\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # inter-interval loss\n    t_inter = t[..., 1:] - t[..., :-1]\n    w_inter = w[..., 1:]\n    loss_inter = torch.sum(w_inter * torch.abs(t_inter), dim=-1)\n\n    # intra-interval loss\n    t_intra = t[..., 1:] - t[..., :-1]\n    w_intra = w[..., 1:]\n    loss_intra = torch.sum(w_intra * torch.abs(t_intra), dim=-1)\n\n    # total loss\n    loss = loss_inter + loss_intra\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Check if the tensors have the same shape\n    if t.shape != w.shape:\n        raise ValueError(\"The tensors 't' and 'w' must have the same shape.\")\n\n    # Check if the weights sum to 1\n    if not torch.isclose(torch.sum(w), torch.tensor(1.0)):\n        raise ValueError(\"The weights must sum to 1.\")\n\n    # Check if the percentile values are valid\n    if not all(0 <= p <= 1 for p in ps):\n        raise ValueError(\"The percentile values must be between 0 and 1.\")\n\n    # Compute the cumulative weights\n    cum_w = torch.cumsum(w, dim=0)\n\n    # Interpolate the cumulative weights to find the weighted percentiles\n    ps_tensor = torch.tensor(ps)\n    weighted_percentiles = torch.interp(ps_tensor, cum_w, t)\n\n    return weighted_percentiles"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Check if t and w have the same shape\n    if t.shape != w.shape:\n        raise ValueError(\"t and w must have the same shape\")\n\n    # Check if t is sorted\n    if not torch.all(torch.diff(t) >= 0):\n        raise ValueError(\"t must be sorted\")\n\n    # Check if w is non-negative\n    if torch.any(w < 0):\n        raise ValueError(\"w must be non-negative\")\n\n    # Check if w sums to 1\n    if not torch.isclose(torch.sum(w), torch.tensor(1.0)):\n        raise ValueError(\"w must sum to 1\")\n\n    # Check if num_samples is positive\n    if num_samples <= 0:\n        raise ValueError(\"num_samples must be positive\")\n\n    # Compute the cumulative sum of w\n    cdf = torch.cumsum(w, dim=0)\n\n    # Generate uniform samples in [0, 1]\n    u = torch.rand(num_samples, device=t.device)\n\n    # Compute the bin indices for each sample\n    bin_indices = torch.searchsorted(cdf, u)\n\n    # Clamp bin indices to the valid range\n    bin_indices = torch.clamp(bin_indices, 0, len(t) - 2)\n\n    # Compute the bin widths\n    bin_widths = t[1:] - t[:-1]\n\n    # Compute the bin offsets\n    bin_offsets = t[:-1]\n\n    # Compute the bin probabilities\n    bin_probs = w / torch.sum(w)\n\n    # Compute the bin cumulative probabilities\n    bin_cum_probs = torch.cumsum(bin_probs, dim=0)\n\n    # Compute the bin cumulative probabilities for each sample\n    bin_cum_probs_u = bin_cum_probs[bin_indices]\n\n    # Compute the bin probabilities for each"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Dilate the time steps\n    dilated_t = t * dilation\n\n    # Clip the dilated time steps to the specified domain\n    dilated_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n\n    # Compute the dilated time steps and weights\n    dilated_t, dilated_w = torch.max(torch.stack([t, dilated_t]), dim=0)\n\n    # Adjust the weights to match the dilated time steps\n    w = w * (dilated_t / t)\n\n    return dilated_t, w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check if the input tensors have the same shape\n    if t.shape != y.shape:\n        raise ValueError(\"Input tensors 't' and 'y' must have the same shape.\")\n\n    # Check if the query times are sorted in ascending order\n    if not torch.all(torch.diff(tq) >= 0):\n        raise ValueError(\"Query times 'tq' must be sorted in ascending order.\")\n\n    # Check if the step function times are sorted in ascending order\n    if not torch.all(torch.diff(t) >= 0):\n        raise ValueError(\"Step function times 't' must be sorted in ascending order.\")\n\n    # Check if the step function values are sorted in ascending order\n    if not torch.all(torch.diff(y) >= 0):\n        raise ValueError(\"Step function values 'y' must be sorted in ascending order.\")\n\n    # Check if the step function times and values have the same length\n    if t.shape[0] != y.shape[0]:\n        raise ValueError(\"Step function times 't' and values 'y' must have the same length.\")\n\n    # Check if the query times are within the range of the step function times\n    if torch.any(tq < t[0]) or torch.any(tq > t[-1]):\n        raise ValueError(\"Query times 'tq' must be within the range of the step function times 't'.\")\n\n    # Check if the step function times are unique\n    if not torch.all(torch.diff(t) > 0):\n        raise ValueError(\"Step function times 't' must be unique.\")\n\n    # Check if the step function values are unique\n    if not torch.all(torch.diff(y) > 0):\n        raise ValueError(\"Step function values 'y' must be unique.\")\n\n    # Check if the step function times and values have the same length\n    if t.shape[0] != y.shape[0]:\n        raise ValueError(\"Step function times 't' and values 'y' must have the same length.\")\n\n    # Check if the query"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # calculate the annealing factor\n    anneal_factor = torch.sigmoid(anneal_slope * (t - train_frac))\n\n    # calculate the annealed weights\n    annealed_weights = w * anneal_factor\n\n    # handle cases where adjacent intervals have zero distance\n    annealed_weights[torch.isclose(annealed_weights[..., 1:], annealed_weights[..., :-1], atol=eps)] = 0.0\n\n    # prevent NaN values\n    annealed_weights = torch.softmax(annealed_weights, dim=-1)\n\n    return annealed_weights"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device=device, non_blocking=True)\n    elif isinstance(batch, (tuple, list)):\n        return [to_cuda(b, device=device, ignore_list=ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        if \"meta\" in batch:\n            return {k: to_cuda(v, device=device, ignore_list=ignore_list) for k, v in batch.items()}\n        else:\n            return {k: to_cuda(v, device=device, ignore_list=ignore_list) for k, v in batch.items()}\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # compute faces normals w.r.t the vertices (considering batch dimension)\n    v1 = v[:, f[:, 0], :]\n    v2 = v[:, f[:, 1], :]\n    v3 = v[:, f[:, 2], :]\n    n = torch.cross(v2 - v1, v3 - v1, dim=dim)\n    n = n / (torch.norm(n, dim=-1, keepdim=True) + 1e-10)\n\n    # adjust the faces tensor to match the batch dimension of the vertices tensor\n    if f.shape[0] != v.shape[0]:\n        f = f.unsqueeze(0).expand(v.shape[0], -1, -1)\n\n    # reshape the result to maintain the original faces tensor structure with additional dimensions for batch processing\n    n = n.reshape(v.shape[0], -1, 3)\n    f = f.reshape(v.shape[0], -1, 3)\n\n    return n, f"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        return [add_batch(item) for item in batch]\n    elif isinstance(batch, dict):\n        return {key: add_batch(value) for key, value in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Convert camera parameters into tensors\n        cam_params = dotdict({\n            'K': torch.tensor(self.K, dtype=torch.float32),\n            'R': torch.tensor(self.R, dtype=torch.float32),\n            'T': torch.tensor(self.T, dtype=torch.float32),\n            'focal': torch.tensor(self.focal, dtype=torch.float32),\n            'ppx': torch.tensor(self.ppx, dtype=torch.float32),\n            'ppy': torch.tensor(self.ppy, dtype=torch.float32),\n            'height': torch.tensor(self.height, dtype=torch.float32),\n            'width': torch.tensor(self.width, dtype=torch.float32),\n            'dist_coeffs': torch.tensor(self.dist_coeffs, dtype=torch.float32),\n            'dist_model': torch.tensor(self.dist_model, dtype=torch.float32),\n            'R_cam2world': torch.tensor(self.R_cam2world, dtype=torch.float32),\n            'T_cam2world': torch.tensor(self.T_cam2world, dtype=torch.float32),\n            'R_world2cam': torch.tensor(self.R_world2cam, dtype=torch.float32),\n            'T_world2cam': torch.tensor(self.T_world2cam, dtype=torch.float32),\n            'R_cam2world_inv': torch.tensor(self.R_cam2world_inv, dtype=torch.float32),\n            'T_cam2world_inv': torch.tensor(self.T_cam2world_inv, dtype=torch.float32),\n            'R_world2cam_inv': torch.tensor(self.R_world2cam_"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            agent_state = agent.get_state()\n            agent_state_dict = agent_state.to_dict()\n            self.save_agent_state(agent_state_dict)\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        if not self.agents:\n            return None, -np.inf\n\n        # Calculate the cosine similarity between the purpose embedding of each agent and the given purpose embedding\n        similarity_scores = np.array([cosine_similarity(agent.purpose_embedding, purpose_embedding) for agent in self.agents])\n\n        # Find the index of the agent with the highest similarity score\n        best_agent_index = np.argmax(similarity_scores)\n\n        # Return the agent with the highest similarity score and the similarity score itself\n        return self.agents[best_agent_index], similarity_scores[best_agent_index]\n\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        self.agent_list.append(Agent(prompt=\"I am a prime agent\", name=\"Prime Agent\", weight=1, flags={\"prime\": True, \"unspecified\": True}))\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        agent_data = self.db.get_agent_data(purpose)\n        if agent_data is None:\n            return None\n        return self.deserializer.deserialize(agent_data, agent_lifecycle, openai_wrapper)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        # Load all agents from the database\n        agents = self.load_agents()\n\n        # Initialize an empty list to store the loaded agents\n        loaded_agents = []\n\n        # Iterate over the agents and load each one\n        for agent in agents:\n            # Get the agent's purpose\n            purpose = agent.get_purpose()\n\n            # Load the agent based on its purpose\n            if purpose == \"chat\":\n                # Load the agent as a chat agent\n                loaded_agent = agent_lifecycle.load_chat_agent(agent, openai_wrapper)\n            elif purpose == \"search\":\n                # Load the agent as a search agent\n                loaded_agent = agent_lifecycle.load_search_agent(agent, openai_wrapper)\n            elif purpose == \"summarization\":\n                # Load the agent as a summarization agent\n                loaded_agent = agent_lifecycle.load_summarization_agent(agent, openai_wrapper)\n            elif purpose == \"translation\":\n                # Load the agent as a translation agent\n                loaded_agent = agent_lifecycle.load_translation_agent(agent, openai_wrapper)\n            else:\n                # If the agent's purpose is not recognized, skip it\n                continue\n\n            # Add the loaded agent to the list of loaded agents\n            loaded_agents.append(loaded_agent)\n\n        # Return the list of loaded agents\n        return loaded_agents\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence_mechanism.save_agent(agent)\n        except Exception as e:\n            self.logger.exception(f\"Error saving agent: {e}\")\n            raise e\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup()\n        return self.agents\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"Write a python function that {goal} using the following sample input: {sample_input}\"\n            return self.llm_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            self.logger.exception(f\"Error generating LLM prompt: {e}\")\n            return \"\"\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Create the table if it doesn't exist\n        c.execute('''CREATE TABLE IF NOT EXISTS agents\n                     (id text, purpose text, data text)''')\n\n        # Insert or update the agent's record\n        c.execute(\"REPLACE INTO agents VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict['data'])))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        # Query the database for the agent data based on the purpose\n        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT agent_data FROM agents WHERE purpose = ?\", (purpose,))\n        row = cursor.fetchone()\n\n        # If an agent with the given purpose is found, deserialize it and return the data as a dictionary\n        if row:\n            agent_data = row[0]\n            agent_data = pickle.loads(agent_data)\n            return agent_data\n\n        # If no agent with the given purpose is found, return None\n        return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        # Connect to the SQLite database\n        conn = sqlite3.connect(self.filename)\n        c = conn.cursor()\n\n        # Execute a SELECT statement to retrieve all purposes from the database\n        c.execute(\"SELECT purpose FROM agents\")\n        purposes = c.fetchall()\n\n        # Close the database connection\n        conn.close()\n\n        # Return the list of purposes\n        return purposes\n\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.db_path)\n        cursor = conn.cursor()\n\n        # Query the database for the cached result\n        cursor.execute(\"SELECT result FROM cache WHERE arg_hash=?\", (arg_hash,))\n        result = cursor.fetchone()\n\n        # Close the database connection\n        conn.close()\n\n        # If the result is found, load it from JSON format and return it\n        if result:\n            return json.loads(result[0])\n\n        # If the result is not found, return None\n        return None\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Connect to the database\n        conn = sqlite3.connect(self.db_path)\n        c = conn.cursor()\n\n        # Create the 'cache' table if it doesn't exist\n        c.execute('''CREATE TABLE IF NOT EXISTS cache (key TEXT PRIMARY KEY, value TEXT)''')\n\n        # Insert the result into the 'cache' table\n        c.execute('''INSERT OR REPLACE INTO cache (key, value) VALUES (?, ?)''', (arg_hash, json.dumps(result)))\n\n        # Commit the changes and close the connection\n        conn.commit()\n        conn.close()\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    global_config.update_config_parameters(args)\n\n    # If quiet mode is enabled, redirect the standard output to a file instead of displaying it in the terminal\n    if quiet_mode:\n        global_config.update_config_parameters({\"output_file\": \"output.txt\"})\n\n    # Execute the command line process based on the provided arguments\n    command_line_process.execute_command_line_process()\n\n"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        # Get the model to use for chat completion\n        model = kwargs.get('model', 'gpt-3.5-turbo')\n\n        # Check if the model is available\n        if model not in self.models:\n            raise BadRequestError(f\"Model '{model}' is not available.\")\n\n        # Get the model's context length\n        context_length = self.models[model]['context_length']\n\n        # Check if the context length is exceeded\n        if len(self.messages) > context_length:\n            # Attempt to use a higher-capacity model if available\n            if 'gpt-3.5-turbo-16k' in self.models:\n                model = 'gpt-3.5-turbo-16k'\n            else:\n                raise BadRequestError(f\"Context length exceeded for model '{model}'.\")\n\n        # Make the chat completion request\n        try:\n            response = openai.ChatCompletion.create(\n                model=model,\n                messages=self.messages,\n                **kwargs\n            )\n        except Exception as e:\n            raise BadRequestError(f\"Error during chat completion: {str(e)}\")\n\n        # Return the response\n        return response\n\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._client_creation_time + self._client_expiration_interval < time.time():\n            self._client = boto3.client(\n                's3',\n                aws_access_key_id=self._aws_access_key_id,\n                aws_secret_access_key=self._aws_secret_access_key,\n                region_name=self._region_name\n            )\n            self._client_creation_time = time.time()\n\n        return self._client"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if self.rank is not None:\n            raise RuntimeError(\"state_dict() should not be called from a DataLoader worker process.\")\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir_path,\n            \"input_url\": self.input_url,\n            \"item_loader_state\": self.item_loader_state,\n            \"last_batch_dropped\": self.last_batch_dropped,\n            \"seed\": self.seed,\n            \"world_size\": self.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self.state_dict = state_dict\n        self.state_dict[\"current_index\"] = self.state_dict[\"current_index\"] - 1\n        self.state_dict[\"current_index\"] = self.state_dict[\"current_index\"] % self.state_dict[\"num_samples\"]\n        self.state_dict[\"current_index\"] = self.state_dict[\"current_index\"] + 1\n        self.state_dict[\"current_index\"] = self.state_dict[\"current_index\"] % self.state_dict[\"num_samples\"]\n        self.state_dict[\"current_index\"] = self.state_dict[\"current_index\"] + 1\n        self.state_dict[\"current_index\"] = self.state_dict[\"current_index\"] % self.state_dict[\"num_samples\"]\n        self.state_dict[\"current_index\"] = self.state_dict[\"current_index\"] + 1\n        self.state_dict[\"current_index\"] = self.state_dict[\"current_index\"] % self.state_dict[\"num_samples\"]\n        self.state_dict[\"current_index\"] = self.state_dict[\"current_index\"] + 1\n        self.state_dict[\"current_index\"] = self.state_dict[\"current_index\"] % self.state_dict[\"num_samples\"]\n        self.state_dict[\"current_index\"] = self.state_dict[\"current_index\"] + 1\n        self.state_dict[\"current_index\"] = self.state_dict[\"current_index\"] % self.state_dict[\"num_samples\"]\n        self.state_dict[\"current_index\"] = self.state_dict[\"current_index\"] + 1\n        self.state_dict[\"current_index\"] = self.state_dict[\"current_index\"] % self.state_dict[\"num_samples\"]\n        self.state_dict[\"current_index\"] = self.state_dict[\"current_index\"] + 1\n        self.state_dict[\"current_index\"] = self.state_dict[\"current_index\"] % self.state_dict[\"num_samples\"]\n        self.state_dict[\""}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"shuffle is not consistent between state_dict and StreamingDataset. \"\n                f\"state_dict: {self._state_dict['shuffle']}, StreamingDataset: {self.shuffle}\"\n            )\n\n        if self._state_dict[\"num_workers\"] != self.num_workers:\n            raise ValueError(\n                f\"num_workers is not consistent between state_dict and StreamingDataset. \"\n                f\"state_dict: {self._state_dict['num_workers']}, StreamingDataset: {self.num_workers}\"\n            )\n\n        if self._state_dict[\"input_dir\"] != self.input_dir:\n            raise ValueError(\n                f\"input_dir is not consistent between state_dict and StreamingDataset. \"\n                f\"state_dict: {self._state_dict['input_dir']}, StreamingDataset: {self.input_dir}\"\n            )\n\n        if self._state_dict[\"url\"] != self.url:\n            raise ValueError(\n                f\"url is not consistent between state_dict and StreamingDataset. \"\n                f\"state_dict: {self._state_dict['url']}, StreamingDataset: {self.url}\"\n            )\n\n        if self._state_dict[\"seed\"] != self.seed:\n            raise ValueError(\n                f\"seed is not consistent between state_dict and StreamingDataset. \"\n                f\"state_dict: {self._state_dict['seed']}, StreamingDataset: {self.seed}\"\n            )\n\n        if self._state_dict[\"item_loader_state\"] != self.item_loader.state_dict():\n            raise ValueError(\n                f\"item_loader_state is not consistent between state_dict and StreamingDataset. \"\n                f\"state_dict: {self._state_dict['item_loader_state']}, StreamingDataset: {self.item_loader.state_dict()}\"\n            )"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n    cache_dir = os.path.join(\n        os.environ.get(\"XDG_CACHE_HOME\", os.path.expanduser(\"~/.cache\")),\n        \"pytorch_lightning\",\n        \"datamodules\",\n        hashlib.sha256(input_dir.encode(\"utf-8\")).hexdigest(),\n    )\n    os.makedirs(cache_dir, exist_ok=True)\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"Invalid remote file path: {remote_filepath}\")\n        if os.path.exists(local_filepath):\n            return\n        with self.file_lock.acquire(timeout=self.lock_timeout):\n            if os.path.exists(local_filepath):\n                return\n            if self.s5cmd_path is not None:\n                self.download_file_s5cmd(remote_filepath, local_filepath)\n            else:\n                self.download_file_boto3(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Initialize an empty dictionary to store the chunks assigned to each worker\n    chunks_per_worker = {}\n\n    # Initialize an empty dictionary to store the intervals corresponding to each chunk assigned to each worker\n    intervals_per_worker = {}\n\n    # Iterate over the number of workers\n    for worker_idx in range(num_workers):\n\n        # Calculate the start and end indices for the chunks assigned to the current worker\n        start_idx = worker_idx * len(chunks_replica) // worker_env.world_size\n        end_idx = (worker_idx + 1) * len(chunks_replica) // worker_env.world_size\n\n        # Assign the chunks and intervals to the current worker\n        chunks_per_worker[worker_idx] = chunks_replica[start_idx:end_idx]\n        intervals_per_worker[worker_idx] = intervals_replica[start_idx:end_idx]\n\n    # Return the dictionaries containing the chunks and intervals assigned to each worker\n    return chunks_per_worker, intervals_per_worker"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n        super().download_file(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        return (\n            struct.pack(\n                \"<II\",\n                item.width,\n                item.height,\n            )\n            + item.mode.encode(\"utf-8\")\n            + item.tobytes(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, Image):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                return item.to_jpeg(), None\n        else:\n            raise TypeError(\"Unsupported image type\")\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = struct.unpack('<3I', data[:12])\n        mode = data[12:12 + mode_size].decode('utf-8')\n        image_data = data[12 + mode_size:]\n\n        return cls(width, height, mode, image_data)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype_str, shape_str = data[:10].decode().split(\",\")\n        dtype = getattr(torch, dtype_str)\n        shape = tuple(map(int, shape_str.split(\",\")))\n\n        # Extract the raw data from the byte array\n        raw_data = data[10:]\n\n        # Reconstruct the tensor from the raw data and the extracted information\n        tensor = torch.frombuffer(raw_data, dtype=dtype).reshape(shape)\n\n        return tensor"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        return (\n            torch.save(item, io.BytesIO()).getvalue(),\n            None,\n        )\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            import torchvision\n            from torchvision.io import read_image\n            from torchvision.io.image import ImageReadMode\n            from torchvision.transforms.functional import to_tensor\n            image = read_image(data, ImageReadMode.RGB)\n            return to_tensor(image)\n        except (ImportError, RuntimeError):\n            from PIL import Image\n            image = Image.open(io.BytesIO(data))\n            return image\n\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        return (item.numpy().tobytes(), f\"no_header_tensor:{self.data_type_to_index[item.dtype]}\")\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Extract the data type and shape information from the byte array\n        dtype = np.dtype(data[:4])\n        shape = tuple(int.from_bytes(data[4 + i * 4:8 + i * 4], byteorder='little', signed=False) for i in range(dtype.shape[0]))\n\n        # Calculate the size of the array data in bytes\n        data_size = np.prod(shape) * dtype.itemsize\n\n        # Extract the array data from the byte array\n        array_data = data[8 + dtype.shape[0] * 4:8 + dtype.shape[0] * 4 + data_size]\n\n        # Reconstruct the numpy array from the data\n        array = np.frombuffer(array_data, dtype=dtype).reshape(shape)\n\n        return array"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        return np.frombuffer(data, dtype=self._dtype)\n\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(), f\"no_header_numpy:{item.dtype.name}\"\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        # Get the data type index\n        dtype_index = self.dtype_to_index(item.dtype)\n        # Get the number of dimensions\n        ndim = item.ndim\n        # Get the shape of each dimension\n        shape = item.shape\n        # Get the binary data of the array\n        data = item.tobytes()\n        # Create the serialized bytes object\n        serialized_bytes = struct.pack(f\"<B{ndim}I{len(data)}s\", dtype_index, *shape, data)\n        # Return the serialized bytes object and None as a placeholder for potential metadata that is not used in this implementation\n        return serialized_bytes, None\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": self.dataset.state_dict(),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self.num_samples_yielded,\n            \"latest_worker_idx\": self.latest_worker_idx,\n        }\n        return state\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not self.check_dependencies():\n            raise Exception(\"torchvision and av are required to deserialize video data\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.seek(0)\n            video, _, _ = torchvision.io.read_video(f.name)\n            return video\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return\n\n        self._is_done = True\n\n        if not self._should_write:\n            return []\n\n        self.write_chunks_index()\n\n        return self.write_chunk()\n\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self.num_samples_yielded = obj[\"num_samples_yielded\"]\n        self.latest_worker_idx = obj[\"latest_worker_idx\"]\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj)\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj)\n        else:\n            raise RuntimeError(\n                \"The dataset is neither a StreamingDataset nor a CombinedStreamingDataset\"\n            )\n        self.dataset.prepare_for_resuming()\n        self.dataset.prepare_for_resuming()\n        self.dataset.prepare_for_resuming()\n        self.dataset.prepare_for_resuming()\n        self.dataset.prepare_for_resuming()\n        self.dataset.prepare_for_resuming()\n        self.dataset.prepare_for_resuming()\n        self.dataset.prepare_for_resuming()\n        self.dataset.prepare_for_resuming()\n        self.dataset.prepare_for_resuming()\n        self.dataset.prepare_for_resuming()\n        self.dataset.prepare_for_resuming()\n        self.dataset.prepare_for_resuming()\n        self.dataset.prepare_for_resuming()\n        self.dataset.prepare_for_resuming()\n        self.dataset.prepare_for_resuming()\n        self.dataset.prepare_for_resuming()\n        self.dataset.prepare_for_resuming()\n        self.dataset.prepare_for_resuming()\n        self.dataset.prepare_for_resuming()\n        self.dataset.prepare_for_resuming()\n        self.dataset.prepare_for_resuming()\n        self.dataset.prepare_for_resuming()\n        self.dataset.prepare_for_resuming()\n        self.dataset.prepare_for_resuming()\n        self.dataset.prepare_for_resuming()\n        self.dataset.prepare_for_"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self.iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self.iterator is None:\n            return {\n                \"num_samples_yielded\": num_samples_yielded,\n                \"num_workers\": num_workers,\n                \"batch_size\": batch_size,\n            }\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"iterator\": self.iterator.state_dict(),\n        }"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self.dataset_state_dict = state_dict\n        for dataset_name, dataset_state in state_dict.items():\n            if dataset_name == \"num_samples_yielded\":\n                self.num_samples_yielded = dataset_state\n            else:\n                self.datasets[dataset_name].load_state_dict(dataset_state)\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return None\n    if isinstance(dir_path, Dir):\n        return dir_path\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n    if dir_path.startswith(\"http://\") or dir_path.startswith(\"https://\"):\n        return Dir(url=dir_path)\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"~/\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"~\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\".\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"~/\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"~\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\".\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path)\n    if dir_path.startswith(\"~/\"):\n        return Dir(path=dir_path)\n    if dir_path.startsw"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The output directory must be an instance of the Dir class.\")\n\n    if not output_dir.startswith(\"s3://\"):\n        raise ValueError(\"The output directory must start with 's3://'.\")\n\n    if output_dir.exists():\n        if append:\n            raise ValueError(\"The output directory already contains data and appending is not allowed.\")\n        if overwrite:\n            raise ValueError(\"The output directory already contains data and overwriting is not allowed.\")\n        raise ValueError(\"The output directory already contains data.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if output_dir.is_s3_bucket:\n        if output_dir.has_index_file:\n            raise ValueError(\n                f\"The directory {output_dir.path} is an S3 bucket and already contains an index file. Please delete the index file and try again.\"\n            )\n        else:\n            output_dir.delete_all_objects()\n\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = 0\n        if node_rank != 0:\n            while not os.path.exists(self.index_path):\n                time.sleep(0.1)\n            return\n        if not os.path.exists(self.index_path):\n            while not os.path.exists(self.index_path):\n                time.sleep(0.1)\n        if not os.path.exists(self.index_path):\n            raise FileNotFoundError(f\"Index file not found at {self.index_path}\")\n        if not os.path.exists(self.cache_dir):\n            os.makedirs(self.cache_dir)\n        if not os.path.exists(self.cache_dir):\n            raise FileNotFoundError(f\"Cache directory not found at {self.cache_dir}\")\n        if not os.path.exists(self.cache_dir):\n            raise FileNotFoundError(f\"Cache directory not found at {self.cache_dir}\")\n        if not os.path.exists(self.cache_dir):\n            raise FileNotFoundError(f\"Cache directory not found at {self.cache_dir}\")\n        if not os.path.exists(self.cache_dir):\n            raise FileNotFoundError(f\"Cache directory not found at {self.cache_dir}\")\n        if not os.path.exists(self.cache_dir):\n            raise FileNotFoundError(f\"Cache directory not found at {self.cache_dir}\")\n        if not os.path.exists(self.cache_dir):\n            raise FileNotFoundError(f\"Cache directory not found at {self.cache_dir}\")\n        if not os.path.exists(self.cache_dir):\n            raise FileNotFoundError(f\"Cache directory not found at {self.cache_dir}\")\n        if not os.path.exists(self.cache_dir):\n            raise FileNotFoundError(f\"Cache directory not found at {self.cache_dir}\")\n        if not os.path.exists(self.cache_dir):\n            raise FileNotFoundError(f\"Cache directory not found at {"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    # Check if the required SDK is available\n    if not is_sdk_available():\n        raise Exception(\"SDK is not available\")\n\n    # Get the current working directory\n    cwd = os.getcwd()\n\n    # Get the environment variables\n    env = os.environ.copy()\n\n    # Create a default machine configuration if not provided\n    if machine is None:\n        machine = Machine(\n            cpu=1,\n            memory=1,\n            gpu=0,\n            gpu_type=\"\",\n            gpu_memory=0,\n            gpu_count=0,\n            gpu_model=\"\",\n            gpu_memory_per_node=0,\n            gpu_memory_per_process=0,\n            gpu_memory_per_process_per_node=0,\n            gpu_memory_per_process_per_node_per_device=0,\n            gpu_memory_per_process_per_node_per_device_per_core=0,\n            gpu_memory_per_process_per_node_per_device_per_core_per_thread=0,\n            gpu_memory_per_process_per_node_per_device_per_core_per_thread_per_stream=0,\n            gpu_memory_per_process_per_node_per_device_per_core_per_thread_per_stream_per_tensor_core=0,\n            gpu_memory_per_process_per_node_per_device_per_core_per_thread_per_stream_per_tensor_core_per_process=0,\n            gpu_memory_per_process_per_node_per_device_per_core_per_thread_per_stream_per_tensor_core_per_process_per_node=0,\n            gpu_memory_per_process_per_node_per_device_per_core_per_thread_per_stream_per_tensor_core_per_process_per_node_per_device=0,\n            gpu_memory_per_process_per_node_per"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        self.deletion_queue.put(chunk_indexes)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._cache_dir is None:\n            return None\n\n        config_path = os.path.join(self._cache_dir, \"config.json\")\n        if not os.path.exists(config_path):\n            return None\n\n        with open(config_path, \"r\") as f:\n            config_dict = json.load(f)\n\n        config = ChunksConfig.from_dict(config_dict)\n\n        self._serializers = config.serializers\n        self._remote_input_dir = config.remote_input_dir\n        self._item_loader = config.item_loader\n\n        return config"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Configuration should be defined before accessing it.\")\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"index must be an instance of ChunkedIndex\")\n\n        if self.index_config is None:\n            raise Exception(\"index_config is not defined\")\n\n        if index.chunk_id not in self.chunks:\n            self.prefetch(index)\n\n        chunk = self.chunks[index.chunk_id]\n\n        if chunk.state == ChunkState.DELETED:\n            raise Exception(\"Chunk is deleted\")\n\n        if chunk.state == ChunkState.DOWNLOADING:\n            self.prepare_thread.join()\n\n        if chunk.state == ChunkState.DOWNLOADED:\n            self.prepare_thread = threading.Thread(\n                target=self.prepare, args=(chunk,))\n            self.prepare_thread.start()\n\n        if chunk.state == ChunkState.PREPARED:\n            return self.item_loader.load(chunk, index.item_id)\n\n        raise Exception(\"Chunk is not prepared\")"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if is_distributed():\n        return get_distributed_map().get(key, obj)\n    else:\n        return obj"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Calculate the number of chunks per node\n    chunks_per_node = [len(chunks) for chunks in chunks_per_ranks]\n\n    # Calculate the number of chunks per rank\n    chunks_per_rank = [len(chunks) for chunks in chunks_per_ranks]\n\n    # Calculate the number of chunks per rank\n    chunks_per_rank = [len(chunks) for chunks in chunks_per_ranks]\n\n    # Calculate the number of chunks per rank\n    chunks_per_rank = [len(chunks) for chunks in chunks_per_ranks]\n\n    # Calculate the number of chunks per rank\n    chunks_per_rank = [len(chunks) for chunks in chunks_per_ranks]\n\n    # Calculate the number of chunks per rank\n    chunks_per_rank = [len(chunks) for chunks in chunks_per_ranks]\n\n    # Calculate the number of chunks per rank\n    chunks_per_rank = [len(chunks) for chunks in chunks_per_ranks]\n\n    # Calculate the number of chunks per rank\n    chunks_per_rank = [len(chunks) for chunks in chunks_per_ranks]\n\n    # Calculate the number of chunks per rank\n    chunks_per_rank = [len(chunks) for chunks in chunks_per_ranks]\n\n    # Calculate the number of chunks per rank\n    chunks_per_rank = [len(chunks) for chunks in chunks_per_ranks]\n\n    # Calculate the number of chunks per rank\n    chunks_per_rank = [len(chunks) for chunks in chunks_per_ranks]\n\n    # Calculate the number of chunks per rank\n    chunks_per_rank = [len(chunks) for chunks in chunks_per_ranks]\n\n    # Calculate the number of chunks per rank\n    chunks_per_"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Extract file paths from the inputs\n    file_paths = [\n        input_item\n        for input_item in inputs\n        if isinstance(input_item, str) and os.path.isfile(input_item)\n    ]\n\n    # Check if there are at least two file paths\n    if len(file_paths) < 2:\n        return None\n\n    # Check if the file paths are consistent\n    if not all(\n        os.path.dirname(file_paths[0]) == os.path.dirname(file_path)\n        for file_path in file_paths\n    ):\n        raise ValueError(\"Inconsistent file paths found in inputs.\")\n\n    # Get the input directory from the file paths\n    input_dir = os.path.dirname(file_paths[0])\n\n    # Return the input directory\n    return input_dir\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        if enable:\n            # Enable DNS optimization\n            pass\n        else:\n            # Disable DNS optimization\n            pass\n\n        # Perform operations within the context\n        yield\n\n    finally:\n        # Disable DNS optimization\n        pass"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Calculate the number of items each rank should process\n    num_items_per_rank = _calculate_num_items_per_rank(\n        len(indexes), distributed_env.world_size, drop_last\n    )\n\n    # Assign chunks and their intervals to each rank\n    chunks_per_rank = []\n    intervals_per_rank = []\n    for rank in range(distributed_env.world_size):\n        start_index = rank * num_items_per_rank\n        end_index = min((rank + 1) * num_items_per_rank, len(indexes))\n        chunks_per_rank.append(indexes[start_index:end_index])\n        intervals_per_rank.append(chunk_intervals[start_index:end_index])\n\n    return chunks_per_rank, intervals_per_rank"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir)\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            response = s3.head_object(Bucket=obj.bucket, Key=obj.key)\n            return response\n        except ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                time.sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    # Check if the inputs are valid\n    if not isinstance(inputs, Sequence):\n        raise ValueError(\"inputs must be a sequence\")\n    if not all(isinstance(input, str) for input in inputs):\n        raise ValueError(\"all inputs must be strings\")\n\n    # Check if the output directory exists\n    if not os.path.exists(output_dir):\n        raise ValueError(\"output_dir does not exist\")\n\n    # Check if the weights are valid\n    if weights is not None:\n        if not isinstance(weights, list):\n            raise ValueError(\"weights must be a list\")\n        if len(weights) != len(inputs):\n            raise ValueError(\"weights must have the same length as inputs\")\n        if not all(isinstance(weight, int) for weight in weights):\n            raise ValueError(\"all weights must be integers\")\n\n    # Check if the chunk size is valid\n    if chunk_size is not None:\n        if not isinstance(chunk_size, int):\n            raise ValueError(\"chunk_size must be an integer\")\n        if chunk_size <= 0:\n            raise ValueError(\"chunk_size must be greater than 0\")\n\n    # Check if the chunk bytes are valid\n    if chunk_bytes is not None:\n        if not isinstance(chunk_bytes, (int, str)):\n            raise ValueError(\"chunk_bytes must be an integer or a string\")\n        if isinstance(chunk_bytes, int) and chunk_bytes <= 0:\n            raise ValueError(\"chunk_bytes must be greater than 0\")\n\n    # Check if the compression is valid\n    if compression is not None:\n        if compression not in [\"gzip\", \"bz2\", \"xz\"]:\n            raise ValueError(\"compression must be one of 'gzip', 'bz2', or 'xz'\")\n\n    # Check if the number of workers is valid\n    if num_workers is not None:\n        if not isinstance(num_workers, int):\n            raise ValueError(\"num_workers must be an integer\")\n        if num_work"}
{"namespace": "litdata.processing.functions.map", "completion": "    # Check if the output directory exists and is empty\n    if error_when_not_empty and os.path.exists(output_dir) and os.listdir(output_dir):\n        raise ValueError(f\"Output directory {output_dir} is not empty\")\n\n    # Create a temporary directory for processing\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        # Process the inputs in parallel\n        with ProcessPoolExecutor(max_workers=num_workers) as executor:\n            # Create a list of futures for each input\n            futures = []\n            for i, input in enumerate(inputs):\n                # Create a temporary file for the input\n                with tempfile.NamedTemporaryFile(dir=tmp_dir, delete=False) as tmp_file:\n                    # Write the input to the temporary file\n                    tmp_file.write(input)\n                    tmp_file.flush()\n                    # Create a future for the input\n                    future = executor.submit(fn, tmp_file.name, i)\n                    futures.append(future)\n\n            # Wait for all futures to complete\n            for future in futures:\n                future.result()\n\n        # Move the processed files to the output directory\n        for file in os.listdir(tmp_dir):\n            shutil.move(os.path.join(tmp_dir, file), output_dir)\n\n    # Remove the temporary directory\n    shutil.rmtree(tmp_dir)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Create the cache directory if it doesn't exist\n    os.makedirs(cache_dir, exist_ok=True)\n\n    # Continuously fetch download tasks from the input queue\n    while True:\n        task_index, file_paths = queue_in.get()\n\n        # Check if the files are already downloaded\n        downloaded_files = os.listdir(cache_dir)\n        missing_files = [file_path for file_path in file_paths if os.path.basename(file_path) not in downloaded_files]\n\n        # Download missing files\n        for file_path in missing_files:\n            source_path = input_dir.get_path(file_path)\n            destination_path = os.path.join(cache_dir, os.path.basename(file_path))\n            download_file(source_path, destination_path)\n\n        # Signal completion by putting the task index into the output queue\n        queue_out.put(task_index)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        item = upload_queue.get()\n        if item is None:\n            break\n        if isinstance(item, tuple):\n            temp_dir, file_path = item\n            file_path = os.path.join(temp_dir, file_path)\n        else:\n            file_path = item\n        if not file_path.startswith(cache_dir):\n            file_path = os.path.join(cache_dir, file_path)\n        if output_dir.scheme == \"s3\":\n            output_path = os.path.join(output_dir.path, file_path[len(cache_dir) + 1:])\n            output_dir.upload(file_path, output_path)\n        else:\n            output_path = os.path.join(output_dir.path, file_path[len(cache_dir):])\n            shutil.copyfile(file_path, output_path)\n        remove_queue.put(file_path)\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    def _distribute_items(num_workers, user_items, weights=None, file_size=False):\n        num_nodes = _get_num_nodes()\n        node_rank = _get_node_rank()\n        world_size = num_nodes * num_workers\n\n        worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n        worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n        # Print distribution details for workers on the current node\n        if node_rank == 0:\n            print(f\"Distributing {len(user_items)} items among {world_size} workers on {num_nodes} nodes\")\n            for worker_id in worker_ids_this_node:\n                items = worker_items[worker_id]\n                if file_size:\n                    sizes = [os.path.getsize(item) for item in items]\n                    total_size = sum(sizes) / 1024 / 1024\n                    print(f\"Worker {worker_id}: {len(items)} items ({total_size:.2f} MB)\")\n                else:\n                    total_weight = sum(worker_weights[worker_id])\n                    print(f\"Worker {worker_id}: {len(items)} items ({total_weight:.2f} MB)\")\n\n        # Shuffle items for each worker\n        worker_items_shuffled = []\n        for worker_id in worker_ids_this_node:\n            items = worker_items[worker_id]\n            random.shuffle(items)\n            worker_items_shuffled.append(items)\n\n        return worker_items_shuffled\n\n\n    \"\"\"\n    This function distributes a list of items among workers in a weighted manner, optionally considering file sizes. It first calculates the total number of workers across all nodes, then distributes items to these workers based on provided weights. It prints the distribution details for workers on the current node"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Get the total number of nodes in the environment\n    num_nodes = _get_num_nodes()\n\n    # Calculate the total number of workers across all nodes\n    total_workers = num_workers * num_nodes\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // total_workers\n\n    # Calculate the number of items that will be assigned to the last workers\n    num_remaining_items = len(user_items) % total_workers\n\n    # Calculate the start and end indices for each worker's items\n    worker_start_indices = np.cumsum([num_items_per_worker + 1 if i < num_remaining_items else num_items_per_worker for i in range(total_workers)])\n    worker_end_indices = np.cumsum([num_items_per_worker + 1 if i < num_remaining_items else num_items_per_worker for i in range(total_workers)]) - 1\n\n    # Get the current node's rank within the environment\n    node_rank = _get_node_rank()\n\n    # Calculate the start and end indices for the current node's workers\n    node_start_index = node_rank * num_workers\n    node_end_index = (node_rank + 1) * num_workers\n\n    # Distribute the items among the workers in the current node\n    worker_items = [user_items[worker_start_indices[i]:worker_end_indices[i] + 1] for i in range(node_start_index, node_end_index)]\n\n    # Ensure that the output list has a length equal to the number of workers\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers\")\n\n    return worker_items"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        # Remove cache directories if they exist\n        if os.path.exists(self.cache_dir):\n            shutil.rmtree(self.cache_dir)\n\n        # Create cache directories\n        os.makedirs(self.cache_dir)\n        os.makedirs(self.cache_dir + \"/\" + self.cache_dir_name_train)\n        os.makedirs(self.cache_dir + \"/\" + self.cache_dir_name_test)\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with ThreadPoolExecutor(max_workers=cpu_count()) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is not None:\n        if not isinstance(input_dir, str):\n            raise TypeError(f\"input_dir must be a string, not {type(input_dir)}\")\n        if not os.path.isdir(input_dir):\n            raise ValueError(f\"input_dir must be a valid directory, not {input_dir}\")\n        input_dir = os.path.abspath(input_dir)\n\n    if not isinstance(element, str):\n        raise TypeError(f\"element must be a string, not {type(element)}\")\n\n    if input_dir is not None:\n        if not element.startswith(input_dir):\n            return False\n\n    if not os.path.exists(element):\n        return False\n\n    return True\n\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"n_layers must be greater than 0\"\n        assert n_neurons > 0, \"n_neurons must be greater than 0\"\n\n        if self.tcnn:\n            if n_neurons > 1024:\n                n_neurons = 1024\n            if n_neurons < 128:\n                n_neurons = 128\n            if n_layers > 10:\n                n_layers = 10\n            if n_layers < 2:\n                n_layers = 2\n            if n_neurons % 2 != 0:\n                n_neurons -= 1\n            if n_layers % 2 != 0:\n                n_layers -= 1\n            if n_neurons < 128:\n                n_neurons = 128\n            if n_layers < 2:\n                n_layers = 2\n            if n_neurons > 1024:\n                n_neurons = 1024\n            if n_layers > 10:\n                n_layers = 10\n            if n_neurons % 2 != 0:\n                n_neurons -= 1\n            if n_layers % 2 != 0:\n                n_layers -= 1\n            if n_neurons < 128:\n                n_neurons = 128\n            if n_layers < 2:\n                n_layers = 2\n            if n_neurons > 1024:\n                n_neurons = 1024\n            if n_layers > 10:\n                n_layers = 10\n            if n_neurons % 2 != 0:\n                n_neurons -= 1\n            if n_layers % 2 != 0:\n                n_layers -= 1\n            if n_neurons < 128:\n                n_neurons = 128\n            if n_layers < 2:\n                n_layers = 2\n            if n_neurons > 1024:"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the number of elements to trim from the beginning and end of the signal\n        trim_start = kernel_offset\n        trim_end = kernel_offset\n\n        # Calculate the number of elements to shift the signal by\n        shift_start = -kernel_offset\n        shift_end = kernel_offset\n\n        # Calculate the number of elements to pad the signal with\n        pad_start = kernel_offset\n        pad_end = kernel_offset\n\n        # Pad the signal with zeros to account for the shifting process\n        signal_padded = np.pad(signal, (pad_start, pad_end), mode='constant', constant_values=0)\n\n        # Create a list to store the shifted signals\n        shifted_signals = []\n\n        # Shift the signal by the specified offset and append the shifted signal to the list\n        for i in range(shift_start, shift_end + 1):\n            shifted_signals.append(signal_padded[i:i + len(signal)])\n\n        # Calculate the rolling median of the shifted signals\n        rolling_median = np.median(shifted_signals, axis=0)\n\n        # Trim the rolling median to remove edge effects\n        rolling_median = rolling_median[trim_start:-trim_end]\n\n        return rolling_median\n\n"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Calculate the Hamming distance for each rotation shift\n    hamming_distances = []\n    for i in range(rotation_shift + 1):\n        # Calculate the Hamming distance between the two templates\n        hamming_distance = hamming_distance_calculation(\n            template_probe, template_gallery, i, nm_dist, weights\n        )\n        hamming_distances.append(hamming_distance)\n\n    # Find the minimum Hamming distance and its corresponding rotation shift\n    min_hamming_distance = min(hamming_distances)\n    min_hamming_distance_index = hamming_distances.index(min_hamming_distance)\n\n    return min_hamming_distance, min_hamming_distance_index\n\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize the number of iterations to 0\n        iteration = 0\n\n        # Initialize the number of bisectors to calculate\n        number_of_bisectors_to_calculate = self.number_of_bisectors_to_calculate\n\n        # Initialize the list of bisectors\n        bisectors = []\n\n        # Initialize the list of bisector start points\n        bisector_start_points = []\n\n        # Initialize the list of bisector end points\n        bisector_end_points = []\n\n        # Initialize the list of bisector start points\n        bisector_start_points_in_px = []\n\n        # Initialize the list of bisector end points\n        bisector_end_points_in_px = []\n\n        # Initialize the list of bisector start points\n        bisector_start_points_in_mm = []\n\n        # Initialize the list of bisector end points\n        bisector_end_points_in_mm = []\n\n        # Initialize the list of bisector start points\n        bisector_start_points_in_mm_and_px = []\n\n        # Initialize the list of bisector end points\n        bisector_end_points_in_mm_and_px = []\n\n        # Initialize the list of bisector start points\n        bisector_start_points_in_mm_and_px_and_deg = []\n\n        # Initialize the list of bisector end points\n        bisector_end_points_in_mm_and_px_and_deg = []\n\n        # Initialize the list of bisector start points\n        bisector_start_points_in_mm_and_px_and_deg_and_rad = []\n\n        # Initialize the list of bisector end points\n        bisector_end_points_in_mm_and_px_and_deg_and_rad = []\n\n        # Initialize the list of bisector start points\n        bisector_start_points_in_mm_and_px_and_deg_and_rad_and_radians = []\n\n        # Initialize the list of bisector end"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n        result = self.run(*args, **kwargs)\n        for callback in self._callbacks:\n            callback.post_execute(self, *args, **kwargs)\n        return result\n\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except json.JSONDecodeError:\n            return False\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n\n        for param_name, param in signature.parameters.items():\n            if param.kind == param.VAR_KEYWORD:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        for i in range(self.hash_count):\n            self.bit_array[self.hash_func(string, i)] = 1\n        return\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        try:\n            bit_array = self.persistence.load()\n            if len(bit_array) != self.size:\n                logging.warning(\"Bit array length mismatch. Reinitializing bit array.\")\n                self.init_bit_array()\n                self.save()\n        except FileNotFoundError:\n            logging.warning(\"Bit array not found. Initializing bit array.\")\n            self.init_bit_array()\n            self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        # Check if the string is empty\n        if not string:\n            return False\n\n        # Generate indices using the hash functions\n        indices = [self.hash_functions[i](string) % self.size for i in range(self.hash_count)]\n\n        # Check if all bits at the generated indices are set in the bit array\n        for index in indices:\n            if not self.bit_array[index]:\n                return False\n\n        return True"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = json_dict.get('distilled_model', None)\n        self.current_model_stats = json_dict.get('current_model_stats', None)\n        self.last_training_run = json_dict.get('last_training_run', None)\n        self.current_training_run = json_dict.get('current_training_run', None)\n        self.nr_of_training_runs = json_dict.get('nr_of_training_runs', None)\n        self.teacher_models = json_dict.get('teacher_models', None)\n\n        return self"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Verify API key\n        if not self.api_key:\n            raise ValueError(\"API key is not set\")\n\n        # Validate parameters\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"model must be an instance of OpenAIConfig\")\n        if not isinstance(system_message, str):\n            raise ValueError(\"system_message must be a string\")\n        if not isinstance(prompt, str):\n            raise ValueError(\"prompt must be a string\")\n        if not isinstance(kwargs, dict):\n            raise ValueError(\"kwargs must be a dictionary\")\n\n        # Set default values for optional parameters\n        kwargs.setdefault(\"temperature\", 0.7)\n        kwargs.setdefault(\"top_p\", 1)\n        kwargs.setdefault(\"frequency_penalty\", 0)\n        kwargs.setdefault(\"presence_penalty\", 0)\n        kwargs.setdefault(\"max_new_tokens\", 100)\n\n        # Set up API request\n        headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n        data = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n            **kwargs,\n        }\n\n        # Retry up to 5 times with exponential backoff\n        for i in range(5):\n            try:\n                response = requests.post(\n                    \"https://api.openai.com/v1/chat/completions\",\n                    headers=headers,\n                    json=data,\n                )\n                response.raise_for_status()\n                break\n            except requests.exceptions.RequestException as e:\n                if i < 4:\n                    time.sleep(2 ** i)\n                else:\n                    raise e\n\n        # Process response\n        response_json = response.json()\n        response_text = response_json[\"choices\"][0][\"message\"][\"content\"]\n\n        # Remove parsing helper tokens"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"Matrix is not square\")\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Matrix is not symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if not np.allclose(x, x.T):\n        raise ValueError(\"The given matrix is not symmetric.\")\n\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal elements of the given matrix are not close to zero.\")\n\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Check if the function is already initialized\n        if func_hash not in self.function_data:\n            # Initialize the function data\n            self.initialize_function_data(function_description, func_hash)\n\n        # Check if the function is suitable for distillation\n        if self.is_suitable_for_distillation(function_description, func_hash):\n            # Use a distilled model for zero-shot prompting\n            model = self.distilled_model\n            prompt = self.construct_distilled_prompt(function_description, args, kwargs)\n            is_suitable_for_distillation = True\n            is_already_initialized = False\n        else:\n            # Use a teacher model for fine-tuning\n            model = self.teacher_model\n            prompt = self.construct_teacher_prompt(function_description, args, kwargs)\n            is_suitable_for_distillation = False\n            is_already_initialized = True\n\n        # Update examples for fine-tuning if necessary\n        if not is_already_initialized:\n            self.update_examples_for_fine_tuning(function_description, args, kwargs, func_hash)\n\n        # Return the prompt, model, and distillation suitability\n        return prompt, model, is_suitable_for_distillation, is_already_initialized\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Use the Higham & Nick (2002) algorithm to find the nearest positive definite matrix\n        cov_nearest = cov.copy()\n        for _ in range(higham_max_iteration):\n            # Compute the eigenvalues and eigenvectors of the covariance matrix\n            eigvals, eigvecs = np.linalg.eigh(cov_nearest)\n\n            # Clip the eigenvalues to ensure they are positive\n            eigvals = np.maximum(eigvals, 0)\n\n            # Reconstruct the covariance matrix from the clipped eigenvalues and eigenvectors\n            cov_nearest = eigvecs @ np.diag(eigvals) @ eigvecs.T\n\n            # Check if the covariance matrix is positive definite\n            if np.all(np.linalg.eigvals(cov_nearest) > 0):\n                break\n        else:\n            raise ValueError(\"The Higham & Nick (2002) algorithm did not converge.\")\n    else:\n        # Clip the eigenvalues to ensure they are positive\n        eigvals, eigvecs = np.linalg.eigh(cov)\n        eigvals = np.maximum(eigvals, 0)\n\n        # Reconstruct the covariance matrix from the clipped eigenvalues and eigenvectors\n        cov_nearest = eigvecs @ np.diag(eigvals) @ eigvecs.T\n\n    return cov_nearest"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    from pathlib import Path\n    from shutil import rmtree\n\n    data_home = Path(data_home) if data_home else Path.home() / \"skfolio_data\"\n    rmtree(data_home, ignore_errors=True)\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj, lambda x: x)\n    elif isinstance(obj, list):\n        return (tuple(obj), lambda x: list(x))\n    elif isinstance(obj, tuple):\n        return (obj, lambda x: x)\n    elif isinstance(obj, dict):\n        return (tuple(obj.items()), lambda x: dict(x))\n    elif isinstance(obj, Instances):\n        return (\n            (\n                obj.pred_boxes,\n                obj.gt_boxes,\n                obj.gt_classes,\n                obj.scores,\n                obj.pred_masks,\n                obj.gt_masks,\n                obj.gt_keypoints,\n                obj.gt_keypoint_heatmaps,\n                obj.gt_keypoint_weights,\n                obj.gt_keypoint_weights_heatmap,\n                obj.gt_keypoint_weights_heatmap_mask,\n                obj.gt_keypoint_weights_heatmap_mask_mask,\n                obj.gt_keypoint_weights_heatmap_mask_mask_mask,\n                obj.gt_keypoint_weights_heatmap_mask_mask_mask_mask,\n                obj.gt_keypoint_weights_heatmap_mask_mask_mask_mask_mask,\n                obj.gt_keypoint_weights_heatmap_mask_mask_mask_mask_mask_mask,\n                obj.gt_keypoint_weights_heatmap_mask_mask_mask_mask_mask_mask_mask,\n                obj.gt_keypoint_weights_heatmap_mask_mask_mask_mask_mask_mask_mask_mask,\n                obj.gt_keypoint_weights_heatmap_mask_mask_mask_mask_mask_mask_mask_mask_mask,\n                obj.gt_keypoint_weights_heatmap_mask_mask_mask_mask_mask_mask_mask_mask_mask_mask,\n                obj"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Check if the input arrays are valid\n    if not isinstance(groups, np.ndarray):\n        raise TypeError(\n            f\"The '{names[0]}' argument must be a numpy array. Got {type(groups)} instead.\"\n        )\n    if not isinstance(equations, np.ndarray):\n        raise TypeError(\n            f\"The '{names[1]}' argument must be a numpy array. Got {type(equations)} instead.\"\n        )\n    if groups.ndim != 2:\n        raise ValueError(\n            f\"The '{names[0]}' argument must be a 2D array. Got {groups.ndim} dimensions instead.\"\n        )\n    if equations.ndim != 1:\n        raise ValueError(\n            f\"The '{names[1]}' argument must be a 1D array. Got {equations.ndim} dimensions instead.\"\n        )\n\n    # Check if the input arrays have the same number of rows\n    if groups.shape[0] != equations.shape[0]:\n        raise ValueError(\n            f\"The number of rows in the '{names[0]}' array must match the number of rows in the '{names[1]}' array.\"\n        )\n\n    # Check if the input arrays have the same number of columns\n    if groups.shape[1] != equations.shape[1]:\n        raise ValueError(\n            f\"The number of columns in the '{names[0]}' array must match the number of columns in the '{names[1]}' array.\"\n        )\n\n    # Check if the input arrays have the same number of rows\n    if groups.shape[0] != equations.shape[0]:\n        raise ValueError(\n            f\"The number of rows in the '{names[0]}' array must match the number of rows in the '{names[1]}' array.\"\n        )\n\n    # Check if the input arrays have the same number of columns\n    if groups.shape[1] != equations.shape[1]:\n        raise ValueError(\n            f\"The number of columns in the '{names[0]}' array must"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    import os\n    import sys\n    import tempfile\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.utils.data\n    import torch.utils.model_zoo\n    import torchvision\n    import torchvision.transforms as transforms\n    import torchvision.models as models\n    import torchvision.datasets as datasets\n    import torchvision.utils as vutils\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import matplotlib.animation as animation\n    from matplotlib import gridspec\n    from IPython.display import HTML\n    from torch.autograd import Variable\n    from torch.nn import functional as F\n    from torch.autograd import Variable\n    from torch.utils.data import DataLoader\n    from torchvision import transforms\n    from torchvision import datasets\n    from torchvision.utils import save_image\n    from torch.utils.data import DataLoader\n    from torchvision import transforms\n    from torchvision.utils import save_image\n    from torch.autograd import Variable\n    from torch.nn import functional as F\n    from torchvision.utils import save_image\n    from torch.autograd import Variable\n    from torch.nn import functional as F\n    from torchvision.utils import save_image\n    from torch.autograd import Variable\n    from torch.nn import functional as F\n    from torchvision.utils import save_image\n    from torch.autograd import Variable\n    from torch.nn import functional as F\n    from torchvision.utils import save_image\n    from torch.autograd import Variable\n    from torch.nn import functional as F\n    from torchvision.utils import save_image\n    from torch.autograd import Variable\n    from torch.nn import functional as F\n    from torchvision.utils import save_image\n    from torch.autograd import Variable\n    from torch.nn import"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # Read the image using PIL\n    image = Image.open(file_name)\n\n    # Check if the image has EXIF data\n    if hasattr(image, 'getexif'):\n        exif_data = image.getexif()\n\n        # Check if the image is rotated\n        if exif_data.get(274, 0) == 3:\n            image = image.rotate(180, expand=True)\n        elif exif_data.get(274, 0) == 6:\n            image = image.rotate(270, expand=True)\n        elif exif_data.get(274, 0) == 8:\n            image = image.rotate(90, expand=True)\n\n    # Convert the image to the specified format\n    if format is not None:\n        if format in ['BGR', 'YUV-BT.601']:\n            image = np.array(image)\n            if format == 'BGR':\n                image = image[:, :, ::-1]\n            elif format == 'YUV-BT.601':\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)\n                image = image[:, :, 0]\n        else:\n            image = image.convert(format)\n\n    # Convert the image to a numpy array\n    image = np.array(image)\n\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # Transform bounding box\n    if \"bbox\" in annotation:\n        bbox = transforms.apply_box([annotation[\"bbox\"]])[0]\n        annotation[\"bbox\"] = bbox\n        annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform segmentation\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            transformed_segmentation = []\n            for seg in annotation[\"segmentation\"]:\n                transformed_segmentation.append(transforms.apply_coords(np.array(seg).reshape(-1, 2)).reshape(-1).tolist())\n            annotation[\"segmentation\"] = transformed_segmentation\n        elif isinstance(annotation[\"segmentation\"], dict):\n            # RLE\n            transformed_segmentation = transforms.apply_coords(\n                np.array(annotation[\"segmentation\"][\"counts\"], dtype=np.uint8).reshape(-1, 2)\n            ).reshape(-1).tolist()\n            annotation[\"segmentation\"][\"counts\"] = transformed_segmentation\n        else:\n            raise ValueError(\"Unknown segmentation format: {}\".format(annotation[\"segmentation\"]))\n\n    # Transform keypoints\n    if \"keypoints\" in annotation:\n        keypoints = np.array(annotation[\"keypoints\"]).reshape(-1, 3)\n        keypoints[:, :2] = transforms.apply_coords(keypoints[:, :2]).reshape(-1)\n        if keypoint_hflip_indices is not None:\n            keypoints[keypoint_hflip_indices] = keypoints[keypoint_hflip_indices][:, [1, 0, 2]]\n        annotation[\"keypoints\"] = keypoints.reshape(-1).tolist()\n\n    return annotation"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if coords.size == 0:\n            return coords\n\n        if self.angle % 360 == 0:\n            return coords\n\n        coords = np.array(coords)\n        coords = np.dot(coords, self.rm_coords)\n\n        return coords"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Initialize a defaultdict to store the flops count for each operator\n    flops_dict = collections.defaultdict(float)\n\n    # Define a function that computes the flops for a single input\n    def compute_flops(input_dict):\n        # Extract the image from the input dictionary\n        image = input_dict[\"image\"]\n\n        # Run the model with the image and compute the flops\n        with torch.no_grad():\n            model(image)\n            flops = profile(model, inputs=(image,))[0]\n\n        # Return the flops\n        return flops\n\n    # Compute the flops for each input and accumulate them in the flops_dict\n    for input_dict in inputs:\n        flops = compute_flops(input_dict)\n        for op_name, op_flops in flops.items():\n            flops_dict[op_name] += op_flops\n\n    # Divide the flops by the number of inputs to obtain the average flops per input\n    num_inputs = len(inputs)\n    for op_name in flops_dict:\n        flops_dict[op_name] /= num_inputs\n\n    # Return the flops_dict\n    return flops_dict"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img is None or self.angle == 0:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        h, w = img.shape[:2]\n        rm_image = self.rm_image\n\n        # Compute the bounding dimensions after rotation\n        bound_w = int(abs(rm_image[0, 0] * w + rm_image[0, 1] * h))\n        bound_h = int(abs(rm_image[1, 0] * w + rm_image[1, 1] * h))\n\n        # Compute the translation vector for the rotation\n        tx = -rm_image[0, 2] * w\n        ty = -rm_image[1, 2] * h\n\n        # Compute the translation matrix for the rotation\n        tm = np.float32([[1, 0, tx], [0, 1, ty]])\n\n        # Compute the final transformation matrix\n        rm = rm_image + tm\n\n        # Apply the rotation transformation using OpenCV's warpAffine function\n        img_rot = cv2.warpAffine(img, rm, (bound_w, bound_h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT,\n                                 borderValue=(0, 0, 0))\n\n        return img_rot"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Create an empty image for drawing the predictions on\n        img = self.output.get_image()\n        # Get the height and width of the image\n        height, width = img.shape[:2]\n        # Create a VisImage object to store the visualizations\n        vis_output = VisImage(img)\n        # Convert the predictions to a list of instances\n        predictions = predictions.to(\"cpu\")\n        # Loop through each instance in the list\n        for i, pred in enumerate(predictions):\n            # Get the predicted class and score for the instance\n            class_name = self.metadata.get(\"thing_classes\", [\"object\"])[pred.pred_classes.item()]\n            score = pred.scores.item()\n            # Get the predicted bounding box coordinates\n            bbox = pred.pred_boxes.tensor.cpu().numpy()[0]\n            # Convert the bounding box coordinates to integers\n            bbox = tuple(int(x) for x in bbox)\n            # Get the predicted mask for the instance\n            mask = pred.pred_masks.cpu().numpy()[i]\n            # Convert the mask to a binary image\n            mask = mask > 0.5\n            # Get the predicted keypoints for the instance\n            keypoints = pred.pred_keypoints.cpu().numpy()[i]\n            # Convert the keypoints to integers\n            keypoints = tuple(int(x) for x in keypoints)\n            # Draw the predicted bounding box on the image\n            vis_output = self.draw_box(vis_output, bbox, class_name, score)\n            # Draw the predicted mask on the image\n            vis_output = self.draw_mask(vis_output, mask)\n            # Draw the predicted keypoints on the image\n            vis_output = self.draw_keypoints(vis_output, keypoints)\n        # Return the VisImage object with the visualizations drawn on it\n        return vis_output"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        image = self.canvas.get_image()\n        image = image.convert('RGB')\n        image = np.array(image)\n        return image\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            for ann in dic[\"annotations\"]:\n                self.draw_instance_predictions(ann)\n        if \"sem_seg_file_name\" in dic:\n            sem_seg = cv2.imread(dic[\"sem_seg_file_name\"], cv2.IMREAD_UNCHANGED)\n            self.draw_sem_seg(sem_seg)\n        if \"panoptic_seg_file_name\" in dic:\n            panoptic_seg = cv2.imread(dic[\"panoptic_seg_file_name\"], cv2.IMREAD_UNCHANGED)\n            self.draw_panoptic(panoptic_seg)\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = self.get_random_color()\n\n        if edge_color is None:\n            edge_color = color\n\n        if text is not None:\n            text_color = self.get_random_color()\n\n        # Convert the binary mask to a polygon mask\n        polygon_mask = self.binary_mask_to_polygon_mask(binary_mask)\n\n        # Draw the polygon mask on the image\n        self.draw_polygon_mask(polygon_mask, color=color, edge_color=edge_color, alpha=alpha)\n\n        # Draw the text on the mask if specified\n        if text is not None:\n            self.draw_text(text, polygon_mask.bbox.center, color=text_color)\n\n        return self"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), f\"{msg} Expected input to be Instances, got {type(input)}\"\n    assert isinstance(other, Instances), f\"{msg} Expected other to be Instances, got {type(other)}\"\n\n    if size_as_tensor:\n        assert torch.allclose(input.image_size, other.image_size), f\"{msg} image_size mismatch: {input.image_size} vs {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg} image_size mismatch: {input.image_size} vs {other.image_size}\"\n\n    for k in input.fields():\n        if k == \"gt_masks\":\n            assert torch.allclose(input.gt_masks, other.gt_masks), f\"{msg} gt_masks mismatch\"\n        elif k == \"gt_boxes\":\n            assert torch.allclose(input.gt_boxes, other.gt_boxes), f\"{msg} gt_boxes mismatch\"\n        elif k == \"proposal_boxes\":\n            assert torch.allclose(input.proposal_boxes, other.proposal_boxes), f\"{msg} proposal_boxes mismatch\"\n        elif k == \"pred_boxes\":\n            assert torch.allclose(input.pred_boxes, other.pred_boxes), f\"{msg} pred_boxes mismatch\"\n        elif k == \"pred_classes\":\n            assert torch.allclose(input.pred_classes, other.pred_classes), f\"{msg} pred_classes mismatch\"\n        elif k == \"scores\":\n            assert torch.allclose(input.scores, other.scores), f\"{msg} scores mismatch\"\n        elif k == \"pred_masks\":\n            assert torch.allclose(input.pred_masks, other.pred_masks), f\"{msg} pred_masks mismatch\"\n        elif k == \"pred_"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.width * self.height\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n    else:\n        from .proposal_generator import build_proposal_generator\n        return build_proposal_generator(cfg, input_shape)"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # Extract the scores and proposal deltas from the predictions\n        scores, proposal_deltas = predictions\n\n        # Extract the ground truth boxes and classes from the proposals\n        gt_boxes = [x.gt_boxes for x in proposals]\n        gt_classes = [x.gt_classes for x in proposals]\n\n        # Calculate the classification loss\n        loss_cls = self.cls_loss(scores, gt_classes)\n\n        # Calculate the box regression loss\n        loss_box_reg = self.box_reg_loss(proposal_deltas, gt_boxes, gt_classes)\n\n        # Return the losses as a dictionary\n        return {\n            \"loss_cls\": loss_cls * self.loss_weight[\"loss_cls\"],\n            \"loss_box_reg\": loss_box_reg * self.loss_weight[\"loss_box_reg\"],\n        }"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACK.NAME\n    tracker_class = get_tracker_class(tracker_name)\n    tracker = tracker_class(cfg)\n    return tracker"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # Extract the width and height of each box\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n\n        # Extract the center coordinates of each box\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        # Extract the deltas for the center coordinates\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n\n        # Extract the deltas for the width and height\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        # Calculate the predicted center coordinates\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n\n        # Calculate the predicted width and height\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        # Calculate the predicted bounding box coordinates\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        # Process the image\n        output = self.process(image)\n\n        # Filter the output based on the annotation type(s)\n        if anno_type is not None:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            filtered_output = {}\n            for anno in anno_type:\n                if anno in output:\n                    filtered_output[anno] = output[anno]\n            if len(filtered_output) == 1:\n                return list(filtered_output.values())[0]\n            elif len(filtered_output) > 1:\n                return filtered_output\n            else:\n                return output\n        else:\n            return output"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query string\n        query = self.normalize(query)\n\n        # Split the query into keywords\n        keywords = query.split()\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = {}\n        for keyword in keywords:\n            if keyword in self.inverted_index:\n                for url, score in self.inverted_index[keyword].items():\n                    if url in scores:\n                        scores[url] += score\n                    else:\n                        scores[url] = score\n\n        # Return the scores as a dictionary\n        return scores\n\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize angles to be within (-180, 180] degrees\n        self.tensor[:, 4] = (self.tensor[:, 4] + 180) % 360 - 180\n\n        # Identify indices of nearly horizontal boxes\n        nearly_horizontal_indices = np.abs(self.tensor[:, 4]) < clip_angle_threshold\n\n        # Convert to (x1, y1, x2, y2) representation\n        x1y1 = self.tensor[:, :2] - self.tensor[:, 2:] / 2\n        x2y2 = self.tensor[:, :2] + self.tensor[:, 2:] / 2\n\n        # Clamp x and y coordinates\n        x1y1 = np.maximum(x1y1, 0)\n        x2y2 = np.minimum(x2y2, box_size)\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        self.tensor[:, :2] = (x1y1 + x2y2) / 2\n        self.tensor[:, 2:] = x2y2 - x1y1\n\n        # Convert back to original representation\n        self.tensor = self.to_original_representation()\n\n        # Clip angles again to ensure they are within (-180, 180] degrees\n        self.tensor[:, 4] = (self.tensor[:, 4] + 180) % 360 - 180\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        self.tensor = self.to_original_representation()\n\n        # Clip angles again to ensure they are within (-180, 180] degrees\n        self.tensor[:, 4] = (self.tensor[:, 4] + 180) % 360 - 180\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        self.tensor = self.to_original_representation()"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        # Initialize the statistics dictionary\n        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n\n        # Iterate over the data and update the statistics\n        for item in self.data:\n            if item['type'] == 'doc':\n                statistics['doc'] += 1\n            elif item['type'] == 'gen':\n                statistics['gen'] += 1\n            elif item['type'] == 'kno':\n                statistics['kno'] += 1\n            elif item['type'] == 'num':\n                statistics['num'] += 1\n\n        # Return the statistics dictionary\n        return statistics\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    neck_type = cfg['type']\n    if neck_type in NECKS:\n        return NECKS[neck_type](cfg)\n    else:\n        return MMDET_NECKS[neck_type](cfg)"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    loss_type = cfg['type']\n    if loss_type == 'CrossEntropyLoss':\n        return CrossEntropyLoss(cfg)\n    elif loss_type == 'FocalLoss':\n        return FocalLoss(cfg)\n    elif loss_type == 'DiceLoss':\n        return DiceLoss(cfg)\n    elif loss_type == 'TverskyLoss':\n        return TverskyLoss(cfg)\n    elif loss_type == 'ComboLoss':\n        return ComboLoss(cfg)\n    elif loss_type == 'DiceFocalLoss':\n        return DiceFocalLoss(cfg)\n    elif loss_type == 'DiceF1Loss':\n        return DiceF1Loss(cfg)\n    elif loss_type == 'DiceFocalF1Loss':\n        return DiceFocalF1Loss(cfg)\n    elif loss_type == 'DiceFocalTverskyLoss':\n        return DiceFocalTverskyLoss(cfg)\n    elif loss_type == 'DiceFocalTverskyF1Loss':\n        return DiceFocalTverskyF1Loss(cfg)\n    elif loss_type == 'DiceFocalTverskyF1Loss':\n        return DiceFocalTverskyF1Loss(cfg)\n    elif loss_type == 'DiceFocalTverskyF1Loss':\n        return DiceFocalTverskyF1Loss(cfg)\n    elif loss_type == 'DiceFocalTverskyF1Loss':\n        return DiceFocalTverskyF1Loss(cfg)\n    elif loss_type == 'DiceFocalTverskyF1Loss':\n        return DiceFocalTverskyF1Loss(cfg)\n    elif loss_type == 'DiceFocalTverskyF1Loss':\n        return DiceFocalTverskyF1Loss(cfg)\n    elif loss_type == 'DiceFocalTverskyF1Loss':\n        return DiceFocalTversky"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    from . import HEADS, MMDET_HEADS\n\n    head_type = cfg.MODEL.HEAD.TYPE\n    if head_type in HEADS:\n        return HEADS[head_type](cfg)\n    elif head_type in MMDET_HEADS:\n        return MMDET_HEADS[head_type](cfg)\n    else:\n        raise NotImplementedError(f\"Head type {head_type} not supported\")"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    from .builder import build_segmentor\n    return build_segmentor(cfg, train_cfg, test_cfg)"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            \"train_cfg and test_cfg should be specified in model, \"\n            \"but got {}\".format(train_cfg)\n        )\n    assert (\n        \"train_cfg\" not in cfg.keys() and \"test_cfg\" not in cfg.keys()\n    ), f\"train_cfg and test_cfg are not supposed to be specified at the outer level, but found in {cfg}.\"\n    if cfg[\"type\"] in DETECTORS:\n        return build_from_cfg(cfg, DETECTORS)\n    elif cfg[\"type\"] in MMDET_DETECTORS:\n        return build_from_cfg(cfg, MMDET_DETECTORS)\n    else:\n        raise NotImplementedError(f\"Unkown detector type {cfg['type']}\")"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    from .indoor_eval_utils import indoor_eval_utils\n    return indoor_eval_utils(gt_annos, dt_annos, metric, label2cat, logger, box_type_3d, box_mode_3d)"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == \"LiDAR\":\n        box_class = LiDARBox\n        mode = \"LiDAR\"\n    elif box_type == \"Camera\":\n        box_class = CameraBox\n        mode = \"Camera\"\n    elif box_type == \"Depth\":\n        box_class = DepthBox\n        mode = \"Depth\"\n    else:\n        raise ValueError(\"Invalid box type: {}\".format(box_type))\n\n    return box_class, mode"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('No model provided')\n\n    if messages is None:\n      raise RequestError('No messages provided')\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    if not all(isinstance(message, Message) or isinstance(message, dict) for message in messages):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    if options is None:\n      options = Options()\n\n    if keep_alive is not None:\n      options.keep_alive = keep_alive\n\n    if format == 'json':\n      options.stream = stream\n\n    if format == 'json' and not stream:\n      return self._request(\n        method='POST',\n        path=f'/chat/{model}',\n        json={\n          'messages': [message.to_dict() for message in messages],\n          'options': options.to_dict(),\n        },\n        format=format,\n      )\n\n    if format == 'json' and stream:\n      return self._request(\n        method='POST',\n        path=f'/chat/{model}',\n        json={\n          'messages': [message.to_dict() for message in messages],\n          'options': options.to_dict(),\n        },\n        format=format,\n        stream=True,\n      )\n\n    if format == '' and not stream:\n      return self._request(\n        method='POST',\n        path=f'/chat/{model}',\n        json={\n          'messages': [message.to_dict() for message in messages],\n          'options': options.to_dict(),\n        },\n        format=format,\n      )\n\n    if format == '' and stream:\n      return self._request(\n        method='POST',\n        path=f'/chat/{model}',\n        json={\n          'messages': [message.to_dict() for message in messages],\n          'options': options.to_dict(),\n        },\n        format=format,\n        stream="}
{"namespace": "ollama._client.Client.pull", "completion": "    url = f\"{self.base_url}/pull\"\n    headers = {\"Content-Type\": \"application/json\"}\n    data = {\"model\": model}\n\n    if insecure:\n      url = url.replace(\"https://\", \"http://\")\n\n    if stream:\n      response = requests.post(url, headers=headers, data=json.dumps(data), stream=True)\n      response.raise_for_status()\n      return self._stream_response(response)\n    else:\n      response = requests.post(url, headers=headers, data=json.dumps(data))\n      response.raise_for_status()\n      return response.json()"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError('model must be provided')\n\n    if not prompt and not system:\n      raise ValueError('prompt or system must be provided')\n\n    if not prompt and system:\n      prompt = system\n\n    if not prompt:\n      raise ValueError('prompt must be provided')\n\n    if not template:\n      template = 'default'\n\n    if not context:\n      context = []\n\n    if not images:\n      images = []\n\n    if not options:\n      options = Options()\n\n    if not keep_alive:\n      keep_alive = None\n\n    if isinstance(keep_alive, str):\n      keep_alive = float(keep_alive)\n\n    if not isinstance(keep_alive, (float, int)):\n      raise ValueError('keep_alive must be a float or an integer')\n\n    if not isinstance(context, list):\n      raise ValueError('context must be a list')\n\n    if not isinstance(images, list):\n      raise ValueError('images must be a list')\n\n    if not isinstance(options, Options):\n      raise ValueError('options must be an instance of Options')\n\n    if not isinstance(format, str):\n      raise ValueError('format must be a string')\n\n    if format not in ['', 'json']:\n      raise ValueError('format must be either an empty string or \"json\"')\n\n    if not isinstance(stream, bool):\n      raise ValueError('stream must be a boolean')\n\n    if not isinstance(raw, bool):\n      raise ValueError('raw must be a boolean')\n\n    if not isinstance(model, str):\n      raise ValueError('model must be a string')\n\n    if not isinstance(prompt, str):\n      raise ValueError('prompt must be a string')\n\n    if not isinstance(system, str):\n      raise ValueError('system must be a string')\n\n    if not isinstance(template, str):\n      raise ValueError('template must be a string')\n\n    if not isinstance(context, list):\n      raise Value"}
{"namespace": "ollama._client.Client.push", "completion": "    url = f\"{self.base_url}/api/push\"\n    params = {\"model\": model}\n    headers = {\"Authorization\": f\"Bearer {self.token}\"}\n\n    if insecure:\n      url = url.replace(\"https://\", \"http://\")\n\n    if stream:\n      response = self.session.post(\n        url,\n        params=params,\n        headers=headers,\n        stream=True,\n      )\n      response.raise_for_status()\n      return self._stream_response(response)\n    else:\n      response = self.session.post(url, params=params, headers=headers)\n      response.raise_for_status()\n      return response.json()"}
{"namespace": "ollama._client.Client.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"Either path or modelfile must be provided\")\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        modelfile = f.read()\n\n    if stream:\n      return self._stream(\n        \"POST\",\n        f\"/models/{model}\",\n        data={\"modelfile\": modelfile},\n        response_type=ProgressResponse,\n      )\n    else:\n      return self._request(\n        \"POST\",\n        f\"/models/{model}\",\n        data={\"modelfile\": modelfile},\n        response_type=ProgressResponse,\n      )"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    # Calculate the SHA-256 checksum of the file\n    sha256 = self._calculate_sha256(path)\n\n    # Check if the blob already exists on the server\n    response = self._session.head(f\"{self._base_url}/blobs/{sha256}\")\n\n    # If the blob does not exist, upload it\n    if response.status_code == 404:\n      with open(path, \"rb\") as f:\n        response = self._session.post(f\"{self._base_url}/blobs/uploads/\", data=f)\n\n      # Check if the upload was successful\n      if response.status_code != 200:\n        raise Exception(f\"Failed to upload blob: {response.text}\")\n\n    # Return the SHA-256 digest of the file\n    return sha256\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise ValueError('model must be provided')\n\n    if not prompt and not system:\n      raise ValueError('prompt or system must be provided')\n\n    if not prompt and system:\n      prompt = system\n\n    if not prompt:\n      raise ValueError('prompt must be provided')\n\n    if not template:\n      template = 'default'\n\n    if not format:\n      format = 'json'\n\n    if not options:\n      options = Options()\n\n    if not keep_alive:\n      keep_alive = self.keep_alive\n\n    if not isinstance(keep_alive, (float, str)):\n      raise ValueError('keep_alive must be a float or a string')\n\n    if isinstance(keep_alive, str):\n      if not keep_alive.isdigit():\n        raise ValueError('keep_alive must be a float or a string containing only digits')\n\n    if not isinstance(options, Options):\n      raise ValueError('options must be an instance of Options')\n\n    if not isinstance(images, (list, tuple)):\n      images = []\n\n    if not isinstance(images, (list, tuple)):\n      images = []\n\n    if not isinstance(images, (list, tuple)):\n      images = []\n\n    if not isinstance(images, (list, tuple)):\n      images = []\n\n    if not isinstance(images, (list, tuple)):\n      images = []\n\n    if not isinstance(images, (list, tuple)):\n      images = []\n\n    if not isinstance(images, (list, tuple)):\n      images = []\n\n    if not isinstance(images, (list, tuple)):\n      images = []\n\n    if not isinstance(images, (list, tuple)):\n      images = []\n\n    if not isinstance(images, (list, tuple)):\n      images = []\n\n    if not isinstance(images, (list, tuple)):\n      images = []\n\n    if not isinstance(images, (list, tuple)):\n     "}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    if not model:\n      raise ValueError(\"model is required\")\n\n    url = f\"{self.base_url}/pull/{model}\"\n\n    headers = {\n      \"Authorization\": f\"Bearer {self.token}\",\n      \"Content-Type\": \"application/json\",\n    }\n\n    if insecure:\n      headers[\"Authorization\"] = f\"Bearer {self.token}\"\n\n    async with self.session.get(url, headers=headers, stream=stream) as response:\n      if response.status != 200:\n        raise ResponseError(response.status, response.reason)\n\n      if stream:\n        async for line in response.content:\n          yield json.loads(line)\n      else:\n        return await response.json()"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    # Validate the input parameters\n    if not model:\n      raise ValueError('Model is required')\n    if not messages:\n      raise ValueError('Messages are required')\n    if not isinstance(messages, list):\n      raise ValueError('Messages must be a list')\n    if not all(isinstance(message, dict) for message in messages):\n      raise ValueError('Messages must be a list of dictionaries')\n    if not all(\n      'role' in message and 'content' in message for message in messages\n    ):\n      raise ValueError('Messages must contain \"role\" and \"content\" keys')\n    if not all(\n      isinstance(message['role'], str) and isinstance(message['content'], str)\n      for message in messages\n    ):\n      raise ValueError('\"role\" and \"content\" must be strings')\n    if not all(\n      'images' in message\n      for message in messages\n      if 'images' in message\n    ):\n      raise ValueError('\"images\" must be a list of dictionaries')\n    if not all(\n      isinstance(message['images'], list)\n      for message in messages\n      if 'images' in message\n    ):\n      raise ValueError('\"images\" must be a list')\n    if not all(\n      isinstance(image, dict) for message in messages for image in message.get(\n        'images', []\n      )\n    ):\n      raise ValueError('\"images\" must be a list of dictionaries')\n    if not all(\n      'url' in image for message in messages for image in message.get(\n        'images', []\n      )\n    ):\n      raise ValueError('\"images\" must contain \"url\" key')\n    if not all(\n      isinstance(image['url'], str) for message in messages for image in message.get(\n        'images', []\n      )\n    ):\n      raise ValueError('\"url\" must be a string')\n    if not all(\n      'caption' in image for message in messages for image in message.get(\n        'images', []\n      )\n    ):\n      raise ValueError('"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    url = self.base_url + \"/api/push\"\n\n    headers = {\n      \"accept\": \"application/json\",\n      \"Content-Type\": \"application/json\",\n    }\n\n    params = {\n      \"model\": model,\n      \"insecure\": insecure,\n      \"stream\": stream,\n    }\n\n    response = await self.client.post(\n      url,\n      headers=headers,\n      params=params,\n    )\n\n    if response.status_code == 200:\n      if stream:\n        async for item in response.aiter_lines():\n          yield json.loads(item)\n      else:\n        return json.loads(await response.text())\n    else:\n      raise Exception(f\"Error: {response.status_code}\")"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    # Calculate the SHA256 checksum of the file\n    checksum = await self._calculate_checksum(path)\n\n    # Check if the blob already exists on the server\n    response = await self._head_blob(checksum)\n    if response.status_code == 200:\n      # The blob already exists, so return the digest\n      return response.headers['Docker-Content-Digest']\n\n    # The blob does not exist, so upload it to the server\n    response = await self._upload_blob(path, checksum)\n    if response.status_code != 201:\n      raise Exception(f'Failed to upload blob: {response.status_code} {response.reason}')\n\n    # Return the digest of the uploaded blob\n    return response.headers['Docker-Content-Digest']\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine user code and test code\n        combined_code = user_code + test_code\n\n        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(combined_code)\n            f.flush()\n            file_path = f.name\n\n        # Run Pyright to perform type checking\n        result = subprocess.run(\n            [\"pyright\", file_path],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            universal_newlines=True,\n        )\n\n        # Remove the temporary file\n        os.remove(file_path)\n\n        # Check if the type check passed\n        if result.returncode == 0:\n            return TypeCheckResult(\n                passed=True,\n                message=\"Type check passed\",\n            )\n\n        # Extract the expected type errors from the output\n        expected_errors = []\n        for line in result.stdout.splitlines():\n            if \"Expected type\" in line:\n                expected_errors.append(line)\n\n        # Check if there are any expected type errors\n        if not expected_errors:\n            return TypeCheckResult(\n                passed=False,\n                message=\"Type check failed\",\n            )\n\n        # Return a result indicating that the type check passed with expected type errors\n        return TypeCheckResult(\n            passed=True,\n            message=\"Type check passed with expected type errors\",\n            expected_errors=expected_errors,\n        )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path is None and modelfile is None:\n      raise RequestError(\"Either path or modelfile must be specified\")\n\n    if path is not None:\n      with open(path, \"rb\") as f:\n        data = f.read()\n    else:\n      data = modelfile\n\n    if stream:\n      async for response in self._request(\n        \"POST\",\n        f\"/models/{model}\",\n        data=data,\n        stream=True,\n      ):\n        yield response\n    else:\n      response = await self._request(\n        \"POST\",\n        f\"/models/{model}\",\n        data=data,\n      )\n      return response\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module_printer(fn)\n    else:\n        return aot_function_printer(fn)\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Read the summary file\n    summary_path = os.path.join(trial_path, \"summary.csv\")\n    summary_df = pd.read_csv(summary_path)\n\n    # Get the best trial number\n    best_trial_num = summary_df[\"trial_num\"].iloc[0]\n\n    # Read the configuration file\n    config_path = os.path.join(trial_path, \"config.yaml\")\n    with open(config_path, \"r\") as f:\n        config = yaml.safe_load(f)\n\n    # Extract the best pipeline configuration\n    best_config = config[\"pipeline\"][best_trial_num]\n\n    # Save the configuration to a YAML file if output_path is provided\n    if output_path is not None:\n        with open(output_path, \"w\") as f:\n            yaml.safe_dump(best_config, f)\n\n    return best_config"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.utils.data\n    import torch.utils.data.dataloader\n    import torch.utils.data.dataset\n    import torch.utils.data.sampler\n    import torch.utils.data.distributed\n    import torch.utils.data.graph\n    import torch.utils.data.graph_settings\n    import torch.utils.data.dataloader_internals\n    import torch.utils.data.datapipes.iter\n    import torch.utils.data.datapipes.iter.util\n    import torch.utils.data.datapipes.iter.filebased\n    import torch.utils.data.datapipes.iter.filebased.fileopener\n    import torch.utils.data.datapipes.iter.filebased.fileopener.fileopener_utils\n    import torch.utils.data.datapipes.iter.filebased.fileopener.fileopener_utils.file_exts\n    import torch.utils.data.datapipes.iter.filebased.fileopener.fileopener_utils.file_exts.image\n    import torch.utils.data.datapipes.iter.filebased.fileopener.fileopener_utils.file_exts.image.image_utils\n    import torch.utils.data.datapipes.iter.filebased.fileopener.fileopener_utils.file_exts.image.image_utils.image_decoder\n    import torch.utils.data.datapipes.iter.filebased.fileopener.fileopener_utils.file_exts.image.image_utils.image_decoder.image_decoder_utils\n    import torch.utils.data.datapipes.iter.filebased.fileopener.fileopener_utils.file_exts.image.image_utils.image_decoder.image_decoder_utils.image_decoder_utils_cpu\n    import torch.utils.data.dat"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        # Extract the best configuration from the trial folder\n        best_config = cls.extract_best_config(trial_path)\n\n        # Set the project directory to the parent directory of the trial folder\n        project_dir = os.path.dirname(trial_path)\n\n        # Initialize the Runner with the best configuration and project directory\n        return cls(best_config, project_dir)\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create a directory for this node line if it doesn't exist\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Initialize a list to store the results of each module\n    results = []\n\n    # Initialize a list to store the execution times of each module\n    execution_times = []\n\n    # Initialize a list to store the evaluation metrics of each module\n    evaluation_metrics = []\n\n    # Initialize a list to store the evaluation results of each module\n    evaluation_results = []\n\n    # Initialize a list to store the selected results of each module\n    selected_results = []\n\n    # Initialize a list to store the selected results of each module\n    selected_results_dfs = []\n\n    # Initialize a list to store the selected results of each module\n    selected_results_dfs_with_previous_result = []\n\n    # Initialize a list to store the selected results of each module\n    selected_results_dfs_with_previous_result_and_context = []\n\n    # Initialize a list to store the selected results of each module\n    selected_results_dfs_with_previous_result_and_context_and_strategies = []\n\n    # Initialize a list to store the selected results of each module\n    selected_results_dfs_with_previous_result_and_context_and_strategies_and_strategies = []\n\n    # Initialize a list to store the selected results of each module\n    selected_results_dfs_with_previous_result_and_context_and_strategies_and_strategies_and_strategies = []\n\n    # Initialize a list to store the selected results of each module\n    selected_results_dfs_with_previous_result_and_context_and_strategies_and_strategies_and_strategies_and_strategies = []\n\n    # Initialize a list to store the selected results of each module\n    selected_results_dfs_with_previous_result_and_context_and_strategies_and_strategies_and_strategies"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create a directory for the current node line if it doesn't exist\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Initialize a list to store the results of each module\n    results = []\n\n    # Initialize a list to store the execution times of each module\n    execution_times = []\n\n    # Initialize a list to store the evaluation metrics of each module\n    evaluation_metrics = []\n\n    # Initialize a list to store the evaluation results of each module\n    evaluation_results = []\n\n    # Initialize a list to store the evaluation results of each module\n    evaluation_results = []\n\n    # Initialize a list to store the evaluation results of each module\n    evaluation_results = []\n\n    # Initialize a list to store the evaluation results of each module\n    evaluation_results = []\n\n    # Initialize a list to store the evaluation results of each module\n    evaluation_results = []\n\n    # Initialize a list to store the evaluation results of each module\n    evaluation_results = []\n\n    # Initialize a list to store the evaluation results of each module\n    evaluation_results = []\n\n    # Initialize a list to store the evaluation results of each module\n    evaluation_results = []\n\n    # Initialize a list to store the evaluation results of each module\n    evaluation_results = []\n\n    # Initialize a list to store the evaluation results of each module\n    evaluation_results = []\n\n    # Initialize a list to store the evaluation results of each module\n    evaluation_results = []\n\n    # Initialize a list to store the evaluation results of each module\n    evaluation_results = []\n\n    # Initialize a list to store the evaluation results of each module\n    evaluation_results = []\n\n    # Initialize a list to store the evaluation results of each module\n    evaluation_results = []\n\n    # Initialize a list to store the evaluation results of each module\n    evaluation_results = []\n\n    # Initialize a list to store the evaluation results of each module\n    evaluation_results = []\n\n    # Initialize a list to store the evaluation results of each module"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    create_dir(node_line_dir)\n    create_dir(os.path.join(node_line_dir, \"prompt_maker\"))\n    create_dir(os.path.join(node_line_dir, \"prompt_maker\", \"results\"))\n    create_dir(os.path.join(node_line_dir, \"prompt_maker\", \"summary\"))\n\n    # Run prompt maker modules\n    results = []\n    for module, params in zip(modules, module_params):\n        result = module(**params)\n        results.append(result)\n\n    # Evaluate prompt maker modules\n    evaluations = []\n    for result in results:\n        evaluation = evaluate_prompt_maker(result, strategies[\"generator_module\"])\n        evaluations.append(evaluation)\n\n    # Select the best prompt maker module\n    best_prompt_maker_index = select_best_prompt_maker(evaluations, strategies)\n    best_prompt_maker_result = results[best_prompt_maker_index]\n\n    # Combine the results of the previous operation and the best prompt maker's output\n    combined_result = combine_results(previous_result, best_prompt_maker_result)\n\n    # Save the results and a summary\n    save_results(combined_result, node_line_dir)\n    save_summary(evaluations, node_line_dir)\n\n    return combined_result\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        for module in node.modules:\n            for module_param in module.module_params:\n                if module_param.key == key:\n                    values.append(module_param.value)\n    return list(set(values))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = SentenceTransformer('all-mpnet-base-v2')\n\n    pred_embedding = embedding_model.encode(pred)\n    gt_embeddings = embedding_model.encode(generation_gt)\n\n    cosine_similarities = cosine_similarity(pred_embedding, gt_embeddings)\n    return np.max(cosine_similarities)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    try:\n        from gfpgan import GFPGANer\n        from basicsr.utils.download_util import load_file_from_url\n        from basicsr.utils.img_util import img2tensor, imwrite, tensor2img\n        from basicsr.utils.registry import ARCH_REGISTRY\n        from basicsr.utils.options import dict2str\n        from basicsr.utils.options import get_option_class\n        from basicsr.utils.options import parse_options\n        from basicsr.utils.options import set_random_seed\n        from basicsr.utils.options import set_random_seed\n        from basicsr.utils.options import set_random_seed\n        from basicsr.utils.options import set_random_seed\n        from basicsr.utils.options import set_random_seed\n        from basicsr.utils.options import set_random_seed\n        from basicsr.utils.options import set_random_seed\n        from basicsr.utils.options import set_random_seed\n        from basicsr.utils.options import set_random_seed\n        from basicsr.utils.options import set_random_seed\n        from basicsr.utils.options import set_random_seed\n        from basicsr.utils.options import set_random_seed\n        from basicsr.utils.options import set_random_seed\n        from basicsr.utils.options import set_random_seed\n        from basicsr.utils.options import set_random_seed\n        from basicsr.utils.options import set_random_seed\n        from basicsr.utils.options import set_random_seed\n        from basicsr.utils.options import set_random_seed\n        from basicsr.utils.options import set_random_seed\n        from basicsr.utils.options import set_random_seed\n        from basicsr.utils.options import set_random_seed\n        from basicsr.utils.options import set_random_seed\n        from basicsr.utils.options import set_random_seed\n        from basicsr.utils.options import set_random_seed\n        from basicsr.utils.options import set_random_seed\n        from basicsr.utils.options"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        from face_restorer import FaceRestorerCodeFormer\n        face_restorer = FaceRestorerCodeFormer(dirname)\n        face_restorers.append(face_restorer)\n    except Exception as e:\n        print(f\"Error setting up model: {e}\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        import facexlib\n        import gfpgan\n        import torch\n\n        facexlib.initialize_facexlib()\n        facexlib.patch_facexlib(dirname)\n        gfpgan.initialize_gfpgan(dirname)\n        gfpgan.load_gfpgan(dirname)\n        gfpgan.load_face_aligner(dirname)\n        gfpgan.load_face_parser(dirname)\n        gfpgan.load_restorers(dirname)\n        gfpgan.load_face_enhancer(dirname)\n        gfpgan.load_face_editor(dirname)\n        gfpgan.load_face_editor_v2(dirname)\n        gfpgan.load_face_editor_v3(dirname)\n        gfpgan.load_face_editor_v4(dirname)\n        gfpgan.load_face_editor_v5(dirname)\n        gfpgan.load_face_editor_v6(dirname)\n        gfpgan.load_face_editor_v7(dirname)\n        gfpgan.load_face_editor_v8(dirname)\n        gfpgan.load_face_editor_v9(dirname)\n        gfpgan.load_face_editor_v10(dirname)\n        gfpgan.load_face_editor_v11(dirname)\n        gfpgan.load_face_editor_v12(dirname)\n        gfpgan.load_face_editor_v13(dirname)\n        gfpgan.load_face_editor_v14(dirname)\n        gfpgan.load_face_editor_v15(dirname)\n        gfpgan.load_face_editor_v16(dirname)\n        gfpgan.load_face_editor_v17(dirname)\n        gfpgan.load_face_editor_v18(dirname)\n        gfpgan.load_face_editor_v19(dirname)\n        gfpgan."}
{"namespace": "quaternion.rotate", "completion": "  q = np.array(q)\n  v = np.array(v)\n  qv = np.array([0.0] + list(v))\n  q_conj = np.array([q[0]] + [-n for n in q[1:]])\n  return qv * q @ q_conj\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  axis_angle = axis_angle / jnp.linalg.norm(axis_angle)\n  angle = jnp.linalg.norm(axis_angle)\n  axis_angle = axis_angle / (angle + eps)\n  angle = angle * 0.5\n  return jnp.array([jnp.cos(angle), *(jnp.sin(angle) * axis_angle)])\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get topk\n    topk = model.topk(prefix, k=k)\n    # get the index of the target index\n    target_idx = topk.indices.tolist().index(idx)\n    # get the log probability of the target index\n    target_log_prob = topk.logits[target_idx].item()\n    # get the number of calls made to the model\n    num_calls = model.num_calls\n    # adjust the search bias until the target index is the most probable\n    while target_idx != 0:\n        # adjust the search bias\n        model.set_search_bias(target_idx, high)\n        # get topk\n        topk = model.topk(prefix, k=k)\n        # get the index of the target index\n        target_idx = topk.indices.tolist().index(idx)\n        # get the log probability of the target index\n        target_log_prob = topk.logits[target_idx].item()\n        # get the number of calls made to the model\n        num_calls = model.num_calls\n        # adjust the high bias value\n        high *= 2\n    # return the log probability of the target index being the top result and the number of calls made to the model\n    return target_log_prob, num_calls\n\n"}
{"namespace": "resample.resample_3d", "completion": "  if coordinate_order == 'xyz':\n    locations = tf.transpose(locations, [0, 3, 1, 2])\n  elif coordinate_order != 'zyx':\n    raise ValueError('Unsupported coordinate_order: {}'.format(coordinate_order))\n\n  if half_pixel_center:\n    locations = locations - 0.5\n\n  if method == 'TRILINEAR':\n    return tf.image.resize3d(\n        data,\n        locations.shape[1:],\n        method='trilinear',\n        align_corners=True,\n        name='resample_3d_trilinear',\n    )\n  elif method == 'NEAREST':\n    return tf.gather_nd(data, tf.cast(locations, tf.int32), name='resample_3d_nearest')\n  else:\n    raise ValueError('Unsupported method: {}'.format(method))"}
{"namespace": "math.plus_eps", "completion": "  eps = 1e-10\n  if x < eps:\n    return eps\n  else:\n    return x\n"}
{"namespace": "math.minus_eps", "completion": "  tiny_val = 1e-10\n  if x < tiny_val:\n    return -tiny_val\n  else:\n    return x - tiny_val\n"}
{"namespace": "math.safe_exp", "completion": "  def safe_exp_helper(x):\n    \"\"\"\n    The helper function computes the safe exponential function for a given input x. It uses a custom gradient function to handle backpropagation in automatic differentiation contexts.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe exponential function is computed. It is used as the argument for the exponential function and its custom gradient.\n    :return: The result of applying the safe exponential function to the input x. This ensures that the output is within a specified range to prevent overflow errors.\n    \"\"\"\n    if x > 100:\n      return np.exp(100)\n    elif x < -100:\n      return np.exp(-100)\n    else:\n      return np.exp(x)\n\n  def safe_exp_grad(x):\n    \"\"\"\n    The custom gradient function for the safe exponential function. It computes the gradient of the safe exponential function with respect to the input x.\n\n    Input-Output Arguments\n    :param x: The input value for which the gradient of the safe exponential function is computed. It is used as the argument for the gradient computation.\n    :return: The gradient of the safe exponential function with respect to the input x. This is used for backpropagation in automatic differentiation contexts.\n    \"\"\"\n    if x > 100:\n      return np.exp(100)\n    elif x < -100:\n      return np.exp(-100)\n    else:\n      return np.exp(x)\n\n  return safe_exp_helper(x), safe_exp_grad(x)\n"}
{"namespace": "math.safe_log", "completion": "  # Define the safe logarithm function using `generate_safe_fn`\n  safe_log_fn = generate_safe_fn(jnp.log, 0, jnp.inf, 1e-10, 1e-10)\n\n  # Apply the safe logarithm function to `x`\n  return safe_log_fn(x)\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  def safe_sqrt_helper(x):\n    \"\"\"\n    This helper function computes the safe square root of a given input x. It clamps the input between 0 and a maximum value to avoid invalid inputs like negative numbers. The custom gradient function is defined to handle backpropagation in automatic differentiation contexts.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe square root will be computed. It is used as the argument for the square root function and its custom gradient computation.\n    :return: The result of applying the safe square root function to the input x. This includes the application of a custom gradient function for automatic differentiation purposes.\n    \"\"\"\n    max_value = 10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 0\n  elif p == 1:\n    return 1\n  elif p == 2:\n    return 2\n  elif p == 3:\n    return 4\n  elif p == 4:\n    return 8\n  elif p == 5:\n    return 16\n  elif p == 6:\n    return 32\n  elif p == 7:\n    return 64\n  elif p == 8:\n    return 128\n  elif p == 9:\n    return 256\n  elif p == 10:\n    return 512\n  elif p == 11:\n    return 1024\n  elif p == 12:\n    return 2048\n  elif p == 13:\n    return 4096\n  elif p == 14:\n    return 8192\n  elif p == 15:\n    return 16384\n  elif p == 16:\n    return 32768\n  elif p == 17:\n    return 65536\n  elif p == 18:\n    return 131072\n  elif p == 19:\n    return 262144\n  elif p == 20:\n    return 524288\n  elif p == 21:\n    return 1048576\n  elif p == 22:\n    return 2097152\n  elif p == 23:\n    return 4194304\n  elif p == 24:\n    return 8388608\n  elif p == 25:\n    return 16777216\n  elif p == 26:\n    return 33554432\n  elif p == 27:\n    return 67108864\n  elif p == 28:\n    return 134217728\n  elif p == 29:\n    return 268435456\n  elif p == 30:\n   "}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == \"tetrahedron\":\n    basis = np.array(\n      [\n        [1, 1, 1],\n        [-1, 1, 1],\n        [1, -1, 1],\n        [1, 1, -1],\n      ]\n    )\n  elif base_shape == \"icosahedron\":\n    basis = np.array(\n      [\n        [0, 0, 1],\n        [0, 0.942809, 0.333333],\n        [0, -0.471405, 0.866025],\n        [0, -0.942809, -0.333333],\n        [0.816497, 0, 0.57735],\n        [0.408248, 0.707107, 0.57735],\n        [-0.408248, 0.707107, 0.57735],\n        [-0.816497, 0, 0.57735],\n        [-0.408248, -0.707107, 0.57735],\n        [0.408248, -0.707107, 0.57735],\n        [0.707107, 0.5, 0.5],\n        [-0.707107, 0.5, 0.5],\n        [0.707107, -0.5, 0.5],\n        [-0.707107, -0.5, 0.5],\n        [0.333333, 0.55470, 0.773501],\n        [-0.333333, 0.55470, 0"}
{"namespace": "math.safe_log1p", "completion": "  import numpy as np\n  from jax import custom_jvp\n  from jax import numpy as jnp\n  from jax import jit\n\n  @custom_jvp\n  def safe_log1p(x):\n    return jnp.log1p(x)\n\n  @safe_log1p.defjvp\n  def safe_log1p_jvp(primals, tangents):\n    x, = primals\n    x_dot, = tangents\n    return (safe_log1p(x), x_dot / (1 + x))\n\n  return jit(safe_log1p)\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    y = x\n  elif p == 0:\n    y = np.log(x)\n  elif p == -np.inf:\n    y = np.log(x)\n    y = np.exp(y)\n  elif p == np.inf:\n    y = np.log(x)\n    y = np.exp(y)\n  else:\n    y = np.power(x, p)\n\n  if postmult is not None:\n    y = y * postmult\n\n  return y"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  if postmult is not None:\n    y = y * postmult\n  if p == 0:\n    return y\n  elif p == 1:\n    return 1 / y\n  elif p == 2:\n    return y ** -2\n  elif p == 3:\n    return 1 / (y ** 3)\n  elif p == 4:\n    return y ** -4\n  elif p == 5:\n    return 1 / (y ** 5)\n  elif p == 6:\n    return y ** -6\n  elif p == 7:\n    return 1 / (y ** 7)\n  elif p == 8:\n    return y ** -8\n  elif p == 9:\n    return 1 / (y ** 9)\n  elif p == 10:\n    return y ** -10\n  elif p == 11:\n    return 1 / (y ** 11)\n  elif p == 12:\n    return y ** -12\n  elif p == 13:\n    return 1 / (y ** 13)\n  elif p == 14:\n    return y ** -14\n  elif p == 15:\n    return 1 / (y ** 15)\n  elif p == 16:\n    return y ** -16\n  elif p == 17:\n    return 1 / (y ** 17)\n  elif p == 18:\n    return y ** -18\n  elif p == 19:\n    return 1 / (y ** 19)\n  elif p == 20:\n    return y ** -20\n  elif p == 21:\n    return 1 / (y ** 21)\n  elif p == 22:\n    return y ** -22\n  elif p == 23:\n    return 1 / (y ** 23)\n  elif p == 24:\n    return y ** -24\n  elif p == 25:\n    return 1 / (y ** 25)\n  elif p == 26:\n    return y **"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init = lr_init * lr_delay_mult\n    lr_delay_mult = 1.0\n\n  if step < lr_delay_steps:\n    lr = lr_init * (step / lr_delay_steps)\n  else:\n    lr = lr_final + (lr_init - lr_final) * (\n      1 - math.exp(-1.0 * (step - lr_delay_steps) / max_steps)\n    )\n\n  return lr"}
{"namespace": "utils.dummy_rays", "completion": "  from .generate_random_rays import generate_random_rays\n  return generate_random_rays(\n    include_exposure_idx = include_exposure_idx,\n    include_exposure_values = include_exposure_values,\n    include_device_idx = include_device_idx,\n  )"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Check if the input arrays have the correct shape\n  assert points.shape[-1] == 3, \"points must have shape (..., 3)\"\n  assert pixtocams.shape[-2:] == (3, 3), \"pixtocams must have shape (..., 3, 3)\"\n  assert camtoworlds.shape[-2:] == (3, 4), \"camtoworlds must have shape (..., 3, 4)\"\n\n  # Check if the distortion parameters are provided and have the correct shape\n  if distortion_params is not None:\n    assert (\n        distortion_params[\"radial\"] is not None\n        and distortion_params[\"radial\"].shape[-1] == 2\n    ), \"radial distortion parameters must have shape (..., 2)\"\n    assert (\n        distortion_params[\"tangential\"] is not None\n        and distortion_params[\"tangential\"].shape[-1] == 2\n    ), \"tangential distortion parameters must have shape (..., 2)\"\n\n  # Calculate the camera coordinates of the points\n  camtoworlds = xnp.expand_dims(camtoworlds, axis=-2)\n  points = xnp.expand_dims(points, axis=-1)\n  camtoworlds = xnp.matmul(camtoworlds, points)\n\n  # Apply distortion to the camera coordinates\n  if distortion_params is not None:\n    radial = distortion_params[\"radial\"]\n    tangential = distortion_params[\"tangential\"]\n    camtoworlds = apply_distortion(camtoworlds, radial, tangential)\n\n  # Project the camera coordinates onto the image plane\n  pixtocams = xnp.expand_dims(pixtocams, axis=-3)\n  camtoworlds = xnp.matmul(pixtocams, camtoworlds)\n\n  # Normalize the camera coordinates\n  camtoworlds = camtoworlds / xnp.expand_dims(camtoworlds[..., -1], axis=-1)\n\n  # Return"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  if theta < eps:\n    return jnp.eye(4)\n  w = screw_axis[:3] / theta\n  v = screw_axis[3:]\n  w_hat = jnp.array([[0, -w[2], w[1]], [w[2], 0, -w[0]], [-w[1], w[0], 0]])\n  return jnp.eye(4) + jnp.sin(theta) * w_hat + (1 - jnp.cos(theta)) * jnp.matmul(\n      w_hat, w_hat\n  ) + (theta - jnp.sin(theta)) * jnp.outer(w, v) / theta\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Extract the axis and angle of rotation from the input axis-angle vector\n  axis = axis_angle[:3]\n  angle = jnp.linalg.norm(axis_angle[:3])\n\n  # Compute the skew-symmetric matrix from the axis of rotation\n  w = jnp.array([[0, -axis[2], axis[1]],\n                 [axis[2], 0, -axis[0]],\n                 [-axis[1], axis[0], 0]])\n\n  # Compute the rotation matrix using Rodrigues' formula\n  R = jnp.eye(3) + jnp.sin(angle) * w + (1 - jnp.cos(angle)) * jnp.matmul(w, w)\n\n  # Add a small epsilon value to the diagonal of the rotation matrix to avoid division by zero or very small values\n  R = R + jnp.eye(3) * eps\n\n  return R\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Normalize the direction vector\n  d = d / jnp.linalg.norm(d)\n\n  # Calculate the angle between the direction vector and the x-axis\n  theta = jnp.arccos(d[0])\n\n  # Calculate the rotation matrix to align the cone with the x-axis\n  R = jnp.array([[1, 0, 0], [0, jnp.cos(theta), -jnp.sin(theta)], [0, jnp.sin(theta), jnp.cos(theta)]])\n\n  # Rotate the direction vector to align with the x-axis\n  d_rot = jnp.dot(R, d)\n\n  # Calculate the radius of the frustum at the starting and ending distances\n  r0 = base_radius * t0\n  r1 = base_radius * t1\n\n  # Calculate the height of the frustum\n  h = t1 - t0\n\n  # Calculate the mean of the Gaussian distribution\n  mean = jnp.array([0, 0, t0])\n\n  # Calculate the covariance of the Gaussian distribution\n  if diag:\n    cov = jnp.diag([r0**2, r1**2, h**2])\n  else:\n    cov = jnp.array([[r0**2, 0, 0], [0, r1**2, 0], [0, 0, h**2]])\n\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  mean = jnp.array([t0, t1, 0.0])\n  cov = jnp.array([[radius, 0.0, 0.0], [0.0, radius, 0.0], [0.0, 0.0, 0.0]])\n  return lift_gaussian(mean, cov, d, diag)\n"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates\n  pix_x_cam = (pix_x_int + 0.5) * pixtocams[..., 0, 0] + pixtocams[..., 0, 2]\n  pix_y_cam = (pix_y_int + 0.5) * pixtocams[..., 1, 1] + pixtocams[..., 1, 2]\n  pix_z_cam = xnp.ones_like(pix_x_cam)\n\n  # Convert camera coordinates to world coordinates\n  pix_x_world = (\n    pix_x_cam[..., None] * camtoworlds[..., 0, :3]\n    + pix_y_cam[..., None] * camtoworlds[..., 1, :3]\n    + pix_z_cam[..., None] * camtoworlds[..., 2, :3]\n    + camtoworlds[..., 3, :3]\n  )\n  pix_y_world = (\n    pix_x_cam[..., None] * camtoworlds[..., 0, 3:]\n    + pix_y_cam[..., None] * camtoworlds[..., 1, 3:]\n    + pix_z_cam[..., None] * camtoworlds[..., 2, 3:]\n    + camtoworlds[..., 3, 3:]\n  )\n\n  # Compute ray origins and directions\n  origins = pix_x_world\n  directions = pix_y_world - origins\n\n  # Normalize ray directions\n  viewdirs = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Compute differential radii\n  radii = xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Compute image plane coordinates\n  imageplane = xnp.stack([pix_x_int, pix_y_int], axis=-"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Calculate the norm-adjusted distances between points\n  norm_tdist = tdist / np.linalg.norm(dirs, axis=1)\n\n  # Compute the alpha weights\n  alpha_weights = compute_alpha_weights_helper(density, norm_tdist, **kwargs)\n\n  return alpha_weights"}
{"namespace": "stepfun.sample", "completion": "  # Check input shapes\n  assert t.ndim == 1, \"t must be a 1D array\"\n  assert w_logits.ndim == 1, \"w_logits must be a 1D array\"\n  assert t.shape == w_logits.shape, \"t and w_logits must have the same shape\"\n\n  # Check input values\n  assert jnp.all(t[1:] >= t[:-1]), \"t must be sorted\"\n  assert jnp.all(w_logits >= 0), \"w_logits must be non-negative\"\n  assert num_samples > 0, \"num_samples must be positive\"\n\n  # Compute the cumulative sum of weights\n  w_cumsum = jnp.cumsum(jax.nn.softmax(w_logits))\n\n  # Compute the bin widths\n  bin_widths = t[1:] - t[:-1]\n\n  # Compute the bin probabilities\n  bin_probs = w_cumsum[1:] - w_cumsum[:-1]\n\n  # Compute the bin probabilities with a small epsilon added for numerical stability\n  bin_probs = bin_probs + eps\n\n  # Compute the bin probabilities as probabilities\n  bin_probs = bin_probs / jnp.sum(bin_probs)\n\n  # Compute the bin probabilities as cumulative probabilities\n  bin_probs_cumsum = jnp.cumsum(bin_probs)\n\n  # Compute the bin probabilities as cumulative probabilities with a small epsilon added for numerical stability\n  bin_probs_cumsum = bin_probs_cumsum + eps\n\n  # Compute the bin probabilities as cumulative probabilities with a small epsilon subtracted for numerical stability\n  bin_probs_cumsum = bin_probs_cumsum - eps\n\n  # Compute the bin probabilities as cumulative probabilities with a small epsilon subtracted for numerical stability\n  bin_probs_cumsum = bin_probs_cumsum - eps\n\n  # Compute the"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function\n  if rng is None:\n    # If rng is None, use linspace sampling\n    t_samples = jnp.linspace(domain[0], domain[1], num_samples + 1)\n  else:\n    # If rng is not None, use inverse CDF sampling\n    t_samples = jax.random.cumulative_distribution(rng, w_logits, num_samples + 1)\n\n  # Calculate midpoints between adjacent samples\n  t_samples = (t_samples[1:] + t_samples[:-1]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain\n  t_samples = jnp.concatenate([[domain[0]], t_samples, [domain[1]]])\n\n  # If single_jitter is True, jitter every sample by the same amount\n  if single_jitter:\n    t_samples = t_samples + jax.random.uniform(rng, (num_samples + 2,))\n  # If single_jitter is False, jitter each sample independently in the inverse CDF\n  else:\n    t_samples = t_samples + jax.random.cumulative_distribution(\n      rng, w_logits, num_samples + 2\n    )\n\n  return t_samples"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Check if the weights sum to 1\n  if not np.isclose(np.sum(w), 1):\n    raise ValueError(\"The weights must sum to 1.\")\n\n  # Integrate the weights\n  w_int = np.cumsum(w)\n\n  # Interpolate the percentiles\n  ps_int = np.interp(ps, w_int, t)\n\n  return ps_int"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  import numpy as np\n  from scipy.interpolate import interp1d\n  from scipy.signal import gaussian\n\n  # Convert the histogram to a probability density function (PDF)\n  pdf = w / np.sum(w)\n\n  # Blur the PDF using a Gaussian kernel\n  kernel = gaussian(2 * blur_halfwidth + 1, blur_halfwidth)\n  pdf_blurred = np.convolve(pdf, kernel, mode='same')\n\n  # Normalize the blurred PDF\n  pdf_blurred /= np.sum(pdf_blurred)\n\n  # Resample the blurred PDF to match the new time points\n  f = interp1d(t, pdf_blurred, kind='linear', bounds_error=False, fill_value=0)\n  w_resampled = f(tq)\n\n  return w_resampled"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Check the dimensionality of the transformation matrix.\n  if transform.shape[0] != transform.shape[1]:\n    raise ValueError(\"The transformation matrix must be square.\")\n\n  # Check the dimensionality of the points.\n  if vectors.shape[-1] != transform.shape[0] - 1:\n    raise ValueError(\"The dimensionality of the points must match the dimensionality of the transformation matrix.\")\n\n  # Check the dimensionality of the points.\n  if vectors.shape[-1] != transform.shape[0] - 1:\n    raise ValueError(\"The dimensionality of the points must match the dimensionality of the transformation matrix.\")\n\n  # Add a dimension to the points.\n  vectors = vectors[..., np.newaxis]\n\n  # Add a dimension to the transformation matrix.\n  transform = transform[np.newaxis, ...]\n\n  # Apply the transformation.\n  transformed_vectors = np.einsum(\"...ij,...j->...i\", transform, vectors)\n\n  # Remove the added dimension from the points.\n  transformed_vectors = transformed_vectors[..., 0]\n\n  # Return the transformed points.\n  return transformed_vectors\n\n"}
{"namespace": "stepfun.resample", "completion": "  # Check if the input tensors have the same shape\n  if tp.shape != vp.shape:\n    raise ValueError(\"Input tensors tp and vp must have the same shape.\")\n\n  # Check if the input tensors are 1D\n  if tp.ndim != 1:\n    raise ValueError(\"Input tensors tp and vp must be 1D.\")\n\n  # Check if the input tensors are sorted\n  if not torch.all(torch.diff(tp) >= 0):\n    raise ValueError(\"Input tensors tp and vp must be sorted.\")\n\n  # Check if the input tensors are of the same type\n  if not torch.all(tp.dtype == vp.dtype):\n    raise ValueError(\"Input tensors tp and vp must have the same dtype.\")\n\n  # Check if the input tensors are of the same device\n  if not torch.all(tp.device == vp.device):\n    raise ValueError(\"Input tensors tp and vp must be on the same device.\")\n\n  # Check if the input tensors are of the same type\n  if not torch.all(t.dtype == tp.dtype):\n    raise ValueError(\"Input tensors t and tp must have the same dtype.\")\n\n  # Check if the input tensors are of the same device\n  if not torch.all(t.device == tp.device):\n    raise ValueError(\"Input tensors t and tp must be on the same device.\")\n\n  # Check if the input tensors are of the same type\n  if not torch.all(t.dtype == vp.dtype):\n    raise ValueError(\"Input tensors t and vp must have the same dtype.\")\n\n  # Check if the input tensors are of the same device\n  if not torch.all(t.device == vp.device):\n    raise ValueError(\"Input tensors t and vp must be on the same device.\")\n\n  # Check if the input tensors are of the same type\n  if not torch.all(t.dtype == vp."}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance\n  mean_scaled = mean / jnp.power(2.0, jnp.arange(min_deg, max_deg))\n  var_scaled = var / jnp.power(2.0, jnp.arange(min_deg, max_deg))\n\n  # Concatenate the scaled mean and variance\n  x = jnp.concatenate([mean_scaled, var_scaled], axis=-1)\n\n  # Apply sinusoidal encoding\n  encoding = jnp.concatenate([jnp.sin(jnp.pi * x), jnp.cos(jnp.pi * x)], axis=-1)\n\n  return encoding"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Importing the necessary libraries\n  import numpy as np\n  from scipy.special import sph_harm\n\n  # Generating the integrated directional encoding function\n  def dir_enc_fn(points):\n\n    # Converting the input points to numpy array\n    points = np.array(points)\n\n    # Checking if the input points are 3D points\n    if points.shape[-1] != 3:\n      raise ValueError(\"Input points must be 3D points.\")\n\n    # Normalizing the input points\n    points = points / np.linalg.norm(points, axis=-1, keepdims=True)\n\n    # Initializing the directional encoding\n    dir_enc = np.zeros((points.shape[0], 2 * deg_view + 1))\n\n    # Computing the directional encoding for each point\n    for i in range(points.shape[0]):\n      for m in range(-deg_view, deg_view + 1):\n        for n in range(abs(m), deg_view + 1):\n          idx = m + deg_view\n          dir_enc[i, idx] += sph_harm(m, n, np.arctan2(points[i, 1], points[i, 0]), np.arcsin(points[i, 2])).real\n\n    # Returning the directional encoding\n    return dir_enc\n\n  # Returning the directional encoding function\n  return dir_enc_fn\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # Initialize variables\n    result = []\n    blocks = []\n    block_index = 0\n    header_index = None\n    list_index = None\n    list_level = None\n    list_items = []\n    list_item_index = 0\n    list_item_level = 0\n    list_item_text = \"\"\n    list_item_start_index = None\n    list_item_end_index = None\n    list_item_start_line = None\n    list_item_end_line = None\n    list_item_start_line_index = None\n    list_item_end_line_index = None\n    list_item_start_line_index_offset = 0\n    list_item_end_line_index_offset = 0\n    list_item_start_line_index_offset_prev = 0\n    list_item_end_line_index_offset_prev = 0\n    list_item_start_line_index_offset_next = 0\n    list_item_end_line_index_offset_next = 0\n    list_item_start_line_index_offset_prev_prev = 0\n    list_item_end_line_index_offset_prev_prev = 0\n    list_item_start_line_index_offset_prev_next = 0\n    list_item_end_line_index_offset_prev_next = 0\n    list_item_start_line_index_offset_next_next = 0\n    list_item_end_line_index_offset_next_next = 0\n    list_item_start_line_index_offset_next_prev = 0\n    list_item_end_line_index_offset_next_prev = 0\n    list_item_start_line_index_offset_next_prev_prev = 0\n    list_item_end_line_index_offset_next_prev_prev = 0\n    list_item_start_line_index_offset_next_prev_next = 0\n    list_item_end_line_index_offset_next_prev_next = 0\n    list_item_start_line_index_offset_next_next"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    # Normalize quotation marks\n    org_texts = org_texts.replace(\"\u201c\", '\"').replace(\"\u201d\", '\"').replace(\"\u2018\", \"'\").replace(\"\u2019\", \"'\")\n\n    # Tokenize the text into sentences\n    sentences = nltk_tokenizer.tokenize(org_texts)\n\n    # Apply rules to handle special cases\n    for i, sentence in enumerate(sentences):\n        # Handle paragraphs separated by new lines\n        if sentence.startswith(\"\\n\"):\n            sentences[i] = sentence[1:]\n\n        # Handle punctuation at the beginning of the text\n        if sentence.startswith(\".\"):\n            sentences[i] = sentence[1:]\n\n        # Handle sentences within brackets\n        sentences[i] = bracket_rule(sentence)\n\n        # Handle special cases\n        sentences[i] = space_rule(sentences[i])\n\n    # Normalize quotation marks\n    sentences = [sentence.replace(\"\u201c\", '\"').replace(\"\u201d\", '\"').replace(\"\u2018\", \"'\").replace(\"\u2019\", \"'\") for sentence in sentences]\n\n    return sentences\n\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            return self.positions_all_docs[token]\n        else:\n            return self.positions_all_docs[token][key]\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == '0':\n        return 0\n    elif spec == '1':\n        return 1\n    elif spec == '2':\n        return 2\n    elif spec == '3':\n        return 3\n    elif spec == '4':\n        return 4\n    elif spec == '5':\n        return 5\n    elif spec == '6':\n        return 6\n    elif spec == '7':\n        return 7\n    elif spec == '8':\n        return 8\n    elif spec == '9':\n        return 9\n    elif spec == '10':\n        return 10\n    elif spec == '11':\n        return 11\n    elif spec == '12':\n        return 12\n    elif spec == '13':\n        return 13\n    elif spec == '14':\n        return 14\n    elif spec == '15':\n        return 15\n    elif spec == '16':\n        return 16\n    elif spec == '17':\n        return 17\n    elif spec == '18':\n        return 18\n    elif spec == '19':\n        return 19\n    elif spec == '20':\n        return 20\n    elif spec == '21':\n        return 21\n    elif spec == '22':\n        return 22\n    elif spec == '23':\n        return 23\n    elif spec == '24':\n        return 24\n    elif spec == '25':\n        return 25\n    elif spec == '26':\n        return 26\n    elif spec == '27':\n        return 27\n    elif spec == '28':\n        return 28\n    elif spec == '29':\n        return 29\n    elif spec == '30':\n        return 30\n    elif spec == '31':\n        return 31\n    elif spec == '32':\n        return 32\n    elif spec == '33':\n        return 33\n    elif spec == '34':"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self._phrase_freq_direct(tokens)\n        else:\n            return self._phrase_freq_slop(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if avoid_copies:\n            array = array.copy()\n\n        if truncate:\n            array = array[:100000]\n\n        term_matrix, positions, term_dict, avg_doc_len, doc_lens = cls._build_index(array, tokenizer, batch_size)\n\n        return cls(term_matrix, positions, term_dict, avg_doc_len, doc_lens)\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.info(\"Initializing ProxifierMessageInterceptor\")\n        self.server = ProxifierMessageInterceptorServer(self.config.get_proxifier_message_interceptor_port(), self.config.get_proxifier_message_interceptor_host(), self.config.get_proxifier_message_interceptor_max_connections(), self.config.get_proxifier_message_interceptor_max_buffer_size(), self.config.get_proxifier_message_interceptor_timeout())\n        self.connections = {}\n        self.lock = threading.Lock()\n        self.server.start()\n        self.logger.info(\"ProxifierMessageInterceptor initialized\")\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Initialize the result array with zeros\n    result = [0] * len(arr)\n\n    # Iterate over the array\n    for i in range(len(arr)):\n        # Initialize the bit count to 0\n        count = 0\n\n        # Iterate over the 64 bits of the current element\n        for j in range(64):\n            # Check if the jth bit is set to 1\n            if arr[i] & (1 << j):\n                # Increment the bit count\n                count += 1\n\n        # Store the bit count in the result array\n        result[i] = count\n\n    # Return the result array\n    return result"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Create a new Elasticsearch client\n    es = Elasticsearch()\n\n    # Define the query\n    query = {\n        \"query\": {\n            \"bool\": {\n                \"should\": [\n                    {\n                        \"multi_match\": {\n                            \"query\": q,\n                            \"fields\": qf,\n                            \"type\": \"best_fields\",\n                            \"minimum_should_match\": mm,\n                            \"operator\": q_op,\n                        }\n                    },\n                    {\n                        \"multi_match\": {\n                            \"query\": q,\n                            \"fields\": pf,\n                            \"type\": \"phrase\",\n                            \"minimum_should_match\": mm,\n                            \"operator\": q_op,\n                        }\n                    },\n                    {\n                        \"multi_match\": {\n                            \"query\": q,\n                            \"fields\": pf2,\n                            \"type\": \"bigram\",\n                            \"minimum_should_match\": mm,\n                            \"operator\": q_op,\n                        }\n                    },\n                    {\n                        \"multi_match\": {\n                            \"query\": q,\n                            \"fields\": pf3,\n                            \"type\": \"trigram\",\n                            \"minimum_should_match\": mm,\n                            \"operator\": q_op,\n                        }\n                    },\n                ]\n            }\n        }\n    }\n\n    # Perform the search\n    res = es.search(index=\"my_index\", body=query)\n\n    # Extract the search results\n    hits = res[\"hits\"][\"hits\"]\n\n    # Calculate the relevance scores\n    scores = np.array([hit[\"_score\"] for hit in hits])\n\n    # Calculate the relevance scores\n    scores = np.array([hit[\"_score\"] for hit in hits])\n\n    # Calculate the relevance scores\n    scores = np.array([hit[\"_score\"] for hit in hits])\n\n    # Calculate the relevance scores\n    scores = np.array([hit[\"_score\"] for hit in hits])\n\n    # Calculate the relevance scores\n    scores = np.array([hit[\"_score\"] for hit in"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            message.data = process.connection.c2s(message.data)\n        elif isinstance(message.data, RecvMessage):\n            message.data = process.connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            process.connection.close()\n        else:\n            raise ValueError(\"Invalid message type\")"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        for connection in self.connections:\n            connection.stop()\n\n        # Stop the server\n        if self.server:\n            self.server.stop()\n\n        # Clear the connections list\n        self.connections.clear()\n\n        # Clear the server attribute\n        self.server = None\n\n        # Clear the server_thread attribute\n        self.server_thread = None\n\n        # Clear the server_thread_stop_event attribute\n        self.server_thread_stop_event = None\n\n        # Clear the server_thread_stop_event_lock attribute\n        self.server_thread_stop_event_lock = None\n\n        # Clear the server_thread_stop_event_lock attribute\n        self.server_thread_stop_event_lock = None\n\n        # Clear the server_thread_stop_event_lock attribute\n        self.server_thread_stop_event_lock = None\n\n        # Clear the server_thread_stop_event_lock attribute\n        self.server_thread_stop_event_lock = None\n\n        # Clear the server_thread_stop_event_lock attribute\n        self.server_thread_stop_event_lock = None\n\n        # Clear the server_thread_stop_event_lock attribute\n        self.server_thread_stop_event_lock = None\n\n        # Clear the server_thread_stop_event_lock attribute\n        self.server_thread_stop_event_lock = None\n\n        # Clear the server_thread_stop_event_lock attribute\n        self.server_thread_stop_event_lock = None\n\n        # Clear the server_thread_stop_event_lock attribute\n        self.server_thread_stop_event_lock = None\n\n        # Clear the server_thread_stop_event_lock attribute\n        self.server_thread_stop_event_lock = None\n\n        # Clear the server_thread_stop_event_lock attribute\n        self.server_thread_stop_event_lock = None\n\n        # Clear the server_thread_stop_event_lock attribute\n        self.server_thread_stop_event_lock = None\n\n        # Clear"}
