{"namespace": "iris.io.validators.are_shapes_equal", "completion": "\n    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if field1 and field2 have the same shape.\"\"\"\n        shape_field_1 = values[field1].shape\n        shape_field_2 = values[field2].shape\n\n        if shape_field_1 != shape_field_2:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {shape_field_1} and {shape_field_2}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(*args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(result)\n\n        return result"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except (json.JSONDecodeError, TypeError):\n            return False"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n\n    \"\"\"\n    Encodes an integer into a single character based on a predefined character set. The character set consists of lowercase letters, digits, and an underscore.\n\n    Input-Output Arguments\n    :param n: Integer. The integer to be encoded. It is used as an index to select a character from the character set.\n    :return: String. The encoded character corresponding to the input integer.\n    \"\"\"\n\n    return charset[n]"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = inspect.getdoc(func_object)\n\n        input_type_hints = [type_hints[param] for param in signature.parameters]\n        output_type_hint = type_hints['return']\n\n        input_class_definitions = [get_class_definition(param) for param in input_type_hints]\n        output_class_definition = get_class_definition(output_type_hint)\n\n        function_type = FunctionType.SYMBOLIC if get_origin(output_type_hint) == Union else FunctionType.EMBEDDABLE\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hints=input_type_hints,\n            output_type_hint=output_type_hint,\n            input_class_definitions=input_class_definitions,\n            output_class_definition=output_class_definition,\n            function_type=function_type\n        )"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        if len(loaded_bit_array) != self.size:\n            logging.warning(\"Mismatch in loaded bit array length. Reinitializing bit array and indices.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if not self.bit_array[index]:\n                return False\n        return True"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    weights = np.random.dirichlet(np.ones(n))\n    if zeros > 0:\n        zero_indices = np.random.choice(range(n), zeros, replace=False)\n        weights[zero_indices] = 0\n        weights /= np.sum(weights)  # Normalize the weights to ensure they sum up to one\n    return weights"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.create_model_config(json_dict.get('distilled_model', DEFAULT_STUDENT_MODELS[DEFAULT_DISTILLED_MODEL_NAME]))\n        self.current_model_stats = json_dict.get('current_model_stats', {\n            \"trained_on_datapoints\": 0,\n            \"running_faults\": []\n        })\n        self.last_training_run = json_dict.get('last_training_run', {\"trained_on_datapoints\": 0})\n        self.current_training_run = json_dict.get('current_training_run', {})\n        self.nr_of_training_runs = json_dict.get('nr_of_training_runs', 0)\n        self.teacher_models = [config_factory.create_model_config(teacher_model) for teacher_model in json_dict.get('teacher_models', [DEFAULT_TEACHER_MODELS[teacher_model_name] for teacher_model_name in DEFAULT_TEACHER_MODEL_NAMES])]\n        return self"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        try:\n            response = requests.post(\n                OPENAI_URL,\n                headers={\"Authorization\": f\"Bearer {self.api_key}\"},\n                json={\n                    \"model\": model.model_name,\n                    \"messages\": [\n                        {\"role\": \"system\", \"content\": system_message},\n                        {\"role\": \"user\", \"content\": prompt}\n                    ],\n                    **kwargs\n                }\n            )\n            response.raise_for_status()\n            data = response.json()\n            generated_text = data[\"choices\"][0][\"message\"][\"content\"]\n            return generated_text\n        except Exception as e:\n            print(f\"An error occurred: {e}\")\n            return None"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2 or x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix must be square\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric\")"}
{"namespace": "ref_utils.l2_normalize", "completion": "  norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  return x / jnp.maximum(norm, grad_eps)"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.islower():\n        return word\n    elif word.isupper():\n        return word\n    else:\n        if word[0].isupper() and word[1].islower():\n            return word.capitalize()\n        else:\n            return word.lower()"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\" \", \"\")  # Remove all whitespace characters\n    return [char for char in line_text]  # Segment the modified text into smaller parts or tokens"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  return generate_ide_fn(deg_view)"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "\n    page_blocks = []\n    line_set = set()\n    for page_idx, page_lines in enumerate(lines):\n        page_blocks, line_set = visual_clean_lines(\n            page_lines,\n            page_stats={},\n            page_info_dict={},\n            page_idx=page_idx,\n            line_set=line_set,\n        )\n    return page_blocks"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    # If the input text is empty or None, return the input as is\n    if not org_texts:\n        return org_texts\n\n    # Apply the space rule to remove any space between punctuations\n    org_texts = re.sub(space_rule, r\"\\1\", org_texts)\n\n    # Apply the bracket rule to ensure that sentences within brackets are not broken\n    org_texts = re.sub(bracket_rule, r\"\\1\", org_texts)\n\n    # Normalize quotation marks within the text\n    org_texts = re.sub(quotation_pattern, '\"', org_texts)\n\n    # Tokenize the text into sentences using the NLTK tokenizer\n    tokenized_sentences = nltk_tokenzier.tokenize(org_texts)\n\n    return tokenized_sentences"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            return self.posns[token][key]\n        else:\n            return [self.posns[token][k] for k in range(len(self))]"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.isdigit():\n        return int(spec)\n    elif '%' in spec:\n        percentage = int(spec.strip('%'))\n        return int(num_clauses * (percentage / 100))\n    elif '<' in spec:\n        threshold = int(spec.strip('<'))\n        return max(num_clauses - threshold, 1)\n    else:\n        raise ValueError(\"Invalid 'min should match' specification\")"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "\n        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_every_diff(tokens, slop=slop)\n        else:\n            return self.phrase_freq_scan(tokens, slop=slop)"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if truncate:\n            array = array[:batch_size]\n\n        postings_arr = cls(array, tokenizer=tokenizer, avoid_copies=avoid_copies)\n        return postings_arr"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(host=self.config['serverHost'], port=self.config['serverPort'])\n        self.connections = {}\n        self.lock = threading.Lock()\n        self.server.start()"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = arr - ((arr >> _1) & s55)\n    arr = (arr & m1) + ((arr >> _2) & m1)\n    arr = (arr + (arr >> _4)) & m2\n    arr = (arr * s01) >> all_but_one_bit\n    return arr"}
{"namespace": "searcharray.solr.edismax", "completion": "    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    if term_centric:\n        return _edismax_term_centric(frame, parse_field_boosts(qf), num_search_terms, search_terms, mm, similarity)\n    else:\n        return _edismax_field_centric(frame, parse_field_boosts(qf), num_search_terms, search_terms, mm, similarity)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            for connection_id, connection in list(self.connections.items()):\n                connection.stop()\n                del self.connections[connection_id]\n            if self.server:\n                self.server.stop()"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(\"`cov` must be a 2D array\")\n\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    return corr, std"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal elements of the matrix must be close to zero\")\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Check if the function is already initialized\n        if func_hash in self.initialized_functions:\n            current_function_setup = self.initialized_functions[func_hash]\n            model = self.api_provider[current_function_setup[\"model\"]]\n            is_distilled_model = False\n            save_to_finetune = False\n        else:\n            # Get the suitable model for distillation\n            model, is_distilled_model = self.function_modeler.get_distilled_model(function_description)\n            save_to_finetune, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs, function_description, model)\n\n            # Update examples for fine-tuning if necessary\n            if save_to_finetune:\n                examples = self.function_modeler.get_symbolic_alignments(func_hash)\n                prompt = self.construct_prompt(function_description.name, args, kwargs, examples, model)\n                self.function_modeler.update_finetune_examples(func_hash, function_description, prompt, input_prompt_token_count)\n\n        return prompt, model, save_to_finetune, is_distilled_model"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        return _nearest_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        return _nearest_clipped(cov)"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "\n    for arr in x:\n        if len(arr) > 1:\n            mid = len(arr) // 2\n            yield [arr[:mid], arr[mid:]]"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    if percent:\n        x *= 100\n    if abs(x) < 1:\n        return f\"{x:.2e}\"\n    elif abs(x) < 10:\n        return f\"{x:.4f}\"\n    else:\n        return f\"{x:.2f}\""}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\"assets_names must be provided when items is a dictionary\")\n        if dim == 1:\n            array = np.array([items.get(asset, fill_value) for asset in assets_names])\n        elif dim == 2:\n            array = np.array([[items.get(asset, fill_value) for asset in assets_names]])\n        else:\n            raise ValueError(\"Invalid value for dim. It must be either 1 or 2.\")\n    else:\n        array = np.asarray(items)\n\n    if dim == 1 and array.shape != (n_assets,):\n        raise ValueError(f\"The shape of {name} must be ({n_assets},) for dim=1\")\n    elif dim == 2 and array.shape[1] != n_assets:\n        raise ValueError(f\"The number of assets in {name} must be {n_assets} for dim=2\")\n\n    return array"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = Path.home() / \"skfolio_data\"\n    else:\n        data_home = Path(data_home)\n\n    if not data_home.exists():\n        data_home.mkdir(parents=True, exist_ok=True)\n\n    return str(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    if os.path.exists(data_home):\n        shutil.rmtree(data_home)"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes, int, float, bool, type(None), torch.Tensor)):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, (list, tuple)):\n        res = [flatten_to_tuple(k) for k in obj]\n        values, sizes = ListSchema._concat([k[0] for k in res])\n        return values, ListSchema([k[1] for k in res], sizes)\n    elif isinstance(obj, collections.abc.Mapping):\n        keys = sorted(obj.keys())\n        values = [obj[k] for k in keys]\n        ret, schema = DictSchema.flatten(values)\n        return ret, DictSchema(schema.schemas, schema.sizes, keys)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, (Boxes, ROIMasks)):\n        return TensorWrapSchema.flatten(obj)\n    else:\n        raise NotImplementedError(f\"Cannot flatten object of type {type(obj)}\")"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    n_groups, n_assets = groups.shape\n    left = np.zeros((len(equations), n_assets))\n    right = np.zeros(len(equations))\n\n    for i, equation in enumerate(equations):\n        try:\n            left[i], right[i] = _string_to_equation(groups, equation, sum_to_one)\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            else:\n                warnings.warn(str(e), UserWarning)\n\n    return left, right"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    cls_name, cls_def = _gen_instance_module(fields)\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n        f.write(cls_def)\n        module_name = f.name\n    try:\n        module = _import(module_name)\n        newInstances = getattr(module, cls_name)\n        _add_instances_conversion_methods(newInstances)\n        with torch.jit._disable_tracing():\n            with torch.jit._disable_emit_hooks():\n                with torch.jit._disable_emit_module_hooks():\n                    with torch.jit._disable_emit_function_hooks():\n                        with torch.jit._disable_emit_class_hooks():\n                            with patch_builtin_len([module_name]):\n                                with freeze_training_mode(newInstances):\n                                    yield newInstances\n    finally:\n        os.remove(module_name)"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "\n    with torch.no_grad():\n        for param in model.parameters():\n            param.requires_grad = False"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.sqrt(safe_x)"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  homogeneous_vectors = to_homogeneous(vectors)\n  transformed_homogeneous_vectors = jnp.matmul(homogeneous_vectors, transform.T)\n  return from_homogeneous(transformed_homogeneous_vectors)"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "stepfun.resample", "completion": "  # Compute the width of the original intervals.\n  dt = jnp.diff(tp)\n\n  # Compute the width of the new intervals.\n  dtq = jnp.diff(t)\n\n  # Compute the weights for each original interval based on the resampling method.\n  if use_avg:\n    weights = dt / dtq\n  else:\n    weights = dt\n\n  # Initialize the resampled values.\n  vp_resampled = jnp.zeros(t.shape[:-1])\n\n  # Iterate over the original intervals and add their contribution to the resampled values.\n  for i in range(len(tp) - 1):\n    # Find the indices of the new intervals that overlap with the current original interval.\n    overlap_indices = jnp.where((t[:-1] <= tp[i + 1]) & (t[1:] >= tp[i]))[0]\n\n    # Compute the contribution of the original interval to the overlapping new intervals.\n    contribution = jnp.minimum(t[i + 1], tp[i + 1]) - jnp.maximum(t[i], tp[i])\n\n    # Add the contribution to the resampled values based on the resampling method.\n    if use_avg:\n      vp_resampled = jax.ops.index_add(vp_resampled, overlap_indices, vp[i] * contribution / dt[i])\n    else:\n      vp_resampled = jax.ops.index_add(vp_resampled, overlap_indices, vp[i] * weights[i])\n\n  return vp_resampled"}
{"namespace": "coord.contract", "completion": "  z_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = 2 * jnp.sqrt(z_mag_sq) - z_mag_sq\n  return x / scale"}
{"namespace": "coord.inv_contract", "completion": "  x_mag_sq = jnp.sum(z**2, axis=-1, keepdims=True)\n  scale = (jnp.sqrt(x_mag_sq) + 1) / (2 * z)\n  x = scale * z\n  return x"}
{"namespace": "grid_utils.trilerp", "completion": "\n  if datastructure == 'grid':\n      # Perform trilinear interpolation on a 3D voxel grid\n      return resample.trilinear_interpolation(values, coordinates)\n  elif datastructure == 'hash':\n      # Perform trilinear interpolation on a hashed data structure\n      return hash_resample.trilinear_interpolation(values, coordinates)\n  else:\n      raise ValueError(\"Invalid datastructure. Supported data structures are 'grid' and 'hash'.\")"}
{"namespace": "coord.track_linearize", "completion": "  fn_mean, lin_fn = jax.linearize(fn, mean)\n\n  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n  )\n\n  # The cube root of the determinant of the Jacobian is the geometric mean\n  # of the eigenvalues of the Jacobian, which gives us the isotropic scaling\n  # implied by `fn` at each mean that `scale` should be multiplied by.\n  eps = jnp.finfo(jnp.float32).tiny  # Guard against an inf gradient at 0.\n  abs_det = jnp.maximum(eps, jnp.abs(jnp.linalg.det(jac)))\n  # Special case d == 3 for speed's sake.\n  fn_scale = scale * (jnp.cbrt(abs_det) if d == 3 else abs_det ** (1 / d))\n\n  # Transform the covariances using the Jacobian of the function.\n  fn_cov = jnp.einsum('...ij,...jk->...ik', jac, jnp.einsum('...ik,...kj->...ij', cov, jac))\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  encoded = expected_sin(\n      jnp.concatenate([scaled_x, scaled_x + 0.5 * jnp.pi], axis=-1),\n      jnp.concatenate([jnp.zeros_like(scaled_x), jnp.zeros_like(scaled_x)], axis=-1),\n  )\n  if append_identity:\n      encoded = jnp.concatenate([x, encoded], axis=-1)\n  return encoded"}
{"namespace": "coord.integrated_pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  scaled_mean = mean * scales\n  scaled_var = var * scales ** 2\n  scaled_input = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n  encoded_output = jnp.sin(scaled_input)\n  return encoded_output"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # Compute the determinant directly for isotropic scaling.\n    det = jnp.linalg.det(cov)\n    isotropic_cov = det ** (1 / cov.shape[-1]) * jnp.eye(cov.shape[-1])\n  elif mode == 'accurate':\n    # Use the logarithm of the determinant for stability.\n    sign, logdet = jnp.linalg.slogdet(cov)\n    det = jnp.exp(logdet)\n    isotropic_cov = det ** (1 / cov.shape[-1]) * jnp.eye(cov.shape[-1])\n  else:\n    raise ValueError(\"Invalid mode. Mode must be 'fast' or 'accurate'.\")\n\n  return isotropic_cov"}
{"namespace": "coord.construct_ray_warps", "completion": "  def t_to_s(t):\n    \"\"\"Maps metric distances to normalized distances in the range [0, 1].\"\"\"\n    return (fn(t) - fn(t_near)) / (fn(t_far) - fn(t_near))\n\n  def s_to_t(s):\n    \"\"\"Maps normalized distances back to metric distances.\"\"\"\n    return fn_inv(s * (fn(t_far) - fn(t_near)) + fn(t_near)) if fn_inv else None\n\n  return t_to_s, s_to_t"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        if self.tcnn:\n            if n_neurons <= 64:\n                return self._get_tinycudann_network(n_input_dims, n_output_dims, n_layers, n_neurons, activation, output_activation)\n            else:\n                return self._get_torch_network(n_input_dims, n_output_dims, n_layers, n_neurons, activation, output_activation)\n        else:\n            return self._get_torch_network(n_input_dims, n_output_dims, n_layers, n_neurons, activation, output_activation)"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    areas = [area(polygon) for polygon in polygons]\n    max_area = max(areas)\n    filtered_polygons = [polygon for polygon, polygon_area in zip(polygons, areas) if polygon_area >= max_area * rel_tr and polygon_area >= abs_tr]\n    return filtered_polygons"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        medians = np.zeros(len(signal) - 2 * kernel_offset)\n        for i in range(kernel_offset, len(signal) - kernel_offset):\n            medians[i - kernel_offset] = np.median(signal[i - kernel_offset : i + kernel_offset + 1])\n        return medians"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    probe_irisbits = template_probe.iriscode\n    gallery_irisbits = template_gallery.iriscode\n    half_width = [template_probe.width // 2, template_gallery.width // 2]\n\n    if nm_dist is not None and weights is not None:\n        sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n            probe_irisbits.size, half_width, weights\n        )\n        irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n            probe_irisbits, gallery_irisbits, half_width, weights\n        )\n        norm_HD_top = normalized_HD(irisbitcount_top, maskbitcount_top, sqrt_totalbitcount_top, nm_dist)\n        norm_HD_bot = normalized_HD(irisbitcount_bot, maskbitcount_bot, sqrt_totalbitcount_bot, nm_dist)\n        norm_HD = (norm_HD_top + norm_HD_bot) / 2\n        return norm_HD, 0\n\n    elif nm_dist is not None:\n        sqrt_totalbitcount = np.sqrt(probe_irisbits.size * 3 / 4)\n        irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n            probe_irisbits, gallery_irisbits, half_width\n        )\n        norm_HD_top = normalized_HD(irisbitcount_top, maskbitcount_top, sqrt_totalbitcount, nm_dist)\n        norm_HD_bot = normalized_HD(irisbitcount_bot, maskbitcount_bot, sqrt_totalbitcount, nm_dist)\n        norm_HD = (norm_HD_top + norm_HD_bot) / 2\n        return norm_HD, 0\n\n    elif weights is not None:\n        sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n            probe_irisbits.size, half_width, weights\n        )\n        irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n            probe_irisbits, gallery_irisbits, half_width, weights\n        )\n        HD_top = irisbitcount_top + maskbitcount_top\n        HD_bot = irisbitcount_bot + maskbitcount_bot\n        HD = (HD_top + HD_bot) / 2\n        return HD, 0\n\n    else:\n        irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n            probe_irisbits, gallery_irisbits, half_width\n        )\n        HD_top = irisbitcount_top + maskbitcount_top\n        HD_bot = irisbitcount_bot + maskbitcount_bot\n        HD = (HD_top + HD_bot) / 2\n        return HD, 0"}
{"namespace": "iris.utils.math.area", "completion": "    if array.ndim != 2 or array.shape[1] != 2:\n        raise ValueError(f\"This function expects a polygon, i.e. an array of shape (_, 2). Got {array.shape}\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_points = polygon.shape[0]\n        max_iterations = self.params.max_iterations\n        bisector_start_points = []\n        bisector_end_points = []\n\n        for _ in range(self.params.num_bisectors):\n            for _ in range(max_iterations):\n                idx1, idx2 = np.random.choice(num_points, size=2, replace=False)\n                point1, point2 = polygon[idx1], polygon[idx2]\n                distance = np.linalg.norm(point2 - point1)\n                if distance > min_distance_between_sector_points_in_px:\n                    midpoint = (point1 + point2) / 2\n                    direction = np.array([point2[1] - point1[1], point1[0] - point2[0]])\n                    direction /= np.linalg.norm(direction)\n                    bisector_start_points.append(midpoint)\n                    bisector_end_points.append(midpoint + direction)\n                    break\n            else:\n                raise EyeCentersEstimationError(\"Failed to find sufficient point pairs within the maximum iterations.\")\n\n        return np.array(bisector_start_points), np.array(bisector_end_points)"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    total_length = 0.0\n    for i in range(len(polygon) - 1):\n        distance = np.linalg.norm(polygon[i + 1] - polygon[i])\n        if distance <= max_point_distance:\n            total_length += distance\n    return total_length"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if v.dtype != bool:\n        raise ValueError(f\"{cls.__name__}: {field.name} must contain only boolean values.\")\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. Got shape {v.shape}\")\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        for val in v:\n            if val <= 0:\n                raise ValueError(f\"{cls.__name__}: {field.name} must contain only positive values.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be a positive value.\")\n\n    return v"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values[\"x_min\"], values[\"x_max\"], values[\"y_min\"], values[\"y_max\"]\n    if x_min >= x_max:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max.\")\n    if y_min >= y_max:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max.\")\n    return values"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "\n    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check if the array has the specified number of dimensions.\"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions.\")\n        return v\n\n    return __validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "\n    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if field1 and field2 have the same shapes.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(f\"{cls.__name__}: {field1} and {field2} length mismatch.\")\n        for arr1, arr2 in zip(values[field1], values[field2]):\n            if arr1.shape != arr2.shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: safe_div(x_dot, jnp.maximum(tiny_val, x)),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return fake_clip(jnp.sqrt(jnp.maximum(0, x)), 0, max_val)"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return 1\n  elif p == 0:\n    return 0\n  elif p == -np.inf:\n    return -1\n  elif p == np.inf:\n    return 1\n  else:\n    return np.nan"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape not in ['tetrahedron', 'icosahedron', 'octahedron']:\n    raise ValueError(f'base_shape {base_shape} must be \"tetrahedron\", \"icosahedron\", or \"octahedron\"')\n\n  if base_shape == 'tetrahedron':\n    base_verts = np.array([\n        [1, 1, 1],\n        [1, -1, -1],\n        [-1, 1, -1],\n        [-1, -1, 1],\n    ], dtype=np.float32)\n    base_faces = np.array([\n        [0, 1, 2],\n        [0, 1, 3],\n        [0, 2, 3],\n        [1, 2, 3],\n    ], dtype=np.int32)\n  elif base_shape == 'icosahedron':\n    phi = (1 + np.sqrt(5)) / 2\n    base_verts = np.array([\n        [0, 1, phi],\n        [0, -1, phi],\n        [0, 1, -phi],\n        [0, -1, -phi],\n        [1, phi, 0],\n        [-1, phi, 0],\n        [1, -phi, 0],\n        [-1, -phi, 0],\n        [phi, 0, 1],\n        [-phi, 0, 1],\n        [phi, 0, -1],\n        [-phi, 0, -1],\n    ], dtype=np.float32)\n    base_faces = np.array([\n        [0, 1, 4],\n        [0, 1, 5],\n        [0, 4, 8],\n        [0, 5, 8],\n        [0, 4, 9],\n        [0, 5, 10],\n        [1, 4, 9],\n        [1, 5, 10],\n        [1, 6, 8],\n        [1, 7, 8],\n        [1, 6, 9],\n        [1, 7, 10],\n        [2, 3, 6],\n        [2, 3, 7],\n        [2, 6, 8],\n        [2, 7, 8],\n        [2, 6, 9],\n        [2, 7, 10],\n        [3, 6, 9],\n        [3, 7, 10],\n        [3, 4, 8],\n        [3, 5, 8],\n        [3, 4, 9],\n        [3, 5, 10],\n    ], dtype=np.int32)\n  elif base_shape == 'octahedron':\n    base_verts = np.array([\n        [1, 0, 0],\n        [-1, 0, 0],\n        [0, 1, 0],\n        [0, -1, 0],\n        [0, 0, 1],\n        [0, 0, -1],\n    ], dtype=np.float32)\n    base_faces = np.array([\n        [0, 2, 4],\n        [0, 3, 4],\n        [0, 2, 5],\n        [0, 3, 5],\n        [1, 2, 4],\n        [1, 3, 4],\n        [1, 2, 5],\n        [1, 3, 5],\n    ], dtype=np.int32)\n\n  verts = tesselate_geodesic(base_verts, base_faces, angular_tesselation, eps)\n\n  if remove_symmetries:\n    sq_dist = compute_sq_dist(verts.T)\n    unique = np.unique(np.argwhere(sq_dist <= eps)[:, 0])\n    verts = verts[unique, :]\n\n  return verts.T"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + x),\n      (-1, max_val - 1),\n  )(x)"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  xp = jnp.abs(x)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x_max = minus_eps(power_ladder_max_output(p))\n  xp = override_gradient(jnp.clip(xp, -x_max, x_max), xp)  # Clip val, not grad.\n  y = safe_sign(x) * select(\n      [\n          (p == 1, xp),\n          (p == 0, safe_log1p(xp)),\n          (p == -jnp.inf, -safe_expm1(-xp)),\n          (p == jnp.inf, safe_expm1(xp)),\n      ],\n      jnp.abs(p_safe - 1)\n      * (\n          ((safe_div(p_safe, jnp.abs(p_safe - 1)) * xp + 1)) ** p_safe - 1\n      ),\n  )\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute the inverse of the power ladder transformation\n  if postmult is not None:\n    y = y / postmult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          (jnp.maximum(tiny_val, jnp.abs(p_safe - 1)) / p_safe) * (jnp.power(1 + ys, 1 / p_safe) - 1)\n      ),\n  )\n  if premult is not None:\n    x = x * premult\n  return x"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  \"\"\"\n  Converts a set of rays from world space to normalized device coordinates (NDC) using a perspective projection model. This is useful for rendering or processing 3D scenes in a standardized coordinate system.\n\n  Input-Output Arguments\n  :param origins: ndarray(float32), The origins of the rays in world space, used to determine their starting points in NDC.\n  :param directions: ndarray(float32), The directions of the rays in world space, used to calculate their orientation in NDC.\n  :param pixtocam: ndarray(float32), The inverse intrinsic matrix of the camera, used for the perspective projection calculation.\n  :param near: float, The distance to the near plane along the negative z-axis, used to define the depth range of the projection.\n  :param xnp: module, Either numpy or jax.numpy, specifies the numerical library to use for calculations.\n  :return: Tuple of ndarray(float32), The origins and directions of the rays in normalized device coordinates.\n\n  This function performs a perspective projection of rays defined in world space into a normalized device coordinate system, assuming an identity extrinsic matrix (camera pose) and intrinsic parameters defined by the pixtocam matrix. The function adjusts ray origins to the near plane and calculates the corresponding directions in NDC, facilitating the rendering or analysis of 3D scenes from a standardized viewpoint.\n  \"\"\"\n  # Convert origins and directions to homogeneous coordinates.\n  origins_h = xnp.concatenate([origins, xnp.ones_like(origins[..., :1])], axis=-1)\n  directions_h = xnp.concatenate([directions, xnp.zeros_like(directions[..., :1])], axis=-1)\n\n  # Apply the inverse intrinsic matrix to the origins and directions.\n  origins_ndc = xnp.einsum('...ij,...j->...i', pixtocam, origins_h)\n  directions_ndc = xnp.einsum('...ij,...j->...i', pixtocam, directions_h)\n\n  # Normalize the directions to have unit length.\n  directions_ndc = xnp.divide(directions_ndc, xnp.linalg.norm(directions_ndc, axis=-1, keepdims=True))\n\n  # Adjust the origins to the near plane.\n  origins_ndc = origins_ndc * near / -origins_ndc[..., -1:]\n\n  return origins_ndc, directions_ndc"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Adjust the spline degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, len(x) - 1)\n  \n  # Perform spline interpolation\n  interpolated_values = scipy.interpolate.UnivariateSpline(t_input, x, k=spline_degree, s=smoothness)(t_output)\n  \n  return interpolated_values"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init = lr_init * lr_delay_mult\n    lr_init = lr_init + (lr_init - lr_final) * (1 - jnp.exp(-step / lr_delay_steps))\n\n  alpha = jnp.log(lr_init / lr_final)\n  decayed_lr = lr_init * jnp.exp(-alpha * step / max_steps)\n\n  return jnp.maximum(decayed_lr, lr_final)"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1]\n  ])"}
{"namespace": "utils.dummy_rays", "completion": "  rng = jax.random.PRNGKey(0)\n  n = 100  # Number of rays\n  origin_lo = 0\n  origin_hi = 1\n  radius_lo = 0.1\n  radius_hi = 0.5\n  near_lo = 0.1\n  near_hi = 1.0\n  far_lo = 1.0\n  far_hi = 10.0\n\n  return generate_random_rays(\n      rng,\n      n,\n      origin_lo,\n      origin_hi,\n      radius_lo,\n      radius_hi,\n      near_lo,\n      near_hi,\n      far_lo,\n      far_hi,\n      include_exposure_idx,\n      include_exposure_values,\n      include_device_idx,\n  )"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Shift the points to the camera coordinate system.\n  points = jax.vmap(spin_math.matmul)(camtoworlds, points)\n\n  # Apply inverse intrinsic matrices.\n  pixel_coords = jax.vmap(spin_math.matmul)(pixtocams, points)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_distort(\n        pixel_coords[Ellipsis, 0],\n        pixel_coords[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    pixel_coords = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.arctan2(pixel_coords[Ellipsis, 1], pixel_coords[Ellipsis, 0])\n    phi = xnp.arctan2(xnp.linalg.norm(pixel_coords[Ellipsis, :2], axis=-1), pixel_coords[Ellipsis, 2])\n    pixel_coords = xnp.stack([theta, phi], axis=-1)\n\n  elif camtype == ProjectionType.PANORAMIC:\n    theta = xnp.arctan2(pixel_coords[Ellipsis, 0], -pixel_coords[Ellipsis, 2])\n    phi = xnp.arctan2(pixel_coords[Ellipsis, 1], -pixel_coords[Ellipsis, 2])\n    pixel_coords = xnp.stack([theta, phi], axis=-1)\n\n  # Extract depth values.\n  depth = pixel_coords[Ellipsis, 2]\n\n  # Normalize pixel coordinates.\n  pixel_coords = pixel_coords[Ellipsis, :2] / depth[..., None]\n\n  return pixel_coords, depth"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    retrieval_funcs = [evaluate_one_query_expansion_node] * len(modules)\n    retrieval_params = make_retrieval_callable_params(strategies)\n    expanded_queries = list(map(lambda module: module(previous_result['queries']), modules))\n    retrieval_gt = previous_result['retrieval_gt']\n    metrics = strategies['metrics']\n    project_dir = node_line_dir\n\n    return evaluate_one_query_expansion_node(retrieval_funcs, retrieval_params, expanded_queries, retrieval_gt, metrics, project_dir, previous_result)"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    generator_funcs = modules\n    generator_params = module_params\n    prompts = previous_result['prompts'].tolist()\n    generation_gt = previous_result['generation_gt'].tolist()\n    metrics = cast_metrics(strategies.get('metrics', ['bleu', 'f1']))\n    project_dir = os.path.join(node_line_dir, 'prompt_maker_node')\n    \n    generator_callable_params = make_generator_callable_params(strategies)\n    generator_funcs.extend(generator_callable_params[0])\n    generator_params.extend(generator_callable_params[1])\n    \n    best_prompt_maker_result = evaluate_one_prompt_maker_node(generator_funcs, generator_params, prompts, generation_gt, metrics, project_dir)\n    \n    return best_prompt_maker_result"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = []\n    for node in nodes:\n        values.extend(extract_values(node, key))\n    return list(set(values))"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(ast.literal_eval)\n\n    return summary_df"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.get('module_type')\n        module_param = {k: v for k, v in module_dict.items() if k != 'module_type'}\n        return cls(module_type=module_type, module_param=module_param)"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    metric_names = []\n    metric_params = []\n\n    for metric in metrics:\n        if isinstance(metric, str):\n            metric_names.append(metric)\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            metric_names.append(list(metric.keys())[0])\n            metric_params.append(list(metric.values())[0])\n\n    return metric_names, metric_params"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "\n    if embedding_model is None:\n        embedding_model = embedding_models.get_default_embedding_model()\n\n    pred_embedding = embedding_model.encode(pred)\n    gt_embeddings = [embedding_model.encode(gt) for gt in generation_gt]\n\n    max_similarity = 0\n    for gt_embedding in gt_embeddings:\n        similarity = calculate_cosine_similarity(pred_embedding, gt_embedding)\n        max_similarity = max(max_similarity, similarity)\n\n    return max_similarity"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    global gfpgan_face_restorer\n\n    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer is not set up. Returning original image.\")\n        return np_image\n\n    try:\n        restored_image = gfpgan_face_restorer.restore(np_image)\n        return restored_image\n    except Exception as e:\n        logger.error(\"Error restoring faces with GFPGAN:\", exc_info=True)\n        return np_image"}
{"namespace": "codeformer_model.setup_model", "completion": "    global codeformer\n\n    try:\n        codeformer = FaceRestorerCodeFormer(dirname)\n    except errors.ModelLoadingError as e:\n        logger.error(f\"Error setting up CodeFormer model: {e}\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    try:\n        face_restorer = FaceRestorerGFPGAN()\n        face_restorer.model_path = dirname\n        face_restorer.setup()\n        gfpgan_face_restorer = face_restorer\n    except Exception as e:\n        logger.error(f\"Failed to set up GFPGAN face restorer: {e}\")"}
{"namespace": "quaternion.rotate", "completion": "  v_quat = jnp.concatenate([v, jnp.zeros_like(v[..., :1])], axis=-1)\n  q_conj = conjugate(q)\n  v_rotated = multiply(multiply(q, v_quat), q_conj)[..., :3]\n  return v_rotated"}
{"namespace": "quaternion.from_axis_angle", "completion": "  half_angle = jnp.linalg.norm(axis_angle) / 2.0\n  sin_half_angle = jnp.sin(half_angle)\n  cos_half_angle = jnp.cos(half_angle)\n\n  # If the angle is close to zero, use a small angle approximation to avoid division by zero\n  small_angle = sin_half_angle < eps\n  scale = jnp.where(small_angle, 0.5 - (half_angle ** 2) / 12.0, sin_half_angle / (2.0 * half_angle))\n\n  # Construct the quaternion\n  w = cos_half_angle\n  xyz = axis_angle * scale\n  return jnp.concatenate([xyz, w], axis=-1)"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    num_calls = k\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n        num_calls += k\n    high = logit_bias[idx]\n\n    # improve estimate\n    low = 0\n    eps = 1e-8\n    mid = (high + low) / 2\n    while high >= low + eps:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n        num_calls += k\n    return model.logprob(prefix, logit_bias), num_calls"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(f\"The provided output_dir `{output_dir.path}` already contains data.\")\n\n    if append:\n        raise NotImplementedError(\"Appending data to the directory is not currently supported.\")\n\n    if overwrite:\n        raise NotImplementedError(\"Overwriting data in the directory is not currently supported.\")"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    index_file_exists = False\n    for content in objects.get(\"Contents\", []):\n        if content[\"Key\"].endswith(\"index.json\"):\n            index_file_exists = True\n            break\n\n    if index_file_exists:\n        raise RuntimeError(f\"The provided output_dir `{output_dir.path}` already contains an index file.\")\n\n    # Delete all objects within the specified prefix in the bucket\n    for content in objects.get(\"Contents\", []):\n        s3.delete_object(Bucket=obj.netloc, Key=content[\"Key\"])"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "\n        if node_rank is not None and node_rank != 0:\n            # Wait for the master node to perform the merge\n            while not os.path.exists(os.path.join(self._cache_dir, _INDEX_FILENAME)):\n                sleep(1)\n            return\n\n        # Perform the merge\n        self._merge_no_wait(node_rank)"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\"The Lightning SDK is not available. Please install the SDK to use this function.\")\n\n    if machine is None:\n        machine = Machine()\n\n    if command is None:\n        command = f\"cd {os.getcwd()} && {os.getenv('LIGHTNING_OPERATOR_COMMAND', 'python operator.py')}\"\n\n    studio = Studio()\n\n    job = studio.create_job(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command\n    )\n\n    print(f\"Job started: {job.url}\")\n\n    while job.status != \"completed\" and job.status != \"failed\":\n        sleep(10)\n        job.refresh()\n\n    if job.status == \"failed\":\n        raise RuntimeError(f\"The job {name} failed. Please check the job logs for more details.\")\n\n    print(f\"Job completed: {job.url}\")"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Your implementation here\n        config_file_path = os.path.join(self._cache_dir, \"config.json\")\n        if os.path.exists(config_file_path):\n            try:\n                self._config = ChunksConfig.load(config_file_path)\n                return self._config\n            except Exception as e:\n                logger.warning(f\"Failed to load ChunksConfig from {config_file_path}: {e}\")\n        return None"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Configuration should be defined before accessing it.\")\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The provided index must be an instance of ChunkedIndex.\")\n\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index configuration is not defined.\")\n\n        if self._prepare_thread is None:\n            raise AssertionError(\"The prepare thread is not defined. Please ensure it is correctly managing the chunks.\")\n\n        chunk_index = self._get_chunk_index_from_index(index.index)\n        self._prepare_thread.download([chunk_index])\n\n        return self._item_loader.load(index)"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            try:\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "\n    distributed_map = _ImmutableDistributedMap()\n\n    try:\n        return distributed_map.set_and_get(key, obj)\n    except RuntimeError:\n        return obj"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Sort items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        min_bin = min(bin_weights, key=bin_weights.get)\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Flatten the list of chunk indexes\n    all_chunks = [chunk for chunks in chunks_per_ranks for chunk in chunks]\n\n    # Set the random seed based on the current epoch\n    np.random.seed(seed + current_epoch)\n\n    # Shuffle the chunk indexes\n    np.random.shuffle(all_chunks)\n\n    return all_chunks"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for suffix in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n        if num_bytes < 1000:\n            return f\"{num_bytes:.2f} {suffix}\"\n        num_bytes /= 1000\n    return f\"{num_bytes:.2f} PB\""}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if not indexed_paths:\n        return None\n\n    input_dirs = set(os.path.dirname(path) for path in indexed_paths.values())\n\n    if len(input_dirs) > 1:\n        raise ValueError(\"Inconsistent input directories found in the provided inputs.\")\n\n    input_dir = input_dirs.pop()\n\n    if not os.path.isabs(input_dir):\n        project_root = Path(__file__).resolve().parent.parent\n        input_dir = str(project_root / input_dir)\n\n    return input_dir"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "\n    # Compute the new dilated time steps\n    t_dilate = t * dilation\n\n    # Clip the dilated time steps within the specified domain\n    t_dilate = torch.clamp(t_dilate, min=domain[0], max=domain[1])\n\n    # Adjust the weights to match the dilated time steps\n    p = weight_to_pdf(t, w)\n    p_dilate = interpolate(t_dilate, t, p)\n    w_dilate = pdf_to_weight(t_dilate, p_dilate)\n\n    return t_dilate, w_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "\n    # Find the indices where tq should be inserted into t to maintain order\n    idx_lo, idx_hi = searchsorted(t, tq)\n\n    # Check if query times exactly match a step change time\n    exact_match = torch.logical_and(t[idx_lo] == tq, t[idx_hi] == tq)\n\n    # Interpolate the values at the query times based on the step function\n    interpolated_values = torch.where(exact_match, outside_value, interpolate(tq, t, y))\n\n    return interpolated_values"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # Calculate the bias function based on the training fraction and annealing slope\n    bias = 1 / (1 + torch.exp(-anneal_slope * (train_frac - 0.5)))\n\n    # Calculate the adjusted weights using the bias function\n    adjusted_weights = w ** bias\n\n    # Handle cases where adjacent intervals have zero distance by setting their weight to zero\n    adjusted_weights = torch.where(t[..., 1:] - t[..., :-1] <= 0, torch.zeros_like(adjusted_weights), adjusted_weights)\n\n    # Prevent NaN values by using a softmax operation on the adjusted weights\n    adjusted_weights = torch.nn.functional.softmax(adjusted_weights, dim=-1)\n\n    return adjusted_weights"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) if k != 'meta' else v for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # expand faces tensor to match the batch dimension of the vertices tensor\n    f_expanded = expand0(f, v.shape[0])\n\n    # gather the vertices for each face\n    gathered_vertices = multi_gather(v, f_expanded, dim)\n\n    # compute the normals of the faces\n    edge1 = gathered_vertices[:, 1] - gathered_vertices[:, 0]\n    edge2 = gathered_vertices[:, 2] - gathered_vertices[:, 0]\n    face_normals = torch.cross(edge1, edge2, dim=dim)\n\n    return face_normals"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n        batch = torch.stack(batch, dim=0) if isinstance(batch[0], torch.Tensor) else np.stack(batch, axis=0)\n    elif isinstance(batch, dict):\n        batch = {k: add_batch(v) for k, v in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        batch = np.expand_dims(batch, axis=0)\n    else:\n        raise ValueError(\"Unsupported data type\")\n\n    return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H = torch.tensor(self.H)\n        batch.W = torch.tensor(self.W)\n        batch.K = torch.tensor(self.K.mT.ravel())\n        batch.R = torch.tensor(self.R.mT.ravel())\n        batch.T = torch.tensor(self.T.ravel())\n        batch.n = torch.tensor(self.n)\n        batch.f = torch.tensor(self.f)\n        batch.t = torch.tensor(self.t)\n        batch.v = torch.tensor(self.v)\n        batch.bounds = torch.tensor(self.bounds.ravel())\n        \n        meta = dotdict()\n        meta.mass = torch.tensor(self.mass)\n        meta.moment_of_inertia = torch.tensor(self.moment_of_inertia)\n        meta.movement_force = torch.tensor(self.movement_force)\n        meta.movement_torque = torch.tensor(self.movement_torque)\n        meta.movement_speed = torch.tensor(self.movement_speed)\n        meta.origin = torch.tensor(self.origin.ravel())\n        meta.world_up = torch.tensor(self.world_up.ravel())\n        \n        batch.meta = meta\n        return batch"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)\n            self.persistence.save_agent(serialized_agent)"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            max_similarity = -np.inf\n            closest_agent = None\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n                if similarity > max_similarity:\n                    max_similarity = similarity\n                    closest_agent = agent\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            return None, -np.inf"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(\"prime prompt\", PRIME_NAME, 0, self, self.openai_wrapper)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.is_prime = True\n        prime_agent.unspecified_flag = True\n        self.agents.append(prime_agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        agent_dict = self.persistence.load_agent(purpose)\n        if agent_dict:\n            agent = AgentSerializer.from_dict(agent_dict, agent_lifecycle, openai_wrapper)\n            return agent\n        else:\n            return None"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name, input_text = agent_info.split(\":\") if \":\" in agent_info else (agent_info, \"\")\n        return agent_name, input_text"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        loaded_agents = []\n        # Fetch all agents from the database\n        all_agents = self.persistence.fetch_all_agents()\n        # Load each agent and add it to the list of loaded agents\n        for serialized_agent in all_agents:\n            loaded_agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if loaded_agent:\n                loaded_agents.append(loaded_agent)\n        return loaded_agents"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the bounding box of the instance\n    bbox = instance[\"bbox\"]\n\n    # Calculate the center of the bounding box\n    center_x = (bbox[0] + bbox[2]) / 2\n    center_y = (bbox[1] + bbox[3]) / 2\n\n    # Calculate the top-left corner of the crop\n    crop_x1 = max(0, int(center_x - crop_size[1] / 2))\n    crop_y1 = max(0, int(center_y - crop_size[0] / 2))\n\n    # Calculate the bottom-right corner of the crop\n    crop_x2 = min(image_size[1], crop_x1 + crop_size[1])\n    crop_y2 = min(image_size[0], crop_y1 + crop_size[0])\n\n    # Create a CropTransform object\n    crop_transform = T.CropTransform(crop_x1, crop_y1, crop_x2, crop_y2)\n\n    return crop_transform"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n    return image"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # Transform bounding box\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox = transforms.apply_box([bbox])[0]\n    annotation[\"bbox\"] = [bbox[0], bbox[1], bbox[2], bbox[3]]\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform segmentation\n    if \"segmentation\" in annotation:\n        if annotation[\"segmentation\"] is None:\n            annotation[\"segmentation\"] = []\n        else:\n            if isinstance(annotation[\"segmentation\"], list):\n                annotation[\"segmentation\"] = [\n                    transforms.apply_segmentation(s) for s in annotation[\"segmentation\"]\n                ]\n            else:\n                raise ValueError(\"Segmentation format not supported.\")\n\n    # Transform keypoints\n    if \"keypoints\" in annotation:\n        if annotation[\"keypoints\"] is None:\n            annotation[\"keypoints\"] = []\n        else:\n            annotation[\"keypoints\"] = transform_keypoint_annotations(\n                annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n            )\n\n    return annotation"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        new_coords = np.column_stack((coords, np.ones(coords.shape[0])))\n        new_coords = np.dot(new_coords, self.rm_coords.T)\n        return new_coords[:, :2]"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [obj[\"bbox\"] for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if \"segmentation\" in annos[0]:\n        if mask_format == \"polygon\":\n            masks = [obj[\"segmentation\"] for obj in annos]\n            masks = [polygon_to_bitmask(p, target.image_size[0], target.image_size[1]) for p in masks]\n            target.gt_masks = BitMasks(masks)\n        elif mask_format == \"bitmask\":\n            masks = [obj[\"segmentation\"] for obj in annos]\n            target.gt_masks = BitMasks(masks)\n\n    if \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n        target.gt_keypoints = Keypoints(keypoints)\n\n    return target"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if len(img) == 0 or self.angle % 360 == 0:\n            return img\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=self.interp)"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes if predictions.has(\"pred_classes\") else None\n        masks = predictions.pred_masks if predictions.has(\"pred_masks\") else None\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        labels = _create_text_labels(classes, scores, self.metadata.thing_classes)\n        assigned_colors = None\n        if self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\n            assigned_colors = [\n                self._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in classes\n            ]\n\n        self.overlay_instances(\n            boxes=boxes,\n            labels=labels,\n            masks=masks,\n            keypoints=keypoints,\n            assigned_colors=assigned_colors,\n        )\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert the RGBA image to RGB format\n        image_rgba = self.canvas.buffer_rgba()\n        image_rgb = Image.fromarray(image_rgba).convert(\"RGB\")\n        image_np = np.array(image_rgb)\n        return image_np"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # Your implementation here\n        # Use the provided annotation/segmentation data in the dictionary to draw visualizations on the image\n        # Return the modified image object with the visualizations drawn on it\n        pass"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    reloaded_module = torch.jit.load(buffer)\n    return reloaded_module"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        shape2d = (binary_mask.shape[0], binary_mask.shape[1])\n        rgba = np.zeros(shape2d + (4,), dtype=\"float32\")\n        rgba[:, :, :3] = color\n        rgba[:, :, 3] = binary_mask * alpha\n        self.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\n\n        if text is not None:\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            self._draw_text_in_mask(binary_mask, text, lighter_color)\n        return self.output"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if size_as_tensor:\n        assert torch.allclose(\n            input.image_size, other.image_size, rtol=rtol\n        ), f\"{msg}Image sizes do not match: {input.image_size} vs {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg}Image sizes do not match: {input.image_size} vs {other.image_size}\"\n\n    for field in input.get_fields():\n        input_val = input[field]\n        other_val = other[field]\n        if isinstance(input_val, Boxes):\n            assert torch.allclose(\n                input_val.tensor, other_val.tensor, rtol=rtol\n            ), f\"{msg}Field {field} does not match\"\n        elif isinstance(input_val, ROIMasks):\n            assert torch.allclose(\n                input_val.tensor, other_val.tensor, rtol=rtol\n            ), f\"{msg}Field {field} does not match\"\n        elif isinstance(input_val, torch.Tensor):\n            assert torch.allclose(\n                input_val, other_val, rtol=rtol\n            ), f\"{msg}Field {field} does not match\"\n        else:\n            assert input_val == other_val, f\"{msg}Field {field} does not match\""}
{"namespace": "detectron2.utils.registry.locate", "completion": "    obj = pydoc.locate(name)\n    if obj is None:\n        raise ImportError(f\"Error locating object: {name}\")\n    return obj"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        raise NotImplementedError(f\"Segmentor model of type {cfg['type']} is not implemented yet.\")"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    return val - torch.floor(val / period + offset) * period"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    pred = {}\n    gt = {}\n    for i in range(len(gt_annos)):\n        gt_anno = gt_annos[i]\n        dt_anno = dt_annos[i]\n        for j in range(len(gt_anno['name'])):\n            name = gt_anno['name'][j]\n            if name not in gt:\n                gt[name] = []\n                pred[name] = []\n            bbox = gt_anno['bbox'][j]\n            gt[name].append({'bbox': bbox, 'score': 1})\n        for j in range(len(dt_anno['name'])):\n            name = dt_anno['name'][j]\n            if name not in pred:\n                pred[name] = []\n                gt[name] = []\n            bbox = dt_anno['bbox'][j]\n            score = dt_anno['score'][j]\n            pred[name].append({'bbox': bbox, 'score': score})\n    ovthresh = metric\n    ret = eval_map_recall(pred, gt, ovthresh)\n    ap_strs = []\n    ar_strs = []\n    for i, ov in enumerate(ovthresh):\n        recall, precision, ap = ret\n        ap_str = []\n        ar_str = []\n        for label in ap[i].keys():\n            ap_str.append(f'{label2cat[label]}: {ap[i][label][0]:.4f}')\n            ar_str.append(f'{label2cat[label]}: {recall[i][label][0]:.4f}')\n        ap_strs.append(', '.join(ap_str))\n        ar_strs.append(', '.join(ar_str))\n    mAP = [np.mean(ap[i][label]) for i, label in enumerate(ap)]\n    mAR = [np.mean(recall[i][label]) for i, label in enumerate(recall)]\n    mAP_str = ', '.join([f'{ov}: {mAP[i]:.4f}' for i, ov in enumerate(ovthresh)])\n    mAR_str = ', '.join([f'{ov}: {mAR[i]:.4f}' for i, ov in enumerate(ovthresh)])\n    if logger is not None:\n        if isinstance(logger, str):\n            print_log(f'mAP: {mAP_str}, mAR: {mAR_str}', logger=logger)\n            for i, ov in enumerate(ovthresh):\n                print_log(f'AP at {ov}: {ap_strs[i]}', logger=logger)\n                print_log(f'AR at {ov}: {ar_strs[i]}', logger=logger)\n        else:\n            logger.info(f'mAP: {mAP_str}, mAR: {mAR_str}')\n            for i, ov in enumerate(ovthresh):\n                logger.info(f'AP at {ov}: {ap_strs[i]}')\n                logger.info(f'AR at {ov}: {ar_strs[i]}')\n    return {'mAP': mAP, 'mAR': mAR, 'AP': ap, 'AR': recall}"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    box_type = box_type.upper()\n    if box_type == \"LIDAR\":\n        return LidarInstance3DBoxes, \"LiDAR\"\n    elif box_type == \"CAMERA\":\n        return CameraInstance3DBoxes, \"Camera\"\n    elif box_type == \"DEPTH\":\n        return DepthInstance3DBoxes, \"Depth\"\n    else:\n        raise ValueError(f\"Unrecognized box type: {box_type}. Please use one of 'LiDAR', 'Camera', or 'Depth'.\")"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.create", "completion": "\n    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    async with aiofiles.open(path, 'rb') as f:\n      while True:\n        chunk = await f.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      async def upload_bytes():\n        async with aiofiles.open(path, 'rb') as f:\n          while True:\n            chunk = await f.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      await self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "\n        self._state_dict = state_dict\n        self._validate_state_dict()"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict:\n            state: Dict[str, Any] = self._state_dict\n            if state[\"shuffle\"] != self.shuffle:\n                raise ValueError(\"Mismatch in shuffle parameter between state dictionary and current state\")\n            if state[\"num_workers\"] != self.worker_env.world_size:\n                raise ValueError(\"Mismatch in num_workers parameter between state dictionary and current state\")\n            if state[\"input_dir_path\"] != self.input_dir.path:\n                raise ValueError(\"Mismatch in input directory path between state dictionary and current state\")\n            if state[\"input_dir_url\"] != self.input_dir.url:\n                raise ValueError(\"Mismatch in input directory URL between state dictionary and current state\")\n            if state[\"seed\"] != self.seed:\n                raise ValueError(\"Mismatch in seed parameter between state dictionary and current state\")\n            if state[\"item_loader\"] != self.item_loader.state_dict() if self.item_loader else None:\n                raise ValueError(\"Mismatch in item_loader state between state dictionary and current state\")\n            if state[\"drop_last\"] != self.drop_last:\n                raise ValueError(\"Mismatch in drop_last flag between state dictionary and current state\")"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Process command line arguments')\n    parser.add_argument('--task', type=str, required=True, help='The task description, specifying what task should be performed.')\n    parser.add_argument('--upload-files', nargs='+', type=str, help='List of files to upload, allowing multiple files to be specified.')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use.')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs.')\n    parser.add_argument('--mode', type=str, default='auto', help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument('--quiet', action='store_true', help='If set, the program runs in quiet mode with minimal output.')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be.')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution.')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans.')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be.')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts, specifying how many times a task can be retried upon failure.')\n    parser.add_argument('--config-file', type=str, default=os.getenv('CONFIG_FILE', 'assets/config.yml'), help=\"Path to the configuration file, specifying where to find the configuration settings.\")\n\n    return parser.parse_args()"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir:\n        cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hashlib.md5(input_dir.encode()).hexdigest())\n    else:\n        cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hashlib.md5(\"\".encode()).hexdigest())\n\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n    except Exception as e:\n        logger.error(f\"Error creating cache directory: {e}\")\n        return None"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"/cache\") or path.startswith(_DEFAULT_CACHE_DIR):\n        return True\n    else:\n        return False"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    samples_per_worker = num_samples_yielded // num_workers\n    remaining_samples = num_samples_yielded % num_workers\n\n    samples_distribution = {i: samples_per_worker for i in range(num_workers)}\n    for i in range(remaining_samples):\n        samples_distribution[i] += 1\n\n    return samples_distribution"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        parsed_url = parse.urlparse(remote_filepath)\n        if parsed_url.scheme != \"s3\":\n            raise ValueError(\"The remote file path must use the 's3' scheme\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        lock_filepath = local_filepath + \".lock\"\n        with FileLock(lock_filepath, timeout=60):\n            if self._s5cmd_available:\n                subprocess.run([\"s5cmd\", \"cp\", remote_filepath, local_filepath], check=True)\n            else:\n                self._client.download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    chunks_per_replica = len(chunks_replica)\n    chunks_per_worker = chunks_per_replica // num_workers\n    remaining_chunks = chunks_per_replica % num_workers\n\n    intervals_per_replica = [interval[-1] - interval[0] for interval in intervals_replica]\n    intervals_per_worker = [sum(intervals_per_replica[i * chunks_per_worker : (i + 1) * chunks_per_worker]) for i in range(num_workers)]\n    remaining_intervals = sum(intervals_per_replica[chunks_per_worker * num_workers:])\n\n    workers_chunks = {}\n    workers_intervals = {}\n\n    for i in range(num_workers):\n        start_chunk = i * chunks_per_worker + min(i, remaining_chunks)\n        end_chunk = start_chunk + chunks_per_worker + (1 if i < remaining_chunks else 0)\n        workers_chunks[i] = chunks_replica[start_chunk:end_chunk]\n\n        start_interval = sum(intervals_per_worker[:i]) + min(i, remaining_chunks) * intervals_per_replica[chunks_per_worker]\n        end_interval = start_interval + intervals_per_worker[i] + (intervals_per_replica[chunks_per_worker] if i < remaining_chunks else 0)\n        workers_intervals[i] = intervals_replica[start_interval:end_interval]\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath.replace(\"local:\", \"\", 1)\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    updated_indexes = {}\n\n    for worker_idx, intervals in workers_intervals.items():\n        chunk_index = 0\n        for interval in intervals:\n            interval_size = interval[1] - interval[0]\n            if indexes[worker_idx] < interval_size:\n                updated_indexes[worker_idx] = indexes[worker_idx]\n                chunks_index[worker_idx] = chunk_index\n                break\n            else:\n                indexes[worker_idx] -= interval_size\n                chunk_index += 1\n\n    return chunks_index, updated_indexes"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        size = item.size\n        mode = item.mode\n        mode_bytes = mode.encode(\"utf-8\")\n        mode_length = len(mode_bytes).to_bytes(4, byteorder=\"big\")\n        size_bytes = b\"\".join([s.to_bytes(4, byteorder=\"big\") for s in size])\n        return size_bytes + mode_length + mode_bytes + item.tobytes(), None"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile) and hasattr(item, \"filename\") and os.path.isfile(item.filename):\n            with open(item.filename, \"rb\") as f:\n                return f.read(), None\n        elif _PIL_AVAILABLE and isinstance(item, Image.Image):\n            with io.BytesIO() as output:\n                item.save(output, format=\"JPEG\")\n                return output.getvalue(), None\n        else:\n            raise TypeError(\"Unsupported image type for JPEG serialization\")"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[:12], dtype=np.uint32)\n        width, height, mode_length = ints\n        mode = data[12:12 + mode_length].decode(\"utf-8\")\n        raw = data[12 + mode_length:]\n        return Image.frombytes(mode, (width, height), raw)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the tensor data\n        tensor_data = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n\n        # reshape the tensor data to match the original shape\n        tensor = torch.from_numpy(tensor_data).reshape(shape)\n\n        return tensor"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = _TORCH_DTYPES_MAPPING[item.dtype]\n        dtype_bytes = np.array([dtype_indice], np.uint32).tobytes()\n        shape_bytes = np.array(item.shape, np.uint32).tobytes()\n        data_bytes = item.tobytes()\n        return dtype_bytes + shape_bytes + data_bytes, None"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb / 12.92\n  linear1 = xnp.power((srgb + 0.055) / 1.055, 2.4)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)"}
{"namespace": "resample.resample_3d", "completion": "  if method == 'NEAREST':\n    # Round the locations to the nearest integer\n    locations = jnp.round(locations).astype(jnp.int32)\n    # Gather the data at the rounded locations\n    resampled_data = gather_volume(data, locations, coordinate_order)\n  elif method == 'TRILINEAR':\n    # If half-pixel centering is used, adjust the locations\n    if half_pixel_center:\n      locations -= 0.5\n    # Extract the integer and fractional parts of the locations\n    int_locations = jnp.floor(locations).astype(jnp.int32)\n    frac_locations = locations - int_locations\n    # Handle edge behavior based on the specified method\n    if edge_behavior == 'CONSTANT_OUTSIDE':\n      # Pad the input data with constant values\n      padded_data = jnp.pad(data, 1, constant_values=constant_values)\n      # Gather the data at the integer locations\n      gathered_data = gather_volume(padded_data, int_locations + 1, coordinate_order)\n    elif edge_behavior == 'CLAMP':\n      # Clamp the integer locations to the valid range\n      clamped_locations = jnp.clip(int_locations, 0, jnp.array(data.shape) - 1)\n      # Gather the data at the clamped locations\n      gathered_data = gather_volume(data, clamped_locations, coordinate_order)\n    else:\n      raise ValueError(\"Invalid edge_behavior. Supported options are 'CONSTANT_OUTSIDE' and 'CLAMP'.\")\n    # Calculate the trilinear interpolation weights\n    w000 = (1 - frac_locations[..., 0]) * (1 - frac_locations[..., 1]) * (1 - frac_locations[..., 2])\n    w001 = (1 - frac_locations[..., 0]) * (1 - frac_locations[..., 1]) * frac_locations[..., 2]\n    w010 = (1 - frac_locations[..., 0]) * frac_locations[..., 1] * (1 - frac_locations[..., 2])\n    w011 = (1 - frac_locations[..., 0]) * frac_locations[..., 1] * frac_locations[..., 2]\n    w100 = frac_locations[..., 0] * (1 - frac_locations[..., 1]) * (1 - frac_locations[..., 2])\n    w101 = frac_locations[..., 0] * (1 - frac_locations[..., 1]) * frac_locations[..., 2]\n    w110 = frac_locations[..., 0] * frac_locations[..., 1] * (1 - frac_locations[..., 2])\n    w111 = frac_locations[..., 0] * frac_locations[..., 1] * frac_locations[..., 2]\n    # Perform trilinear interpolation\n    resampled_data = (\n        w000 * gathered_data[Ellipsis, 0] + w001 * gathered_data[Ellipsis, 1] +\n        w010 * gathered_data[Ellipsis, 2] + w011 * gathered_data[Ellipsis, 3] +\n        w100 * gathered_data[Ellipsis, 4] + w101 * gathered_data[Ellipsis, 5] +\n        w110 * gathered_data[Ellipsis, 6] + w111 * gathered_data[Ellipsis, 7]\n    )\n  else:\n    raise ValueError(\"Invalid method. Supported options are 'TRILINEAR' and 'NEAREST'.\")\n\n  return resampled_data"}
{"namespace": "linspline.integrate", "completion": "  dt = jnp.diff(t)\n  return jnp.sum((w[Ellipsis, :-1] + w[Ellipsis, 1:]) * dt / 2, axis=-1)"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the spline values corresponding to each input query.\n  idx0, idx1 = sorted_lookup(tq, t)\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Linearly interpolate between the two nearest values.\n  slope = (v1 - v0) / (t1 - t0)\n  interpolated_values = v0 + slope * (tq - t0)\n  return jnp.where((tq < t[Ellipsis, 0]) | (tq > t[Ellipsis, -1]), 0, interpolated_values)"}
{"namespace": "geometry.are_lines_parallel", "completion": "  epsilon = 1e-6  # Small epsilon to account for numerical precision\n  dir1_normalized = spin_math.normalize(dir1)  # Normalize the direction vector of the first line\n  dir2_normalized = spin_math.normalize(dir2)  # Normalize the direction vector of the second line\n  dot_product = jnp.dot(dir1_normalized, dir2_normalized)  # Calculate the dot product of the normalized direction vectors\n  return jnp.abs(dot_product - 1) < epsilon  # Return True if the dot product is close to 1, indicating parallel lines, otherwise False"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if not isinstance(v, int) or v < 1:\n    raise ValueError(f'v {v} must be an integer greater than or equal to 1')\n\n  # Generate all combinations of weights for the tessellation factor v\n  weights = np.array([[i, j, v - i - j] for i in range(v + 1) for j in range(v - i + 1)])\n\n  # Normalize the weights to get the barycentric coordinates\n  weights = weights / v\n\n  return weights"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = jnp.sqrt(x**2 + y**2 + z**2)\n  theta = jnp.arccos(z / (r + eps))\n  phi = jnp.arctan2(y, x)\n  return r, theta, phi"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    # Compute pairwise distances within mat0\n    diff = mat0[:, :, None] - mat0[:, None, :]\n    sq_dist = np.sum(diff**2, 0)\n  else:\n    # Compute pairwise distances between mat0 and mat1\n    diff = mat0[:, :, None] - mat1[:, None, :]\n    sq_dist = np.sum(diff**2, 0)\n\n  # Set negative distances to zero\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x), jnp.inf)\n  )"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, np.nextafter(np.log1p(max_val), np.float32)),\n  )(x)"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                tensor = decode_jpeg(data)\n                return tensor\n            except RuntimeError:\n                pass\n\n        # Fallback to using PIL to deserialize the data\n        pil_image = Image.open(io.BytesIO(data))\n\n        if _TORCH_VISION_AVAILABLE:\n            # Convert the PIL image to a PyTorch tensor\n            tensor = pil_to_tensor(pil_image)\n            return tensor\n        else:\n            return pil_image"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "    pass\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        tensor = torch.frombuffer(data, dtype=self._dtype)\n        return tensor"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        array_data = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        return array_data.reshape(shape)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = _NUMPY_DTYPES_MAPPING[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_streaming\n            if isinstance(self.dataset, StreamingDataset)\n            else self._num_samples_yielded_combined,\n            \"latest_worker_idx\": self._latest_worker_idx - 1,\n        }\n\n        if isinstance(self.dataset, CombinedStreamingDataset):\n            state[\"dataset\"] = self.dataset.state_dict()\n        elif isinstance(self.dataset, StreamingDataset):\n            state[\"dataset\"] = self.dataset.state_dict()\n\n        return state"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE or not _AV_AVAILABLE:\n            raise Exception(\"Required libraries (torchvision and av) are not installed.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as temp_file:\n            temp_file.write(data)\n            temp_file_path = temp_file.name\n\n        try:\n            video, audio, info = torchvision.io.read_video(temp_file_path)\n            return video\n        finally:\n            os.unlink(temp_file_path)"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        while not self.filled:\n            if self._should_write():\n                self.write_chunk()\n        return [self.write_chunks_index()]\n        self._is_done = True"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        else:\n            raise RuntimeError(\"The dataset associated with the StreamingDataLoader must be either a StreamingDataset or a CombinedStreamingDataset.\")"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        return _state_dict(self._datasets, self._num_samples_yielded, num_workers, batch_size)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if self._iterator is not None:\n            num_workers = len(state_dict)\n            batch_size = len(list(state_dict.values())[0])\n            num_samples_yielded = [len(state_dict[str(i)]) for i in range(len(state_dict))]\n            self._iterator.load_state_dict(state_dict, num_workers, batch_size, num_samples_yielded)"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path.startswith(\"s3://\"):\n        return _resolve_s3_connections(dir_path)\n    elif dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n    elif dir_path.startswith(\"projects/\"):\n        return _resolve_studio(dir_path, None, None)\n    else:\n        raise ValueError(\"Unsupported directory path format.\")"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "\n    class OptimizeDNSContext:\n        def __init__(self, enable: bool):\n            self.enable = enable\n\n        def __enter__(self):\n            _optimize_dns(self.enable)\n\n        def __exit__(self, exc_type, exc_value, traceback):\n            _optimize_dns(False)\n\n    return OptimizeDNSContext(enable)"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    total_items = len(indexes)\n    items_per_rank = total_items // distributed_env.world_size\n    remainder = total_items % distributed_env.world_size\n\n    chunks_per_ranks = []\n    intervals_per_ranks = []\n\n    start = 0\n    for rank in range(distributed_env.world_size):\n        num_items = items_per_rank + (1 if rank < remainder else 0)\n        end = start + num_items\n        chunks_per_ranks.append(indexes[start:end])\n        intervals_per_ranks.append(chunk_intervals[start:end])\n        start = end\n\n    if drop_last and remainder > 0:\n        chunks_per_ranks[-1] = chunks_per_ranks[-1][:-1]\n        intervals_per_ranks[-1] = intervals_per_ranks[-1][:-1]\n\n    return chunks_per_ranks, intervals_per_ranks"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {\"output_dir\": output_dir}\n        if self._contains_device:\n            self._find_device()\n            kwargs[\"device\"] = self._device\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        if isinstance(self._fn, partial):\n            self._fn(item_metadata, **kwargs)\n        elif isinstance(self._fn, FunctionType):\n            self._fn(item_metadata, **kwargs)\n        elif callable(self._fn):\n            self._fn.__call__(item_metadata, **kwargs)\n        else:\n            raise ValueError(f\"The provided {self._fn} isn't supported.\")"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            response = s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return response\n        except botocore.exceptions.ClientError as e:\n            error_code = e.response[\"Error\"][\"Code\"]\n            if error_code == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "        import psutil\n    import psutil\n\n    while True:\n        disk_usage = psutil.disk_usage(input_dir)\n        free_space_gb = disk_usage.free / (2**30)  # Convert bytes to gigabytes\n        if free_space_gb < threshold_in_gb:\n            break\n        else:\n            sleep(sleep_time)"}
{"namespace": "litdata.processing.functions.optimize", "completion": "\n    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if error_when_not_empty:\n            _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default_num_workers(),\n            fast_dev_run=fast_dev_run,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_files=reorder_files,\n            weights=weights,\n            reader=reader,\n        )\n        with optimize_dns_context(True):\n            return data_processor.run(LambdaDataTransformRecipe(fn, inputs))\n    return _execute(\n        f\"data-prep-map-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\",\n        num_nodes,\n        machine,\n    )"}
{"namespace": "litdata.processing.functions.map", "completion": "\n    if error_when_not_empty and os.path.exists(output_dir) and os.listdir(output_dir):\n        raise ValueError(f\"The output directory {output_dir} is not empty.\")\n\n    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default_num_workers(),\n            fast_dev_run=fast_dev_run,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_files=reorder_files,\n            reader=reader,\n        )\n\n        with optimize_dns_context(True):\n            data_processor.run(\n                LambdaDataTransformRecipe(\n                    fn,\n                    inputs,\n                )\n            )\n        return None\n    return _execute(\n        f\"data-prep-optimize-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\",\n        num_nodes,\n        machine,\n    )"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        task = queue_in.get()\n        if task is None:\n            return\n\n        index, file_paths = task\n        downloaded_files = []\n\n        for file_path in file_paths:\n            destination_path = os.path.join(cache_dir, os.path.basename(file_path))\n            if not os.path.exists(destination_path):\n                # Download the file from the source directory to the cache directory\n                # Code for downloading the file goes here\n                # Example: download_file(file_path, destination_path)\n                downloaded_files.append(destination_path)\n            else:\n                downloaded_files.append(destination_path)\n\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "\n    while True:\n        # 1. Fetch from the queue\n        data = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if data is None:\n            return\n\n        # 3. Upload the data to the output directory\n        if isinstance(data, str):\n            # Single file upload\n            if output_dir.url:\n                # Upload to S3\n                s3 = S3Client()\n                s3.client.upload_file(data, output_dir.url, os.path.basename(data))\n            else:\n                # Move within the local filesystem\n                shutil.move(data, os.path.join(output_dir.path, os.path.basename(data)))\n        elif isinstance(data, tuple) and len(data) == 2:\n            # Temporary directory and file path upload\n            temp_dir, file_path = data\n            if output_dir.url:\n                # Upload to S3\n                s3 = S3Client()\n                s3.client.upload_file(file_path, output_dir.url, os.path.basename(file_path))\n            else:\n                # Move within the local filesystem\n                shutil.move(file_path, os.path.join(output_dir.path, os.path.basename(file_path)))\n            # Clean up the temporary directory\n            shutil.rmtree(temp_dir)\n\n        # 4. Inform the remove queue that the file has been successfully uploaded\n        remove_queue.put(data)"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "\n    # Calculate the total weight\n    total_weight = sum(weights) if weights else len(user_items)\n\n    # Calculate the weight per worker\n    weight_per_worker = total_weight / (num_workers)\n\n    # Initialize variables\n    worker_items = [[] for _ in range(num_workers)]\n    current_worker = 0\n    current_weight = 0\n\n    # Iterate through the items and assign them to workers based on weights\n    for i, item in enumerate(user_items):\n        if weights:\n            current_weight += weights[i]\n        else:\n            current_weight += 1\n\n        # Check if the current weight exceeds the weight per worker\n        if current_weight > weight_per_worker and current_worker < num_workers - 1:\n            current_worker += 1\n            current_weight = 0\n\n        # Assign the item to the current worker\n        worker_items[current_worker].append(item)\n\n    # Shuffle the items for each worker\n    for i in range(num_workers):\n        random.shuffle(worker_items[i])\n\n    return worker_items"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    total_items = len(user_items)\n    total_workers = _get_num_nodes() * num_workers\n    items_per_worker = total_items // total_workers\n    remainder = total_items % total_workers\n\n    start = 0\n    end = 0\n    worker_items = []\n\n    for i in range(total_workers):\n        end += items_per_worker + (1 if i < remainder else 0)\n        worker_items.append(user_items[start:end])\n        start = end\n\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers\")\n\n    return worker_items"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        if self.input_dir and self.input_dir.path:\n            cache_dir = _get_cache_dir()\n            if os.path.exists(cache_dir):\n                shutil.rmtree(cache_dir)\n            os.makedirs(cache_dir, exist_ok=True)\n\n        if self.output_dir and self.output_dir.path:\n            cache_data_dir = _get_cache_data_dir()\n            if os.path.exists(cache_data_dir):\n                shutil.rmtree(cache_data_dir)\n            os.makedirs(cache_data_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "        import os\n        import concurrent.futures\n    import concurrent.futures\n    import os\n\n    def get_file_size(item):\n        if base_path:\n            file_path = os.path.join(base_path, item)\n        else:\n            file_path = item\n        return os.path.getsize(file_path)\n\n    with concurrent.futures.ThreadPoolExecutor() as executor:\n        file_sizes = list(executor.map(get_file_size, items))\n\n    return file_sizes"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if isinstance(element, str):\n        path = Path(element)\n        if input_dir:\n            input_path = Path(input_dir)\n            if path.is_absolute():\n                return path.parts[:len(input_path.parts)] == input_path.parts\n            else:\n                return (input_path / path).exists()\n        else:\n            return path.exists()\n    else:\n        return False"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = spin_math.safe_sqrt(jnp.sum(screw_axis[:3]**2))\n  w = screw_axis[:3] / theta\n  v = screw_axis[3:]\n\n  W = skew(w)\n  V = jnp.outer(w, v)\n\n  R = exp_so3(w * theta, eps)\n  t = (jnp.eye(3) * theta + (1 - jnp.cos(theta)) * W + (theta - jnp.sin(theta)) * V) @ v\n\n  return jnp.block([[R, t[:, jnp.newaxis]], [jnp.array([0.0, 0.0, 0.0, 1.0])]])"}
{"namespace": "rigid_body.exp_so3", "completion": "  \"\"\"\n  This function computes the exponential map from the Lie algebra so3 to the Lie group SO3, using Rodrigues' formula. It translates a 3D axis-angle representation of a rotation into a 3x3 rotation matrix. The function includes a numerical stability mechanism for small angles of rotation.\n\n  Input-Output Arguments\n  :param axis_angle: A 3-vector (numpy array). Represents the axis of rotation and the magnitude of rotation. It is used to compute the rotation matrix.\n  :param eps: Float. A small epsilon value for numerical stability, used to avoid division by zero or very small values that could lead to numerical instability. Defaults to the machine epsilon for float32.\n  :return: A (3, 3) numpy array. An orthonormal rotation matrix representing the same rotation as the input axis-angle representation.\n  \"\"\"\n  theta = jnp.linalg.norm(axis_angle)\n  if theta < eps:\n      return jnp.eye(3) + skew(axis_angle)\n  else:\n      K = skew(axis_angle / theta)\n      return jnp.eye(3) + jnp.sin(theta) * K + (1 - jnp.cos(theta)) * K @ K"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  d_mag_sq = jnp.maximum(1e-10, jnp.sum(d**2, axis=-1, keepdims=True))\n  mean = d[Ellipsis, None, :] * t_mean[Ellipsis, None]\n  \n  if diag:\n    d_outer_diag = d**2\n    null_outer_diag = 1 - d_outer_diag / d_mag_sq\n    t_cov_diag = t_var[Ellipsis, None] * d_outer_diag[Ellipsis, None, :]\n    xy_cov_diag = r_var[Ellipsis, None] * null_outer_diag[Ellipsis, None, :]\n    cov_diag = t_cov_diag + xy_cov_diag\n    return mean, cov_diag\n  else:\n    d_outer = d[Ellipsis, :, None] * d[Ellipsis, None, :]\n    eye = jnp.eye(d.shape[-1])\n    null_outer = eye - d[Ellipsis, :, None] * (d / d_mag_sq)[Ellipsis, None, :]\n    t_cov = t_var[Ellipsis, None, None] * d_outer[Ellipsis, None, :, :]\n    xy_cov = r_var[Ellipsis, None, None] * null_outer[Ellipsis, None, :, :]\n    cov = t_cov + xy_cov\n    return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Shift ray origins to near plane, such that oz = -near.\n  # This makes the new near bound equal to 0.\n  t = -(near + origins[Ellipsis, 2]) / directions[Ellipsis, 2]\n  origins = origins + t[Ellipsis, None] * directions\n\n  dx, dy, dz = xnp.moveaxis(directions, -1, 0)\n  ox, oy, oz = xnp.moveaxis(origins, -1, 0)\n\n  xmult = 1.0 / pixtocams[0, 2]  # Equal to -2. * focal / cx\n  ymult = 1.0 / pixtocams[1, 2]  # Equal to -2. * focal / cy\n\n  # Perspective projection into NDC for the t = 0 near points\n  #     origins + 0 * directions\n  origins_ndc = xnp.stack(\n      [xmult * ox / oz, ymult * oy / oz, -xnp.ones_like(oz)], axis=-1\n  )\n\n  # Perspective projection into NDC for the t = infinity far points\n  #     origins + infinity * directions\n  infinity_ndc = xnp.stack(\n      [xmult * dx / dz, ymult * dy / dz, xnp.ones_like(oz)], axis=-1\n  )\n\n  # directions_ndc points from origins_ndc to infinity_ndc\n  directions_ndc = infinity_ndc - origins_ndc\n\n  return origins_ndc, directions_ndc"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  return w / jnp.diff(t)"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * jnp.linalg.norm(dirs, axis=-1) * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  w = p * td\n  w = w / jnp.sum(w)\n  return w"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 1:\n    raise ValueError(f'num_samples must be > 1, is {num_samples}.')\n\n  if rng is None:\n    # Deterministic sampling based on linspace.\n    centers = jnp.linspace(t[..., 0], t[..., -1], num_samples + 1)[..., 1:-1]\n  else:\n    # Random sampling using the provided random number generator.\n    u = jax.random.uniform(rng, shape=t.shape[:-1] + (num_samples,), dtype=jnp.float32)\n    t_samples = invert_cdf(u, t, w_logits)\n    if single_jitter:\n      # Jitter each sample along each ray by the same amount in the inverse CDF.\n      jitter = (2 * jax.random.bernoulli(rng, 0.5, shape=t_samples.shape) - 1) * eps\n      t_samples += jitter\n    centers = (t_samples[..., 1:] + t_samples[..., :-1]) / 2\n\n  if deterministic_center:\n    # Center the samples in each interval of the PDF.\n    centers = jnp.clip(centers, t[..., 0:1], t[..., -1:])\n\n  return centers"}
{"namespace": "stepfun.sample_intervals", "completion": "  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n  midpoints = (t_samples[..., :-1] + t_samples[..., 1:]) / 2\n  midpoints = jnp.clip(midpoints, domain[0], domain[1])\n  t_samples = jnp.concatenate([t_samples[..., :1], midpoints, t_samples[..., -1:]], axis=-1)\n  return t_samples"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n\n  # Ensure that the weights sum to 1\n  w = jnp.clip(w, 0, 1)  # Clip to ensure non-negative values\n  w = w / jnp.sum(w)  # Normalize to ensure sum of weights is 1\n\n  # Compute the cumulative sum of the weights\n  cw = integrate_weights(w)\n\n  # Interpolate into the integrated weights to find the percentiles\n  percentiles = math.sorted_interp(ps / 100, cw, t, utils.device_is_tpu())\n\n  return percentiles"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert weights to a probability density function (PDF)\n  p = weight_to_pdf(t, w)\n  \n  # Blur the PDF using a Gaussian filter\n  blurred_p = linspline.gaussian_filter1d(p, blur_halfwidth)\n  \n  # Resample the blurred PDF to match the new time points\n  resampled_w = resample(tq, t, blurred_p, use_avg=False)\n  \n  return resampled_w"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Perform type check using Pyright\n        process = subprocess.Popen(\n            [\"pyright\", \"--outputjson\", \"-\"],\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n        )\n        input_code = f\"{user_code}\\n{test_code}\"\n        stdout, stderr = process.communicate(input_code)\n\n        # Parse Pyright output to identify type errors\n        error_lines = []\n        for line in stderr.splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                error_lines.append(int(match.group(1)))\n\n        # Determine if type check passed or failed\n        passed = len(error_lines) == 0\n        message = \"Type check passed\" if passed else \"Type check failed\"\n\n        return TypeCheckResult(message, passed, {\"error_lines\": error_lines})"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        with no_fake_tensor():\n            compiled_module = aot_module(fn, get_compiler_fn(\"Forward Graph\"), get_compiler_fn(\"Backward Graph\"))\n        return compiled_module\n    else:\n        with no_fake_tensor():\n            compiled_function = aot_function(fn, get_compiler_fn(\"Forward Graph\"), get_compiler_fn(\"Backward Graph\"))\n        return compiled_function"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_file_path = os.path.join(trial_path, \"summary.csv\")\n    config_file_path = os.path.join(trial_path, \"config.yaml\")\n\n    summary_df = load_summary_file(summary_file_path)\n    with open(config_file_path, 'r') as f:\n        config_dict = yaml.safe_load(f)\n\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n\n    if output_path:\n        with open(output_path, 'w') as file:\n            yaml.dump(best_config, file, default_flow_style=False)\n\n    return best_config"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    cache = {}\n\n    def wrapper(*args, **kwargs):\n        inputs = (args, kwargs)\n        key = (func, hash_arg(inputs), hash_arg(kwargs_))\n        if key in cache:\n            return cache[key](*args, **kwargs)\n        else:\n            traced_func = trace_with_kwargs(func, *args, **kwargs_)\n            if ts_compiler is not None:\n                traced_func = ts_compiler(traced_func, *args, **kwargs_)\n            cache[key] = traced_func\n            return traced_func(*args, **kwargs_)\n\n    return wrapper"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        output_path = os.path.join(trial_path, 'best_config.yaml')\n        yaml_dict = extract_best_config(trial_path, output_path)\n        project_dir = os.path.dirname(trial_path)\n        return cls(yaml_dict, project_dir)"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = [result for result, val in zip(results, value) if val <= threshold]\n    filtered_metadatas = [metadata for metadata, val in zip(metadatas, value) if val <= threshold]\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    result_dfs = []\n    execution_times = []\n    for module, params in zip(modules, module_params):\n        result_df, execution_time = measure_speed(module, params, previous_result)\n        result_dfs.append(result_df)\n        execution_times.append(execution_time)\n\n    summary_df = pd.DataFrame({\n        'filename': [f\"result_{i}.parquet\" for i in range(len(result_dfs))],\n        'module_name': [module.__name__ for module in modules],\n        'module_params': module_params,\n        'execution_time': execution_times\n    })\n\n    summary_df.to_csv(os.path.join(node_line_dir, \"summary.csv\"), index=False)\n\n    filtered_results = filter_by_threshold(result_dfs, strategies)\n    selected_result = select_best_average(filtered_results)\n\n    selected_result.to_parquet(os.path.join(node_line_dir, \"selected_result.parquet\"), index=False)\n\n    return selected_result"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    fused_ids = []\n    fused_scores = []\n    for i in range(len(ids)):\n        ids_list, scores_list = cc_pure(ids[i], scores[i], weights, top_k)\n        fused_ids.append(ids_list)\n        fused_scores.append(scores_list)\n    return fused_ids, fused_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "\n    weighted_sums = [sum(w * s for w, s in zip(weights, score)) for score in zip(*scores)]\n    normalized_sums = [(x - min(weighted_sums)) / (max(weighted_sums) - min(weighted_sums)) for x in weighted_sums]\n    top_indices = sorted(range(len(normalized_sums)), key=lambda i: normalized_sums[i], reverse=True)[:top_k]\n    top_ids = [ids[i] for i in top_indices]\n    top_scores = [normalized_sums[i] for i in top_indices]\n\n    return top_ids, top_scores"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        data = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"purpose_embedding\": agent.purpose_embedding.tolist() if agent.purpose_embedding is not None else None,\n            \"usage_count\": agent.usage_count,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n        return data"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        prompt = f\"{PROMPT_ENGINEERING_SYSTEM_PROMPT} {goal}\\n\\n{PROMPT_ENGINEERING_TEMPLATE} {sample_input}\\n\\n{EXAMPLES}\"\n        try:\n            completion = self.openai_wrapper.get_chat_completion(prompt)\n            return completion\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\")\n        if purpose_embedding is not None and isinstance(purpose_embedding, list):\n            purpose_embedding = np.array(purpose_embedding)  # Convert list back to ndarray\n\n        return MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\"),\n            purpose=data.get(\"purpose\"),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\"),\n            max_depth=data.get(\"max_depth\"),\n            usage_count=data.get(\"usage_count\"),\n            id=data.get(\"id\"),\n            parent_id=data.get(\"parent_id\"),\n            working_agent=data.get(\"working_agent\"),\n            is_prime=data.get(\"is_prime\"),\n            evolve_count=data.get(\"evolve_count\"),\n            number_of_code_executions=data.get(\"number_of_code_executions\"),\n            last_input=data.get(\"last_input\"),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper\n        )"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT id FROM agents WHERE id = ?\", (agent_dict['id'],))\n            existing_record = cursor.fetchone()\n            if existing_record:\n                conn.execute(\"UPDATE agents SET purpose = ?, data = ? WHERE id = ?\", (agent_dict['purpose'], json.dumps(agent_dict), agent_dict['id']))\n            else:\n                conn.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            result = cursor.fetchone()\n            if result:\n                return json.loads(result[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        hash_object = hashlib.sha256(json.dumps(data, sort_keys=True).encode())\n        return hash_object.hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash=?\", (arg_hash,))\n        result = cursor.fetchone()\n        if result:\n            return json.loads(result[0])\n        else:\n            return None"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "        def decorator(func):\n            @functools.wraps(func)\n            def wrapper(*args, **kwargs):\n                with SQLiteMemoization(filename) as memoization:\n                    return memoization.fetch_or_compute(func, func_name, *args, **kwargs)\n            return wrapper\n        return decorator"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        result_json = json.dumps(result)\n        self.connection.execute(\"INSERT INTO cache (hash, result) VALUES (?, ?)\", (arg_hash, result_json))"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    for key, value in vars(args).items():\n        if value is not None:\n            CONFIG[key.upper()] = value\n\n    # Redirect standard output to a file if quiet mode is enabled\n    if quiet_mode:\n        with open('output.log', 'w') as f:\n            with redirect_stdout(f):\n                start_command_line(vars(args))\n    else:\n        start_command_line(vars(args))"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "            model_name = get_model_name(\n                kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n            )\n            logger.debug(\"chatcompletion: using \" + model_name)\n            chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n            if \"azure_endpoint\" in chatcompletion_kwargs:\n                api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n                chatcompletion_kwargs.update({\"api_base\": api_base})\n            chatcompletion_kwargs.update(kwargs)\n\n            try:\n                response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n                response = json.loads(str(response))\n                if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                    raise BadRequestError(\"maximum context length exceeded\", None)\n            except BadRequestError as e:\n                if \"maximum context length\" in e._message:\n                    if model_name == \"gpt-4\":\n                        if \"gpt-4-32k\" in CONFIG.api_keys:\n                            model_name = \"gpt-4-32k\"\n                        elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                            model_name = \"gpt-4-1106-preview\"\n                        else:\n                            model_name = \"gpt-3.5-turbo-16k\"\n                    elif model_name == \"gpt-3.5-turbo\":\n                        if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                            model_name = \"gpt-3.5-turbo-1106\"\n                        else:\n                            model_name = \"gpt-3.5-turbo-16k\"\n                    else:\n                        raise e\n                    print(\"max context length reached, retrying with \" + model_name)\n                    chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n                    chatcompletion_kwargs.update(kwargs)\n                    chatcompletion_kwargs.pop(\"schema_error_retry\", None)\n\n                    response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n                    response = json.loads(str(response))\n                else:\n                    raise e\n\n            return response"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        current_time = time()\n        if self._client is None or self._last_time is None or current_time - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = current_time\n        return self._client"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        state_dict = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"shuffle\": self.shuffle,\n            \"world_size\": self.distributed_env.world_size\n        }\n        return state_dict"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "\n        # Resize the rendering context to match the camera's width and height\n        eglctx.resize(camera.W, camera.H)\n\n        # Perform offscreen rendering of the Mesh instance using the specified camera configuration\n        self.render(camera)"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    return NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        n_embd=bert_config.hidden_size,\n        n_layer=bert_config.num_hidden_layers,\n        n_head=bert_config.num_attention_heads,\n        n_inner=bert_config.intermediate_size,\n        activation_function=bert_config.hidden_act,\n        resid_pdrop=bert_config.hidden_dropout_prob,\n        attn_pdrop=bert_config.attention_probs_dropout_prob,\n        n_positions=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        # Additional attributes specific to NomicBertConfig\n        last_layer_subset=False,  # Example value, replace with actual default\n        pad_vocab_size_multiple=1,  # Example value, replace with actual default\n    )"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        \"\"\"\n        The function renders a mesh object based on its visibility, render type, and associated OpenGL programs. It handles different rendering types such as points, lines, triangles, quads, and triangle strips by setting up the appropriate OpenGL state and issuing the correct draw call.\n\n        Input-Output Arguments\n        :param self: Mesh. An instance of the Mesh class, which contains information about the mesh to be rendered, including visibility, render type, vertex array object (VAO), element buffer object (EBO), vertices, faces, and OpenGL programs.\n        :param camera: Camera. The camera object used to set up the view and projection matrices in the shader programs. It is used in the upload_gl_uniforms method to pass these matrices to the GPU.\n        :return: No return values. This function directly interacts with the GPU to render the mesh on the screen.\n\n        The function first checks if the mesh is visible. If it is not, the function returns immediately. Depending on the render type of the mesh, it selects the appropriate shader program (for point rendering or general mesh rendering) and binds it. It then uploads necessary uniforms to the GPU, binds the vertex array object of the mesh, and issues the appropriate OpenGL draw call based on the render type. For lines, triangles, quads, and triangle strips, it handles both indexed and non-indexed drawing. Finally, it unbinds the vertex array object to clean up.\n        \"\"\"\n        if not self.visible:\n            return\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glUseProgram(self.point_program)\n            self.upload_gl_uniforms(camera)\n            gl.glBindVertexArray(self.vao)\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n            gl.glBindVertexArray(0)\n        else:\n            gl.glUseProgram(self.mesh_program)\n            self.upload_gl_uniforms(camera)\n            gl.glBindVertexArray(self.vao)\n\n            if self.faces is not None:\n                gl.glDrawElements(self.render_type.value, len(self.faces), gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(self.render_type.value, 0, len(self.verts))\n\n            gl.glBindVertexArray(0)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        # Check if the input is a PyTorch tensor and convert it to a numpy array\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        # Set the default width and height if not provided\n        w = w or self.W\n        h = h or self.H\n\n        # Bind the texture to be updated\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Upload the pixel data to the texture\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_FLOAT, ptr)"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "\n    # Validate input shapes\n    assert R.shape[-2:] == (3, 3), \"Invalid shape for R\"\n    assert tvec.shape[-2:] == (3, 1), \"Invalid shape for tvec\"\n    assert camera_matrix.shape[-2:] == (3, 3), \"Invalid shape for camera_matrix\"\n    assert image_size.shape[-1] == 2, \"Invalid shape for image_size\"\n\n    # Compute camera position\n    C = -torch.matmul(torch.transpose(R, -1, -2), tvec)\n\n    # Compute camera rotation\n    R_pulsar = matrix_to_rotation_6d(R)\n\n    # Compute intrinsic parameters\n    fx = camera_matrix[..., 0, 0]\n    fy = camera_matrix[..., 1, 1]\n    cx = camera_matrix[..., 0, 2]\n    cy = camera_matrix[..., 1, 2]\n    sensor_width = image_size[..., 1]\n\n    # Normalize focal length\n    f = (fx + fy) / 2\n\n    # Adjust principal point offsets\n    cx_pulsar = (cx - image_size[..., 0] / 2) / sensor_width\n    cy_pulsar = (cy - image_size[..., 1] / 2) / sensor_width\n\n    # Compute camera parameters\n    camera_params = torch.stack([C, R_pulsar, f, cx_pulsar, cy_pulsar], dim=-1)\n\n    return camera_params"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n        else:\n            if w == 0:\n                w = self.W\n            if h == 0:\n                h = self.H\n            gl.glViewport(x, y, w, h)\n            gl.glScissor(x, y, w, h)\n            gl.glUseProgram(self.quad_program)\n            gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n            gl.glBindVertexArray(self.vao)\n            gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n            gl.glBindVertexArray(0)\n            gl.glBindTexture(gl.GL_TEXTURE_2D, 0)\n            gl.glUseProgram(0)\n            gl.glViewport(0, 0, self.W, self.H)\n            gl.glScissor(0, 0, self.W, self.H)"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "\n    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    ixt = get_ndc_perspective_matrix(K, H, W, n[..., 0], f[..., 0]).to(xyz.dtype)  # to opengl, remove last dim of n and f\n    w2c = affine_padding(torch.cat([R, T], dim=-1)).to(xyz.dtype)\n    c2w = affine_inverse(w2c)\n    c2w[..., 0] *= 1  # flip x\n    c2w[..., 1] *= -1  # flip y\n    c2w[..., 2] *= -1  # flip z\n    ext = affine_inverse(c2w)\n    pix_xyz = torch.cat([xyz, torch.ones_like(xyz[..., :1])], dim=-1) @ ext.mT @ ixt.mT\n    pix_rad = abs(H * ixt[..., 1, 1][..., None, None] * rad / pix_xyz[..., -1:])  # z: B, 1 * B, N, world space radius -> ndc radius B, N, 1\n\n    # Prepare data to be rendered\n    data = torch.cat([pix_xyz, rgb, pix_rad], dim=-1).ravel()  # organize the data inside vbo\n    return H, W, K, R, T, C"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        # Move content from write_fbo to screen fbo\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.write_fbo)\n        gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, 0)  # render the final content onto screen\n        gl.glReadBuffer(gl.GL_COLOR_ATTACHMENT0)\n        gl.glBlitFramebuffer(x, y, w, h, 0, 0, self.W, self.H, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, 0)"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    t0, t1 = t0.unsqueeze(-1), t1.unsqueeze(-1)\n    t_diff = t0 - t1\n    t_diff = torch.clamp(t_diff, min=0)\n    y1 = y1.unsqueeze(-1)\n    w_inner = torch.cumsum(y1 * t_diff, dim=-2)\n    w_outer = torch.cumsum(y1 * (1 - t_diff), dim=-2)\n    return w_inner, w_outer"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "\n    t, w = matchup_channels(t, w)\n    t_env, w_env = matchup_channels(t_env, w_env)\n\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], eps)\n\n    t_, w_ = blur_stepfun(t, w_normalize, pulse_width)\n    w_ = torch.clip(w_, min=0.)\n    assert (w_ >= 0.0).all()\n\n    # piecewise linear pdf to piecewise quadratic cdf\n    area = 0.5 * (w_[..., 1:] + w_[..., :-1]) * (t_[..., 1:] - t_[..., :-1])\n\n    cdf = torch.cat([torch.zeros_like(area[..., :1]), torch.cumsum(area, dim=-1)], dim=-1)\n\n    # query piecewise quadratic interpolation\n    cdf_interp = sorted_interp_quad(t_env, t_, w_, cdf)\n    # difference between adjacent interpolated values\n    w_s = torch.diff(cdf_interp, dim=-1)\n\n    return ((w_s - w_env).clip(0.).pow(2) / (w_env + eps)).mean()"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "\n    # Calculate the inter-interval loss\n    t_inner, t_outer = inner_outer(t[..., :-1], t[..., 1:], w)\n    inter_interval_loss = interval_distortion(t_inner, t_outer, t[..., :-1], t[..., 1:])\n\n    # Calculate the intra-interval loss\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], torch.finfo(torch.float32).eps)\n    t_, w_ = blur_stepfun(t, w_normalize, pulse_width=1.0)\n    w_ = torch.clip(w_, min=0.)\n    intra_interval_loss = lossfun_zip_outer(t, w, t_, w_, pulse_width=1.0)\n\n    # Combine inter-interval and intra-interval losses to get the total distortion loss\n    total_distortion_loss = inter_interval_loss + intra_interval_loss\n\n    return total_distortion_loss"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "\n    cw = integrate_weights(w)  # Compute the cumulative sum of w\n    percentiles = torch.tensor(ps, dtype=t.dtype, device=t.device)  # Convert percentile values to tensor\n\n    # Interpolate into the inverse CDF to find the weighted percentiles\n    return interpolate(percentiles, cw, t)"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, side='left')\n    idx_hi = torch.searchsorted(a, v, side='right')\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n    # Compute the PDF from the weights\n    p = weight_to_pdf(t, w)\n    \n    # Generate uniform random numbers for sampling\n    u = torch.rand(num_samples, device=t.device, dtype=t.dtype)\n    \n    # Invert the CDF to get the samples\n    samples = invert_cdf(u, t, p)\n    \n    # Apply perturbation to avoid sample clustering at bin boundaries\n    if perturb:\n        if single_jitter:\n            jitter = torch.rand(1, device=t.device, dtype=t.dtype) * (t[1] - t[0])\n            samples += jitter\n        else:\n            jitter = torch.rand(num_samples, device=t.device, dtype=t.dtype) * (t[1] - t[0])\n            samples += jitter\n    \n    return samples"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.tensor[:, 2] * self.tensor[:, 3]"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "\n        scores, proposal_deltas = predictions\n        proposal_boxes = [proposal.proposal_boxes.tensor for proposal in proposals]\n        gt_boxes = [proposal.gt_boxes.tensor for proposal in proposals]\n        gt_classes = [proposal.gt_classes for proposal in proposals]\n\n        # Calculate classification loss\n        pred_class_logits = scores\n        loss_cls = cross_entropy(pred_class_logits, cat(gt_classes, dim=0), reduction=\"mean\")\n\n        # Calculate box regression loss\n        loss_box_reg = self.box_reg_loss(cat(proposal_boxes, dim=0), cat(gt_boxes, dim=0), proposal_deltas, cat(gt_classes, dim=0))\n\n        # Scale the losses by their respective weights\n        loss_cls *= self.loss_weight[\"loss_cls\"]\n        loss_box_reg *= self.loss_weight[\"loss_box_reg\"]\n\n        return {\"loss_cls\": loss_cls, \"loss_box_reg\": loss_box_reg}"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.MODEL.TRACKER.NAME\n    return TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # Ensure the output is a valid box. See Sec 2.1 of https://arxiv.org/abs/2006.09214\n        deltas = F.relu(deltas)\n        boxes = boxes.to(deltas.dtype)\n\n        ctr_x = 0.5 * (boxes[:, 0] + boxes[:, 2])\n        ctr_y = 0.5 * (boxes[:, 1] + boxes[:, 3])\n\n        dx = deltas[:, 0::4]\n        dy = deltas[:, 1::4]\n        dw = deltas[:, 2::4]\n        dh = deltas[:, 3::4]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = ctr_x[:, None] - dx  # x1\n        pred_boxes[:, 1::4] = ctr_y[:, None] - dy  # y1\n        pred_boxes[:, 2::4] = ctr_x[:, None] + dw  # x2\n        pred_boxes[:, 3::4] = ctr_y[:, None] + dh  # y2\n        return pred_boxes"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        img, k = resize_image(image, 512)\n        img = np.transpose(img, (2, 0, 1))\n        img = np.expand_dims(img, axis=0)\n        img = img.astype(np.float32)\n        img = (img - 127.5) / 127.5\n        with torch.no_grad():\n            output = self.general_ins({'img': img})\n        if anno_type is None:\n            return output\n        elif isinstance(anno_type, str):\n            return output[anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            return {anno: output[anno] for anno in anno_type if anno in output}\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        result = {}\n        keywords = normalize_string(query).split(\" \")\n        for keyword in keywords:\n            scores = self.bm25(keyword)\n            result = update_url_scores(result, scores)\n        return result"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        height, width = box_size\n        self.normalize_angles()\n        nearly_horizontal = torch.abs(self.tensor[:, 4]) < clip_angle_threshold\n        nearly_horizontal_boxes = self.tensor[nearly_horizontal]\n\n        if len(nearly_horizontal_boxes) > 0:\n            x_c, y_c, w, h, a = nearly_horizontal_boxes.unbind(dim=1)\n            x1 = x_c - w / 2\n            y1 = y_c - h / 2\n            x2 = x_c + w / 2\n            y2 = y_c + h / 2\n\n            x1 = torch.clamp(x1, 0, width)\n            y1 = torch.clamp(y1, 0, height)\n            x2 = torch.clamp(x2, 0, width)\n            y2 = torch.clamp(y2, 0, height)\n\n            w_clipped = x2 - x1\n            h_clipped = y2 - y1\n            self.tensor[nearly_horizontal] = torch.stack([x1 + w_clipped / 2, y1 + h_clipped / 2, w_clipped, h_clipped, a], dim=1)"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n        for item in self.data:\n            if 'type' in item:\n                statistics[item['type']] += 1\n        return statistics"}
{"namespace": "common.bleu4_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['bleu']\n    if with_penalty:\n        brevity_penalty = min(1, len(continuation) / len(reference))\n        score *= brevity_penalty\n    return score"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['rougeL']\n    return score"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Running command: {cmd}\")\n    return os.system(cmd)"}
