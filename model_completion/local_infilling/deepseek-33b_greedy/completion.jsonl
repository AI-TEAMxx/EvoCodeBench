{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Executing command: {cmd}\")\n    return os.system(cmd)"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "coord.inv_contract", "completion": "  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (1 + jnp.sqrt(1 + 4 * z_mag_sq)) / (2 * z_mag_sq)\n  x = scale * z\n  return x"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as memo:\n                return memo.fetch_or_compute(func, func_name, *args, **kwargs)\n        return wrapper\n    return decorator"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"] or values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: invalid bounding box. \"\n            f\"x_min={values['x_min']}, x_max={values['x_max']}, y_min={values['y_min']}, y_max={values['y_max']}\"\n        )\n\n    return values"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  sq_norm0 = np.sum(mat0**2, 0, keepdims=True)\n  sq_norm1 = np.sum(mat1**2, 0, keepdims=True)\n  inner_product = np.matmul(mat0.T, mat1)\n  sq_dist = sq_norm0 - 2 * inner_product + sq_norm1.T\n  sq_dist[sq_dist < 0] = 0\n  return sq_dist"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None:\n        return True\n    if path == \"\":\n        return True\n    if path.startswith(\"s3://\"):\n        return True\n    if path.startswith(\"gs://\"):\n        return True\n    if path.startswith(\"hdfs://\"):\n        return True\n    if path.startswith(\"http://\"):\n        return True\n    if path.startswith(\"https://\"):\n        return True\n    return False"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"{name} is a dictionary, but no assets names were provided.\"\n            )\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"{name} is a dictionary, but the number of assets names does not match the number of assets.\"\n            )\n        if dim == 1:\n            arr = np.full(n_assets, fill_value)\n            for i, name in enumerate(assets_names):\n                if name in items:\n                    arr[i] = items[name]\n            return arr\n        if dim == 2:\n            arr = np.full((n_assets, n_assets), fill_value)\n            for i, name in enumerate(assets_names):\n                if name in items:\n                    arr[i, :] = items[name]\n            return arr\n        raise ValueError(f\"{name} is a dictionary, but dim must be 1 or 2.\")\n    if isinstance(items, np.ndarray):\n        if dim == 1:\n            if items.ndim == 1:\n                if items.shape[0] != n_assets:\n                    raise ValueError(\n                        f\"{name} is a 1D array, but the number of assets does not match the number of assets.\"\n                    )\n                return items\n            if items.ndim == 2:\n                if items.shape[0] != n_assets:\n                    raise ValueError(\n                        f\"{name} is a 2D array, but the number of assets does not match the number of assets.\"\n                    )\n                return items[0, :]\n            raise ValueError(f\"{name} is a 2D array, but dim must be 1.\")\n        if dim == 2:\n            if items.ndim == 2:\n                if items.shape[0] != n_assets or items.shape[1] != n_assets:\n                    raise ValueError(\n                        f\"{name} is a 2D array, but the number of assets does not match the number of"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\", None)\n        if purpose_embedding is not None and isinstance(purpose_embedding, list):\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        return MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\", \"\"),\n            purpose=data.get(\"purpose\", \"\"),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\", 0),\n            max_depth=data.get(\"max_depth\", 0),\n            usage_count=data.get(\"usage_count\", 0),\n            id=data.get(\"id\", None),\n            parent_id=data.get(\"parent_id\", None),\n            working_agent=data.get(\"working_agent\", False),\n            is_prime=data.get(\"is_prime\", False),\n            evolve_count=data.get(\"evolve_count\", 0),\n            number_of_code_executions=data.get(\"number_of_code_executions\", 0),\n            last_input=data.get(\"last_input\", None),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  srgb = xnp.array(srgb)\n  linear0 = srgb / 12.92\n  linear1 = xnp.power((srgb + 0.055) / 1.055, 2.4)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Adjust the spline degree to be at most one less than the number of points in x.\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Create a spline object using the input data and the specified degree and smoothness.\n  tck = scipy.interpolate.splrep(t_input, x, k=spline_degree, s=smoothness)\n\n  # Evaluate the spline at the output times to obtain the interpolated values.\n  x_interp = scipy.interpolate.splev(t_output, tck)\n\n  return x_interp\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word.lower()\n    if word.islower():\n        return word.upper()\n    if word[0].isupper() and word[1].isupper():\n        return word.lower()\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    return word"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must contain only boolean values. Received {v.dtype}.\"\n        )\n\n    return v"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n    return summary_df"}
{"namespace": "coord.isotropize", "completion": "  # Compute the determinant of the covariance matrix.\n  det = jnp.linalg.det(cov)\n\n  # Check if the determinant is valid.\n  if jnp.any(det <= 0):\n    raise ValueError('Covariance matrix must have positive determinant.')\n\n  # Compute the logarithm of the determinant.\n  log_det = jnp.log(det)\n\n  # Check if the logarithm of the determinant is valid.\n  if jnp.any(jnp.isnan(log_det)):\n    raise ValueError('Covariance matrix must have positive determinant.')\n\n  # Compute the square root of the determinant.\n  if mode == 'fast':\n    sqrt_det = jnp.sqrt(det)\n  elif mode == 'accurate':\n    sqrt_det = jnp.exp(0.5 * log_det)\n  else:\n    raise ValueError(f'Invalid mode: {mode}')\n\n  # Compute the isotropic covariance matrix.\n  cov_iso = cov / sqrt_det[Ellipsis, None, None]\n\n  # Return the isotropic covariance matrix.\n  return cov_iso"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='XAgent Command Line Interface')\n    parser.add_argument('--task', type=str, required=True, help='Task description')\n    parser.add_argument('--upload-files', type=str, nargs='*', help='List of files to upload')\n    parser.add_argument('--model', type=str, help='Model identifier')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs')\n    parser.add_argument('--mode', type=str, default='auto', choices=['auto', 'manual'], help='Operational mode')\n    parser.add_argument('--quiet', action='store_true', help='Run in quiet mode with minimal output')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Enable asking for human assistance during task execution')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts')\n    parser.add_argument('--config-file', type=str, default=os.environ.get('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file')\n\n    return parser.parse_args()"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. Got shape {v.shape}\")\n\n    return v"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunks_index[worker_idx] = 0\n        for interval in intervals:\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                indexes[worker_idx] -= interval[1] - interval[0]\n                chunks_index[worker_idx] += 1\n            else:\n                break\n    return chunks_index, indexes"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return resample.trilerp(values, coordinates)\n  elif datastructure == 'hash':\n    return hash_resample.trilerp(values, coordinates, datastructure)\n  else:\n    raise ValueError(f'Invalid datastructure: {datastructure}')"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError(f'v {v} must be >= 1')\n\n  # Generate integer weights for each vertex of the triangle.\n  weights = np.array([\n      np.arange(v - i, -1, -1)\n      for i in range(v)\n  ])\n  weights = np.concatenate([weights, np.flip(weights[1:, :], 0)], 0)\n\n  # Normalize the weights to get the barycentric coordinates.\n  weights = weights / np.sum(weights)\n\n  return weights"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the spline values corresponding to each input query.\n  idx0, idx1 = sorted_lookup(tq, t)\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Interpolate the spline values.\n  tq_ex = jnp.broadcast_to(tq, tq.shape + (1,) * (t.ndim - tq.ndim))\n  t0_ex = jnp.broadcast_to(t0, tq_ex.shape)\n  t1_ex = jnp.broadcast_to(t1, tq_ex.shape)\n  v0_ex = jnp.broadcast_to(v0, tq_ex.shape)\n  v1_ex = jnp.broadcast_to(v1, tq_ex.shape)\n  vq = jnp.where(\n      t0 == t1,\n      v0,\n      v0_ex + (tq_ex - t0_ex) * (v1_ex - v0_ex) / (t1_ex - t0_ex),\n  )\n  return vq\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(x > 0 for x in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Shift ray origins to near plane.\n  t = (near / directions[Ellipsis, 2:3])[Ellipsis, None]\n  origins = origins + t * directions\n\n  # Projection\n  ones = xnp.ones(origins[Ellipsis, 0:1].shape)\n  s = origins[Ellipsis, 2:3] / pixtocam[Ellipsis, 2:3]\n  origins = origins * s\n  directions = directions * s\n\n  # Transform ray directions to be relative to the camera optical axis.\n  x, y, z = xnp.split(directions, 3, axis=-1)\n  xcam, ycam, zcam = xnp.split(pixtocam, 3, axis=-1)\n  directions = xcam * x + ycam * y + zcam * z\n  return origins, directions\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.isclose(jnp.dot(dir1, dir2), 1.0, atol=1e-6)\n\n"}
{"namespace": "common.bleu4_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f, max_order=4)\n    score = results['bleu']\n    if with_penalty:\n        score = score * min(1, len(continuation) / len(reference))\n    return score\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.sqrt(safe_x)"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  return w / jnp.diff(t)"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for f in filenames:\n            try:\n                fp = os.path.join(dirpath, f)\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    limited_val = val - torch.floor(val / period + offset) * period\n    return limited_val\n\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        data = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"usage_count\": agent.usage_count,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n\n        if agent.purpose_embedding is not None:\n            data[\"purpose_embedding\"] = agent.purpose_embedding.tolist()\n\n        return data\n\n    "}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of items and weights are equal\n    assert len(items) == len(weights)\n\n    # Check that all weights are positive\n    assert all(w > 0 for w in weights)\n\n    # Check that the number of bins is positive\n    assert num_bins > 0\n\n    # Create a dictionary to store the items in each bin\n    bins = defaultdict(list)\n\n    # Create a dictionary to store the total weight of each bin\n    bin_weights = defaultdict(int)\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Iterate over the sorted items\n    for item, weight in sorted_items:\n\n        # Find the bin with the current lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Add the item to the bin with the current lowest total weight\n        bins[min_bin].append(item)\n\n        # Update the total weight of the bin\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        data_bytes = json.dumps(data).encode(\"utf-8\")\n        return hashlib.sha256(data_bytes).hexdigest()"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the pairwise distances between all points in the polygon\n    distances = np.linalg.norm(polygon[:-1] - polygon[1:], axis=1)\n\n    # Filter out distances that exceed the maximum point distance\n    filtered_distances = distances[distances <= max_point_distance]\n\n    # Compute the total length of the polygon\n    total_length = np.sum(filtered_distances)\n\n    return total_length"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if not polygons:\n        return polygons\n\n    areas = [area(polygon) for polygon in polygons]\n    max_area = max(areas)\n\n    filtered_polygons = [\n        polygon\n        for polygon, polygon_area in zip(polygons, areas)\n        if polygon_area > max(rel_tr * max_area, abs_tr)\n    ]\n\n    return filtered_polygons"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_per_worker_batch = num_samples_per_worker // batch_size\n    num_samples_per_worker_batch_remainder = num_samples_per_worker % batch_size\n\n    num_samples_per_worker_batch_remainder_per_worker = num_samples_per_worker_batch_remainder // num_workers\n    num_samples_per_worker_batch_remainder_remainder = num_samples_per_worker_batch_remainder % num_workers\n\n    num_samples_per_worker_batch_per_worker = num_samples_per_worker_batch + num_samples_per_worker_batch_remainder_per_worker\n\n    num_samples_per_worker_batch_remainder_per_worker_remainder = num_samples_per_worker_batch_remainder_remainder\n\n    num_samples_per_worker_per_worker = [\n        num_samples_per_worker_batch_per_worker + num_samples_per_worker_batch_remainder_per_worker_remainder\n    ]\n\n    for _ in range(num_workers - 1):\n        num_samples_per_worker_per_worker.append(num_samples_per_worker_batch_per_worker)\n\n    indexes = {}\n\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] = num_samples_per_worker_per_worker[worker_idx]\n\n    return indexes"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n    assert len(results) == len(value) == len(metadatas), \"results, value, and metadatas must have the same length.\"\n    assert all([isinstance(v, (int, float)) for v in value]), \"value must be numeric.\"\n    assert all([isinstance(result, pd.DataFrame) for result in results]), \"results must be pd.DataFrame.\"\n    assert all([isinstance(metadata, (int, float, str, list, tuple, dict, type(None))) for metadata in metadatas]), \\\n        \"metadatas must be int, float, str, list, tuple, dict, or None.\"\n    return [result for result, v in zip(results, value) if v <= threshold], \\\n           [metadata for metadata, v in zip(metadatas, value) if v <= threshold]\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"The input array must have shape (_, 2), where _ can be any number of points.\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    # Shoelace formula\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx = torch.searchsorted(a, v, right=True)\n    idx_lo = torch.max(torch.zeros_like(idx - 1), idx - 1)\n    idx_hi = torch.min(idx, torch.full_like(idx, a.shape[-1] - 1))\n    return idx_lo, idx_hi"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ])"}
{"namespace": "coord.contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = 2 * jnp.sqrt(x_mag_sq) - x_mag_sq\n  z = x / scale\n  return z"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n        if num_bytes < 1000.0:\n            return f\"{num_bytes:3.1f} {unit}\"\n        num_bytes /= 1000.0\n    return f\"{num_bytes:.1f} PB\"\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __array_n_dimensions(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions.\")\n        return v\n\n    return __array_n_dimensions"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = jnp.linalg.norm(cartesian_vector, axis=-1)\n  theta = jnp.arccos(z / (r + eps))\n  phi = jnp.arctan2(y, x)\n  return r, theta, phi\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[reference], tokenizer=f)\n    return results['rougeL']"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return pydoc.locate(name)\n    except ImportError:\n        # Fallback to a manual scan.\n        obj = pydoc.locate(name, forceload=False)\n        if obj is None:\n            raise ImportError(f\"Cannot locate object: {name}\")\n        return obj"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    return cc_pure(ids, scores, weights, top_k)\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if x == 0:\n        return \"0%\"\n    if x < 1:\n        return f\"{x:.2f}%\"\n    if x < 10:\n        return f\"{x:.1f}%\"\n    return f\"{x:.0f}%\""}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        disk_usage = shutil.disk_usage(input_dir)\n        free_space_in_gb = disk_usage.free / (1024 ** 3)\n        if free_space_in_gb < threshold_in_gb:\n            sleep(sleep_time)\n        else:\n            break"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, p * td)"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\" \", \"\")\n    line_text = line_text.replace(\"\\t\", \"\")\n    line_text = line_text.replace(\"\\n\", \"\")\n    line_text = line_text.replace(\"\\r\", \"\")\n    line_text = line_text.replace(\"\\f\", \"\")\n    line_text = line_text.replace(\"\\v\", \"\")\n    line_text = line_text.replace(\"\\u200b\", \"\")\n    line_text = line_text.replace(\"\\u200c\", \"\")\n    line_text = line_text.replace(\"\\u200d\", \"\")\n    line_text = line_text.replace(\"\\u200e\", \"\")\n    line_text = line_text.replace(\"\\u200f\", \"\")\n    line_text = line_text.replace(\"\\u202a\", \"\")\n    line_text = line_text.replace(\"\\u202b\", \"\")\n    line_text = line_text.replace(\"\\u202c\", \"\")\n    line_text = line_text.replace(\"\\u202d\", \"\")\n    line_text = line_text.replace(\"\\u202e\", \"\")\n    line_text = line_text.replace(\"\\u2060\", \"\")\n    line_text = line_text.replace(\"\\u2061\", \"\")\n    line_text = line_text.replace(\"\\u2062\", \"\")\n    line_text = line_text.replace(\"\\u2063\", \"\")\n    line_text = line_text.replace(\"\\u2064\", \"\")\n    line_text = line_text.replace(\"\\u2066\", \"\")\n    line_text = line_text.replace(\"\\u2067\", \"\")\n    line_text = line_text.replace(\"\\u2068\", \"\")\n    line_text = line_text.replace(\"\\u2069\", \"\")\n    line_text = line_text.replace(\"\\u2028\", \"\")\n    line_text"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zeros must be less than or equal to the number of weights.\")\n    weights = np.random.dirichlet(np.ones(n - zeros))\n    if zeros > 0:\n        weights = np.insert(weights, np.random.choice(n, zeros, replace=False), 0)\n    return weights"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    crop_y = max(0, int(bbox[1] - crop_size[0] / 2))\n    crop_x = max(0, int(bbox[0] - crop_size[1] / 2))\n    crop_y2 = min(image_size[0], crop_y + crop_size[0])\n    crop_x2 = min(image_size[1], crop_x + crop_size[1])\n    crop_y = crop_y2 - crop_size[0]\n    crop_x = crop_x2 - crop_size[1]\n    return T.CropTransform(crop_x, crop_y, crop_x2, crop_y2)"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Clamp the squared norm to prevent division by zero in the forward pass.\n  clamped_x = jnp.maximum(jnp.finfo(jnp.float32).eps, jnp.sum(x * x, axis=-1, keepdims=True))\n  # Clamp the squared norm to prevent division by zero in the backward pass.\n  clamped_x_grad = jnp.maximum(grad_eps, jnp.sum(x * x, axis=-1, keepdims=True))\n  return x / jnp.sqrt(clamped_x)\n\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name, input_text = agent_info.split(\":\")\n        return agent_name, input_text"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        if mask_format == \"polygon\":\n            masks = [PolygonMasks([rle]) for rle in annos[\"segmentation\"]]\n        elif mask_format == \"bitmask\":\n            masks = []\n            for poly in annos[\"segmentation\"]:\n                rles = mask_util.frPyObjects([poly], *image_size)\n                rle = mask_util.merge(rles)\n                masks.append(rle)\n        target.gt_masks = BitMasks(masks)\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = Keypoints(kpts)\n\n    return target"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\n            \"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\")\n        )\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    std = np.sqrt(np.diag(cov))\n    corr = cov / std / std[:, None]\n    return corr, std"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # TODO: find a more automatic way to enable import of other classes\n    MODULES = [\n        \"detectron2.modeling.backbone\",\n        \"detectron2.modeling.proposal_generator\",\n        \"detectron2.modeling.roi_heads\",\n        \"detectron2.modeling.meta_arch\",\n    ]\n    with ExitStack() as stack:\n        ctxs = [stack.enter_context(mock.patch(mod + \".training\")) for mod in MODULES]\n        for m in ctxs:\n            m.side_effect = lambda: True\n        yield\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if the shape of field1 equals the shape of field2.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1].shape} and {values[field2].shape}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list):\n        if all(isinstance(metric, str) for metric in metrics):\n            return metrics, [{} for _ in metrics]\n        elif all(isinstance(metric, dict) for metric in metrics):\n            return [metric['name'] for metric in metrics], [deepcopy(metric) for metric in metrics]\n        else:\n            raise ValueError('Invalid metrics format.')\n    else:\n        raise ValueError('Invalid metrics format.')"}
{"namespace": "coord.construct_ray_warps", "completion": "  def t_to_s(t):\n    \"\"\"Maps metric distances to normalized distances in the range [0, 1].\"\"\"\n    # Clamp t to the valid range [t_near, t_far].\n    t = jnp.clip(t, t_near, t_far)\n    # Compute the normalized distance using the function fn.\n    s = fn(t)\n    # Normalize the normalized distance to the range [0, 1].\n    s = (s - fn(t_near)) / (fn(t_far) - fn(t_near))\n    return s\n\n  def s_to_t(s):\n    \"\"\"Maps normalized distances in the range [0, 1] to metric distances.\"\"\"\n    # Denormalize the normalized distance to the range [fn(t_near), fn(t_far)].\n    t = s * (fn(t_far) - fn(t_near)) + fn(t_near)\n    # Invert the function fn to get the metric distance.\n    if fn_inv is None:\n      # If fn_inv is not provided, try to automatically determine its inverse.\n      if fn == contract:\n        t = inv_contract(t)\n      else:\n        raise ValueError(\n            f'fn_inv must be provided for fn={fn} to be inverted.'\n        )\n    else:\n      t = fn_inv(t)\n    # Clamp the metric distance to the valid range [t_near, t_far].\n    t = jnp.clip(t, t_near, t_far)\n    return t\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  check_zero_endpoints(w)\n\n  # Compute the integral of the linear spline using the trapezoid rule.\n  integral = jnp.trapz(w, t, axis=-1)\n\n  return integral"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    id_df = pd.DataFrame({f'id_{i}': id_list for i, id_list in enumerate(ids)})\n    score_df = pd.DataFrame({f'score_{i}': score_list for i, score_list in enumerate(scores)})\n    df = pd.concat([id_df, score_df], axis=1)\n\n    def cc_pure_apply(row):\n        ids_tuple = tuple(row[[f'id_{i}' for i in range(len(ids))]].values)\n        scores_tuple = tuple(row[[f'score_{i}' for i in range(len(scores))]].values)\n        return pd.Series(cc_pure(ids_tuple, scores_tuple, weights, top_k))\n\n    df[['cc_id', 'cc_score']] = df.apply(cc_pure_apply, axis=1)\n    return df['cc_id'].tolist(), df['cc_score'].tolist()"}
{"namespace": "coord.track_linearize", "completion": "  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(jax.jacfwd(fn), in_axes=-1, out_axes=-1)(mean)\n\n  # Compute the mean and covariance of the transformed Gaussian.\n  fn_mean = fn(mean)\n  fn_cov = jnp.matmul(jac, jnp.matmul(cov, jnp.swapaxes(jac, -1, -2)))\n\n  return fn_mean, fn_cov"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for xi in x:\n        if xi.size > 1:\n            yield [xi[: xi.size // 2], xi[xi.size // 2 :]]\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  sin_x = jnp.sin(scaled_x)\n  if append_identity:\n    return jnp.concatenate([sin_x, x[Ellipsis, None]], axis=-1)\n  else:\n    return sin_x"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Check if len(field1) equals len(field2) and if each pair of arrays have the same shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for arr1, arr2 in zip(values[field1], values[field2]):\n            if arr1.shape != arr2.shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n\n        return values\n\n    return __root_validator"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize(camera.W, camera.H)\n\n        # Render the mesh using the camera's settings\n        self.render(camera)\n"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object with the same attributes as the BertConfig\n    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        # Additional attributes specific to the Nomic model\n        orig_vocab_size=bert_config.vocab_size,\n        activation_function=bert_config.hidden_act,\n        n_inner=bert_config.intermediate_size,\n        n_embd=bert_config.hidden_size,\n        n_layer=bert_config.num_hidden_layers,\n        n_head=bert_config.num_attention_heads,\n        n_positions=bert_config.max_position_embeddings,\n        resid_pdrop=bert_config.hidden_dropout_prob,\n        attn_pdrop=bert_config.attention_probs_dropout_prob,\n        embd_pdrop=bert_config.hidden_dropout_prob,\n        layer_norm_epsilon=bert"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            self.use_gl_program(self.mesh_program)\n\n        self.upload_gl_uniforms(camera)\n\n        gl.glBindVertexArray(self.vao)\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.TRIS:\n            if self.faces.dtype == torch.int32:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of faces\n            else:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_SHORT, ctypes.c_void_p(0))  # number of faces\n        elif self.render_type == Mesh.RenderType.QUADS:\n            if self.faces.dtype == torch.int32:\n                gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of faces\n            else:\n                gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_SHORT, ctypes.c_void_p(0)) "}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        w = w or self.W\n        h = h or self.H\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n        ptr = np.asarray(ptr, dtype=np.uint8, order='C')\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Check if the input tensors are batched\n    assert R.ndim >= 2 and R.shape[-2:] == (3, 3), \"R must be a batch of rotation matrices of shape (*, 3, 3)\"\n    assert tvec.ndim >= 2 and tvec.shape[-1] == 3, \"tvec must be a batch of translation vectors of shape (*, 3)\"\n    assert camera_matrix.ndim >= 2 and camera_matrix.shape[-2:] == (3, 3), \"camera_matrix must be a batch of camera intrinsic matrices of shape (*, 3, 3)\"\n    assert image_size.ndim >= 1 and image_size.shape[-1] == 2, \"image_size must be a batch of image sizes of shape (*, 2)\"\n\n    # Extract focal lengths and principal points from camera_matrix\n    fx = camera_matrix[..., 0, 0]\n    fy = camera_matrix[..., 1, 1]\n    cx = camera_matrix[..., 0, 2]\n    cy = camera_matrix[..., 1, 2]\n\n    # Compute the average focal length and warn if it differs significantly from fx and fy\n    f = (fx + fy) * 0.5\n    if (abs(fx - fy) / f) > 0.01:\n        warn_once_about_pulsar_fxfy()\n\n    # Compute the principal point offsets\n    px = (cx - image_size[..., 0] * 0.5) / f\n    py = (image_size[..., 1] * 0.5 - cy) / f\n\n    # Compute the camera position\n    cam_pos = -R.transpose(-2, -1) @ tvec\n\n    # Compute the camera rotation\n    cam_rot = matrix_to_rotation_6d(R)\n\n    # Compute the camera parameters\n    cam_params = torch.cat([cam_pos, cam_rot, f, px, py, znear * torch.ones_like("}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        gl.glUseProgram(self.quad_program)\n\n        # Prepare to render to textures\n        gl.glBindVertexArray(self.vao)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glBindVertexArray(old_vao)\n        gl.glUseProgram(old_prog)\n\n        # Restore the original state\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Sort the source times and values based on the source times.\n    t1, y1 = t1.sort()\n\n    # Compute the cumulative sum of the values.\n    y1_cumsum = torch.cumsum(y1, dim=-1)\n\n    # Find the indices of the target times in the sorted source times.\n    t0_idx = searchsorted(t1, t0)\n\n    # Compute the inner measure using the cumulative sum of the values.\n    y0_inner = y1_cumsum.gather(-1, t0_idx)\n\n    # Compute the outer measure by subtracting the cumulative sum of the values at the previous index.\n    y0_outer = y1_cumsum.gather(-1, t0_idx - 1)\n\n    return y0_inner, y0_outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # The loss incurred between all pairs of intervals.\n    ut = (t[..., 1:] + t[..., :-1]) / 2  # 64\n    dut = torch.abs(ut[..., :, None] - ut[..., None, :])  # 64\n    loss_inter = torch.sum(w * torch.sum(w[..., None, :] * dut, dim=-1), dim=-1)\n\n    # The loss incurred within each individual interval with itself.\n    loss_intra = torch.sum(w**2 * (t[..., 1:] - t[..., :-1]), dim=-1) / 3\n\n    # The loss incurred between the intervals and the environment.\n    w_env_outer = w_env[..., 1:] - w_env[..., :-1]  # 127\n    w_env_outer = torch.cat([w_env_outer, torch.zeros_like(w_env_outer[..., :1])], dim=-1)  # 128\n    w_env_outer = torch.cat([torch.zeros_like(w_env_outer[..., :1]), w_env_outer], dim=-1)  # 129\n    w_env_inner, w_env_outer = inner_outer(t, t_env, w_env_outer)\n    loss_env = torch.sum(w * (w_env_inner + w_env_outer), dim=-1)\n\n    # The loss incurred within each individual interval with itself.\n    loss_intra = torch.sum(w**2 * (t[..., 1:] - t[..., :-1]), dim=-1) / 3\n\n    return loss_inter + loss_intra + loss_env"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # Compute the inner and outer measures on (t, w) for t_env.\n    w_inner, w_outer = inner_outer(t, t, w)\n\n    # Compute the distortion loss for the inner and outer measures.\n    loss_inner = (w_inner - w).clip(0.).pow(2)\n    loss_outer = (w_outer - w).clip(0.).pow(2)\n\n    # Combine the inner and outer losses to produce the total distortion loss.\n    loss = loss_inner + loss_outer\n\n    return loss.mean()"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n\n    # Compute the CDF.\n    cw = integrate_weights(w)\n\n    # Interpolate into the inverse CDF.\n    s = interpolate(torch.tensor(ps, device=t.device), cw, t)\n\n    return s\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n    # Draw uniform samples.\n    u = torch.linspace(0., 1., num_samples, device=t.device)\n    # Match the batch shape of u to that of t.\n    # u = torch.zeros_like(t[..., 0, None]) + u\n    u = u.expand(t.shape[:-1] + (num_samples,))\n    # Draw samples from the inverse CDF.\n    t_new = invert_cdf(u, t, w)\n    # Perturb the samples.\n    if perturb:\n        # Interpolate into the PDF.\n        t0 = torch.max(t[..., :-1], t_new - 1e-5)\n        t1 = torch.min(t[..., 1:], t_new + 1e-5)\n        u_lo = (t_new - t0) / (t1 - t0)\n        u_hi = (t1 - t_new) / (t1 - t0)\n        cdf_lo = torch.cumsum(w * u_lo, dim=-1)\n        cdf_lo = torch.cat([torch.zeros_like(cdf_lo[..., :1]), cdf_lo], dim=-1)\n        cdf_hi = torch.cumsum(w * u_hi, dim=-1)\n        cdf_hi = torch.cat([torch.zeros_like(cdf_hi[..., :1]), cdf_hi], dim=-1)\n        cdf_hat = cdf_lo + torch.gather(cdf_hi, dim=-1, index=idx_hi)\n        # Draw uniform samples.\n        u = torch.linspace(0., 1., num_samples, device=t.device)\n        u = u + (u - u.floor()) / num_samples\n        # Invert the CDF.\n        t_new = invert_cdf(u,"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute the dilation factor for each interval.\n    dilation_factor = torch.clamp(t[..., 1:] - t[..., :-1], min=dilation)\n\n    # Compute the dilated time steps.\n    t_dilate = t[..., :-1] + dilation_factor\n\n    # Clip the dilated time steps to the specified domain.\n    t_dilate = torch.clip(t_dilate, domain[0], domain[1])\n\n    # Compute the adjusted weights for the dilated time steps.\n    w_dilate = w * dilation_factor\n\n    return t_dilate, w_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # tq: [..., num_query_times]\n    # t: [..., num_steps]\n    # y: [..., num_steps]\n\n    # Find the indices of the step changes that occur before each query time.\n    indices = torch.searchsorted(t, tq, side='right') - 1\n\n    # Clamp the indices to the valid range of step changes.\n    indices = torch.clamp(indices, 0, t.shape[-1] - 1)\n\n    # Compute the values of the step function at the query times.\n    yq = torch.where(\n        (tq[..., None] == t[..., indices]) & (indices < t.shape[-1] - 1),\n        y[..., indices],\n        outside_value,\n    )\n\n    return yq\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    t_lo = t[..., :-1]\n    t_hi = t[..., 1:]\n    t_mid = (t_lo + t_hi) / 2\n    t_width = t_hi - t_lo\n    t_mid_ = t_mid + (2 - 2 * train_frac) * t_width * anneal_slope\n    t_mid_ = torch.where(t_width > 0, t_mid_, t_mid)\n    w_ = torch.where(t_width > 0, w, 0)\n    w_ = torch.where(t_mid_ > t_lo, w_, 0)\n    w_ = torch.where(t_mid_ < t_hi, w_, 0)\n    w_ = w_ / (torch.sum(w_, dim=-1, keepdim=True) + eps)\n    return t, w_"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)) and not ignore_list:\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # index will first be augmented to match the values' dimentionality at the back\n    # take care of batch dimension of, and acts like a linear indexing in the target dimention\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    return v.gather(dim, multi_indexing(f, v.shape, dim))"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None]\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = batch[None]\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        batch.meta = dotdict()\n        batch.meta.H, batch.meta.W, batch.meta.K, batch.meta.R, batch.meta.T, batch.meta.n, batch.meta.f, batch.meta.t, batch.meta.v, batch.meta.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        batch.meta.mass, batch.meta.moment_of_inertia, batch.meta.movement_force, batch.meta.movement_torque, batch.meta.movement_speed, batch.meta.origin, batch.meta.world_up = self.mass, self.moment_of_inertia, self.movement_force, self.movement_torque, self.movement_speed, self.origin, self.world_up\n        return batch\n\n    "}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            max_similarity = -float('inf')\n            closest_agent = None\n\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n\n                if similarity > max_similarity:\n                    max_similarity = similarity\n                    closest_agent = agent\n\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.prime = True\n        prime_agent.usage_count = 1\n        self.agents.append(prime_agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads an agent with a specified purpose from the database. If an agent with the given purpose is found, it is deserialized and returned; otherwise, None is returned.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str, The purpose of the agent to be loaded. It is used to identify the agent in the database.\n        :param agent_lifecycle: The lifecycle state of the agent. It is passed to the deserializer to properly initialize the agent.\n        :param openai_wrapper: An instance or interface used for interacting with OpenAI services. It is passed to the deserializer for initializing the agent with OpenAI functionalities.\n        :return: An instance of the deserialized agent if found, otherwise None.\n        \"\"\"\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        for purpose in self.persistence.fetch_all_agents():\n            agent = self.load_agent(purpose, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = self.openai_wrapper.get_chat_completion(\n                PROMPT_ENGINEERING_SYSTEM_PROMPT,\n                PROMPT_ENGINEERING_TEMPLATE,\n                EXAMPLES,\n                goal,\n                sample_input\n            )\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"SELECT result FROM cache WHERE hash = ?\",\n            (arg_hash,)\n        )\n        result = cursor.fetchone()\n        if result is None:\n            return None\n        return json.loads(result[0])"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_config(args)\n\n    # If quiet mode is enabled, redirect the standard output to a file instead of displaying it in the terminal\n    if quiet_mode:\n        with open(os.devnull, 'w') as devnull:\n            with redirect_stdout(devnull):\n                start_command_line(vars(args))\n    else:\n        start_command_line(vars(args))"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (self._last_time is not None and time() - self._last_time > self._refetch_interval):\n            self._create_client()\n            self._last_time = time()\n        return self._client\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\n                \"The `state_dict` method should not be called from a DataLoader worker process. \"\n                \"It should be called from the main process or from a worker process that is not part of a DataLoader.\"\n            )\n\n        if self.item_loader is not None:\n            item_loader_state = self.item_loader.state_dict()\n        else:\n            item_loader_state = None\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": item_loader_state,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n    "}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            return\n\n        state = self._state_dict\n\n        if self.shuffle != state[\"shuffle\"]:\n            raise ValueError(\n                f\"The state dict shuffle ({state['shuffle']}) doesn't match the current shuffle ({self.shuffle}).\"\n            )\n\n        if self.input_dir.path != state[\"input_dir_path\"]:\n            raise ValueError(\n                f\"The state dict input_dir_path ({state['input_dir_path']}) doesn't match the current input_dir_path ({self.input_dir.path}).\"\n            )\n\n        if self.input_dir.url != state[\"input_dir_url\"]:\n            raise ValueError(\n                f\"The state dict input_dir_url ({state['input_dir_url']}) doesn't match the current input_dir_url ({self.input_dir.url}).\"\n            )\n\n        if self.seed != state[\"seed\"]:\n            raise ValueError(f\"The state dict seed ({state['seed']}) doesn't match the current seed ({self.seed}).\")\n\n        if self.distributed_env.world_size != state[\"world_size\"]:\n            raise ValueError(\n                f\"The state dict world_size ({state['world_size']}) doesn't match the current world_size ({self.distributed_env.world_size}).\"\n            )\n\n        if self.drop_last != state[\"drop_last\"]:\n            raise ValueError(\n                f\"The state dict drop_last ({state['drop_last']}) doesn't match the current drop_last ({self.drop_last}).\"\n            )\n\n        if self.item_loader is not None and self.item_loader.state_dict() != state[\"item_loader\"]:\n            raise ValueError(\n                f\"The state dict item_loader ({state['item_loader']}) doesn't match the current item_loader ({self.item_loader.state_dict()}).\""}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Generate a unique directory name based on the input directory\n    unique_dir_name = hashlib.sha256(input_dir.encode()).hexdigest()\n\n    # Check if the environment variables are set\n    if os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\") is not None:\n        cache_dir = os.path.join(os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"), unique_dir_name)\n    else:\n        cache_dir = os.path.join(_DEFAULT_CACHE_DIR, unique_dir_name)\n\n    # Create the cache directory if it doesn't exist\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n    except Exception as e:\n        logger.error(f\"Failed to create cache directory: {e}\")\n        return None"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        parsed_remote_filepath = parse.urlparse(remote_filepath)\n        if parsed_remote_filepath.scheme != \"s3\":\n            raise ValueError(f\"The provided remote_path is not an S3 URL: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        lock_filepath = f\"{local_filepath}.lock\"\n        with FileLock(lock_filepath, timeout=10):\n            if self._s5cmd_available:\n                subprocess.run(\n                    f\"s5cmd --no-sign-request cp {remote_filepath} {local_filepath}\",\n                    shell=True,\n                    check=True,\n                )\n            else:\n                self._client.download_file(parsed_remote_filepath.netloc, parsed_remote_filepath.path, local_filepath)"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Initialize dictionaries to store the assigned chunks and intervals for each worker\n    workers_chunks = {i: [] for i in range(num_workers)}\n    workers_intervals = {i: [] for i in range(num_workers)}\n\n    # Determine the distribution strategy based on the worker's index and the total world size\n    if worker_env.rank == 0:\n        # If the worker is the first one, distribute chunks and intervals evenly across all workers\n        for i, (chunk, interval) in enumerate(zip(chunks_replica, intervals_replica)):\n            worker_idx = i % num_workers\n            workers_chunks[worker_idx].append(chunk)\n            workers_intervals[worker_idx].append(interval)\n    else:\n        # If the worker is not the first one, distribute chunks and intervals based on the worker's index\n        for i, (chunk, interval) in enumerate(zip(chunks_replica, intervals_replica)):\n            worker_idx = (i + worker_env.rank) % num_workers\n            workers_chunks[worker_idx].append(chunk)\n            workers_intervals[worker_idx].append(interval)\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        mode = item.mode\n        size = item.width, item.height\n        raw = item.tobytes()\n        data = [np.uint32(len(mode)).tobytes()]\n        data.append(mode.encode(\"utf-8\"))\n        data.append(np.uint32(size[0]).tobytes())\n        data.append(np.uint32(size[1]).tobytes())\n        data.append(raw)\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            if isinstance(item, JpegImageFile):\n                if item.filename and os.path.isfile(item.filename):\n                    with open(item.filename, \"rb\") as f:\n                        return f.read(), None\n                else:\n                    return item.tobytes(), None\n            elif isinstance(item, Image.Image):\n                with io.BytesIO() as f:\n                    item.save(f, format=\"JPEG\")\n                    return f.getvalue(), None\n        raise TypeError(f\"Unsupported image type: {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[0:12], np.uint32)\n        width, height, mode_size = ints[0], ints[1], ints[2]\n        mode = data[12:12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size:]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the tensor bytes\n        tensor = torch.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return torch.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        if _PIL_AVAILABLE:\n            return Image.open(io.BytesIO(data))\n\n        raise ModuleNotFoundError(\"PIL is required. Run `pip install Pillow`\")"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.from_numpy(np.frombuffer(data, dtype=self._dtype))"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        array = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if array.shape == shape:\n            return array\n        return np.reshape(array, shape)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(order=\"C\"), f\"no_header_numpy:{self._dtype_to_indices[self._dtype]}\""}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n    "}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise ImportError(\"The av library is required to deserialize video data. Please install it with `pip install av`.\")\n        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\n                \"The torchvision library is required to deserialize video data. Please install it with `pip install torchvision`.\"\n            )\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(data)\n            f.flush()\n            return read_video(f.name)"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        # Write the last chunk\n        if self._serialized_items:\n            self.write_chunk(on_done=True)\n\n        # Write the index\n        self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        # Return the list of file paths to the written chunks\n        return [os.path.join(self._cache_dir, chunk[\"filename\"]) for chunk in self._chunks_info]"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        self.restore = True\n\n        if isinstance(self.dataset, StreamingDataset):\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n        self._worker_idx_iter = iter(self._worker_idx)\n        for _ in range(self._latest_worker_idx):\n            next(self._worker_idx_iter)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            return _state_dict(self._datasets, num_samples_yielded, num_workers, batch_size)\n\n        if self._use_streaming_dataloader:\n            return self._iterator.state_dict(num_workers, batch_size)\n\n        return self._iterator.state_dict()"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if self._iterator is not None:\n            self._iterator.load_state_dict(state_dict)\n        else:\n            self._num_samples_yielded = num_samples_yielded\n\n        for dataset_idx, dataset in enumerate(self._datasets):\n            dataset.load_state_dict(state_dict[str(dataset_idx)])"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"s3-connections://\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"datasets://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"~\"):\n        return Dir(path=os.path.expanduser(dir_path))\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.abspath(dir_path))\n\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.abspath(dir_path))\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.abspath(dir_path))\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.abspath(dir_path))\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.abspath(dir_path))\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.abspath(dir_path))\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.abspath(dir_path))\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.abspath(dir_path))"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    # No files are found in this folder\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    if append:\n        raise NotImplementedError(\"Appending to an existing dataset is not supported yet.\")\n\n    if overwrite:\n        raise NotImplementedError(\"Overwriting an existing dataset is not supported yet.\")\n\n    raise RuntimeError(\n        f\"The provided output_dir `{output_dir.path}` already contains data.\"\n        \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n    )"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # We aren't alloweing to add more data\n    # TODO: Add support for `append` and `overwrite`.\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    # Check if the index file exists\n    try:\n        s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/index.json\")\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"404\":\n            # The index file does not exist, so we can proceed\n            return\n        else:\n            # An error occurred while checking for the index file\n            raise e\n\n    # The index file exists, so we need to delete all objects within the specified prefix\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # Delete all objects within the specified prefix\n    for obj in objects[\"Contents\"]:\n       "}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = get_worker_rank()\n\n        if node_rank is not None and node_rank != 0:\n            while not os.path.exists(os.path.join(self._cache_dir, _INDEX_FILENAME)):\n                sleep(1)\n            return\n\n        while len(os.listdir(self._cache_dir)) < num_workers:\n            sleep(1)\n\n        self._merge_no_wait(node_rank)\n\n    "}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\"The Lightning SDK is required to execute this operator remotely.\")\n\n    if not _BOTO3_AVAILABLE:\n        raise RuntimeError(\"The boto3 library is required to execute this operator remotely.\")\n\n    if machine is None:\n        machine = Machine(\n            name=\"default\",\n            accelerators=\"auto\",\n            cpu_count=num_nodes,\n            disk_size=100,\n            image=\"lightning/pytorch:latest\",\n            instance_type=\"m5.4xlarge\",\n            is_spot=False,\n            region=\"us-east-1\",\n            request_spot_instances=False,\n            shutdown_timeout=60,\n            wait_timeout=600,\n        )\n\n    if command is None:\n        command = \" \".join(\n            [\n                \"python\",\n                \"-m\",\n                \"lightning_datasets\",\n                \"--name\",\n                name,\n                \"--working_dir\",\n                os.getcwd(),\n                \"--env\",\n                \" \".join([f\"{k}={v}\" for k, v in os.environ.items()]),\n            ]\n        )\n\n    # TODO: Add support for `append` and `overwrite`.\n    job = Studio(\n        name=name,\n        machine=machine,\n        command=command,\n        cloud_compute=\"default\",\n        wait_timeout=600,\n        request_spot_instances=False,\n        is_spot=False,\n        region=\"us-east-1\",\n        cloud_compute_id=os.getenv(\"LIGHTNING_CLOUD_COMPUTE_ID\", None),\n    ).run()\n\n    print(f\"Job URL: {_get_lightning_cloud_url()}/{job.id}\")\n\n    while True:\n        job.reload()\n        if job.status == \"succeeded\":\n            break\n        if job.status =="}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config is None:\n            try:\n                self._config = ChunksConfig.from_cache_dir(\n                    self._cache_dir,\n                    self._serializers,\n                    self._remote_input_dir,\n                    self._compression,\n                    self._item_loader,\n                )\n            except FileNotFoundError:\n                return None\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The config should be defined before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The index should be an instance of ChunkedIndex. Got {type(index)}.\")\n\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Load the config containing the index\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Check whether the chunk is available locally\n        chunk_filepath, _, _ = self._config[index]\n        if not os.path.exists(chunk_filepath):\n            # Download the chunk if it's not available locally\n            self._prepare_thread.download([chunk_index])\n\n        # Load the chunk\n        item = self._item_loader.load_chunk(chunk_index, chunk_filepath, index.index)\n\n        # Prefetch the next chunk\n        if self._prepare_thread is not None:\n            self._prepare_thread.download([chunk_index + 1])\n\n        # Delete the chunk if it's fully consumed\n        if self._prepare_thread is not None:\n            if self._last_chunk_index is not None and self._last_chunk_index != chunk_index:\n                self._prepare_thread.delete([self._last_chunk_index])\n            self._last_chunk_index = chunk_index\n\n        return item\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Flatten the chunks_per_ranks list\n    chunks_per_ranks = [chunk for sublist in chunks_per_ranks for chunk in sublist]\n\n    # 2. Shuffle the chunks_per_ranks list\n    np.random.seed(seed + current_epoch)\n    np.random.shuffle(chunks_per_ranks)\n\n    # 3. Split the shuffled chunks_per_ranks list into chunks_per_ranks_per_node lists\n    chunks_per_ranks_per_node = [\n        chunks_per_ranks[i : i + distributed_env.world_size]\n        for i in range(0, len(chunks_per_ranks), distributed_env.world_size)\n    ]\n\n    # 4. Flatten the chunks_per_ranks_per_node lists\n    chunks_per_ranks_per_node = [\n        chunk for sublist in chunks_per_ranks_per_node for chunk in sublist\n    ]\n\n    return chunks_per_ranks_per_node"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if len(inputs) == 0:\n        return None\n\n    if len(inputs) == 1:\n        if isinstance(inputs[0], str):\n            return os.path.dirname(inputs[0])\n\n        if isinstance(inputs[0], Path):\n            return str(inputs[0].parent)\n\n        if isinstance(inputs[0], Sequence):\n            return _get_input_dir(inputs[0])\n\n        if isinstance(inputs[0], dict):\n            return _get_input_dir(list(inputs[0].values()))\n\n        if isinstance(inputs[0], (tuple, list)):\n            return _get_input_dir(inputs[0])\n\n        if isinstance(inputs[0], dict):\n            return _get_input_dir(list(inputs[0].values()))\n\n        if isinstance(inputs[0], (tuple, list)):\n            return _get_input_dir(inputs[0])\n\n        if isinstance(inputs[0], dict):\n            return _get_input_dir(list(inputs[0].values()))\n\n        if isinstance(inputs[0], (tuple, list)):\n            return _get_input_dir(inputs[0])\n\n        if isinstance(inputs[0], dict):\n            return _get_input_dir(list(inputs[0].values()))\n\n        if isinstance(inputs[0], (tuple, list)):\n            return _get_input_dir(inputs[0])\n\n        if isinstance(inputs[0], dict):\n            return _get_input_dir(list(inputs[0].values()))\n\n        if isinstance(inputs[0], (tuple, list)):\n            return _get_input_dir(inputs[0])\n\n        if isinstance(inputs[0], dict):\n            return _get_input_dir(list(inputs[0].values()))\n\n        if isinstance(inputs[0], (tuple, list)):"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        optimize_dns(enable)\n        yield\n    finally:\n        optimize_dns(False)"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    if drop_last:\n        num_items_per_rank = num_items_per_rank * distributed_env.world_size\n    else:\n        num_items_per_rank = num_items_per_rank + len(indexes) % distributed_env.world_size\n\n    # assign chunks and their intervals to each rank\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    for rank in range(distributed_env.world_size):\n        start = rank * num_items_per_rank\n        end = start + num_items_per_rank\n        chunks_per_ranks[rank] = indexes[start:end]\n        intervals_per_ranks[rank] = chunk_intervals[start:end]\n\n    return chunks_per_ranks, intervals_per_ranks"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_device:\n            if self._device is None:\n                self._find_device()\n            kwargs[\"device\"] = self._device\n\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default_num_workers(),\n            fast_dev_run=fast_dev_run,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_files=reorder_files,\n            weights=weights"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if error_when_not_empty:\n            _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    obj = parse.urlparse(input_dir.url if input_dir.url else input_dir.path)\n\n    if obj.scheme == \"s3\":\n        s3 = S3Client()\n\n    while True:\n        # 1. Collect paths\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if index is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(cache_dir) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if not os.path.exists(path):\n                    if obj.scheme == \"s3\":\n                        obj = parse.urlparse(path)\n                        _wait_for_file_to_exist(s3, obj)\n                        with open(path, \"wb\") as f:\n                            s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n                    else:\n                        raise ValueError(f\"The provided {input_dir.path} isn't supported.\")\n\n            elif not os.path.exists(path):\n                raise ValueError(f\"The provided {path} doesn't exist.\")\n\n        # 4. Signal that the files are ready\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Collect paths\n        data = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if data is None:\n            return\n\n        # 3. Upload the data\n        if isinstance(data, str):\n            local_path = data\n            if not local_path.startswith(cache_dir):\n                local_path = os.path.join(cache_dir, local_path)\n\n            if not os.path.exists(local_path):\n                raise ValueError(f\"The file {local_path} doesn't exist.\")\n\n            if output_dir.url:\n                obj = parse.urlparse(output_dir.url)\n                s3.client.upload_file(local_path, obj.netloc, os.path.join(obj.path.lstrip(\"/\"), os.path.basename(local_path)))\n            elif output_dir.path:\n                shutil.copyfile(local_path, os.path.join(output_dir.path, os.path.basename(local_path)))\n\n        elif isinstance(data, tuple):\n            temp_dir, local_path = data\n            if not local_path.startswith(cache_dir):\n                local_path = os.path.join(cache_dir, local_path)\n\n            if not os.path.exists(local_path):\n                raise ValueError(f\"The file {local_path} doesn't exist.\")\n\n            if output_dir.url:\n                obj = parse.urlparse(output_dir.url)\n                s3.client.upload_file(local_path, obj.netloc, os.path.join(obj.path.lstrip(\"/\"), os.path.basename(local_path)))\n            elif output_dir.path:\n                shutil.copyfile(local_path, os.path.join(output_dir.path, os.path.basename(local_path)))\n\n            shutil.rmtree(temp_dir)\n\n        # 4."}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if num_workers <= 0:\n        raise ValueError(\"num_workers must be greater than 0\")\n\n    if len(user_items) != len(weights):\n        raise ValueError(\"The number of items and weights must be the same\")\n\n    if num_nodes <= 0:\n        raise ValueError(\"num_nodes must be greater than 0\")\n\n    if node_rank < 0 or node_rank >= num_nodes:\n        raise ValueError(\"node_rank must be between 0 and num_nodes-1\")\n\n    total_workers = num_nodes * num_workers\n\n    if total_workers > len(user_items):\n        raise ValueError(\"The number of workers is greater than the number of items\")\n\n    # Calculate the number of items per worker\n    items_per_worker = len(user_items) // total_workers\n    extra_items = len(user_items) % total_workers\n\n    # Calculate the start and end indices for each worker\n    start = 0\n    result = []\n    for i in range(total_workers):\n        worker_items = items_per_worker + 1 if i < extra_items else items_per_worker\n        end = start + worker_items\n        result.append(user_items[start:end])\n        start = end\n\n    # Print the distribution details for workers on the current node\n    for i, worker_items in enumerate(result):\n        if i // num_workers == node_rank:\n            worker_weights = [weights[j] for j in range(start, end)]\n            total_weight = sum(worker_weights)\n            if file_size:\n                total_weight = total_weight / 1024 / 1024\n            print(f\"Worker {i} has {len(worker_items)} items with total weight {total_weight}\")\n\n    # Shuffle the items for each worker\n    for i in range(len(result)):\n        random.shuffle(result[i])\n\n    return result\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Calculate the total number of workers across all nodes\n    num_nodes = _get_num_nodes()\n    total_workers = num_workers * num_nodes\n\n    # Calculate the number of items each worker should process\n    items_per_worker = len(user_items) // total_workers\n    remainder = len(user_items) % total_workers\n\n    # Adjust for any remainder by adding extra items to the workers starting from the end of the list\n    for i in range(remainder):\n        user_items[i * (items_per_worker + 1) : (i + 1) * (items_per_worker + 1)] = [\n            user_items[i * (items_per_worker + 1) : (i + 1) * (items_per_worker + 1)]\n        ]\n\n    # Use cumulative sum to efficiently calculate the start and end indices for each worker's items\n    cumulative_sum = np.cumsum([0] + [items_per_worker + 1] * remainder + [items_per_worker] * (total_workers - remainder))\n    worker_items = [user_items[start:end] for start, end in zip(cumulative_sum[:-1], cumulative_sum[1:])]\n\n    # Ensure the output list has a length equal to the number of workers; otherwise, raise a RuntimeError\n    if len(worker_items) != total_workers:\n        raise RuntimeError(\"Improper assignment of items to workers.\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "    def _cleanup_cache(self) -> None:\n        cache_dir = _get_cache_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is not None and element.startswith(input_dir):\n        return True\n\n    if _IS_IN_STUDIO and element.startswith(\"/teamspace\"):\n        return True\n\n    if os.path.exists(element):\n        return True\n\n    return False\n\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            import tinycudann as tcnn\n\n            if n_neurons < 16:\n                network_type = tcnn.NetworkWithInputEncoding\n            elif n_neurons < 64:\n                network_type = tcnn.NetworkWithInputSkips\n            else:\n                network_type = tcnn.Network\n\n            return network_type(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                encoding_config={\n                    \"otype\": \"HashGrid\",\n                    \"n_levels\": 16,\n                    \"n_features_per_level\": 2,\n                    \"log2_hashmap_size\": 15,\n                    \"base_resolution\": 16,\n                    \"per_level_scale\": 1.3819,\n                },\n                network_config={\n                    \"otype\": \"FullyFusedMLP\",\n                    \"activation\": activation,\n                    \"output_activation\": output_activation,\n                    \"n_neurons\": n_neurons,\n                    \"n_hidden_layers\": n_layers - 1,\n                },\n            )\n\n        model_list = []\n        for i in range(n_layers - 1):\n            model_list.extend(\n                self._get_torch_layer(\n                    in_features=n_input_dims,\n                    out_features=n_neurons,\n                    activation_name=activation,\n                )\n            )\n            n_input_dims = n_neurons\n        model_list.extend(\n            self._get_torch_layer(\n                in_features=n_input_dims,\n                out_features=n_output_dims,\n                activation_name=output_activation,\n            )\n        )\n        return nn.Sequential(*model_list)\n\n    "}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        shifted_signals = [np.roll(signal, i) for i in range(-kernel_offset, kernel_offset + 1)]\n        median_array = np.median(shifted_signals, axis=0)\n        return median_array[kernel_offset:-kernel_offset]"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if template_probe.code.shape != template_gallery.code.shape:\n        raise MatcherError(\"IrisTemplate codes must have the same shape\")\n\n    if template_probe.mask.shape != template_gallery.mask.shape:\n        raise MatcherError(\"IrisTemplate masks must have the same shape\")\n\n    if template_probe.code.dtype != template_gallery.code.dtype:\n        raise MatcherError(\"IrisTemplate codes must have the same dtype\")\n\n    if template_probe.mask.dtype != template_gallery.mask.dtype:\n        raise MatcherError(\"IrisTemplate masks must have the same dtype\")\n\n    if template_probe.code.dtype != bool:\n        raise MatcherError(\"IrisTemplate codes must be of type bool\")\n\n    if template_probe.mask.dtype != bool:\n        raise MatcherError(\"IrisTemplate masks must be of type bool\")\n\n    if rotation_shift < 0:\n        raise MatcherError(\"rotation_shift must be non-negative\")\n\n    if nm_dist is not None and nm_dist < 0:\n        raise MatcherError(\"nm_dist must be non-negative\")\n\n    if weights is not None:\n        if len(weights) != len(template_probe.code):\n            raise MatcherError(\"weights must have the same length as the number of codes in the IrisTemplate\")\n\n        for i in range(len(weights)):\n            if weights[i].shape != template_probe.code[i].shape:\n                raise MatcherError(\"weights must have the same shape as the codes in the IrisTemplate\")\n\n            if weights[i].dtype != bool:\n                raise MatcherError(\"weights must be of type bool\")\n\n    toal_codesize = np.sum([np.sum(x) for x in template_probe.code])\n    half_width = [x.shape[1] // 2 for x in template_probe.code]\n\n    sqrt_totalbitcount, sqrt_totalbitcount_top"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        # Initialize arrays to store the starting and ending points of the perpendicular bisectors\n        first_bisectors_point = np.zeros((num_bisectors, 2))\n        second_bisectors_point = np.zeros((num_bisectors, 2))\n\n        # Initialize a counter to keep track of the number of iterations\n        iteration_counter = 0\n\n        # Loop until we have found enough pairs of points that meet the distance criterion\n        while iteration_counter < max_iterations:\n            # Randomly choose two points from the polygon's vertices\n            first_point_index, second_point_index = np.random.choice(len(polygon), size=2, replace=False)\n\n            # Calculate the distance between the two points\n            distance_between_points = np.linalg.norm(polygon[first_point_index] - polygon[second_point_index])\n\n            # If the distance between the two points is greater than the minimum distance,\n            # add the points to the arrays of starting and ending points for the perpendicular bisectors\n            if distance_between_points > min_distance_between_sector_points_in_px:\n                first_bisectors_point[iteration_counter] = polygon[first_point_index]\n                second_bisectors_point[iteration_counter] = polygon[second_point_index]\n\n                # Increment the iteration counter\n                iteration_counter += 1\n\n                # If we have found enough pairs of points, break out of the loop\n                if iteration_counter == num_bisectors:\n                    break\n\n        # If we have not found enough pairs of points after the maximum number of iterations, raise an error\n        if iteration_counter < num_bisectors:\n            raise EyeCentersEstimationError(\n                f\"Failed to find {num_bisectors} pairs of points with a distance greater than {min_distance_between_sector_points_in_px} in {max_iterations} iterations.\""}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_execute(self, *args, **kwargs)\n\n        return result"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = inspect.getdoc(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n\n        for param_name, param in signature.parameters.items():\n            if param.kind == param.POSITIONAL_OR_KEYWORD:\n                input_type_hints[param_name] = type_hints.get(param_name, None)\n\n        output_type_hint = type_hints.get('return', None)\n\n        if output_type_hint:\n            if inspect.isclass(output_type_hint) or issubclass(output_type_hint, Union):\n                output_class_definition = get_class_definition(output_type_hint)\n                if issubclass(output_class_definition, Embedding):\n                    function_type = FunctionType.EMBEDDABLE\n                else:\n                    function_type = FunctionType.SYMBOLIC\n            else:\n                output_class_definition = get_class_definition(output_type_hint)\n                function_type = FunctionType.SYMBOLIC\n        else:\n            output_class_definition = None\n            function_type = FunctionType.SYMBOLIC\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hints=input_type_hints,\n            output_type_hints=output_type_hints,\n            input_class_definitions={param_name: get_class_definition(type_hint) for param_name, type_hint in\n                                     input_type_hints.items()},\n            output_class_definition=output_class_definition,\n            function_type=function_type,\n            source=get_source(func_object)\n        )\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        expected_length = self.size\n        if len(loaded_bit_array) != expected_length:\n            logging.warning(f\"BloomFilter: Loaded bit array length ({len(loaded_bit_array)}) does not match expected length ({expected_length}). Reinitializing and saving.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.get_model_config(json_dict[DISTILLED_MODEL])\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if \"teacher_models\" in json_dict:\n            self.teacher_models = [config_factory.get_model_config(teacher_model) for teacher_model in json_dict[\"teacher_models\"]]\n        else:\n            self.teacher_models = [config_factory.get_model_config(teacher_model) for teacher_model in DEFAULT_TEACHER_MODEL_NAMES]\n        return self"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Check if the API key is set\n        self.check_api_key()\n\n        # Validate the parameters\n        for parameter in LLM_GENERATION_PARAMETERS:\n            if parameter in kwargs:\n                if not isinstance(kwargs[parameter], (int, float)):\n                    raise ValueError(f\"{parameter} must be a number\")\n\n        # Set default values for parameters\n        temperature = kwargs.get(\"temperature\", 0.7)\n        top_p = kwargs.get(\"top_p\", 1)\n        frequency_penalty = kwargs.get(\"frequency_penalty\", 0)\n        presence_penalty = kwargs.get(\"presence_penalty\", 0)\n        max_new_tokens = kwargs.get(\"max_new_tokens\", 100)\n\n        # Set up the request body\n        request_body = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"frequency_penalty\": frequency_penalty,\n            \"presence_penalty\": presence_penalty,\n            \"max_tokens\": max_new_tokens,\n        }\n\n        # Set up the request headers\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.api_key}\",\n        }\n\n        # Set up the retry parameters\n        max_retries = 5\n        retry_delay = 1\n\n        # Make the request\n        for i in range(max_retries):\n            try:\n                response = requests.post(OPENAI_URL, headers=headers, json=request_body)\n                response.raise_for_status()\n                break\n            except requests.exceptions.RequestException as e:\n                if i == max_retries - 1:\n                    raise e"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must have a zero diagonal\")\n\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the models\n        distilled_model, teacher_models = self.function_modeler.get_models(function_description)\n        # get the alignments\n        aligns = self.function_modeler.get_symbolic_alignments(func_hash, max=5)\n        # get the function description\n        f = str(function_description.__dict__.__repr__() + \"\\n\")\n        # get the token counts\n        examples = [f\"Inputs:\\nArgs: {align['args']}\\nKwargs: {align['kwargs']}\\nOutput: {align['output']}\" for align in\n                 aligns]\n        examples_token_count = sum([approximate_token_count(example) for example in examples])\n        input_prompt_token_count = approximate_token_count(f\"Function: {f}\\n---\\nInputs:\\nArgs: {args}\\nKwargs: {kwargs}\\nOutput:\")\n        generation_tokens = llm_parameters.get(\"max_new_tokens\", self.default_generation_length)\n        # check if finetunable\n        suitable_for_finetune, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs, f, distilled_model)\n        # check if the model is suitable for distillation\n        model = self.choose_model_from_tokens(teacher_models, \n                                              examples_token_count+input_prompt_token_count+generation_tokens,\n                                              len(examples))\n        if model:\n            prompt = self.construct_prompt(f, args, kwargs, examples, model)\n            return prompt, model, suitable_for_finetune, False\n        else:\n            # if the model is not suitable for distillation, use the distilled model\n            prompt = self.construct_prompt(f, args, kwargs, examples, distilled_model)\n            return prompt, distilled_model, False, True\n\n    "}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if not is_positive_definite(cov):\n        if higham:\n            cov = _higham_nearest(cov, max_iteration=higham_max_iteration)\n        else:\n            cov = _clip_eigenvalues(cov)\n    return cov\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, collections.abc.Sequence):\n        return ListSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, (Boxes, ROIMasks)):\n        return TensorWrapSchema.flatten(obj)\n    elif isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    else:\n        raise ValueError(f\"Unsupported type {type(obj)}\")"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n    if groups.ndim != 2:\n        raise EquationToMatrixError(\n            f\"{names[0]} should be a 2D-array, but it is a {groups.ndim}D-array\"\n        )\n    if equations.ndim != 1:\n        raise EquationToMatrixError(\n            f\"{names[1]} should be a 1D-array, but it is a {equations.ndim}D-array\"\n        )\n    n_groups, n_assets = groups.shape\n    n_equations = len(equations)\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n    for i, equation in enumerate(equations):\n        try:\n            left[i, :], right[i] = _string_to_equation(\n                groups=groups, string=equation, sum_to_one=sum_to_one\n            )\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            else:\n                warnings.warn(str(e))\n                return None\n    return left, right"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    # Create a new class that inherits from Instances\n    newInstances = type(\"ScriptedInstances\", (Instances,), {})\n\n    # Add from_instances method to the new class\n    _add_instances_conversion_methods(newInstances)\n\n    # Generate a new module for the new class\n    cls_name, module_str = _gen_instance_module(fields)\n\n    # Write the new module to a temporary file\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\") as f:\n        f.write(module_str)\n        f.flush()\n\n        # Import the new module\n        module = _import(f.name)\n\n        # Replace the original Instances class with the new class\n        sys.modules[__name__].Instances = module.__dict__[cls_name]\n\n        # Set up the environment so that torchscript can recognize and work with the new class\n        _clear_jit_cache()\n        yield\n\n    # Clean up by removing modifications\n    del sys.modules[__name__].Instances\n    _clear_jit_cache()\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        return convert_PIL_to_numpy(image, format)"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(image_size)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform polygon or uncompressed RLE to tight polygon\n    segmentation = annotation.get(\"segmentation\", None)\n    if segmentation is not None:\n        # Pytorch transforms operations are not working for polygons\n        if isinstance(transforms, T.TransformList):\n            transforms = transforms.transforms\n        for t in transforms:\n            if isinstance(t, T.HFlipTransform):\n                if t.width != image_size[1]:\n                    raise ValueError(\n                        \"TransformList with a HFlipTransform in it cannot be used \"\n                        \"if image_size is not the same as the image width used to \"\n                        \"create the HFlipTransform!\"\n                    )\n                segmentation = transform_polygons(\n                    segmentation, t.width, keypoint_hflip_indices, t._flip_y\n                )\n            if isinstance(t, T.CropTransform):\n                segmentation = transform_polygons(\n                    segmentation, t.w, keypoint_hflip_indices, t._flip_y\n                )\n\n    # Transform keypoints\n    keypoints = annotation.get(\"keypoints\", None)\n    if keypoints is not None:\n        keypoints = transform_keypoint_annotations(keypoints, transforms, image_size, keypoint_hflip_indices)\n        annotation[\"keypoints\"] = keypoints\n\n    return annotation\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        # Transform the coordinates to the center of the image\n        coords = coords - self.center\n        # Apply the rotation matrix without the translation\n        coords = np.dot(coords, self.rm_coords[:, :2].T)\n        # Transform the coordinates back to the original frame where\n        # the center was at (0, 0)\n        coords = coords + self.center\n        return coords"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE, **kwargs)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if self._instance_mode == ColorMode.IMAGE_BW:  # change to BW mode for segmentation\n            self.output.reset_image(self._create_grayscale_image(predictions.has(\"pred_masks\")))\n\n        # draw masks\n        if predictions.has(\"pred_masks\"):\n            self.overlay_instances(\n                masks=predictions.pred_masks,\n                labels=None,\n                boxes=predictions.pred_boxes if predictions.has(\"pred_boxes\") else None,\n            )\n\n        # draw boxes\n        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if predictions.has(\"pred_boxes\"):\n            self.overlay_boxes(boxes=boxes, labels=labels)\n\n        if predictions.has(\"pred_keypoints\"):\n            self.overlay_keypoints(keypoints=keypoints)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        canvas = self.canvas\n        s, (width, height) = canvas.print_to_buffer()\n        buffer = np.frombuffer(s, np.uint8)\n\n        img_bgr = buffer.reshape(height, width, 4)\n        img_rgb = img_bgr[:, :, :3][:, :, ::-1]\n        return img_rgb.astype(\"uint8\")\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic.pop(\"sem_seg\"), area_threshold=0.5)\n        if \"instances\" in dic:\n            self.draw_instance_predictions(dic.pop(\"instances\"))\n        if \"panoptic_seg\" in dic:\n            panoptic_seg, segments_info = dic.pop(\"panoptic_seg\")\n            self.draw_panoptic_seg(panoptic_seg, segments_info, area_threshold=0.5)\n        if \"keypoints\" in dic:\n            self.draw_and_connect_keypoints(dic.pop(\"keypoints\"))\n        if \"keypoints_crop\" in dic:\n            self.draw_and_connect_keypoints(dic.pop(\"keypoints_crop\"))\n        if \"proposal_boxes\" in dic:\n            self.overlay_boxes(dic.pop(\"proposal_boxes\"))\n        if \"proposal_objectness\" in dic:\n            self.overlay_scores(dic.pop(\"proposal_objectness\"))\n        if \"proposal_bbox_mode\" in dic:\n            self.set_bbox_mode(dic.pop(\"proposal_bbox_mode\"))\n        if \"pred_boxes\" in dic:\n            self.overlay_boxes(dic.pop(\"pred_boxes\"))\n        if \"scores\" in dic:\n            self.overlay_scores(dic.pop(\"scores\"))\n        if \"pred_classes\" in dic:\n            self.overlay_class_names(dic.pop(\"pred_classes\"))\n        if \"pred_masks\" in dic:\n            self.overlay_masks(dic.pop(\"pred_masks\"))\n        if \"pred_keypoints\" in dic:\n            self.overlay_keypoints(dic.pop(\"pred_keypoints\"))\n        if \"pred_keypoints_crop\" in dic:\n            self.overlay_"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        if edge_color is None:\n            # make edge color darker than the polygon color\n            if alpha > 0.8:\n                edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n            else:\n                edge_color = color\n        edge_color = mplc.to_rgb(edge_color) + (1,)\n\n        if binary_mask.ndim == 2:\n            binary_mask = np.repeat(binary_mask[..., None], 3, axis=-1)\n\n        # draw mask\n        mask_polygon = GenericMask(binary_mask, self.output.height, self.output.width)\n        if mask_polygon.area() < area_threshold:\n            return self.output\n\n        if mask_polygon.has_holes:\n            # draw mask contour\n            for seg in mask_polygon.polygons:\n                self.draw_polygon(seg, color, edge_color=edge_color, alpha=alpha)\n        else:\n            # draw mask\n            self.draw_polygon(mask_polygon.polygons, color, edge_color=edge_color, alpha=alpha)\n\n        if text is not None:\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            self._draw_text_in_mask(binary_mask, text, lighter_color)\n        return self.output"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances) and isinstance(\n        other, Instances\n    ), f\"Expect input and other to be Instances, but got {type(input)} and {type(other)}!\"\n    if size_as_tensor:\n        assert torch.allclose(\n            torch.as_tensor(input.image_size), torch.as_tensor(other.image_size)\n        ), f\"{msg} image_size mismatch: {input.image_size} vs {other.image_size}\"\n    else:\n        assert (\n            input.image_size == other.image_size\n        ), f\"{msg} image_size mismatch: {input.image_size} vs {other.image_size}\"\n\n    for name in input._field_names:\n        val = getattr(input, name, None)\n        other_val = getattr(other, name, None)\n        if val is None:\n            assert other_val is None, f\"{msg} {name} mismatch: {val} vs {other_val}\"\n        elif isinstance(val, Boxes):\n            assert torch.allclose(\n                val.tensor, other_val.tensor, rtol=rtol\n            ), f\"{msg} {name} mismatch: {val} vs {other_val}\"\n        elif isinstance(val, ROIMasks):\n            assert val.tensor.shape == other_val.tensor.shape, f\"{msg} {name} shape mismatch: {val.tensor.shape} vs {other_val.tensor.shape}\"\n            assert torch.allclose(\n                val.tensor, other_val.tensor, rtol=rtol\n            ), f\"{msg} {name} mismatch: {val} vs {other_val}\"\n        elif isinstance(val, torch.Tensor):\n            assert torch.allclose(\n                val, other_val, rtol=rtol\n            ), f\"{msg} {name} mismatch: {val} vs {other_val}\"\n        else:\n            raise ValueError(f\"{msg} {name} is of"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        box = self.tensor\n        area = box[:, 2] * box[:, 3]\n        return area\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # parse classification outputs\n        gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n        _log_classification_stats(scores, gt_classes)\n\n        # parse box regression outputs\n        if len(proposal_deltas) > 0:\n            proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)  # Nx4\n            assert not proposal_boxes.requires_grad, \"Proposals do not need gradients!\"\n            # If \"gt_boxes\" does not exist, the proposals must be all negative and\n            # should not be included in regression loss computation.\n            # Here we just use proposal_boxes as an arbitrary placeholder because the\n            # Fast R-CNN network cannot handle negative ground truth boxes.\n            gt_boxes = cat(\n                [(p.gt_boxes if p.has(\"gt_boxes\") else p.proposal_boxes).tensor for p in proposals],\n                dim=0,\n            )\n        else:\n            proposal_boxes = gt_boxes = torch.empty((0, 4), device=proposal_deltas.device)\n\n        # cls_loss = F.cross_entropy(scores, gt_classes, reduction=\"mean\")\n        # cls_loss = cross_entropy(scores, gt_classes, reduction=\"mean\")\n        cls_loss = F.cross_entropy(scores, gt_classes, reduction=\"none\")\n        cls_loss = cls_loss.mean()\n\n        # box_loss = smooth_l1_loss(\n        #     proposal_boxes, gt_boxes, proposal_deltas, self.smooth_l1_beta, reduction=\"mean\"\n        # )\n        box_loss = self.box_reg_loss(proposal_boxes, gt_boxes, proposal_deltas, gt_classes)\n\n        return {\n            \"loss_cls"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.TRACKER.NAME\n    return TRACKER_HEADS_REGISTRY.get(name)(cfg)"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        if deltas.numel() == 0:\n            return torch.empty_like(deltas)\n\n        boxes = boxes.to(deltas.dtype)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w  # x1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h  # y1\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w  # x2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h  # y2\n\n       "}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            return self.general_ins(image)\n        elif isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        output = self.general_ins(image)\n        if len(anno_type) == 1:\n            return output[anno_type[0]]\n        else:\n            return {tp: output[tp] for tp in anno_type}"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        keywords = normalize_string(query).split(\" \")\n        scores = defaultdict(float)\n        for kw in keywords:\n            scores = update_url_scores(scores, self.bm25(kw))\n        return scores\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n\n    "}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        w, h = box_size\n\n        # Clip boxes to image size\n        x1 = torch.clamp(self.tensor[:, 0] - self.tensor[:, 2] / 2.0, min=0, max=w)\n        y1 = torch.clamp(self.tensor[:, 1] - self.tensor[:, 3] / 2.0, min=0, max=h)\n        x2 = torch.clamp(self.tensor[:, 0] + self.tensor[:, 2] / 2.0, min=0, max=w)\n        y2 = torch.clamp(self.tensor[:, 1] + self.tensor[:, 3] / 2.0, min=0, max=h)\n\n        # Clip boxes to ensure they are nearly horizontal\n        # to avoid the complexities and ambiguities involved in clipping highly rotated boxes\n        # Note: +1 is to make the clipping more stable when the box is nearly horizontal\n        theta = (self.tensor[:, 4] + 180.0) % 180.0 - 90.0\n        abs_theta = torch.abs(theta)\n        near_horizontal_inds = abs_theta < clip_angle_threshold\n        near_horizontal_boxes = self.tensor[near_horizontal_inds]\n        if near_horizontal_boxes.numel() > 0:\n            # Convert the nearly horizontal boxes to horizontal boxes\n            # by setting their widths to the maximum width of the image\n            # and their heights to the sum of the original box heights\n            # This ensures that the clipped boxes have the same aspect ratio as the original boxes\n            # while also being fully contained within the image\n            near_horizontal_boxes[:, 2] = w\n            near_horizontal_boxes[:, 3] = near_horizontal_boxes[:, 3] + near_horizontal_boxes[:, 3]\n\n            # Update the original boxes with the clipped boxes\n            self.tensor[near_horizontal_inds] ="}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n        for item in self.data:\n            statistics[item['type']] += 1\n        return statistics\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(dt_annos) == len(gt_annos)\n    if box_type_3d == 'Depth' or box_type_3d == 'BEV':\n        gt_annos = ind_eval_utils.depth_box3d_to_bev_box3d(gt_annos)\n        dt_annos = ind_eval_utils.depth_box3d_to_bev_box3d(dt_annos)\n    elif box_type_3d != 'Camera' and box_type_3d is not None:\n        raise NotImplementedError(\n            f'Unsupported box_type_3d {box_type_3d} for indoor eval')\n\n    if box_mode_3d != 'lidar' and box_mode_3d is not None:\n        raise NotImplementedError(\n            f'Unsupported box_mode_3d {box_mode_3d} for indoor eval')\n\n    metric = [0.2, 0.5] if metric is None else metric\n    ap_result_str, ar_result_str = '', ''\n    ap_dict = {}\n    for i, thresh in enumerate(metric):\n        recall_dict, precision_dict, ap_dict_ = ind_eval_utils.eval_pre_rec(\n            gt_annos, dt_annos, metric=thresh, box_type_3d=box_type_3d)\n        for label, ap in ap_dict_.items():\n            ap_dict[f'{label}/ap_iou_{thresh:.2f}'] = float(ap)\n        recall = list(recall_dict.values())\n        precision = list(precision_dict.values())\n        recall = [np.nan if np.isnan(i) else i for i in recall]\n        precision = [np.nan if np.isnan(i) else i for i in precision]\n        recall_dict = dict(zip(list(recall_dict.keys()), recall))\n        precision_dict = dict(zip(list(precision_dict.keys"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    from .box_3d_mode import (Box3DMode, CameraInstance3DBoxes, DepthInstance3DBoxes,\n                              LiDARInstance3DBoxes)\n    box_type_lower = box_type.lower()\n    if box_type_lower == 'lidar':\n        box_type_3d = LiDARInstance3DBoxes\n        box_mode_3d = Box3DMode.LIDAR\n    elif box_type_lower == 'camera':\n        box_type_3d = CameraInstance3DBoxes\n        box_mode_3d = Box3DMode.CAM\n    elif box_type_lower == 'depth':\n        box_type_3d = DepthInstance3DBoxes\n        box_mode_3d = Box3DMode.DEPTH\n    else:\n        raise ValueError('Only \"box_type\" of \"camera\", \"lidar\", \"depth\" are supported for visualization!'\n                         f'Got unsupported {box_type}')\n\n    return box_type_3d, box_mode_3d"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine the user code and test code\n        code = user_code + test_code\n\n        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(code)\n            f.flush()\n\n        # Run Pyright on the temporary file\n        result = subprocess.run(\n            [\"pyright\", f.name], capture_output=True, text=True\n        )\n\n        # Delete the temporary file\n        Path(f.name).unlink()\n\n        # Parse the Pyright output to extract error messages and line numbers\n        error_messages = []\n        error_lines = set()\n        for line in result.stdout.splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_no, message = match.groups()\n                error_messages.append(message)\n                error_lines.add(int(line_no))\n\n        # Identify lines with expected type errors\n        expected_error_lines = set()\n        for token in tokenize.generate_tokens(io.StringIO(test_code).readline):\n            if token.type == tokenize.COMMENT and token.string.strip() == cls.EXPECT_ERROR_COMMENT:\n                expected_error_lines.add(token.start[0])\n\n        # Check if the type check passed or failed\n        passed = error_lines == expected_error_lines\n\n        # Create the result object\n        message = \"\\n\".join(error_messages)\n        return TypeCheckResult(message=message, passed=passed)"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, fw_compiler=get_compiler_fn(\"Forward\"),\n                          bw_compiler=get_compiler_fn(\"Backward\"))\n    else:\n        return aot_function(fn, fw_compiler=get_compiler_fn(\"Forward\"),\n                            bw_compiler=get_compiler_fn(\"Backward\"))"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        try:\n            config_dict = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            logger.error(exc)\n            raise exc\n\n    config_dict = summary_df_to_yaml(summary_df, config_dict)\n\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError(\"The output_path must be a yaml file.\")\n        with open(output_path, 'w') as f:\n            yaml.dump(config_dict, f)\n\n    return config_dict"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    if ts_compiler is not None:\n        kwargs = ts_compiler(**kwargs_)\n    else:\n        kwargs = kwargs_\n\n    lock = threading.Lock()\n    cache = {}\n\n    def wrapper(*args, **kwargs):\n        nonlocal cache\n        nonlocal lock\n        with lock:\n            key = (args, tuple(sorted(kwargs.items())))\n            if key not in cache:\n                cache[key] = func(*args, **kwargs)\n            return cache[key]\n\n    return wrapper"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        project_dir = os.path.dirname(trial_path)\n        return cls(config, project_dir=project_dir)\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create node line directory if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Initialize the best result dataframe with the previous result columns\n    best_result_df = previous_result.copy()\n\n    # Initialize the best execution time and best module name\n    best_execution_time = float('inf')\n    best_module_name = None\n\n    # Initialize the summary dataframe\n    summary_df = pd.DataFrame(columns=['filename', 'module_name', 'module_params', 'execution_time'])\n\n    # Iterate over each module and its parameters\n    for module, params in zip(modules, module_params):\n        # Get the module name\n        module_name = module.__name__\n\n        # Create a directory for the module\n        module_dir = os.path.join(node_line_dir, module_name)\n        pathlib.Path(module_dir).mkdir(parents=True, exist_ok=True)\n\n        # Run the module with the given parameters\n        result_df = module(**params)\n\n        # Measure the execution time of the module\n        execution_time = measure_speed(module, params)\n\n        # Evaluate the module's result\n        result_df = evaluate_retrieval_node(result_df, previous_result, strategies['metrics'])\n\n        # Save the result dataframe to disk\n        result_df.to_parquet(os.path.join(module_dir, f\"{module_name}.parquet\"))\n\n        # Add the module's execution time and result to the summary dataframe\n        summary_df = pd.concat([summary_df, pd.DataFrame({\n            'filename': [f\"{module_name}.parquet\"],\n            'module_name': [module_name],\n            'module_params': [params],\n            'execution_time': [execution_time],\n        })], ignore_index=True)\n\n        # Check if the module's execution time is within the specified threshold\n        if filter_by_th"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create the directory for the query expansion node if it doesn't exist\n    node_dir = os.path.join(node_line_dir, 'query_expansion')\n    pathlib.Path(node_dir).mkdir(parents=True, exist_ok=True)\n\n    # Evaluate each module and measure their execution times\n    evaluation_results = list(map(lambda x: evaluate_one_query_expansion_node(x[0], x[1], previous_result, node_dir, strategies),\n                                  zip(modules, module_params)))\n\n    # Save the results and a summary of the evaluation\n    evaluation_results_df = pd.DataFrame(evaluation_results)\n    evaluation_results_df.to_csv(os.path.join(node_dir, 'evaluation_results.csv'), index=False)\n    evaluation_results_df.describe().to_csv(os.path.join(node_dir, 'evaluation_summary.csv'))\n\n    # Select the best result based on the evaluation\n    best_result, _ = select_best_average(evaluation_results, strategies['metrics'])\n    best_result.to_csv(os.path.join(node_dir, 'best_result.csv'), index=False)\n\n    return best_result"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n    node_dir = os.path.join(node_line_dir, 'prompt_maker')\n    os.makedirs(node_dir, exist_ok=True)\n\n    # Create a list of all combinations of modules and parameters\n    module_combinations = list(map(lambda module_dict: make_combinations({**module_dict, **module_params}),\n                                  modules))\n    module_combinations = explode(modules, module_combinations)\n\n    # Evaluate each combination of module and parameters\n    evaluation_results = []\n    for module_func, module_param in module_combinations:\n        module_name = module_func.__name__\n        module_dir = os.path.join(node_dir, module_name)\n        os.makedirs(module_dir, exist_ok=True)\n\n        # Run the module and save the results\n        result = module_func(node_dir=module_dir, **module_param)\n        result.to_csv(os.path.join(module_dir, 'result.csv'), index=False)\n\n        # Evaluate the module's performance\n        evaluation_result = evaluate_one_prompt_maker_node(**strategies,\n                                                          project_dir=module_dir,\n                                                          prompts=result['prompts'].tolist(),\n                                                          generation_gt=result['generation_gt'].tolist())\n        evaluation_result.to_csv(os.path.join(module_dir, 'evaluation_result.csv'), index=False)\n\n        # Add the evaluation result to the list\n        evaluation_results.append(evaluation_result)\n\n    # Select the best module based on the specified strategies\n    best_module_result = select_best_prompt_maker_node(evaluation_results, strategies)\n\n    # Combine the best module's result with the previous result\n    best_module_result = pd.concat([previous_result, best_module_result], axis="}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_default_embedding_model()\n    gt_embeddings = embedding_model.get_query_embedding(generation_gt)\n    pred_embedding = embedding_model.get_query_embedding(pred)\n    return calculate_cosine_similarity(gt_embeddings, pred_embedding)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN not loaded, skipping\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global codeformer\n        codeformer = FaceRestorerCodeFormer(dirname)\n        face_restoration.face_restorers.append(codeformer)\n    except Exception as e:\n        errors.report(f\"Error setting up CodeFormer: {e}\", exc_info=True)\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    try:\n        facexlib_path = os.path.join(dirname, \"facexlib\")\n        if os.path.exists(facexlib_path):\n            logger.info(f\"Patching facexlib with {facexlib_path}\")\n            face_restoration_utils.patch_facexlib(facexlib_path)\n        else:\n            logger.info(f\"No facexlib found in {facexlib_path}\")\n        gfpgan_face_restorer = FaceRestorerGFPGAN(\n            model_path=dirname,\n            upscale=shared.opts.gfpgan_upscale,\n            arch=\"clean\",\n            channel_multiplier=2,\n            bg_upsampler=shared.opts.gfpgan_bg_upsampler,\n        )\n        gfpgan_face_restorer.net.eval()\n        gfpgan_face_restorer.net.to(devices.device_gfpgan)\n        logger.info(\"GFPGAN face restorer initialized\")\n    except Exception as e:\n        logger.error(f\"Error setting up GFPGAN face restorer: {e}\")\n        raise errors.InitError(f\"Error setting up GFPGAN face restorer: {e}\")"}
{"namespace": "quaternion.rotate", "completion": "  v = jnp.concatenate([v, jnp.zeros_like(v[Ellipsis, :1])], axis=-1)\n  return (multiply(q, v) * conjugate(q))[Ellipsis, :3]\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  angle = linalg.norm(axis_angle, axis=-1, keepdims=True)\n  axis = axis_angle / (angle + eps)\n  return jnp.concatenate(\n      [\n          axis * jnp.sin(angle / 2.0),\n          jnp.cos(angle / 2.0),\n      ],\n      axis=-1,\n  )"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n    high = logit_bias[idx]\n\n    # improve estimate\n    mid = (high + low) / 2\n    while high >= low + eps:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n    return -mid, 1"}
{"namespace": "resample.resample_3d", "completion": "  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError(\n        f'Invalid edge behavior {edge_behavior}. Must be one of \"CONSTANT_OUTSIDE\" or \"CLAMP\".'\n    )\n\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError(\n        f'Invalid interpolation method {method}. Must be one of \"TRILINEAR\" or \"NEAREST\".'\n    )\n\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError(\n        f'Invalid coordinate order {coordinate_order}. Must be one of \"xyz\" or \"zyx\".'\n    )\n\n  if data.ndim != 4:\n    raise ValueError(\n        f'Invalid data shape {data.shape}. Must be of shape [D, H, W, C].'\n    )\n\n  if locations.ndim < 2 or locations.shape[-1] != 3:\n    raise ValueError(\n        f'Invalid locations shape {locations.shape}. Must be of shape [D, ..., 3].'\n    )\n\n  if data.shape[0] != locations.shape[0]:\n    raise ValueError(\n        f'Invalid data and locations shapes {data.shape} and {locations.shape}. The first dimension of data and locations must match.'\n    )\n\n  if half_pixel_center:\n    # Convert locations to the voxel centers.\n    locations = locations + 0.5\n\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    # Pad the data volume with constant values.\n    data = jnp.pad(\n        data,\n        pad_width=((1, 1), (1, 1), (1, 1), (0, 0)),\n        mode='constant',\n        constant_values=constant_values,\n    )\n\n    # Adjust the locations to account for the padding.\n    locations = locations + 1\n\n  if coordinate_order == 'xyz':\n    x_coordinate = locations"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x),\n                                                                jnp.inf))\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), jnp.NINF)\n  )"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, y, x_dot: x_dot / jnp.maximum(tiny_val, x),\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: 0.5 * x_dot / y,\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return 1\n  elif p == 0:\n    return jnp.inf\n  elif p == -jnp.inf:\n    return 0\n  elif p == jnp.inf:\n    return jnp.inf\n  else:\n    return jnp.abs(p - 1) / p"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array(\n        [\n            [1, 1, 1],\n            [-1, -1, 1],\n            [1, -1, -1],\n            [-1, 1, -1],\n        ]\n    )\n    base_faces = np.array([[0, 1, 2], [0, 2, 3], [0, 3, 1], [1, 3, 2]])\n  elif base_shape == 'icosahedron':\n    phi = (1 + np.sqrt(5)) / 2\n    base_verts = np.array(\n        [\n            [0, 1, phi],\n            [0, -1, phi],\n            [1, phi, 0],\n            [-1, phi, 0],\n            [phi, 0, 1],\n            [-phi, 0, -1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 2, 1],\n            [0, 3, 2],\n            [0, 4, 3],\n            [0, 1, 4],\n            [1, 2, 5],\n            [2, 3, 5],\n            [3, 4, 5],\n            [4, 1, 5],\n        ]\n    )\n  elif base_shape == 'octahedron':\n    base_verts = np.array(\n        [\n            [1, 0, 0],\n            [-1, 0, 0],\n            [0, 1, 0],\n            [0, -1, 0],\n            [0, 0, 1],\n            [0, 0, -1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 2, 4],\n            [0, 4, 3],\n            [0, 5, 2],\n            [0, 3"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + x),\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  xp = jnp.abs(x)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  y_max = minus_eps(power_ladder_max_output(p))\n  y = select(\n      [\n          (p == 1, xp),\n          (p == 0, safe_expm1(xp)),\n          (p == -jnp.inf, -safe_log1p(-xp)),\n          (p == jnp.inf, safe_log1p(xp)),\n      ],\n      jnp.abs(p_safe - 1) * (jnp.abs(xp) ** p_safe - 1),\n  )\n  y = jnp.clip(y, -y_max, y_max)\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # Avoid dividing by 0 before it's needed.\n    p = jnp.maximum(1.0 - jnp.power(lr_delay_mult, 1 / lr_delay_steps), 1e-8)\n  else:\n    p = 1.0\n\n  # If we have not reached the maximum number of steps yet, use the decay schedule.\n  t = jnp.clip(step - lr_delay_steps, 0, max_steps) / (max_steps - lr_delay_steps)\n  log_lerp_t = (1 - p) * log_lerp(t, lr_init, lr_final) + p * lr_init\n\n  return jnp.asarray(log_lerp_t)"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1,\n      origin_lo=0.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=0.0,\n      far_hi=1.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Must add half pixel offset to shoot rays through pixel centers.\n  def pix_to_dir(x, y):\n    return xnp.stack([x + 0.5, y + 0.5, xnp.ones_like(x)], axis=-1)\n\n  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          pix_to_dir(pix_x_int, pix_y_int),\n          pix_to_dir(pix_x_int + 1, pix_y_int),\n          pix_to_dir(pix_x_int, pix_y_int + 1),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_dirs_stacked[Ellipsis, :2]), axis=-1))\n    theta ="}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[Ellipsis, :3]\n  v = screw_axis[Ellipsis, 3:]\n  theta_squared = jnp.sum(w**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  R_taylor = exp_so3(w, eps)\n  p_taylor = v\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta_squared > eps**2, w, 0.0)\n  theta_safe = jnp.where(theta_squared > eps**2, theta, 1.0)\n  w_safe = w_safe / theta_safe\n  W = skew(w_safe)\n  WW = spin_math.matmul(W, W)\n  G_theta = (\n      jnp.eye(3)\n      + (1.0 - jnp.cos(theta_safe)) * W\n      + (theta_safe - jnp.sin(theta_safe)) * WW\n  )\n  V = (\n      jnp.eye(3)\n      + (1.0 - jnp.cos(theta_safe)) * W\n      + (theta_safe - jnp.sin(theta_safe)) * WW\n  )\n  p = spin_math.matmul(V, p_taylor[Ellipsis, jnp.newaxis]).squeeze(-1)\n  return rp_to_se3(R_taylor, p)"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta_squared = jnp.sum(axis_angle**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n  W = skew(axis_angle / theta)\n  # Note that theta = 0 means the axis_angle is a zero vector.\n  R = (\n      jnp.eye(3)\n      + jnp.sin(theta) * W\n      + (1.0 - jnp.cos(theta)) * spin_math.matmul(W, W)\n  )\n  # If theta = 0 then the rotation is the identity.\n  R = jnp.where(theta_squared > eps, R, jnp.eye(3))\n  return R"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= base_radius**2\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean = (t0 + t1) / 2\n  r_var = radius**2 / 4\n  t_var = (t1 - t0)**2 / 12\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Undo OpenGL to OpenCV coordinate system convention.\n  pix_x_int, pix_y_int = xnp.array(pix_x_int), xnp.array(pix_y_int)\n  pix_x_int, pix_y_int = pix_x_int + 0.5, pix_y_int + 0.5\n\n  # OpenGL convention:\n  # +y is up, +x is right, +z is behind us.\n  # Convert to:\n  # +y is up, +x is right, -z is in front of us.\n  # (This changes our +z sign.)\n  # This just affects the viewdirs, so we can skip it if we're using OpenGL.\n  if camtype == ProjectionType.PERSPECTIVE:\n    pix_y_int = xnp.maximum(0, pix_y_int)\n    pix_x_int = xnp.maximum(0, pix_x_int)\n\n  # OpenGL coordinate system convention.\n  # +y is up, +x is right, -z is in front of us.\n  # Convert to:\n  # +y is up, +x is right, +z is behind us.\n  # (This changes our +z sign.)\n  # This just affects the viewdirs, so we can skip it if we're using OpenGL.\n  if camtype == ProjectionType.PERSPECTIVE:\n    pix_y_int = xnp.maximum(0, pix_y_int)\n    pix_x_int = xnp.maximum(0, pix_x_int)\n\n  # OpenGL coordinate system convention.\n  # +y is up, +x is right, -z is in front of us.\n  #"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density[Ellipsis, 1:] * jnp.linalg.norm(dirs[Ellipsis, :-1], axis=-1)\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 1:\n    raise ValueError(f'num_samples must be > 1, is {num_samples}.')\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Pad CDFs to make them work for offsets.\n  cw0 = jnp.concatenate([jnp.zeros(cw.shape[:-1] + (1,)), cw, jnp.ones(cw.shape[:-1] + (1,))], axis=-1)\n\n  # Draw uniform samples.\n  if rng is None:\n    # Match the behavior of jax.random.uniform() by using linspace instead.\n    shape = cw.shape[:-1] + (num_samples,)\n    u = jnp.linspace(0.0 + eps, 1.0 - eps, num_samples)\n    u = jnp.broadcast_to(u, shape)\n  else:\n    shape = cw.shape[:-1] + (num_samples,)\n    if single_jitter:\n      u = jax.random.uniform(rng, shape, minval=eps, maxval=1.0 - eps)\n    else:\n      u = jax.random.uniform(rng, shape)\n\n  # Identify the location in `cw` that corresponds to a random sample `u`.\n  # The output of `np.interp()` matches the batching of `u`.\n  t_new = jnp.interp(u, cw0, t)\n\n  if deterministic_center:\n    # Match the behavior of jnp.interp().\n    t_new = jnp.clip(t_new, t[Ellipsis, :-1], t[Ellipsis, 1:])\n\n  return t_new"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[..., 1:] + t_samples[..., :-1]) / 2\n\n  # Adjust the first and last intervals to fit within the specified domain.\n  t_samples = jnp.concatenate([jnp.full_like(t_samples[..., :1], domain[0]), t_midpoints, jnp.full_like(t_samples[..., :1], domain[1])], axis=-1)\n\n  return t_samples"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  w = jnp.where(w < 0, 0, w)\n  w = w / jnp.sum(w, axis=-1, keepdims=True)\n  cw = jnp.cumsum(w, axis=-1)\n  cw = jnp.concatenate([jnp.zeros(cw.shape[:-1] + (1,)), cw], axis=-1)\n  t_new = jnp.interp(ps / 100, cw, t)\n  return t_new"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w, axis=-1)\n  cw = integrate_weights(w)\n  # Blur the PDF.\n  cw_blur = linspline.blur(cw, blur_halfwidth)\n  # Interpolate into the inverse CDF.\n  t_new = math.sorted_interp(cw_blur, t, tq, utils.device_is_tpu())\n  return t_new"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(transform, to_homogeneous(vectors)))"}
{"namespace": "stepfun.resample", "completion": "  # Compute the width of each interval in tp.\n  dt = jnp.diff(tp, axis=-1)\n\n  # Compute the width of each interval in t.\n  dtq = jnp.diff(t, axis=-1)\n\n  # Compute the width of each interval in tp, but with the same shape as t.\n  dt_t = jnp.broadcast_to(dt, t.shape[:-1] + (dt.shape[-1],))\n\n  # Compute the width of each interval in t, but with the same shape as tp.\n  dtq_tp = jnp.broadcast_to(dtq, tp.shape[:-1] + (dtq.shape[-1],))\n\n  # Compute the overlap between each interval in t and each interval in tp.\n  overlap = jnp.minimum(dt_t, dtq_tp)\n\n  # Compute the weight of each interval in tp, based on its overlap with each interval in t.\n  wp = overlap / dt_t\n\n  # Compute the weight of each interval in t, based on its overlap with each interval in tp.\n  wq = overlap / dtq_tp\n\n  # Compute the resampled values of the step function.\n  if use_avg:\n    vq = jnp.sum(vp * wp, axis=-1) / jnp.sum(wp, axis=-1)\n  else:\n    vq = jnp.sum(vp * wp, axis=-1)\n\n  return vq"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance to be in the range [0, 1].\n  mean_scaled = (mean - min_deg) / (max_deg - min_deg)\n  var_scaled = var / (max_deg - min_deg) ** 2\n\n  # Concatenate the scaled mean and variance.\n  x = jnp.concatenate([mean_scaled, var_scaled], axis=-1)\n\n  # Apply the sinusoidal encoding.\n  x_encoded = jnp.sin(2 ** jnp.arange(min_deg, max_deg) * jnp.pi * x)\n\n  return x_encoded"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def integrated_dir_enc_fn(xyz, kappa_inv):\n    \"\"\"\n    Evaluates the integrated directional encoding (IDE) for a given set of 3D points and inverse concentration parameter of the von Mises-Fisher distribution.\n\n    Input-Output Arguments\n    :param xyz: Array. The 3D points for which to compute the IDE.\n    :param kappa_inv: Array. The inverse concentration parameter of the von Mises-Fisher distribution.\n    :return: Array. The computed IDE values for the given points.\n\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    line_set = set()\n    block_idx = 0\n    header_block_idx = -1\n    text_group_start = True\n    text_group_start_idx = 0\n    group_id = 0\n    for idx, line in enumerate(lines):\n        line = clean_lines(line)\n        if should_skip(line):\n            continue\n        if line in line_set:\n            continue\n        if len(line.split()) > 8:\n            line_set.add(line)\n        line = line_parser.Line(line)\n        if line.is_header:\n            header_block_idx = block_idx\n        if line.is_table_row:\n            line.line_type = \"table_row\"\n        if line.is_list_item:\n            line.line_type = \"list_item\"\n        if line.is_list_or_row:\n            line.line_type = \"list_or_row\"\n        if line.is_table_row or line.is_list_or_row:\n            if text_group_start:\n                text_group_start = False\n                text_group_start_idx = block_idx\n        if line.line_type == \"para\":\n            text_group_start = True\n            text_group_start_idx = -1\n        block = {\n            \"block_idx\": block_idx,\n            \"block_text\": line.text,\n            \"block_type\": line.line_type,\n            \"header_block_idx\": header_block_idx,\n            \"block_group\": [line.text_list],\n            \"text_group_start_idx\": text_group_start_idx,\n            \"block_list\": line.text_list,\n            \"line\": line,\n            \"group_id\": group_id,\n        }\n        if idx + 1 < len(lines):\n            next_line = line_parser.Line(lines[idx + 1])\n            if line.line_type == \"para\" and next_line.line_type == \"para\":\n                if check_block"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    # split text into paragraphs\n    texts = org_texts.split(\"\\n\")\n\n    # tokenize each paragraph\n    sents = []\n    for text in texts:\n        # remove space between punctuations (.')\n        text = space_rule.sub(r\"\\1\", text)\n\n        # normalize quotation marks\n        text = quotation_pattern.sub('\"', text)\n\n        # tokenize text\n        text_sents = nltk_tokenzier.tokenize(text)\n\n        # handle special cases\n        for rule, replaced in rules:\n            text_sents = [rule.sub(replaced, sent) for sent in text_sents]\n\n        # handle brackets\n        text_sents = [bracket_rule.sub(r\"\\1\", sent) for sent in text_sents]\n\n        sents.extend(text_sents)\n\n    return sents\n\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            return self.posns.positions(token)\n        else:\n            return self.posns.positions(token, key)\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith(\"%\"):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n    elif spec.startswith(\"<\"):\n        return int(num_clauses - float(spec[1:]))\n    else:\n        return int(spec)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_direct(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if truncate:\n            array = array[:batch_size]\n        term_mat, posns, term_dict, avg_doc_length, doc_lens = build_index_from_tokenizer(array,\n                                                                                          tokenizer,\n                                                                                          Terms,\n                                                                                          batch_size=batch_size)\n        return cls(postings=postings,\n                   tokenizer=tokenizer,\n                   avoid_copies=avoid_copies)\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(\n            self.config['serverHost'],\n            self.config['serverPort'],\n            self.config['proxyHost'],\n            self.config['proxyPort'],\n            self.config['strategies'][self.config['strategy']]\n        )\n        self.server.start()\n\n        self.connections = {}\n        self.lock = threading.Lock()"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> _1) & s55\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr + (arr >> _4)) & s0F\n    arr = (arr * _1) >> all_but_one_bit\n    return arr"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = \"100%\"\n\n    if q_op not in [\"OR\", \"AND\"]:\n        raise ValueError(\"q_op must be either 'OR' or 'AND'\")\n\n    if q_op == \"OR\":\n        q_op_explain = \"|\"\n    else:\n        q_op_explain = \"&\"\n\n    query_fields = parse_field_boosts(qf)\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, query_fields.keys())\n\n    if term_centric:\n        qf_scores, qf_explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, qf_explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    explain = f\"{qf_explain} {q_op_explain} \"\n\n    if pf is not None:\n        pf_scores, pf_explain = _edismax_field_centric(frame, parse_field_boosts(pf), num_search_terms, search_terms, mm, similarity)\n        qf_scores += pf_scores\n        explain += f\"{pf_explain} {q_op_explain} \"\n\n    if pf2 is not None:\n        pf2_scores, pf2_explain = _edismax_field_centric(frame, parse_field_boosts(pf2), num_search_terms, search_terms, mm, similarity)\n        qf_scores += pf2_scores\n        explain += f\"{pf2_explain} {q_op_explain} \"\n\n    if pf3 is not None:\n        pf3_scores, pf3_explain = _edismax_field_centric(frame, parse_field_"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            for connection in self.connections.values():\n                connection.stop()\n            self.connections.clear()\n\n        if self.server is not None:\n            self.server.stop()\n            self.server = None"}
