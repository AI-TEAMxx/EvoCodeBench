{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Running command: {cmd}\")\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    import subprocess\n\n    print(cmd)\n    return subprocess.call(cmd, shell=True)"}
{"namespace": "chat.utils.run_cmd", "completion": "    import subprocess\n\n    print(f\"Running command: {cmd}\")\n    return subprocess.call(cmd, shell=True)"}
{"namespace": "chat.utils.run_cmd", "completion": "    import subprocess\n\n    print(f\"Running command: {cmd}\")\n    return subprocess.run(cmd, shell=True).returncode"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Running command: {cmd}\")\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Running cmd: {cmd}\")\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Running command: {cmd}\")\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Executing bash command: {cmd}\")\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Running: {cmd}\")\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(\"Executing:\", cmd)\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    import subprocess\n\n    print(\"Running command:\", cmd)\n    process = subprocess.Popen(cmd, shell=True)\n    process.wait()\n    return process.returncode"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    if x.ndim != 2 or x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix must be square\")\n    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    if x.ndim != 2:\n        raise ValueError(\"The matrix must be 2D\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix must be square\")\n    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")\n\n    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  x = scale * z\n  return x"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |z| < 1\n  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(z_mag_sq) - 1) / z_mag_sq\n  x = scale * z\n  return x"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(z_mag_sq) - 1) / z_mag_sq\n  x = scale * z\n  return x"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "coord.inv_contract", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  x = z / scale\n  return x\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |z| < 1\n  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(z_mag_sq) - 1) / z_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Compute the scaling factor for the input vector z\n  scale = jnp.sqrt(jnp.sum(z**2, axis=-1, keepdims=True))\n  # Compute the inverse scaling factor for the input vector z\n  inv_scale = jnp.sqrt(jnp.sum(z**2, axis=-1, keepdims=True)) / 2\n  # Compute the inverse of the contract function for the input vector z\n  inv_z = inv_scale * z\n  return inv_z\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |z| < 1\n  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(z_mag_sq) - 1) / z_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "coord.inv_contract", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |z| < 1\n  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(z_mag_sq) - 1) / z_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |z| < 1\n  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(z_mag_sq) - 1) / z_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "coord.inv_contract", "completion": "  z_mag_sq = jnp.sum(z**2, axis=-1, keepdims=True)\n  scale = (2 * jnp.sqrt(z_mag_sq) - 1) / z_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "coord.inv_contract", "completion": "  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(z_mag_sq) - 1) / z_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    @functools.wraps(func_name)\n    def wrapper(*args, **kwargs):\n        with SQLiteMemoization(filename) as memo:\n            return memo.fetch_or_compute(func_name, func_name.__name__, *args, **kwargs)\n\n    return wrapper"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    @functools.wraps(func_name)\n    def wrapper(*args, **kwargs):\n        with SQLiteMemoization(filename) as memo:\n            return memo.fetch_or_compute(func_name, func_name.__name__, *args, **kwargs)\n\n    return wrapper"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    @functools.wraps(func_name)\n    def wrapper(*args, **kwargs):\n        with SQLiteMemoization(filename) as memo:\n            return memo.fetch_or_compute(func_name, func_name.__name__, *args, **kwargs)\n\n    return wrapper"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as memo:\n                return memo.fetch_or_compute(func, func_name, *args, **kwargs)\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as memo:\n                return memo.fetch_or_compute(func, func_name, *args, **kwargs)\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as memo:\n                return memo.fetch_or_compute(func, func_name, *args, **kwargs)\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as memo:\n                return memo.fetch_or_compute(func, func_name, *args, **kwargs)\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as memo:\n                return memo.fetch_or_compute(func, func_name, *args, **kwargs)\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    @functools.wraps(func_name)\n    def wrapper(*args, **kwargs):\n        with SQLiteMemoization(filename) as cache:\n            return cache.fetch_or_compute(func_name, func_name.__name__, *args, **kwargs)\n\n    return wrapper"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    @functools.wraps(func_name)\n    def wrapper(*args, **kwargs):\n        with SQLiteMemoization(filename) as cache:\n            return cache.fetch_or_compute(func_name, func_name.__name__, *args, **kwargs)\n\n    return wrapper"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    @functools.wraps(func_name)\n    def wrapper(*args, **kwargs):\n        with SQLiteMemoization(filename) as memo:\n            return memo.fetch_or_compute(func_name, *args, **kwargs)\n\n    return wrapper"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    @functools.wraps(func_name)\n    def wrapper(*args, **kwargs):\n        with SQLiteMemoization(filename) as memo:\n            return memo.fetch_or_compute(func_name, func_name.__name__, *args, **kwargs)\n\n    return wrapper"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    @functools.wraps(func_name)\n    def wrapper(*args, **kwargs):\n        with SQLiteMemoization(filename) as memoizer:\n            return memoizer.fetch_or_compute(func_name, func_name.__name__, *args, **kwargs)\n\n    return wrapper"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    @functools.wraps(func_name)\n    def decorator(*args, **kwargs):\n        with SQLiteMemoization(filename) as memo:\n            return memo.fetch_or_compute(func_name, func_name.__name__, *args, **kwargs)\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    @functools.wraps(func_name)\n    def wrapper(*args, **kwargs):\n        with SQLiteMemoization(filename) as memo:\n            return memo.fetch_or_compute(func_name, func_name, *args, **kwargs)\n\n    return wrapper"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        with SQLiteMemoization(filename) as memoizer:\n            return memoizer.fetch_or_compute(func, func_name, *args, **kwargs)\n\n    return wrapper"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    @functools.wraps(func_name)\n    def wrapper(*args, **kwargs):\n        with SQLiteMemoization(filename) as memoization:\n            return memoization.fetch_or_compute(\n                func_name, func_name.__name__, *args, **kwargs\n            )\n\n    return wrapper"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            arg_hash = hashlib.sha256(\n                f\"{func_name}:{repr(args)}:{repr(kwargs)}\".encode(\"utf-8\")\n            ).hexdigest()\n\n            with SQLiteMemoization(filename) as memo:\n                result = memo.fetch_or_compute(func, func_name, *args, **kwargs)\n\n            return result\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as memo:\n                return memo.fetch_or_compute(func, func_name, *args, **kwargs)\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def memoize_to_sqlite_decorator(func):\n        @functools.wraps(func)\n        def memoized_func(*args, **kwargs):\n            with SQLiteMemoization(filename) as memo:\n                return memo.fetch_or_compute(func, func_name, *args, **kwargs)\n        return memoized_func\n\n    return memoize_to_sqlite_decorator"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max.\")\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max.\")\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max.\")\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max.\")\n\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max.\")\n\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max.\")\n\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values.values()\n\n    if x_min >= x_max or y_min >= y_max:\n        raise ValueError(\n            f\"{cls.__name__}: x_min must be less than x_max and y_min must be less than y_max.\"\n        )\n\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max. Received {values}\")\n\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max. Received {values}\")\n\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min must be strictly inferior to x_max.\")\n\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min must be strictly inferior to y_max.\")\n\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max\")\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max\")\n\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min = values.get(\"x_min\")\n    x_max = values.get(\"x_max\")\n    y_min = values.get(\"y_min\")\n    y_max = values.get(\"y_max\")\n\n    if x_min is not None and x_max is not None and y_min is not None and y_max is not None:\n        if x_min >= x_max:\n            raise ValueError(f\"{cls.__name__}: x_min must be less than x_max.\")\n        if y_min >= y_max:\n            raise ValueError(f\"{cls.__name__}: y_min must be less than y_max.\")\n\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values[\"x_min\"], values[\"x_max\"], values[\"y_min\"], values[\"y_max\"]\n\n    if x_min >= x_max or y_min >= y_max:\n        raise ValueError(\n            f\"{cls.__name__}: x_min ({x_min}) must be less than x_max ({x_max}) and y_min ({y_min}) must be less than y_max ({y_max}).\"\n        )\n\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] > values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than or equal to x_max.\")\n    if values[\"y_min\"] > values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than or equal to y_max.\")\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: x_min must be less than x_max. Got {values['x_min']} >= {values['x_max']}\"\n        )\n\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: y_min must be less than y_max. Got {values['y_min']} >= {values['y_max']}\"\n        )\n\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values.values()\n    if x_min >= x_max or y_min >= y_max:\n        raise ValueError(\n            f\"{cls.__name__}: invalid bounding box. x_min must be less than x_max, and y_min must be less than y_max.\"\n        )\n\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] > values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min > x_max\")\n    if values[\"y_min\"] > values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min > y_max\")\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values[\"x_min\"], values[\"x_max\"], values[\"y_min\"], values[\"y_max\"]\n\n    if x_min >= x_max:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max. Got {x_min} >= {x_max}.\")\n\n    if y_min >= y_max:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max. Got {y_min} >= {y_max}.\")\n\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values.values()\n    if x_min >= x_max or y_min >= y_max:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max, and y_min must be less than y_max.\")\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max.\")\n\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max.\")\n\n    return values\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: x_min must be less than x_max, received x_min={values['x_min']} and x_max={values['x_max']}\"\n        )\n\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: y_min must be less than y_max, received y_min={values['y_min']} and y_max={values['y_max']}\"\n        )\n\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values.values()\n    if x_min >= x_max or y_min >= y_max:\n        raise ValueError(f\"{cls.__name__}: invalid bounding box. \" f\"x_min: {x_min} >= x_max: {x_max} \" f\"or y_min: {y_min} >= y_max: {y_max}\")\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values[\"x_min\"], values[\"x_max\"], values[\"y_min\"], values[\"y_max\"]\n\n    if x_min >= x_max or y_min >= y_max:\n        raise ValueError(f\"{cls.__name__}: invalid bounding box values. x_min >= x_max or y_min >= y_max.\")\n\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] > values[\"x_max\"] or values[\"y_min\"] > values[\"y_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: x_min and x_max must be less than x_max and y_max, respectively. \"\n            f\"Received {values['x_min']} and {values['x_max']}, respectively. \"\n            f\"Received {values['y_min']} and {values['y_max']}, respectively.\"\n        )\n    return values"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  dist = np.sum(mat0**2, 0, keepdims=True) + np.sum(mat1**2, 0) - 2 * np.dot(\n      mat0, mat1\n  )\n  dist[dist < 0] = 0\n  return dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  dist = np.sum(mat0**2, 0, keepdims=True) + np.sum(mat1**2, 0) - 2 * np.dot(\n      mat0, mat1\n  )\n  dist[dist < 0] = 0\n  return dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  dist = np.sum(mat0**2, 0, keepdims=True) + np.sum(mat1**2, 0) - 2 * np.dot(\n      mat0, mat1\n  )\n  dist[dist < 0] = 0\n  return dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  return np.maximum(0, np.sum(mat0**2, axis=0, keepdims=True) +\n                    np.sum(mat1**2, axis=0) - 2 * np.dot(mat0, mat1.T))"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  mat0 = mat0.T\n  mat1 = mat1.T\n  norm0 = np.sum(mat0**2, 0, keepdims=True)\n  norm1 = np.sum(mat1**2, 0, keepdims=True)\n  dot = np.matmul(mat0, mat1)\n  dist = norm0 + norm1.T - 2 * dot\n  dist[dist < 0] = 0\n  return dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  return np.maximum(0, np.sum(mat0**2, axis=0, keepdims=True) +\n                    np.sum(mat1**2, axis=0) - 2 * np.dot(mat0.T, mat1))"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  diff = mat0[:, None, :] - mat1[None, :, :]\n  sq_dist = np.sum(diff * diff, axis=-1)\n  sq_dist[sq_dist < 0] = 0\n  return sq_dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  sq_dist = np.sum(mat0**2, 0, keepdims=True) + np.sum(mat1**2, 0) - 2 * np.dot(\n      mat0.T, mat1\n  )\n  sq_dist = np.maximum(sq_dist, 0)\n  return sq_dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  mat0_norm = np.sum(mat0**2, 0, keepdims=True)\n  mat1_norm = np.sum(mat1**2, 0, keepdims=True)\n  dot_product = np.matmul(mat0.T, mat1)\n  dist = mat0_norm + mat1_norm.T - 2 * dot_product\n  dist[dist < 0] = 0\n  return dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  mat0_norm = np.linalg.norm(mat0, axis=0, keepdims=True)\n  mat1_norm = np.linalg.norm(mat1, axis=0, keepdims=True)\n\n  dot_prod = np.matmul(mat0.T, mat1)\n  dist = mat0_norm + mat1_norm.T - 2 * dot_prod\n  dist = np.maximum(dist, 0)\n\n  return dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  mat0 = np.reshape(mat0, [-1, 1])\n  mat1 = np.reshape(mat1, [-1, 1])\n  mat0_norm = np.sum(mat0**2, 0)\n  mat1_norm = np.sum(mat1**2, 0)\n  mat01_dot = np.matmul(mat0.T, mat1)\n  dist = mat0_norm + mat1_norm - 2 * mat01_dot\n  dist = np.maximum(dist, 0)\n  return dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  mat0_norm = np.sum(mat0**2, 0, keepdims=True)\n  mat1_norm = np.sum(mat1**2, 0, keepdims=True)\n  dot_prod = np.matmul(mat0, mat1.T)\n\n  dist = mat0_norm + mat1_norm.T - 2 * dot_prod\n  dist = np.maximum(dist, 0)  # Set negative distances to zero.\n\n  return dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  mat0 = mat0.T\n  mat1 = mat1.T\n  norm0 = np.sum(mat0**2, 0)\n  norm1 = np.sum(mat1**2, 0)\n  dot = np.matmul(mat0.T, mat1)\n  sq_dist = norm0[:, None] + norm1[None, :] - 2 * dot\n  sq_dist = np.maximum(sq_dist, 0)\n  return sq_dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  mat0_norm = np.linalg.norm(mat0, axis=0, keepdims=True)\n  mat1_norm = np.linalg.norm(mat1, axis=0, keepdims=True)\n  mat0_mat1_dot = np.matmul(mat0, mat1.T)\n\n  sq_dist = mat0_norm + mat1_norm.T - 2 * mat0_mat1_dot\n  sq_dist = np.maximum(sq_dist, 0)  # Set negative distances to zero.\n\n  return sq_dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  mat0_norm = np.sqrt(np.sum(mat0**2, 0, keepdims=True))\n  mat1_norm = np.sqrt(np.sum(mat1**2, 0, keepdims=True))\n  dot_prod = np.matmul(mat0.T, mat1)\n  dist = mat0_norm + mat1_norm.T - 2 * dot_prod\n  dist[dist < 0] = 0  # Avoid numerical errors.\n  return dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  mat0 = np.reshape(mat0, [-1, 1])\n  mat1 = np.reshape(mat1, [1, -1])\n  dist = np.sum(mat0**2, 0, keepdims=True) + np.sum(mat1**2, 0) - 2 * np.matmul(\n      mat0, mat1\n  )\n  dist = np.maximum(dist, 0)\n  return dist\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  mat0 = np.reshape(mat0, [-1, mat0.shape[-1]])\n  mat1 = np.reshape(mat1, [-1, mat1.shape[-1]])\n  norm0 = np.sum(mat0**2, 1, keepdims=True)\n  norm1 = np.sum(mat1**2, 1, keepdims=True)\n  mat = np.matmul(mat0, mat1.T)\n  mat = mat0 * mat1.T\n  mat = norm0 + norm1.T - 2 * mat\n  mat = np.maximum(mat, 0)\n  return mat"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  mat0_sq = np.sum(mat0**2, axis=0)\n  mat1_sq = np.sum(mat1**2, axis=0)\n  mat01 = np.matmul(mat0, mat1.T)\n  dist = mat0_sq[:, None] + mat1_sq[None, :] - 2 * mat01\n  dist[dist < 0] = 0  # Zero out negative distances.\n  return dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  if mat0.shape[0] != mat1.shape[0]:\n    raise ValueError('mat0 and mat1 must have the same number of rows')\n\n  # Compute the norm of each column in mat0 and mat1.\n  norm0 = np.sum(mat0**2, 0)\n  norm1 = np.sum(mat1**2, 0)\n\n  # Compute the dot product between each column in mat0 and mat1.\n  dot_product = np.matmul(mat0.T, mat1)\n\n  # Compute the squared distance between each pair of columns in mat0 and mat1.\n  sq_dist = norm0[:, np.newaxis] + norm1[np.newaxis, :] - 2 * dot_product\n\n  # Set negative distances to zero.\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  mat0 = mat0.T\n  mat1 = mat1.T\n  sq_dist = np.zeros((mat0.shape[0], mat1.shape[0]))\n  for i in range(mat0.shape[0]):\n    mat0_i = mat0[i, :]\n    mat1_sq = mat1**2\n    mat1_sq_sum = np.sum(mat1_sq, axis=0)\n    mat1_sq_sum_i = mat1_sq_sum[i]\n    mat1_sq_sum_i_mat0 = mat1_sq_sum_i * mat0_i\n    mat1_sq_sum_mat0_i = mat1_sq_sum * mat0_i\n    mat1_sq_sum_mat0_i_sum = np.sum(mat1_sq_sum_mat0_i, axis=0)\n    sq_dist[i, :] = mat1_sq_sum_i_mat0 - 2 * mat1_sq_sum_mat0_i_sum\n  sq_dist = np.maximum(sq_dist, 0)\n  return sq_dist"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"s3://\") or path.startswith(\"gs://\"):\n        return True\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"/cache\") or path.startswith(\"/data\"):\n        return True\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"http\") or path.startswith(\"s3\"):\n        return False\n    return True"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"http://\") or path.startswith(\"https://\"):\n        return False\n\n    return True"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"/cache\"):\n        return True\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"/cache\") or path.startswith(\"s3://\"):\n        return True\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if not path:\n        return False\n\n    if path.startswith(\"s3://\") or path.startswith(\"gs://\"):\n        return True\n\n    if path.startswith(\"hdfs://\") or path.startswith(\"file://\"):\n        return True\n\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if not path:\n        return False\n\n    if path == \"\":\n        return False\n\n    if path.startswith(\"gs://\"):\n        return True\n\n    if path.startswith(\"s3://\"):\n        return True\n\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if not path:\n        return False\n    if path == \"\":\n        return False\n    if path.startswith(\"gs://\"):\n        return True\n    if path.startswith(\"http://\") or path.startswith(\"https://\"):\n        return True\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"/cache\") or path.startswith(\"/data\") or path.startswith(\"/mnt\"):\n        return True\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if not path:\n        return False\n    if path.startswith(\"gs://\") or path.startswith(\"hdfs://\"):\n        return True\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"/cache/chunks/\") or path.startswith(\"s3://\"):\n        return False\n    return True"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if not path:\n        return False\n    if path == \"\":\n        return False\n    if path.startswith(\"s3://\") or path.startswith(\"gs://\") or path.startswith(\"hdfs://\"):\n        return True\n    if path.startswith(\"http://\") or path.startswith(\"https://\"):\n        return True\n    if path.startswith(\"ftp://\"):\n        return True\n    if path.startswith(\"file://\"):\n        return True\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\":\n        return False\n\n    if path.startswith(\"s3://\") or path.startswith(\"gs://\"):\n        return True\n\n    if path.startswith(\"hdfs://\"):\n        return True\n\n    if path.startswith(\"http://\") or path.startswith(\"https://\"):\n        return True\n\n    if path.startswith(\"file://\"):\n        return True\n\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if not path:\n        return True\n\n    if path.startswith(\"gs://\") or path.startswith(\"s3://\"):\n        return True\n\n    if path.startswith(\"hdfs://\") or path.startswith(\"file://\"):\n        return True\n\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if not path:\n        return False\n    if path.startswith(\"/cache/\"):\n        return False\n    if path.startswith(\"gs://\"):\n        return False\n    if path.startswith(\"s3://\"):\n        return False\n    if path.startswith(\"hdfs://\"):\n        return False\n    if path.startswith(\"az://\"):\n        return False\n    if path.startswith(\"file://\"):\n        return False\n    return True"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if not path:\n        return False\n    if path.startswith(\"s3://\") or path.startswith(\"gs://\"):\n        return True\n    if path.startswith(\"hdfs://\") or path.startswith(\"adl://\"):\n        return True\n    if path.startswith(\"file://\"):\n        return True\n    if path.startswith(\"http://\") or path.startswith(\"https://\"):\n        return True\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if not path:\n        return False\n    if path == \"\":\n        return False\n    if path.startswith(\"s3://\"):\n        return True\n    if path.startswith(\"gs://\"):\n        return True\n    if path.startswith(\"hdfs://\"):\n        return True\n    if path.startswith(\"http://\"):\n        return True\n    if path.startswith(\"https://\"):\n        return True\n    if path.startswith(\"file://\"):\n        return True\n    if path.startswith(\"ftp://\"):\n        return True\n    if path.startswith(\"sftp://\"):\n        return True\n    if path.startswith(\"s3a://\"):\n        return True\n    if path.startswith(\"s3n://\"):\n        return True\n    if path.startswith(\"s3fs://\"):\n        return True\n    if path.startswith(\"s3select://\"):\n        return True\n    if path.startswith(\"s3select+http://\"):\n        return True\n    if path.startswith(\"s3select+https://\"):\n        return True\n    if path.startswith(\"s3select+file://\"):\n        return True\n    if path.startswith(\"s3select+ftp://\"):\n        return True\n    if path.startswith(\"s3select+sftp://\"):\n        return True\n    if path.startswith(\"s3select+s3a://\"):\n        return True\n    if path.startswith(\"s3select+s3n://\"):\n        return True\n    if path.startswith(\"s3select+s3fs://\"):\n        return True\n    if path.startswith(\"s3select+s3select://\"):\n        return True\n    if path.startswith(\"s3select+s3select+http://\"):\n        return True\n    if path.startswith(\"s3select+s3select+https://\"):\n        return True\n    if path.startswith(\"s3"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if not path:\n        return False\n\n    if path.startswith(\"/cache\"):\n        return True\n\n    if path.startswith(\"s3://\") or path.startswith(\"gs://\"):\n        return True\n\n    if path.startswith(\"hdfs://\"):\n        return True\n\n    if path.startswith(\"http://\") or path.startswith(\"https://\"):\n        return True\n\n    if path.startswith(\"az://\"):\n        return True\n\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if not path:\n        return False\n    if path == \"\":\n        return False\n    if path.startswith(\"/cache\"):\n        return False\n    if path.startswith(\"gs://\"):\n        return False\n    if path.startswith(\"s3://\"):\n        return False\n    if path.startswith(\"hdfs://\"):\n        return False\n    if path.startswith(\"file://\"):\n        return False\n    if path.startswith(\"http://\"):\n        return False\n    if path.startswith(\"https://\"):\n        return False\n    if path.startswith(\"ftp://\"):\n        return False\n    if path.startswith(\"ftps://\"):\n        return False\n    if path.startswith(\"sftp://\"):\n        return False\n    if path.startswith(\"ssh://\"):\n        return False\n    if path.startswith(\"scp://\"):\n        return False\n    if path.startswith(\"s3a://\"):\n        return False\n    if path.startswith(\"s3n://\"):\n        return False\n    if path.startswith(\"s3p://\"):\n        return False\n    if path.startswith(\"s3r://\"):\n        return False\n    if path.startswith(\"s3k://\"):\n        return False\n    if path.startswith(\"s3m://\"):\n        return False\n    if path.startswith(\"s3v://\"):\n        return False\n    if path.startswith(\"s3y://\"):\n        return False\n    if path.startswith(\"s3z://\"):\n        return False\n    if path.startswith(\"s3x://\"):\n        return False\n    if path.startswith(\"s3w://\"):\n        return False\n    if path.startswith(\"s3d://\"):\n        return False\n    if path.startswith(\"s3e://\"):\n        return False\n    if path.startswith(\"s3f://\"):\n        return False\n   "}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"When 'items' is a dictionary, 'assets_names' must be provided.\"\n            )\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"When 'items' is a dictionary, the length of 'assets_names' must be equal to 'n_assets'.\"\n            )\n        arr = np.full(n_assets, fill_value)\n        for i, v in enumerate(assets_names):\n            if v in items:\n                arr[i] = items[v]\n            else:\n                raise ValueError(\n                    f\"When 'items' is a dictionary, the key '{v}' must be present in 'items'.\"\n                )\n    else:\n        arr = np.asarray(items)\n        if arr.ndim != dim:\n            raise ValueError(\n                f\"When 'items' is not a dictionary, the dimension of 'items' must be equal to 'dim'.\"\n            )\n        if arr.shape[0] != n_assets:\n            raise ValueError(\n                f\"When 'items' is not a dictionary, the first dimension of 'items' must be equal to 'n_assets'.\"\n            )\n    return arr"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(f\"Missing assets_names for {name}\")\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"Expected {n_assets} assets, got {len(assets_names)} for {name}\"\n            )\n        if len(items) != n_assets:\n            raise ValueError(\n                f\"Expected {n_assets} assets, got {len(items)} for {name}\"\n            )\n        arr = np.full(n_assets, fill_value=fill_value)\n        for i, name in enumerate(assets_names):\n            if name in items:\n                arr[i] = items[name]\n    else:\n        arr = np.asarray(items)\n    if arr.ndim != dim:\n        raise ValueError(\n            f\"Expected {dim}D array, got {arr.ndim}D for {name} with shape {arr.shape}\"\n        )\n    if arr.shape[0] != n_assets:\n        raise ValueError(\n            f\"Expected {n_assets} assets, got {arr.shape[0]} for {name} with shape \"\n            f\"{arr.shape}\"\n        )\n    return arr"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(f\"{name} must be a dictionary if assets_names is None\")\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"{name} must be a dictionary with {n_assets} elements, got {len(assets_names)}\"\n            )\n        if dim == 1:\n            return np.asarray([items.get(a, fill_value) for a in assets_names])\n        elif dim == 2:\n            return np.asarray([[items.get(a, fill_value) for a in assets_names]])\n        else:\n            raise ValueError(f\"Invalid dim value {dim}\")\n    elif isinstance(items, np.ndarray):\n        if items.shape == (n_assets,) and dim == 1:\n            return items\n        elif items.shape == (1, n_assets) and dim == 2:\n            return items\n        else:\n            raise ValueError(\n                f\"{name} must be a numpy array with shape ({n_assets},) or (1, {n_assets})\"\n            )\n    else:\n        raise ValueError(f\"{name} must be a numpy array or a dictionary\")"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"When 'items' is a dictionary, 'assets_names' must be provided.\"\n            )\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"The number of assets in 'assets_names' ({len(assets_names)}) must\"\n                f\" match the expected number of assets ({n_assets}).\"\n            )\n        arr = np.full(n_assets, fill_value=fill_value)\n        for i, name in enumerate(assets_names):\n            if name in items:\n                arr[i] = items[name]\n        return arr\n    if not isinstance(items, np.ndarray):\n        items = np.asarray(items)\n    if items.ndim == 1:\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"The shape of '{name}' ({items.shape}) does not match the expected\"\n                f\" shape ({n_assets},).\"\n            )\n        return items\n    if items.ndim != dim:\n        raise ValueError(\n            f\"The dimension of '{name}' ({items.ndim}) does not match the expected\"\n            f\" dimension ({dim}).\"\n        )\n    if items.shape[1] != n_assets:\n        raise ValueError(\n            f\"The shape of '{name}' ({items.shape}) does not match the expected\"\n            f\" shape ({items.shape[0], n_assets}).\"\n        )\n    return items"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(f\"{name} must be a dictionary if assets_names is None\")\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"{name} must have the same number of items as the number of assets\"\n            )\n        items_ = np.full(n_assets, fill_value)\n        for i, asset_name in enumerate(assets_names):\n            if asset_name in items:\n                items_[i] = items[asset_name]\n    else:\n        items_ = np.asarray(items)\n        if items_.ndim == 0:\n            items_ = items_.reshape(1)\n        if items_.ndim != dim:\n            raise ValueError(f\"{name} must be a {dim}D array\")\n        if items_.shape[0] != n_assets:\n            raise ValueError(\n                f\"{name} must have the same number of items as the number of assets\"\n            )\n    return items_"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"When 'items' is a dictionary, 'assets_names' must be provided.\"\n            )\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"When 'items' is a dictionary, 'assets_names' must have length equal to 'n_assets' ({n_assets}).\"\n            )\n        if len(items) != n_assets:\n            raise ValueError(\n                f\"When 'items' is a dictionary, it must have length equal to 'n_assets' ({n_assets}).\"\n            )\n        for asset_name in assets_names:\n            if asset_name not in items:\n                items[asset_name] = fill_value\n        items = np.array([items[asset_name] for asset_name in assets_names])\n    elif isinstance(items, np.ndarray):\n        if len(items.shape) != dim:\n            raise ValueError(\n                f\"When 'items' is an array, it must have {dim} dimensions (got {len(items.shape)}).\"\n            )\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"When 'items' is an array, it must have shape ({n_assets}, {items.shape[1]}) (got {items.shape}).\"\n            )\n    else:\n        raise ValueError(\n            f\"When 'items' is not a dictionary or an array, it must be array-like.\"\n        )\n    return items"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(f\"{name} must be a dictionary if assets_names is None.\")\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"{name} must have {n_assets} elements, got {len(assets_names)}.\"\n            )\n        items_array = np.full(n_assets, fill_value)\n        for i, asset_name in enumerate(assets_names):\n            if asset_name in items:\n                items_array[i] = items[asset_name]\n        if dim == 1:\n            items_array = items_array.reshape(1, n_assets)\n        elif dim != 2:\n            raise ValueError(f\"{name} must have dimension 1 or 2, got {dim}.\")\n    elif isinstance(items, np.ndarray):\n        if dim == 1:\n            if len(items.shape) != 1:\n                raise ValueError(\n                    f\"{name} must have dimension 1 if items is an array, got {len(items.shape)}.\"\n                )\n            if items.shape[0] != n_assets:\n                raise ValueError(\n                    f\"{name} must have {n_assets} elements, got {items.shape[0]}.\"\n                )\n            items_array = items.reshape(1, n_assets)\n        elif dim == 2:\n            if len(items.shape) != 2:\n                raise ValueError(\n                    f\"{name} must have dimension 2 if items is an array, got {len(items.shape)}.\"\n                )\n            if items.shape[1] != n_assets:\n                raise ValueError(\n                    f\"{name} must have {n_assets} elements, got {items.shape[1]}.\"\n                )\n            items_array = items\n        else:\n            raise ValueError(f\"{name} must have dimension 1 or 2, got {dim}.\")\n    else:\n        raise TypeError(f\"{name} must"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"When 'items' is a dictionary, 'assets_names' must be provided.\"\n            )\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"The length of 'assets_names' ({len(assets_names)}) must match the number of assets ({n_assets}).\"\n            )\n        items_array = np.full(n_assets, fill_value)\n        for i, asset_name in enumerate(assets_names):\n            if asset_name in items:\n                items_array[i] = items[asset_name]\n        return items_array\n    if isinstance(items, np.ndarray):\n        if items.shape != (n_assets,) and items.shape != (n_assets, 1):\n            raise ValueError(\n                f\"The shape of '{name}' ({items.shape}) must match the number of assets ({n_assets}).\"\n            )\n        if items.shape == (n_assets, 1):\n            items = items.flatten()\n        return items\n    raise ValueError(\n        f\"The type of '{name}' ({type(items)}) is not supported. Only dict, ndarray or array-like are allowed.\"\n    )"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is not None:\n            if len(assets_names) != n_assets:\n                raise ValueError(\n                    f\"Expected {n_assets} assets names, got {len(assets_names)}\"\n                )\n            if not all(isinstance(name, str) for name in assets_names):\n                raise ValueError(\n                    f\"Expected assets names to be a list of string, got {type(assets_names)}\"\n                )\n            if not all(name in items for name in assets_names):\n                raise ValueError(\n                    f\"Expected all assets names to be in {name}, got {set(items.keys()) - set(assets_names)}\"\n                )\n            array = np.full(n_assets, fill_value)\n            for i, name in enumerate(assets_names):\n                if name in items:\n                    array[i] = items[name]\n        else:\n            array = np.array(list(items.values()))\n    else:\n        array = np.asarray(items)\n    if dim == 1:\n        if array.ndim != 1:\n            raise ValueError(\n                f\"Expected 1D array, got {array.ndim}D array for {name} input\"\n            )\n        if array.shape[0] != n_assets:\n            raise ValueError(\n                f\"Expected {n_assets} assets, got {array.shape[0]} for {name} input\"\n            )\n    elif dim == 2:\n        if array.ndim != 2:\n            raise ValueError(\n                f\"Expected 2D array, got {array.ndim}D array for {name} input\"\n            )\n        if array.shape[1] != n_assets:\n            raise ValueError(\n                f\"Expected {n_assets} assets, got {array.shape[1]} for {name} input\"\n            )\n    else:\n        raise ValueError(f\"Expected dim=1 or dim=2, got {dim}\")\n    return array"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is not None:\n            if len(assets_names) != n_assets:\n                raise ValueError(\n                    f\"The number of assets in the dictionary ({len(assets_names)}) \"\n                    f\"does not match the expected number of assets ({n_assets}).\"\n                )\n            if not all(isinstance(k, str) for k in assets_names):\n                raise ValueError(\n                    f\"The keys of the dictionary ({assets_names}) must be strings.\"\n                )\n            if not all(k in assets_names for k in items.keys()):\n                raise ValueError(\n                    f\"The dictionary keys ({items.keys()}) must be a subset of the \"\n                    f\"expected assets names ({assets_names}).\"\n                )\n            if not all(isinstance(v, (int, float)) for v in items.values()):\n                raise ValueError(\n                    f\"The values of the dictionary ({items.values()}) must be \"\n                    f\"numeric.\"\n                )\n            if not all(isinstance(v, (int, float)) for v in items.values()):\n                raise ValueError(\n                    f\"The values of the dictionary ({items.values()}) must be \"\n                    f\"numeric.\"\n                )\n            x = np.full(n_assets, fill_value=fill_value)\n            for k, v in items.items():\n                x[assets_names == k] = v\n        else:\n            if not all(isinstance(v, (int, float)) for v in items.values()):\n                raise ValueError(\n                    f\"The values of the dictionary ({items.values()}) must be \"\n                    f\"numeric.\"\n                )\n            x = np.array(list(items.values()))\n    elif isinstance(items, np.ndarray):\n        if len(items.shape) != dim:\n            raise ValueError(\n                f\"The dimension of the array ({len(items.shape)}) does not match \"\n                f\"the expected dimension ({dim}).\"\n            )"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is not None:\n            if len(assets_names) != n_assets:\n                raise ValueError(\n                    f\"The number of assets ({len(assets_names)}) must match the number of\"\n                    f\" assets in '{name}' ({n_assets})\"\n                )\n            items_array = np.full(n_assets, fill_value=fill_value)\n            for i, asset_name in enumerate(assets_names):\n                if asset_name in items:\n                    items_array[i] = items[asset_name]\n        else:\n            items_array = np.array(list(items.values()))\n    else:\n        items_array = np.asarray(items)\n    if items_array.shape != (n_assets,) and items_array.shape != (n_assets, 1):\n        raise ValueError(\n            f\"The shape of '{name}' must be ({n_assets},) or ({n_assets}, 1), got \"\n            f\"{items_array.shape}\"\n        )\n    if dim == 1:\n        return items_array.ravel()\n    elif dim == 2:\n        return items_array\n    else:\n        raise ValueError(\n            f\"The dimension of '{name}' must be either 1 or 2, got {dim}\"\n        )"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"When 'items' is a dictionary, 'assets_names' must be provided.\"\n            )\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"The number of assets in 'assets_names' ({len(assets_names)}) must\"\n                f\" match the expected number of assets ({n_assets}).\"\n            )\n        if len(items) < n_assets:\n            raise ValueError(\n                f\"The number of items in 'items' ({len(items)}) must match the\"\n                f\" expected number of assets ({n_assets}).\"\n            )\n        if any(k not in assets_names for k in items.keys()):\n            raise ValueError(\n                f\"The keys of 'items' must be a subset of 'assets_names'.\"\n            )\n        x = np.full(n_assets, fill_value, dtype=np.float64)\n        for i, k in enumerate(assets_names):\n            if k in items:\n                x[i] = items[k]\n    else:\n        x = np.asarray(items)\n        if x.ndim == 1:\n            if x.shape[0] != n_assets:\n                raise ValueError(\n                    f\"The shape of '{name}' ({x.shape}) must match the expected\"\n                    f\" number of assets ({n_assets}).\"\n                )\n        elif x.ndim == 2:\n            if x.shape[1] != n_assets:\n                raise ValueError(\n                    f\"The shape of '{name}' ({x.shape}) must match the expected\"\n                    f\" number of assets ({n_assets}).\"\n                )\n        else:\n            raise ValueError(\n                f\"The dimension of '{name}' ({x.ndim}) must be either 1 or 2.\"\n            )\n    if dim == 1:\n        x = x.reshape((n_assets,))\n    return x"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"If 'items' is a dictionary, 'assets_names' must be provided.\"\n            )\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"If 'items' is a dictionary, the length of 'assets_names' must be equal to 'n_assets' ({n_assets}).\"\n            )\n        if not all(isinstance(v, (int, float)) for v in items.values()):\n            raise ValueError(\n                f\"If 'items' is a dictionary, all values must be numerical (int or float).\"\n            )\n        if not all(isinstance(k, str) for k in items.keys()):\n            raise ValueError(\n                f\"If 'items' is a dictionary, all keys must be strings.\"\n            )\n        if not all(k in assets_names for k in items.keys()):\n            raise ValueError(\n                f\"If 'items' is a dictionary, all keys must be in 'assets_names'.\"\n            )\n        if dim == 1:\n            return np.array([items.get(k, fill_value) for k in assets_names])\n        else:\n            return np.array([items.get(k, fill_value) for k in assets_names]).reshape(\n                1, -1\n            )\n    elif isinstance(items, np.ndarray):\n        if dim == 1:\n            if items.shape != (n_assets,):\n                raise ValueError(\n                    f\"If 'items' is an array, 'items.shape' must be equal to '(n_assets,)' ({n_assets}).\"\n                )\n            return items\n        else:\n            if items.shape != (1, n_assets):\n                raise ValueError(\n                    f\"If 'items' is an array, 'items.shape' must be equal to '(1, n_assets)' ({n_assets}).\"\n                )\n            return items[0]\n    else:\n        raise TypeError("}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\"assets_names must be provided when items is a dictionary\")\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"Number of assets in 'assets_names' ({len(assets_names)}) must match the\"\n                f\" number of assets in 'items' ({n_assets})\"\n            )\n        if dim == 1:\n            x = np.full(n_assets, fill_value)\n            for i, key in enumerate(assets_names):\n                if key in items:\n                    x[i] = items[key]\n        else:\n            x = np.full((n_assets, 1), fill_value)\n            for i, key in enumerate(assets_names):\n                if key in items:\n                    x[i, 0] = items[key]\n    else:\n        x = np.asarray(items)\n        if x.ndim != dim:\n            raise ValueError(f\"Expected {dim}D array, got {x.ndim}D array\")\n        if dim == 1:\n            if x.shape[0] != n_assets:\n                raise ValueError(\n                    f\"Expected {n_assets} items in array, got {x.shape[0]} items in array\"\n                )\n        else:\n            if x.shape[1] != n_assets:\n                raise ValueError(\n                    f\"Expected {n_assets} items in array, got {x.shape[1]} items in array\"\n                )\n\n    return x"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"The argument '{name}' is a dictionary, you must provide a list of\"\n                \" assets names with the argument 'assets_names'\"\n            )\n        if not isinstance(assets_names, np.ndarray):\n            raise ValueError(\n                f\"The argument '{name}' is a dictionary, the argument 'assets_names'\"\n                \" must be a numpy array\"\n            )\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"The argument '{name}' is a dictionary, the number of assets in the\"\n                f\" argument 'assets_names' ({len(assets_names)}) must be equal to the\"\n                f\" argument 'n_assets' ({n_assets})\"\n            )\n        if dim == 1:\n            if len(assets_names) != n_assets:\n                raise ValueError(\n                    f\"The argument '{name}' is a dictionary, the number of assets in\"\n                    f\" the argument 'assets_names' ({len(assets_names)}) must be equal\"\n                    f\" to the argument 'n_assets' ({n_assets})\"\n                )\n            x = np.full(n_assets, fill_value)\n            for k, v in items.items():\n                idx = np.where(assets_names == k)[0]\n                if len(idx) == 0:\n                    raise ValueError(\n                        f\"The argument '{name}' is a dictionary, the asset name '{k}'\"\n                        \" is not in the argument 'assets_names'\"\n                    )\n                x[idx] = v\n        else:\n            if len(assets_names) != n_assets:\n                raise ValueError(\n                    f\"The argument '{name}' is a dictionary, the number of assets in\"\n                    f\" the argument 'assets_names' ({len(assets_names)}) must be equal\"\n                    f\" to the argument 'n_assets' ({n_assets})\"\n                )\n            x = np.full(("}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(f\"{name} must be a dictionary with keys corresponding to asset names.\")\n        if not isinstance(assets_names, np.ndarray):\n            assets_names = np.asarray(assets_names)\n        if not np.all(np.isin(assets_names, list(items.keys()))):\n            raise ValueError(f\"{name} must be a dictionary with keys corresponding to asset names.\")\n        if len(assets_names) != n_assets:\n            raise ValueError(f\"{name} must have {n_assets} assets, got {len(assets_names)}.\")\n        x = np.full(n_assets, fill_value, dtype=object)\n        for i, name in enumerate(assets_names):\n            if name in items:\n                x[i] = items[name]\n            else:\n                x[i] = fill_value\n        if dim == 2:\n            x = np.expand_dims(x, axis=0)\n    elif isinstance(items, np.ndarray):\n        if not np.issubdtype(items.dtype, np.number):\n            raise ValueError(f\"{name} must be a numeric array.\")\n        if items.ndim != dim:\n            raise ValueError(f\"{name} must have {dim} dimensions, got {items.ndim}.\")\n        if items.shape[0] != n_assets:\n            raise ValueError(f\"{name} must have {n_assets} assets, got {items.shape[0]}.\")\n        x = items\n    else:\n        raise ValueError(f\"{name} must be a dictionary or a numeric array.\")\n    return x"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(f\"{name} must be a dictionary when assets_names is None\")\n        if not all(isinstance(k, str) for k in items.keys()):\n            raise ValueError(\n                f\"{name} keys must be of type str, got {type(list(items.keys())[0])}\"\n            )\n        if not all(isinstance(v, (np.ndarray, list, tuple)) for v in items.values()):\n            raise ValueError(\n                f\"{name} values must be of type ndarray, list or tuple, got \"\n                f\"{type(list(items.values())[0])}\"\n            )\n        if len(items) != len(assets_names):\n            raise ValueError(\n                f\"{name} must have the same number of items as assets_names, \"\n                f\"got {len(items)} and {len(assets_names)}\"\n            )\n        if dim == 1:\n            items_array = np.full((n_assets,), fill_value=fill_value)\n            for i, name in enumerate(assets_names):\n                if name in items:\n                    items_array[i] = items[name]\n        elif dim == 2:\n            items_array = np.full((n_assets, len(items)), fill_value=fill_value)\n            for i, name in enumerate(assets_names):\n                if name in items:\n                    items_array[i] = items[name]\n        else:\n            raise ValueError(f\"dim must be 1 or 2, got {dim}\")\n    elif isinstance(items, (np.ndarray, list, tuple)):\n        if dim == 1:\n            if len(items) != n_assets:\n                raise ValueError(\n                    f\"{name} must have the same number of items as n_assets, \"\n                    f\"got {len(items)} and {n_assets}\"\n                )\n            items_array = np.array(items)\n        elif dim == 2"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is not None:\n            if len(assets_names) != n_assets:\n                raise ValueError(\n                    f\"The length of 'assets_names' ({len(assets_names)}) must be equal to the number of assets ({n_assets})\"\n                )\n            if not all(isinstance(item, (int, str)) for item in assets_names):\n                raise TypeError(\n                    f\"'assets_names' must be an array of strings or integers, got {type(assets_names)}\"\n                )\n            if not all(isinstance(item, str) for item in assets_names):\n                assets_names = np.asarray(assets_names, dtype=object)\n            if not all(isinstance(key, str) for key in items.keys()):\n                items = {str(key): value for key, value in items.items()}\n            if not all(key in assets_names for key in items.keys()):\n                raise ValueError(\n                    f\"The keys of 'items' ({list(items.keys())}) must be in 'assets_names' ({list(assets_names)})\"\n                )\n            if not all(\n                isinstance(value, (int, float, np.number)) for value in items.values()\n            ):\n                raise TypeError(\n                    f\"'items' must be a dictionary of numbers, got {type(items)}\"\n                )\n            items = np.full(n_assets, fill_value, dtype=float)\n            items[np.isin(assets_names, list(items.keys()))] = list(items.values())\n        else:\n            if not all(\n                isinstance(key, (int, str)) for key in items.keys()\n            ) or not all(\n                isinstance(value, (int, float, np.number)) for value in items.values()\n            ):\n                raise TypeError(\n                    f\"'items' must be a dictionary of numbers, got {type(items)}\"\n                )\n            items = np.fromiter(items.values(), dtype="}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"Assets names must be provided when {name} is a dictionary\"\n            )\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"Number of assets names must match number of assets, got {len(assets_names)} != {n_assets}\"\n            )\n        if len(items) != n_assets:\n            raise ValueError(\n                f\"Number of assets names must match number of assets, got {len(items)} != {n_assets}\"\n            )\n        if not np.all([isinstance(k, str) for k in items.keys()]):\n            raise ValueError(\n                f\"Keys of {name} must be strings, got {[type(k) for k in items.keys()]}\"\n            )\n        if not np.all([isinstance(v, np.ndarray) for v in items.values()]):\n            raise ValueError(\n                f\"Values of {name} must be arrays, got {[type(v) for v in items.values()]}\"\n            )\n        if not np.all([v.shape == (n_assets,) for v in items.values()]):\n            raise ValueError(\n                f\"Values of {name} must be arrays of shape (n_assets,), got {[v.shape for v in items.values()]}\"\n            )\n        if not np.all([k in assets_names for k in items.keys()]):\n            raise ValueError(\n                f\"Keys of {name} must be in assets names, got {[k for k in items.keys() if k not in assets_names]}\"\n            )\n        return np.array([items[k] for k in assets_names]).reshape(\n            (n_assets,) if dim == 1 else (1, n_assets)\n        )\n    else:\n        if not isinstance(items, np.ndarray):\n            raise ValueError(\n                f\"{name} must be a dictionary or an array-like, got {type("}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is not None:\n            if len(assets_names) != n_assets:\n                raise ValueError(\n                    f\"Expected {n_assets} assets, got {len(assets_names)}\"\n                )\n            if not all(isinstance(x, str) for x in assets_names):\n                raise ValueError(\"Expected string assets names\")\n            if not all(isinstance(x, str) for x in items.keys()):\n                raise ValueError(\"Expected string assets names\")\n            if not all(x in assets_names for x in items.keys()):\n                raise ValueError(f\"{name} contains unknown assets names\")\n            # noinspection PyTypeChecker\n            items = np.array([items.get(x, fill_value) for x in assets_names])\n        else:\n            raise ValueError(\"assets_names must be provided when items is a dictionary\")\n    elif isinstance(items, np.ndarray):\n        if items.shape == (n_assets,):\n            pass\n        elif items.shape == (n_assets, 1):\n            items = items.squeeze(axis=1)\n        elif items.shape == (1, n_assets):\n            items = items.squeeze(axis=0)\n        else:\n            raise ValueError(f\"Expected shape ({n_assets},), got {items.shape}\")\n    else:\n        items = np.asarray(items)\n        if items.shape == (n_assets,):\n            pass\n        elif items.shape == (n_assets, 1):\n            items = items.squeeze(axis=1)\n        elif items.shape == (1, n_assets):\n            items = items.squeeze(axis=0)\n        else:\n            raise ValueError(f\"Expected shape ({n_assets},), got {items.shape}\")\n    if dim == 1:\n        if items.shape != (n_assets,):\n            raise ValueError(f\"Expected shape ({n_assets},), got {items.shape}\")\n    elif dim == "}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": ""}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": ""}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": ""}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\")\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        return MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\"),\n            purpose=data.get(\"purpose\"),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\"),\n            max_depth=data.get(\"max_depth\"),\n            usage_count=data.get(\"usage_count\"),\n            id=data.get(\"id\"),\n            parent_id=data.get(\"parent_id\"),\n            working_agent=data.get(\"working_agent\"),\n            is_prime=data.get(\"is_prime\"),\n            evolve_count=data.get(\"evolve_count\"),\n            number_of_code_executions=data.get(\"number_of_code_executions\"),\n            last_input=data.get(\"last_input\"),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\")\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        return MicroAgent(\n            dynamic_prompt=data[\"dynamic_prompt\"],\n            purpose=data[\"purpose\"],\n            purpose_embedding=purpose_embedding,\n            depth=data[\"depth\"],\n            max_depth=data[\"max_depth\"],\n            usage_count=data[\"usage_count\"],\n            id=data[\"id\"],\n            parent_id=data[\"parent_id\"],\n            working_agent=data[\"working_agent\"],\n            is_prime=data[\"is_prime\"],\n            evolve_count=data[\"evolve_count\"],\n            number_of_code_executions=data[\"number_of_code_executions\"],\n            last_input=data[\"last_input\"],\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\")\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        agent = MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\"),\n            purpose=data.get(\"purpose\"),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\"),\n            max_depth=data.get(\"max_depth\"),\n            usage_count=data.get(\"usage_count\"),\n            id=data.get(\"id\"),\n            parent_id=data.get(\"parent_id\"),\n            working_agent=data.get(\"working_agent\"),\n            is_prime=data.get(\"is_prime\"),\n            evolve_count=data.get(\"evolve_count\"),\n            number_of_code_executions=data.get(\"number_of_code_executions\"),\n            last_input=data.get(\"last_input\"),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n        return agent"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\")\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        return MicroAgent(\n            data.get(\"dynamic_prompt\"),\n            data.get(\"purpose\"),\n            purpose_embedding,\n            data.get(\"depth\"),\n            data.get(\"max_depth\"),\n            data.get(\"usage_count\"),\n            data.get(\"id\"),\n            data.get(\"parent_id\"),\n            data.get(\"working_agent\"),\n            data.get(\"is_prime\"),\n            data.get(\"evolve_count\"),\n            data.get(\"number_of_code_executions\"),\n            data.get(\"last_input\"),\n            agent_lifecycle,\n            openai_wrapper,\n        )"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        dynamic_prompt = data.get(\"dynamic_prompt\", None)\n        purpose = data.get(\"purpose\", None)\n        purpose_embedding = data.get(\"purpose_embedding\", None)\n        depth = data.get(\"depth\", 0)\n        max_depth = data.get(\"max_depth\", 0)\n        usage_count = data.get(\"usage_count\", 0)\n        id = data.get(\"id\", None)\n        parent_id = data.get(\"parent_id\", None)\n        working_agent = data.get(\"working_agent\", None)\n        is_prime = data.get(\"is_prime\", False)\n        evolve_count = data.get(\"evolve_count\", 0)\n        number_of_code_executions = data.get(\"number_of_code_executions\", 0)\n        last_input = data.get(\"last_input\", None)\n\n        return MicroAgent(\n            dynamic_prompt=dynamic_prompt,\n            purpose=purpose,\n            purpose_embedding=purpose_embedding,\n            depth=depth,\n            max_depth=max_depth,\n            usage_count=usage_count,\n            id=id,\n            parent_id=parent_id,\n            working_agent=working_agent,\n            is_prime=is_prime,\n            evolve_count=evolve_count,\n            number_of_code_executions=number_of_code_executions,\n            last_input=last_input,\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        dynamic_prompt = data.get(\"dynamic_prompt\", None)\n        purpose = data.get(\"purpose\", None)\n        purpose_embedding = data.get(\"purpose_embedding\", None)\n        depth = data.get(\"depth\", 0)\n        max_depth = data.get(\"max_depth\", 0)\n        usage_count = data.get(\"usage_count\", 0)\n        id = data.get(\"id\", None)\n        parent_id = data.get(\"parent_id\", None)\n        working_agent = data.get(\"working_agent\", None)\n        is_prime = data.get(\"is_prime\", False)\n        evolve_count = data.get(\"evolve_count\", 0)\n        number_of_code_executions = data.get(\"number_of_code_executions\", 0)\n        last_input = data.get(\"last_input\", None)\n\n        return MicroAgent(\n            dynamic_prompt=dynamic_prompt,\n            purpose=purpose,\n            purpose_embedding=purpose_embedding,\n            depth=depth,\n            max_depth=max_depth,\n            usage_count=usage_count,\n            id=id,\n            parent_id=parent_id,\n            working_agent=working_agent,\n            is_prime=is_prime,\n            evolve_count=evolve_count,\n            number_of_code_executions=number_of_code_executions,\n            last_input=last_input,\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\", None)\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        return MicroAgent(\n            dynamic_prompt=data[\"dynamic_prompt\"],\n            purpose=data[\"purpose\"],\n            purpose_embedding=purpose_embedding,\n            depth=data[\"depth\"],\n            max_depth=data[\"max_depth\"],\n            usage_count=data[\"usage_count\"],\n            id=data[\"id\"],\n            parent_id=data[\"parent_id\"],\n            working_agent=data[\"working_agent\"],\n            is_prime=data[\"is_prime\"],\n            evolve_count=data[\"evolve_count\"],\n            number_of_code_executions=data[\"number_of_code_executions\"],\n            last_input=data[\"last_input\"],\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\", None)\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        return MicroAgent(\n            dynamic_prompt=data[\"dynamic_prompt\"],\n            purpose=data[\"purpose\"],\n            purpose_embedding=purpose_embedding,\n            depth=data[\"depth\"],\n            max_depth=data[\"max_depth\"],\n            usage_count=data[\"usage_count\"],\n            id=data[\"id\"],\n            parent_id=data[\"parent_id\"],\n            working_agent=data[\"working_agent\"],\n            is_prime=data[\"is_prime\"],\n            evolve_count=data[\"evolve_count\"],\n            number_of_code_executions=data[\"number_of_code_executions\"],\n            last_input=data[\"last_input\"],\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\")\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        return MicroAgent(\n            dynamic_prompt=data[\"dynamic_prompt\"],\n            purpose=data[\"purpose\"],\n            purpose_embedding=purpose_embedding,\n            depth=data[\"depth\"],\n            max_depth=data[\"max_depth\"],\n            usage_count=data[\"usage_count\"],\n            id=data[\"id\"],\n            parent_id=data[\"parent_id\"],\n            working_agent=data[\"working_agent\"],\n            is_prime=data[\"is_prime\"],\n            evolve_count=data[\"evolve_count\"],\n            number_of_code_executions=data[\"number_of_code_executions\"],\n            last_input=data[\"last_input\"],\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        dynamic_prompt = data.get(\"dynamic_prompt\", None)\n        purpose = data.get(\"purpose\", None)\n        purpose_embedding = data.get(\"purpose_embedding\", None)\n        depth = data.get(\"depth\", None)\n        max_depth = data.get(\"max_depth\", None)\n        usage_count = data.get(\"usage_count\", None)\n        id = data.get(\"id\", None)\n        parent_id = data.get(\"parent_id\", None)\n        working_agent = data.get(\"working_agent\", None)\n        is_prime = data.get(\"is_prime\", None)\n        evolve_count = data.get(\"evolve_count\", None)\n        number_of_code_executions = data.get(\"number_of_code_executions\", None)\n        last_input = data.get(\"last_input\", None)\n\n        return MicroAgent(\n            dynamic_prompt=dynamic_prompt,\n            purpose=purpose,\n            purpose_embedding=purpose_embedding,\n            depth=depth,\n            max_depth=max_depth,\n            usage_count=usage_count,\n            id=id,\n            parent_id=parent_id,\n            working_agent=working_agent,\n            is_prime=is_prime,\n            evolve_count=evolve_count,\n            number_of_code_executions=number_of_code_executions,\n            last_input=last_input,\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\")\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        return MicroAgent(\n            data[\"dynamic_prompt\"],\n            data[\"purpose\"],\n            purpose_embedding,\n            data[\"depth\"],\n            data[\"max_depth\"],\n            data[\"usage_count\"],\n            data[\"id\"],\n            data[\"parent_id\"],\n            data[\"working_agent\"],\n            data[\"is_prime\"],\n            data[\"evolve_count\"],\n            data[\"number_of_code_executions\"],\n            data[\"last_input\"],\n            agent_lifecycle,\n            openai_wrapper,\n        )\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\")\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list back to ndarray\n\n        return MicroAgent(\n            data.get(\"dynamic_prompt\", \"\"),\n            data.get(\"purpose\", \"\"),\n            purpose_embedding,\n            data.get(\"depth\", 0),\n            data.get(\"max_depth\", 0),\n            data.get(\"usage_count\", 0),\n            data.get(\"id\", None),\n            data.get(\"parent_id\", None),\n            data.get(\"working_agent\", None),\n            data.get(\"is_prime\", False),\n            data.get(\"evolve_count\", 0),\n            data.get(\"number_of_code_executions\", 0),\n            data.get(\"last_input\", None),\n            agent_lifecycle,\n            openai_wrapper,\n        )"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\")\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        return MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\"),\n            purpose=data.get(\"purpose\"),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\"),\n            max_depth=data.get(\"max_depth\"),\n            usage_count=data.get(\"usage_count\"),\n            id=data.get(\"id\"),\n            parent_id=data.get(\"parent_id\"),\n            working_agent=data.get(\"working_agent\"),\n            is_prime=data.get(\"is_prime\"),\n            evolve_count=data.get(\"evolve_count\"),\n            number_of_code_executions=data.get(\"number_of_code_executions\"),\n            last_input=data.get(\"last_input\"),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\", None)\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)\n\n        return MicroAgent(\n            data.get(\"dynamic_prompt\", None),\n            data.get(\"purpose\", None),\n            purpose_embedding,\n            data.get(\"depth\", None),\n            data.get(\"max_depth\", None),\n            data.get(\"usage_count\", None),\n            data.get(\"id\", None),\n            data.get(\"parent_id\", None),\n            data.get(\"working_agent\", None),\n            data.get(\"is_prime\", None),\n            data.get(\"evolve_count\", None),\n            data.get(\"number_of_code_executions\", None),\n            data.get(\"last_input\", None),\n            agent_lifecycle,\n            openai_wrapper,\n        )\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\")\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        return MicroAgent(\n            data.get(\"dynamic_prompt\", \"\"),\n            data.get(\"purpose\", \"\"),\n            purpose_embedding,\n            data.get(\"depth\", 0),\n            data.get(\"max_depth\", 10),\n            data.get(\"usage_count\", 0),\n            data.get(\"id\", None),\n            data.get(\"parent_id\", None),\n            data.get(\"working_agent\", None),\n            data.get(\"is_prime\", False),\n            data.get(\"evolve_count\", 0),\n            data.get(\"number_of_code_executions\", 0),\n            data.get(\"last_input\", None),\n            agent_lifecycle,\n            openai_wrapper,\n        )\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\")\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        return MicroAgent(\n            dynamic_prompt=data[\"dynamic_prompt\"],\n            purpose=data[\"purpose\"],\n            purpose_embedding=purpose_embedding,\n            depth=data[\"depth\"],\n            max_depth=data[\"max_depth\"],\n            usage_count=data[\"usage_count\"],\n            id=data[\"id\"],\n            parent_id=data[\"parent_id\"],\n            working_agent=data[\"working_agent\"],\n            is_prime=data[\"is_prime\"],\n            evolve_count=data[\"evolve_count\"],\n            number_of_code_executions=data[\"number_of_code_executions\"],\n            last_input=data[\"last_input\"],\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\")\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        return MicroAgent(\n            data[\"dynamic_prompt\"],\n            data[\"purpose\"],\n            purpose_embedding,\n            data[\"depth\"],\n            data[\"max_depth\"],\n            data[\"usage_count\"],\n            data[\"id\"],\n            data[\"parent_id\"],\n            data[\"working_agent\"],\n            data[\"is_prime\"],\n            data[\"evolve_count\"],\n            data[\"number_of_code_executions\"],\n            data[\"last_input\"],\n            agent_lifecycle,\n            openai_wrapper,\n        )\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb / 12.92\n  linear1 = xnp.power((srgb + 0.055) / 1.055, 2.4)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = 25 * srgb / 323\n  linear1 = (200 * srgb + 11) / 211\n  return xnp.where(srgb <= 0.04045, linear0, linear1)"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = 0.055 * (xnp.maximum(eps, srgb) ** (1 / 2.4) - 0.055)\n  linear1 = (srgb + 0.055) / 1.055\n  linear1 = linear1 ** (1 / 2.4)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear = xnp.where(srgb <= 0.04045, srgb / 12.92, ((srgb + 0.055) / 1.055) ** 2.4)\n  return linear\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = 25 * srgb / 323\n  linear1 = (200 * srgb + 11 * eps) / (211 + eps)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  linear = xnp.where(srgb <= 0.04045, srgb / 12.92, ((srgb + 0.055) / 1.055) ** 2.4)\n  return linear"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb * 12.92\n  linear1 = xnp.power((srgb + 0.055) / 1.055, 2.4)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb * 12.92\n  linear1 = 1.055 * xnp.power(xnp.maximum(eps, srgb), 1 / 2.4) - 0.055\n  return xnp.where(srgb <= 0.04045, linear0, linear1)\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb * 25 / 323\n  linear1 = 200 * (srgb + 11) / 211\n  return xnp.where(srgb <= 0.04045, linear0, linear1)"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear = xnp.where(srgb <= 0.04045, srgb / 12.92, ((srgb + 0.055) / 1.055) ** 2.4)\n  return linear\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = 255 * (srgb / 255) ** 2.4\n  linear1 = (255 * (srgb + 15 / 128) / 255) ** 2.4\n  return xnp.where(srgb <= 0.04045, linear0, linear1)"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb * 12.92\n  linear1 = 1.055 * xnp.power((srgb + 0.055) / 1.055, 2.4)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  linear0 = 4095 / 1023 * srgb\n  linear1 = (2048 * xnp.maximum(eps, srgb) ** (12 / 11) - 512) / 1023\n  return xnp.where(srgb <= 0.04045, linear0, linear1)\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb / 12.92\n  linear1 = xnp.power((srgb + 0.055) / 1.055, 2.4)\n  linear = xnp.where(srgb <= 0.04045, linear0, linear1)\n  return linear\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  linear = xnp.where(srgb <= 0.04045, srgb / 12.92, ((srgb + 0.055) / 1.055) ** 2.4)\n  return linear\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  # Assumes srgb is in [0, 1], see https://en.wikipedia.org/wiki/SRGB.\n  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  linear0 = 0.07739938080495356 * srgb\n  linear1 = (srgb + 0.055) / 1.055\n  linear1 = linear1 ** 2.4\n  linear1 = (linear1 * 12.92) if (srgb <= 0.04045) else (linear1 * 1.055 - 0.055)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = 0.055 * (xnp.maximum(eps, srgb) ** (2.4) - 0.055)\n  linear1 = (srgb + 0.055) / 1.055\n  linear1 = (1000 * xnp.maximum(eps, linear1)) ** (2.4)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  # Check if the input data type is float32\n  if xnp.float32 != srgb.dtype:\n    raise TypeError(\n        f'Expected input data type to be float32, but got {srgb.dtype}'\n    )\n\n  # Set the epsilon value if not provided\n  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n\n  # Define the piecewise function for linear conversion\n  def linear_function(x):\n    return 12.92 * x if x <= 0.04045 else (1.055 * x ** (1 / 2.4) - 0.055)\n\n  # Convert the input values to linear space\n  linear = xnp.where(\n      srgb <= 0.04045,\n      srgb / 12.92,\n      ((srgb + 0.055) / 1.055) ** 2.4,\n  )\n\n  return linear\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear = xnp.where(srgb <= 0.04045, srgb / 12.92, ((srgb + 0.055) / 1.055) ** 2.4)\n  return linear\n\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = 0.055 * (srgb + 0.055) ** (1 / 2.4)\n  linear1 = srgb / 12.92\n  linear = xnp.where(srgb <= 0.04045, linear1, linear0)\n  return linear\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  tck, u_keyframes = scipy.interpolate.splprep(x.T, k=spline_degree, s=smoothness, per=False)\n  new_x = np.array(scipy.interpolate.splev(t_output, tck))\n  return new_x"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  tck, u_keyframes = scipy.interpolate.splprep(x.T, k=spline_degree, s=smoothness)\n  new_x = scipy.interpolate.splev(t_output, tck)\n  return np.array(new_x).T"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  tck, u_keyframes = scipy.interpolate.splprep(t_input, u=t_output, k=spline_degree, s=smoothness)\n  new_x = scipy.interpolate.splev(t_output, tck)\n  return new_x"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  tck, u_keyframes = scipy.interpolate.splprep(t_input, s=smoothness, k=spline_degree)\n  t_output = scipy.interpolate.splev(t_output, tck)\n  return t_output"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  tck, u_keyframes = scipy.interpolate.splprep(t_input, s=smoothness, k=spline_degree, per=periodic)\n  new_t_output = scipy.interpolate.splev(t_output, tck)\n  return new_t_output"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  tck, u_keyframes = scipy.interpolate.splprep(\n      x.T,\n      k=spline_degree,\n      s=smoothness,\n      per=periodic,\n  )\n  return scipy.interpolate.splev(t_output, tck)"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Adapted from https://github.com/google/nerfies/blob/main/nerfies/camera.py\n  tck, u_keyframes = scipy.interpolate.splprep(\n      t_input.T, k=spline_degree, s=smoothness, per=False\n  )\n  t_output = np.linspace(t_input[0], t_input[-1], len(t_output), endpoint=True)\n  return scipy.interpolate.splev(t_output, tck)"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Convert input and output times to numpy arrays.\n  t_input = np.array(t_input)\n  t_output = np.array(t_output)\n\n  # Compute the spline coefficients.\n  spline_coeffs = scipy.interpolate.splrep(t_input, x, k=spline_degree, s=smoothness)\n\n  # Evaluate the spline at the output times.\n  x_interp = scipy.interpolate.splev(t_output, spline_coeffs)\n\n  return x_interp"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Get the spline coefficients.\n  tck, u_keyframes = scipy.interpolate.splprep(x.T, k=spline_degree, s=smoothness)\n\n  # Interpolate the spline at the desired output times.\n  y = scipy.interpolate.splev(t_output, tck)\n\n  # Return the interpolated values.\n  return np.array(y).T"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Convert input to numpy arrays.\n  x = np.array(x)\n  t_input = np.array(t_input)\n  t_output = np.array(t_output)\n\n  # Check for invalid input.\n  if x.ndim != 1:\n    raise ValueError('x must be a 1-dimensional array.')\n  if t_input.ndim != 1:\n    raise ValueError('t_input must be a 1-dimensional array.')\n  if t_output.ndim != 1:\n    raise ValueError('t_output must be a 1-dimensional array.')\n  if len(x) != len(t_input):\n    raise ValueError('x and t_input must have the same length.')\n\n  # Compute the spline interpolation.\n  tck, u_keyframes = scipy.interpolate.splprep(x.T, k=spline_degree, s=smoothness)\n  new_x = np.array(scipy.interpolate.splev(t_output, tck))\n\n  return new_x.T"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  if len(x.shape) != 1:\n    raise ValueError('x must be a 1-dimensional array.')\n\n  if len(t_input) != len(x):\n    raise ValueError('t_input must have the same length as x.')\n\n  if len(t_output) != len(x):\n    raise ValueError('t_output must have the same length as x.')\n\n  if spline_degree >= len(x):\n    raise ValueError('spline_degree must be less than the length of x.')\n\n  tck, u_keyframes = scipy.interpolate.splprep(x, k=spline_degree, s=smoothness)\n  y = scipy.interpolate.splev(t_output, tck)\n\n  return y"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # We need to use the same spline interpolation code as in\n  # generate_interpolated_path().\n  def interp(points, u, k, s):\n    \"\"\"Runs multidimensional B-spline interpolation on the input points.\"\"\"\n    sh = points.shape\n    pts = np.reshape(points, (sh[0], -1))\n    k = min(k, sh[0] - 1)\n    tck, u_keyframes = scipy.interpolate.splprep(pts.T, k=k, s=s, per=False)\n    new_points = np.array(scipy.interpolate.splev(u, tck))\n    new_points = np.reshape(new_points.T, (len(u), sh[1], sh[2]))\n    return new_points, u_keyframes\n\n  points = np.array(x)\n  u = np.array(t_input)\n  new_points, _ = interp(points, u=u, k=spline_degree, s=smoothness)\n  return new_points[Ellipsis, 0]"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Convert input times to indices.\n  t_input_indices = np.round(t_input * (len(x) - 1)).astype(int)\n  t_output_indices = np.round(t_output * (len(x) - 1)).astype(int)\n\n  # Create a spline interpolator.\n  tck, u_keyframes = scipy.interpolate.splprep(\n      x.T, k=spline_degree, s=smoothness, per=False\n  )\n\n  # Interpolate the signal at the output times.\n  t_output_indices = np.clip(t_output_indices, 0, len(x) - 1)\n  t_output_indices = np.unique(t_output_indices)\n  t_output_indices = np.sort(t_output_indices)\n  t_output_indices = np.concatenate([t_output_indices, [len(x) - 1]])\n  t_output_indices = np.concatenate([[0], t_output_indices])\n  t_output_indices = np.unique(t_output_indices)\n  t_output_indices = np.sort(t_output_indices)\n\n  t_output_indices = t_output_indices[t_output_indices < len(x) - 1]\n  t_output_indices = t_output_indices[t_output_indices > 0]\n\n  t_output_indices = np.concatenate([t_output_indices, [len(x) - 1]])\n  t_output_indices = np.concatenate([[0], t_output_indices])\n\n  t_output_indices = np.unique(t_output_indices)\n  t_output_indices = np.sort(t_output_indices)\n\n  return scipy.interpolate.splev(t_output_indices, tck)"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Convert t_input and t_output to arrays if they are scalars.\n  if np.isscalar(t_input):\n    t_input = np.array([t_input])\n  if np.isscalar(t_output):\n    t_output = np.array([t_output])\n\n  # Convert x to a 2D array if it is a 1D array.\n  if len(x.shape) == 1:\n    x = x[:, None]\n\n  # Convert t_input and t_output to 1D arrays if they are 2D arrays.\n  if len(t_input.shape) == 2:\n    t_input = t_input.flatten()\n  if len(t_output.shape) == 2:\n    t_output = t_output.flatten()\n\n  # Check if the input and output arrays are sorted.\n  if not np.all(np.diff(t_input) >= 0):\n    raise ValueError('t_input must be sorted in ascending order.')\n  if not np.all(np.diff(t_output) >= 0):\n    raise ValueError('t_output must be sorted in ascending order.')\n\n  # Compute the spline coefficients.\n  spline_degree = min(spline_degree, x.shape[0] - 1)\n  tck, u = scipy.interpolate.splprep(x.T, k=spline_degree, s=smoothness)\n\n  # Interpolate the signal at the output times.\n  y = scipy.interpolate.splev(t_output, tck)\n\n  return y.T"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  if t_input.shape[0] < 3:\n    raise ValueError('Input array t_input must have at least 3 elements.')\n  if t_input.shape[0] != t_input.size:\n    raise ValueError('Input array t_input must be 1-dimensional.')\n  if t_output.shape[0] != t_output.size:\n    raise ValueError('Input array t_output must be 1-dimensional.')\n  if t_output.shape[0] < 1:\n    raise ValueError('Input array t_output must have at least 1 element.')\n  if spline_degree < 1:\n    raise ValueError('Input spline_degree must be at least 1.')\n  if spline_degree >= t_input.shape[0] - 1:\n    raise ValueError('Input spline_degree must be less than t_input.shape[0] - 1.')\n  if spline_degree >= t_output.shape[0]:\n    raise ValueError('Input spline_degree must be less than t_output.shape[0].')\n\n  tck, u_keyframes = scipy.interpolate.splprep(\n      x.T, k=spline_degree, s=smoothness, per=False\n  )\n  new_x = np.array(scipy.interpolate.splev(t_output, tck))\n  return new_x.T"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  assert len(x.shape) == 1, 'x must be a 1-dimensional signal.'\n  assert spline_degree >= 0, 'spline_degree must be non-negative.'\n  assert spline_degree < len(\n      t_input\n  ), 'spline_degree must be less than the number of input times.'\n  assert len(t_input) == len(\n      t_output\n  ), 't_input and t_output must have the same length.'\n  assert np.all(\n      np.diff(t_input) > 0\n  ), 't_input must be a strictly increasing sequence.'\n  assert np.all(\n      np.diff(t_output) > 0\n  ), 't_output must be a strictly increasing sequence.'\n  assert np.all(\n      t_input[1:] > t_input[:-1]\n  ), 't_input must be a strictly increasing sequence.'\n  assert np.all(\n      t_output[1:] > t_output[:-1]\n  ), 't_output must be a strictly increasing sequence.'\n  assert spline_degree <= len(\n      t_input\n  ), 'spline_degree must be less than or equal to the number of input times.'\n  assert spline_degree <= len(\n      t_output\n  ), 'spline_degree must be less than or equal to the number of output times.'\n\n  # Pad x with zeros at the beginning and end.\n  x = np.pad(x, (spline_degree, spline_degree), 'constant', constant_values=0)\n\n  # Compute the spline coefficients.\n  tck, u = scipy.interpolate.splrep(t_input, x, k=spline_degree, s=smoothness)\n\n  # Evaluate the spline at the output times.\n  y = scipy.interpolate.splev(t_output, tck)\n\n  return y"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Adapted from https://github.com/google/nerfies/blob/main/nerfies/camera.py\n  # Let t be the time variable and x be the signal to be interpolated.\n  # Let t_input be the times at which x is defined and t_output be the times\n  # at which the interpolated signal is queried.\n  # Let k be the degree of the spline and s be the smoothness parameter.\n  # Let N be the number of points in x.\n  # Let t_i be the i-th input time, t_o be the i-th output time, and x_i be the\n  # i-th input value.\n  # Let t_k be the k-th input time and x_k be the k-th input value.\n  # Let t_0 be the first input time and t_N be the last input time.\n  # Let t_k_minus_1 be the k-1-th input time and x_k_minus_1 be the k-1-th input\n  # value.\n  # Let t_k_plus_1 be the k+1-th input time and x_k_plus_1 be the k+1-th input\n  # value.\n  # Let t_k_minus_2 be the k-2-th input time and x_k_minus_2 be the k-2-th input\n  # value.\n  # Let t_k_plus_2 be the k+2-th input time and x_k_plus_2 be the k+2-th input\n  # value.\n  # Let t_k_minus_3 be the k-3-th input time and x_k_minus_3 be the k-3-th input\n  # value.\n  # Let t_k_plus_3 be the k+3-th input time and x_k_plus_3 be the k+3-th input\n  # value.\n  # Let t_k_minus_4 be the k-4-th input time and x_k_minus_4 be the k-4-th input\n  # value.\n  # Let"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  if spline_degree > len(x) - 1:\n    spline_degree = len(x) - 1\n  spline_degree = min(spline_degree, len(x) - 1)\n  tck, u_keyframes = scipy.interpolate.splprep(x.T, k=spline_degree, s=smoothness, per=False)\n  y = scipy.interpolate.splev(t_output, tck)\n  return y\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  if len(x.shape) != 1:\n    raise ValueError('Input signal x must be a 1-dimensional array.')\n  if len(t_input.shape) != 1:\n    raise ValueError('Input times t_input must be a 1-dimensional array.')\n  if len(t_output.shape) != 1:\n    raise ValueError('Input times t_output must be a 1-dimensional array.')\n\n  # Adapted from https://github.com/google/nerfies/blob/main/nerfies/camera.py\n  # Let x be the input signal, t_input be the times at which it is defined, and\n  # t_output be the times at which it is queried.\n  # Let N be the number of input times, and M be the number of output times.\n  # Let k be the degree of the spline fit.\n  # Let s be a smoothness parameter.\n  # Let t_input_i be the i-th input time, and t_output_j be the j-th output time.\n  # Let x_i be the value of x at t_input_i, and x_j be the value of x at t_output_j.\n  # Let s_i be the value of s at t_input_i, and s_j be the value of s at t_output_j.\n  # Let y_j be the value of the spline interpolation of x at t_output_j.\n  # Let y_j = sum_{i=1}^N w_i(t_output_j) x_i, where w_i(t) is the weight of\n  # the i-th input time.\n  # Let w_i(t) = (t - t_input_i)^k / (t_input_{i+1} - t_input_i)^k.\n  # Let s_j = sum_{i=1}^N w_i(t_output_j) s_i.\n  # Let y_j = sum_{i=1}^N w_i(t_output_j) x_"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Check if input and output times are monotonic\n  if not np.all(np.diff(t_input) >= 0):\n    raise ValueError(\"Input times must be monotonically increasing\")\n  if not np.all(np.diff(t_output) >= 0):\n    raise ValueError(\"Output times must be monotonically increasing\")\n\n  # Check if input and output times are within the range of the input times\n  if not np.all(t_output >= t_input[0]) or not np.all(t_output <= t_input[-1]):\n    raise ValueError(\"Output times must be within the range of the input times\")\n\n  # Check if the input signal has at least two points\n  if len(x) < 2:\n    raise ValueError(\"Input signal must have at least two points\")\n\n  # Check if the input signal and input times have the same length\n  if len(x) != len(t_input):\n    raise ValueError(\"Input signal and input times must have the same length\")\n\n  # Check if the input signal and output times have the same length\n  if len(x) != len(t_output):\n    raise ValueError(\"Input signal and output times must have the same length\")\n\n  # Check if the spline degree is at most one less than the number of points\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Interpolate the signal using spline interpolation\n  spline_tck, _ = scipy.interpolate.splrep(t_input, x, k=spline_degree, s=smoothness)\n  y = scipy.interpolate.splev(t_output, spline_tck)\n\n  return y\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n    if word[0].isupper() and word[1].isupper():\n        return word\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    if word[0].islower() and word[1].isupper():\n        return word.capitalize()\n    if word[0].islower() and word[1].islower():\n        return word.capitalize()\n    return word"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n    if word[0].isupper() and word[1].isupper():\n        return word.capitalize()\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    if word[0].islower() and word[1].isupper():\n        return word.capitalize()\n    if word[0].islower() and word[1].islower():\n        return word.capitalize()\n    return word"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n\n    if word[0].isupper() and word[1].isupper():\n        return word\n    if word[0].islower() and word[1].islower():\n        return word\n\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    if word[0].islower() and word[1].isupper():\n        return word.capitalize()\n\n    return word"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n    if word[0].isupper() and word[1].isupper():\n        return word.capitalize()\n    return word.lower()"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    else:\n        if word[0:2].islower():\n            return word.lower()\n        elif word[0:2].isupper():\n            return word.capitalize()\n        else:\n            return word"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    elif word[0].isupper() and word[1].isupper():\n        return word\n    elif word[0].islower() and word[1].islower():\n        return word\n    elif word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    elif word[0].islower() and word[1].isupper():\n        return word.capitalize()\n    else:\n        return word"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    else:\n        if word[0:2].islower():\n            return word.lower()\n        elif word[0:2].isupper():\n            return word.capitalize()\n        else:\n            return word"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    elif word[0].isupper() and word[1].isupper():\n        return word\n    elif word[0].islower() and word[1].islower():\n        return word.capitalize()\n    elif word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    else:\n        return word"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.islower():\n        return word\n    if word.isupper():\n        return word\n    if word[0:2].isupper():\n        return word.capitalize()\n    return word.lower()"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.islower():\n        return word\n    if word.isupper():\n        return word\n    if word[0].isupper() and word[1].isupper():\n        return word.capitalize()\n    return word.lower()"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.islower():\n        return word\n    if word.isupper():\n        return word\n    if word[0].isupper() and word[1].isupper():\n        return word\n    if word[0].islower() and word[1].islower():\n        return word.capitalize()\n    return word"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    else:\n        if word[0].islower() and word[1].isupper():\n            return word.capitalize()\n        else:\n            return word"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n\n    if word[0].isupper() and word[1].isupper():\n        return word\n    if word[0].islower() and word[1].islower():\n        return word\n\n    if word[0].isupper() and word[1].islower():\n        return word[0].upper() + word[1:].lower()\n    if word[0].islower() and word[1].isupper():\n        return word[0].upper() + word[1:].lower()\n\n    return word"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    else:\n        if word[0].islower() and word[1].isupper():\n            return word.capitalize()\n        else:\n            return word"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    # if lower no uppers after\n    # if upper no\n\n    if word.isupper():\n        return word\n    if word.islower():\n        return word\n\n    # if mixed case\n    if word[0].isupper() and word[1].isupper():\n        return word\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    if word[0].islower() and word[1].isupper():\n        return word.capitalize()\n\n    return word"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n    if word[0].islower() and word[1].islower():\n        return word.lower()\n    if word[0].isupper() and word[1].isupper():\n        return word.lower()\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    if word[0].islower() and word[1].isupper():\n        return word.capitalize()"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    # if lower no uppers after\n    # if upper no\n    # if mixed case\n    if word.isupper():\n        return word\n    if word.islower():\n        return word\n\n    if word[0].isupper() and word[1].isupper():\n        return word.capitalize()\n    if word[0].islower() and word[1].islower():\n        return word.capitalize()\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    if word[0].islower() and word[1].isupper():\n        return word.capitalize()"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n\n    if len(word) > 2:\n        if word[0].isupper() and word[1].isupper():\n            return word\n\n    if len(word) > 2:\n        if word[0].islower() and word[1].islower():\n            return word\n\n    if len(word) > 2:\n        if word[0].isupper() and word[1].islower():\n            return word[0].lower() + word[1:]\n\n    if len(word) > 2:\n        if word[0].islower() and word[1].isupper():\n            return word[0].upper() + word[1:]\n\n    if len(word) > 2:\n        if word[0].isupper() and word[1].isupper() and word[2].islower():\n            return word[0] + word[1].lower() + word[2:]\n\n    if len(word) > 2:\n        if word[0].islower() and word[1].islower() and word[2].isupper():\n            return word[0] + word[1].upper() + word[2:]\n\n    if len(word) > 2:\n        if word[0].isupper() and word[1].islower() and word[2].isupper():\n            return word[0] + word[1].upper() + word[2:]\n\n    if len(word) > 2:\n        if word[0].islower() and word[1].isupper() and word[2].islower():\n            return word[0] + word[1].lower() + word[2:]\n\n    return word"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    # if lower no uppers after\n    # if upper no\n\n    if word.islower():\n        return word\n    if word.isupper():\n        return word\n\n    # if mixed case\n    # if first two letters are uppercase\n    # if first two letters are lowercase\n    # if first two letters are mixed case\n\n    if word[0:2].isupper():\n        return word.lower().capitalize()\n    if word[0:2].islower():\n        return word.capitalize()\n    if word[0:2].islower() and word[0:2].isupper():\n        return word.capitalize()"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n    if word[0].isupper() and word[1].isupper():\n        return word\n    if word[0].islower() and word[1].islower():\n        return word.capitalize()\n    if word[0].islower() and word[1].isupper():\n        return word.capitalize()\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}: {field.name} must be binary.\")\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a binary array, found {v.dtype} instead.\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}: {field.name} must be binary. Received {v.dtype}\")\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a binary array. Found {v.dtype} instead.\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a binary array. Found {v.dtype} instead.\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}: {field.name} must be binary. Received {v.dtype}.\")\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.array_equal(v, v.astype(bool)):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a binary array. \"\n            f\"Found {v.dtype} instead.\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.array_equal(v, v.astype(bool)):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be binary. Received {v.dtype} instead of bool.\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a binary array. Received {v.dtype}\")\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must contain only boolean values. \"\n            f\"Found {v.dtype} instead.\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a binary array. Found {v.dtype} instead.\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.array_equal(v, v.astype(bool)):\n        raise ValueError(f\"{cls.__name__}: {field.name} must be binary.\")\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}: {field.name} must be binary, received {v.dtype}.\")\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.array_equal(v, v.astype(bool)):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a binary array. \"\n            f\"Found {np.unique(v)} instead.\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.array_equal(v, v.astype(bool)):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must contain only boolean values. \"\n            f\"Found {v.dtype} instead.\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a binary array (dtype=np.bool_). \"\n            f\"Received dtype={v.dtype}\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}: {field.name} must be binary.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}: {field.name} must be binary. Received {v.dtype}\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.array_equal(v, v.astype(bool)):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a binary array. \"\n            f\"Received array with data types {np.unique(v.dtype)}\"\n        )\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}: {field.name} must be binary. Received {v.dtype}.\")\n\n    return v\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  x_mag_sq = jnp.maximum(1, x_mag**2)\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag = jnp.sqrt(jnp.sum(x**2, axis=-1))\n  x_mag_sq = jnp.maximum(1, x_mag**2)\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag = jnp.sqrt(jnp.sum(x**2, axis=-1))\n  x_mag_sq = jnp.maximum(1, x_mag**2)\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  x_norm_inv = jnp.where(x_norm > 0, 1 / x_norm, 0)\n  x_norm_inv_3 = jnp.power(x_norm_inv, 3)\n  x_norm_inv_5 = jnp.power(x_norm_inv, 5)\n  x_norm_inv_7 = jnp.power(x_norm_inv, 7)\n  x_norm_inv_9 = jnp.power(x_norm_inv, 9)\n  x_norm_inv_11 = jnp.power(x_norm_inv, 11)\n  x_norm_inv_13 = jnp.power(x_norm_inv, 13)\n  x_norm_inv_15 = jnp.power(x_norm_inv, 15)\n  x_norm_inv_17 = jnp.power(x_norm_inv, 17)\n  x_norm_inv_19 = jnp.power(x_norm_inv, 19)\n  x_norm_inv_21 = jnp.power(x_norm_inv, 21)\n  x_norm_inv_23 = jnp.power(x_norm_inv, 23)\n  x_norm_inv_25 = jnp.power(x_norm_inv, 25)\n  x_norm_inv_27 = jnp.power(x_norm_inv, 27)\n  x_norm_inv_29 = jnp.power(x_norm_inv, 29)\n  x_norm_inv_31 = jnp.power(x_norm_inv, 31)\n  x_norm_inv_33 = jnp.power(x_norm_inv, 33)\n  x_norm_inv_35 = jnp.power(x_norm_inv, 35)\n  x_norm_inv_3"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  x_norm_sq = x_norm**2\n  scale = jnp.where(\n      x_norm_sq > 1,\n      (2 * jnp.sqrt(x_norm_sq) - 1) / x_norm_sq,\n      jnp.ones_like(x_norm_sq),\n  )\n  z = scale * x\n  return z"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  x_norm_sq = jnp.square(x_norm)\n  x_norm_cube = jnp.power(x_norm_sq, 1.5)\n  x_norm_cube_inv = 1 / x_norm_cube\n  x_norm_cube_inv_sq = jnp.square(x_norm_cube_inv)\n  x_norm_cube_inv_cube = jnp.power(x_norm_cube_inv, 3)\n  x_norm_cube_inv_cube_sq = jnp.square(x_norm_cube_inv_cube)\n  x_norm_cube_inv_cube_sq_inv = 1 / x_norm_cube_inv_cube_sq\n  x_norm_cube_inv_cube_sq_inv_sq = jnp.square(x_norm_cube_inv_cube_sq_inv)\n  x_norm_cube_inv_cube_sq_inv_sq_inv = 1 / x_norm_cube_inv_cube_sq_inv_sq\n  x_norm_cube_inv_cube_sq_inv_sq_inv_sq = jnp.square(\n      x_norm_cube_inv_cube_sq_inv_sq_inv\n  )\n  x_norm_cube_inv_cube_sq_inv_sq_inv_sq_inv = 1 / x_norm_cube_inv_cube_sq_inv_sq_inv_sq\n  x_norm_cube_inv_cube_sq_inv_sq_inv_sq_inv_sq = jnp.square(\n      x_norm_cube_inv_cube_sq_inv_sq_inv_sq_inv\n  )\n  x_norm_cube_inv_cube_sq_inv_sq_inv_sq_inv_sq_inv = 1 / x_norm_cube_inv_c"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  x_scaled = x / jnp.maximum(x_norm, 1)\n  return x_scaled\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_norm = jnp.linalg.norm(x, axis=-1)\n  x_norm = jnp.where(x_norm > 0, x_norm, 1)\n  x_norm_inv = jnp.reciprocal(x_norm)\n  x_norm_inv_sq = jnp.square(x_norm_inv)\n  x_norm_inv_cube = jnp.power(x_norm_inv, 3)\n  x_norm_inv_cube_sq = jnp.square(x_norm_inv_cube)\n  x_norm_inv_cube_sq_sq = jnp.square(x_norm_inv_cube_sq)\n  x_norm_inv_cube_sq_sq_sq = jnp.power(x_norm_inv_cube_sq, 3)\n  x_norm_inv_cube_sq_sq_sq_sq = jnp.square(x_norm_inv_cube_sq_sq_sq)\n  x_norm_inv_cube_sq_sq_sq_sq_sq = jnp.power(x_norm_inv_cube_sq_sq_sq_sq, 2)\n  x_norm_inv_cube_sq_sq_sq_sq_sq_sq = jnp.power(\n      x_norm_inv_cube_sq_sq_sq_sq_sq, 3\n  )\n  x_norm_inv_cube_sq_sq_sq_sq_sq_sq_sq = jnp.power(\n      x_norm_inv_cube_sq_sq_sq_sq_sq_sq, 4\n  )\n  x_norm_inv_cube_sq_sq_sq_sq_sq_sq_sq_sq = jnp.power(\n      x_norm_inv_cube_sq_sq_sq_sq_sq_sq_sq, 5\n  )\n  x_norm_inv_cube_sq_sq_sq_sq_sq_sq_sq_sq_sq = jnp.power("}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n    summary_df = pd.read_csv(summary_path)\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    df = pd.read_csv(summary_path)\n    if dict_columns is None:\n        dict_columns = ['module_params']\n    for column in dict_columns:\n        df[column] = df[column].apply(lambda x: ast.literal_eval(x))\n    return df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    df = pd.read_csv(summary_path)\n    for column in dict_columns:\n        df[column] = df[column].apply(lambda x: ast.literal_eval(x))\n    return df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    summary_df = pd.read_csv(summary_path)\n    if dict_columns is None:\n        dict_columns = ['module_params']\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    dict_columns = dict_columns or ['module_params']\n    summary_df = pd.read_csv(summary_path)\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(ast.literal_eval)\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n    summary_df = pd.read_csv(summary_path, converters={col: ast.literal_eval for col in dict_columns})\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n    summary_df = pd.read_csv(summary_path, converters={col: ast.literal_eval for col in dict_columns})\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    df = pd.read_csv(summary_path)\n\n    for column in dict_columns:\n        df[column] = df[column].apply(lambda x: ast.literal_eval(x))\n\n    return df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    df = pd.read_csv(summary_path, sep='\\t')\n    if dict_columns is None:\n        dict_columns = ['module_params']\n    for column in dict_columns:\n        df[column] = df[column].apply(lambda x: ast.literal_eval(x))\n    return df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    df = pd.read_csv(summary_path)\n    if dict_columns is not None:\n        for column_name in dict_columns:\n            df[column_name] = df[column_name].apply(lambda x: ast.literal_eval(x))\n    return df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    dict_columns = dict_columns or ['module_params']\n\n    df = pd.read_csv(summary_path)\n    for column in dict_columns:\n        df[column] = df[column].apply(lambda x: ast.literal_eval(x))\n    return df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n    df = pd.read_csv(summary_path)\n    for col in dict_columns:\n        df[col] = df[col].apply(lambda x: ast.literal_eval(x))\n    return df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    df = pd.read_csv(summary_path, sep='\\t')\n\n    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    for column in dict_columns:\n        df[column] = df[column].apply(ast.literal_eval)\n\n    return df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    def convert_dict_columns(df):\n        for col in dict_columns:\n            df[col] = df[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n        return df\n\n    df = pd.read_csv(summary_path)\n    df = convert_dict_columns(df)\n    return df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    def convert_dict_str(x):\n        if x is None:\n            return None\n        if isinstance(x, str):\n            try:\n                return ast.literal_eval(x)\n            except (ValueError, SyntaxError):\n                return x\n        else:\n            return x\n\n    summary_df = pd.read_csv(summary_path, converters={col: convert_dict_str for col in dict_columns})\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    df = pd.read_csv(summary_path, sep='\\t')\n\n    for column in dict_columns:\n        if column in df.columns:\n            df[column] = df[column].apply(lambda x: ast.literal_eval(x))\n\n    return df\n\n"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # Use the determinant directly.\n    det = jnp.linalg.det(cov)\n    isotropic_cov = cov / jnp.sqrt(det)\n  elif mode == 'accurate':\n    # Use the logarithm of the determinant for stability.\n    log_det = jnp.linalg.slogdet(cov)[1]\n    isotropic_cov = cov * jnp.exp(-0.5 * log_det)\n  else:\n    raise ValueError(f'Invalid mode: {mode}')\n\n  return isotropic_cov"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # Use the determinant directly.\n    det = jnp.linalg.det(cov)\n    sqrt_det = jnp.sqrt(det)\n    return cov / sqrt_det[Ellipsis, None, None] / sqrt_det[Ellipsis, None, :]\n  elif mode == 'accurate':\n    # Use the logarithm of the determinant for stability.\n    log_det = jnp.linalg.slogdet(cov)[1]\n    sqrt_det = jnp.exp(log_det / 2)\n    return cov / sqrt_det[Ellipsis, None, None] / sqrt_det[Ellipsis, None, :]\n  else:\n    raise ValueError(f'mode={mode} not implemented.')"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # Compute the determinant of the input covariance matrices.\n    det = jnp.linalg.det(cov)\n    # Compute the square root of the determinant.\n    sqrt_det = jnp.sqrt(det)\n    # Compute the inverse square root of the determinant.\n    inv_sqrt_det = 1.0 / sqrt_det\n    # Compute the isotropic covariance matrices by multiplying the input\n    # covariance matrices with the inverse square root of the determinant.\n    isotropic_cov = cov * inv_sqrt_det\n    return isotropic_cov\n  elif mode == 'accurate':\n    # Compute the logarithm of the determinant of the input covariance matrices.\n    log_det = jnp.linalg.slogdet(cov)[1]\n    # Compute the square root of the logarithm of the determinant.\n    sqrt_log_det = jnp.sqrt(log_det)\n    # Compute the inverse square root of the logarithm of the determinant.\n    inv_sqrt_log_det = 1.0 / sqrt_log_det\n    # Compute the isotropic covariance matrices by multiplying the input\n    # covariance matrices with the inverse square root of the logarithm of the\n    # determinant.\n    isotropic_cov = cov * inv_sqrt_log_det\n    return isotropic_cov\n  else:\n    raise ValueError(\n        f'Invalid mode {mode}. Must be either \"fast\" or \"accurate\".'\n    )"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    det_cov = jnp.linalg.det(cov)\n    isotropic_cov = cov / jnp.sqrt(det_cov)\n  elif mode == 'accurate':\n    log_det_cov = jnp.linalg.slogdet(cov)[1]\n    isotropic_cov = cov / jnp.exp(0.5 * log_det_cov)\n  else:\n    raise ValueError(f'mode={mode} not implemented.')\n  return isotropic_cov"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # Compute the determinant of the input matrix.\n    det = jnp.linalg.det(cov)\n    # Compute the square root of the determinant.\n    sqrt_det = jnp.sqrt(det)\n    # Compute the inverse square root of the determinant.\n    inv_sqrt_det = 1.0 / sqrt_det\n    # Compute the isotropic covariance matrix.\n    isotropic_cov = inv_sqrt_det * cov * inv_sqrt_det\n    return isotropic_cov\n  elif mode == 'accurate':\n    # Compute the logarithm of the determinant of the input matrix.\n    log_det = jnp.linalg.slogdet(cov)[1]\n    # Compute the square root of the logarithm of the determinant.\n    sqrt_log_det = jnp.sqrt(log_det)\n    # Compute the inverse square root of the logarithm of the determinant.\n    inv_sqrt_log_det = 1.0 / sqrt_log_det\n    # Compute the isotropic covariance matrix.\n    isotropic_cov = inv_sqrt_log_det * cov * inv_sqrt_log_det\n    return isotropic_cov\n  else:\n    raise ValueError(f\"Invalid mode: {mode}. Must be 'fast' or 'accurate'.\")"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # Compute the isotropic covariance matrices directly from the determinant.\n    cov_sqrt = jnp.sqrt(jnp.abs(jnp.linalg.det(cov)))\n    return cov_sqrt * jnp.eye(cov.shape[-1])\n  elif mode == 'accurate':\n    # Compute the isotropic covariance matrices using the logarithm of the determinant.\n    cov_logdet = jnp.linalg.slogdet(cov)[1]\n    cov_sqrt = jnp.exp(0.5 * (cov_logdet - jnp.log(cov.shape[-1])))\n    return cov_sqrt * jnp.eye(cov.shape[-1])\n  else:\n    raise ValueError(f'Invalid mode: {mode}')"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'accurate':\n    det = jnp.linalg.det(cov)\n    if jnp.any(det <= 0):\n      raise ValueError('Invalid determinant encountered in isotropize.')\n    sqrt_det = jnp.sqrt(det)\n    return cov / sqrt_det[..., None, None] * sqrt_det[..., None, None]\n  elif mode == 'fast':\n    return cov / jnp.linalg.det(cov)[..., None, None]\n  else:\n    raise ValueError(f'Unknown mode {mode} for isotropize.')"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # Compute the determinant of the input covariance matrix.\n    det = jnp.linalg.det(cov)\n\n    # Compute the square root of the determinant.\n    sqrt_det = jnp.sqrt(det)\n\n    # Compute the square root of the input covariance matrix.\n    sqrt_cov = jnp.linalg.inv(jnp.linalg.cholesky(cov))\n\n    # Multiply the square root of the determinant to the square root of the input covariance matrix.\n    sqrt_cov = sqrt_det * sqrt_cov\n\n    # Return the isotropic covariance matrix.\n    return sqrt_cov @ sqrt_cov.T\n\n  elif mode == 'accurate':\n    # Compute the logarithm of the determinant of the input covariance matrix.\n    log_det = jnp.linalg.slogdet(cov)[1]\n\n    # Compute the square root of the logarithm of the determinant.\n    sqrt_log_det = jnp.sqrt(log_det)\n\n    # Compute the square root of the input covariance matrix.\n    sqrt_cov = jnp.linalg.inv(jnp.linalg.cholesky(cov))\n\n    # Multiply the square root of the logarithm of the determinant to the square root of the input covariance matrix.\n    sqrt_cov = sqrt_log_det * sqrt_cov\n\n    # Return the isotropic covariance matrix.\n    return sqrt_cov @ sqrt_cov.T\n\n  else:\n    raise ValueError(f'Invalid mode: {mode}')"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # Compute the square root of the determinant directly.\n    det = jnp.linalg.det(cov)\n    sqrt_det = jnp.sqrt(det)\n    sqrt_cov = sqrt_det * jnp.linalg.inv(jnp.linalg.cholesky(cov))\n  elif mode == 'accurate':\n    # Compute the square root of the determinant using the logarithm.\n    log_det = jnp.linalg.slogdet(cov)[1]\n    sqrt_det = jnp.exp(0.5 * log_det)\n    sqrt_cov = sqrt_det * jnp.linalg.inv(jnp.linalg.cholesky(cov))\n  else:\n    raise ValueError(f'mode={mode} not implemented.')\n\n  # Check if the determinant is valid (i.e., non-negative).\n  if jnp.any(det < 0):\n    raise ValueError('Invalid determinant encountered.')\n\n  # Check if the square root of the determinant is valid (i.e., non-negative).\n  if jnp.any(sqrt_det < 0):\n    raise ValueError('Invalid square root of determinant encountered.')\n\n  return sqrt_cov"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'accurate':\n    det_cov = jnp.linalg.det(cov)\n    det_cov = jnp.where(\n        det_cov < 0,\n        jnp.zeros_like(det_cov),\n        jnp.where(\n            det_cov == 0,\n            jnp.ones_like(det_cov) * jnp.finfo(det_cov.dtype).eps,\n            det_cov,\n        ),\n    )\n    sqrt_det_cov = jnp.sqrt(det_cov)\n    return cov / sqrt_det_cov[:, None, None] * sqrt_det_cov[Ellipsis, None, :]\n  elif mode == 'fast':\n    return cov / jnp.sqrt(jnp.linalg.det(cov))[:, None, None] * jnp.sqrt(\n        jnp.linalg.det(cov)\n    )[Ellipsis, None, :]\n  else:\n    raise ValueError(f'mode={mode} not implemented.')"}
{"namespace": "coord.isotropize", "completion": "  # Check if the determinant of the input matrix is positive.\n  det = jnp.linalg.det(cov)\n  if mode == 'fast':\n    sqrt_cov = jnp.sqrt(jnp.abs(det)) * jnp.linalg.inv(jnp.sqrt(cov))\n  elif mode == 'accurate':\n    sqrt_cov = jnp.sqrt(jnp.abs(det)) * jnp.linalg.inv(cov)\n  else:\n    raise ValueError(f'Invalid mode {mode}.')\n\n  return sqrt_cov"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # Compute the determinant of the input covariance matrix\n    det = jnp.linalg.det(cov)\n\n    # Compute the square root of the determinant\n    sqrt_det = jnp.sqrt(det)\n\n    # Compute the square root of the determinant, with a small epsilon added to\n    # prevent division by zero\n    eps = jnp.finfo(jnp.float32).tiny\n    sqrt_det = jnp.sqrt(det + eps)\n\n    # Compute the square root of the determinant, with a small epsilon added to\n    # prevent division by zero\n    eps = jnp.finfo(jnp.float32).tiny\n    sqrt_det = jnp.sqrt(det + eps)\n\n    # Compute the square root of the determinant, with a small epsilon added to\n    # prevent division by zero\n    eps = jnp.finfo(jnp.float32).tiny\n    sqrt_det = jnp.sqrt(det + eps)\n\n    # Compute the square root of the determinant, with a small epsilon added to\n    # prevent division by zero\n    eps = jnp.finfo(jnp.float32).tiny\n    sqrt_det = jnp.sqrt(det + eps)\n\n    # Compute the square root of the determinant, with a small epsilon added to\n    # prevent division by zero\n    eps = jnp.finfo(jnp.float32).tiny\n    sqrt_det = jnp.sqrt(det + eps)\n\n    # Compute the square root of the determinant, with a small epsilon added to\n    # prevent division by zero\n    eps = jnp.finfo(jnp.float32).tiny\n    sqrt_det = jnp.sqrt(det + eps)\n\n    # Compute the square root of the determinant, with a small epsilon added to\n    # prevent division by zero\n    eps = jnp.finfo(jnp.float32).tiny\n    sqrt_det"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'accurate':\n    det = jnp.linalg.det(cov)\n    # Check if the determinant is zero or negative.\n    is_invalid = jnp.any(det <= 0, axis=-1)\n    # If the determinant is negative or zero, set it to a small positive value.\n    det = jnp.where(is_invalid, 1e-6, det)\n    # Compute the square root of the determinant.\n    sqrt_det = jnp.sqrt(det)\n    # Compute the square root of the covariance matrix.\n    sqrt_cov = jnp.linalg.solve(cov, jnp.eye(cov.shape[-1])) * sqrt_det[..., None, None]\n    return sqrt_cov\n  elif mode == 'fast':\n    # Compute the square root of the covariance matrix.\n    sqrt_cov = jnp.linalg.cholesky(cov)\n    return sqrt_cov\n  else:\n    raise ValueError(f'Invalid mode: {mode}')"}
{"namespace": "coord.isotropize", "completion": "  # Compute the determinant of the input covariance matrix\n  det = jnp.linalg.det(cov)\n\n  if mode == 'fast':\n    # Compute the isotropic covariance matrix directly\n    isotropic_cov = cov / jnp.sqrt(det)\n  elif mode == 'accurate':\n    # Compute the logarithm of the determinant\n    log_det = jnp.log(det)\n\n    # Compute the isotropic covariance matrix using the logarithm of the determinant\n    isotropic_cov = cov * jnp.exp(-log_det / 2)\n  else:\n    raise ValueError(f\"Invalid mode '{mode}'.\")\n\n  # Check if the determinant of the isotropic covariance matrix is positive\n  if jnp.any(jnp.linalg.det(isotropic_cov) <= 0):\n    raise ValueError(\n        \"The determinant of the isotropic covariance matrix is not positive.\"\n    )\n\n  return isotropic_cov"}
{"namespace": "coord.isotropize", "completion": "  # Compute the determinant of the input covariance matrix\n  det = jnp.linalg.det(cov)\n\n  # Check if the determinant is positive or zero\n  if mode == 'accurate':\n    # If the determinant is positive, compute the square root of the determinant\n    sqrt_det = jnp.sqrt(det)\n    # If the determinant is zero, return the input covariance matrix\n    sqrt_cov = jnp.where(det > 0, cov / sqrt_det, cov)\n  else:\n    # If the determinant is positive, compute the square root of the determinant\n    sqrt_det = jnp.where(det > 0, jnp.sqrt(det), 1)\n    # If the determinant is zero, return the input covariance matrix\n    sqrt_cov = jnp.where(det > 0, cov / sqrt_det, cov)\n\n  return sqrt_cov"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    det = jnp.linalg.det(cov)\n    return jnp.sqrt(det) * cov\n  elif mode == 'accurate':\n    # Compute the determinant and take its square root.\n    # We use the logarithm of the determinant to avoid numerical issues.\n    logdet = jnp.linalg.slogdet(cov)[1]\n    sqrt_det = jnp.exp(0.5 * logdet)\n    # Compute the square root of the covariance matrix.\n    sqrt_cov = sqrtm(cov)\n    # Multiply the square root of the covariance matrix by the square root of the determinant.\n    return sqrt_det * sqrt_cov\n  else:\n    raise ValueError(f'mode={mode} not implemented.')"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    return jnp.sqrt(cov)\n  elif mode == 'accurate':\n    # If the determinant is very small, we can't take a square root.\n    # In this case, we'll just use the determinant directly.\n    det = jnp.linalg.det(cov)\n    sqrt_det = jnp.sqrt(det)\n    sqrt_det = jnp.where(\n        jnp.abs(sqrt_det) > jnp.finfo(jnp.float32).eps,\n        sqrt_det,\n        det,\n    )\n    return jnp.sqrt(cov) * jnp.sqrt(det) / sqrt_det\n  else:\n    raise ValueError(f'mode={mode} not implemented.')"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # The determinant of a covariance matrix is the product of the eigenvalues.\n    # We can therefore take the square root of the determinant directly.\n    sqrt_det = jnp.sqrt(jnp.linalg.det(cov))\n  elif mode == 'accurate':\n    # The determinant of a covariance matrix is the product of the eigenvalues.\n    # We can therefore take the square root of the determinant directly.\n    sqrt_det = jnp.sqrt(jnp.linalg.det(cov))\n  else:\n    raise ValueError(f'Invalid mode: {mode}')\n  # The square root of a covariance matrix is the matrix square root of the\n  # covariance matrix.\n  # We can therefore take the square root of the covariance matrix directly.\n  sqrt_cov = jnp.linalg.matrix_power(cov, 0.5)\n  # Check if the determinant of the covariance matrix is zero.\n  if jnp.any(jnp.isnan(sqrt_det)):\n    raise ValueError('Invalid determinant')\n  # Check if the determinant of the covariance matrix is negative.\n  if jnp.any(sqrt_det < 0):\n    raise ValueError('Invalid determinant')\n  # Check if the determinant of the covariance matrix is zero.\n  if jnp.any(jnp.isnan(sqrt_det)):\n    raise ValueError('Invalid determinant')\n  # Check if the determinant of the covariance matrix is negative.\n  if jnp.any(sqrt_det < 0):\n    raise ValueError('Invalid determinant')\n  # Check if the determinant of the covariance matrix is zero.\n  if jnp.any(jnp.isnan(sqrt_det)):\n    raise ValueError('Invalid determinant')\n  # Check if the determinant of the covariance matrix is negative.\n  if jnp.any(sqrt_det < 0):\n    raise ValueError('Invalid determinant')\n  # Check if the determinant of the covariance matrix is zero.\n  if jnp.any(jnp.isnan(sqrt_det)):\n    raise ValueError"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # Use the determinant directly.\n    det = jnp.linalg.det(cov)\n    # The determinant is zero if any of the eigenvalues are zero.\n    # We need to check this because the sqrtm function can't handle zero determinants.\n    # If the determinant is zero, we return the original covariance matrix.\n    if jnp.any(det == 0):\n      return cov\n    sqrt_det = jnp.sqrt(det)\n    return cov / sqrt_det\n  elif mode == 'accurate':\n    # Use the logarithm of the determinant for stability.\n    log_det = jnp.linalg.slogdet(cov)[1]\n    # The log-determinant is -inf if any of the eigenvalues are zero.\n    # We need to check this because the sqrtm function can't handle negative determinants.\n    # If the log-determinant is -inf, we return the original covariance matrix.\n    if jnp.any(log_det == -jnp.inf):\n      return cov\n    sqrt_det = jnp.exp(log_det / 2)\n    return cov / sqrt_det\n  else:\n    raise ValueError(f'Invalid mode: {mode}.')"}
{"namespace": "coord.isotropize", "completion": "  # Compute the determinant of the input covariance matrix.\n  det_cov = jnp.linalg.det(cov)\n\n  if mode == 'fast':\n    # If the determinant is negative, flip the sign of the determinant and the\n    # covariance matrix.\n    det_cov = jnp.abs(det_cov)\n    cov = -cov\n\n  elif mode == 'accurate':\n    # If the determinant is negative, flip the sign of the determinant and the\n    # covariance matrix.\n    # If the determinant is very close to zero, replace the determinant with a\n    # small value to avoid numerical instability.\n    det_cov = jnp.abs(det_cov)\n    eps = jnp.finfo(jnp.float32).eps\n    cov = -cov\n    det_cov = jnp.where(\n        jnp.abs(det_cov) < eps, jnp.ones_like(det_cov) * eps, det_cov\n    )\n\n  # Compute the square root of the determinant.\n  sqrt_det_cov = jnp.sqrt(det_cov)\n\n  # Compute the square root of the covariance matrix.\n  sqrt_cov = cov / sqrt_det_cov\n\n  # Compute the square root of the covariance matrix again, but with the\n  # determinant replaced by the square root of the determinant.\n  sqrt_cov_sqrt_det_cov = sqrt_cov * sqrt_det_cov\n\n  return sqrt_cov_sqrt_det_cov"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"+\", help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts, specifying how many times a task can be retried upon failure.\")\n    parser.add_argument(\""}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs='+', help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts, specifying how many times a task can be retried upon failure.\")\n    parser.add_argument(\""}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"XAgent Command Line Interface\")\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"*\", help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", choices=[\"auto\", \"manual\"], help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts, specifying how many"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser()\n    parser.add_argument('--task', type=str, required=True, help='The task description, specifying what task should be performed.')\n    parser.add_argument('--upload-files', type=str, nargs='*', help='List of files to upload, allowing multiple files to be specified.')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use.')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs.')\n    parser.add_argument('--mode', type=str, default=\"auto\", help='Operational mode, which can be \\'auto\\' or \\'manual\\', specifying how the task should be executed.')\n    parser.add_argument('--quiet', action='store_true', help='If set, the program runs in quiet mode with minimal output.')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be.')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution.')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans.')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be.')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts, specifying how many times a task can"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser()\n    parser.add_argument('--task', type=str, required=True, help='The task description, specifying what task should be performed.')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload, allowing multiple files to be specified.')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use.')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs.')\n    parser.add_argument('--mode', type=str, default=\"auto\", help='Operational mode, which can be \\'auto\\' or \\'manual\\', specifying how the task should be executed.')\n    parser.add_argument('--quiet', action='store_true', default=False, help='If set, the program runs in quiet mode with minimal output.')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be.')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', default=False, help='Flag to enable asking for human assistance during task execution.')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans.')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be.')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts,"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='XAgent Command Line Interface')\n\n    parser.add_argument('--task', type=str, required=True, help='The task description, specifying what task should be performed.')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload, allowing multiple files to be specified.')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use.')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs.')\n    parser.add_argument('--mode', type=str, default=\"auto\", help='Operational mode, which can be \\'auto\\' or \\'manual\\', specifying how the task should be executed.')\n    parser.add_argument('--quiet', action='store_true', help='If set, the program runs in quiet mode with minimal output.')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be.')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution.')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans.')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be.')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--task\",\n        type=str,\n        required=True,\n        help=\"The task description, specifying what task should be performed.\",\n    )\n    parser.add_argument(\n        \"--upload-files\",\n        type=str,\n        nargs=\"+\",\n        help=\"List of files to upload, allowing multiple files to be specified.\",\n    )\n    parser.add_argument(\n        \"--model\",\n        type=str,\n        help=\"Model identifier for the task, specifying which model to use.\",\n    )\n    parser.add_argument(\n        \"--record-dir\",\n        type=str,\n        help=\"Directory to record task execution logs, specifying where to save the logs.\",\n    )\n    parser.add_argument(\n        \"--mode\",\n        type=str,\n        default=\"auto\",\n        help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\",\n    )\n    parser.add_argument(\n        \"--quiet\",\n        action=\"store_true\",\n        help=\"If set, the program runs in quiet mode with minimal output.\",\n    )\n    parser.add_argument(\n        \"--max-subtask-chain-length\",\n        type=int,\n        help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\",\n    )\n    parser.add_argument(\n        \"--enable-ask-human-for-help\",\n        action=\"store_true\",\n        help=\"Flag to enable asking for human assistance during task execution.\",\n    )\n    parser.add_argument(\n        \"--max-plan-refine-chain-length\",\n        type=int,\n        help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\",\n    )\n    parser.add_argument(\n        \"--max-plan-tree-depth\",\n        type=int,\n        help=\"Maximum depth of the plan tree, specifying how deep the plan"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='XAgent Command Line Interface')\n    parser.add_argument('--task', type=str, required=True, help='The task description, specifying what task should be performed.')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload, allowing multiple files to be specified.')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use.')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs.')\n    parser.add_argument('--mode', type=str, default=\"auto\", help='Operational mode, which can be \\'auto\\' or \\'manual\\', specifying how the task should be executed.')\n    parser.add_argument('--quiet', action='store_true', help='If set, the program runs in quiet mode with minimal output.')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be.')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution.')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans.')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be.')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Command Line Interface for XAgent')\n    parser.add_argument('--task', type=str, required=True, help='The task description, specifying what task should be performed')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload, allowing multiple files to be specified')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs')\n    parser.add_argument('--mode', type=str, default=\"auto\", help='Operational mode, which can be \\'auto\\' or \\'manual\\', specifying how the task should be executed')\n    parser.add_argument('--quiet', action='store_true', help='If set, the program runs in quiet mode with minimal output')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts, specifying how many times a task can be"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--task\",\n        type=str,\n        required=True,\n        help=\"The task description, specifying what task should be performed.\",\n    )\n    parser.add_argument(\n        \"--upload-files\",\n        type=str,\n        nargs=\"*\",\n        help=\"List of files to upload, allowing multiple files to be specified.\",\n    )\n    parser.add_argument(\n        \"--model\",\n        type=str,\n        help=\"Model identifier for the task, specifying which model to use.\",\n    )\n    parser.add_argument(\n        \"--record-dir\",\n        type=str,\n        help=\"Directory to record task execution logs, specifying where to save the logs.\",\n    )\n    parser.add_argument(\n        \"--mode\",\n        type=str,\n        default=\"auto\",\n        help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\",\n    )\n    parser.add_argument(\n        \"--quiet\",\n        action=\"store_true\",\n        help=\"If set, the program runs in quiet mode with minimal output.\",\n    )\n    parser.add_argument(\n        \"--max-subtask-chain-length\",\n        type=int,\n        help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\",\n    )\n    parser.add_argument(\n        \"--enable-ask-human-for-help\",\n        action=\"store_true\",\n        help=\"Flag to enable asking for human assistance during task execution.\",\n    )\n    parser.add_argument(\n        \"--max-plan-refine-chain-length\",\n        type=int,\n        help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\",\n    )\n    parser.add_argument(\n        \"--max-plan-tree-depth\",\n        type=int,\n        help=\"Maximum depth of the plan tree, specifying how deep the plan"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='XAgent command line interface')\n    parser.add_argument('--task', type=str, required=True, help='The task description, specifying what task should be performed')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload, allowing multiple files to be specified')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs')\n    parser.add_argument('--mode', type=str, default=\"auto\", help='Operational mode, which can be \\'auto\\' or \\'manual\\', specifying how the task should be executed')\n    parser.add_argument('--quiet', action='store_true', help='If set, the program runs in quiet mode with minimal output')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts, specifying how many times a task can be ret"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"+\", help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", choices=[\"auto\", \"manual\"], help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", default=False, help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", default=False, help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts, specifying how many times"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"Command Line Interface for XAgent\")\n    parser.add_argument(\n        \"--task\",\n        type=str,\n        required=True,\n        help=\"The task description, specifying what task should be performed.\",\n    )\n    parser.add_argument(\n        \"--upload-files\",\n        type=str,\n        nargs=\"+\",\n        help=\"List of files to upload, allowing multiple files to be specified.\",\n    )\n    parser.add_argument(\n        \"--model\",\n        type=str,\n        help=\"Model identifier for the task, specifying which model to use.\",\n    )\n    parser.add_argument(\n        \"--record-dir\",\n        type=str,\n        help=\"Directory to record task execution logs, specifying where to save the logs.\",\n    )\n    parser.add_argument(\n        \"--mode\",\n        type=str,\n        default=\"auto\",\n        choices=[\"auto\", \"manual\"],\n        help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\",\n    )\n    parser.add_argument(\n        \"--quiet\",\n        action=\"store_true\",\n        help=\"If set, the program runs in quiet mode with minimal output.\",\n    )\n    parser.add_argument(\n        \"--max-subtask-chain-length\",\n        type=int,\n        help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\",\n    )\n    parser.add_argument(\n        \"--enable-ask-human-for-help\",\n        action=\"store_true\",\n        help=\"Flag to enable asking for human assistance during task execution.\",\n    )\n    parser.add_argument(\n        \"--max-plan-refine-chain-length\",\n        type=int,\n        help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\",\n    )\n    parser.add_argument(\n        \"--max-plan-tree-depth\",\n        type="}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Command line interface for XAgent')\n    parser.add_argument('--task', type=str, required=True, help='The task description, specifying what task should be performed.')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload, allowing multiple files to be specified.')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use.')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs.')\n    parser.add_argument('--mode', type=str, default=\"auto\", help='Operational mode, which can be \\'auto\\' or \\'manual\\', specifying how the task should be executed.')\n    parser.add_argument('--quiet', action='store_true', help='If set, the program runs in quiet mode with minimal output.')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be.')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution.')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans.')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be.')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"Command Line Interface for XAgent\")\n\n    parser.add_argument(\n        \"--task\",\n        type=str,\n        required=True,\n        help=\"The task description, specifying what task should be performed.\",\n    )\n\n    parser.add_argument(\n        \"--upload-files\",\n        type=str,\n        nargs=\"+\",\n        help=\"List of files to upload, allowing multiple files to be specified.\",\n    )\n\n    parser.add_argument(\n        \"--model\",\n        type=str,\n        help=\"Model identifier for the task, specifying which model to use.\",\n    )\n\n    parser.add_argument(\n        \"--record-dir\",\n        type=str,\n        help=\"Directory to record task execution logs, specifying where to save the logs.\",\n    )\n\n    parser.add_argument(\n        \"--mode\",\n        type=str,\n        default=\"auto\",\n        choices=[\"auto\", \"manual\"],\n        help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\",\n    )\n\n    parser.add_argument(\n        \"--quiet\",\n        action=\"store_true\",\n        help=\"If set, the program runs in quiet mode with minimal output.\",\n    )\n\n    parser.add_argument(\n        \"--max-subtask-chain-length\",\n        type=int,\n        help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\",\n    )\n\n    parser.add_argument(\n        \"--enable-ask-human-for-help\",\n        action=\"store_true\",\n        help=\"Flag to enable asking for human assistance during task execution.\",\n    )\n\n    parser.add_argument(\n        \"--max-plan-refine-chain-length\",\n        type=int,\n        help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\",\n    )\n\n    parser.add_argument(\n        \"--max-"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"Command line interface for XAgent.\")\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"*\", help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", default=False, help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", default=False, help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts, specifying how many"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Command Line Interface for XAgent')\n    parser.add_argument('--task', type=str, required=True, help='Task description')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='Files to upload')\n    parser.add_argument('--model', type=str, help='Model identifier')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs')\n    parser.add_argument('--mode', type=str, default=\"auto\", help='Operational mode, can be \"auto\" or \"manual\"')\n    parser.add_argument('--quiet', action='store_true', help='Run in quiet mode, outputting to a file instead of the terminal')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Enable asking for human assistance during task execution')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts')\n    parser.add_argument('--config-file', type=str, default=os.environ.get('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file')\n\n    return parser.parse_args()"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"XAgent Command Line Interface\")\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"+\", help=\"List of files to upload\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", help=\"Operational mode, can be 'auto' or 'manual'\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"Run in quiet mode with minimal output\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Enable asking for human assistance\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts\")\n    parser.add_argument(\"--config-file\", type=str, default=os.getenv(\"CONFIG_FILE\", \"assets/config.yml\"), help=\"Path to the configuration file\")\n    return parser.parse_args()"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Command Line Interface for XAgent')\n    parser.add_argument('--task', type=str, required=True, help='Task description')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload')\n    parser.add_argument('--model', type=str, help='Model identifier for the task')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs')\n    parser.add_argument('--mode', type=str, default=\"auto\", help='Operational mode, auto or manual')\n    parser.add_argument('--quiet', action='store_true', help='Run in quiet mode')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Enable asking for human assistance')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts')\n    parser.add_argument('--config-file', type=str, default=os.environ.get('CONFIG_FILE', 'assets/config.yml'),\n                        help='Path to the configuration file')\n    return parser.parse_args()"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Execute a task')\n    parser.add_argument('--task', type=str, required=True, help='The task description')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload')\n    parser.add_argument('--model', type=str, help='Model identifier for the task')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs')\n    parser.add_argument('--mode', type=str, default='auto', help='Operational mode, auto or manual')\n    parser.add_argument('--quiet', action='store_true', help='Run in quiet mode, outputting to a file')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Enable asking for human assistance during task execution')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts')\n    parser.add_argument('--config-file', type=str, default=os.environ.get('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file')\n    args = parser.parse_args()\n    return args"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[-1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points.\")\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[-1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points.\")\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points.\")\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points.\")\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[-1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points.\")\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape != (_, 2):\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape != (_, 2):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a list of 2D points. \"\n            f\"Expected shape (_, 2), got {v.shape}\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points.\")\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape != (_, 2):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a list of 2D points, \"\n            f\"got shape {v.shape} instead of (_, 2)\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[-1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must have a shape of (_, 2), got {v.shape}\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[-1] != 2:\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a list of 2D points. \"\n            f\"Got {v.shape} instead of (_, 2)\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a list of 2D points, \"\n            f\"got shape {v.shape}. Expected shape (_, 2)\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[-1] != 2:\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a list of 2D points, i.e. of shape (_, 2).\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape != (_, 2):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a list of 2D points. \"\n            f\"Expected shape of (_, 2), got {v.shape}\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[-1] != 2:\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a list of 2D points, got shape {v.shape}\"\n        )\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[-1] != 2:\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a list of 2D points. \"\n            f\"Received shape {v.shape} instead of (_, 2).\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape != (0, 2):\n        if len(v.shape) != 2 or v.shape[1] != 2:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must be a list of 2D points. Got shape {v.shape}.\"\n            )\n\n    return v"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    if n == 0:\n        return charset[0]\n    result = \"\"\n    while n > 0:\n        result = charset[n % len(charset)] + result\n        n //= len(charset)\n    return result"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    if n < 0:\n        raise ValueError(\"Input integer must be non-negative\")\n    if n == 0:\n        return charset[0]\n    result = \"\"\n    while n > 0:\n        result = charset[n % len(charset)] + result\n        n //= len(charset)\n    return result"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    if n < 0:\n        raise ValueError(\"Input integer must be non-negative\")\n    if n == 0:\n        return charset[0]\n    result = \"\"\n    while n > 0:\n        result = charset[n % len(charset)] + result\n        n //= len(charset)\n    return result"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    if n < 0:\n        return \"-\" + encode_int(-n)\n    if n < len(charset):\n        return charset[n]\n    return encode_int(n // len(charset)) + encode_int(n % len(charset))"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    if n == 0:\n        return charset[0]\n    s = \"\"\n    while n > 0:\n        s = charset[n % len(charset)] + s\n        n //= len(charset)\n    return s"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    if n < 0:\n        raise ValueError(\"Input integer must be non-negative.\")\n    if n == 0:\n        return charset[0]\n    result = []\n    while n > 0:\n        result.append(charset[n % len(charset)])\n        n //= len(charset)\n    return \"\".join(reversed(result))"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    if n < 0:\n        raise ValueError(\"Input integer must be non-negative\")\n    if n == 0:\n        return \"a\"\n    result = \"\"\n    while n > 0:\n        result = charset[n % len(charset)] + result\n        n = n // len(charset)\n    return result"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    if n < 0:\n        raise ValueError(\"Input integer must be non-negative\")\n    if n >= len(charset):\n        raise ValueError(\"Input integer must be less than the length of the character set\")\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    if n < 0:\n        return \"n\" + encode_int(-n)\n    if n < len(charset):\n        return charset[n]\n    return encode_int(n // len(charset)) + encode_int(n % len(charset))"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    charset = string.ascii_lowercase + string.digits + \"_\"\n    # Encode the integer into a single character\n    if n < 0:\n        raise ValueError(\"Input integer must be non-negative\")\n    if n == 0:\n        return charset[0]\n    result = \"\"\n    while n > 0:\n        result = charset[n % len(charset)] + result\n        n = n // len(charset)\n    return result"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    charset = string.ascii_lowercase + string.digits + \"_\"\n    # Convert the integer to a string representation\n    s = str(n)\n    # Initialize the encoded string\n    encoded = \"\"\n    # Iterate over the characters of the string\n    for c in s:\n        # Convert the character to an integer and append it to the encoded string\n        encoded += charset[int(c)]\n    # Return the encoded string\n    return encoded"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.clip(x, eps, 1.0 - eps))"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.clip(x, eps, 1.0 - eps))\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunk_index = 0\n        for interval in intervals:\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                chunk_index += 1\n                indexes[worker_idx] -= interval[1] - interval[0]\n            else:\n                break\n        chunks_index[worker_idx] = chunk_index\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunk_index = 0\n        for interval in intervals:\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                chunk_index += 1\n                indexes[worker_idx] -= interval[1] - interval[0]\n            else:\n                break\n        chunks_index[worker_idx] = chunk_index\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunk_index = 0\n        for interval in intervals:\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                chunk_index += 1\n                indexes[worker_idx] -= interval[1] - interval[0]\n\n        chunks_index[worker_idx] = chunk_index\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunk_index = 0\n        for interval in intervals:\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                chunk_index += 1\n                indexes[worker_idx] -= interval[1] - interval[0]\n            else:\n                break\n        chunks_index[worker_idx] = chunk_index\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, worker_intervals in workers_intervals.items():\n        chunk_index = 0\n        for interval in worker_intervals:\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                chunk_index += 1\n                indexes[worker_idx] -= interval[1] - interval[0]\n            else:\n                break\n        chunks_index[worker_idx] = chunk_index\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunk_index = 0\n        for interval in intervals:\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                chunk_index += 1\n                indexes[worker_idx] -= interval[1] - interval[0]\n\n        chunks_index[worker_idx] = chunk_index\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunk_index = 0\n        for interval in intervals:\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                chunk_index += 1\n                indexes[worker_idx] -= interval[1] - interval[0]\n        chunks_index[worker_idx] = chunk_index\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, worker_intervals in workers_intervals.items():\n        chunk_index = 0\n        for i, interval in enumerate(worker_intervals):\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                indexes[worker_idx] -= interval[1] - interval[0]\n                chunk_index += 1\n            else:\n                break\n        chunks_index[worker_idx] = chunk_index\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_indexes = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunk_index = 0\n        for interval in intervals:\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                indexes[worker_idx] -= interval[1] - interval[0]\n                chunk_index += 1\n            else:\n                break\n        chunk_indexes[worker_idx] = chunk_index\n    return chunk_indexes, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunk_index = 0\n        for i, interval in enumerate(intervals):\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                indexes[worker_idx] -= interval[1] - interval[0]\n                chunk_index = i + 1\n            else:\n                break\n        chunks_index[worker_idx] = chunk_index\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunks_index[worker_idx] = 0\n        for i, interval in enumerate(intervals):\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                chunks_index[worker_idx] = i + 1\n                indexes[worker_idx] -= interval[1] - interval[0]\n            else:\n                break\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, worker_intervals in workers_intervals.items():\n        chunk_index = 0\n        for i, interval in enumerate(worker_intervals):\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                indexes[worker_idx] -= interval[1] - interval[0]\n                chunk_index += 1\n            else:\n                break\n        chunks_index[worker_idx] = chunk_index\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx in range(len(workers_intervals)):\n        chunk_index = 0\n        for i, interval in enumerate(workers_intervals[worker_idx]):\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                indexes[worker_idx] -= interval[1] - interval[0]\n                chunk_index = i + 1\n            else:\n                break\n        chunks_index[worker_idx] = chunk_index\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx in range(len(workers_intervals)):\n        chunk_index = 0\n        for i, interval in enumerate(workers_intervals[worker_idx]):\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                indexes[worker_idx] -= interval[1] - interval[0]\n                chunk_index += 1\n            else:\n                break\n        chunks_index[worker_idx] = chunk_index\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunk_index = 0\n        for i, interval in enumerate(intervals):\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                chunk_index = i + 1\n                indexes[worker_idx] -= interval[1] - interval[0]\n            else:\n                break\n        chunks_index[worker_idx] = chunk_index\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx in range(len(workers_intervals)):\n        chunk_index = 0\n        for interval in workers_intervals[worker_idx]:\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                chunk_index += 1\n                indexes[worker_idx] -= interval[1] - interval[0]\n        chunks_index[worker_idx] = chunk_index\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx in range(len(workers_intervals)):\n        chunk_index = 0\n        for i, interval in enumerate(workers_intervals[worker_idx]):\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                chunk_index = i + 1\n                indexes[worker_idx] -= interval[1] - interval[0]\n            else:\n                break\n\n        chunks_index[worker_idx] = chunk_index\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        current_index = 0\n        chunk_index = 0\n        for interval in intervals:\n            if current_index + interval[1] - interval[0] > indexes[worker_idx]:\n                current_index += indexes[worker_idx]\n                break\n            current_index += interval[1] - interval[0]\n            chunk_index += 1\n        chunks_index[worker_idx] = chunk_index\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunk_index = 0\n        current_index = 0\n        for interval in intervals:\n            if current_index + interval[1] - interval[0] > indexes[worker_idx]:\n                chunks_index[worker_idx] = chunk_index\n                indexes[worker_idx] = current_index + interval[1] - interval[0] - indexes[worker_idx]\n                break\n            else:\n                current_index += interval[1] - interval[0]\n                chunk_index += 1\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx in range(len(workers_intervals)):\n        chunks_index[worker_idx] = 0\n\n    for worker_idx, worker_intervals in workers_intervals.items():\n        for i, interval in enumerate(worker_intervals):\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                indexes[worker_idx] = indexes[worker_idx] - (interval[1] - interval[0])\n                chunks_index[worker_idx] += 1\n\n    return chunks_index, indexes"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # The grid is a 4D array with shape (D,H,W,C).\n    # The coordinates are a 2D array with shape (..., 3).\n    # The output is a 2D array with shape (..., C).\n    # The coordinates are assumed to be in the range [0, 1].\n    # The output is interpolated using trilinear interpolation.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same shape as the coordinates.\n    # The output is the same"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # Sample from a 3D voxel grid.\n    # This is the standard trilinear interpolation.\n    # The coordinates are assumed to be in [0, 1]^3.\n    # The values are assumed to be in [0, 1]^3.\n    # The values are assumed to be in [0, 1]^3.\n    # The values are assumed to be in [0, 1]^3.\n    # The values are assumed to be in [0, 1]^3.\n    # The values are assumed to be in [0, 1]^3.\n    # The values are assumed to be in [0, 1]^3.\n    # The values are assumed to be in [0, 1]^3.\n    # The values are assumed to be in [0, 1]^3.\n    # The values are assumed to be in [0, 1]^3.\n    # The values are assumed to be in [0, 1]^3.\n    # The values are assumed to be in [0, 1]^3.\n    # The values are assumed to be in [0, 1]^3.\n    # The values are assumed to be in [0, 1]^3.\n    # The values are assumed to be in [0, 1]^3.\n    # The values are assumed to be in [0, 1]^3.\n    # The values are assumed to be in [0, 1]^3.\n    # The values are assumed to be in [0, 1]^3.\n    # The values are assumed to be in [0, 1]^3.\n    # The values are assumed to be in [0, 1]^3.\n    # The values are assumed to be in [0, 1]^3.\n    # The values are assumed to be in [0, 1]^3.\n    # The values are assumed to be in [0, 1]^3.\n    # The values are assumed to be in [0, 1]^3.\n    # The values are"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return resample.trilerp(values, coordinates)\n  elif datastructure == 'hash':\n    return hash_resample.trilerp(values, coordinates)\n  else:\n    raise ValueError(\n        f'trilerp does not support datastructure {datastructure}.'\n        ' Supported datastructures are \"grid\" and \"hash\".'\n    )"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # The grid is a 3D voxel grid.\n    # The coordinates are assumed to be within the bounds of the grid.\n    # The values are assumed to be a 4D array with shape (D,H,W,C) representing a 3D voxel grid with channels.\n    # The coordinates are assumed to be a 2D or higher array with shape (..., 3) containing the coordinates for sampling.\n    # The output is a 2D or higher array with shape (..., C) containing the interpolated values at the specified coordinates.\n    # The shape of the output array matches the input coordinates array, with an additional dimension for channels.\n    # The coordinates are assumed to be within the bounds of the grid.\n    # The values are assumed to be a 4D array with shape (D,H,W,C) representing a 3D voxel grid with channels.\n    # The coordinates are assumed to be a 2D or higher array with shape (..., 3) containing the coordinates for sampling.\n    # The output is a 2D or higher array with shape (..., C) containing the interpolated values at the specified coordinates.\n    # The shape of the output array matches the input coordinates array, with an additional dimension for channels.\n    # The coordinates are assumed to be within the bounds of the grid.\n    # The values are assumed to be a 4D array with shape (D,H,W,C) representing a 3D voxel grid with channels.\n    # The coordinates are assumed to be a 2D or higher array with shape (..., 3) containing the coordinates for sampling.\n    # The output is a 2D or higher array with shape (..., C) containing the interpolated values at the specified coordinates.\n    # The shape of the output array matches the input coordinates array, with an additional dimension for channels.\n    # The coordinates are assumed to be within the bounds of the grid.\n    # The values are assumed to be a 4D array with shape (D,H,W,C) representing a 3D voxel grid with channels.\n    # The coordinates are assumed to be a 2D or higher array with"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # The grid is assumed to be of shape (D, H, W, C).\n    # The coordinates are assumed to be of shape (..., 3).\n    # The output is of shape (..., C).\n    # The coordinates are assumed to be within the bounds of the grid.\n    # The coordinates are assumed to be in the range [0, 1].\n    # The coordinates are assumed to be in the range [0, 1].\n    # The coordinates are assumed to be in the range [0, 1].\n    # The coordinates are assumed to be in the range [0, 1].\n    # The coordinates are assumed to be in the range [0, 1].\n    # The coordinates are assumed to be in the range [0, 1].\n    # The coordinates are assumed to be in the range [0, 1].\n    # The coordinates are assumed to be in the range [0, 1].\n    # The coordinates are assumed to be in the range [0, 1].\n    # The coordinates are assumed to be in the range [0, 1].\n    # The coordinates are assumed to be in the range [0, 1].\n    # The coordinates are assumed to be in the range [0, 1].\n    # The coordinates are assumed to be in the range [0, 1].\n    # The coordinates are assumed to be in the range [0, 1].\n    # The coordinates are assumed to be in the range [0, 1].\n    # The coordinates are assumed to be in the range [0, 1].\n    # The coordinates are assumed to be in the range [0, 1].\n    # The coordinates are assumed to be in the range [0, 1].\n    # The coordinates are assumed to be in the range [0, 1].\n    # The coordinates are assumed to be in the range [0, 1].\n    # The coordinates are assumed to be in the range [0, 1].\n    # The coordinates are assumed to be in the range [0, 1].\n    # The coordinates are assumed to be in the range [0, 1].\n    # The coordinates are assumed to be in"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # For smaller levels (fewer cells), store entries in a dense grid.\n    values = values.astype(jnp.float32)\n    coordinates = coordinates.astype(jnp.float32)\n    return resample.trilerp(values, coordinates)\n  elif datastructure == 'hash':\n    # For larger levels (more cells), store entries in a hash table.\n    values = values.astype(jnp.float32)\n    coordinates = coordinates.astype(jnp.float32)\n    return hash_resample.trilerp(values, coordinates)\n  else:\n    raise ValueError(f'Invalid datastructure: {datastructure}.')"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # Coordinates are in [0, 1]^3.\n    # We sample from a 3D voxel grid.\n    # We need to rescale the coordinates to the voxel grid.\n    # We also need to rescale the coordinates to the voxel grid.\n    # We need to rescale the coordinates to the voxel grid.\n    # We need to rescale the coordinates to the voxel grid.\n    # We need to rescale the coordinates to the voxel grid.\n    # We need to rescale the coordinates to the voxel grid.\n    # We need to rescale the coordinates to the voxel grid.\n    # We need to rescale the coordinates to the voxel grid.\n    # We need to rescale the coordinates to the voxel grid.\n    # We need to rescale the coordinates to the voxel grid.\n    # We need to rescale the coordinates to the voxel grid.\n    # We need to rescale the coordinates to the voxel grid.\n    # We need to rescale the coordinates to the voxel grid.\n    # We need to rescale the coordinates to the voxel grid.\n    # We need to rescale the coordinates to the voxel grid.\n    # We need to rescale the coordinates to the voxel grid.\n    # We need to rescale the coordinates to the voxel grid.\n    # We need to rescale the coordinates to the voxel grid.\n    # We need to rescale the coordinates to the voxel grid.\n    # We need to rescale the coordinates to the voxel grid.\n    # We need to rescale the coordinates to the voxel grid.\n    # We need to rescale the coordinates to the voxel grid.\n    # We need to rescale the coordinates to the voxel grid.\n    # We need to rescale the coordinates to the voxel grid.\n    # We need to rescale the coordinates to the voxel grid.\n    # We need to rescale the coordinates to the voxel grid.\n    # We need to rescale the coordinates to the voxel grid"}
{"namespace": "grid_utils.trilerp", "completion": "  # TODO(barron): Add support for 3D voxel grid.\n  if datastructure == 'grid':\n    # TODO(barron): Add support for 3D voxel grid.\n    raise NotImplementedError('Only hash data structure is supported.')\n\n  # TODO(barron): Add support for 3D voxel grid.\n  if datastructure == 'hash':\n    # TODO(barron): Add support for 3D voxel grid.\n    raise NotImplementedError('Only hash data structure is supported.')\n\n  raise ValueError(f'Invalid datastructure {datastructure}.')"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # Convert coordinates to grid indices.\n    coordinates = (coordinates - 0.5) * (values.shape[:3] - 1)\n    coordinates = jnp.floor(coordinates).astype(jnp.int32)\n\n    # Clip coordinates to the valid range.\n    coordinates = jnp.clip(coordinates, 0, values.shape[:3] - 2)\n\n    # Perform trilinear interpolation.\n    x0, y0, z0 = coordinates[..., 0], coordinates[..., 1], coordinates[..., 2]\n    x1, y1, z1 = x0 + 1, y0 + 1, z0 + 1\n\n    v000 = values[x0, y0, z0]\n    v100 = values[x1, y0, z0]\n    v010 = values[x0, y1, z0]\n    v110 = values[x1, y1, z0]\n    v001 = values[x0, y0, z1]\n    v101 = values[x1, y0, z1]\n    v011 = values[x0, y1, z1]\n    v111 = values[x1, y1, z1]\n\n    x, y, z = coordinates[..., 0] - x0, coordinates[..., 1] - y0, coordinates[\n        ..., 2\n    ] - z0\n\n    v00 = v000 * (1 - x) + v100 * x\n    v10 = v010 * (1 - x) + v110 * x\n    v01 = v001 * (1 - x) + v101 * x\n    v11 = v011 * (1 - x) + v111 * x\n\n    v0 = v00 * (1 - y) + v10 * y\n    v1 = v01 * (1 - y) + v11 * y\n\n   "}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # For smaller levels (fewer cells), store entries in a dense grid.\n    # Scale `coordinates` by the grid size to get the indices of the coordinates.\n    grid_size = values.shape[0]\n    x = coordinates * grid_size\n\n    # Optionally jitter the scaled coordinates by [-0.5, 0.5).\n    if jitter_coordinates:\n      x += random.uniform(x.shape) - 0.5\n\n    # Clamp to the range [0, grid_size - 1].\n    x = jnp.clip(x, 0, grid_size - 1)\n\n    # Interpolate into `values` to get a per-coordinate feature vector.\n    f = resample.trilerp(values, x)\n\n  elif datastructure == 'hash':\n    # For larger levels (more cells), store entries in a hash table.\n    # Hash the coordinates into a table with T entries.\n    x = hash_resample.hash_coordinates(coordinates)\n\n    # Interpolate into `values` to get a per-coordinate feature vector.\n    f = hash_resample.trilerp(values, x)\n\n  else:\n    raise ValueError(\n        f'Unsupported datastructure {datastructure}. '\n        'Only \"grid\" and \"hash\" are supported.'\n    )\n\n  return f"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    values = onp.array(values)\n    values = values.reshape((values.shape[0], values.shape[1], values.shape[2], -1))\n    values = values.transpose((3, 0, 1, 2))\n    coordinates = onp.array(coordinates)\n    coordinates = coordinates.reshape((coordinates.shape[0], -1, 3))\n    coordinates = coordinates.transpose((1, 0, 2))\n    coordinates = coordinates.astype(onp.int32)\n    coordinates = onp.clip(coordinates, 0, values.shape[1] - 1)\n    values = values.reshape((values.shape[0], values.shape[1] * values.shape[2] * values.shape[3]))\n    values = values.transpose((1, 0))\n    values = values.reshape((values.shape[0], values.shape[1], 1))\n    values = values.transpose((1, 0, 2))\n    values = values.reshape((values.shape[0], values.shape[1], values.shape[2], -1))\n    values = values.transpose((3, 0, 1, 2))\n    values = values.reshape((values.shape[0], values.shape[1] * values.shape[2], values.shape[3]))\n    values = values.transpose((1, 0, 2))\n    values = values.reshape((values.shape[0], values.shape[1], values.shape[2], -1))\n    values = values.transpose((3, 0, 1, 2))\n    values = values.reshape((values.shape[0], values.shape[1] * values.shape[2] * values.shape[3]))\n    values = values.transpose((1, 0))\n    values = values.reshape((values.shape[0], values.shape[1], 1))\n    values = values.transpose((1, 0, 2))\n    values = values."}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    values = onp.asarray(values)\n    coordinates = onp.asarray(coordinates)\n    values = jnp.asarray(values)\n    coordinates = jnp.asarray(coordinates)\n    values = jnp.transpose(values, (0, 1, 2, 3, 4))\n    values = jnp.reshape(values, (-1, values.shape[4]))\n    values = jnp.reshape(values, (values.shape[0], values.shape[1], values.shape[2], values.shape[3], 1, values.shape[4]))\n    coordinates = jnp.reshape(coordinates, (-1, coordinates.shape[1], 1, 1, 1, 1))\n    coordinates = jnp.transpose(coordinates, (0, 1, 2, 3, 4, 5))\n    values = jnp.reshape(values, (-1, values.shape[1], values.shape[2], values.shape[3], values.shape[5]))\n    values = jnp.reshape(values, (values.shape[0], values.shape[1], values.shape[2], values.shape[3], values.shape[4], 1))\n    values = jnp.transpose(values, (0, 1, 2, 3, 4, 5))\n    values = jnp.reshape(values, (-1, values.shape[2], values.shape[3], values.shape[4], values.shape[5]))\n    values = jnp.reshape(values, (values.shape[0], values.shape[1], values.shape[2], values.shape[3], 1, values.shape[4]))\n    values = jnp.reshape(values, (-1, values.shape[1], values.shape[2], values.shape[3], values.shape[4], values.shape[5]))\n    values = jnp.transpose(values, (0, 1, 2, 3, 5, 4))\n    values = jnp"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    values = onp.asarray(values)\n    coordinates = onp.asarray(coordinates)\n    # Check if the coordinates are within the bounds of the dimensions they\n    # refer to.\n    if not (\n        (coordinates >= 0).all()\n        and (coordinates <= 1).all()\n        and (coordinates.ndim >= 2)\n        and (coordinates.shape[-1] == 3)\n    ):\n      raise ValueError(\n          'Coordinates must be within the bounds of the dimensions they refer '\n          'to.'\n      )\n    # Adjust the coordinates to the range [0, 1]^3.\n    coordinates = coordinates * (values.shape[:3] - 1)\n    # Convert the coordinates to integers.\n    coordinates = onp.floor(coordinates).astype(onp.int32)\n    # Clip the coordinates to the range [0, D-1]^3.\n    coordinates = onp.clip(coordinates, 0, values.shape[:3] - 1)\n    # Perform trilinear interpolation.\n    c000 = values[coordinates[..., 0], coordinates[..., 1], coordinates[..., 2]]\n    c001 = values[\n        coordinates[..., 0], coordinates[..., 1], coordinates[..., 2] + 1\n    ]\n    c010 = values[\n        coordinates[..., 0], coordinates[..., 1] + 1, coordinates[..., 2]\n    ]\n    c011 = values[\n        coordinates[..., 0], coordinates[..., 1] + 1, coordinates[..., 2] + 1\n    ]\n    c100 = values[\n        coordinates[..., 0] + 1, coordinates[..., 1], coordinates[..., 2]\n    ]\n    c101 = values[\n        coordinates[..., 0] + 1, coordinates[..., 1], coordinates[..., 2] + 1\n    ]\n    c110 = values["}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    values = onp.array(values)\n    coordinates = onp.array(coordinates)\n    # Adjust the coordinates to be within the bounds of the voxel grid.\n    # The coordinates are clipped to the minimum and maximum values of the\n    # voxel grid.\n    coordinates = jnp.clip(coordinates, 0, values.shape[0] - 1)\n    # Perform trilinear interpolation on the values using the adjusted\n    # coordinates.\n    f = resample.trilerp(values, coordinates)\n  elif datastructure == 'hash':\n    values = onp.array(values)\n    coordinates = onp.array(coordinates)\n    # Adjust the coordinates to be within the bounds of the hash data structure.\n    # The coordinates are clipped to the minimum and maximum values of the\n    # hash data structure.\n    coordinates = jnp.clip(coordinates, 0, values.shape[0] - 1)\n    # Perform trilinear interpolation on the values using the adjusted\n    # coordinates.\n    f = hash_resample.trilerp(values, coordinates)\n  else:\n    raise ValueError(f'datastructure {datastructure} not recognized.')\n\n  return f"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # x, y, z, c -> (x, y, z, c)\n    coordinates = jnp.transpose(coordinates, (0, 3, 1, 2))\n    values = jnp.transpose(values, (0, 3, 1, 2))\n\n    # Bilinearly interpolate the values.\n    values_000 = resample.bilinear_sample(values, coordinates)\n    values_001 = resample.bilinear_sample(values, coordinates + [0, 0, 1])\n    values_010 = resample.bilinear_sample(values, coordinates + [0, 1, 0])\n    values_011 = resample.bilinear_sample(values, coordinates + [0, 1, 1])\n    values_100 = resample.bilinear_sample(values, coordinates + [1, 0, 0])\n    values_101 = resample.bilinear_sample(values, coordinates + [1, 0, 1])\n    values_110 = resample.bilinear_sample(values, coordinates + [1, 1, 0])\n    values_111 = resample.bilinear_sample(values, coordinates + [1, 1, 1])\n\n    # Trilinearly interpolate the values.\n    values_00 = values_000 * (1 - coordinates[..., 2]) + values_001 * coordinates[\n        ..., 2\n    ]\n    values_01 = values_010 * (1 - coordinates[..., 2]) + values_011 * coordinates[\n        ..., 2\n    ]\n    values_10 = values_100 * (1 - coordinates[..., 2]) + values_101 * coordinates[\n        ..., 2\n    ]\n    values_11 = values_110 * (1 - coordinates[..., 2]) + values_111 * coordinates["}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    values = onp.asarray(values)\n    coordinates = onp.asarray(coordinates)\n    if len(values.shape) != 4:\n      raise ValueError(\n          'values must have shape (D,H,W,C) for grid, not', values.shape\n      )\n    if len(coordinates.shape) < 2:\n      raise ValueError(\n          'coordinates must have shape (..., 3) for grid, not',\n          coordinates.shape,\n      )\n    if len(coordinates.shape) > len(values.shape):\n      raise ValueError(\n          'coordinates must have shape (..., 3) for grid, not',\n          coordinates.shape,\n      )\n    if len(coordinates.shape) > 2:\n      coordinates = coordinates.reshape(\n          (-1,) + coordinates.shape[-2:]\n      )  # (..., 3) -> (..., 1, 3)\n    if len(values.shape) > 2:\n      values = values.reshape(\n          (-1,) + values.shape[-2:]\n      )  # (D,H,W,C) -> (1,D,H,W,C)\n    # values.shape == (1,D,H,W,C)\n    # coordinates.shape == (..., 1, 3)\n    # output.shape == (..., C)\n    output = resample.trilinear_sample(values, coordinates)\n    if len(coordinates.shape) > 2:\n      output = output.reshape(coordinates.shape[:-2] + output.shape[-1:])\n    return output\n\n  elif datastructure == 'hash':\n    values = onp.asarray(values)\n    coordinates = onp.asarray(coordinates)\n    if len(values.shape) != 2:\n      raise ValueError(\n          'values must have shape (N,C) for hash, not', values.shape\n      )\n    if len(coordinates.shape) < 2:\n      raise ValueError(\n          '"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # Sample from a 3D voxel grid.\n    values = onp.asarray(values)\n    coordinates = onp.asarray(coordinates)\n    if values.ndim != 4:\n      raise ValueError(\n          'values should be a 4D array with shape (D,H,W,C), got '\n          + f'{values.shape}'\n      )\n    if coordinates.ndim < 2 or coordinates.shape[-1] != 3:\n      raise ValueError(\n          'coordinates should be a 2D or higher array with shape (..., 3), '\n          + f'got {coordinates.shape}'\n      )\n\n    # Adjust the coordinates to be within the bounds of the dimensions they\n    # refer to.\n    coordinates = jnp.clip(coordinates, 0, 1)\n\n    # Compute the indices of the surrounding voxels.\n    indices = jnp.floor(coordinates * values.shape[:3]).astype(jnp.int32)\n\n    # Compute the interpolation weights.\n    weights = coordinates - indices.astype(jnp.float32)\n\n    # Compute the indices of the surrounding voxels.\n    indices_flat = indices[..., 0] * values.shape[1] * values.shape[2] + indices[\n        ..., 1\n    ] * values.shape[2] + indices[..., 2]\n\n    # Perform trilinear interpolation.\n    values_flat = values.reshape([-1, values.shape[-1]])\n    values_flat = jnp.take(values_flat, indices_flat, axis=0)\n    f = math.trilinear_interpolate(values_flat, weights)\n\n  elif datastructure == 'hash':\n    # Sample from a hash data structure.\n    values = onp.asarray(values)\n    coordinates = onp.asarray(coordinates)\n    if values.ndim != 2:\n      raise ValueError(\n          'values should be a 2D"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # Sample from a 3D voxel grid.\n    # This is a special case of trilinear interpolation.\n    # See Section 3.3 of the Instant NGP paper.\n    values = values.astype(jnp.float32)\n    coordinates = coordinates.astype(jnp.float32)\n    voxel_coords = coordinates.astype(jnp.int32)\n    voxel_fracs = coordinates - voxel_coords\n    voxel_fracs = jnp.clip(voxel_fracs, 0.0, 1.0)\n    voxel_fracs = voxel_fracs.astype(jnp.float32)\n    voxel_fracs_prod = jnp.prod(voxel_fracs, axis=-1)\n    voxel_fracs_prod = voxel_fracs_prod.astype(jnp.float32)\n    voxel_fracs_prod_1m = 1.0 - voxel_fracs_prod\n    voxel_fracs_prod_1m = voxel_fracs_prod_1m.astype(jnp.float32)\n    voxel_fracs_1m = 1.0 - voxel_fracs\n    voxel_fracs_1m = voxel_fracs_1m.astype(jnp.float32)\n    voxel_fracs_1m_prod = jnp.prod(voxel_fracs_1m, axis=-1)\n    voxel_fracs_1m_prod = voxel_fracs_1m_prod.astype(jnp.float32)\n    voxel_fracs_prod_1m_prod = voxel_fracs_prod_1m * voxel_fracs_1m_prod\n    voxel_fracs_prod_1m_prod = voxel_fracs_prod_1m_prod.astype("}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    values = onp.asarray(values)\n    coordinates = onp.asarray(coordinates)\n    if len(coordinates.shape) > 3:\n      raise ValueError(\n          f'coordinates must be a 2D or higher array, got shape {coordinates.shape}'\n      )\n    if len(values.shape) != 4:\n      raise ValueError(\n          f'values must be a 4D array with shape (D,H,W,C), got shape {values.shape}'\n      )\n    if len(coordinates.shape) == 2:\n      coordinates = coordinates[..., None]\n    if len(values.shape) == 3:\n      values = values[None]\n    if len(coordinates.shape) == 3:\n      coordinates = coordinates[..., None]\n    if len(values.shape) == 4:\n      values = values[..., None]\n    if len(coordinates.shape) != 4 or len(values.shape) != 5:\n      raise ValueError(\n          f'coordinates and values must be 4D arrays, got shapes {coordinates.shape} and {values.shape}'\n      )\n    if not (\n        coordinates.shape[-1] == 3\n        and coordinates.shape[:-1] == values.shape[:-2]\n    ):\n      raise ValueError(\n          f'coordinates and values must have matching shapes, got shapes {coordinates.shape} and {values.shape}'\n      )\n    # Rescale the coordinates to the range [0, 1]^3.\n    coordinates = coordinates / (values.shape[:3] - 1)\n    # Perform trilinear interpolation.\n    # The implementation uses the following convention:\n    #   - coordinates are in the range [0, 1]^3\n    #   - values are in the range [0, 1]^3\n    #   - the coordinates are the centers of the voxels\n    #   - the values are the values of the voxels\n    #   - the interpolation is done in the range [0, 1]^3"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # 3D voxel grid.\n    values = onp.asarray(values)\n    coordinates = onp.asarray(coordinates)\n    if values.ndim != 4:\n      raise ValueError(\n          f'values.ndim = {values.ndim} != 4 (expected a 4D array).'\n      )\n    if coordinates.ndim < 2:\n      raise ValueError(\n          f'coordinates.ndim = {coordinates.ndim} < 2 (expected at least a 2D array).'\n      )\n    if coordinates.shape[-1] != 3:\n      raise ValueError(\n          f'coordinates.shape[-1] = {coordinates.shape[-1]} != 3 (expected a 3D array).'\n      )\n    if not (\n        (coordinates >= 0).all()\n        and (coordinates < values.shape[:3]).all()\n    ):\n      raise ValueError(\n          'coordinates should be within the bounds of the dimensions they refer to.'\n      )\n    # Trilinearly interpolate.\n    # Note: We use jnp.floor instead of jnp.trunc,\n    # because jnp.trunc(0.5) = 0,\n    # which is not the same as jnp.floor(0.5) = 0.\n    # (This is a difference between jnp.trunc and jnp.floor.)\n    # TODO(barron): Use jnp.floor instead of jnp.trunc.\n    xi = jnp.floor(coordinates).astype(jnp.int32)\n    xf = coordinates - xi\n    xf = jnp.clip(xf, 0, 1)\n    c000 = values[xi[..., 0], xi[..., 1], xi[..., 2]]\n    c100 = values[xi[..., 0] + 1, xi[..., 1], xi[..., 2]]\n    c010 = values[xi[..., 0], xi["}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Generate the integer weights for each vertex of the triangle.\n  weights = np.array(list(itertools.product(range(v + 1), repeat=3)))\n  weights = weights[weights.sum(axis=1) <= v, :]\n\n  # Normalize the weights to get the barycentric coordinates.\n  weights = weights / weights.sum(axis=1, keepdims=True)\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Generate the weights for each vertex of the triangle.\n  weights = np.zeros((v**3, 3))\n  for i in range(v):\n    for j in range(v):\n      for k in range(v):\n        weights[i * v**2 + j * v + k, :] = [i, j, k]\n\n  # Normalize the weights to get the barycentric coordinates.\n  weights /= v**3\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError(f'v {v} must be >= 1')\n  weights = []\n  for i in range(v):\n    for j in range(v - i):\n      for k in range(v - i - j):\n        weights.append((i, j, k))\n  weights = np.array(weights)\n  weights = weights / np.sum(weights, 1, keepdims=True)\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if not isinstance(v, int):\n    raise ValueError(f'v {v} must an integer')\n  if v < 1:\n    raise ValueError(f'v {v} must be >= 1')\n  weights = np.zeros((v, v, v, 3))\n  for i in range(v):\n    for j in range(v):\n      for k in range(v):\n        weights[i, j, k, 0] = i / v\n        weights[i, j, k, 1] = j / v\n        weights[i, j, k, 2] = k / v\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Generate the integer weights for each vertex of the triangle.\n  weights = np.array([\n      np.arange(v + 1),\n      np.arange(v + 1),\n      np.arange(v + 1),\n  ])\n\n  # Normalize the weights to get the barycentric coordinates.\n  weights = weights / np.sum(weights, axis=0)\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError(f'v {v} must be >= 1')\n\n  weights = []\n  for i in range(v + 1):\n    for j in range(v + 1 - i):\n      weights.append((i, j, v - i - j))\n\n  weights = np.array(weights)\n  weights = weights / np.sum(weights, 1, keepdims=True)\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError('v must be >= 1')\n  if v == 1:\n    return np.ones([3, 1])\n\n  weights = np.zeros([3, v**3])\n  for i in range(v):\n    for j in range(v):\n      for k in range(v):\n        weights[0, i * v**2 + j * v + k] = i / v\n        weights[1, i * v**2 + j * v + k] = j / v\n        weights[2, i * v**2 + j * v + k] = k / v\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Generate the weights for the vertices of the triangle.\n  weights = np.array([\n      np.arange(v + 1),\n      np.arange(v + 1),\n      np.arange(v + 1),\n  ])\n  weights = weights.T\n\n  # Normalize the weights to get the barycentric coordinates.\n  weights /= np.sum(weights, axis=1, keepdims=True)\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  w = np.arange(v + 1)\n  weights = np.array(list(itertools.product(w, repeat=3)))\n  weights = weights[np.sum(weights, axis=1) == v, :]\n  weights = weights / np.sum(weights, axis=1, keepdims=True)\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  weights = np.zeros((v + 1)**3)\n  for i in range(v + 1):\n    for j in range(v + 1):\n      for k in range(v + 1):\n        weights[i * (v + 1)**2 + j * (v + 1) + k] = (\n            i / v * (v - i) + j / v * (v - j) + k / v * (v - k)\n        )\n  weights = np.sort(weights)\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if not isinstance(v, int):\n    raise ValueError(f'v {v} must an integer')\n  if v < 1:\n    raise ValueError(f'v {v} must be greater than or equal to 1')\n\n  # Generate integer weights for each vertex of the triangle.\n  weights = np.zeros((v + 1)**3, dtype=np.int32)\n  for i in range(v + 1):\n    for j in range(v + 1 - i):\n      k = v - i - j\n      weights[i * (v + 1)**2 + j * (v + 1) + k] = 1\n\n  # Normalize the weights to get the barycentric coordinates.\n  weights = weights / np.sum(weights)\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Generate the weights for the triangle.\n  weights = np.zeros((v**2, 3))\n  for i in range(v):\n    for j in range(v):\n      weights[i * v + j, 0] = i\n      weights[i * v + j, 1] = j\n      weights[i * v + j, 2] = v - i - j\n  weights = weights / v\n\n  # Normalize the weights.\n  weights = weights / np.sqrt(np.sum(weights**2, 1, keepdims=True))\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Compute the barycentric weights for the vertices of a triangle.\n  weights = np.zeros((3, 3**v))\n  for i in range(3):\n    for j in range(3**v):\n      weights[i, j] = (j // (3**(v - 1))) % 3\n  weights = weights.T\n\n  # Normalize the weights to get the barycentric coordinates.\n  weights /= np.sum(weights, axis=1, keepdims=True)\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Compute the barycentric weights for each vertex of the triangle.\n  weights = []\n  for i in range(v):\n    for j in range(v - i):\n      weights.append((i, j, v - i - j))\n  weights = np.array(weights)\n  weights = weights / v\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Compute the barycentric weights for each vertex of the triangle.\n  weights = np.zeros((3, v**2))\n  for i in range(3):\n    weights[i, :] = np.arange(v) + np.arange(v)[:, None]\n  weights = weights.reshape((-1,))\n\n  # Normalize the weights to get the barycentric coordinates.\n  weights = weights / np.sum(weights)\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if not isinstance(v, int):\n    raise ValueError(f'v {v} must an integer')\n  if v < 1:\n    raise ValueError(f'v {v} must be greater than or equal to 1')\n  # Generate the weights for each vertex of the triangle.\n  weights = np.zeros((3, v + 1))\n  weights[0, 0] = 1\n  for i in range(1, v + 1):\n    weights[0, i] = weights[0, i - 1] * (v - i + 1) / i\n  for i in range(1, 3):\n    for j in range(v + 1):\n      weights[i, j] = weights[i - 1, j] * (v - j) / (j + 1)\n  # Normalize the weights to get the barycentric coordinates.\n  weights /= np.sum(weights, 1, keepdims=True)\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if not isinstance(v, int):\n    raise ValueError(f'v {v} must an integer')\n  if v < 1:\n    raise ValueError(f'v {v} must be >= 1')\n\n  weights = np.zeros((v**2, 3))\n  for i in range(v):\n    for j in range(v):\n      weights[i * v + j, 0] = i\n      weights[i * v + j, 1] = j\n      weights[i * v + j, 2] = v - i - j\n  weights = np.floor(weights)\n  weights = weights / np.sum(weights, 1, keepdims=True)\n  weights = weights[weights[:, 2] >= 0, :]\n  weights = weights[weights[:, 1] >= 0, :]\n  weights = weights[weights[:, 0] >= 0, :]\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError(f'v {v} must be greater than or equal to 1')\n  if v == 1:\n    return np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n  # Compute the barycentric weights for each vertex of the triangle.\n  weights = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n  for i in range(2, v):\n    weights = np.concatenate([\n        weights,\n        np.array([[i - 1, 1, 0], [0, i - 1, 1], [1, 0, i - 1]]),\n    ])\n  weights = np.concatenate([weights, np.array([[v - 1, 1, 1]])])\n  weights = weights / v\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Compute the weights for each vertex of the triangle.\n  weights = np.array(\n      [\n          [0, 0, 1],\n          [1, 0, 0],\n          [0, 1, 0],\n      ]\n  )\n\n  # If the tessellation factor is greater than 1, then we need to subdivide each edge of the triangle.\n  if v > 1:\n    # Compute the number of subdivisions for each edge.\n    num_subdivisions = np.array([v, v, v])\n\n    # Compute the weights for each subdivision of each edge.\n    weights = np.repeat(weights, num_subdivisions, axis=0)\n    weights = np.repeat(weights, num_subdivisions, axis=1)\n    weights = weights / np.sum(weights, axis=0)\n\n    # Compute the weights for each vertex of the triangle.\n    weights = np.sum(weights, axis=1)\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError(f'v {v} must be >= 1')\n  if v == 1:\n    return np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n  else:\n    # The weights are computed by considering the vertices of a triangle as\n    # points in the 3D space. We then generate a grid of points in the\n    # triangle, and compute the distance from each point to the triangle's\n    # vertices. We then normalize the weights by dividing by the sum of the\n    # distances.\n    #\n    # We generate the grid of points by first generating a regular grid of\n    # points in the xy-plane, and then adding a small offset to the z-coordinate\n    # of each point. The offset is chosen such that the points are evenly\n    # distributed in the xy-plane, but are offset in the z-direction to ensure\n    # that the points are inside the triangle.\n    #\n    # We then compute the distance from each point to the triangle's vertices\n    # using the dot product. We then normalize the weights by dividing by the\n    # sum of the distances.\n    #\n    # We then repeat this process for each of the triangle's vertices.\n    #\n    # The resulting weights are then used to interpolate values across the\n    # triangle.\n    #\n    # The weights are computed by considering the vertices of a triangle as\n    # points in the 3D space. We then generate a grid of points in the\n    # triangle, and compute the distance from each point to the triangle's\n    # vertices. We then normalize the weights by dividing by the sum of the\n    # distances.\n    #\n    # We generate the grid of points by first generating a regular grid of\n    # points in the xy-plane, and then adding a small offset to the z-coordinate\n    # of each point. The offset is chosen such that the points are evenly\n    # distributed in the xy-plane, but are offset in the z-direction to ensure\n    # that the points are inside the triangle."}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the value corresponding to each query point.\n  idx0, idx1 = sorted_lookup(tq, t)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Interpolate between the two values.\n  td = tq - t[Ellipsis, idx0]\n  return v0 + td * (v1 - v0)"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the value corresponding to each query point.\n  idx0, idx1 = sorted_lookup(tq, t)\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Interpolate the value at each query point.\n  return v0 + (v1 - v0) * (tq - t0) / (t1 - t0)"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_stepfun(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the value corresponding to each input query.\n  idx0, idx1 = sorted_lookup(tq, t)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Interpolate between the two values.\n  td = tq - t[Ellipsis, idx0]\n  vq = v0 + td * (v1 - v0)\n  return vq"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the spline values corresponding to each input query.\n  idx0, idx1 = sorted_lookup(tq, t)\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Interpolate the spline values.\n  vq = v0 + (v1 - v0) * (tq - t0) / (t1 - t0)\n  return vq"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the spline values corresponding to each input query.\n  idx0, idx1 = sorted_lookup(tq, t)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Interpolate between the two adjacent spline values.\n  td = tq - t[Ellipsis, idx0]\n  vq = v0 + td * (v1 - v0)\n  return vq"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the value corresponding to each query point.\n  idx0, idx1 = sorted_lookup(tq, t)\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Linearly interpolate between the values at the query points.\n  return v0 + (v1 - v0) * (tq - t0) / (t1 - t0)"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the values corresponding to each input query.\n  idx0, idx1 = sorted_lookup(tq, t)\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Interpolate between the values at the two adjacent knots.\n  vq = (tq - t0) / (t1 - t0) * (v1 - v0) + v0\n  return vq"}
{"namespace": "linspline.query", "completion": "  # Check that the spline is valid.\n  check_zero_endpoints(v)\n\n  # Clip the query points to the range of the spline.\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the spline value corresponding to each query point.\n  idx0, idx1 = sorted_lookup(tq, t)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n  td = tq - t[Ellipsis, idx0]\n  return v0 + td * (v1 - v0)"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the value corresponding to each input query.\n  idx0, idx1 = sorted_lookup(tq, t)\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Interpolate the value at each query point.\n  vq = v0 + (v1 - v0) * (tq - t0) / (t1 - t0)\n  return vq\n\n"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the value corresponding to each query point.\n  idx0, idx1 = sorted_lookup(tq, t)\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Interpolate between the values at the two adjacent time points.\n  td = tq - t0\n  vq = v0 + td * (v1 - v0) / (t1 - t0)\n  return vq"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the spline value corresponding to each input query.\n  idx0, idx1 = sorted_lookup(tq, t)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Interpolate the spline value at each query point.\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  td = tq - t0\n  v = v0 + (v1 - v0) * td / (t1 - t0)\n\n  # Extrapolate the spline value outside of the original range.\n  v = jnp.where(tq < t[Ellipsis, :1], 0, jnp.where(tq > math.plus_eps(t[Ellipsis, -1:]), 0, v))\n  return v"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the value corresponding to each input query.\n  idx0, idx1 = sorted_lookup(tq, t)\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Interpolate the values.\n  tq_lo = tq - t0\n  tq_hi = t1 - tq\n  vq = v0 * tq_hi + v1 * tq_lo\n  return vq"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the values corresponding to each input query.\n  idx0, idx1 = sorted_lookup(tq, t)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Compute the slope between the two values.\n  dt = t[Ellipsis, idx1] - t[Ellipsis, idx0]\n  dv = v1 - v0\n  dv = jnp.where(dt == 0, 0, dv / dt)\n\n  # Interpolate between the two values.\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  vq = v0 + dv * (tq - t0)\n  return vq"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the values corresponding to each input query.\n  idx0, idx1 = sorted_lookup(tq, t)\n  y0 = jnp.take_along_axis(v, idx0, axis=-1)\n  y1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Compute the interpolated values at each query point.\n  dt = t[Ellipsis, idx1] - t[Ellipsis, idx0]\n  dy = y1 - y0\n  return y0 + (tq - t[Ellipsis, idx0]) * dy / dt"}
{"namespace": "linspline.query", "completion": "  # Ensure the spline is valid.\n  utils.assert_valid_linspline(t, v)\n\n  # Clip the query points to the valid range.\n  tq = jnp.clip(tq, t[Ellipsis, :1], t[Ellipsis, -1:])\n\n  # Compute the linear interpolant for each query point.\n  idx0, idx1 = sorted_lookup(tq, t)\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n  return v0 + (v1 - v0) * (tq - t0) / (t1 - t0)"}
{"namespace": "linspline.query", "completion": "  # Ensure the spline is valid.\n  utils.assert_valid_linspline(t, v)\n\n  # Clamp the query points to the range of the spline.\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the spline values corresponding to each query point.\n  idx0, idx1 = sorted_lookup(tq, t)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Interpolate the spline values.\n  dt = t[Ellipsis, idx1] - t[Ellipsis, idx0]\n  vq = (v1 - v0) / jnp.maximum(dt, jnp.finfo(jnp.float32).eps) * (tq - t[Ellipsis, idx0]) + v0\n  return vq"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the values corresponding to each input query.\n  idx0, idx1 = sorted_lookup(tq, t)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Interpolate linearly between the values.\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  tq = jnp.broadcast_to(tq, t0.shape)\n  return jnp.where(t1 == t0, v0, v0 + (tq - t0) * (v1 - v0) / (t1 - t0))"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Compute the slope between each pair of adjacent points.\n  v0 = jnp.concatenate([v, v[Ellipsis, :1]], axis=-1)\n  v1 = jnp.concatenate([v[Ellipsis, 1:], v[Ellipsis, :1]], axis=-1)\n  dt = jnp.diff(t)\n  dv = (v1 - v0) / jnp.maximum(dt, jnp.finfo(jnp.float32).eps)\n\n  # Lookup the slope corresponding to each input query.\n  idx0, _ = sorted_lookup(tq, t)\n  dv0 = jnp.take_along_axis(dv, idx0, axis=-1)\n\n  # Interpolate the values at the query points.\n  vq = v0 + dv0 * (tq - t[Ellipsis, idx0])\n  return vq"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Compute the slope of the linear spline at each knot.\n  dt = jnp.diff(t)\n  dv = jnp.diff(v)\n  dv0 = jnp.concatenate([jnp.zeros_like(v[Ellipsis, :1]), dv], axis=-1)\n  m = dv0 / jnp.maximum(jnp.finfo(jnp.float32).eps, dt)\n\n  # Compute the offset of the linear spline at each knot.\n  v0 = jnp.concatenate([jnp.zeros_like(v[Ellipsis, :1]), v], axis=-1)\n  b = v0 - m * t\n\n  # Interpolate the query points.\n  return m * tq + b"}
{"namespace": "linspline.query", "completion": "  # Check the spline is valid.\n  utils.assert_valid_linspline(t, v)\n\n  # Clip the query points to the range of the spline.\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the values corresponding to each query point.\n  idx0, idx1 = sorted_lookup(tq, t)\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Linearly interpolate between the two values.\n  dt = t1 - t0\n  v = v0 + (v1 - v0) * (tq - t0) / dt\n\n  # Extrapolate outside the range of the spline.\n  v = jnp.where(tq < t0, 0, jnp.where(tq > t1, 0, v))\n\n  return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(value > 0 for value in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(isinstance(value, (int, float)) and value > 0 for value in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if not isinstance(v, (int, float)) or v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(value > 0 for value in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    elif v <= 0:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(isinstance(value, (int, float)) and value > 0 for value in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if not isinstance(v, (int, float)) or v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(x > 0 for x in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(value > 0 for value in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must contain only positive values.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(x > 0 for x in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive. got {v}\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive. got {v}\")\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(x > 0 for x in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    elif v <= 0:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(isinstance(val, (int, float)) and val > 0 for val in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if not isinstance(v, (int, float)) or v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(value > 0 for value in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive. Got {v}\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive. Got {v}\")\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(value > 0 for value in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(value > 0 for value in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must contain only positive values.\")\n    elif v <= 0:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if not isinstance(v, Iterable):\n        if v <= 0:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must be positive. Got {v}.\"\n            )\n    else:\n        if not all(value > 0 for value in v):\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must be positive. Got {v}.\"\n            )\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(isinstance(value, (int, float)) for value in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be a number or an iterable of numbers.\")\n\n        if not all(value > 0 for value in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if not isinstance(v, (int, float)):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be a number or an iterable of numbers.\")\n\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable) and not isinstance(v, str):\n        for element in v:\n            if element <= 0:\n                raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    elif v <= 0:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        for val in v:\n            if val <= 0:\n                raise ValueError(\n                    f\"{cls.__name__}: {field.name} must be positive. got {val}\"\n                )\n    else:\n        if v <= 0:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must be positive. got {v}\"\n            )\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(isinstance(value, (int, float)) and value > 0 for value in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if not isinstance(v, (int, float)) or v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(val > 0 for val in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if any(val <= 0 for val in v):\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must be positive. Got {v}.\"\n            )\n    else:\n        if v <= 0:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must be positive. Got {v}.\"\n            )\n\n    return v\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(isinstance(x, (int, float)) and x > 0 for x in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if not isinstance(v, (int, float)) or v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = xnp.broadcast_to(origins, origins.shape[:-1] + (4,))\n  origins[..., -1] = 1.0\n  directions = xnp.broadcast_to(directions, directions.shape[:-1] + (4,))\n  directions[..., -1] = 0.0\n  origins_ndc = (pixtocam @ origins.T).T\n  directions_ndc = (pixtocam @ directions.T).T\n  origins_ndc[..., -1] -= near\n  directions_ndc[..., -1] -= near\n  return origins_ndc, directions_ndc\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Convert ray origins to NDC.\n  origins = xnp.concatenate([origins, xnp.ones_like(origins[Ellipsis, :1])], -1)\n  origins = xnp.einsum('...ij,...j->...i', pixtocam, origins)\n  origins = origins[..., :3] / origins[..., 2:3]\n\n  # Convert ray directions to NDC.\n  directions = xnp.concatenate(\n      [directions, xnp.ones_like(directions[Ellipsis, :1])], -1\n  )\n  directions = xnp.einsum('...ij,...j->...i', pixtocam, directions)\n  directions = directions[..., :3] / directions[..., 2:3]\n\n  # Adjust ray origins to the near plane.\n  origins = origins * near\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Convert to homogeneous coordinates.\n  origins = xnp.concatenate([origins, xnp.ones_like(origins[..., :1])], axis=-1)\n  directions = xnp.concatenate(\n      [directions, xnp.zeros_like(directions[..., :1])], axis=-1\n  )\n\n  # Apply inverse intrinsic matrix.\n  origins = xnp.einsum('...ij,...j->...i', pixtocam, origins)\n  directions = xnp.einsum('...ij,...j->...i', pixtocam, directions)\n\n  # Apply inverse projection.\n  origins = origins / origins[..., -1:]\n  directions = directions / directions[..., -1:]\n\n  # Apply near plane.\n  origins = origins * near\n  directions = directions * near\n\n  # Convert back to cartesian coordinates.\n  origins = origins[..., :3]\n  directions = directions[..., :3]\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Convert to homogeneous coordinates.\n  rays = xnp.concatenate([origins, directions], axis=-1)\n  rays = xnp.concatenate([rays, xnp.ones_like(rays[Ellipsis, :1])], axis=-1)\n\n  # Transform to NDC.\n  rays = xnp.linalg.inv(pixtocam) @ rays[..., None]\n  rays = rays[..., :3]\n\n  # Adjust to near plane.\n  rays = rays / (rays[Ellipsis, 2:3] + near)\n\n  # Split back out.\n  origins = rays[..., :3]\n  directions = rays[..., 3:]\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  xnp = xnp or np\n  pixtocam = xnp.array(pixtocam)\n  origins = xnp.array(origins)\n  directions = xnp.array(directions)\n  origins = origins / origins[Ellipsis, 2:3]\n  directions = directions / directions[Ellipsis, 2:3]\n  origins = origins * near\n  directions = directions * near\n  origins = origins - origins[Ellipsis, :1]\n  directions = directions - directions[Ellipsis, :1]\n  origins = origins[Ellipsis, :2]\n  directions = directions[Ellipsis, :2]\n  origins = origins @ pixtocam.T\n  directions = directions @ pixtocam.T\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Convert to homogeneous coordinates.\n  origins = xnp.concatenate([origins, xnp.ones_like(origins[Ellipsis, :1])],\n                            axis=-1)\n  directions = xnp.concatenate(\n      [directions, xnp.ones_like(directions[Ellipsis, :1])], axis=-1)\n\n  # Convert to NDC.\n  origins = (pixtocam @ origins.T).T\n  origins = origins / origins[Ellipsis, -1:]\n  directions = (pixtocam @ directions.T).T\n  directions = directions / directions[Ellipsis, -1:]\n\n  # Adjust origins to near plane.\n  origins = origins * near / (near - origins[Ellipsis, -1:])\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  pixtocam = xnp.array(pixtocam)\n  # Convert to homogeneous coordinates.\n  rays = xnp.concatenate([origins, directions], axis=-1)\n  # Apply the perspective projection.\n  rays = pixtocam @ rays[..., :3] * near\n  # Convert back to cartesian coordinates.\n  rays = rays[..., :3] / rays[..., 3, None]\n  # Normalize the rays.\n  rays = rays / xnp.linalg.norm(rays, axis=-1, keepdims=True)\n  return rays\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Convert to homogeneous coordinates.\n  origins = xnp.concatenate([origins, xnp.ones_like(origins[Ellipsis, :1])],\n                            axis=-1)\n  directions = xnp.concatenate(\n      [directions, xnp.ones_like(directions[Ellipsis, :1])], axis=-1)\n\n  # Convert to NDC.\n  origins_ndc = xnp.linalg.inv(pixtocam) @ origins\n  directions_ndc = xnp.linalg.inv(pixtocam) @ directions\n\n  # Adjust to near plane.\n  t = near / (near - origins_ndc[Ellipsis, -1])\n  origins_ndc = origins_ndc + t * (origins_ndc - directions_ndc)\n\n  # Convert back to world space.\n  origins = pixtocam @ origins_ndc\n  directions = pixtocam @ directions_ndc\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Convert from world space to camera space.\n  # Note: This is the inverse of the extrinsic matrix.\n  origins = xnp.einsum('...ij,...j->...i', pixtocam, origins)\n  directions = xnp.einsum('...ij,...j->...i', pixtocam, directions)\n\n  # Convert from camera space to NDC space.\n  origins /= origins[Ellipsis, 2, None]\n  directions /= directions[Ellipsis, 2, None]\n\n  # Adjust the near plane to be at the origin.\n  origins[Ellipsis, 2] += near\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Convert to homogeneous coordinates.\n  origins = xnp.concatenate([origins, xnp.ones_like(origins[Ellipsis, :1])], axis=-1)\n  directions = xnp.concatenate(\n      [directions, xnp.ones_like(directions[Ellipsis, :1])], axis=-1\n  )\n\n  # Apply perspective projection.\n  origins = xnp.einsum('...ij,...j->...i', pixtocam, origins)\n  directions = xnp.einsum('...ij,...j->...i', pixtocam, directions)\n\n  # Normalize by the depth.\n  origins = origins / (origins[Ellipsis, -1:])\n  directions = directions / (directions[Ellipsis, -1:])\n\n  # Adjust the near plane.\n  origins = origins * near\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  xnp.seterr(divide='ignore', invalid='ignore')\n  origins = xnp.array(origins)\n  directions = xnp.array(directions)\n  pixtocam = xnp.array(pixtocam)\n\n  # Convert the origins to NDC.\n  origins_ndc = (origins - pixtocam[:3, 2]) / origins[:, 2:3]\n  origins_ndc = origins_ndc * near\n\n  # Calculate the directions in NDC.\n  directions_ndc = directions / origins[:, 2:3]\n  directions_ndc = directions_ndc * near\n\n  return origins_ndc, directions_ndc\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Convert to homogeneous coordinates.\n  rays = xnp.concatenate([origins, directions], axis=-1)\n  rays = xnp.concatenate([rays, xnp.ones_like(rays[Ellipsis, :1])], axis=-1)\n  # Convert to NDC.\n  rays = xnp.linalg.inv(pixtocam) @ rays\n  rays = rays / rays[Ellipsis, 2:3]\n  # Convert to camera space.\n  rays = rays * near\n  # Convert to world space.\n  rays = rays[Ellipsis, :3]\n  return rays\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # TODO(bmild): remove xnp argument and always use numpy.\n\n  # Convert from world space to camera space.\n  origins = origins - pixtocam[Ellipsis, 2, 3]\n  directions = directions @ pixtocam[Ellipsis, :3, :3].T\n\n  # Project to NDC.\n  directions = directions * (near / directions[Ellipsis, 2])[Ellipsis, None]\n  origins = origins * (near / directions[Ellipsis, 2])[Ellipsis, None]\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Convert from world space to camera space.\n  camera_dirs = xnp.einsum('...ij,...j->...i', pixtocam, directions)\n\n  # Apply perspective projection.\n  z = xnp.einsum('...i,...i->...', camera_dirs, directions)\n  z = near * z\n  origins = origins + (z / xnp.einsum('...i,...i->...', camera_dirs, directions)) * directions\n\n  # Convert from camera space to NDC.\n  origins = xnp.einsum('...ij,...j->...i', pixtocam, origins)\n  directions = camera_dirs / z\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Add half pixel offset to shoot rays through pixel centers.\n  pix_x_int, pix_y_int = origins[Ellipsis, 0], origins[Ellipsis, 1]\n  pix_x_int = pix_x_int + 0.5\n  pix_y_int = pix_y_int + 0.5\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs = xnp.stack([pix_x_int, pix_y_int, xnp.ones_like(pix_x_int)], -1)\n  camera_dirs = xnp.matmul(pixtocam, camera_dirs[Ellipsis, :, None])[Ellipsis, :, 0]\n\n  # Apply camera rotation matrices.\n  directions = xnp.matmul(camtoworld[:3, :3], camera_dirs[Ellipsis, :, None])[\n      Ellipsis, :, 0\n  ]\n  origins = xnp.broadcast_to(camtoworld[:3, -1], directions.shape)\n\n  # Distance from each unit-norm direction vector to its neighbors.\n  dx_norm = xnp.linalg.norm(directions[Ellipsis, :, :2] - directions, axis=-1)\n  dy_norm = xnp.linalg.norm(directions[Ellipsis, :, 1:] - directions, axis=-1)\n\n  # Cut the distance in half, multiply it to match the variance of a uniform\n  # distribution the size of a pixel (1/12, see paper).\n  # TODO(barron): Add a unit test that this is correct.\n  radii = (0.5 * (dx_norm + dy_norm))[Ellipsis, None] * 2 / xnp.sqrt(12)\n\n  # Clip the near plane to the specified distance.\n  origins = origins + near * directions\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdim"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Convert to homogeneous coordinates.\n  origins = xnp.concatenate([origins, xnp.ones_like(origins[Ellipsis, :1])], -1)\n  directions = xnp.concatenate(\n      [directions, xnp.ones_like(directions[Ellipsis, :1])], -1\n  )\n\n  # Project to NDC space.\n  origins_ndc = xnp.einsum('...ij,...j->...i', pixtocam, origins)\n  directions_ndc = xnp.einsum('...ij,...j->...i', pixtocam, directions)\n\n  # Adjust for near plane.\n  origins_ndc = origins_ndc * near / (near - origins_ndc[Ellipsis, 2:3])\n\n  return origins_ndc, directions_ndc\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # TODO(bmild): consider removing the xnp argument.\n\n  # Convert from world space to camera space.\n  origins = origins - pixtocam[:3, 3]\n  origins = origins / (pixtocam[:3, :3] * origins)\n\n  # Convert from camera space to NDC space.\n  origins = origins / origins[Ellipsis, 2, None]\n  directions = directions / directions[Ellipsis, 2, None]\n\n  # Adjust origins to the near plane.\n  origins = origins * near\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Convert from world space to camera space.\n  origins = xnp.matmul(xnp.linalg.inv(camtoworld), origins.T).T\n  directions = xnp.matmul(xnp.linalg.inv(camtoworld), directions.T).T\n\n  # Convert from camera space to NDC space.\n  origins = xnp.concatenate([origins, xnp.ones_like(origins[Ellipsis, :1])], axis=-1)\n  origins = xnp.matmul(pixtocam, origins.T).T\n  directions = xnp.matmul(pixtocam, directions.T).T\n\n  # Adjust ray origins to the near plane.\n  origins[Ellipsis, 2] += near\n\n  # Convert from NDC space to image plane.\n  origins = origins / origins[Ellipsis, 2:3]\n  directions = directions / directions[Ellipsis, 2:3]\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Convert to homogeneous coordinates.\n  rays = np.concatenate([origins, directions], -1)\n  rays = xnp.asarray(rays)\n  rays = xnp.concatenate([rays, xnp.ones_like(rays[Ellipsis, :1])], -1)\n  # Apply perspective projection.\n  rays = xnp.linalg.inv(pixtocam) @ rays[Ellipsis, :, None]\n  rays = xnp.squeeze(rays, -1)\n  # Convert to cartesian coordinates.\n  rays = rays[Ellipsis, :3] / rays[Ellipsis, 3:]\n  # Apply near plane.\n  rays = rays * (near - origins[Ellipsis, 2:3]) / directions[Ellipsis, 2:3]\n  return rays\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = xnp.broadcast_to(origins, pixtocam.shape[:-2] + (3,))\n  directions = xnp.broadcast_to(directions, pixtocam.shape[:-2] + (3,))\n  # Convert from world space to camera space.\n  origins = xnp.linalg.inv(camtoworlds) @ origins\n  directions = xnp.linalg.inv(camtoworlds) @ directions\n  # Convert from camera space to NDC.\n  origins = (pixtocam @ origins.T).T\n  directions = (pixtocam @ directions.T).T\n  # Move to near plane.\n  origins = origins * near / (-origins[:, -1, None])\n  # Convert from NDC to pixel coordinates.\n  origins = origins[Ellipsis, :2]\n  directions = directions[Ellipsis, :2] - origins\n  return origins, directions\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.abs(jnp.dot(dir1, dir2)) > 1 - 1e-6"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.abs(jnp.dot(dir1, dir2)) > 1 - 1e-6"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.allclose(jnp.dot(dir1, dir2), 1.0)"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.abs(jnp.dot(dir1, dir2)) >= 1 - 1e-6"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  dot_product = jnp.sum(dir1 * dir2, axis=-1)\n  return jnp.abs(dot_product) > 1 - 1e-6"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.abs(jnp.dot(dir1, dir2)) > 1.0 - 1e-3"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1_norm = spin_math.normalize(dir1)\n  dir2_norm = spin_math.normalize(dir2)\n  return jnp.abs(jnp.dot(dir1_norm, dir2_norm)) > 1 - 1e-5"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.abs(jnp.dot(dir1, dir2)) >= 1.0 - 1e-6\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.abs(jnp.dot(dir1, dir2)) > 1.0 - 1e-6\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.abs(jnp.dot(dir1, dir2)) >= 1 - chex.epsilon()"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.allclose(jnp.dot(dir1, dir2), 1.0)\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.abs(jnp.dot(dir1, dir2)) > 1.0 - 1e-6\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.abs(jnp.dot(dir1, dir2)) > 1 - 1e-5\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.abs(jnp.dot(dir1, dir2)) > 1 - 1e-6\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1_norm = spin_math.normalize(dir1)\n  dir2_norm = spin_math.normalize(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = jnp.sum(dir1_norm * dir2_norm, axis=-1)\n\n  # Compare the dot product with a small epsilon to account for numerical\n  # precision issues\n  is_parallel = jnp.abs(dot_product) > 1 - 1e-6\n\n  return is_parallel"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.allclose(jnp.dot(dir1, dir2), 1.0, atol=1e-5)\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.abs(jnp.dot(dir1, dir2)) > 1 - 1e-8\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.abs(jnp.dot(dir1, dir2)) > 1 - 1e-5\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.abs(jnp.dot(dir1, dir2)) > 1 - 1e-6\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors to prevent numerical issues.\n  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  # Compute the dot product of the normalized direction vectors.\n  dot_product = jnp.sum(dir1 * dir2, axis=-1)\n  # Check if the dot product is within a small epsilon of 1.0.\n  return jnp.abs(dot_product - 1.0) < 1e-6\n\n"}
{"namespace": "common.bleu4_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f, smooth=with_penalty)\n    score = results['bleu']\n    return score"}
{"namespace": "common.bleu4_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f, smooth=with_penalty)\n    score = results['bleu']\n    return score"}
{"namespace": "common.bleu4_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f, smooth=with_penalty)\n    score = results['bleu']\n    return score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f, smooth=with_penalty)\n    score = results['bleu']\n    return score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f, max_order=4, smooth=with_penalty)\n    score = results['bleu']\n    return score"}
{"namespace": "common.bleu4_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f, max_order=4, smooth=with_penalty)\n    score = results['bleu']\n    return score"}
{"namespace": "common.bleu4_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['bleu']\n    if with_penalty:\n        score *= min(1, len(continuation) / len(reference))\n    return score"}
{"namespace": "common.bleu4_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f, smooth=with_penalty)\n    score = results['bleu']\n    return score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['bleu']\n    if with_penalty:\n        penalty = min(1, len(continuation) / len(reference))\n        score *= penalty\n    return score"}
{"namespace": "common.bleu4_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f, smooth=with_penalty)\n    score = results['bleu']\n    return score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    f = lambda text: list(jieba.cut(text))\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f, smooth=with_penalty)\n    score = results['bleu']\n    return score"}
{"namespace": "common.bleu4_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f, max_order=4)\n    score = results['bleu']\n    if with_penalty:\n        return score\n    else:\n        return score / 100"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts using a custom tokenizer function\n    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f, smooth=with_penalty)\n    score = results['bleu']\n    return score"}
{"namespace": "common.bleu4_score", "completion": "    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    f = lambda text: list(jieba.cut(text))\n    result = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f, max_order=4, smooth=with_penalty)\n    score = result['bleu']\n    return score"}
{"namespace": "common.bleu4_score", "completion": "    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    f = lambda text: list(jieba.cut(text))\n    score = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f, smooth=with_penalty)\n    return score"}
{"namespace": "common.bleu4_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    result = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = result['bleu']\n    if with_penalty:\n        score *= min(1, len(continuation) / len(reference))\n    return score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts using a custom tokenizer function\n    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f, smooth=with_penalty)\n    score = results['bleu']\n    return score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    result = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f, smooth=with_penalty)\n    return result['bleu']\n\n"}
{"namespace": "common.bleu4_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['bleu']\n    if with_penalty:\n        penalty = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f, smooth=True)['bleu_penalty']\n        score *= penalty\n    return score"}
{"namespace": "common.bleu4_score", "completion": "    # Define the tokenizer function\n    tokenizer = lambda text: list(jieba.cut(text))\n\n    # Compute the BLEU-4 score\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    result = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=tokenizer, max_order=4)\n    score = result['bleu']\n\n    # Adjust the score for brevity penalty, if requested\n    if with_penalty:\n        score *= min(1, len(continuation) / len(reference))\n\n    return score"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x > eps, jnp.sqrt(x), value_at_zero)\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.sqrt(jnp.where(x > eps, x, value_at_zero))\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x > eps, jnp.sqrt(x), value_at_zero)\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x > eps, jnp.sqrt(x), value_at_zero)\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x > eps, jnp.sqrt(x), value_at_zero)\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.sqrt(jnp.maximum(x, eps))\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x > eps, jnp.sqrt(x), value_at_zero)\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.sqrt(jnp.where(x > eps, x, value_at_zero))\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x > eps, jnp.sqrt(x), value_at_zero)\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x > eps, jnp.sqrt(x), value_at_zero)\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.sqrt(jnp.maximum(x, jnp.full_like(x, eps)))\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x > eps, jnp.sqrt(x), value_at_zero)\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x > eps, jnp.sqrt(x), value_at_zero)\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.sqrt(jnp.where(x > eps, x, value_at_zero))\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.sqrt(jnp.maximum(x, eps))\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x > eps, jnp.sqrt(x), jnp.full_like(x, value_at_zero))\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.sqrt(jnp.where(x > eps, x, value_at_zero))\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x > eps, jnp.sqrt(x), value_at_zero)\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.sqrt(safe_x)\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, value_at_zero, jnp.sqrt(x))\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Compute the PDF by dividing the weights by the difference between consecutive elements in the input vector t.\n  pdf = w / jnp.diff(t)\n  return pdf"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Compute the PDF by dividing the weights by the difference between consecutive elements in the input vector t.\n  pdf = w / jnp.diff(t)\n  return pdf"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Compute the PDF by dividing the weights by the difference between consecutive elements in the input vector t.\n  pdf = w / jnp.diff(t)\n  return pdf"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  w_pdf = w / jnp.diff(t)\n  return w_pdf"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Compute the PDF by dividing the weights by the difference between consecutive elements in the input vector t.\n  pdf = w / jnp.diff(t)\n  return pdf\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Compute the PDF by dividing the weights by the difference between consecutive elements in the input vector t.\n  pdf = w / jnp.diff(t)\n  return pdf\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  pdf = w / jnp.diff(t)\n  return pdf"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  return w / jnp.diff(t)"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n\n  # Compute the PDF by dividing the weights by the difference between consecutive elements in the input vector t.\n  pdf = w / jnp.diff(t)\n\n  return pdf"}
{"namespace": "stepfun.weight_to_pdf", "completion": "def pdf_to_weight(t, p):\n  \"\"\"Turn a PDF that integrates to 1 into a vector of weights that sums to 1.\"\"\"\n  utils.assert_valid_stepfun(t, p)\n  return p * jnp.diff(t)\n\n\ndef integrate_weights(w):\n  \"\"\"Compute the cumulative sum of w, assuming all weight vectors sum to 1.\n\n  The output's size on the last dimension is one greater than that of the input,\n  because we're computing the integral corresponding to the endpoints of a step\n  function, not the integral of the interior/bin values.\n\n  Args:\n    w: Tensor, which will be integrated along the last axis. This is assumed to\n      sum to 1 along the last axis, and this function will (silently) break if\n      that is not the case.\n\n  Returns:\n    cw0: Tensor, the integral of w, where cw0[..., 0] = 0 and cw0[..., -1] = 1\n  \"\"\"\n  cw = jnp.minimum(1, jnp.cumsum(w[Ellipsis, :-1], axis=-1))\n  shape = cw.shape[:-1] + (1,)\n  # Ensure that the CDF starts with exactly 0 and ends with exactly 1.\n  cw0 = jnp.concatenate([jnp.zeros(shape), cw, jnp.ones(shape)], axis=-1)\n  return cw0\n\n\ndef invert_cdf(u, t, w_logits):\n  \"\"\"Invert the CDF defined by (t, w) at the points specified by u in [0, 1).\"\"\"\n  utils.assert_valid_stepfun(t, w_logits)\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n  # Interpolate into the inverse CDF.\n  t_new = math."}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n\n  # Compute the PDF by dividing the weights by the difference between consecutive elements in the input vector t.\n  pdf = w / jnp.diff(t)\n\n  # Return the resulting PDF.\n  return pdf"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n\n  # Compute the PDF by dividing the weights by the difference between consecutive elements in the input vector t.\n  pdf = w / jnp.diff(t)\n\n  # Return the resulting PDF that integrates to 1.\n  return pdf"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Convert the weights to a probability density function (PDF) by dividing them by the difference between consecutive elements in the input vector t.\n  pdf = w / jnp.diff(t)\n  return pdf"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Compute the PDF by dividing the weights by the difference between consecutive elements in the input vector t.\n  pdf = w / jnp.diff(t)\n\n  # Return the resulting PDF that integrates to 1.\n  return pdf"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Convert the weights into a probability density function (PDF) by dividing them by the difference between consecutive elements in the input vector t.\n  pdf = w / jnp.diff(t)\n\n  # Return the resulting PDF.\n  return pdf"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Compute the difference between consecutive elements in the input vector t.\n  t_diff = jnp.diff(t)\n  # Divide the weights by the difference between consecutive elements in the input vector t.\n  p = w / t_diff\n  # Return the resulting PDF.\n  return p"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n\n  # Compute the difference between consecutive elements in the input vector t.\n  t_diff = jnp.diff(t)\n\n  # Divide the weights by the difference between consecutive elements in the input vector t.\n  pdf = w / t_diff\n\n  return pdf"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Normalize the weights to sum to 1.\n  w_norm = w / jnp.sum(w, axis=-1, keepdims=True)\n  # Compute the PDF by dividing the normalized weights by the difference between consecutive elements in the input vector t.\n  pdf = w_norm / jnp.diff(t)\n  return pdf"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Ensure that the input vector t is sorted in ascending order.\n  t = jnp.sort(t)\n\n  # Compute the PDF by dividing the input vector w by the difference between consecutive elements in the input vector t.\n  pdf = w / jnp.diff(t)\n\n  # Return the resulting PDF.\n  return pdf"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Ensure that the input t and w are 1D arrays and have the same shape.\n  t = np.array(t)\n  w = np.array(w)\n  if t.ndim != 1 or w.ndim != 1 or t.shape != w.shape:\n    raise ValueError(\n        f't and w must be 1D arrays with the same shape, but got shapes {t.shape} and {w.shape}.'\n    )\n\n  # Compute the difference between consecutive elements in t.\n  dt = np.diff(t)\n\n  # Divide each element of w by the corresponding element of dt.\n  p = w / dt\n\n  # Return the resulting PDF as a 1D array.\n  return p"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    with contextlib.suppress(FileNotFoundError):\n        for dirpath, _, filenames in os.walk(path):\n            for filename in filenames:\n                file_path = os.path.join(dirpath, filename)\n                try:\n                    total_size += os.path.getsize(file_path)\n                except FileNotFoundError:\n                    pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    with contextlib.suppress(FileNotFoundError):\n        for dirpath, _, filenames in os.walk(path):\n            for filename in filenames:\n                file_path = os.path.join(dirpath, filename)\n                try:\n                    total_size += os.path.getsize(file_path)\n                except FileNotFoundError:\n                    pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    with contextlib.suppress(FileNotFoundError):\n        for dirpath, _, filenames in os.walk(path):\n            for filename in filenames:\n                file_path = os.path.join(dirpath, filename)\n                try:\n                    total_size += os.path.getsize(file_path)\n                except FileNotFoundError:\n                    pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    try:\n        for dirpath, _, filenames in os.walk(path):\n            for f in filenames:\n                fp = os.path.join(dirpath, f)\n                try:\n                    total_size += os.path.getsize(fp)\n                except FileNotFoundError:\n                    pass\n    except FileNotFoundError:\n        pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    with contextlib.suppress(FileNotFoundError):\n        for dirpath, _, filenames in os.walk(path):\n            for filename in filenames:\n                file_path = os.path.join(dirpath, filename)\n                try:\n                    total_size += os.path.getsize(file_path)\n                except FileNotFoundError:\n                    pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    with contextlib.suppress(FileNotFoundError):\n        for dirpath, dirnames, filenames in os.walk(path):\n            for filename in filenames:\n                file_path = os.path.join(dirpath, filename)\n                try:\n                    total_size += os.path.getsize(file_path)\n                except FileNotFoundError:\n                    pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    try:\n        for dirpath, dirnames, filenames in os.walk(path):\n            for f in filenames:\n                fp = os.path.join(dirpath, f)\n                try:\n                    total_size += os.path.getsize(fp)\n                except FileNotFoundError:\n                    pass\n    except FileNotFoundError:\n        pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    try:\n        for dirpath, dirnames, filenames in os.walk(path):\n            for filename in filenames:\n                file_path = os.path.join(dirpath, filename)\n                try:\n                    total_size += os.path.getsize(file_path)\n                except FileNotFoundError:\n                    pass\n    except FileNotFoundError:\n        pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    with contextlib.suppress(FileNotFoundError):\n        for dirpath, dirnames, filenames in os.walk(path):\n            for filename in filenames:\n                file_path = os.path.join(dirpath, filename)\n                total_size += os.path.getsize(file_path)\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    with contextlib.suppress(FileNotFoundError):\n        for dirpath, dirnames, filenames in os.walk(path):\n            for f in filenames:\n                fp = os.path.join(dirpath, f)\n                try:\n                    total_size += os.path.getsize(fp)\n                except FileNotFoundError:\n                    pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    size = 0\n    with contextlib.suppress(FileNotFoundError):\n        for root, dirs, files in os.walk(path):\n            for file in files:\n                file_path = os.path.join(root, file)\n                size += os.path.getsize(file_path)\n    return size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    size = 0\n    with contextlib.suppress(FileNotFoundError):\n        for root, _, files in os.walk(path):\n            for file in files:\n                file_path = os.path.join(root, file)\n                size += os.path.getsize(file_path)\n    return size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    try:\n        for dirpath, dirnames, filenames in os.walk(path):\n            for f in filenames:\n                fp = os.path.join(dirpath, f)\n                try:\n                    total_size += os.path.getsize(fp)\n                except FileNotFoundError:\n                    pass\n    except FileNotFoundError:\n        pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    with contextlib.suppress(FileNotFoundError):\n        for dirpath, _, filenames in os.walk(path):\n            for f in filenames:\n                fp = os.path.join(dirpath, f)\n                # skip if it is symbolic link\n                if not os.path.islink(fp):\n                    total_size += os.path.getsize(fp)\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    size = 0\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            try:\n                size += os.path.getsize(file_path)\n            except FileNotFoundError:\n                pass\n    return size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    with contextlib.suppress(FileNotFoundError):\n        for dirpath, dirnames, filenames in os.walk(path):\n            for f in filenames:\n                fp = os.path.join(dirpath, f)\n                try:\n                    total_size += os.path.getsize(fp)\n                except FileNotFoundError:\n                    pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for filename in filenames:\n            file_path = os.path.join(dirpath, filename)\n            try:\n                total_size += os.path.getsize(file_path)\n            except FileNotFoundError:\n                pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    size = 0\n    for root, _, files in os.walk(path):\n        for file in files:\n            try:\n                size += os.path.getsize(os.path.join(root, file))\n            except FileNotFoundError:\n                pass\n    return size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    size = 0\n    try:\n        for root, _, files in os.walk(path):\n            for f in files:\n                fp = os.path.join(root, f)\n                size += os.path.getsize(fp)\n    except FileNotFoundError:\n        pass\n    return size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    size = 0\n    try:\n        for root, _, files in os.walk(path):\n            for file in files:\n                file_path = os.path.join(root, file)\n                size += os.path.getsize(file_path)\n    except FileNotFoundError as e:\n        logger.debug(e)\n    return size"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val = val - offset * period\n    val = torch.fmod(val, period)\n    val = val + offset * period\n    return val"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    period = torch.as_tensor(period)\n    offset = torch.as_tensor(offset)\n    val = torch.as_tensor(val)\n    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val = val - offset * period\n    val = val - period * torch.floor(val / period + 0.5)\n    val = val + offset * period\n    return val"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val_size = val.shape\n    val = val.reshape(-1)\n    val = val - offset * period\n    val = (val + period) % period\n    val = val - offset * period\n    val = val.reshape(val_size)\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    period = torch.as_tensor(period)\n    offset = torch.as_tensor(offset)\n    return val - torch.floor(val / period + offset) * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    if isinstance(val, torch.Tensor):\n        val = val.detach().cpu().numpy()\n\n    val = np.mod(val - offset * period, period)\n    val = np.where(val < -0.5 * period, val + period, val)\n    val = np.where(val > 0.5 * period, val - period, val)\n\n    return val"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val = val - offset * period\n    val = torch.fmod(val, 2 * period)\n    val = torch.where(val > period, val - 2 * period, val)\n    val = torch.where(val < -period, val + 2 * period, val)\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    period_half = period / 2\n    val = val - offset * period\n    val = torch.where(val < -period_half, val + period, val)\n    val = torch.where(val >= period_half, val - period, val)\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val_size = val.shape\n    val = val - offset * period\n    val = val - period * (val // period).type_as(val)\n    val = val + offset * period\n    val = val.reshape(val_size)\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    period_ = torch.as_tensor(period, dtype=val.dtype)\n    offset_ = torch.as_tensor(offset, dtype=val.dtype)\n    return val - torch.floor(val / period_) * period_ + offset_ * period_\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val_size = val.size()\n    val = val.reshape(-1, 1)\n    val_offset = val - offset * period\n    val_mod = val_offset % period\n    val_mod = (val_mod + period) % period\n    val_mod = val_mod.reshape(val_size)\n    return val_mod\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    if isinstance(val, torch.Tensor):\n        offset = torch.full_like(val, offset)\n        period = torch.full_like(val, period)\n    else:\n        offset = np.full_like(val, offset)\n        period = np.full_like(val, period)\n    val = val - offset\n    val = torch.remainder(val, period)\n    val = val + offset\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    period = torch.as_tensor(period)\n    offset = torch.as_tensor(offset)\n    val = torch.as_tensor(val)\n\n    val = val - offset * period\n    val = torch.fmod(val, period)\n    val = val + offset * period\n\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    if isinstance(val, np.ndarray):\n        val = torch.from_numpy(val)\n    val = val - offset * period\n    val = torch.fmod(val, 2 * period)\n    val = torch.where(val < -period, val + 2 * period, val)\n    val = torch.where(val >= period, val - 2 * period, val)\n    val = val + offset * period\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    period_val = val - offset * period\n    period_val = torch.fmod(period_val, period * 2)\n    return period_val + offset * period\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    if isinstance(val, torch.Tensor):\n        val = val.detach().cpu().numpy()\n    val = val - offset * period\n    val = (val + period) % period\n    val = val + offset * period\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    if isinstance(val, np.ndarray):\n        val = torch.from_numpy(val).float()\n    if not torch.is_tensor(val):\n        raise TypeError(f'Invalid type for val: {type(val)}. '\n                        'Must be a numpy array or torch tensor.')\n\n    period_val = val - offset * period\n    period_val = torch.fmod(period_val, period)\n    period_val = torch.fmod(period_val + period, period)\n    period_val = period_val + offset * period\n    return period_val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    period = torch.as_tensor(period)\n    offset = torch.as_tensor(offset)\n    val = torch.as_tensor(val)\n    val = val - offset * period\n    val = val - period * torch.floor(val / period + 0.5)\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    period = torch.as_tensor(period)\n    offset = torch.as_tensor(offset)\n    val = torch.as_tensor(val)\n    if offset != 0.5:\n        warning.warn(\n            'Typically, offset should be set to 0.5 when using periodic '\n            'functions such as sin and cos. Otherwise, the results may be '\n            'incorrect.')\n    val = val - offset * period\n    val = val - period * torch.floor(val / period + 0.5)\n    val = val + offset * period\n    return val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val = val - offset * period\n    val = val - period * torch.floor(val / period + 0.5)\n    val = val + offset * period\n    return val\n\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        data = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding.tolist() if agent.purpose_embedding is not None else None,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n        return data"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if agent.purpose_embedding is not None:\n            data = {\n                \"dynamic_prompt\": agent.dynamic_prompt,\n                \"purpose\": agent.purpose,\n                \"purpose_embedding\": agent.purpose_embedding.tolist(),\n                \"depth\": agent.depth,\n                \"max_depth\": agent.max_depth,\n                \"usage_count\": agent.usage_count,\n                \"id\": agent.id,\n                \"parent_id\": agent.parent_id,\n                \"working_agent\": agent.working_agent,\n                \"is_prime\": agent.is_prime,\n                \"evolve_count\": agent.evolve_count,\n                \"number_of_code_executions\": agent.number_of_code_executions,\n                \"last_input\": agent.last_input\n            }\n        else:\n            data = {\n                \"dynamic_prompt\": agent.dynamic_prompt,\n                \"purpose\": agent.purpose,\n                \"depth\": agent.depth,\n                \"max_depth\": agent.max_depth,\n                \"usage_count\": agent.usage_count,\n                \"id\": agent.id,\n                \"parent_id\": agent.parent_id,\n                \"working_agent\": agent.working_agent,\n                \"is_prime\": agent.is_prime,\n                \"evolve_count\": agent.evolve_count,\n                \"number_of_code_executions\": agent.number_of_code_executions,\n                \"last_input\": agent.last_input\n            }\n        return data\n\n    "}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        data = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding.tolist() if agent.purpose_embedding is not None else None,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n        return data\n\n    "}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        data = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding.tolist() if agent.purpose_embedding is not None else None,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n        return data\n\n    "}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        data = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding.tolist() if agent.purpose_embedding is not None else None,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n        return data\n\n    "}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if agent.purpose_embedding is not None:\n            data = {\n                \"dynamic_prompt\": agent.dynamic_prompt,\n                \"purpose\": agent.purpose,\n                \"purpose_embedding\": agent.purpose_embedding.tolist(),\n                \"depth\": agent.depth,\n                \"max_depth\": agent.max_depth,\n                \"usage_count\": agent.usage_count,\n                \"id\": agent.id,\n                \"parent_id\": agent.parent_id,\n                \"working_agent\": agent.working_agent,\n                \"is_prime\": agent.is_prime,\n                \"evolve_count\": agent.evolve_count,\n                \"number_of_code_executions\": agent.number_of_code_executions,\n                \"last_input\": agent.last_input\n            }\n        else:\n            data = {\n                \"dynamic_prompt\": agent.dynamic_prompt,\n                \"purpose\": agent.purpose,\n                \"depth\": agent.depth,\n                \"max_depth\": agent.max_depth,\n                \"usage_count\": agent.usage_count,\n                \"id\": agent.id,\n                \"parent_id\": agent.parent_id,\n                \"working_agent\": agent.working_agent,\n                \"is_prime\": agent.is_prime,\n                \"evolve_count\": agent.evolve_count,\n                \"number_of_code_executions\": agent.number_of_code_executions,\n                \"last_input\": agent.last_input\n            }\n\n        return data\n\n    "}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if agent.purpose_embedding is not None:\n            data = {\n                \"dynamic_prompt\": agent.dynamic_prompt,\n                \"purpose\": agent.purpose,\n                \"purpose_embedding\": agent.purpose_embedding.tolist(),\n                \"depth\": agent.depth,\n                \"max_depth\": agent.max_depth,\n                \"usage_count\": agent.usage_count,\n                \"id\": agent.id,\n                \"parent_id\": agent.parent_id,\n                \"working_agent\": agent.working_agent,\n                \"is_prime\": agent.is_prime,\n                \"evolve_count\": agent.evolve_count,\n                \"number_of_code_executions\": agent.number_of_code_executions,\n                \"last_input\": agent.last_input\n            }\n        else:\n            data = {\n                \"dynamic_prompt\": agent.dynamic_prompt,\n                \"purpose\": agent.purpose,\n                \"depth\": agent.depth,\n                \"max_depth\": agent.max_depth,\n                \"usage_count\": agent.usage_count,\n                \"id\": agent.id,\n                \"parent_id\": agent.parent_id,\n                \"working_agent\": agent.working_agent,\n                \"is_prime\": agent.is_prime,\n                \"evolve_count\": agent.evolve_count,\n                \"number_of_code_executions\": agent.number_of_code_executions,\n                \"last_input\": agent.last_input\n            }\n\n        return data\n\n    "}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if agent.purpose_embedding is not None:\n            data = {\n                \"dynamic_prompt\": agent.dynamic_prompt,\n                \"purpose\": agent.purpose,\n                \"purpose_embedding\": agent.purpose_embedding.tolist(),\n                \"depth\": agent.depth,\n                \"max_depth\": agent.max_depth,\n                \"usage_count\": agent.usage_count,\n                \"id\": agent.id,\n                \"parent_id\": agent.parent_id,\n                \"working_agent\": agent.working_agent,\n                \"is_prime\": agent.is_prime,\n                \"evolve_count\": agent.evolve_count,\n                \"number_of_code_executions\": agent.number_of_code_executions,\n                \"last_input\": agent.last_input\n            }\n        else:\n            data = {\n                \"dynamic_prompt\": agent.dynamic_prompt,\n                \"purpose\": agent.purpose,\n                \"depth\": agent.depth,\n                \"max_depth\": agent.max_depth,\n                \"usage_count\": agent.usage_count,\n                \"id\": agent.id,\n                \"parent_id\": agent.parent_id,\n                \"working_agent\": agent.working_agent,\n                \"is_prime\": agent.is_prime,\n                \"evolve_count\": agent.evolve_count,\n                \"number_of_code_executions\": agent.number_of_code_executions,\n                \"last_input\": agent.last_input\n            }\n\n        return data\n\n    "}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding.tolist() if agent.purpose_embedding is not None else None,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n        return agent_dict"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if agent.purpose_embedding is not None:\n            data = {\n                \"dynamic_prompt\": agent.dynamic_prompt,\n                \"purpose\": agent.purpose,\n                \"purpose_embedding\": agent.purpose_embedding.tolist(),\n                \"depth\": agent.depth,\n                \"max_depth\": agent.max_depth,\n                \"usage_count\": agent.usage_count,\n                \"id\": agent.id,\n                \"parent_id\": agent.parent_id,\n                \"working_agent\": agent.working_agent,\n                \"is_prime\": agent.is_prime,\n                \"evolve_count\": agent.evolve_count,\n                \"number_of_code_executions\": agent.number_of_code_executions,\n                \"last_input\": agent.last_input\n            }\n        else:\n            data = {\n                \"dynamic_prompt\": agent.dynamic_prompt,\n                \"purpose\": agent.purpose,\n                \"depth\": agent.depth,\n                \"max_depth\": agent.max_depth,\n                \"usage_count\": agent.usage_count,\n                \"id\": agent.id,\n                \"parent_id\": agent.parent_id,\n                \"working_agent\": agent.working_agent,\n                \"is_prime\": agent.is_prime,\n                \"evolve_count\": agent.evolve_count,\n                \"number_of_code_executions\": agent.number_of_code_executions,\n                \"last_input\": agent.last_input\n            }\n\n        return data"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if agent.purpose_embedding is not None:\n            data = {\n                \"dynamic_prompt\": agent.dynamic_prompt,\n                \"purpose\": agent.purpose,\n                \"purpose_embedding\": agent.purpose_embedding.tolist(),\n                \"depth\": agent.depth,\n                \"max_depth\": agent.max_depth,\n                \"usage_count\": agent.usage_count,\n                \"id\": agent.id,\n                \"parent_id\": agent.parent_id,\n                \"working_agent\": agent.working_agent,\n                \"is_prime\": agent.is_prime,\n                \"evolve_count\": agent.evolve_count,\n                \"number_of_code_executions\": agent.number_of_code_executions,\n                \"last_input\": agent.last_input\n            }\n        else:\n            data = {\n                \"dynamic_prompt\": agent.dynamic_prompt,\n                \"purpose\": agent.purpose,\n                \"depth\": agent.depth,\n                \"max_depth\": agent.max_depth,\n                \"usage_count\": agent.usage_count,\n                \"id\": agent.id,\n                \"parent_id\": agent.parent_id,\n                \"working_agent\": agent.working_agent,\n                \"is_prime\": agent.is_prime,\n                \"evolve_count\": agent.evolve_count,\n                \"number_of_code_executions\": agent.number_of_code_executions,\n                \"last_input\": agent.last_input\n            }\n\n        return data"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding.tolist() if agent.purpose_embedding is not None else None,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n        return agent_dict\n\n    "}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding.tolist() if agent.purpose_embedding is not None else None,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n        return agent_dict\n\n    "}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        data = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n\n        if agent.purpose_embedding is not None:\n            data[\"purpose_embedding\"] = agent.purpose_embedding.tolist()\n        else:\n            data[\"purpose_embedding\"] = None\n\n        return data"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = agent.__dict__\n        if agent_dict[\"purpose_embedding\"] is not None:\n            agent_dict[\"purpose_embedding\"] = agent_dict[\"purpose_embedding\"].tolist()\n        return agent_dict\n\n    "}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding.tolist() if agent.purpose_embedding is not None else None,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n        }\n        return agent_dict"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n\n        if agent.purpose_embedding is not None:\n            agent_dict[\"purpose_embedding\"] = agent.purpose_embedding.tolist()\n        else:\n            agent_dict[\"purpose_embedding\"] = None\n\n        return agent_dict\n\n    "}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if agent.purpose_embedding is not None:\n            data = {\n                \"dynamic_prompt\": agent.dynamic_prompt,\n                \"purpose\": agent.purpose,\n                \"purpose_embedding\": agent.purpose_embedding.tolist(),\n                \"depth\": agent.depth,\n                \"max_depth\": agent.max_depth,\n                \"usage_count\": agent.usage_count,\n                \"id\": agent.id,\n                \"parent_id\": agent.parent_id,\n                \"working_agent\": agent.working_agent,\n                \"is_prime\": agent.is_prime,\n                \"evolve_count\": agent.evolve_count,\n                \"number_of_code_executions\": agent.number_of_code_executions,\n                \"last_input\": agent.last_input\n            }\n        else:\n            data = {\n                \"dynamic_prompt\": agent.dynamic_prompt,\n                \"purpose\": agent.purpose,\n                \"purpose_embedding\": None,\n                \"depth\": agent.depth,\n                \"max_depth\": agent.max_depth,\n                \"usage_count\": agent.usage_count,\n                \"id\": agent.id,\n                \"parent_id\": agent.parent_id,\n                \"working_agent\": agent.working_agent,\n                \"is_prime\": agent.is_prime,\n                \"evolve_count\": agent.evolve_count,\n                \"number_of_code_executions\": agent.number_of_code_executions,\n                \"last_input\": agent.last_input\n            }\n\n        return data"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = agent.__dict__.copy()\n\n        if agent_dict[\"purpose_embedding\"] is not None:\n            agent_dict[\"purpose_embedding\"] = agent_dict[\"purpose_embedding\"].tolist()\n\n        return agent_dict\n\n    "}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n\n        if agent.purpose_embedding is not None:\n            agent_dict[\"purpose_embedding\"] = agent.purpose_embedding.tolist()\n\n        return agent_dict\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    "}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    "}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    "}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    "}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    "}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    "}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    "}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    "}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    "}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    "}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    "}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": ""}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": ""}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"The number of bins must be positive.\")\n\n    # Check that the number of items and weights are the same\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items and weights must be the same.\")\n\n    # Check that all weights are positive\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"All weights must be positive.\")\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and their weights\n    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        bin_idx = min(bin_weights, key=bin_weights.get)\n        bins[bin_idx].append(item)\n        bin_weights[bin_idx] += weight\n\n    return bins, bin_weights"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"Number of bins must be positive\")\n\n    # Check that the number of items and weights is the same\n    if len(items) != len(weights):\n        raise ValueError(\"Number of items and weights must be the same\")\n\n    # Check that all weights are positive\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"All weights must be positive\")\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bin dictionaries\n    bins: Dict[int, List[Any]] = defaultdict(list)\n    bin_weights: Dict[int, int] = defaultdict(int)\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        min_bin = min(bin_weights, key=bin_weights.get)\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Sort the items by weight in descending order\n    sorted_items = [(item, weight) for item, weight in sorted(zip(items, weights), key=lambda x: x[1], reverse=True)]\n\n    # Initialize the bin dictionaries\n    bins: Dict[int, List[Any]] = defaultdict(list)\n    bin_weights: Dict[int, int] = defaultdict(int)\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        min_bin_index = min(bin_weights, key=lambda x: bin_weights[x])\n        bins[min_bin_index].append(item)\n        bin_weights[min_bin_index] += weight\n\n    return bins, bin_weights"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"Number of bins must be positive.\")\n\n    # Check that the number of items is equal to the number of weights\n    if len(items) != len(weights):\n        raise ValueError(\"Number of items and weights must be equal.\")\n\n    # Check that all weights are positive\n    if any(w < 0 for w in weights):\n        raise ValueError(\"All weights must be positive.\")\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and total weights dictionaries\n    bins: Dict[int, List[Any]] = defaultdict(list)\n    total_weights: Dict[int, int] = defaultdict(int)\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        min_bin = min(total_weights, key=total_weights.get)\n        bins[min_bin].append(item)\n        total_weights[min_bin] += weight\n\n    # Return the bins and total weights dictionaries\n    return bins, total_weights"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check input arguments\n    if len(items) != len(weights):\n        raise ValueError(\"The length of the 'items' list must be equal to the length of the 'weights' list.\")\n    if num_bins <= 0:\n        raise ValueError(\"The number of bins must be a positive integer.\")\n\n    # Sort items by weight in descending order\n    sorted_items = sorted(zip(weights, items), reverse=True)\n\n    # Initialize bin dictionaries\n    bins: Dict[int, List[Any]] = defaultdict(list)\n    bin_weights: Dict[int, int] = defaultdict(int)\n\n    # Place items into bins greedily\n    for weight, item in sorted_items:\n        # Find the bin with the lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Place item in the bin with the lowest total weight\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check if the number of bins is less than or equal to zero\n    if num_bins <= 0:\n        raise ValueError(\"Number of bins must be positive\")\n\n    # Check if the number of items and weights match\n    if len(items) != len(weights):\n        raise ValueError(\"Number of items and weights must match\")\n\n    # Check if the weights are all positive\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"Weights must be positive\")\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize a dictionary to store the items in each bin\n    bins = defaultdict(list)\n\n    # Initialize a dictionary to store the total weight of each bin\n    bin_weights = defaultdict(int)\n\n    # Iterate over the sorted items and distribute them into bins\n    for item, weight in sorted_items:\n        # Find the bin with the current lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Add the item to the bin with the current lowest total weight\n        bins[min_bin].append(item)\n\n        # Update the total weight of the bin\n        bin_weights[min_bin] += weight\n\n    # Return the dictionaries containing the items and total weights in each bin\n    return bins, bin_weights"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check if the number of bins is greater than the number of items\n    if num_bins > len(items):\n        raise ValueError(\"Number of bins cannot be greater than the number of items\")\n\n    # Check if the number of bins is less than or equal to zero\n    if num_bins <= 0:\n        raise ValueError(\"Number of bins must be greater than zero\")\n\n    # Check if the number of weights is equal to the number of items\n    if len(weights) != len(items):\n        raise ValueError(\"Number of weights must be equal to the number of items\")\n\n    # Check if any weight is negative\n    if any(weight < 0 for weight in weights):\n        raise ValueError(\"All weights must be non-negative\")\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and their weights\n    bins: Dict[int, List[Any]] = defaultdict(list)\n    bin_weights: Dict[int, int] = defaultdict(int)\n\n    # Iterate over the sorted items and place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        bin_idx = min(bin_weights.keys(), key=lambda x: bin_weights[x])\n        bins[bin_idx].append(item)\n        bin_weights[bin_idx] += weight\n\n    return bins, bin_weights"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        return hashlib.sha256(json.dumps(data).encode(\"utf-8\")).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        return hashlib.sha256(json.dumps(data).encode(\"utf-8\")).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        return hashlib.sha256(json.dumps(data).encode(\"utf-8\")).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        return hashlib.sha256(json.dumps(data).encode()).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        return hashlib.sha256(json.dumps(data).encode()).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        return hashlib.sha256(json.dumps(data).encode(\"utf-8\")).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        return hashlib.sha256(json.dumps(data).encode(\"utf-8\")).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        return hashlib.sha256(str(data).encode(\"utf-8\")).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        return hashlib.sha256(json.dumps(data).encode(\"utf-8\")).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        return hashlib.sha256(json.dumps(data).encode(\"utf-8\")).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        return hashlib.sha256(str(data).encode(\"utf-8\")).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        return hashlib.sha256(json.dumps(data).encode()).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        return hashlib.sha256(str(data).encode()).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        return hashlib.sha256(json.dumps(data).encode()).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        return hashlib.sha256(json.dumps(data, sort_keys=True).encode()).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = func_name.encode() + str(args).encode() + str(kwargs).encode()\n        return hashlib.sha256(data).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, *args, json.dumps(kwargs))\n        return hashlib.sha256(str(data).encode(\"utf-8\")).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, *args, kwargs)\n        return hashlib.sha256(str(data).encode()).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = func_name.encode(\"utf-8\")\n        for arg in args:\n            data += json.dumps(arg).encode(\"utf-8\")\n        for kwarg in kwargs:\n            data += json.dumps(kwarg).encode(\"utf-8\")\n        return hashlib.sha256(data).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, *args, json.dumps(kwargs, sort_keys=True))\n        return hashlib.sha256(json.dumps(data, sort_keys=True).encode(\"utf-8\")).hexdigest()"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    length = 0.0\n    for i in range(len(polygon) - 1):\n        distance = np.linalg.norm(polygon[i] - polygon[i + 1])\n        if distance <= max_point_distance:\n            length += distance\n\n    return length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    if len(polygon.shape) != 2 or polygon.shape[1] != 2:\n        raise ValueError(f\"Unable to determine the length of a polygon with shape {polygon.shape}. Expecting (_, 2).\")\n\n    length = 0.0\n    for i in range(len(polygon) - 1):\n        distance = np.linalg.norm(polygon[i] - polygon[i + 1])\n        if distance <= max_point_distance:\n            length += distance\n\n    return length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    if len(polygon.shape) != 2 or polygon.shape[1] != 2:\n        raise ValueError(f\"Unable to determine the length of a polygon with shape {polygon.shape}. Expecting (_, 2).\")\n\n    length = 0.0\n    for i in range(len(polygon) - 1):\n        distance = np.linalg.norm(polygon[i] - polygon[i + 1])\n        if distance > max_point_distance:\n            continue\n        length += distance\n\n    return length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    if len(polygon.shape) != 2 or polygon.shape[1] != 2:\n        raise ValueError(f\"Unable to determine the length of a polygon with shape {polygon.shape}. Expecting (_, 2).\")\n\n    length = 0\n    for i in range(len(polygon) - 1):\n        point_1 = polygon[i]\n        point_2 = polygon[i + 1]\n        distance = np.linalg.norm(point_1 - point_2)\n        if distance <= max_point_distance:\n            length += distance\n\n    return float(length)"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    if len(polygon.shape) != 2 or polygon.shape[1] != 2:\n        raise ValueError(f\"Unable to determine the length of a polygon with shape {polygon.shape}. Expecting (_, 2).\")\n\n    length = 0.0\n    for i in range(len(polygon) - 1):\n        dist = np.linalg.norm(polygon[i] - polygon[i + 1])\n        if dist <= max_point_distance:\n            length += dist\n\n    return length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    if len(polygon.shape) != 2 or polygon.shape[1] != 2:\n        raise ValueError(f\"Unable to determine the length of a polygon with shape {polygon.shape}. Expecting (_, 2).\")\n\n    polygon_length = 0.0\n    for i in range(len(polygon) - 1):\n        distance = np.linalg.norm(polygon[i] - polygon[i + 1])\n        if distance > max_point_distance:\n            continue\n        polygon_length += distance\n\n    return polygon_length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the distances between consecutive points\n    distances = np.linalg.norm(polygon[1:] - polygon[:-1], axis=1)\n\n    # Exclude distances that exceed the maximum threshold\n    distances = distances[distances <= max_point_distance]\n\n    # Sum the distances to obtain the total length\n    total_length = np.sum(distances)\n\n    return total_length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    if len(polygon) < 2:\n        return 0.0\n\n    length = 0.0\n    for i in range(len(polygon) - 1):\n        point_1 = polygon[i]\n        point_2 = polygon[i + 1]\n        distance = np.linalg.norm(point_2 - point_1)\n        if distance <= max_point_distance:\n            length += distance\n\n    return length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the distances between consecutive points in the polygon\n    distances = np.linalg.norm(polygon[:-1] - polygon[1:], axis=1)\n\n    # Exclude distances that exceed the maximum\n    distances = distances[distances <= max_point_distance]\n\n    # Sum the distances to obtain the total length\n    total_length = np.sum(distances)\n\n    return total_length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the distance between consecutive points in the polygon\n    distances = np.linalg.norm(polygon[1:] - polygon[:-1], axis=1)\n\n    # Remove distances that exceed the maximum distance threshold\n    distances = distances[distances <= max_point_distance]\n\n    # Return the sum of the distances as the total length of the polygon\n    return float(np.sum(distances))"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Calculate the distances between consecutive points in the polygon\n    distances = np.linalg.norm(polygon[1:] - polygon[:-1], axis=1)\n\n    # Filter out distances that exceed the maximum distance threshold\n    distances = distances[distances <= max_point_distance]\n\n    # Sum the distances to obtain the total length of the polygon\n    polygon_length = np.sum(distances)\n\n    return polygon_length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    if len(polygon.shape) != 2 or polygon.shape[1] != 2:\n        raise ValueError(f\"Unable to determine the length of a polygon with shape {polygon.shape}. Expecting (_, 2).\")\n\n    length = 0.0\n\n    for i in range(len(polygon) - 1):\n        p1 = polygon[i]\n        p2 = polygon[i + 1]\n\n        distance = np.linalg.norm(p1 - p2)\n\n        if distance <= max_point_distance:\n            length += distance\n\n    return length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    if len(polygon.shape) != 2 or polygon.shape[1] != 2:\n        raise ValueError(f\"Unable to compute the polygon length of a polygon with shape {polygon.shape}. Expecting (_, 2).\")\n\n    length = 0\n    for i in range(len(polygon) - 1):\n        distance = np.linalg.norm(polygon[i] - polygon[i + 1])\n        if distance <= max_point_distance:\n            length += distance\n\n    return length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the distances between consecutive points\n    distances = np.linalg.norm(polygon[1:] - polygon[:-1], axis=1)\n\n    # Filter out distances that exceed the maximum\n    distances = distances[distances <= max_point_distance]\n\n    # Compute the total length\n    length = np.sum(distances)\n\n    return length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    total_length = 0\n    for i in range(len(polygon)):\n        if i < len(polygon) - 1:\n            distance = np.linalg.norm(polygon[i + 1] - polygon[i])\n            if distance <= max_point_distance:\n                total_length += distance\n    return total_length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Initialize the length to zero\n    length = 0.0\n\n    # Iterate over the points in the polygon\n    for i in range(len(polygon) - 1):\n        # Compute the distance between the current point and the next point\n        distance = np.linalg.norm(polygon[i] - polygon[i + 1])\n\n        # If the distance is below the maximum distance, add it to the length\n        if distance <= max_point_distance:\n            length += distance\n\n    # Return the total length of the polygon\n    return length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    if len(polygon.shape) != 2 or polygon.shape[1] != 2:\n        raise ValueError(f\"Unable to determine the length of a polygon with shape {polygon.shape}. Expecting (_, 2).\")\n\n    # Compute the distance between consecutive points in the polygon\n    distances = np.linalg.norm(polygon[:-1] - polygon[1:], axis=1)\n\n    # Exclude distances that exceed the maximum distance threshold\n    distances = distances[distances <= max_point_distance]\n\n    # Sum the distances to obtain the total length of the polygon\n    total_length = np.sum(distances)\n\n    return float(total_length)\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Initialize the length to 0\n    length = 0\n\n    # Loop over all pairs of consecutive points in the polygon\n    for i in range(len(polygon) - 1):\n        # Calculate the distance between the current point and the next point\n        distance = np.linalg.norm(polygon[i] - polygon[i + 1])\n\n        # If the distance is below the maximum, add it to the total length\n        if distance <= max_point_distance:\n            length += distance\n\n    return length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    if len(polygon.shape) != 2 or polygon.shape[1] != 2:\n        raise ValueError(f\"Unable to determine the length of a polygon with shape {polygon.shape}. Expecting (_, 2).\")\n\n    # Compute the distances between consecutive points in the polygon\n    distances = np.linalg.norm(polygon[:-1] - polygon[1:], axis=1)\n\n    # Exclude distances that exceed the maximum distance threshold\n    distances = distances[distances <= max_point_distance]\n\n    # Sum the distances to get the total length\n    length = np.sum(distances)\n\n    return float(length)\n\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    if len(polygon.shape) != 2 or polygon.shape[1] != 2:\n        raise ValueError(f\"Unable to determine the length of a polygon with shape {polygon.shape}. Expecting (_, 2).\")\n\n    if len(polygon) < 2:\n        raise ValueError(f\"Unable to determine the length of a polygon with shape {polygon.shape}. Expecting at least 2 points.\")\n\n    polygon_length = np.sum(\n        np.linalg.norm(polygon[:-1] - polygon[1:], axis=1)\n    )  # type: ignore\n\n    return float(polygon_length)\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    largest_polygon_area = max(area(polygon) for polygon in polygons)\n    return [polygon for polygon in polygons if area(polygon) >= max(rel_tr * largest_polygon_area, abs_tr)]"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    largest_area = max([area(polygon) for polygon in polygons])\n\n    return [polygon for polygon in polygons if area(polygon) >= max(rel_tr * largest_area, abs_tr)]"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    largest_area = max([area(polygon) for polygon in polygons])\n    return [polygon for polygon in polygons if area(polygon) > max(rel_tr * largest_area, abs_tr)]"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    max_area = max([area(polygon) for polygon in polygons])\n    return [polygon for polygon in polygons if area(polygon) >= max(abs_tr, rel_tr * max_area)]"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    max_area = max(area(polygon) for polygon in polygons)\n    polygons = [polygon for polygon in polygons if area(polygon) >= rel_tr * max_area or area(polygon) >= abs_tr]\n\n    return polygons"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if not polygons:\n        return polygons\n\n    largest_area = max(area(polygon) for polygon in polygons)\n\n    return [polygon for polygon in polygons if area(polygon) >= rel_tr * largest_area or area(polygon) >= abs_tr]"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if not polygons:\n        return []\n\n    largest_polygon_area = area(polygons[0])\n    polygons = [polygon for polygon in polygons if area(polygon) > rel_tr * largest_polygon_area or area(polygon) > abs_tr]\n\n    return polygons"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    largest_area = max([area(polygon) for polygon in polygons])\n\n    return [polygon for polygon in polygons if area(polygon) > rel_tr * largest_area or area(polygon) > abs_tr]"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    largest_area = max([area(polygon) for polygon in polygons])\n\n    return [polygon for polygon in polygons if area(polygon) > rel_tr * largest_area or area(polygon) > abs_tr]"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    largest_polygon_area = max([area(polygon) for polygon in polygons])\n    polygons = [polygon for polygon in polygons if area(polygon) >= max(rel_tr * largest_polygon_area, abs_tr)]\n\n    return polygons"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if not polygons:\n        return polygons\n\n    largest_polygon_area = area(polygons[0])\n    filtered_polygons = [polygon for polygon in polygons if area(polygon) > largest_polygon_area * rel_tr]\n    filtered_polygons = [polygon for polygon in filtered_polygons if area(polygon) > abs_tr]\n\n    return filtered_polygons"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    areas = [area(polygon) for polygon in polygons]\n    max_area = max(areas)\n    rel_threshold = rel_tr * max_area\n    abs_threshold = abs_tr\n\n    return [polygon for polygon, area in zip(polygons, areas) if area >= rel_threshold or area >= abs_threshold]"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    largest_polygon = max(polygons, key=area)\n    rel_tr = rel_tr * area(largest_polygon)\n    abs_tr = abs_tr * area(largest_polygon)\n\n    return [polygon for polygon in polygons if area(polygon) > rel_tr or area(polygon) > abs_tr]"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    largest_polygon = max(polygons, key=area)\n    largest_area = area(largest_polygon)\n\n    return [polygon for polygon in polygons if area(polygon) > rel_tr * largest_area or area(polygon) > abs_tr]"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    areas = [area(polygon) for polygon in polygons]\n    max_area = max(areas)\n\n    rel_tr_area = rel_tr * max_area\n    abs_tr_area = abs_tr\n\n    filtered_polygons = [polygon for polygon, area in zip(polygons, areas) if area >= rel_tr_area or area >= abs_tr_area]\n\n    return filtered_polygons"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    largest_polygon = max(polygons, key=lambda x: area(x))\n    largest_polygon_area = area(largest_polygon)\n\n    filtered_polygons = [\n        polygon for polygon in polygons if area(polygon) > abs_tr or area(polygon) > largest_polygon_area * rel_tr\n    ]\n\n    return filtered_polygons"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if not polygons:\n        return polygons\n\n    largest_area = max(area(polygon) for polygon in polygons)\n    rel_threshold = rel_tr * largest_area\n    abs_threshold = abs_tr\n\n    return [polygon for polygon in polygons if area(polygon) >= max(rel_threshold, abs_threshold)]\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        raise ValueError(\"filter_polygon_areas: No polygons provided.\")\n\n    largest_area = max(area(polygon) for polygon in polygons)\n    filtered_polygons = [polygon for polygon in polygons if area(polygon) >= rel_tr * largest_area or area(polygon) >= abs_tr]\n\n    return filtered_polygons"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    largest_polygon_area = max([area(polygon) for polygon in polygons], default=0)\n    filtered_polygons = [polygon for polygon in polygons if area(polygon) >= rel_tr * largest_polygon_area or area(polygon) >= abs_tr]\n\n    return filtered_polygons"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if not polygons:\n        raise ValueError(\"Polygons list is empty.\")\n\n    if rel_tr < 0 or abs_tr < 0:\n        raise ValueError(\"Relative and absolute thresholds must be non-negative.\")\n\n    largest_area = max(area(polygon) for polygon in polygons)\n\n    filtered_polygons = [polygon for polygon in polygons if area(polygon) >= max(rel_tr * largest_area, abs_tr)]\n\n    return filtered_polygons"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    indexes = {}\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] = 0\n\n    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_remaining = num_samples_yielded % num_workers\n\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] = num_samples_per_worker\n\n    for worker_idx in range(num_samples_remaining):\n        indexes[worker_idx] += 1\n\n    return indexes"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    indexes = {}\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] = num_samples_yielded // num_workers\n\n    remaining_samples = num_samples_yielded % num_workers\n    for worker_idx in range(num_workers):\n        if remaining_samples > 0:\n            indexes[worker_idx] += 1\n            remaining_samples -= 1\n\n    return indexes"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    samples_per_worker = num_samples_yielded // num_workers\n    remainder = num_samples_yielded % num_workers\n\n    indexes = {}\n    for worker_idx in range(num_workers):\n        if worker_idx < remainder:\n            indexes[worker_idx] = samples_per_worker + 1\n        else:\n            indexes[worker_idx] = samples_per_worker\n\n    return indexes"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    indexes = {}\n\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] = 0\n\n    for worker_idx in range(num_workers):\n        samples_per_worker = num_samples_yielded // num_workers\n        if worker_idx < num_samples_yielded % num_workers:\n            samples_per_worker += 1\n\n        samples_per_batch = samples_per_worker // batch_size\n        if worker_idx < samples_per_worker % batch_size:\n            samples_per_batch += 1\n\n        indexes[worker_idx] = samples_per_batch\n\n    return indexes"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    indexes = {}\n\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] = num_samples_yielded // num_workers\n\n    for worker_idx in range(num_samples_yielded % num_workers):\n        indexes[worker_idx] += 1\n\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] *= batch_size\n\n    return indexes"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    indexes = {}\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] = num_samples_yielded // num_workers\n\n    remaining_samples = num_samples_yielded % num_workers\n    for worker_idx in range(num_workers):\n        if remaining_samples > 0:\n            indexes[worker_idx] += 1\n            remaining_samples -= 1\n\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] *= batch_size\n\n    return indexes"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_remaining = num_samples_yielded % num_workers\n\n    indexes = {}\n    for worker_idx in range(num_workers):\n        num_samples = num_samples_per_worker\n        if num_samples_remaining > 0:\n            num_samples += 1\n            num_samples_remaining -= 1\n\n        indexes[worker_idx] = num_samples // batch_size\n\n    return indexes"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    samples_per_worker = num_samples_yielded // num_workers\n    remaining_samples = num_samples_yielded % num_workers\n\n    indexes = {}\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] = samples_per_worker\n        if remaining_samples > 0:\n            indexes[worker_idx] += 1\n            remaining_samples -= 1\n\n    return indexes"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    indexes = {}\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] = 0\n\n    for worker_idx in range(num_workers):\n        num_samples = num_samples_yielded // num_workers\n        if worker_idx < num_samples_yielded % num_workers:\n            num_samples += 1\n\n        indexes[worker_idx] = num_samples * batch_size\n\n    return indexes"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    indexes = {}\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] = 0\n\n    if num_samples_yielded == 0:\n        return indexes\n\n    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_remaining = num_samples_yielded % num_workers\n\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] = num_samples_per_worker\n\n    for worker_idx in range(num_samples_remaining):\n        indexes[worker_idx] += 1\n\n    return indexes"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_remaining = num_samples_yielded % num_workers\n\n    indexes = {}\n    for worker_idx in range(num_workers):\n        num_samples_to_process = num_samples_per_worker\n        if num_samples_remaining > 0:\n            num_samples_to_process += 1\n            num_samples_remaining -= 1\n\n        num_batches_to_process = num_samples_to_process // batch_size\n        num_samples_to_process = num_batches_to_process * batch_size\n\n        indexes[worker_idx] = num_samples_to_process\n\n    return indexes"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    indexes = {}\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] = num_samples_yielded % num_workers\n        num_samples_yielded -= indexes[worker_idx]\n\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] += worker_idx\n\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] = int(indexes[worker_idx] / num_workers * batch_size)\n\n    return indexes"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples per worker\n    samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples to be distributed evenly among workers\n    even_samples_per_worker = samples_per_worker // batch_size\n\n    # Calculate the number of remaining samples\n    remaining_samples = samples_per_worker - (even_samples_per_worker * batch_size)\n\n    # Create a dictionary to store the number of samples processed by each worker\n    indexes = {}\n\n    # Distribute the even samples evenly among workers\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] = even_samples_per_worker\n\n    # Distribute the remaining samples among workers\n    for worker_idx in range(num_workers):\n        if remaining_samples > 0:\n            indexes[worker_idx] += 1\n            remaining_samples -= 1\n\n    return indexes"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples per worker\n    samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples per worker, taking into account the batch size\n    samples_per_worker = samples_per_worker // batch_size * batch_size\n\n    # Distribute any remaining samples evenly among the workers\n    remaining_samples = num_samples_yielded - samples_per_worker * num_workers\n    for worker_idx in range(num_workers):\n        if remaining_samples > 0:\n            samples_per_worker += 1\n            remaining_samples -= 1\n\n    # Create a dictionary to store the number of samples per worker\n    samples_per_worker_dict = {}\n    for worker_idx in range(num_workers):\n        samples_per_worker_dict[worker_idx] = samples_per_worker\n\n    return samples_per_worker_dict"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples per worker\n    samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples to be distributed to each worker\n    distributed_samples = samples_per_worker * num_workers\n\n    # Calculate the number of samples that will be distributed to the last worker\n    remaining_samples = num_samples_yielded - distributed_samples\n\n    # Calculate the number of samples to be distributed to each worker\n    samples_per_worker = samples_per_worker + remaining_samples // num_workers\n\n    # Calculate the number of samples that will be distributed to the last worker\n    remaining_samples = remaining_samples - (remaining_samples // num_workers) * num_workers\n\n    # Calculate the number of samples to be distributed to each worker\n    samples_per_worker = samples_per_worker + remaining_samples // num_workers\n\n    # Calculate the number of samples that will be distributed to the last worker\n    remaining_samples = remaining_samples - (remaining_samples // num_workers) * num_workers\n\n    # Calculate the number of samples to be distributed to each worker\n    samples_per_worker = samples_per_worker + remaining_samples // num_workers\n\n    # Calculate the number of samples that will be distributed to the last worker\n    remaining_samples = remaining_samples - (remaining_samples // num_workers) * num_workers\n\n    # Calculate the number of samples to be distributed to each worker\n    samples_per_worker = samples_per_worker + remaining_samples // num_workers\n\n    # Calculate the number of samples that will be distributed to the last worker\n    remaining_samples = remaining_samples - (remaining_samples // num_workers) * num_workers\n\n    # Calculate the number of samples to be distributed to each worker\n    samples_per_worker = samples_per_worker + remaining_samples // num_workers\n\n    # Calculate the number of samples that will be distributed"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples to be processed by each worker\n    samples_per_worker = num_samples_yielded // num_workers\n    remaining_samples = num_samples_yielded % num_workers\n\n    # Initialize a dictionary to store the number of samples processed by each worker\n    samples_processed = {}\n\n    # Distribute the samples evenly among the workers\n    for worker_idx in range(num_workers):\n        samples_processed[worker_idx] = samples_per_worker\n\n    # Distribute the remaining samples\n    for worker_idx in range(remaining_samples):\n        samples_processed[worker_idx] += 1\n\n    return samples_processed"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    samples_per_worker = num_samples_yielded // num_workers\n    samples_per_worker_remaining = num_samples_yielded % num_workers\n\n    samples_per_worker_batch = samples_per_worker // batch_size\n    samples_per_worker_batch_remaining = samples_per_worker % batch_size\n\n    indexes = {}\n\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] = (\n            worker_idx * samples_per_worker_batch * batch_size\n            + min(worker_idx, samples_per_worker_batch_remaining) * batch_size\n            + max(worker_idx - samples_per_worker_batch_remaining, 0)\n        )\n\n    return indexes"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples per worker\n    samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples to be distributed to each worker\n    samples_to_distribute = num_samples_yielded % num_workers\n\n    # Distribute the samples to each worker\n    samples_per_worker += samples_to_distribute // num_workers\n\n    # Calculate the number of samples that should be added to each worker\n    samples_to_add = samples_to_distribute % num_workers\n\n    # Add the samples to the workers with the highest index\n    for i in range(samples_to_add):\n        samples_per_worker += 1\n\n    # Create a dictionary to store the number of samples per worker\n    samples_per_worker_dict = {}\n\n    # Add the samples per worker to the dictionary\n    for i in range(num_workers):\n        samples_per_worker_dict[i] = samples_per_worker\n\n    # Calculate the number of samples that should be added to each worker\n    samples_to_add = num_samples_yielded % batch_size\n\n    # Add the samples to the workers with the highest index\n    for i in range(samples_to_add):\n        samples_per_worker_dict[i] += num_samples_yielded % batch_size\n\n    # Return the number of samples per worker\n    return samples_per_worker_dict"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples that each worker should process\n    samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples that should be distributed to each worker\n    samples_per_worker_distribution = samples_per_worker // batch_size\n\n    # Calculate the number of remaining samples\n    remaining_samples = num_samples_yielded % num_workers\n\n    # Initialize a dictionary to store the number of samples each worker should process\n    samples_per_worker_dict = {}\n\n    # Distribute the remaining samples to the workers\n    for worker_idx in range(num_workers):\n        samples_per_worker_dict[worker_idx] = samples_per_worker_distribution\n\n        if remaining_samples > 0:\n            samples_per_worker_dict[worker_idx] += 1\n            remaining_samples -= 1\n\n    # Return the dictionary\n    return samples_per_worker_dict"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples per worker\n    samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples to be distributed among the workers\n    samples_to_distribute = num_samples_yielded - samples_per_worker * num_workers\n\n    # Calculate the number of samples to be distributed for each worker\n    samples_to_distribute_per_worker = samples_to_distribute // num_workers\n\n    # Initialize the dictionary to store the number of samples processed by each worker\n    samples_processed = {}\n\n    # Loop through the workers and distribute the samples\n    for worker_idx in range(num_workers):\n        # Calculate the number of samples to be distributed for this worker\n        samples_to_distribute_for_worker = samples_to_distribute_per_worker\n        if worker_idx < samples_to_distribute % num_workers:\n            samples_to_distribute_for_worker += 1\n\n        # Calculate the number of samples processed by this worker\n        samples_processed[worker_idx] = samples_per_worker + samples_to_distribute_for_worker\n\n    # Calculate the number of samples to be processed by each worker for the next batch\n    samples_to_process = {}\n    for worker_idx in range(num_workers):\n        samples_to_process[worker_idx] = samples_processed[worker_idx] + batch_size\n\n    # Return the number of samples processed by each worker for the next batch\n    return samples_to_process"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results, filtered_metadatas = [], []\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results, filtered_metadatas = [], []\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n    assert len(results) == len(value) == len(metadatas), \"results, value and metadatas must have the same length.\"\n    filtered_results = [result for result, value, metadata in zip(results, value, metadatas) if value <= threshold]\n    filtered_metadatas = [metadata for result, value, metadata in zip(results, value, metadatas) if value <= threshold]\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n    assert len(results) == len(value), \"results and value must have the same length.\"\n    assert len(results) == len(metadatas), \"results and metadatas must have the same length.\"\n    filtered_results = []\n    filtered_metadatas = []\n    for result, v, metadata in zip(results, value, metadatas):\n        if v <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n    assert len(results) == len(value), \"results and value must have the same length.\"\n    assert len(results) == len(metadatas), \"results and metadatas must have the same length.\"\n    filtered_results = []\n    filtered_metadatas = []\n    for result, v, metadata in zip(results, value, metadatas):\n        if v <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n    for i in range(len(results)):\n        if value[i] <= threshold:\n            filtered_results.append(results[i])\n            filtered_metadatas.append(metadatas[i])\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n    for i in range(len(results)):\n        if value[i] <= threshold:\n            filtered_results.append(results[i])\n            filtered_metadatas.append(metadatas[i])\n    return filtered_results, filtered_metadatas"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f\"This function expects a polygon, i.e. an array of shape (_, 2). Got {array.shape}\")\n\n    xs = array[:, 0]\n    ys = array[:, 1]\n\n    return 0.5 * np.abs(np.dot(xs, np.roll(ys, 1)) - np.dot(ys, np.roll(xs, 1)))"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f\"This function expects a polygon, i.e. an array of shape (_, 2). Got {array.shape}\")\n\n    xs = array[:, 0]\n    ys = array[:, 1]\n\n    return 0.5 * np.abs(np.dot(xs, np.roll(ys, 1)) - np.dot(ys, np.roll(xs, 1)))"}
{"namespace": "iris.utils.math.area", "completion": "    if array.ndim != 2 or array.shape[1] != 2:\n        raise ValueError(f\"This function expects a polygon, i.e. an array of shape (_, 2). Got {array.shape}\")\n\n    xs, ys = array[:, 0], array[:, 1]\n\n    return 0.5 * np.abs(np.dot(xs, np.roll(ys, 1)) - np.dot(ys, np.roll(xs, 1)))"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f\"This function expects a polygon, i.e. an array of shape (_, 2). Got {array.shape}\")\n\n    # fmt: off\n    area = 0.5 * np.abs(\n        np.dot(array[:, 0], np.roll(array[:, 1], 1))\n        - np.dot(array[:, 1], np.roll(array[:, 0], 1))\n    )\n    # fmt: on\n\n    return area"}
{"namespace": "iris.utils.math.area", "completion": "    # fmt: off\n    return 0.5 * np.abs(\n        np.dot(array[:, 0], np.roll(array[:, 1], 1))\n        - np.dot(array[:, 1], np.roll(array[:, 0], 1))\n    )"}
{"namespace": "iris.utils.math.area", "completion": "    if array.ndim != 2 or array.shape[1] != 2:\n        raise ValueError(f\"This function expects a polygon, i.e. an array of shape (_, 2). Got {array.shape}\")\n\n    xs = array[:, 0]\n    ys = array[:, 1]\n\n    return 0.5 * np.abs(np.dot(xs, np.roll(ys, 1)) - np.dot(ys, np.roll(xs, 1)))"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f\"This function expects a polygon, i.e. an array of shape (_, 2). Got {array.shape}\")\n\n    # fmt: off\n    xs = array[:, 0]\n    ys = array[:, 1]\n\n    area = 0.5 * np.abs(np.dot(xs, np.roll(ys, 1)) - np.dot(ys, np.roll(xs, 1)))\n    # fmt: on\n\n    return area"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f\"This function expects a polygon, i.e. an array of shape (_, 2). Got {array.shape}\")\n\n    x, y = array[:, 0], array[:, 1]\n\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f\"This function expects a polygon, i.e. an array of shape (_, 2). Got {array.shape}\")\n\n    n = array.shape[0]\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i, 0] * array[j, 1]\n        area -= array[j, 0] * array[i, 1]\n    area /= 2.0\n\n    return abs(area)"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f\"Expected array of shape (_, 2). Got {array.shape}\")\n\n    n = array.shape[0]\n    area = 0.0\n    for i in range(n):\n        area += array[i, 0] * array[(i + 1) % n, 1]\n        area -= array[i, 1] * array[(i + 1) % n, 0]\n    area /= 2.0\n\n    return area"}
{"namespace": "iris.utils.math.area", "completion": "    if array.ndim != 2 or array.shape[1] != 2:\n        raise ValueError(f\"This function expects a polygon, i.e. an array of shape (_, 2). Got {array.shape}\")\n\n    if array.shape[0] < 3:\n        raise ValueError(f\"This function expects a polygon with at least 3 points. Got {array.shape[0]} points.\")\n\n    # Shoelace formula\n    return 0.5 * np.abs(\n        np.dot(array[:, 0], np.roll(array[:, 1], 1)) - np.dot(array[:, 1], np.roll(array[:, 0], 1))\n    )"}
{"namespace": "iris.utils.math.area", "completion": "    if array.ndim != 2 or array.shape[1] != 2:\n        raise ValueError(f\"This function expects a polygon, i.e. an array of shape (_, 2). Got {array.shape}\")\n\n    n = len(array)\n    A = 0\n    for i in range(n):\n        j = (i + 1) % n\n        A += array[i][0] * array[j][1]\n        A -= array[j][0] * array[i][1]\n\n    return 0.5 * abs(A)\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f\"This function expects a polygon, i.e. an array of shape (_, 2). Got {array.shape}\")\n\n    n = len(array)\n    return 0.5 * np.abs(\n        np.dot(array[:, 0], np.roll(array[:, 1], 1)) - np.dot(array[:, 1], np.roll(array[:, 0], 1))\n    )\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f\"This function expects a polygon, i.e. an array of shape (_, 2). Got {array.shape}\")\n\n    # fmt: off\n    return 0.5 * np.abs(\n        np.dot(array[:, 0], np.roll(array[:, 1], 1))\n        - np.dot(array[:, 1], np.roll(array[:, 0], 1))\n    )\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f\"Expected array of shape (_, 2). Got {array.shape}\")\n\n    xs = array[:, 0]\n    ys = array[:, 1]\n\n    return 0.5 * np.abs(np.dot(xs, np.roll(ys, 1)) - np.dot(ys, np.roll(xs, 1)))\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.ndim != 2 or array.shape[1] != 2:\n        raise ValueError(f\"This function expects a polygon, i.e. an array of shape (_, 2). Got {array.shape}\")\n\n    n = len(array)\n    A = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        A += array[i][0] * array[j][1]\n        A -= array[j][0] * array[i][1]\n    A = abs(A) / 2.0\n    return A\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.ndim != 2 or array.shape[1] != 2:\n        raise ValueError(f\"This function expects a polygon, i.e. an array of shape (_, 2). Got {array.shape}\")\n\n    xs, ys = array[:, 0], array[:, 1]\n\n    # fmt: off\n    area = 0.5 * np.abs(np.dot(xs, np.roll(ys, 1)) - np.dot(ys, np.roll(xs, 1)))\n    # fmt: on\n\n    return area\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f\"This function expects a polygon, i.e. an array of shape (_, 2). Got {array.shape}\")\n\n    # fmt: off\n    area = (\n        np.dot(array[:, 0], np.roll(array[:, 1], 1))\n        - np.dot(array[:, 1], np.roll(array[:, 0], 1))\n    ) / 2\n    # fmt: on\n\n    return area\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.ndim != 2 or array.shape[1] != 2:\n        raise ValueError(f\"This function expects a polygon, i.e. an array of shape (_, 2). Got {array.shape}\")\n\n    array = np.append(array, [array[0]], axis=0)\n    x = array[:, 0]\n    y = array[:, 1]\n\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"This function expects an array of shape (_, 2), where _ can be any number of points.\")\n\n    # Calculate the sum of the cross products of the coordinates\n    cross_product_sum: float = np.sum(np.roll(array[:, 0], -1) * array[:, 1] - np.roll(array[:, 1], -1) * array[:, 0])\n\n    # Divide by 2 to get the area\n    return abs(cross_product_sum / 2)"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Find the indices where elements of v should be inserted into a.\n    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, right=True)\n    idx_hi = torch.clamp(idx_lo - 1, min=0)\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, right=True)\n    idx_hi = torch.clamp(idx_lo - 1, min=0)\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, right=True)\n    idx_hi = torch.clamp(idx_lo - 1, min=0)\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.clamp(torch.searchsorted(a, v, right=True), max=a.shape[-1] - 1)\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Find the indices where v should be inserted into a.\n    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Find the indices where the elements of v should be inserted into a.\n    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Get the insertion index for each query\n    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.clamp(torch.searchsorted(a, v, right=True), max=a.shape[-1] - 1)\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # a is a 1D tensor\n    # v is a 1D tensor\n    # idx_lo is a 1D tensor\n    # idx_hi is a 1D tensor\n    # idx_lo and idx_hi are the indices of a where v could be inserted to maintain order\n    # idx_lo and idx_hi are the indices of a where v could be inserted to maintain order\n    # idx_lo and idx_hi are the indices of a where v could be inserted to maintain order\n    # idx_lo and idx_hi are the indices of a where v could be inserted to maintain order\n    # idx_lo and idx_hi are the indices of a where v could be inserted to maintain order\n    # idx_lo and idx_hi are the indices of a where v could be inserted to maintain order\n    # idx_lo and idx_hi are the indices of a where v could be inserted to maintain order\n    # idx_lo and idx_hi are the indices of a where v could be inserted to maintain order\n    # idx_lo and idx_hi are the indices of a where v could be inserted to maintain order\n    # idx_lo and idx_hi are the indices of a where v could be inserted to maintain order\n    # idx_lo and idx_hi are the indices of a where v could be inserted to maintain order\n    # idx_lo and idx_hi are the indices of a where v could be inserted to maintain order\n    # idx_lo and idx_hi are the indices of a where v could be inserted to maintain order\n    # idx_lo and idx_hi are the indices of a where v could be inserted to maintain order\n    # idx_lo and idx_hi are the indices of a where v could be inserted to maintain order\n    # idx_lo and idx_hi are the indices of a where v could be inserted to maintain order\n    # idx_lo and idx_hi are the indices of a where v could be inserted to maintain order\n    # idx_lo and idx_hi are the indices of a where v could be inserted to maintain order\n    # idx_lo and idx_hi are the indices of a where v could be inserted to maintain order\n    # idx_lo and idx_hi are the indices of a where v could be"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # TODO: Use torch.searchsorted once it supports gradients.\n    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # a must be sorted in ascending order\n    # v must be sorted in ascending order\n    # a and v must have the same shape except for the last dimension\n    # idx_lo and idx_hi will have the same shape as v\n    # idx_lo and idx_hi will be integers\n    # idx_lo will be the index of the first element in a that is greater than or equal to each element in v\n    # idx_hi will be the index of the first element in a that is greater than each element in v\n    # if a is empty, idx_lo will be 0 and idx_hi will be 0\n    # if v is empty, idx_lo will be 0 and idx_hi will be 0\n    # if a is full of NaNs, idx_lo will be 0 and idx_hi will be 0\n    # if v is full of NaNs, idx_lo will be 0 and idx_hi will be 0\n    # if a is full of infinities, idx_lo will be 0 and idx_hi will be 0\n    # if v is full of infinities, idx_lo will be 0 and idx_hi will be 0\n    # if a is full of zeros, idx_lo will be 0 and idx_hi will be 0\n    # if v is full of zeros, idx_lo will be 0 and idx_hi will be 0\n    # if a is full of ones, idx_lo will be 0 and idx_hi will be 0\n    # if v is full of ones, idx_lo will be 0 and idx_hi will be 0\n    # if a is full of -infinities, idx_lo will be 0 and idx_hi will be 0\n    # if v is full of -infinities, idx_lo will be 0 and idx_hi will be 0\n    # if a is full of -ones, idx_lo will be 0 and idx_hi will be 0\n    # if v is full of -ones, idx_lo will be 0 and idx_hi will be 0\n    # if a is full of -zeros, idx_lo will be 0"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Define the search space for each element of v\n    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n\n    # Clip the search space to ensure that idx_lo and idx_hi are within the valid range of a\n    idx_lo = torch.clamp(idx_lo, 0, a.shape[-1] - 1)\n    idx_hi = torch.clamp(idx_hi, 0, a.shape[-1] - 1)\n\n    # Return the indices\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Compute the number of elements in a\n    n = a.shape[-1]\n\n    # Create a tensor of ones with the same shape as v\n    ones = torch.ones_like(v)\n\n    # Compute the indices where elements of v should be inserted into a\n    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n\n    # Clamp the indices to ensure they are within the valid range of a\n    idx_lo = torch.clamp(idx_lo, 0, n - 1)\n    idx_hi = torch.clamp(idx_hi, 0, n - 1)\n\n    # Return the indices as a tuple\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # torch.searchsorted() returns the index of the first element that is greater than v\n    # if the element is not found, it returns the length of the tensor\n    # we want to return the index of the last element that is less than v\n    # if the element is not found, it returns the length of the tensor\n    # we want to return the index of the last element that is less than v\n    # if the element is not found, it returns the length of the tensor\n    # we want to return the index of the last element that is less than v\n    # if the element is not found, it returns the length of the tensor\n    # we want to return the index of the last element that is less than v\n    # if the element is not found, it returns the length of the tensor\n    # we want to return the index of the last element that is less than v\n    # if the element is not found, it returns the length of the tensor\n    # we want to return the index of the last element that is less than v\n    # if the element is not found, it returns the length of the tensor\n    # we want to return the index of the last element that is less than v\n    # if the element is not found, it returns the length of the tensor\n    # we want to return the index of the last element that is less than v\n    # if the element is not found, it returns the length of the tensor\n    # we want to return the index of the last element that is less than v\n    # if the element is not found, it returns the length of the tensor\n    # we want to return the index of the last element that is less than v\n    # if the element is not found, it returns the length of the tensor\n    # we want to return the index of the last element that is less than v\n    # if the element is not found, it returns the length of the tensor\n    # we want to return the index of the last element that is less than v\n    # if the element is not found, it returns the length of the tensor\n    # we want to return the index of the last element that is less than v\n    # if the element is not found, it returns the length of the tensor\n    # we want to"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # torch.searchsorted() only accepts 1-D tensors, so we need to flatten the input tensors\n    a_flat = a.flatten()\n    v_flat = v.flatten()\n\n    # Find the indices where each element of v should be inserted into a to maintain order\n    idx_lo = torch.searchsorted(a_flat, v_flat)\n    idx_hi = torch.searchsorted(a_flat, v_flat, right=True)\n\n    # Reshape the output tensors to match the original shape of the input tensors\n    idx_lo = idx_lo.reshape(v.shape[:-1] + (1,))\n    idx_hi = idx_hi.reshape(v.shape[:-1] + (1,))\n\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # The input tensor a is expected to be sorted.\n    # The output tensor idx_lo and idx_hi are such that\n    # a[idx_lo] <= v < a[idx_hi].\n    # If v is less than the smallest element in a, then idx_lo = 0 and idx_hi = 0.\n    # If v is greater than the largest element in a, then idx_lo = -1 and idx_hi = -1.\n\n    # Compute the indices of the elements in a that are less than or equal to v.\n    idx_lo = torch.searchsorted(a, v, right=False)\n\n    # Compute the indices of the elements in a that are greater than or equal to v.\n    idx_hi = torch.searchsorted(a, v, right=True)\n\n    # Return the indices idx_lo and idx_hi.\n    return idx_lo, idx_hi"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ])"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ])"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ])"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ])"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ])"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ])"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ])"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array(\n      [[fx, 0, cx], [0, fy, cy], [0, 0, 1]], dtype=xnp.float32\n  )"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  if xnp == np:\n    return xnp.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]], dtype=np.float32)\n  else:\n    return xnp.array(\n        [[fx, 0, cx], [0, fy, cy], [0, 0, 1]], dtype=jnp.float32\n    )"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  if xnp is np:\n    return xnp.array([\n        [fx, 0, cx],\n        [0, fy, cy],\n        [0, 0, 1],\n    ])\n  else:\n    return xnp.array([\n        [fx, 0, cx],\n        [0, fy, cy],\n        [0, 0, 1],\n    ],\n                      dtype=xnp.float32)"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  # Adapted from https://github.com/google/nerfies/blob/main/nerfies/camera.py\n  # Construct the intrinsic matrix.\n  return xnp.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  xnp.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]], dtype=xnp.float32)\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  # Adapted from https://github.com/google/nerfies/blob/main/nerfies/camera.py\n  return xnp.array(\n      [\n          [fx, 0, cx],\n          [0, fy, cy],\n          [0, 0, 1],\n      ],\n      dtype=xnp.float32,\n  )\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ])\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  if xnp == np:\n    return xnp.array([\n        [fx, 0, cx],\n        [0, fy, cy],\n        [0, 0, 1],\n    ])\n  else:\n    return xnp.array([\n        [fx, 0, cx],\n        [0, fy, cy],\n        [0, 0, 1],\n    ])\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array(\n      [\n          [fx, 0, cx],\n          [0, fy, cy],\n          [0, 0, 1],\n      ]\n  )\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ])\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array(\n      [\n          [fx, 0, cx],\n          [0, fy, cy],\n          [0, 0, 1],\n      ]\n  )\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  # Adapted from https://github.com/google/nerfies/blob/main/nerfies/camera.py\n  # The intrinsic matrix is a 3x3 matrix that maps from 2D image coordinates to\n  # 3D camera coordinates. It is used to project 3D points into 2D image\n  # coordinates and is essential for camera calibration and 3D reconstruction\n  # tasks.\n  #\n  # The intrinsic matrix is defined as follows:\n  #\n  # [fx 0  cx]\n  # [0  fy cy]\n  # [0  0  1 ]\n  #\n  # where fx and fy are the focal lengths in the x and y directions, respectively,\n  # cx and cy are the coordinates of the principal point, which is the center of\n  # the image, and 1 is the scale factor.\n  #\n  # The principal point is the point in the image where the camera is located.\n  # It is used to align the center of the image with the principal point, which\n  # is the center of projection of the camera.\n  #\n  # The scale factor is a scaling factor that is applied to the image coordinates\n  # to convert them to camera coordinates. It is typically set to 1, but can be\n  # adjusted to account for image distortion or other factors.\n  #\n  # The intrinsic matrix is typically used in conjunction with an extrinsic matrix\n  # to project 3D points into 2D image coordinates. The extrinsic matrix is a\n  # 3x4 matrix that maps from 3D camera coordinates to 3D world coordinates. It\n  # is used to transform the 3D points from the camera coordinate system to the\n  # world coordinate system.\n  #\n  # The intrinsic matrix and extrinsic matrix together are used to project 3D\n  # points into 2D image coordinates and to transform the 3D points from the\n  # camera coordinate system to the world coordinate system, respectively.\n  #\n  # It's worth noting that the intrinsic matrix and extrinsic matrix are\n  # typically estimated from a set of corresponding 2D and 3D points using\n  # camera"}
{"namespace": "coord.contract", "completion": "  z_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = 2 * jnp.sqrt(z_mag_sq) - z_mag_sq\n  z = x / scale\n  return z"}
{"namespace": "coord.contract", "completion": "  z_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = 2 * jnp.sqrt(z_mag_sq) - z_mag_sq\n  z = x / scale\n  return z"}
{"namespace": "coord.contract", "completion": "  z_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = 2 * jnp.sqrt(z_mag_sq) - z_mag_sq\n  z = x / scale\n  return z"}
{"namespace": "coord.contract", "completion": "  z_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = 2 * jnp.sqrt(z_mag_sq) - z_mag_sq\n  z = x / scale\n  return z"}
{"namespace": "coord.contract", "completion": "  z_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = 2 * jnp.sqrt(z_mag_sq) - z_mag_sq\n  z = x / scale\n  return z"}
{"namespace": "coord.contract", "completion": "  z_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = 2 * jnp.sqrt(z_mag_sq) - z_mag_sq\n  z = x / scale\n  return z"}
{"namespace": "coord.contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = 2 * jnp.sqrt(x_mag_sq) - x_mag_sq\n  z = x / scale\n  return z"}
{"namespace": "coord.contract", "completion": "  z_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = 2 * jnp.sqrt(z_mag_sq) - z_mag_sq\n  return x / scale"}
{"namespace": "coord.contract", "completion": "  x_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = 2 * jnp.sqrt(x_mag_sq) - x_mag_sq\n  return x / scale"}
{"namespace": "coord.contract", "completion": "  x_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = 2 * jnp.sqrt(x_mag_sq) - x_mag_sq\n  return x / scale"}
{"namespace": "coord.contract", "completion": "  z_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = 2 * jnp.sqrt(z_mag_sq) - z_mag_sq\n  return x / scale"}
{"namespace": "coord.contract", "completion": "  z_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = 2 * jnp.sqrt(z_mag_sq) - z_mag_sq\n  z = x / scale\n  return z"}
{"namespace": "coord.contract", "completion": "  z_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = jnp.sqrt(z_mag_sq)\n  inv_scale = 2 * scale - z_mag_sq\n  x = x / inv_scale\n  return x"}
{"namespace": "coord.contract", "completion": "  x_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = 2 * jnp.sqrt(x_mag_sq) - x_mag_sq\n  return x / scale"}
{"namespace": "coord.contract", "completion": "  z_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  inv_scale = 2 * jnp.sqrt(z_mag_sq) - z_mag_sq\n  return x / inv_scale"}
{"namespace": "coord.contract", "completion": "  z_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  inv_scale = 2 * jnp.sqrt(z_mag_sq) - z_mag_sq\n  z = x / inv_scale\n  return z"}
{"namespace": "coord.contract", "completion": "  z_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = jnp.sqrt(2 * z_mag_sq) - z_mag_sq\n  return x / scale"}
{"namespace": "coord.contract", "completion": "  z_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = jnp.sqrt(z_mag_sq)\n  # The scale is 0 when the input is the origin, so we clamp it to 1.\n  scale = jnp.maximum(1, scale)\n  x = x / scale\n  return x"}
{"namespace": "coord.contract", "completion": "  z_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = jnp.sqrt(jnp.maximum(1, z_mag_sq))\n  return x / scale\n\n"}
{"namespace": "coord.contract", "completion": "  x_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  x_mag_sq = jnp.maximum(1, x_mag_sq)\n  x_scale = 2 * jnp.sqrt(x_mag_sq) - x_mag_sq\n  x_scale = jnp.where(\n      x_scale < 0, jnp.zeros_like(x_scale), x_scale\n  )  # Clamping to 0 produces correct scale inside |x| < 1\n  x_scale = jnp.where(\n      x_scale == 0, jnp.ones_like(x_scale), x_scale\n  )  # Avoid dividing by zero\n  x = x / x_scale\n  return x"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    "}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    "}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    "}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    \"\"\"\n    Converts a number representing bytes into a human-readable format with appropriate units (B, KB, MB, GB, TB, PB). It scales the number down by 1000 and changes units from bytes to petabytes as necessary until a human-friendly format is achieved.\n    Input-Output Arguments\n    :param num_bytes: Float. The number of bytes to be converted into a human-readable format.\n    :return: String. The number of bytes converted into a human-readable format with the appropriate unit.\n    \"\"\"\n    \"\"\"\n    Converts a number representing bytes into a human-readable format with appropriate units (B, KB, MB, GB, TB, PB). It scales the number down by 1000 and changes units from bytes to petabytes as necessary until a human-friendly format is achieved.\n    Input-Output Arguments\n    :param num_bytes: Float. The number of bytes to be converted into a human-readable format.\n    :return: String. The number of bytes converted into a human-readable format with the appropriate unit.\n    \"\"\"\n    \"\"\"\n    Converts a number representing bytes into a human-readable format with appropriate units (B, KB, MB, GB, TB, PB). It scales the number down by 1000 and changes units from bytes to petabytes as necessary until a human-friendly format is achieved.\n    Input-Output Arguments\n    :param num_bytes: Float. The number of bytes to be converted into a human-readable format.\n    :return: String. The number of bytes converted into a human-readable format with the appropriate unit.\n    \"\"\"\n    \"\"\"\n    Converts a number representing bytes into a human-readable format with appropriate units (B, KB, MB, GB, TB, PB). It scales the number down by 1000 and changes units from bytes to petabytes as necessary until a human-friendly format is achieved.\n    Input-Output Arguments\n    :param num_bytes: Float. The number of bytes to be converted into a human-readable format.\n    :return: String. The number of bytes converted into a human-readable format"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0 B\"\n    for suffix in _FORMAT_TO_RATIO:\n        if num_bytes >= _FORMAT_TO_RATIO[suffix]:\n            return f\"{num_bytes / _FORMAT_TO_RATIO[suffix]:.2f} {suffix}\"\n    return f\"{num_bytes} B\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    num_bytes = float(num_bytes)\n    "}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0 B\"\n    for unit, ratio in _FORMAT_TO_RATIO.items():\n        if num_bytes >= ratio:\n            num_bytes /= ratio\n        else:\n            return f\"{num_bytes:.2f} {unit}\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0 B\"\n\n    suffix_index = 0\n    while num_bytes >= 1000 and suffix_index < len(_FORMAT_TO_RATIO) - 1:\n        num_bytes /= 1000.0\n        suffix_index += 1\n\n    return f\"{num_bytes:.2f} {list(_FORMAT_TO_RATIO.keys())[suffix_index]}\".upper()"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0 B\"\n    i = int(floor(log(num_bytes, 1000)))\n    p = pow(1000, i)\n    s = round(num_bytes / p, 2)\n    return f\"{s} {_FORMAT_TO_RATIO.keys()[i]}\".replace(\".0\", \"\")"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0B\"\n    num_bytes = float(num_bytes)\n    unit_list = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    i = 0\n    while num_bytes >= 1000 and i < len(unit_list) - 1:\n        num_bytes /= 1000.0\n        i += 1\n    return f\"{num_bytes:.2f}{unit_list[i]}\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0 B\"\n    elif num_bytes < 1000:\n        return f\"{num_bytes} B\"\n    elif num_bytes < 1000**2:\n        return f\"{num_bytes / 1000:.2f} KB\"\n    elif num_bytes < 1000**3:\n        return f\"{num_bytes / 1000**2:.2f} MB\"\n    elif num_bytes < 1000**4:\n        return f\"{num_bytes / 1000**3:.2f} GB\"\n    elif num_bytes < 1000**5:\n        return f\"{num_bytes / 1000**4:.2f} TB\"\n    elif num_bytes < 1000**6:\n        return f\"{num_bytes / 1000**5:.2f} PB\"\n    else:\n        return f\"{num_bytes / 1000**6:.2f} EB\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0 B\"\n    i = int(floor(log(num_bytes, 1000)))\n    p = pow(1000, i)\n    s = round(num_bytes / p, 2)\n    return f\"{s} {_UNITS[i]}\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0 B\"\n\n    i = 0\n    while num_bytes >= 1000:\n        num_bytes /= 1000.0\n        i += 1\n\n    return f\"{num_bytes:.2f} {_UNITS[i]}B\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0 B\"\n    num_bytes = float(num_bytes)\n    unit_list = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    idx = 0\n    while num_bytes >= 1000 and idx < len(unit_list) - 1:\n        num_bytes = num_bytes / 1000.0\n        idx += 1\n    return f\"{num_bytes:.2f} {unit_list[idx]}\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    num_bytes = float(num_bytes)\n    if num_bytes < 0:\n        raise ValueError(\"!!!number of bytes can't be negative\")\n    if num_bytes == 0:\n        return \"0B\"\n    suffix_index = 0\n    while num_bytes >= 1000 and suffix_index < 5:\n        num_bytes /= 1000.0\n        suffix_index += 1\n    return f\"{num_bytes:.1f}{_SUFFIXES[suffix_index]}\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    # Check if the input is a number\n    if not isinstance(num_bytes, (int, float)):\n        raise TypeError(\"Input must be a number.\")\n\n    # Check if the input is positive\n    if num_bytes <= 0:\n        raise ValueError(\"Input must be a positive number.\")\n\n    # Convert the number of bytes to a human-readable format\n    "}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0 B\"\n    if num_bytes == 1:\n        return \"1 B\"\n    if num_bytes < 0:\n        raise ValueError(\"Bytes must be greater than or equal to 0.\")\n    if num_bytes < 1000:\n        return f\"{num_bytes:.2f} B\"\n    if num_bytes < 1000**2:\n        return f\"{num_bytes / 1000:.2f} KB\"\n    if num_bytes < 1000**3:\n        return f\"{num_bytes / 1000**2:.2f} MB\"\n    if num_bytes < 1000**4:\n        return f\"{num_bytes / 1000**3:.2f} GB\"\n    if num_bytes < 1000**5:\n        return f\"{num_bytes / 1000**4:.2f} TB\"\n    if num_bytes < 1000**6:\n        return f\"{num_bytes / 1000**5:.2f} PB\"\n    return f\"{num_bytes / 1000**6:.2f} EB\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes < 0:\n        raise ValueError(\"The value of bytes must be positive.\")\n    if num_bytes == 0:\n        return \"0 B\"\n    if num_bytes == 1:\n        return \"1 B\"\n    for suffix in _FORMAT_TO_RATIO:\n        if num_bytes < _FORMAT_TO_RATIO[suffix]:\n            return f\"{num_bytes / _FORMAT_TO_RATIO[suffix]:.2f} {suffix}\"\n    return f\"{num_bytes / _FORMAT_TO_RATIO['tb']:.2f} tb\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    # The following code is adapted from https://stackoverflow.com/questions/1094841/reusable-library-to-get-human-readable-version-of-file-size\n\n    if num_bytes == 0:\n        return \"0 B\"\n    if num_bytes < 0:\n        raise ValueError(\"!!!num_bytes should be non-negative.\")\n    suffix_index = 0\n    while num_bytes >= 1000 and suffix_index < len(SUFFIXES) - 1:\n        num_bytes /= 1000.0\n        suffix_index += 1\n    return f\"{num_bytes:.1f} {SUFFIXES[suffix_index]}\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    # If the number is less than 1024, return the number in bytes\n    if num_bytes < 1024:\n        return f\"{num_bytes}B\"\n\n    # If the number is greater than 1024, scale the number down by 1000 and change units from bytes to petabytes as necessary\n    for unit in _FORMAT_TO_RATIO:\n        if num_bytes < 1000 * _FORMAT_TO_RATIO[unit]:\n            return f\"{num_bytes / _FORMAT_TO_RATIO[unit]:.2f}{unit}\"\n\n    # If the number is greater than 1024^5, return the number in petabytes\n    return f\"{num_bytes / 1000**4:.2f}PB\""}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. got {len(v.shape)}\"\n            )\n\n        return v\n\n    return __validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. Got {len(v.shape)} dimensions.\"\n            )\n\n        return v\n\n    return __validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check that the array has the specified number of dimensions.\"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. got {len(v.shape)}\"\n            )\n\n        return v\n\n    return __validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. \"\n                f\"Received {len(v.shape)} dimensions.\"\n            )\n        return v\n\n    return __validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. \"\n                f\"got {len(v.shape)} dimensions.\"\n            )\n\n        return v\n\n    return __validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check that the array has a specific number of dimensions.\"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. got {len(v.shape)}\"\n            )\n\n        return v\n\n    return __validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check that the array has the specified number of dimensions.\"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. Got {len(v.shape)}.\"\n            )\n\n        return v\n\n    return __validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check that the array has the specified number of dimensions.\"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions, got {len(v.shape)}\"\n            )\n\n        return v\n\n    return __validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. got {len(v.shape)} dimensions.\"\n            )\n\n        return v\n\n    return __validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check that the array has the specified number of dimensions.\"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. \"\n                f\"Received {len(v.shape)} dimensions.\"\n            )\n        return v\n\n    return __validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions.\")\n\n        return v\n\n    return __validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check if array has a specific number of dimensions.\"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. got {len(v.shape)}\"\n            )\n\n        return v\n\n    return __validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check if array has nb_dimensions dimensions.\"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. \"\n                f\"got {len(v.shape)} dimensions.\"\n            )\n\n        return v\n\n    return __validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check that the array has the specified number of dimensions.\"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. got {len(v.shape)}\"\n            )\n        return v\n\n    return __validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be {nb_dimensions}D.\")\n\n        return v\n\n    return __validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check that array has nb_dimensions dimensions.\"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. Got {len(v.shape)} dimensions.\"\n            )\n\n        return v\n\n    return __validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check if the array has the specified number of dimensions.\"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. Got {len(v.shape)}\"\n            )\n        return v\n\n    return __validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check if array has `nb_dimensions` dimensions.\"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. got {len(v.shape)}\"\n            )\n\n        return v\n\n    return __validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check if the array has the specified number of dimensions.\"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. \"\n                f\"Received {len(v.shape)} dimensions.\"\n            )\n\n        return v\n\n    return __validator\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"\n        This function is the validator itself. It checks if the array has the specified number of dimensions and returns the array if it does, raising a ValueError if it does not.\n\n        Input-Output Arguments\n        :param cls: Type. The class of the model being validated.\n        :param v: np.ndarray. The array being validated.\n        :param field: fields.ModelField. The field of the model that is being validated.\n        :return: np.ndarray. The validated array if it meets the specified number of dimensions.\n        \"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. Got {len(v.shape)}.\"\n            )\n\n        return v\n\n    return __validator\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(cartesian_vector[..., 2] / (r + eps))\n  phi = jnp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n  return r, theta, phi"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(cartesian_vector[..., 2] / (r + eps))\n  phi = jnp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n  return r, theta, phi"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(cartesian_vector[..., 2] / (r + eps))\n  phi = jnp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n  return r, theta, phi"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  r = jnp.linalg.norm(cartesian_vector, axis=-1)\n  theta = jnp.arccos(cartesian_vector[..., 2] / (r + eps))\n  phi = jnp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n  return r, theta, phi"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  r = jnp.linalg.norm(cartesian_vector, axis=-1)\n  theta = jnp.arccos(cartesian_vector[..., 2] / (r + eps))\n  phi = jnp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n  return r, theta, phi"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = jnp.sqrt(x**2 + y**2 + z**2)\n  theta = jnp.arccos(z / (r + eps))\n  phi = jnp.arctan2(y, x)\n  return r, theta, phi"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  r = jnp.linalg.norm(cartesian_vector, axis=-1)\n  theta = jnp.arccos(cartesian_vector[..., 2] / (r + eps))\n  phi = jnp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n\n  return r, theta, phi"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = jnp.split(cartesian_vector, 3, axis=-1)\n  r = jnp.sqrt(x**2 + y**2 + z**2)\n  theta = jnp.arccos(z / (r + eps))\n  phi = jnp.arctan2(y, x)\n  return r, theta, phi"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(cartesian_vector[..., 2] / (r + eps))\n  phi = jnp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n\n  return r, theta, phi"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(cartesian_vector[..., 2] / (r + eps))\n  phi = jnp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n\n  return r, theta, phi"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = jnp.sqrt(x**2 + y**2 + z**2)\n  theta = jnp.arccos(z / (r + eps))\n  phi = jnp.arctan2(y, x)\n\n  return r, theta, phi"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = jnp.split(cartesian_vector, 3, axis=-1)\n  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(z / (r + eps))\n  phi = jnp.arctan2(y, x)\n  return r, theta, phi"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  r = jnp.linalg.norm(cartesian_vector, axis=-1)\n  theta = jnp.arccos(cartesian_vector[..., 2] / jnp.maximum(r, eps))\n  phi = jnp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n  return r, theta, phi"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(cartesian_vector[..., 2] / jnp.maximum(r, eps))\n  phi = jnp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n\n  return r, theta, phi"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector.T\n  r = jnp.sqrt(x**2 + y**2 + z**2)\n  theta = jnp.arccos(z / (r + eps))\n  phi = jnp.arctan2(y, x)\n\n  return r, theta, phi"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = jnp.linalg.norm(cartesian_vector, axis=-1)\n  theta = jnp.arccos(z / (r + eps))\n  phi = jnp.arctan2(y, x)\n\n  return r, theta, phi"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  r = jnp.linalg.norm(cartesian_vector, axis=-1)\n  theta = jnp.arccos(cartesian_vector[..., 2] / (r + eps))\n  phi = jnp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  r = jnp.sqrt(x**2 + y**2 + z**2)\n  theta = jnp.arccos(z / (r + eps))\n  phi = jnp.arctan2(y, x)\n\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Normalize the input vector to get x, y, z coordinates.\n  cartesian_vector = spin_math.normalize(cartesian_vector)\n\n  # Calculate radius (r) using the Pythagorean theorem.\n  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n\n  # Calculate inclination (theta) using arccos.\n  theta = jnp.arccos(cartesian_vector[..., 2] / jnp.maximum(r, eps))\n\n  # Calculate azimuth (phi) using arctan2.\n  phi = jnp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n\n  # Return spherical coordinates as a tuple.\n  return r, theta, phi"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  r = jnp.linalg.norm(cartesian_vector, axis=-1)\n  theta = jnp.arccos(cartesian_vector[..., 2] / (r + eps))\n  phi = jnp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n\n  return r, theta, phi\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    return results['rougeL']"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    return results['rougeL']"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    return results['rougeL']"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['rougeL']\n    return score"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['rougeL']\n    return score"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['rougeL']\n    return score"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['rougeL']\n    return score"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['rougeL']\n    return score"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['rougeL']\n    return score"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['rougeL']\n    return score"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['rougeL']\n    return score"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['rougeL']\n    return score"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['rougeL']\n    return score"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['rougeL']\n    return score"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['rougeL']\n    return score"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['rougeL']\n    return score"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    return results['rougeL']"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    return results['rougeL']"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['rougeL']\n    return score\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['rougeL']\n    return score\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # First, try to locate the object using the standard method.\n    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    # If the object cannot be located using the standard method, try to locate it using a fallback method.\n    try:\n        return pydoc.resolve(name)\n    except ImportError:\n        pass\n\n    # If the object cannot be located using either method, raise an exception.\n    raise ImportError(f\"Failed to locate object '{name}'\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # First, try to locate the object using the standard method.\n    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    # If the object cannot be located using the standard method, try to locate it using a fallback method.\n    try:\n        return pydoc.resolve(name)\n    except ImportError:\n        pass\n\n    # If the object cannot be located using either method, raise an exception.\n    raise ImportError(f\"Failed to locate object '{name}'\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return pydoc.locate(name)\n    except (ImportError, AttributeError):\n        pass\n\n    "}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # Try the standard method first\n    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    # Try the fallback method\n    try:\n        module_name, class_name = name.rsplit(\".\", 1)\n        module = pydoc.locate(module_name)\n        return getattr(module, class_name)\n    except ImportError:\n        pass\n\n    raise ImportError(f\"Failed to locate {name}\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n    try:\n        return pydoc.resolve(name)\n    except ImportError:\n        pass\n    raise ImportError(f\"Failed to locate object: {name}\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # Try standard method\n    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    # Try fallback method\n    try:\n        module_name, class_name = name.rsplit(\".\", 1)\n        module = pydoc.locate(module_name)\n        return getattr(module, class_name)\n    except ImportError:\n        pass\n\n    raise ImportError(f\"Failed to locate {name}\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # First try the standard method.\n    try:\n        return pydoc.locate(name)\n    except Exception:\n        pass\n\n    # Then try the fallback method.\n    try:\n        module_name, class_name = name.rsplit(\".\", 1)\n        module = pydoc.locate(module_name)\n        return getattr(module, class_name)\n    except Exception:\n        pass\n\n    raise ImportError(f\"Failed to locate {name}\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # First, try to locate the object using the standard method.\n    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    # If the object cannot be located using the standard method, try to locate it using a fallback method.\n    try:\n        return Registry._get_locator(name)\n    except KeyError:\n        pass\n\n    # If the object cannot be located using either method, raise an exception.\n    raise ImportError(f\"Unable to locate object '{name}'\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # Try to locate the object using the standard method.\n    try:\n        return pydoc.locate(name)\n    except ImportError as e:\n        pass\n\n    # If the object cannot be located using the standard method, try to locate it using a fallback method.\n    try:\n        module_name, obj_name = name.rsplit(\".\", 1)\n        module = pydoc.locate(module_name)\n        return getattr(module, obj_name)\n    except (ImportError, AttributeError) as e:\n        pass\n\n    # If the object cannot be located using either method, raise an exception.\n    raise ImportError(f\"Failed to locate object: {name}\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # First try the standard method.\n    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    # Fallback to a custom method.\n    try:\n        module_name, class_name = name.rsplit(\".\", 1)\n        module = pydoc.locate(module_name)\n        return getattr(module, class_name)\n    except (ImportError, AttributeError):\n        pass\n\n    raise ImportError(f\"Failed to locate {name}\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return pydoc.locate(name)\n    except ImportError:\n        # Fallback to importing the object directly\n        module, cls_name = name.rsplit(\".\", 1)\n        module = __import__(module, fromlist=[cls_name])\n        return getattr(module, cls_name)"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    \"\"\"\n    Locates and returns a Python object based on its fully qualified name given as a string. If the object cannot be found using the standard method, it attempts to locate the object using a fallback method. An exception is raised if the object cannot be located by either method.\n\n    Input-Output Arguments\n    :param name: str, The fully qualified name of the object to locate, in the format \"module.submodule.class_name\".\n    :return: Any. The located Python object.\n    \"\"\"\n\n    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    # Fallback to the old way\n    module, qualname = name.rsplit(\".\", 1)\n    try:\n        return pydoc.locate(f\"{module}.{qualname}\")\n    except ImportError:\n        pass\n\n    raise ImportError(f\"Failed to locate {name}\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # First try standard method\n    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    # Next try fallback method\n    try:\n        module_name, class_name = name.rsplit(\".\", 1)\n        module = pydoc.locate(module_name)\n        return getattr(module, class_name)\n    except (ImportError, AttributeError):\n        pass\n\n    # Finally raise exception\n    raise ImportError(f\"Cannot locate {name}\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    # If the object is not found using the standard method, try to locate it using a fallback method.\n    # The fallback method is to split the name into module and class name, and then import the module and return the class.\n    # This is useful for locating classes that are defined in the same module as the calling code.\n    module_name, class_name = name.rsplit(\".\", 1)\n    module = pydoc.locate(module_name)\n    if module is None:\n        raise ImportError(f\"Failed to locate module {module_name}\")\n    return getattr(module, class_name)"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # Try importing the module.\n    try:\n        obj = pydoc.locate(name)\n        return obj\n    except ImportError:\n        pass\n\n    # Try importing the module with a fallback method.\n    try:\n        obj = pydoc.locate(name)\n        return obj\n    except ImportError:\n        pass\n\n    # Try importing the module with a fallback method.\n    try:\n        obj = pydoc.locate(name)\n        return obj\n    except ImportError:\n        pass\n\n    raise ImportError(f\"Failed to locate object '{name}'\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    \"\"\"\n    Locates a Python object based on its fully qualified name given as a string.\n\n    Input-Output Arguments\n    :param name: str, The fully qualified name of the object to locate, in the format \"module.submodule.class_name\".\n    :return: Any. The located Python object.\n    \"\"\"\n    try:\n        return pydoc.locate(name)\n    except Exception:\n        # If the object cannot be located using the standard method, attempt to locate the object using a fallback method.\n        # If the object cannot be located using either method, raise an exception.\n        pass\n\n    # Split the fully qualified name into module and class name\n    module_name, class_name = name.rsplit(\".\", 1)\n\n    # Import the module and return the class\n    module = importlib.import_module(module_name)\n    return getattr(module, class_name)"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    # Fallback method: locate by splitting the name into module and class name,\n    # and then importing the module and getting the class from the module.\n    # This is useful for classes that are defined in the same file as the registry.\n    module_name, class_name = name.rsplit(\".\", 1)\n    try:\n        module = pydoc.locate(module_name)\n        return getattr(module, class_name)\n    except ImportError:\n        pass\n\n    raise ImportError(f\"Failed to locate {name}\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    if name.startswith(\"fvcore.common.registry.\"):\n        name = name[len(\"fvcore.common.registry.\") :]\n\n    if \".\" not in name:\n        raise ImportError(f\"Cannot locate {name}. It is not a fully qualified name.\")\n\n    module, qualname = name.rsplit(\".\", 1)\n    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    try:\n        module = pydoc.locate(module)\n    except ImportError:\n        raise ImportError(f\"Cannot locate {name}. Failed to locate module {module}.\")\n\n    try:\n        return getattr(module, qualname)\n    except AttributeError:\n        raise ImportError(f\"Cannot locate {name}. Failed to locate {qualname} in module {module}.\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # The following code is adapted from\n    # https://github.com/facebookresearch/detectron2/blob/main/detectron2/utils/imports.py\n    # with some changes.\n\n    # First, try the standard method.\n    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    # If the object is not found using the standard method, try the fallback method.\n    # The fallback method is to import the module and get the object from the module.\n    # The fallback method is used when the object is not found using the standard method.\n    # The fallback method is used when the object is not found using the standard method.\n    # The fallback method is used when the object is not found using the standard method.\n    # The fallback method is used when the object is not found using the standard method.\n    # The fallback method is used when the object is not found using the standard method.\n    # The fallback method is used when the object is not found using the standard method.\n    # The fallback method is used when the object is not found using the standard method.\n    # The fallback method is used when the object is not found using the standard method.\n    # The fallback method is used when the object is not found using the standard method.\n    # The fallback method is used when the object is not found using the standard method.\n    # The fallback method is used when the object is not found using the standard method.\n    # The fallback method is used when the object is not found using the standard method.\n    # The fallback method is used when the object is not found using the standard method.\n    # The fallback method is used when the object is not found using the standard method.\n    # The fallback method is used when the object is not found using the standard method.\n    # The fallback method is used when the object is not found using the standard method.\n    # The fallback method is used when the object is not found using the standard method.\n    # The fallback method is used when the object is not found using the standard method.\n    # The fallback method is used when the object is not"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # First, try to locate the object using the standard method.\n    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    # If the object cannot be located using the standard method, try to locate the object using a fallback method.\n    # The fallback method is to use the ``__import__`` function to import the module and then get the object from the module using the ``getattr`` function.\n    # The fallback method is used if the standard method fails to locate the object because the object is not in the standard Python library.\n    # The fallback method is also used if the standard method fails to locate the object because the object is in the standard Python library but the import path is not correct.\n    # The fallback method is used if the standard method fails to locate the object because the object is in a third-party library.\n    # The fallback method is also used if the standard method fails to locate the object because the object is in a third-party library but the import path is not correct.\n    # The fallback method is used if the standard method fails to locate the object because the object is in a local module.\n    # The fallback method is also used if the standard method fails to locate the object because the object is in a local module but the import path is not correct.\n    # The fallback method is used if the standard method fails to locate the object because the object is in a local module and the import path is correct.\n    # The fallback method is also used if the standard method fails to locate the object because the object is in a local module and the import path is correct but the object is not in the module.\n    # The fallback method is used if the standard method fails to locate the object because the object is in a local module and the import path is correct but the object is in the module but the object is not in the module because the module is not in the Python path.\n    # The fallback method is also used if the standard method fails to locate the object because the object is in a local module and the import path is correct but the object is in the module but the object is not in the module because the module is not in the Python path and the object is not in the module.\n    # The fall"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buf = io.BytesIO()\n    torch.jit.save(module, buf)\n    buf.seek(0)\n    return torch.jit.load(buf)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buf = io.BytesIO()\n    torch.jit.save(module, buf)\n    buf.seek(0)\n    return torch.jit.load(buf)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    "}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    "}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buf = io.BytesIO()\n    torch.jit.save(module, buf)\n    buf.seek(0)\n    return torch.jit.load(buf)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buf = io.BytesIO()\n    torch.jit.save(module, buf)\n    buf.seek(0)\n    return torch.jit.load(buf)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    module = torch.jit.load(buffer)\n    return module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    reloaded_module = torch.jit.load(buffer)\n    return reloaded_module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the module to an in-memory buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Load the module from the in-memory buffer\n    buffer.seek(0)\n    module = torch.jit.load(buffer)\n\n    return module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buf = io.BytesIO()\n    torch.jit.save(module, buf)\n    buf.seek(0)\n    return torch.jit.load(buf)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    reloaded_module = torch.jit.load(buffer)\n    return reloaded_module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buf = io.BytesIO()\n    torch.jit.save(module, buf)\n    buf.seek(0)\n    module = torch.jit.load(buf)\n    return module"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the script module to an in-memory buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Load the script module from the in-memory buffer\n    buffer.seek(0)\n    reloaded_module = torch.jit.load(buffer)\n\n    return reloaded_module"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    from autorag.nodes.retrieval.hybrid.cc_pure import cc_pure"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    cc_ids, cc_scores = cc_pure(ids, scores, weights, top_k)\n    return cc_ids, cc_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    df = pd.concat([pd.Series(dict(zip(_id, score))) for _id, score in zip(ids, scores)], axis=1)\n    normalized_scores = (df - df.min()) / (df.max() - df.min())\n    normalized_scores = normalized_scores.fillna(0)\n    normalized_scores['weighted_sum'] = normalized_scores.mul(weights).sum(axis=1)\n    normalized_scores = normalized_scores.sort_values(by='weighted_sum', ascending=False)\n    return normalized_scores.index.tolist()[:top_k], normalized_scores['weighted_sum'][:top_k].tolist()"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    ids, scores = zip(*[(ids[i], scores[i]) for i in range(len(ids))])\n    ids, scores = cc_pure(ids, scores, weights, top_k)\n    return ids, scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    from autorag.nodes.retrieval.convex_combination.pure import cc_pure\n    return cc_pure(ids, scores, weights, top_k)"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    ids = [ids[i][:top_k] for i in range(len(ids))]\n    scores = [scores[i][:top_k] for i in range(len(scores))]\n    return cc_pure(ids, scores, weights, top_k)"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    ids_ = [ids[i] for i in range(len(ids))]\n    scores_ = [scores[i] for i in range(len(scores))]\n    return cc_pure(ids_, scores_, weights, top_k)"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    ids = [ids[i] for i in range(len(ids))]\n    scores = [scores[i] for i in range(len(scores))]\n    weights = [weights[i] for i in range(len(weights))]\n    return cc_pure(ids, scores, weights, top_k)"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    from .cc_pure import cc_pure\n\n    ids_fused, scores_fused = cc_pure(ids, scores, weights, top_k)\n\n    return ids_fused, scores_fused"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    ids_list, scores_list = [], []\n    for _id, score in zip(ids, scores):\n        ids_list.append(_id)\n        scores_list.append(score)\n\n    ids_list = [ids_list[i][:top_k] for i in range(len(ids_list))]\n    scores_list = [scores_list[i][:top_k] for i in range(len(scores_list))]\n\n    return cc_pure(ids_list, scores_list, weights, top_k)"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    assert len(ids) == len(scores) == len(weights)\n    assert sum(weights) == 1\n    ids_fused = []\n    scores_fused = []\n    for i in range(len(ids)):\n        ids_fused.extend(ids[i][:top_k])\n        scores_fused.extend(scores[i][:top_k])\n    df = pd.concat([pd.Series(dict(zip(_id, score))) for _id, score in zip(ids_fused, scores_fused)], axis=1)\n    normalized_scores = (df - df.min()) / (df.max() - df.min())\n    normalized_scores = normalized_scores.fillna(0)\n    normalized_scores['weighted_sum'] = normalized_scores.mul(weights).sum(axis=1)\n    normalized_scores = normalized_scores.sort_values(by='weighted_sum', ascending=False)\n    return normalized_scores.index.tolist()[:top_k], normalized_scores['weighted_sum'][:top_k].tolist()"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    ids = list(ids)\n    scores = list(scores)\n    weights = list(weights)\n\n    assert len(ids) == len(scores) == len(weights)\n\n    if len(ids) == 1:\n        return ids[0], scores[0]\n\n    if len(ids) == 2:\n        return cc_pure(ids[0], scores[0], weights[0], top_k), cc_pure(ids[1], scores[1], weights[1], top_k)\n\n    return cc_pure(ids, scores, weights, top_k)"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    assert len(ids) == len(scores) == len(weights), 'The length of ids, scores, and weights must be the same.'\n\n    fused_ids = []\n    fused_scores = []\n\n    for i in range(len(ids)):\n        _ids, _scores = cc_pure(ids[i], scores[i], weights[i], top_k)\n        fused_ids.extend(_ids)\n        fused_scores.extend(_scores)\n\n    return fused_ids, fused_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    assert len(ids) == len(scores) == len(weights)\n    assert sum(weights) == 1\n    ids, scores = zip(*[(list(id), list(score)) for id, score in zip(ids, scores)])\n    fused_ids, fused_scores = cc_pure(ids, scores, weights, top_k)\n    return fused_ids, fused_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    ids, scores = zip(*[(ids[i], scores[i]) for i in range(len(ids))])\n    ids, scores = zip(*sorted(zip(ids, scores), key=lambda x: x[1], reverse=True))\n    ids, scores = zip(*[(ids[i], scores[i]) for i in range(len(ids))])\n\n    ids = [ids[i][:top_k] for i in range(len(ids))]\n    scores = [scores[i][:top_k] for i in range(len(scores))]\n\n    ids, scores = zip(*[(ids[i], scores[i]) for i in range(len(ids))])\n\n    ids, scores = cc_pure(ids, scores, weights, top_k)\n\n    return ids, scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    assert len(ids) == len(scores) == len(weights), \"The number of ids, scores, and weights must match.\"\n\n    ids_fused = []\n    scores_fused = []\n\n    for i in range(len(ids)):\n        ids_fused += ids[i]\n        scores_fused += scores[i]\n\n    df = pd.concat([pd.Series(dict(zip(_id, score))) for _id, score in zip(ids_fused, scores_fused)], axis=1)\n    normalized_scores = (df - df.min()) / (df.max() - df.min())\n    normalized_scores = normalized_scores.fillna(0)\n    normalized_scores['weighted_sum'] = normalized_scores.mul(weights).sum(axis=1)\n    normalized_scores = normalized_scores.sort_values(by='weighted_sum', ascending=False)\n\n    return normalized_scores.index.tolist()[:top_k], normalized_scores['weighted_sum'][:top_k].tolist()"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    ids, scores = zip(*[(ids[i], scores[i]) for i in range(len(ids))])\n\n    if len(ids) != len(scores):\n        raise ValueError(\"The number of ids and scores must match.\")\n\n    if len(ids) != len(weights):\n        raise ValueError(\"The number of ids and weights must match.\")\n\n    if len(ids) == 1:\n        return ids[0][:top_k], scores[0][:top_k]\n\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of the weights must equal 1.\")\n\n    return cc_pure(ids, scores, weights, top_k)"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    assert len(ids) == len(scores) == len(weights), \"The ids, scores and weights tuples must have the same length\"\n    assert sum(weights) == 1, \"The sum of the weights must equal 1\"\n\n    ids_list = []\n    scores_list = []\n    for _id, score, weight in zip(ids, scores, weights):\n        ids_list.append(_id)\n        scores_list.append([s * weight for s in score])\n\n    ids_list = [id_ for sublist in ids_list for id_ in sublist]\n    scores_list = [score for sublist in scores_list for score in sublist]\n\n    df = pd.DataFrame({'id': ids_list, 'score': scores_list})\n    df = df.groupby('id').sum().sort_values(by='score', ascending=False).reset_index()\n    return df['id'].tolist()[:top_k], df['score'].tolist()[:top_k]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    ids = list(ids)\n    scores = list(scores)\n    weights = list(weights)\n\n    if len(ids) != len(scores) or len(ids) != len(weights):\n        raise ValueError(\n            f\"The lengths of the ids, scores and weights tuples must be the same. Got {len(ids)}, {len(scores)} and {len(weights)}.\")\n\n    if len(ids) == 0:\n        raise ValueError(\"The ids, scores and weights tuples cannot be empty.\")\n\n    if sum(weights) != 1:\n        raise ValueError(f\"The sum of the weights must be 1. Got {sum(weights)}.\")\n\n    if top_k <= 0:\n        raise ValueError(f\"The top_k parameter must be greater than 0. Got {top_k}.\")\n\n    if top_k > len(ids[0]):\n        raise ValueError(f\"The top_k parameter cannot be greater than the number of ids in the first retrieval result. Got {top_k}.\")\n\n    return cc_pure(ids, scores, weights, top_k)"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check that the input arguments are valid\n    if len(ids) != len(scores) != len(weights):\n        raise ValueError(\"The number of ids, scores, and weights must be the same.\")\n\n    # Calculate the normalized scores for each retrieval result\n    normalized_scores = []\n    for _id, score in zip(ids, scores):\n        normalized_scores.append(pd.Series(dict(zip(_id, score))).fillna(0) / sum(score))\n\n    # Combine the normalized scores using a convex combination method\n    fused_scores = pd.concat(normalized_scores, axis=1).mul(weights).sum(axis=1)\n\n    # Select the top_k results\n    top_k_indices = fused_scores.nlargest(top_k).index\n    return [ids[i][top_k_indices] for i in range(len(ids))], [scores[i][top_k_indices] for i in range(len(scores))]"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    if percent:\n        x *= 100\n        return f\"{x:,.2f}%\"\n    return f\"{x:,.2f}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if x >= 1000:\n        return f\"{x:.0f}%\"\n    if x >= 100:\n        return f\"{x:.1f}%\"\n    if x >= 10:\n        return f\"{x:.2f}%\"\n    if x >= 1:\n        return f\"{x:.3f}%\"\n    if x >= 0.1:\n        return f\"{x:.4f}%\"\n    if x >= 0.01:\n        return f\"{x:.5f}%\"\n    if x >= 0.001:\n        return f\"{x:.6f}%\"\n    return f\"{x:.7f}%\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    if percent:\n        x *= 100\n        return f\"{x:.2f}%\"\n    if x < 1e-4:\n        return f\"{x:.4e}\"\n    if x < 1e-2:\n        return f\"{x:.2e}\"\n    if x < 1:\n        return f\"{x:.3f}\"\n    if x < 10:\n        return f\"{x:.2f}\"\n    if x < 100:\n        return f\"{x:.1f}\"\n    return f\"{x:.0f}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    if percent:\n        x *= 100\n    if x < 1e-2:\n        return f\"{x:.2e}\"\n    if x < 1e-1:\n        return f\"{x:.3e}\"\n    if x < 1e0:\n        return f\"{x:.4e}\"\n    if x < 1e1:\n        return f\"{x:.5e}\"\n    if x < 1e2:\n        return f\"{x:.6e}\"\n    if x < 1e3:\n        return f\"{x:.7e}\"\n    if x < 1e4:\n        return f\"{x:.8e}\"\n    if x < 1e5:\n        return f\"{x:.9e}\"\n    if x < 1e6:\n        return f\"{x:.10e}\"\n    if x < 1e7:\n        return f\"{x:.11e}\"\n    if x < 1e8:\n        return f\"{x:.12e}\"\n    if x < 1e9:\n        return f\"{x:.13e}\"\n    if x < 1e10:\n        return f\"{x:.14e}\"\n    if x < 1e11:\n        return f\"{x:.15e}\"\n    if x < 1e12:\n        return f\"{x:.16e}\"\n    if x < 1e13:\n        return f\"{x:.17e}\"\n    if x < 1e14:\n        return f\"{x:.18e}\"\n    if x < 1e15:\n        return f\"{x:.19e}\"\n    if x < 1e16:\n        return f\"{x:.20e}\"\n    if x < 1e17:\n        return f\"{x:.21e}\"\n    if x < 1e18:\n        return f\"{x:.22e}\"\n    if x < 1e19:\n        return f\"{x:.23e}\"\n    if x"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    if percent:\n        x *= 100\n    if x >= 1:\n        return f\"{x:.0f}%\"\n    if x >= 0.1:\n        return f\"{x:.1f}%\"\n    if x >= 0.01:\n        return f\"{x:.2f}%\"\n    if x >= 0.001:\n        return f\"{x:.3f}%\"\n    if x >= 0.0001:\n        return f\"{x:.4f}%\"\n    if x >= 0.00001:\n        return f\"{x:.5f}%\"\n    if x >= 0.000001:\n        return f\"{x:.6f}%\"\n    if x >= 0.0000001:\n        return f\"{x:.7f}%\"\n    if x >= 0.00000001:\n        return f\"{x:.8f}%\"\n    return f\"{x:.9f}%\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    if percent:\n        x *= 100\n    if x == 0:\n        return \"0%\"\n    if x < 1:\n        return f\"{x:.2f}%\"\n    if x < 10:\n        return f\"{x:.1f}%\"\n    if x < 100:\n        return f\"{x:.0f}%\"\n    return f\"{x:.0f}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(np.nan)\n    if percent:\n        x *= 100\n        s = \"%\"\n    else:\n        s = \"\"\n    if x < 0.01:\n        return f\"{x:0.2e}{s}\"\n    if x < 10:\n        return f\"{x:0.2f}{s}\"\n    if x < 100:\n        return f\"{x:0.1f}{s}\"\n    return f\"{x:0.0f}{s}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x = x * 100\n    if x < 1e-3:\n        return f\"{x:0.3e}\"\n    if x < 1e-2:\n        return f\"{x:0.2e}\"\n    if x < 1e-1:\n        return f\"{x:0.1e}\"\n    if x < 1:\n        return f\"{x:0.3f}\"\n    if x < 10:\n        return f\"{x:0.2f}\"\n    if x < 100:\n        return f\"{x:0.1f}\"\n    return f\"{x:0.0f}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    if percent:\n        x *= 100\n    if x == 0:\n        return \"0%\"\n    if x < 0:\n        sign = \"-\"\n        x = -x\n    else:\n        sign = \"\"\n    if x < 1:\n        return f\"{sign}{x:.2f}%\"\n    if x < 10:\n        return f\"{sign}{x:.1f}%\"\n    if x < 100:\n        return f\"{sign}{x:.0f}%\"\n    if x < 1000:\n        return f\"{sign}{x:.0f}\"\n    if x < 10000:\n        return f\"{sign}{x:.1f}\"\n    if x < 100000:\n        return f\"{sign}{x:.2f}\"\n    if x < 1000000:\n        return f\"{sign}{x:.3f}\"\n    if x < 10000000:\n        return f\"{sign}{x:.4f}\"\n    if x < 100000000:\n        return f\"{sign}{x:.5f}\"\n    if x < 1000000000:\n        return f\"{sign}{x:.6f}\"\n    return f\"{sign}{x:.7f}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    if percent:\n        x = x * 100\n    if x < 1000:\n        return f\"{x:.2f}\"\n    if x < 1000000:\n        return f\"{x / 1000:.2f}K\"\n    if x < 1000000000:\n        return f\"{x / 1000000:.2f}M\"\n    if x < 1000000000000:\n        return f\"{x / 1000000000:.2f}B\"\n    if x < 1000000000000000:\n        return f\"{x / 1000000000000:.2f}T\"\n    return f\"{x:.2f}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    if percent:\n        x *= 100\n    if x < 0.0001:\n        return f\"{x:.1e}\"\n    if x < 0.001:\n        return f\"{x:.2f}\"\n    if x < 0.01:\n        return f\"{x:.3f}\"\n    if x < 0.1:\n        return f\"{x:.4f}\"\n    if x < 1:\n        return f\"{x:.5f}\"\n    if x < 10:\n        return f\"{x:.6f}\"\n    if x < 100:\n        return f\"{x:.7f}\"\n    return f\"{x:.8f}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    if percent:\n        x *= 100\n    if x < 1e-10:\n        return f\"{x:.1e}\"\n    if x < 1e-3:\n        return f\"{x:.3f}\"\n    if x < 1e-2:\n        return f\"{x:.2f}\"\n    if x < 1e-1:\n        return f\"{x:.1f}\"\n    if x < 1e0:\n        return f\"{x:.0f}\"\n    if x < 1e1:\n        return f\"{x:.1f}\"\n    if x < 1e2:\n        return f\"{x:.2f}\"\n    if x < 1e3:\n        return f\"{x:.3f}\"\n    return f\"{x:.0e}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(np.nan)\n    if percent:\n        x = x * 100\n    if x == 0:\n        return \"0%\"\n    if x < 0.001:\n        return f\"{x:0.3e}\"\n    if x < 0.01:\n        return f\"{x:0.2f}%\"\n    if x < 1:\n        return f\"{x:0.1f}%\"\n    return f\"{x:0.0f}%\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    if percent:\n        x *= 100\n        s = \"%\"\n    else:\n        s = \"\"\n    if x == 0:\n        return \"0\" + s\n    if np.abs(x) < 1e-1:\n        return f\"{x:0.1e}\" + s\n    if np.abs(x) < 1e1:\n        return f\"{x:0.2f}\" + s\n    if np.abs(x) < 1e2:\n        return f\"{x:0.3f}\" + s\n    if np.abs(x) < 1e3:\n        return f\"{x:0.4f}\" + s\n    if np.abs(x) < 1e4:\n        return f\"{x:0.5f}\" + s\n    if np.abs(x) < 1e5:\n        return f\"{x:0.6f}\" + s\n    if np.abs(x) < 1e6:\n        return f\"{x:0.7f}\" + s\n    if np.abs(x) < 1e7:\n        return f\"{x:0.8f}\" + s\n    if np.abs(x) < 1e8:\n        return f\"{x:0.9f}\" + s\n    if np.abs(x) < 1e9:\n        return f\"{x:0.10f}\" + s\n    if np.abs(x) < 1e10:\n        return f\"{x:0.11f}\" + s\n    if np.abs(x) < 1e11:\n        return f\"{x:0.12f}\" + s\n    if np.abs(x) < 1e12:\n        return f\"{x:0.13f}\" + s\n    if np.abs(x) < 1e13:\n        return f\"{x:0.14f}\" + s\n    if np.abs(x) < 1e14:\n        return f\"{x:0"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(np.nan)\n    if percent:\n        x *= 100\n    if x == 0:\n        return \"0%\" if percent else \"0\"\n    if abs(x) < 1e-3:\n        return \"< 0.001%\" if percent else \"< 0.001\"\n    if abs(x) < 1e-2:\n        return \"< 0.01%\" if percent else \"< 0.01\"\n    if abs(x) < 1e-1:\n        return \"< 0.1%\" if percent else \"< 0.1\"\n    if abs(x) < 0.5:\n        return \"< 0.5%\" if percent else \"< 0.5\"\n    if abs(x) < 1:\n        return \"< 1%\" if percent else \"< 1\"\n    if abs(x) < 5:\n        return \"< 5%\" if percent else \"< 5\"\n    if abs(x) < 10:\n        return \"< 10%\" if percent else \"< 10\"\n    if abs(x) < 20:\n        return \"< 20%\" if percent else \"< 20\"\n    if abs(x) < 50:\n        return \"< 50%\" if percent else \"< 50\"\n    if abs(x) < 100:\n        return \"< 100%\" if percent else \"< 100\"\n    if abs(x) < 200:\n        return \"< 200%\" if percent else \"< 200\"\n    if abs(x) < 500:\n        return \"< 500%\" if percent else \"< 500\"\n    if abs(x) < 1000:\n        return \"< 1000%\" if percent else \"< 1000\"\n    if abs(x) < 2000:\n        return \"< 2000%\" if percent else \"< 2000\"\n    if abs(x)"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(np.nan)\n\n    if percent:\n        x *= 100\n\n    if x < 1e-5:\n        return f\"{x:.0e}\"\n\n    if x < 1e-2:\n        return f\"{x:.2f}\"\n\n    if x < 1:\n        return f\"{x:.3f}\"\n\n    if x < 10:\n        return f\"{x:.2f}\"\n\n    if x < 100:\n        return f\"{x:.1f}\"\n\n    if x < 1000:\n        return f\"{x:.0f}\"\n\n    if x < 1e5:\n        return f\"{x / 1e3:.1f}K\"\n\n    if x < 1e6:\n        return f\"{x / 1e3:.0f}K\"\n\n    if x < 1e7:\n        return f\"{x / 1e6:.1f}M\"\n\n    if x < 1e8:\n        return f\"{x / 1e6:.0f}M\"\n\n    if x < 1e9:\n        return f\"{x / 1e9:.1f}B\"\n\n    return f\"{x / 1e9:.0f}B\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if np.abs(x) >= 1e6:\n        return f\"{x / 1e6:,.2f}M\"\n    if np.abs(x) >= 1e3:\n        return f\"{x / 1e3:,.2f}K\"\n    if np.abs(x) >= 1e-1:\n        return f\"{x:.2f}\"\n    if np.abs(x) >= 1e-3:\n        return f\"{x:.3f}\"\n    if np.abs(x) >= 1e-5:\n        return f\"{x:.5f}\"\n    if np.abs(x) >= 1e-7:\n        return f\"{x:.7f}\"\n    if np.abs(x) >= 1e-9:\n        return f\"{x:.9f}\"\n    return f\"{x:.11f}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n\n    if percent:\n        x *= 100\n\n    if x == 0:\n        return \"0%\"\n\n    if x < 0:\n        sign = \"-\"\n        x = -x\n    else:\n        sign = \"\"\n\n    if x < 0.001:\n        return f\"{sign}0.000%\"\n\n    if x < 0.01:\n        return f\"{sign}0.00%\"\n\n    if x < 0.1:\n        return f\"{sign}0.0%\"\n\n    if x < 1:\n        return f\"{sign}0%\"\n\n    if x < 10:\n        return f\"{sign}{x:.2f}%\"\n\n    if x < 100:\n        return f\"{sign}{x:.1f}%\"\n\n    return f\"{sign}{x:.0f}%\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    if percent:\n        x *= 100\n        suffix = \"%\"\n    else:\n        suffix = \"\"\n    if x < 1e-3:\n        fmt = \"{:,.2e}\"\n    elif x < 1e-2:\n        fmt = \"{:,.2f}\"\n    elif x < 1:\n        fmt = \"{:,.3f}\"\n    elif x < 1e2:\n        fmt = \"{:,.1f}\"\n    elif x < 1e3:\n        fmt = \"{:,.0f}\"\n    elif x < 1e4:\n        fmt = \"{:,.0f}\"\n    elif x < 1e5:\n        fmt = \"{:,.0f}\"\n    elif x < 1e6:\n        fmt = \"{:,.0f}\"\n    elif x < 1e7:\n        fmt = \"{:,.0f}\"\n    elif x < 1e8:\n        fmt = \"{:,.0f}\"\n    elif x < 1e9:\n        fmt = \"{:,.0f}\"\n    elif x < 1e10:\n        fmt = \"{:,.0f}\"\n    elif x < 1e11:\n        fmt = \"{:,.0f}\"\n    elif x < 1e12:\n        fmt = \"{:,.0f}\"\n    elif x < 1e13:\n        fmt = \"{:,.0f}\"\n    elif x < 1e14:\n        fmt = \"{:,.0f}\"\n    elif x < 1e15:\n        fmt = \"{:,.0f}\"\n    elif x < 1e16:\n        fmt = \"{:,.0f}\"\n    elif x < 1e17:\n        fmt = \"{:,.0f}\"\n    elif x < 1e18:\n        fmt = \"{:,.0f}\"\n    elif x < 1e19:\n        fmt = \"{:,.0f}\"\n    else:\n        fmt = \"{:,.0f}\"\n    return fmt.format(x) +"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    if percent:\n        x *= 100\n    if x < 0:\n        return f\"-{format_measure(-x, percent)}\"\n    if x == 0:\n        return \"0\"\n    if x < 1e-6:\n        return \"0\"\n    if x < 1e-3:\n        return f\"{x:1.2e}\"\n    if x < 0.1:\n        return f\"{x:1.3f}\"\n    if x < 1:\n        return f\"{x:1.2f}\"\n    if x < 10:\n        return f\"{x:1.1f}\"\n    if x < 100:\n        return f\"{x:1.0f}\"\n    if x < 1e3:\n        return f\"{x:1.0f}\"\n    if x < 1e4:\n        return f\"{x / 1e3:1.1f}K\"\n    if x < 1e6:\n        return f\"{x / 1e3:1.0f}K\"\n    if x < 1e7:\n        return f\"{x / 1e6:1.1f}M\"\n    if x < 1e9:\n        return f\"{x / 1e6:1.0f}M\"\n    if x < 1e12:\n        return f\"{x / 1e9:1.1f}B\"\n    return f\"{x / 1e9:1.0f}B\""}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        free_space = shutil.disk_usage(input_dir).free / 1e9\n        if free_space < threshold_in_gb:\n            sleep(sleep_time)\n        else:\n            break"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        free_space = shutil.disk_usage(input_dir).free / 1e9\n        if free_space > threshold_in_gb:\n            break\n        sleep(sleep_time)"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        free_space = shutil.disk_usage(input_dir).free\n        if free_space > threshold_in_gb * 1024 * 1024 * 1024:\n            break\n        sleep(sleep_time)"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        disk_usage = shutil.disk_usage(input_dir)\n        free_space = disk_usage.free / 1e9\n\n        if free_space > threshold_in_gb:\n            break\n\n        sleep(sleep_time)"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        disk_usage = shutil.disk_usage(input_dir)\n        free_space = disk_usage.free / (1024**3)\n        if free_space > threshold_in_gb:\n            break\n        sleep(sleep_time)"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        free_space = shutil.disk_usage(input_dir).free / (1024**3)\n        if free_space < threshold_in_gb:\n            sleep(sleep_time)\n        else:\n            break"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        free_space = shutil.disk_usage(input_dir).free / 1e9\n        if free_space < threshold_in_gb:\n            sleep(sleep_time)\n        else:\n            return"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        free_space = shutil.disk_usage(input_dir).free\n        if free_space > threshold_in_gb * 1e9:\n            return\n        sleep(sleep_time)"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        disk = psutil.disk_usage(input_dir)\n        if disk.free / 1e9 < threshold_in_gb:\n            sleep(sleep_time)\n        else:\n            break"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        statvfs = os.statvfs(input_dir)\n        free_space_in_gb = statvfs.f_bavail * statvfs.f_frsize / 1024 / 1024 / 1024\n        if free_space_in_gb > threshold_in_gb:\n            return\n        sleep(sleep_time)"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        # 1. Get the disk usage\n        disk_usage = shutil.disk_usage(input_dir)\n\n        # 2. Check if the free space is lower than the threshold\n        if disk_usage.free / 1e9 < threshold_in_gb:\n            break\n\n        # 3. Sleep for a while\n        sleep(sleep_time)"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        free_space = shutil.disk_usage(input_dir).free / (1024**3)\n        if free_space > threshold_in_gb:\n            break\n        sleep(sleep_time)"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        # 1. Get the free space in gigabytes\n        free_space = shutil.disk_usage(input_dir).free / 1e9\n\n        # 2. Check if the free space is lower than the threshold\n        if free_space < threshold_in_gb:\n            return\n\n        # 3. Sleep for a given amount of time before checking the disk usage again\n        sleep(sleep_time)"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        # 1. Get the free disk space in gigabytes\n        free_space = shutil.disk_usage(input_dir).free / 1e9\n\n        # 2. Check if the free space is lower than the threshold\n        if free_space < threshold_in_gb:\n            return\n\n        # 3. Sleep for a while\n        sleep(sleep_time)"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        # 1. Get the disk usage of the input directory\n        disk_usage = shutil.disk_usage(input_dir)\n        free_space_in_gb = disk_usage.free / 1024 / 1024 / 1024\n\n        # 2. Check if the free space is lower than the threshold\n        if free_space_in_gb > threshold_in_gb:\n            break\n\n        # 3. Wait for the specified time before checking again\n        sleep(sleep_time)"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        # 1. Get the free space\n        _, _, free_space = shutil.disk_usage(input_dir)\n\n        # 2. Terminate the process if the free space is higher than the threshold\n        if free_space > threshold_in_gb * 1e9:\n            return\n\n        # 3. Sleep\n        sleep(sleep_time)"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        # Get the disk usage of the input directory\n        disk_usage = shutil.disk_usage(input_dir)\n\n        # Check if the free space is lower than the threshold\n        if disk_usage.free / (1024**3) > threshold_in_gb:\n            return\n\n        # Sleep for the specified time before checking the disk usage again\n        sleep(sleep_time)"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        # 1. Get the current disk usage\n        free_space = shutil.disk_usage(input_dir).free\n\n        # 2. Check if the free space is lower than the threshold\n        if free_space / 1e9 < threshold_in_gb:\n            sleep(sleep_time)\n            continue\n\n        # 3. Exit the loop if the free space is higher than the threshold\n        break"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        # Get the free space in the root directory in gigabytes\n        free_space = shutil.disk_usage(input_dir).free / 1e9\n\n        # If the free space is higher than the threshold, break the loop\n        if free_space > threshold_in_gb:\n            break\n\n        # Otherwise, sleep for a specified amount of time\n        sleep(sleep_time)"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        # 1. Get the free space in the specified directory\n        statvfs = os.statvfs(input_dir)\n        free_space_in_bytes = statvfs.f_frsize * statvfs.f_bfree\n\n        # 2. Check if the free space is lower than the threshold\n        if free_space_in_bytes / 1e9 > threshold_in_gb:\n            break\n\n        # 3. If not, sleep for a specified amount of time\n        sleep(sleep_time)"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  w = jnp.where(td < np.finfo(np.float32).tiny, 0, p * td)\n  return w"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  w = jnp.where(td < np.finfo(np.float32).tiny, 0, p * td)\n  return w"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  w = jnp.where(td < np.finfo(np.float32).tiny, 0, p * td)\n  return w"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  w = jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))\n  return w"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  w = jnp.where(td < np.finfo(np.float32).tiny, 0, p * td)\n  return w"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  w = jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))\n  return w"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, p * td)"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  # Compute the differences between consecutive elements in the time or position vector.\n  td = jnp.diff(t)\n\n  # Multiply the PDF values by the differences to obtain the weights.\n  w = p * td\n\n  return w"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  w = jnp.where(td < np.finfo(np.float32).tiny, 0, p * td)\n  return w\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  # Compute the differences between consecutive elements in the time or position vector.\n  td = jnp.diff(t)\n\n  # Multiply the PDF values by the differences to obtain the weights.\n  w = jnp.where(td < np.finfo(np.float32).tiny, 0, p * td)\n\n  # Ensure that the weights sum to 1.\n  w = w / jnp.sum(w, axis=-1, keepdims=True)\n\n  return w"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  w = jnp.where(td < np.finfo(np.float32).tiny, 0, p * td)\n  return w\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  w = jnp.where(td < np.finfo(np.float32).tiny, 0, p * td)\n  return w\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  # Convert the PDF to a vector of weights by multiplying each PDF value by the difference between consecutive elements in the time or position vector.\n  p_diff = jnp.diff(p)\n  w = jnp.concatenate([p[..., :1], p_diff], axis=-1)\n  return w"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, p * td)\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  # Compute the differences between consecutive elements in 't'\n  td = jnp.diff(t)\n\n  # Multiply the PDF values by the differences to obtain weights.\n  w = p * td\n\n  # Return the weights.\n  return w\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = \"\".join(line_text.split())\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = nlm_tokenize(line_text)\n\n    # Return the segmented parts of the modified input text\n    return tokens"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = re.sub(r\"\\s+\", \"\", line_text)\n    tokens = nlm_tokenize(line_text)\n    return tokens"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text.\n    line_text = \"\".join(line_text.split())\n\n    # Segment the modified text into smaller parts or tokens.\n    tokens = nlm_tokenize(line_text)\n\n    # Return the segmented parts of the modified input text.\n    return tokens"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = re.sub(r\"\\s+\", \"\", line_text)\n    tokens = nlm_tokenize(line_text)\n    return tokens"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # remove all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # segment the modified text into smaller parts or tokens\n    tokens = nlm_tokenize(line_text)\n\n    # join the tokens back into a single string\n    line_text = \" \".join(tokens)\n\n    return line_text"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = \"\".join(line_text.split())\n    # Segment the modified text into smaller parts or tokens\n    tokens = nlm_tokenize(line_text)\n    # Join the tokens back into a single string\n    line_text = \" \".join(tokens)\n    return line_text"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # remove all whitespace characters from the input text\n    line_text = \"\".join(line_text.split())\n    # segment the modified text into smaller parts or tokens\n    tokens = nlm_tokenize(line_text)\n    # join the tokens back into a single string\n    line_text = \" \".join(tokens)\n    return line_text"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text.\n    line_text = re.sub(r\"\\s+\", \"\", line_text)\n\n    # Segment the modified text into smaller parts or tokens.\n    tokens = nlm_tokenize(line_text)\n\n    # Return the segmented parts of the modified input text.\n    return tokens"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = re.sub(r\"\\s+\", \" \", line_text)\n    tokens = nlm_tokenize(line_text)\n    return tokens"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # remove all spaces from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # segment the modified text into smaller parts or tokens\n    tokens = nlm_tokenize(line_text)\n\n    # join the tokens back into a single string\n    line_text = \" \".join(tokens)\n\n    return line_text"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    modified_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = nlm_tokenize(modified_text)\n\n    # Join the tokens back into a single string with spaces between them\n    result = \" \".join(tokens)\n\n    return result"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = \"\".join(line_text.split())\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = nlm_tokenize(line_text)\n\n    # Reconstruct the modified text using the segmented parts\n    line_text = \" \".join(tokens)\n\n    return line_text"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = nlm_tokenize(line_text)\n\n    # Join the tokens back into a string with a single space between each token\n    result = \" \".join(tokens)\n\n    return result"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # remove all whitespace characters from the input text\n    line_text = \"\".join(line_text.split())\n\n    # segment the modified text into smaller parts or tokens\n    line_text = su.segment(line_text)\n\n    return line_text"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\" \", \"\")\n    line_text = line_text.replace(\"-\", \" \")\n    line_text = line_text.replace(\"\u2013\", \" \")\n    line_text = line_text.replace(\"\u2014\", \" \")\n    line_text = line_text.replace(\"\u2019\", \"'\")\n    line_text = line_text.replace(\"\u201c\", '\"')\n    line_text = line_text.replace(\"\u201d\", '\"')\n    line_text = line_text.replace(\"\u2018\", \"'\")\n    line_text = line_text.replace(\"\u2019\", \"'\")\n    line_text = line_text.replace(\"\u2018\", \"'\")\n    line_text = line_text.replace(\"\u2019\", \"'\")\n    line_text = line_text.replace(\"\u2018\", \"'\")\n    line_text = line_text.replace(\"\u2019\", \"'\")\n    line_text = line_text.replace(\"\u2018\", \"'\")\n    line_text = line_text.replace(\"\u2019\", \"'\")\n    line_text = line_text.replace(\"\u2018\", \"'\")\n    line_text = line_text.replace(\"\u2019\", \"'\")\n    line_text = line_text.replace(\"\u2018\", \"'\")\n    line_text = line_text.replace(\"\u2019\", \"'\")\n    line_text = line_text.replace(\"\u2018\", \"'\")\n    line_text = line_text.replace(\"\u2019\", \"'\")\n    line_text = line_text.replace(\"\u2018\", \"'\")\n    line_text = line_text.replace(\"\u2019\", \"'\")\n    line_text = line_text.replace(\"\u2018\", \"'\")\n    line_text = line_text.replace(\"\u2019\", \"'\")\n    line_text = line_text.replace(\"\u2018\", \"'\")\n    line_text = line_text.replace(\"\u2019\", \"'\")\n    line_text = line_text.replace(\"\u2018\", \"'\")\n    line_text = line_text.replace(\"\u2019\", \"'\")\n    line_text = line_text.replace(\"\u2018\", \"'\")\n    line_text = line_text.replace(\"\u2019\", \"'\")\n    line_text"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\" \", \"\")\n    line_text = line_text.replace(\"-\", \" \")\n    line_text = line_text.replace(\"_\", \" \")\n    line_text = line_text.replace(\".\", \" \")\n    line_text = line_text.replace(\",\", \" \")\n    line_text = line_text.replace(\";\", \" \")\n    line_text = line_text.replace(\":\", \" \")\n    line_text = line_text.replace(\"?\", \" \")\n    line_text = line_text.replace(\"(\", \" \")\n    line_text = line_text.replace(\")\", \" \")\n    line_text = line_text.replace(\"[\", \" \")\n    line_text = line_text.replace(\"]\", \" \")\n    line_text = line_text.replace(\"{\", \" \")\n    line_text = line_text.replace(\"}\", \" \")\n    line_text = line_text.replace(\"<\", \" \")\n    line_text = line_text.replace(\">\", \" \")\n    line_text = line_text.replace(\"=\", \" \")\n    line_text = line_text.replace(\"+\", \" \")\n    line_text = line_text.replace(\"*\", \" \")\n    line_text = line_text.replace(\"&\", \" \")\n    line_text = line_text.replace(\"|\", \" \")\n    line_text = line_text.replace(\"^\", \" \")\n    line_text = line_text.replace(\"%\", \" \")\n    line_text = line_text.replace(\"$\", \" \")\n    line_text = line_text.replace(\"@\", \" \")\n    line_text = line_text.replace(\"#\", \" \")\n    line_text = line_text.replace(\"~\", \" \")\n    line_text = line_text.replace(\"`\", \" \")\n    line_text = line_text.replace(\"'\", \" \")\n    line_text = line_text.replace('\"', \" \")\n    line_text = line_text.replace(\"\\\\\", \" \")\n    line_text = line_text"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all spaces from the input text\n    modified_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = nlm_tokenize(modified_text)\n\n    # Convert the tokens back to a string\n    fixed_line_text = \" \".join(tokens)\n\n    # Return the fixed line text\n    return fixed_line_text"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\" \", \"\")\n    line_text = line_text.replace(\"-\", \"\")\n    line_text = line_text.replace(\"\u2013\", \"\")\n    line_text = line_text.replace(\"\u2014\", \"\")\n    line_text = line_text.replace(\"\u2019\", \"\")\n    line_text = line_text.replace(\"\u2018\", \"\")\n    line_text = line_text.replace(\"\u201c\", \"\")\n    line_text = line_text.replace(\"\u201d\", \"\")\n    line_text = line_text.replace(\"(\", \"\")\n    line_text = line_text.replace(\")\", \"\")\n    line_text = line_text.replace(\"?\", \"\")\n    line_text = line_text.replace(\".\", \"\")\n    line_text = line_text.replace(\",\", \"\")\n    line_text = line_text.replace(\";\", \"\")\n    line_text = line_text.replace(\":\", \"\")\n    line_text = line_text.replace(\"'\", \"\")\n    line_text = line_text.replace(\"`\", \"\")\n    line_text = line_text.replace('\"', \"\")\n    line_text = line_text.replace(\"\u2019\", \"\")\n    line_text = line_text.replace(\"\u2018\", \"\")\n    line_text = line_text.replace(\"\u201c\", \"\")\n    line_text = line_text.replace(\"\u201d\", \"\")\n    line_text = line_text.replace(\"-\", \"\")\n    line_text = line_text.replace(\"\u2013\", \"\")\n    line_text = line_text.replace(\"\u2014\", \"\")\n    line_text = line_text.replace(\"?\", \"\")\n    line_text = line_text.replace(\".\", \"\")\n    line_text = line_text.replace(\",\", \"\")\n    line_text = line_text.replace(\";\", \"\")\n    line_text = line_text.replace(\":\", \"\")\n    line_text = line_text.replace(\"'\", \"\")\n    line_text = line_"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\" \", \"\")\n    line_text = line_text.replace(\"-\", \" \")\n    line_text = line_text.replace(\"(\", \" \")\n    line_text = line_text.replace(\")\", \" \")\n    line_text = line_text.replace(\".\", \" \")\n    line_text = line_text.replace(\"?\", \" \")\n    line_text = line_text.replace(\";\", \" \")\n    line_text = line_text.replace(\":\", \" \")\n    line_text = line_text.replace(\",\", \" \")\n    line_text = line_text.replace(\"'\", \" \")\n    line_text = line_text.replace(\"`\", \" \")\n    line_text = line_text.replace(\"\u201c\", \" \")\n    line_text = line_text.replace(\"\u201d\", \" \")\n    line_text = line_text.replace(\"\u2018\", \" \")\n    line_text = line_text.replace(\"\u2019\", \" \")\n    line_text = line_text.replace(\"\u2014\", \" \")\n    line_text = line_text.replace(\"\u2013\", \" \")\n    line_text = line_text.replace(\"\u2026\", \" \")\n    line_text = line_text.replace(\"\u2022\", \" \")\n    line_text = line_text.replace(\"\u27a2\", \" \")\n    line_text = line_text.replace(\"\u0192\", \" \")\n    line_text = line_text.replace(\"\uf0b7\", \" \")\n    line_text = line_text.replace(\"\u2011\", \" \")\n    line_text = line_text.replace(\"\u2013\", \" \")\n    line_text = line_text.replace(\"\u2015\", \" \")\n    line_text = line_text.replace(\"\u2016\", \" \")\n    line_text = line_text.replace(\"\u2043\", \" \")\n    line_text = line_text.replace(\"\u207b\", \" \")\n    line_text = line_text.replace(\"\u207a\", \" \")\n    line_text = line_text.replace(\"\u207c\", \" \")\n    line_text = line"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\" \", \"\")\n    line_text = line_text.replace(\"'\", \"\")\n    line_text = line_text.replace(\"-\", \"\")\n    line_text = line_text.replace(\"\u2013\", \"\")\n    line_text = line_text.replace(\"\u2014\", \"\")\n    line_text = line_text.replace(\"\u2013\", \"\")\n    line_text = line_text.replace(\"\u2014\", \"\")\n    line_text = line_text.replace(\"\u2019\", \"\")\n    line_text = line_text.replace(\"\u2018\", \"\")\n    line_text = line_text.replace(\"`\", \"\")\n    line_text = line_text.replace(\"\u00b4\", \"\")\n    line_text = line_text.replace(\"\u2053\", \"\")\n    line_text = line_text.replace(\"\u2043\", \"\")\n    line_text = line_text.replace(\"\u2052\", \"\")\n    line_text = line_text.replace(\"\u2051\", \"\")\n    line_text = line_text.replace(\"\u2040\", \"\")\n    line_text = line_text.replace(\"\u2040\", \"\")\n    line_text = line_text.replace(\"\u2057\", \"\")\n    line_text = line_text.replace(\"\u2058\", \"\")\n    line_text = line_text.replace(\"\u2059\", \"\")\n    line_text = line_text.replace(\"\u205a\", \"\")\n    line_text = line_text.replace(\"\u205b\", \"\")\n    line_text = line_text.replace(\"\u205c\", \"\")\n    line_text = line_text.replace(\"\u205d\", \"\")\n    line_text = line_text.replace(\"\u205e\", \"\")\n    line_text = line_text.replace(\"\u205f\", \"\")\n    line_text = line_text.replace(\"\u2060\", \"\")\n    line_text = line_text.replace(\"\u2061\", \"\")\n    line_text = line_text.replace(\"\u2062\", \"\")"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    weights = np.zeros(n)\n    if zeros > 0:\n        weights[np.random.choice(n, zeros, replace=False)] = 0\n        weights = weights / np.sum(weights)\n    else:\n        weights = np.ones(n) / n\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    weights = np.random.uniform(0, 1, n)\n    if zeros > 0:\n        weights[np.random.choice(n, zeros, replace=False)] = 0\n    weights = weights / np.sum(weights)\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    weights = np.random.uniform(size=n)\n    weights /= weights.sum()\n    if zeros > 0:\n        weights[np.random.choice(n, zeros, replace=False)] = 0\n        weights /= weights.sum()\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Generate n random weights that sum to one\n    weights = np.random.rand(n)\n    weights /= np.sum(weights)\n\n    # If zeros is greater than zero, set zeros random weights to zero\n    if zeros > 0:\n        zero_idx = np.random.choice(n, zeros, replace=False)\n        weights[zero_idx] = 0\n\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros >= n:\n        raise ValueError(\"The number of zeros must be less than the number of weights\")\n\n    weights = np.zeros(n)\n    weights[np.random.choice(n, n - zeros, replace=False)] = 1\n    weights /= weights.sum()\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if n < 0:\n        raise ValueError(\"n must be a positive integer\")\n    if zeros > n:\n        raise ValueError(\"zeros must be less than or equal to n\")\n\n    weights = np.random.dirichlet(np.ones(n - zeros))\n    weights = np.append(weights, np.zeros(zeros))\n    weights = weights / np.sum(weights)\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if n < 0:\n        raise ValueError(\"The number of weights must be positive\")\n    if zeros > n:\n        raise ValueError(\"The number of zeros must not exceed the number of weights\")\n    weights = np.random.uniform(0, 1, n)\n    weights = weights / np.sum(weights)\n    if zeros > 0:\n        weights[np.random.choice(n, zeros, replace=False)] = 0\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros < 0:\n        raise ValueError(\"zeros must be non-negative\")\n    if zeros > n:\n        raise ValueError(\"zeros must not exceed n\")\n    weights = np.random.rand(n)\n    weights /= np.sum(weights)\n    if zeros > 0:\n        weights[np.random.choice(n, zeros, replace=False)] = 0\n        weights /= np.sum(weights)\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"`zeros` must be less than or equal to `n`\")\n    weights = np.random.rand(n)\n    if zeros > 0:\n        weights[np.random.choice(n, zeros, replace=False)] = 0\n    weights /= np.sum(weights)\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros >= n:\n        raise ValueError(\"The number of zeros must be smaller than the number of weights.\")\n    if zeros == 0:\n        weights = np.random.rand(n)\n        weights /= np.sum(weights)\n    else:\n        weights = np.zeros(n)\n        weights[:zeros] = np.random.rand(zeros)\n        weights[zeros:] = np.random.rand(n - zeros)\n        weights /= np.sum(weights)\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Generate n random weights\n    weights = np.random.rand(n)\n\n    # Normalize the weights to sum to 1\n    weights /= np.sum(weights)\n\n    # Set the specified number of weights to zero\n    if zeros > 0:\n        # Sort the weights in descending order\n        sorted_indices = np.argsort(weights)[::-1]\n\n        # Set the specified number of weights to zero\n        weights[sorted_indices[:zeros]] = 0\n\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Generate a random vector of weights\n    weights = np.random.rand(n)\n\n    # If zeros is greater than 0, set a specified number of weights to zero\n    if zeros > 0:\n        weights[np.random.choice(n, zeros, replace=False)] = 0\n\n    # Normalize the weights to sum to one\n    weights /= np.sum(weights)\n\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Check if the input arguments are valid\n    if n <= 0:\n        raise ValueError(\"The number of weights must be positive\")\n    if zeros < 0 or zeros > n:\n        raise ValueError(\"The number of zero weights must be between 0 and n\")\n\n    # Generate n-zeros_ random weights that sum up to one\n    weights = np.random.dirichlet(np.ones(n - zeros))\n\n    # Set zeros_ random weights to zero\n    if zeros > 0:\n        zero_indices = np.random.choice(n, zeros, replace=False)\n        weights[zero_indices] = 0\n\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(f\"`zeros` must be smaller than `n`, got {zeros} > {n}\")\n    weights = np.random.dirichlet(np.ones(n - zeros))\n    weights = np.concatenate((np.zeros(zeros), weights))\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros == 0:\n        return np.random.rand(n)\n    if zeros >= n:\n        return np.zeros(n)\n    weights = np.zeros(n)\n    weights[0:n - zeros] = np.random.rand(n - zeros)\n    weights /= np.sum(weights)\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(f\"`zeros` must be less than or equal to `n`, got `zeros`={zeros} and `n`={n}\")\n    if zeros == n:\n        return np.zeros(n)\n    if zeros == 0:\n        weights = np.random.dirichlet(np.ones(n))\n        return weights\n    weights = np.random.dirichlet(np.ones(n - zeros))\n    weights = np.concatenate([np.zeros(zeros), weights])\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"Number of zeros must be less than or equal to n\")\n    if zeros == n:\n        return np.zeros(n)\n    weights = np.random.rand(n - zeros)\n    weights = weights / np.sum(weights)\n    weights = np.append(weights, np.zeros(zeros))\n    np.random.shuffle(weights)\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros >= n:\n        raise ValueError(\"The number of zeros must be less than the number of weights\")\n\n    # Generate n random weights\n    weights = np.random.rand(n)\n\n    # Normalize the weights to sum up to one\n    weights /= np.sum(weights)\n\n    # Set the specified number of weights to zero\n    if zeros > 0:\n        # Randomly select zeros indices\n        zero_indices = np.random.choice(n, zeros, replace=False)\n\n        # Set the selected weights to zero\n        weights[zero_indices] = 0\n\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Check input\n    if n <= 0:\n        raise ValueError(\"n must be greater than 0\")\n    if zeros < 0 or zeros > n:\n        raise ValueError(\"zeros must be between 0 and n\")\n\n    # Generate n random weights that sum up to one\n    weights = np.random.dirichlet(np.ones(n))\n\n    # Set zeros weights to zero\n    if zeros > 0:\n        zero_idx = np.random.choice(n, zeros, replace=False)\n        weights[zero_idx] = 0\n\n    return weights\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Check input arguments\n    if n < 1:\n        raise ValueError(\"Number of weights must be greater than zero.\")\n    if zeros < 0:\n        raise ValueError(\"Number of zero weights must be non-negative.\")\n    if zeros > n:\n        raise ValueError(\"Number of zero weights cannot exceed the total number of weights.\")\n\n    # Generate random weights\n    weights = np.random.random(n)\n\n    # Normalize weights to sum to one\n    weights /= np.sum(weights)\n\n    # Set zero weights\n    if zeros > 0:\n        zero_indices = np.random.choice(n, zeros, replace=False)\n        weights[zero_indices] = 0\n\n    return weights"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": ""}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": ""}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": ""}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": ""}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": ""}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": ""}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": ""}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": ""}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type=module_type, module_param=module_param)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n\n        return cls(module_type, module_param)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = module_dict\n\n        return cls(module_type, module_param)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = module_dict\n        return cls(module_type, module_param)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = module_dict\n        return cls(module_type, module_param)\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n\n    # Ensure that the cropping region is within the image boundaries\n    x1, y1, x2, y2 = bbox\n    w, h = x2 - x1, y2 - y1\n    crop_w, crop_h = crop_size\n    x1 = max(0, x1 - (crop_w - w) // 2)\n    y1 = max(0, y1 - (crop_h - h) // 2)\n    x2 = min(image_size[1], x2 + (crop_w - w) // 2)\n    y2 = min(image_size[0], y2 + (crop_h - h) // 2)\n\n    # Create the CropTransform object\n    crop_transform = T.CropTransform(x1, y1, x2, y2)\n\n    return crop_transform"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n\n    # Compute the cropping region\n    crop_y, crop_x, crop_h, crop_w = T.crop_box_to_bbox(bbox, image_size, crop_size)\n\n    # Adjust the cropping region to ensure it fits within the image boundaries\n    crop_y = max(0, crop_y)\n    crop_x = max(0, crop_x)\n    crop_h = min(image_size[0] - crop_y, crop_h)\n    crop_w = min(image_size[1] - crop_x, crop_w)\n\n    # Create the CropTransform object\n    crop_transform = T.CropTransform(crop_x, crop_y, crop_w, crop_h)\n\n    return crop_transform"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n    center = [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2]\n    crop_x = max(0, center[0] - crop_size[1] / 2)\n    crop_y = max(0, center[1] - crop_size[0] / 2)\n    crop_w = min(crop_size[1], image_size[1] - crop_x)\n    crop_h = min(crop_size[0], image_size[0] - crop_y)\n    crop_transform = T.CropTransform(x=crop_x, y=crop_y, x2=crop_x + crop_w, y2=crop_y + crop_h)\n    return crop_transform"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n    bbox_center_x = (bbox[0] + bbox[2]) / 2\n    bbox_center_y = (bbox[1] + bbox[3]) / 2\n    crop_size_x, crop_size_y = crop_size\n    image_size_x, image_size_y = image_size\n\n    # Adjust the crop size to ensure it fits within the image boundaries\n    crop_size_x = min(crop_size_x, image_size_x - bbox_center_x)\n    crop_size_y = min(crop_size_y, image_size_y - bbox_center_y)\n    crop_size_x = min(crop_size_x, bbox_center_x)\n    crop_size_y = min(crop_size_y, bbox_center_y)\n\n    # Calculate the top-left corner of the crop\n    crop_x = bbox_center_x - crop_size_x / 2\n    crop_y = bbox_center_y - crop_size_y / 2\n\n    # Create the CropTransform object\n    crop_transform = T.CropTransform(crop_x, crop_y, crop_size_x, crop_size_y)\n\n    return crop_transform"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the bounding box of the instance\n    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n\n    # Calculate the cropping region\n    x_min = max(0, int(bbox[0] - crop_size[1] / 2))\n    y_min = max(0, int(bbox[1] - crop_size[0] / 2))\n    x_max = min(image_size[1], int(bbox[2] + crop_size[1] / 2))\n    y_max = min(image_size[0], int(bbox[3] + crop_size[0] / 2))\n\n    # Adjust the cropping region to ensure it fits within the image boundaries\n    x_min = max(0, x_min)\n    y_min = max(0, y_min)\n    x_max = min(image_size[1], x_max)\n    y_max = min(image_size[0], y_max)\n\n    # Create a CropTransform object with the adjusted cropping region\n    crop = T.CropTransform(x_min, y_min, x_max - x_min, y_max - y_min)\n\n    # Return the CropTransform object\n    return crop"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n    bbox_h = bbox[3] - bbox[1]\n    bbox_w = bbox[2] - bbox[0]\n    bbox_center_h = bbox[1] + bbox_h / 2\n    bbox_center_w = bbox[0] + bbox_w / 2\n    crop_h = crop_size[0]\n    crop_w = crop_size[1]\n    image_h = image_size[0]\n    image_w = image_size[1]\n    crop_top = int(bbox_center_h - crop_h / 2)\n    crop_left = int(bbox_center_w - crop_w / 2)\n    crop_bottom = int(bbox_center_h + crop_h / 2)\n    crop_right = int(bbox_center_w + crop_w / 2)\n    crop_top = max(crop_top, 0)\n    crop_left = max(crop_left, 0)\n    crop_bottom = min(crop_bottom, image_h)\n    crop_right = min(crop_right, image_w)\n    crop_h = crop_bottom - crop_top\n    crop_w = crop_right - crop_left\n    crop_transform = T.CropTransform(x_min=crop_left, y_min=crop_top, x_max=crop_right, y_max=crop_bottom)\n    return crop_transform"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n    # Crop the image around the instance\n    crop_t, crop_l, crop_h, crop_w = T.crop_box(bbox, image_size, crop_size)\n    # Adjust the crop region to ensure it contains the center of the instance\n    center_x, center_y = (bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2\n    crop_l = min(max(0, int(center_x - crop_w / 2)), image_size[1] - crop_w)\n    crop_t = min(max(0, int(center_y - crop_h / 2)), image_size[0] - crop_h)\n    crop_h = min(image_size[0] - crop_t, crop_h)\n    crop_w = min(image_size[1] - crop_l, crop_w)\n    return T.CropTransform(crop_t, crop_l, crop_h, crop_w)"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the instance's bounding box coordinates and mode\n    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n\n    # Convert the bounding box to absolute coordinates\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n\n    # Calculate the center of the instance\n    center_x = (bbox[0] + bbox[2]) / 2\n    center_y = (bbox[1] + bbox[3]) / 2\n\n    # Calculate the cropping region's top-left corner\n    top_left_x = max(0, int(center_x - crop_size[1] / 2))\n    top_left_y = max(0, int(center_y - crop_size[0] / 2))\n\n    # Calculate the cropping region's dimensions\n    crop_width = min(crop_size[1], image_size[1] - top_left_x)\n    crop_height = min(crop_size[0], image_size[0] - top_left_y)\n\n    # Create the CropTransform object\n    crop_transform = CropTransform(\n        top_left_x, top_left_y, crop_width, crop_height, image_size[1], image_size[0]\n    )\n\n    return crop_transform"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Extract the bounding box coordinates from the instance\n    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n\n    # Calculate the cropping region\n    crop_h, crop_w = crop_size\n    img_h, img_w = image_size\n    x1, y1, x2, y2 = bbox\n    crop_x = max(0, int(x1 + (x2 - x1) / 2 - crop_w / 2))\n    crop_y = max(0, int(y1 + (y2 - y1) / 2 - crop_h / 2))\n    crop_x = min(crop_x, img_w - crop_w)\n    crop_y = min(crop_y, img_h - crop_h)\n\n    # Create the CropTransform object\n    crop_transform = T.CropTransform(x=crop_x, y=crop_y, x2=crop_x + crop_w, y2=crop_y + crop_h)\n\n    return crop_transform"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    crop_h, crop_w = crop_size\n    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n    bbox_h, bbox_w = bbox[2] - bbox[0], bbox[3] - bbox[1]\n    bbox_y_ctr = bbox[1] + bbox_h / 2.0\n    bbox_x_ctr = bbox[0] + bbox_w / 2.0\n    crop_y_ctr = bbox_y_ctr\n    crop_x_ctr = bbox_x_ctr\n    crop_y_min = max(crop_y_ctr - crop_h / 2.0, 0.0)\n    crop_y_max = min(crop_y_ctr + crop_h / 2.0, image_size[0])\n    crop_x_min = max(crop_x_ctr - crop_w / 2.0, 0.0)\n    crop_x_max = min(crop_x_ctr + crop_w / 2.0, image_size[1])\n    crop_y_min = max(crop_y_min, 0.0)\n    crop_x_min = max(crop_x_min, 0.0)\n    crop_y_max = min(crop_y_max, image_size[0])\n    crop_x_max = min(crop_x_max, image_size[1])\n    crop_h = crop_y_max - crop_y_min\n    crop_w = crop_x_max - crop_x_min\n    crop_transform = T.CropTransform(crop_x_min, crop_y_min, crop_w, crop_h)\n    return crop_transform"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n    if bbox_mode == BoxMode.XYXY_ABS:\n        bbox_mode = BoxMode.XYWH_ABS\n    crop_transform = T.CropTransform(\n        *T.CropTransform.get_crop_box(\n            bbox,\n            bbox_mode,\n            image_size,\n            crop_size,\n            dtype=instance[\"bbox\"].dtype,\n        ),\n    )\n    return crop_transform"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    if isinstance(crop_size, int):\n        crop_size = (crop_size, crop_size)\n    if isinstance(image_size, int):\n        image_size = (image_size, image_size)\n\n    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox = np.clip(bbox, 0, image_size)\n    bbox_center = (bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2\n    crop_center = bbox_center\n\n    crop_w, crop_h = crop_size\n    crop_left = crop_center[0] - crop_w / 2\n    crop_top = crop_center[1] - crop_h / 2\n    crop_right = crop_left + crop_w\n    crop_bottom = crop_top + crop_h\n\n    crop_left = np.clip(crop_left, 0, image_size[1])\n    crop_top = np.clip(crop_top, 0, image_size[0])\n    crop_right = np.clip(crop_right, 0, image_size[1])\n    crop_bottom = np.clip(crop_bottom, 0, image_size[0])\n\n    crop_w = crop_right - crop_left\n    crop_h = crop_bottom - crop_top\n\n    return T.CropTransform(crop_left, crop_top, crop_w, crop_h)"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n    bbox_center_x = (bbox[0] + bbox[2]) / 2\n    bbox_center_y = (bbox[1] + bbox[3]) / 2\n\n    # Adjust the crop size to ensure that the cropping region contains the instance's bounding box\n    crop_width = crop_size[1]\n    crop_height = crop_size[0]\n    if bbox_center_x - crop_width / 2 < 0:\n        crop_width = min(bbox_center_x * 2, image_size[1])\n    if bbox_center_x + crop_width / 2 > image_size[1]:\n        crop_width = min((image_size[1] - bbox_center_x) * 2, image_size[1])\n    if bbox_center_y - crop_height / 2 < 0:\n        crop_height = min(bbox_center_y * 2, image_size[0])\n    if bbox_center_y + crop_height / 2 > image_size[0]:\n        crop_height = min((image_size[0] - bbox_center_y) * 2, image_size[0])\n\n    # Compute the top-left corner of the cropping region\n    crop_x1 = max(0, int(bbox_center_x - crop_width / 2))\n    crop_y1 = max(0, int(bbox_center_y - crop_height / 2))\n\n    return T.CropTransform(crop_x1, crop_y1, crop_width, crop_height)"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n\n    # Compute cropping region based on instance bounding box and desired crop size\n    x1 = max(0, bbox[0] - crop_size[1] // 2)\n    y1 = max(0, bbox[1] - crop_size[0] // 2)\n    x2 = min(image_size[1], bbox[2] + crop_size[1] // 2)\n    y2 = min(image_size[0], bbox[3] + crop_size[0] // 2)\n\n    # Adjust cropping region to ensure it fits within image boundaries\n    x1 = max(0, x1)\n    y1 = max(0, y1)\n    x2 = min(image_size[1], x2)\n    y2 = min(image_size[0], y2)\n\n    # Compute cropping region based on adjusted bounding box and desired crop size\n    x1_crop = max(0, x1 - crop_size[1] // 2)\n    y1_crop = max(0, y1 - crop_size[0] // 2)\n    x2_crop = min(image_size[1], x2 + crop_size[1] // 2)\n    y2_crop = min(image_size[0], y2 + crop_size[0] // 2)\n\n    # Compute cropping region based on adjusted bounding box and desired crop size\n    x1_crop = max(0, x1 - crop_size[1] // 2)\n    y1_crop = max(0, y1 - crop_size[0] // 2)\n    x2_crop = min(image_size[1], x2 + crop_size[1] // 2)\n    y2_crop = min(image_size[0], y2 + crop_size[0]"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the bounding box coordinates of the instance\n    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n    bbox_center_x = (bbox[0] + bbox[2]) / 2\n    bbox_center_y = (bbox[1] + bbox[3]) / 2\n\n    # Calculate the cropping region\n    crop_w, crop_h = crop_size\n    crop_left = max(0, int(bbox_center_x - crop_w / 2))\n    crop_top = max(0, int(bbox_center_y - crop_h / 2))\n    crop_right = min(image_size[1], int(bbox_center_x + crop_w / 2))\n    crop_bottom = min(image_size[0], int(bbox_center_y + crop_h / 2))\n\n    # Adjust the cropping region to ensure it fits within the image boundaries\n    crop_left = max(0, crop_left)\n    crop_top = max(0, crop_top)\n    crop_right = min(image_size[1], crop_right)\n    crop_bottom = min(image_size[0], crop_bottom)\n\n    # Create the CropTransform object\n    crop_transform = T.CropTransform(x_min=crop_left, y_min=crop_top, x_max=crop_right, y_max=crop_bottom)\n\n    # Return the CropTransform object\n    return crop_transform"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    box = instance[\"bbox\"]\n    box_mode = instance[\"bbox_mode\"]\n    box = BoxMode.convert(box, box_mode, BoxMode.XYXY_ABS)\n    box_h, box_w = box[2] - box[0], box[3] - box[1]\n    crop_h, crop_w = crop_size\n    # Ensure that the cropping region is within the image boundaries\n    box[0] = max(box[0] - crop_w // 2, 0)\n    box[1] = max(box[1] - crop_h // 2, 0)\n    box[2] = min(box[2] + crop_w // 2, image_size[1])\n    box[3] = min(box[3] + crop_h // 2, image_size[0])\n    box_h, box_w = box[2] - box[0], box[3] - box[1]\n    # Adjust the cropping region to ensure it contains the center of the instance\n    if box_h < crop_h:\n        crop_h = box_h\n    if box_w < crop_w:\n        crop_w = box_w\n    box[0] += (crop_w - box_w) // 2\n    box[1] += (crop_h - box_h) // 2\n    crop_transform = T.CropTransform(0, 0, crop_w, crop_h)\n    return crop_transform"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n    bbox_center = [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2]\n    crop_center = [int(bbox_center[0]), int(bbox_center[1])]\n\n    crop_size = np.array(crop_size)\n    image_size = np.array(image_size)\n\n    # Compute the size of the crop region based on the desired size and the aspect ratio of the instance's bounding box\n    crop_size_wh = crop_size * np.array([image_size[1] / image_size[0], image_size[0] / image_size[1]])\n    crop_size_wh = crop_size_wh.astype(int)\n\n    # Compute the top-left corner of the crop region, ensuring that it is within the image boundaries\n    crop_tl = [\n        max(0, min(image_size[1] - crop_size_wh[1], crop_center[1] - crop_size_wh[1] // 2)),\n        max(0, min(image_size[0] - crop_size_wh[0], crop_center[0] - crop_size_wh[0] // 2)),\n    ]\n\n    # Compute the size of the crop region, ensuring that it is within the image boundaries\n    crop_size_wh = [\n        min(crop_size_wh[1], image_size[1] - crop_tl[1]),\n        min(crop_size_wh[0], image_size[0] - crop_tl[0]),\n    ]\n\n    # Return the CropTransform object with the computed parameters\n    return T.CropTransform(crop_tl, crop_size_wh)"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox = np.clip(bbox, 0, image_size)\n    bbox_center = (bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2\n    crop_center = np.array(bbox_center)\n    crop_size = np.array(crop_size)\n    crop_size = crop_size.astype(np.float32)\n    crop_center = crop_center.astype(np.float32)\n    crop_size = crop_size.astype(np.float32)\n    crop_size = crop_size * 1.25\n    crop_size = crop_size.astype(np.int32)\n    crop_center = crop_center.astype(np.int32)\n    crop_center = np.clip(crop_center, 0, image_size)\n    crop_size = np.clip(crop_size, 0, image_size)\n    crop_size = np.minimum(crop_size, image_size - crop_center)\n    crop_size = np.maximum(crop_size, crop_size - crop_center)\n    crop_size = crop_size.astype(np.int32)\n    crop_center = crop_center.astype(np.int32)\n    crop_size = crop_size.astype(np.int32)\n    crop_center = crop_center.astype(np.int32)\n    crop_transform = T.CropTransform(crop_center, crop_size)\n    return crop_transform"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    if not isinstance(instance, dict):\n        raise TypeError(\n            \"Expected instance to be a dict, got {}\".format(type(instance))\n        )\n    if not \"bbox\" in instance:\n        raise KeyError('Instance must have key \"bbox\"')\n    if not \"bbox_mode\" in instance:\n        raise KeyError('Instance must have key \"bbox_mode\"')\n\n    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n    bbox = np.array(bbox, dtype=np.float32)\n    bbox_center = (bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2\n\n    crop_h, crop_w = crop_size\n    image_h, image_w = image_size\n\n    # crop_h, crop_w should be smaller than image_h, image_w\n    if crop_h > image_h or crop_w > image_w:\n        raise ValueError(\n            \"crop_size {} should be smaller than image_size {}\".format(\n                crop_size, image_size\n            )\n        )\n\n    # crop_h, crop_w should be smaller than bbox\n    if crop_h > bbox[3] or crop_w > bbox[2]:\n        raise ValueError(\n            \"crop_size {} should be smaller than bbox {}\".format(crop_size, bbox)\n        )\n\n    # crop_h, crop_w should be smaller than image_h - bbox_center[1]\n    if crop_h > image_h - bbox_center[1]:\n        crop_h = image_h - bbox_center[1]\n\n    # crop_w, crop_h should be smaller than image_w - bbox_center[0]\n    if crop_w > image_w - bbox_center[0]:\n        crop_w = image"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    if isinstance(instance, dict):\n        bbox = instance[\"bbox\"]\n        bbox_mode = instance[\"bbox_mode\"]\n    else:\n        bbox = instance.gt_boxes.tensor.numpy()[0]\n        bbox_mode = BoxMode.XYXY_ABS\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n    # Crop based on bbox center\n    crop_center = np.array([(bbox[0] + bbox[2]) / 2.0, (bbox[1] + bbox[3]) / 2.0])\n    crop_center = np.round(crop_center).astype(int)\n\n    # Crop size is not specified\n    if crop_size is None:\n        return T.CropTransform(0, 0, image_size[0], image_size[1])\n\n    # Crop size is specified\n    crop_size = np.array(crop_size)\n    crop_size = crop_size.astype(int)\n    crop_size = np.minimum(crop_size, image_size)\n\n    # Adjust crop size to ensure that the cropping region contains the center of the instance\n    crop_size = np.maximum(crop_size, crop_center[::-1] - crop_center[::-1] + bbox[::2])\n    crop_size = np.maximum(crop_size, bbox[2:] - bbox[:2] + crop_center[::-1] - crop_center[::-1])\n\n    # Adjust crop region to ensure that it does not exceed the image boundaries\n    crop_region = np.zeros_like(crop_size, dtype=np.int32)\n    crop_region[:2] = np.maximum(crop_center - crop_size // 2, 0)\n    crop_region[2:] = np.minimum(crop_region[:2] + crop_size, image_size)\n    crop_region[:2] = np."}
{"namespace": "ref_utils.l2_normalize", "completion": "  squared_norm = jnp.sum(x**2, axis=-1, keepdims=True)\n  safe_norm = jnp.sqrt(jnp.maximum(squared_norm, grad_eps**2))\n  return x / safe_norm\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  x_norm = jnp.sqrt(jnp.sum(x**2, axis=-1, keepdims=True))\n  x_norm_clamped = jnp.maximum(x_norm, grad_eps)\n  return x / x_norm_clamped"}
{"namespace": "ref_utils.l2_normalize", "completion": "  squared_norm = jnp.sum(x**2, axis=-1, keepdims=True)\n  normalized_x = x / jnp.sqrt(jnp.maximum(squared_norm, grad_eps**2))\n  return normalized_x"}
{"namespace": "ref_utils.l2_normalize", "completion": "  eps = jnp.finfo(x.dtype).eps\n  norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  norm_clamped = jnp.maximum(norm, grad_eps)\n  return x / norm_clamped\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Calculate the squared norm of the input vector(s)\n  squared_norm = jnp.sum(x**2, axis=-1, keepdims=True)\n\n  # Clamp the squared norm to a minimum value during the forward pass\n  squared_norm_clamped = jnp.maximum(squared_norm, grad_eps)\n\n  # Calculate the denominator of the normalization\n  denominator = jnp.sqrt(squared_norm_clamped)\n\n  # Clamp the denominator to a minimum value during the backward pass\n  denominator_clamped = jnp.maximum(denominator, grad_eps)\n\n  # Normalize the input vector(s)\n  normalized_x = x / denominator_clamped\n\n  return normalized_x"}
{"namespace": "ref_utils.l2_normalize", "completion": "  eps = jnp.finfo(jnp.float32).eps\n  squared_norm = jnp.sum(x * x, axis=-1, keepdims=True)\n  safe_squared_norm = jnp.maximum(squared_norm, grad_eps)\n  safe_norm = jnp.sqrt(safe_squared_norm)\n  safe_norm = jnp.maximum(safe_norm, eps)\n  return x / safe_norm"}
{"namespace": "ref_utils.l2_normalize", "completion": "  squared_norm = jnp.sum(x**2, axis=-1, keepdims=True)\n  safe_norm = jnp.sqrt(jnp.maximum(squared_norm, grad_eps**2))\n  return x / safe_norm\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Calculate the squared norm of the input vector(s)\n  x_squared_norm = jnp.sum(x**2, axis=-1, keepdims=True)\n\n  # Clamp the squared norm to prevent exploding gradients during the backward pass\n  x_squared_norm = jnp.clip(x_squared_norm, a_min=grad_eps, a_max=None)\n\n  # Calculate the denominator of the normalization\n  denominator = jnp.sqrt(x_squared_norm)\n\n  # Clamp the denominator to prevent exploding gradients during the backward pass\n  denominator = jnp.clip(denominator, a_min=grad_eps, a_max=None)\n\n  # Normalize the input vector(s)\n  x_normalized = x / denominator\n\n  return x_normalized\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  norm = jnp.sqrt(jnp.sum(x**2, axis=-1, keepdims=True))\n  norm_clamped = jnp.maximum(norm, grad_eps)\n  return x / norm_clamped\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  norm_clamped = jnp.maximum(norm, grad_eps)\n  return x / norm_clamped\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Compute the squared norm of the input vector(s)\n  squared_norm = jnp.sum(x**2, axis=-1, keepdims=True)\n\n  # Compute the squared norm of the input vector(s) in the backward pass\n  squared_norm_grad = jnp.sum(jnp.square(x) * jnp.square(jnp.grad(squared_norm)))\n\n  # Compute the denominator in the backward pass\n  denominator_grad = 2.0 * jnp.sqrt(squared_norm_grad + grad_eps)\n\n  # Compute the denominator in the forward pass\n  denominator = jnp.sqrt(squared_norm + grad_eps)\n\n  # Normalize the input vector(s)\n  return x / denominator"}
{"namespace": "ref_utils.l2_normalize", "completion": "  norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  norm_clamped = jnp.maximum(norm, grad_eps)\n  return x / norm_clamped\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  norm_clamped = jnp.maximum(norm, grad_eps)\n  return x / norm_clamped\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  norm = jnp.sqrt(jnp.sum(x**2, axis=-1, keepdims=True))\n  norm = jnp.maximum(norm, grad_eps)\n  return x / norm\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  norm_clamped = jnp.maximum(norm, grad_eps)\n  return x / norm_clamped\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Compute the squared norm of the input vector(s)\n  squared_norm = jnp.sum(x**2, axis=-1, keepdims=True)\n\n  # Clamp the squared norm to a minimum value during the forward pass\n  clamped_norm = jnp.maximum(squared_norm, grad_eps)\n\n  # Compute the normalization factor\n  normalization_factor = jnp.sqrt(clamped_norm)\n\n  # Clamp the squared norm to a minimum value during the backward pass\n  clamped_norm = jnp.maximum(squared_norm, grad_eps**2)\n\n  # Compute the normalization factor for the backward pass\n  backward_normalization_factor = jnp.sqrt(clamped_norm)\n\n  # Compute the normalized vector(s)\n  normalized_x = x / normalization_factor\n\n  # Compute the gradient of the normalization factor with respect to the input\n  # vector(s)\n  grad_normalization_factor = (\n      0.5 * backward_normalization_factor / normalization_factor\n  )\n\n  # Compute the gradient of the normalized vector(s) with respect to the input\n  # vector(s)\n  grad_normalized_x = (\n      normalized_x / normalization_factor\n      - x * grad_normalization_factor / normalization_factor**2\n  )\n\n  # Return the normalized vector(s) and the gradients with respect to the input\n  # vector(s)\n  return normalized_x, grad_normalized_x"}
{"namespace": "ref_utils.l2_normalize", "completion": "  squared_norm = jnp.sum(x**2, axis=-1, keepdims=True)\n  norm = jnp.sqrt(squared_norm)\n  normalized = x / jnp.maximum(norm, grad_eps)\n  return normalized\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  squared_norm = jnp.sum(x * x, axis=-1, keepdims=True)\n  squared_norm_clamped = jnp.maximum(squared_norm, grad_eps**2)\n  inv_norm = jnp.sqrt(squared_norm_clamped)\n  return x / inv_norm\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Compute the squared norm of the input vector(s)\n  norm = jnp.sum(x**2, axis=-1, keepdims=True)\n\n  # Clamp the squared norm to a minimum value during both the forward and backward passes\n  norm_clamped = jnp.maximum(norm, grad_eps**2)\n\n  # Compute the normalization factor\n  norm_factor = jnp.sqrt(norm_clamped)\n\n  # Normalize the input vector(s)\n  x_normalized = x / norm_factor\n\n  return x_normalized\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Calculate the squared norm of x along the last axis.\n  x_norm = jnp.sum(x**2, axis=-1, keepdims=True)\n\n  # Clamp the squared norm to a minimum value to prevent exploding gradients\n  # during the backward pass.\n  x_norm = jnp.maximum(x_norm, grad_eps)\n\n  # Calculate the reciprocal of the square root of the squared norm.\n  x_rsqrt = 1.0 / jnp.sqrt(x_norm)\n\n  # Multiply the reciprocal of the square root by the input vector(s) to normalize.\n  x_normalized = x * x_rsqrt\n\n  # Return the normalized array.\n  return x_normalized\n\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name = agent_info.split(\":\")[0].strip()\n        input_text = agent_info.split(\":\")[1].strip() if len(agent_info.split(\":\")) > 1 else \"\"\n        return agent_name, input_text"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name, *input_text = agent_info.split(\":\")\n        return agent_name.strip(), \":\".join(input_text).strip()"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name = agent_info.split(\":\")[0]\n        input_text = agent_info.split(\":\")[1] if len(agent_info.split(\":\")) > 1 else \"\"\n        return agent_name, input_text"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name = agent_info.split(\":\")[0]\n        input_text = agent_info.split(\":\")[1] if \":\" in agent_info else \"\"\n        return agent_name, input_text"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name = agent_info.split(\":\")[0].strip()\n        input_text = agent_info.split(\":\")[1].strip() if len(agent_info.split(\":\")) > 1 else \"\"\n        return agent_name, input_text"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name = agent_info.split(\":\")[0].strip()\n        input_text = agent_info.split(\":\")[1].strip() if \":\" in agent_info else \"\"\n        return agent_name, input_text"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name = agent_info.split(\":\")[0]\n        if len(agent_info.split(\":\")) > 1:\n            input_text = agent_info.split(\":\")[1]\n        else:\n            input_text = \"\"\n        return agent_name, input_text"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name = agent_info.split(\":\")[0]\n        input_text = agent_info.split(\":\")[1] if len(agent_info.split(\":\")) > 1 else \"\"\n\n        return agent_name, input_text"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name = agent_info.split(\"]\")[0]\n        input_text = agent_info.split(\":\")[1].strip() if len(agent_info.split(\":\")) > 1 else \"\"\n        return agent_name, input_text"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0].split(\":\")\n        agent_name = agent_info[0]\n        input_text = agent_info[1] if len(agent_info) > 1 else \"\"\n        return agent_name, input_text"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name = agent_info.split(\"]\")[0]\n        input_text = agent_info.split(\":\")[1] if \":\" in agent_info else \"\"\n        return agent_name, input_text"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name, *input_text = agent_info.split(\":\")\n        return agent_name, \"\".join(input_text)"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_name = response.split(\"[\")[1].split(\"]\")[0]\n        input_text = response.split(\":\")[1].strip() if \":\" in response else \"\"\n        return agent_name, input_text"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info_start = response.index(\"Use Agent[\") + len(\"Use Agent[\")\n        agent_info_end = response.index(\"]\")\n        agent_info = response[agent_info_start:agent_info_end]\n        agent_name, _, input_text = agent_info.partition(\":\")\n        return agent_name, input_text.strip()"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name, *input_text = agent_info.split(\":\")\n        return agent_name, \":\".join(input_text) if input_text else \"\""}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        start_index = response.find(\"Use Agent[\") + len(\"Use Agent[\")\n        end_index = response.find(\"]\")\n        agent_info = response[start_index:end_index]\n        agent_name, _, input_text = agent_info.partition(\":\")\n        return agent_name, input_text.strip()"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info_start = response.find(\"Use Agent[\") + len(\"Use Agent[\")\n        agent_info_end = response.find(\"]\")\n        agent_info = response[agent_info_start:agent_info_end]\n        agent_name, *input_text = agent_info.split(\":\")\n        return agent_name, input_text[0] if input_text else \"\""}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split('Use Agent[')[1].split(']')[0]\n        agent_name = agent_info.split(']')[0]\n        input_text = agent_info.split(']')[1].split(':')[1].strip() if ':' in agent_info else ''\n        return agent_name, input_text"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info_start = response.find(\"Use Agent[\")\n        agent_info_end = response.find(\"]\")\n        agent_info = response[agent_info_start + 10:agent_info_end]\n        agent_name = agent_info.split(\":\")[0]\n        input_text = agent_info.split(\":\")[1] if len(agent_info.split(\":\")) > 1 else \"\"\n        return agent_name, input_text"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name = agent_info.split(\":\")[0]\n        if len(agent_info.split(\":\")) > 1:\n            input_text = agent_info.split(\":\")[1]\n        else:\n            input_text = \"\"\n\n        return agent_name, input_text\n\n    "}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [obj[\"bbox\"] for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if mask_format == \"polygon\":\n        segms = [obj.get(\"segmentation\", None) for obj in annos]\n        masks = PolygonMasks(segms)\n        masks = masks.to(dtype=torch.uint8)\n        target.gt_masks = masks\n    elif mask_format == \"bitmask\":\n        segms = [obj.get(\"segmentation\", None) for obj in annos]\n        masks = polygons_to_bitmask(segms, *image_size)\n        masks = BitMasks(masks)\n        target.gt_masks = masks\n    else:\n        raise ValueError(\"Unknown mask format: {}\".format(mask_format))\n\n    keypoints = [obj.get(\"keypoints\", None) for obj in annos]\n    keypoints = Keypoints(keypoints)\n    target.gt_keypoints = keypoints\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [obj[\"bbox\"] for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if mask_format == \"polygon\":\n        if \"segmentation\" in annos[0]:\n            segms = [obj[\"segmentation\"] for obj in annos]\n            target.gt_masks = PolygonMasks(segms)\n        elif \"masks\" in annos[0]:\n            masks = [obj[\"masks\"] for obj in annos]\n            target.gt_masks = BitMasks(masks)\n\n    if \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n        keypoints = Keypoints(keypoints)\n        target.gt_keypoints = keypoints\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [obj[\"bbox\"] for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if mask_format == \"polygon\":\n        segms = [obj.get(\"segmentation\", None) for obj in annos]\n        masks = PolygonMasks(segms)\n        masks = masks.to(dtype=torch.uint8)\n        target.gt_masks = masks\n    elif mask_format == \"bitmask\":\n        masks = [obj.get(\"segmentation\", None) for obj in annos]\n        masks = BitMasks(masks)\n        masks = masks.to(dtype=torch.bool)\n        target.gt_masks = masks\n    else:\n        raise ValueError(\"Unknown mask format: {}\".format(mask_format))\n\n    if \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n        keypoints = Keypoints(keypoints)\n        target.gt_keypoints = keypoints\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [obj[\"bbox\"] for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if mask_format == \"polygon\":\n        segms = [obj.get(\"segmentation\", None) for obj in annos]\n        masks = PolygonMasks(segms)\n        masks = masks.to(device=boxes.device)\n        target.gt_masks = masks\n    elif mask_format == \"bitmask\":\n        segms = [obj.get(\"segmentation\", None) for obj in annos]\n        masks = polygons_to_bitmask(segms, *image_size)\n        masks = BitMasks(masks)\n        masks = masks.to(device=boxes.device)\n        target.gt_masks = masks\n    else:\n        raise ValueError(\"Unknown mask format: {}!\".format(mask_format))\n\n    if \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n        keypoints = Keypoints(keypoints)\n        keypoints.clip(image_size)\n        target.gt_keypoints = keypoints\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [obj[\"bbox\"] for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if mask_format == \"polygon\":\n        if \"segmentation\" in annos[0]:\n            segms = [obj[\"segmentation\"] for obj in annos]\n            target.gt_masks = PolygonMasks(segms)\n        else:\n            target.gt_masks = PolygonMasks([])\n    elif mask_format == \"bitmask\":\n        masks = [obj[\"segmentation\"] for obj in annos]\n        masks = [\n            polygons_to_bitmask(segm, *image_size)\n            for segm in masks\n            if isinstance(segm, list)\n        ]\n        masks = BitMasks(masks)\n        target.gt_masks = masks\n    else:\n        raise ValueError(\"Unknown mask format: {}!\".format(mask_format))\n\n    if \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n        keypoints = Keypoints(keypoints)\n        target.gt_keypoints = keypoints\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    if not annos:\n        return Instances(image_size)\n\n    boxes = [obj[\"bbox\"] for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if mask_format == \"polygon\":\n        if \"segmentation\" in annos[0]:\n            segms = [obj[\"segmentation\"] for obj in annos]\n            target.gt_masks = PolygonMasks(segms)\n        elif \"mask\" in annos[0]:\n            masks = [obj[\"mask\"] for obj in annos]\n            target.gt_masks = BitMasks(masks)\n    elif mask_format == \"bitmask\":\n        masks = [obj[\"mask\"] for obj in annos]\n        target.gt_masks = BitMasks(masks)\n    else:\n        raise ValueError(\"Unknown mask format: {}\".format(mask_format))\n\n    if \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n        keypoints = Keypoints(keypoints)\n        target.gt_keypoints = keypoints\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [obj[\"bbox\"] for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if mask_format == \"polygon\":\n        segms = [obj.get(\"segmentation\", None) for obj in annos]\n        target.gt_masks = PolygonMasks(segms)\n    elif mask_format == \"bitmask\":\n        masks = [obj.get(\"segmentation\", None) for obj in annos]\n        masks = [m.get(\"counts\", None) for m in masks]\n        masks = [mask_util.decode(m) for m in masks]\n        masks = [m.reshape(-1) for m in masks]\n        masks = BitMasks(masks)\n        masks = polygons_to_bitmask(masks.polygons, image_size)\n        target.gt_masks = masks\n    else:\n        raise ValueError(\"Unknown mask format: {}!\".format(mask_format))\n\n    if \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n        keypoints = [Keypoints(k) for k in keypoints]\n        target.gt_keypoints = keypoints\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [obj[\"bbox\"] for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if mask_format == \"polygon\":\n        segms = [obj.get(\"segmentation\", None) for obj in annos]\n        masks = PolygonMasks(segms)\n        masks = masks.to(device=boxes.tensor.device)\n        target.gt_masks = masks\n    elif mask_format == \"bitmask\":\n        masks = [obj.get(\"segmentation\", None) for obj in annos]\n        masks = [\n            polygons_to_bitmask(segm, image_size[0], image_size[1])\n            for segm in masks\n            if segm is not None\n        ]\n        masks = BitMasks(masks)\n        masks = masks.to(device=boxes.tensor.device)\n        target.gt_masks = masks\n    else:\n        raise ValueError(\"Unknown mask format: {}\".format(mask_format))\n\n    keypoints = None\n    if \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n        keypoints = Keypoints(keypoints)\n        keypoints.clip(image_size)\n        target.gt_keypoints = keypoints\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [obj[\"bbox\"] for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if \"segmentation\" in annos[0]:\n        if mask_format == \"polygon\":\n            segms = [obj[\"segmentation\"] for obj in annos]\n            target.gt_masks = PolygonMasks(segms)\n        elif mask_format == \"bitmask\":\n            masks = [obj[\"segmentation\"] for obj in annos]\n            masks = [\n                polygons_to_bitmask(segm, *image_size) for segm in masks\n            ]\n            target.gt_masks = BitMasks(masks)\n        else:\n            raise ValueError(\"Unknown mask format: {}\".format(mask_format))\n\n    if \"keypoints\" in annos[0]:\n        kpts = [obj[\"keypoints\"] for obj in annos]\n        kpts = [Keypoints(kpt, image_size) for kpt in kpts]\n        target.gt_keypoints = kpts\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [obj[\"bbox\"] for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if \"segmentation\" in annos[0]:\n        if mask_format == \"polygon\":\n            segms = [obj[\"segmentation\"] for obj in annos]\n            target.gt_masks = PolygonMasks(segms)\n        elif mask_format == \"bitmask\":\n            masks = [obj[\"segmentation\"] for obj in annos]\n            masks = [\n                mask_util.decode(mask) for mask in masks\n            ]  # decode RLE to bitmask\n            masks = [\n                mask_util.encode(np.array(mask[:, :, None], order=\"F\", dtype=\"uint8\"))\n                for mask in masks\n            ]  # encode bitmask to RLE\n            target.gt_masks = BitMasks(masks)\n        else:\n            raise NotImplementedError(\"Unknown mask format: {}\".format(mask_format))\n\n    if \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n        keypoints = [Keypoints(k) for k in keypoints]\n        target.gt_keypoints = keypoints\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    if mask_format == \"polygon\":\n        masks = PolygonMasks(polygons_to_bitmask(annos[\"segmentation\"], *image_size))\n    elif mask_format == \"bitmask\":\n        masks = BitMasks(annos[\"segmentation\"])\n    else:\n        raise ValueError(\"Unknown mask format: {}\".format(mask_format))\n\n    keypoints = None\n    if \"keypoints\" in annos:\n        keypoints = Keypoints(annos[\"keypoints\"])\n\n    return Instances(image_size, **{\n        \"gt_boxes\": Boxes(annos[\"bbox\"]),\n        \"gt_classes\": torch.tensor([annos[\"category_id\"]]),\n        \"gt_masks\": masks,\n        \"gt_keypoints\": keypoints,\n    })"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [obj[\"bbox\"] for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if mask_format == \"polygon\":\n        if \"segmentation\" in annos[0]:\n            segms = [obj[\"segmentation\"] for obj in annos]\n            target.gt_masks = PolygonMasks(segms)\n        else:\n            target.gt_masks = None\n    elif mask_format == \"bitmask\":\n        masks = [obj[\"segmentation\"] for obj in annos]\n        masks = [\n            mask_util.decode(mask) for mask in masks\n        ]  # decode RLE to binary mask\n        masks = [\n            mask_util.encode(np.array(mask, dtype=np.uint8, order=\"F\"))\n            for mask in masks\n        ]  # encode binary mask to RLE\n        target.gt_masks = BitMasks(masks)\n    else:\n        raise ValueError(\"Unknown mask format: {}\".format(mask_format))\n\n    if \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n        keypoints = Keypoints(keypoints)\n        target.gt_keypoints = keypoints\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    if not annos:\n        return Instances(image_size)\n\n    boxes = [obj[\"bbox\"] for obj in annos]\n    target = Instances(image_size)\n    target.gt_boxes = Boxes(boxes)\n    target.gt_boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if mask_format == \"polygon\":\n        segms = [obj.get(\"segmentation\", None) for obj in annos]\n        masks = PolygonMasks(segms)\n        masks = masks.to(dtype=torch.uint8)\n        target.gt_masks = masks\n    elif mask_format == \"bitmask\":\n        segms = [obj.get(\"segmentation\", None) for obj in annos]\n        masks = polygons_to_bitmask(segms, image_size)\n        masks = BitMasks(masks)\n        target.gt_masks = masks\n    else:\n        raise ValueError(\"Unknown mask format: {}\".format(mask_format))\n\n    if \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n        keypoints = Keypoints(keypoints)\n        target.gt_keypoints = keypoints\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [obj[\"bbox\"] for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if annos and \"segmentation\" in annos[0]:\n        if mask_format == \"polygon\":\n            segms = [obj[\"segmentation\"] for obj in annos]\n            target.gt_masks = PolygonMasks(segms)\n        elif mask_format == \"bitmask\":\n            masks = [obj[\"segmentation\"] for obj in annos]\n            masks = [\n                mask_util.decode(mask) for mask in masks\n            ]  # decode RLEs to bitmasks\n            masks, _ = mask_util.polygon_to_bitmask(masks, image_size[0], image_size[1])\n            target.gt_masks = BitMasks(masks)\n        else:\n            raise ValueError(\"Unknown mask format: {}\".format(mask_format))\n\n    if annos and \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n        keypoints = Keypoints(keypoints)\n        target.gt_keypoints = keypoints\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [obj[\"bbox\"] for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if mask_format == \"polygon\":\n        segms = [obj.get(\"segmentation\", None) for obj in annos]\n        masks = PolygonMasks(segms)\n        masks = masks.to(device=classes.device, dtype=torch.bool)\n        target.gt_masks = masks\n    elif mask_format == \"bitmask\":\n        masks = [obj.get(\"segmentation\", None) for obj in annos]\n        masks = [\n            polygons_to_bitmask(mask, image_size[0], image_size[1])\n            for mask in masks\n            if mask is not None\n        ]\n        masks = BitMasks(masks)\n        masks = masks.to(device=classes.device, dtype=torch.bool)\n        target.gt_masks = masks\n    else:\n        raise ValueError(\n            \"Only 'polygon' and 'bitmask' are supported as mask_format.\"\n            f\" Got '{mask_format}' instead.\"\n        )\n\n    if \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n        keypoints = Keypoints(keypoints)\n        target.gt_keypoints = keypoints\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [obj[\"bbox\"] for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if mask_format == \"polygon\":\n        if \"segmentation\" in annos[0]:\n            segms = [obj[\"segmentation\"] for obj in annos]\n            target.gt_masks = PolygonMasks(segms)\n        else:\n            target.gt_masks = PolygonMasks([])\n    elif mask_format == \"bitmask\":\n        masks = [obj[\"segmentation\"] for obj in annos]\n        masks = [\n            mask_util.decode(mask) for mask in masks\n        ]  # decode RLEs to bitmasks\n        masks = [\n            mask_util.encode(np.array(mask[:, :, None], order=\"F\", dtype=\"uint8\"))\n            for mask in masks\n        ]  # encode bitmasks to RLEs\n        target.gt_masks = BitMasks(masks)\n    else:\n        raise ValueError(\"Unknown mask format: {}\".format(mask_format))\n\n    if \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n        keypoints = [\n            Keypoints(kp, keypoint_names=[\"x\", \"y\", \"vis\"]) for kp in keypoints\n        ]  # keypoints are in x,y,vis order\n        target.gt_keypoints = keypoints\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    assert isinstance(annos, list), annos\n    assert isinstance(image_size, tuple), image_size\n    assert mask_format in [\"polygon\", \"bitmask\"], mask_format\n\n    if len(annos) == 0:\n        return None\n\n    boxes = [obj[\"bbox\"] for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if mask_format == \"polygon\":\n        segms = [obj.get(\"segmentation\", None) for obj in annos]\n        masks = PolygonMasks(segms)\n        masks = masks.to(device=classes.device)\n        target.gt_masks = masks\n    elif mask_format == \"bitmask\":\n        masks = [obj.get(\"segmentation\", None) for obj in annos]\n        masks = [\n            polygons_to_bitmask(mask, image_size[0], image_size[1])\n            for mask in masks\n            if mask is not None\n        ]\n        masks = BitMasks(masks)\n        masks = masks.to(device=classes.device)\n        target.gt_masks = masks\n\n    keypoints = [obj.get(\"keypoints\", None) for obj in annos]\n    keypoints = [k for k in keypoints if k is not None]\n    keypoints = Keypoints(keypoints)\n    target.gt_keypoints = keypoints\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    # Check that annotations are valid\n    assert isinstance(annos, list), \"annos must be a list\"\n    assert all(isinstance(anno, dict) for anno in annos), \"annos must be a list of dicts\"\n    assert all(\n        \"bbox\" in anno for anno in annos\n    ), \"annos must have 'bbox' field for each annotation\"\n    assert all(\n        \"category_id\" in anno for anno in annos\n    ), \"annos must have 'category_id' field for each annotation\"\n\n    # Check that image size is valid\n    assert isinstance(image_size, tuple) and len(image_size) == 2, \"image_size must be a tuple\"\n    assert all(\n        isinstance(dim, int) and dim > 0 for dim in image_size\n    ), \"image_size must be a tuple of positive integers\"\n\n    # Check that mask format is valid\n    assert mask_format in [\"polygon\", \"bitmask\"], \"mask_format must be 'polygon' or 'bitmask'\"\n\n    # Convert annotations to Instances object\n    boxes = [obj[\"bbox\"] for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if \"segmentation\" in annos[0]:\n        masks = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(masks)\n        else:\n            masks = BitMasks(polygons_to_bitmask(masks, image_size))\n        target.gt_masks = masks\n\n    if \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in ann"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    # Convert the annotations into a list of instances.\n    instances = []\n    for anno in annos:\n        # Skip annotations that are empty (usually due to crowd annotations).\n        if anno.get(\"iscrowd\", 0) > 0:\n            continue\n        # Convert the segmentation annotations from polygons to bitmasks.\n        if mask_format == \"polygon\":\n            if \"segmentation\" in anno:\n                segm = anno[\"segmentation\"]\n                if isinstance(segm, list):\n                    # polygon\n                    segm = PolygonMasks(segm)\n                else:\n                    # RLE\n                    segm = mask_util.decode(segm)\n                    segm = PolygonMasks(segm)\n            else:\n                segm = None\n        elif mask_format == \"bitmask\":\n            if \"segmentation\" in anno:\n                segm = anno[\"segmentation\"]\n                if isinstance(segm, list):\n                    # polygon\n                    segm = polygons_to_bitmask(segm, *image_size)\n                else:\n                    # RLE\n                    segm = mask_util.decode(segm)\n                    segm = polygons_to_bitmask(segm, *image_size)\n            else:\n                segm = None\n        else:\n            raise ValueError(\"Unknown mask format: {}\".format(mask_format))\n        # Create the instance object.\n        instance = Instances.from_boxes(\n            Boxes(anno[\"bbox\"]),\n            image_size=image_size,\n            # Add a field that stores the original annotations for this instance,\n            # so that users can access them via `get_fields`.\n            annotations=anno,\n        )\n        if segm is not None:\n            if mask_format == \"polygon\":\n                instance.gt_masks = segm\n            elif mask_format == \"bitmask\":\n                instance.gt_masks = BitMasks(segm)\n        if \"keypoints\" in anno:\n            instance.gt_"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    if mask_format == \"polygon\":\n        masks = [obj[\"segmentation\"] for obj in annos]\n        # In case of polygons, some instances might have multiple\n        # annotations (e.g., one instance might have two masks,\n        # each designated for a different part of the object).\n        # In this case, we merge them into a single mask,\n        # treating the first instance annotation as \"canonical\"\n        # (i.e., referring to the class_id, mask, etc.).\n        if len(masks) > 1:\n            masks = [\n                mask_util.merge(masks)\n            ]  # pycocotools's merge works for a list of masks\n    elif mask_format == \"bitmask\":\n        masks = [obj[\"segmentation\"] for obj in annos]\n    else:\n        raise ValueError(\n            \"Only polygon and bitmask segmentation are supported for now, \"\n            \"provided {}.\".format(mask_format)\n        )\n\n    # filter out invalid polygons\n    valid_masks = [\n        m for m in masks if m.count_nonzero() != 0\n    ]  # valid polygons have at least one pixel inside\n    if len(valid_masks) != len(masks):\n        logger = logging.getLogger(__name__)\n        logger.warning(\n            \"Filtered out %d invalid polygons in your dataset (<=0 pixels in mask).\"\n            % (len(masks) - len(valid_masks))\n        )\n    masks = valid_masks\n\n    # if there are no valid masks (e.g., all masks have zero area),\n    # we make a fake mask that consists of ones\n    if len(masks) == 0:\n        masks = [np.ones(image_size[::-1], dtype=np.uint8)]\n\n    if mask_format == \"polygon\":\n        masks = [\n            p if isinstance(p, PolygonMasks) else PolygonMasks(p)\n            for p in masks\n        ]  #"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\"))\n    data_home = os.path.expanduser(data_home)\n    os.makedirs(data_home, exist_ok=True)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\"))\n    data_home = os.path.expanduser(data_home)\n    os.makedirs(data_home, exist_ok=True)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\"))\n    data_home = os.path.expanduser(data_home)\n    os.makedirs(data_home, exist_ok=True)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\"))\n    data_home = os.path.expanduser(data_home)\n    os.makedirs(data_home, exist_ok=True)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\"))\n    data_home = os.path.expanduser(data_home)\n    os.makedirs(data_home, exist_ok=True)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\"))\n    data_home = Path(data_home).expanduser().absolute()\n    os.makedirs(data_home, exist_ok=True)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\"))\n    data_home = str(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\"))\n    data_home = Path(data_home).expanduser().absolute()\n    data_home.mkdir(parents=True, exist_ok=True)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\"))\n    data_home = Path(data_home).expanduser().absolute()\n    data_home.mkdir(parents=True, exist_ok=True)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\"))\n    data_home = str(Path(data_home).expanduser().absolute())\n    os.makedirs(data_home, exist_ok=True)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\"))\n    data_home = str(Path(data_home).expanduser().absolute())\n    os.makedirs(data_home, exist_ok=True)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\"))\n    data_home = str(Path(data_home).expanduser().absolute())\n    os.makedirs(data_home, exist_ok=True)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\"))\n    data_home = Path(data_home).expanduser().absolute()\n    data_home.mkdir(parents=True, exist_ok=True)\n    return str(data_home)"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\"))\n    data_home = Path(data_home).expanduser().absolute()\n    os.makedirs(data_home, exist_ok=True)\n    return str(data_home)"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    else:\n        data_home = Path(data_home)\n\n    data_home.mkdir(parents=True, exist_ok=True)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    else:\n        data_home = Path(data_home)\n\n    data_home.mkdir(parents=True, exist_ok=True)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\"))\n    data_home = Path(data_home).expanduser().absolute()\n    os.makedirs(data_home, exist_ok=True)\n    return str(data_home)"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", \"~/skfolio_data\")\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", \"~/skfolio_data\")\n    data_home = str(data_home)\n    if data_home == \"\":\n        raise OSError(\n            \"Invalid value for SKFOLIO_DATA: empty string or None. \"\n            \"The environment variable is used to set the path to the skfolio data directory \"\n            \"and it cannot be an empty string or None.\"\n        )\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert_is_square(cov)\n    assert_is_symmetric(cov)\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert_is_square(cov)\n    assert_is_symmetric(cov)\n\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert_is_square(cov)\n    assert_is_symmetric(cov)\n    std = np.sqrt(np.diag(cov))\n    corr = cov / std[:, None] / std[None, :]\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert_is_square(cov)\n    assert_is_symmetric(cov)\n\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert_is_square(cov)\n    assert_is_symmetric(cov)\n\n    std = np.sqrt(np.diag(cov))\n    corr = cov / std[:, None] / std[None, :]\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert_is_square(cov)\n    assert_is_symmetric(cov)\n    std = np.sqrt(np.diag(cov))\n    corr = cov / std / std[:, None]\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    std = np.sqrt(np.diag(cov))\n    corr = cov / std / std[:, None]\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert_is_square(cov)\n    assert_is_symmetric(cov)\n\n    std = np.sqrt(np.diag(cov))\n    corr = cov / (std * std[:, None])\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert_is_square(cov)\n    assert_is_symmetric(cov)\n    std = np.sqrt(np.diag(cov))\n    corr = cov / (std * std[:, None])\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    assert_is_square(cov)\n    assert_is_symmetric(cov)\n    std = np.sqrt(np.diag(cov))\n    corr = cov / std / std[:, None]\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    if not np.allclose(cov, cov.T):\n        raise ValueError(\"`cov` must be symmetric\")\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(\"The matrix must be 2D\")\n    if cov.shape[0] != cov.shape[1]:\n        raise ValueError(\"The matrix must be square\")\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert_is_symmetric(cov)\n    std = np.sqrt(np.diag(cov))\n    corr = cov / (std * std[:, None])\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(\"`cov` must be a 2D array\")\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(\"`cov` must be a 2D array\")\n    if cov.shape[0] != cov.shape[1]:\n        raise ValueError(\"`cov` must be a square matrix\")\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    if not np.allclose(cov, cov.T):\n        raise ValueError(\"The covariance matrix must be symmetric\")\n    std = np.sqrt(np.diag(cov))\n    corr = cov / std / std[:, None]\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert_is_square(cov)\n    assert_is_symmetric(cov)\n\n    # Around 100 times faster than checking for positive eigenvalues with np.linalg.eigh\n    if is_cholesky_dec(cov) and is_positive_definite(cov):\n        return cov, np.sqrt(np.diag(cov))\n\n    eig_vals, eig_vecs = np.linalg.eigh(cov)\n    # Clipping the eigenvalues with a value smaller than 1e-13 can cause scipy to\n    # consider the matrix non-psd is some corner cases (see test/test_stats.py)\n    corr = eig_vecs * np.maximum(eig_vals, _CLIPPING_VALUE) @ eig_vecs.T\n    std = np.sqrt(np.diag(cov))\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    if not np.allclose(cov, cov.T):\n        raise ValueError(\"`cov` must be a symmetric matrix\")\n    if not np.allclose(np.diag(cov), np.zeros(cov.shape[0]), atol=1e-5):\n        raise ValueError(\"`cov` must be a covariance matrix\")\n\n    std = np.sqrt(np.diag(cov))\n    corr = cov / (std * std[:, None])\n    return corr, std"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    \"\"\"\n    This function temporarily sets the \"training\" attribute of every submodule in a given model to a constant value, allowing for optimization by meta-compilation. It uses a context manager to ensure that these changes are reverted back to their original state after the context manager exits.\n    Input-Output Arguments\n    :param model: The model whose submodules' \"training\" attributes are to be temporarily annotated as constants. It is used to iterate through all submodules and modify their class definitions.\n    :return: No return values. This function operates by side effects, modifying the class definitions of the model's submodules within the context.\n    \"\"\"\n\n    \"\"\"\n    This function temporarily sets the \"training\" attribute of every submodule in a given model to a constant value, allowing for optimization by meta-compilation. It uses a context manager to ensure that these changes are reverted back to their original state after the context manager exits.\n    Input-Output Arguments\n    :param model: The model whose submodules' \"training\" attributes are to be temporarily annotated as constants. It is used to iterate through all submodules and modify their class definitions.\n    :return: No return values. This function operates by side effects, modifying the class definitions of the model's submodules within the context.\n    \"\"\"\n\n    \"\"\"\n    This function temporarily sets the \"training\" attribute of every submodule in a given model to a constant value, allowing for optimization by meta-compilation. It uses a context manager to ensure that these changes are reverted back to their original state after the context manager exits.\n    Input-Output Arguments\n    :param model: The model whose submodules' \"training\" attributes are to be temporarily annotated as constants. It is used to iterate through all submodules and modify their class definitions.\n    :return: No return values. This function operates by side effects, modifying the class definitions of the model's submodules within the context.\n    \"\"\"\n\n    \"\"\"\n    This function temporarily sets the \"training\" attribute of every submodule in a given model to a constant value, allowing for optimization by meta-compilation. It uses a context manager to ensure that these changes are reverted back to their original state after the context manager exits.\n    Input"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    \"\"\"\n    This function temporarily sets the \"training\" attribute of every submodule in a given model to a constant value, allowing for optimization by meta-compilation. It uses a context manager to ensure that these changes are reverted back to their original state after the context manager exits.\n    Input-Output Arguments\n    :param model: The model whose submodules' \"training\" attributes are to be temporarily annotated as constants. It is used to iterate through all submodules and modify their class definitions.\n    :return: No return values. This function operates by side effects, modifying the class definitions of the model's submodules within the context.\n    \"\"\"\n\n    \"\"\"\n    This function temporarily sets the \"training\" attribute of every submodule in a given model to a constant value, allowing for optimization by meta-compilation. It uses a context manager to ensure that these changes are reverted back to their original state after the context manager exits.\n    Input-Output Arguments\n    :param model: The model whose submodules' \"training\" attributes are to be temporarily annotated as constants. It is used to iterate through all submodules and modify their class definitions.\n    :return: No return values. This function operates by side effects, modifying the class definitions of the model's submodules within the context.\n    \"\"\"\n\n    \"\"\"\n    This function temporarily sets the \"training\" attribute of every submodule in a given model to a constant value, allowing for optimization by meta-compilation. It uses a context manager to ensure that these changes are reverted back to their original state after the context manager exits.\n    Input-Output Arguments\n    :param model: The model whose submodules' \"training\" attributes are to be temporarily annotated as constants. It is used to iterate through all submodules and modify their class definitions.\n    :return: No return values. This function operates by side effects, modifying the class definitions of the model's submodules within the context.\n    \"\"\"\n\n    \"\"\"\n    This function temporarily sets the \"training\" attribute of every submodule in a given model to a constant value, allowing for optimization by meta-compilation. It uses a context manager to ensure that these changes are reverted back to their original state after the context manager exits.\n    Input"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # "}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # This function temporarily sets the \"training\" attribute of every submodule in a given model to a constant value, allowing for optimization by meta-compilation.\n    # It uses a context manager to ensure that these changes are reverted back to their original state after the context manager exits.\n    # Input-Output Arguments\n    # :param model: The model whose submodules' \"training\" attributes are to be temporarily annotated as constants. It is used to iterate through all submodules and modify their class definitions.\n    # :return: No return values. This function operates by side effects, modifying the class definitions of the model's submodules within the context.\n    "}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # TODO: find a more automatic way to enable import of other classes\n    s = \"\"\""}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO: add a test case for this\n    # TODO:"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # Iterate through all submodules and modify their class definitions\n    for name, module in model.named_modules():\n        if hasattr(module, \"__annotations__\"):\n            module.__annotations__[\"training\"] = torch.jit.Final[bool]\n\n    # Return a context manager that reverts the changes made to the class definitions\n    yield\n\n    # Revert the changes made to the class definitions\n    for name, module in model.named_modules():\n        if hasattr(module, \"__annotations__\"):\n            del module.__annotations__[\"training\"]"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    def _freeze_training_mode(mod):\n        if hasattr(mod, \"training\"):\n            mod.training = torch.jit.final(mod.training)\n\n    with torch.jit.optimized_execution(True):\n        model.apply(_freeze_training_mode)\n        yield\n        model.apply(_freeze_training_mode)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    from detectron2.modeling.meta_arch import GeneralizedRCNN\n    from detectron2.modeling.meta_arch import MaskRCNN\n    from detectron2.modeling.meta_arch import RetinaNet\n    from detectron2.modeling.meta_arch import ProposalNetwork\n    from detectron2.modeling.meta_arch import KeypointRCNN\n    from detectron2.modeling.meta_arch import CascadeRCNN\n    from detectron2.modeling.meta_arch import PanopticFPN\n\n    from detectron2.modeling.meta_arch import GeneralizedRCNN\n    from detectron2.modeling.meta_arch import MaskRCNN\n    from detectron2.modeling.meta_arch import RetinaNet\n    from detectron2.modeling.meta_arch import ProposalNetwork\n    from detectron2.modeling.meta_arch import KeypointRCNN\n    from detectron2.modeling.meta_arch import CascadeRCNN\n    from detectron2.modeling.meta_arch import PanopticFPN\n\n    from detectron2.modeling.meta_arch import GeneralizedRCNN\n    from detectron2.modeling.meta_arch import MaskRCNN\n    from detectron2.modeling.meta_arch import RetinaNet\n    from detectron2.modeling.meta_arch import ProposalNetwork\n    from detectron2.modeling.meta_arch import KeypointRCNN\n    from detectron2.modeling.meta_arch import CascadeRCNN\n    from detectron2.modeling.meta_arch import PanopticFPN\n\n    from detectron2.modeling.meta_arch import GeneralizedRCNN\n    from detectron2.modeling.meta_arch import MaskRCNN\n    from detectron2.modeling.meta_arch import RetinaNet\n    from detectron2.modeling.meta_arch import ProposalNetwork\n    from detectron2.modeling.meta_arch import KeypointRCNN\n    from detectron2.modeling.meta_arch import CascadeRCNN\n    from detect"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    from detectron2.modeling.meta_arch.build import META_ARCH_REGISTRY\n\n    def _freeze_training_mode(self):\n        self.training = torch.jit.final(self.training)\n\n    for name, cls in META_ARCH_REGISTRY.items():\n        cls.__setattr__ = _freeze_training_mode\n    yield\n    for name, cls in META_ARCH_REGISTRY.items():\n        delattr(cls, \"__setattr__\")\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    \"\"\"\n    This function temporarily sets the \"training\" attribute of every submodule in a given model to a constant value, allowing for optimization by meta-compilation. It uses a context manager to ensure that these changes are reverted back to their original state after the context manager exits.\n    Input-Output Arguments\n    :param model: The model whose submodules' \"training\" attributes are to be temporarily annotated as constants. It is used to iterate through all submodules and modify their class definitions.\n    :return: No return values. This function operates by side effects, modifying the class definitions of the model's submodules within the context.\n    \"\"\"\n    from detectron2.modeling.meta_arch import GeneralizedRCNN\n\n    if isinstance(model, GeneralizedRCNN):\n        model.roi_heads.freeze_training_mode()\n\n    from detectron2.modeling.meta_arch import MaskRCNN\n\n    if isinstance(model, MaskRCNN):\n        model.roi_heads.freeze_training_mode()\n\n    from detectron2.modeling.meta_arch import KeypointRCNN\n\n    if isinstance(model, KeypointRCNN):\n        model.roi_heads.freeze_training_mode()\n\n    from detectron2.modeling.meta_arch import RetinaNet\n\n    if isinstance(model, RetinaNet):\n        model.head.freeze_training_mode()\n\n    from detectron2.modeling.meta_arch import ProposalNetwork\n\n    if isinstance(model, ProposalNetwork):\n        model.head.freeze_training_mode()\n\n    from detectron2.modeling.meta_arch import CascadeRCNN\n\n    if isinstance(model, CascadeRCNN):\n        for head in model.heads:\n            head.freeze_training_mode()\n\n    from detectron2.modeling.meta_arch import CascadeMaskRCNN\n\n    if isinstance(model, CascadeMaskRCNN):\n        for head in model.heads:\n            head.freeze_training_mode()"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # Iterate through all submodules in the model\n    for name, module in model.named_modules():\n        # Check if the module has a \"training\" attribute\n        if hasattr(module, \"training\"):\n            # Create a new class definition for the module with the \"training\" attribute annotated as a constant\n            new_class = type(module.__class__.__name__, (module.__class__,), {\"training\": torch.jit.Final[bool]})\n            # Replace the module's class with the new class definition\n            setattr(model, name, new_class())\n    # Return the context manager that reverts the changes made to the model's submodules\n    yield\n    # Iterate through all submodules in the model\n    for name, module in model.named_modules():\n        # Check if the module has a \"training\" attribute\n        if hasattr(module, \"training\"):\n            # Revert the changes made to the module's class definition\n            setattr(model, name, module.__class__())\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # This function temporarily sets the \"training\" attribute of every submodule in a given model to a constant value, allowing for optimization by meta-compilation. It uses a context manager to ensure that these changes are reverted back to their original state after the context manager exits.\n    # Input-Output Arguments\n    # :param model: The model whose submodules' \"training\" attributes are to be temporarily annotated as constants. It is used to iterate through all submodules and modify their class definitions.\n    # :return: No return values. This function operates by side effects, modifying the class definitions of the model's submodules within the context.\n\n    # Iterate through all submodules of the model\n    for name, module in model.named_modules():\n        # Check if the module has a \"training\" attribute\n        if hasattr(module, \"training\"):\n            # Get the original value of the \"training\" attribute\n            original_training_value = getattr(module, \"training\")\n            # Define a new class for the module with a constant \"training\" attribute\n            module.__class__ = type(\n                module.__class__.__name__,\n                (module.__class__,),\n                {\"training\": torch.jit.final(original_training_value)},\n            )\n\n    # Yield the modified model to the context manager\n    yield model\n\n    # Revert the changes made to the model's submodules\n    for name, module in model.named_modules():\n        if hasattr(module, \"training\"):\n            module.__class__ = type(\n                module.__class__.__name__,\n                (module.__class__,),\n                {\"training\": original_training_value},\n            )\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    def _annotate_training_mode(module):\n        \"\"\"\n        This function is a helper function that annotates the \"training\" attribute of a given module as a constant value. It is used within the context manager to modify the class definitions of the submodules of the input model.\n        Input-Output Arguments\n        :param module: The module whose \"training\" attribute is to be annotated as a constant value. It is used to modify the class definition of the module.\n        :return: No return values. This function operates by side effects, modifying the class definition of the input module.\n        \"\"\"\n        if hasattr(module, \"training\"):\n            module.__annotations__[\"training\"] = torch.jit.Final[bool]\n\n    with torch.jit.annotate_module(model):\n        model.apply(_annotate_training_mode)\n        yield\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    from detectron2.modeling.meta_arch import GeneralizedRCNN\n    from detectron2.modeling.meta_arch.build import META_ARCH_REGISTRY\n\n    # Freeze the training mode of the model\n    model.training = torch.jit.final(model.training)\n\n    # Freeze the training mode of the meta arch\n    if isinstance(model, GeneralizedRCNN):\n        model.roi_heads.training = torch.jit.final(model.roi_heads.training)\n\n    # Freeze the training mode of the meta arch registry\n    for name, meta_arch in META_ARCH_REGISTRY.items():\n        if issubclass(meta_arch, GeneralizedRCNN):\n            META_ARCH_REGISTRY[name].roi_heads.training = torch.jit.final(\n                META_ARCH_REGISTRY[name].roi_heads.training\n            )\n\n    try:\n        yield\n    finally:\n        # Unfreeze the training mode of the model\n        model.training = False\n\n        # Unfreeze the training mode of the meta arch\n        if isinstance(model, GeneralizedRCNN):\n            model.roi_heads.training = True\n\n        # Unfreeze the training mode of the meta arch registry\n        for name, meta_arch in META_ARCH_REGISTRY.items():\n            if issubclass(meta_arch, GeneralizedRCNN):\n                META_ARCH_REGISTRY[name].roi_heads.training = True"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    from detectron2.modeling import META_ARCH_REGISTRY\n\n    for name, m in model.named_modules():\n        if name in META_ARCH_REGISTRY:\n            # skip meta arch\n            continue\n        if hasattr(m, \"training\"):\n            if torch.jit.is_scripting():\n                m.training = torch.jit.final(m.training)\n            else:\n                m.training = torch.jit.Attribute(m.training, torch.jit.Final[bool])\n    "}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    from detectron2.modeling.meta_arch.build import META_ARCH_REGISTRY\n\n    # Iterate through all submodules of the model and modify their class definitions\n    for name, module in model.named_modules():\n        if hasattr(module, \"training\"):\n            # If the module has a \"training\" attribute, temporarily set it as a constant\n            module.training = torch.jit.Final[module.training]\n\n    # Iterate through all meta-architectures in the registry and modify their class definitions\n    for name, cls in META_ARCH_REGISTRY.items():\n        if hasattr(cls, \"training\"):\n            # If the meta-architecture has a \"training\" attribute, temporarily set it as a constant\n            cls.training = torch.jit.Final[cls.training]\n\n    # Return the modified model\n    yield model"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # TODO: This is a workaround for a bug in torch.jit.script.\n    #       It should be removed once the bug is fixed.\n    from detectron2.modeling.meta_arch import GeneralizedRCNN\n\n    GeneralizedRCNN.__annotations__[\"training\"] = torch.jit.Final[bool]\n\n    from detectron2.modeling.meta_arch import MaskRCNN\n\n    MaskRCNN.__annotations__[\"training\"] = torch.jit.Final[bool]\n\n    from detectron2.modeling.meta_arch import KeypointRCNN\n\n    KeypointRCNN.__annotations__[\"training\"] = torch.jit.Final[bool]\n\n    from detectron2.modeling.meta_arch import RetinaNet\n\n    RetinaNet.__annotations__[\"training\"] = torch.jit.Final[bool]\n\n    from detectron2.modeling.meta_arch import ProposalNetwork\n\n    ProposalNetwork.__annotations__[\"training\"] = torch.jit.Final[bool]\n\n    from detectron2.modeling.meta_arch import RetinaNet\n\n    RetinaNet.__annotations__[\"training\"] = torch.jit.Final[bool]\n\n    from detectron2.modeling.meta_arch import CascadeRCNN\n\n    CascadeRCNN.__annotations__[\"training\"] = torch.jit.Final[bool]\n\n    from detectron2.modeling.meta_arch import PanopticFPN\n\n    PanopticFPN.__annotations__[\"training\"] = torch.jit.Final[bool]\n\n    from detectron2.modeling.meta_arch import SemanticSegmentor\n\n    SemanticSegmentor.__annotations__[\"training\"] = torch.jit.Final[bool]\n\n    from detectron2.modeling.meta_arch import TwoStageDetector\n\n    TwoStageDetector.__annotations__[\"training\"] = torch.jit.Final[bool]\n\n    from detectron2.modeling.meta_arch import SemanticSegmentor\n\n   "}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # TODO: find a more automatic way to enable import of other classes\n    s = \"\"\""}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # TODO: This is not a good solution, because it modifies the class definitions of the model.\n    #  Instead, we should modify the model's state_dict, but it is not possible to do so in a scripted model.\n    #  Another solution is to use a context manager to temporarily modify the model's state_dict, but this requires a lot of code duplication.\n    #  A better solution is to use a context manager to temporarily modify the model's class definitions, but this requires a lot of code duplication.\n    #  A better solution is to use a context manager to temporarily modify the model's class definitions, but this requires a lot of code duplication.\n    #  A better solution is to use a context manager to temporarily modify the model's class definitions, but this requires a lot of code duplication.\n    #  A better solution is to use a context manager to temporarily modify the model's class definitions, but this requires a lot of code duplication.\n    #  A better solution is to use a context manager to temporarily modify the model's class definitions, but this requires a lot of code duplication.\n    #  A better solution is to use a context manager to temporarily modify the model's class definitions, but this requires a lot of code duplication.\n    #  A better solution is to use a context manager to temporarily modify the model's class definitions, but this requires a lot of code duplication.\n    #  A better solution is to use a context manager to temporarily modify the model's class definitions, but this requires a lot of code duplication.\n    #  A better solution is to use a context manager to temporarily modify the model's class definitions, but this requires a lot of code duplication.\n    #  A better solution is to use a context manager to temporarily modify the model's class definitions, but this requires a lot of code duplication.\n    #  A better solution is to use a context manager to temporarily modify the model's class definitions, but this requires a lot of code duplication.\n    #  A better solution is to use a context manager to temporarily modify the model's class definitions, but this requires a lot of code duplication.\n    #  A better solution is to use a context manager to temporarily modify the"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if the shapes of field1 and field2 match.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1].shape} and {values[field2].shape}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1].shape} and {values[field2].shape}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if the shapes of field1 and field2 match.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if the shapes of field1 and field2 are equal.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1].shape} and {values[field2].shape}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(\n                    f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1][i].shape} and {values[field2][i].shape}\"\n                )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Check if the shapes of the two fields match.\n\n        :param cls: The class of the Pydantic model being validated.\n        :param values: A dictionary containing the values of the fields being validated.\n        :return: The dictionary of values, unchanged.\n        :raises ValueError: If the shapes of the two fields do not match.\n        \"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1].shape} and {values[field2].shape}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Check if the shapes of the two fields match.\n\n        :param cls: type, The class type of the Pydantic model.\n        :param values: Dict[str, np.ndarray], A dictionary containing the values of the fields being validated.\n        :return: Dict[str, np.ndarray], The validated values of the fields.\n        :raises ValueError: If the shapes of the two fields do not match.\n        \"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1].shape} and {values[field2].shape}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Check if the shapes of the two fields match.\n\n        :param cls: The class of the Pydantic model that the validator is being used in.\n        :param values: A dictionary containing the values of the fields being checked.\n        :return: The original values dictionary if the shapes match, otherwise raises a ValueError.\n        \"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Check if the shapes of two fields match.\n\n        :param cls: The class of the model containing the fields being compared.\n        :param values: A dictionary containing the values of the fields being compared.\n        :return: The dictionary of values if the shapes match, otherwise raises a ValueError.\n        :raises ValueError: If the shapes of the two fields do not match.\n        \"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Check if the shapes of two fields match.\n\n        This function is a validator for a Pydantic model. It takes in a dictionary of values and checks if the shapes of two fields match. If the shapes do not match, it raises a ValueError indicating the mismatch.\n\n        :param cls: The class of the Pydantic model being validated.\n        :param values: A dictionary of values, where the keys are the names of the fields being checked and the values are the corresponding data structures.\n        :return: The same dictionary of values, if the shapes match.\n        :raises ValueError: If the shapes of the two fields do not match, it raises a ValueError indicating the mismatch.\n        \"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1].shape} and {values[field2].shape}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Check if the shapes of field1 and field2 match.\n\n        :param cls: type, The class type of the Pydantic model.\n        :param values: Dict[str, np.ndarray], A dictionary containing the values of the fields specified in the model.\n        :return: Dict[str, np.ndarray], The modified values dictionary, which may contain additional validated data.\n        :raises ValueError: If the shapes of field1 and field2 do not match.\n        \"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Check if the shapes of field1 and field2 match.\n\n        :param cls: type, The class of the Pydantic model.\n        :param values: Dict[str, np.ndarray], The values of the fields within the Pydantic model.\n        :return: Dict[str, np.ndarray], The same values if the shapes match, otherwise it raises a ValueError.\n        \"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        This function is a validator function that checks if the shapes of two fields within a Pydantic model match.\n        It takes a dictionary of values as input and returns the same dictionary if the shapes match.\n        If the shapes do not match, it raises a ValueError indicating the mismatch.\n\n        :param cls: The class of the Pydantic model being validated.\n        :param values: A dictionary of values, where the keys are the names of the fields being compared and the values are the corresponding data structures.\n        :return: The same dictionary of values if the shapes match. If the shapes do not match, it raises a ValueError.\n        \"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1].shape} and {values[field2].shape}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Check if the shapes of two fields match.\n\n        This function is a validator for a Pydantic model. It compares the shapes of two fields within the model. If the shapes do not match, it raises a ValueError indicating the mismatch.\n\n        Input-Output Arguments\n        :param cls: type, The type of the Pydantic model.\n        :param values: Dict[str, np.ndarray], A dictionary containing the values of the fields to be compared. The keys of the dictionary should be the names of the fields, and the values should be the corresponding NumPy arrays.\n        :return: Dict[str, np.ndarray], The same dictionary of values as input, with no modifications.\n        \"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1].shape} and {values[field2].shape}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Check if the shapes of two fields match.\n\n        This function is a validator that is used within a Pydantic model to ensure that the shapes of two fields match. It takes in a dictionary of values and compares the shapes of the fields specified in the function arguments. If the shapes do not match, it raises a ValueError indicating the mismatch.\n\n        :param cls: The class of the Pydantic model.\n        :param values: Dict[str, np.ndarray], A dictionary containing the values of the fields specified in the function arguments.\n        :return: Dict[str, np.ndarray], The same dictionary of values that was passed in.\n        \"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1].shape} and {values[field2].shape}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Check if the shapes of two fields match within a Pydantic model.\n\n        This function is a Pydantic validator that takes in a dictionary of values and checks if the shapes of two specified fields match. The function raises a ValueError if the shapes do not match.\n\n        :param cls: The Pydantic model class.\n        :param values: A dictionary containing the values of the model fields.\n        :return: The dictionary of values, unchanged.\n        :raises ValueError: If the shapes of the two fields do not match.\n        \"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1].shape} and {values[field2].shape}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Check if the shapes of field1 and field2 match.\n\n        This function takes a dictionary of values as input, where field1 and field2 are expected to be present.\n        It checks if the shapes of the two fields match. If the shapes are not identical, it raises a ValueError\n        indicating the mismatch.\n\n        :param cls: type, The type of the model that is being validated.\n        :param values: Dict[str, np.ndarray], A dictionary containing the values of the model fields.\n        :return: Dict[str, np.ndarray], The same dictionary of values that was passed as input.\n        \"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Check if the shapes of the two fields match.\n\n        This function takes a dictionary of values and checks if the shapes of two fields match. The field names are specified as input arguments. If the shapes do not match, a ValueError is raised.\n\n        :param cls: The class of the Pydantic model being validated.\n        :param values: A dictionary of values, where the field names are the keys.\n        :return: The validated dictionary of values.\n        :raises ValueError: If the shapes of the two fields do not match.\n        \"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1].shape} and {values[field2].shape}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        This function is a Pydantic validator that checks if the shapes of two fields match.\n\n        Input-Output Arguments\n        :param cls: type, The type of the model that is being validated. It is used to retrieve the name of the model for error messages.\n        :param values: Dict[str, np.ndarray], A dictionary containing the values of the model fields. The function uses the field names specified in the input arguments to retrieve the corresponding values.\n        :return: Dict[str, np.ndarray], The original values dictionary, unchanged. This function does not modify the input values.\n        \"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1].shape} and {values[field2].shape}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n\n        \"\"\"\n        This function is the validator that performs the actual comparison of the shapes of the two fields. It takes in a dictionary of values, where the field names are used to retrieve the corresponding data structures. It checks if the lengths of the two fields are equal, and if any of the elements in field1 have a different shape than the corresponding element in field2. If any of these conditions is not met, it raises a ValueError with a descriptive message.\n\n        :param cls: type, The class type of the Pydantic model that contains the fields.\n        :param values: Dict[str, np.ndarray], A dictionary of values, where the keys are the field names and the values are the corresponding data structures.\n        :return: Dict[str, np.ndarray], The same dictionary of values that was passed as input.\n        \"\"\"\n\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for i, element in enumerate(values[field1]):\n            if element.shape != values[field2][i].shape:\n                raise ValueError(\n                    f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {element.shape} and {values[field2][i].shape}\"\n                )\n\n        return values\n\n    return __root_validator"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics[0], str):\n        return metrics, [{} for _ in metrics]\n    else:\n        return [metric['name'] for metric in metrics], metrics"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics[0], str):\n        return metrics, [{} for _ in range(len(metrics))]\n    else:\n        return [metric['name'] for metric in metrics], metrics"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list):\n        if all(isinstance(metric, str) for metric in metrics):\n            return metrics, [{} for _ in metrics]\n        elif all(isinstance(metric, dict) for metric in metrics):\n            return [metric['name'] for metric in metrics], metrics\n        else:\n            raise ValueError('Metrics must be either strings or dictionaries.')\n    else:\n        raise ValueError('Metrics must be a list.')"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics[0], str):\n        metric_names = metrics\n        metric_params = [{} for _ in range(len(metrics))]\n    else:\n        metric_names = [metric['name'] for metric in metrics]\n        metric_params = [deepcopy(metric) for metric in metrics]\n        for metric in metric_params:\n            del metric['name']\n\n    return metric_names, metric_params"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list):\n        if all(isinstance(metric, str) for metric in metrics):\n            return metrics, [{} for _ in metrics]\n        elif all(isinstance(metric, dict) for metric in metrics):\n            return [metric['name'] for metric in metrics], metrics\n        else:\n            raise ValueError('Metrics must be either all strings or all dictionaries.')\n    else:\n        raise ValueError('Metrics must be a list.')"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics[0], str):\n        return metrics, [{} for _ in range(len(metrics))]\n    else:\n        return [metric['name'] for metric in metrics], [deepcopy(metric) for metric in metrics]"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics[0], str):\n        return metrics, [{}] * len(metrics)\n    else:\n        return [m['metric'] for m in metrics], metrics"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics[0], str):\n        return metrics, [{} for _ in metrics]\n\n    else:\n        return [metric['name'] for metric in metrics], metrics"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list) and len(metrics) > 0:\n        if isinstance(metrics[0], str):\n            return metrics, [{} for _ in metrics]\n        elif isinstance(metrics[0], dict):\n            return list(map(lambda x: list(x.keys())[0], metrics)), metrics\n        else:\n            raise ValueError('Metrics must be a list of strings or dictionaries.')\n    else:\n        return [], []"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list):\n        if all(isinstance(metric, str) for metric in metrics):\n            return metrics, [{} for _ in range(len(metrics))]\n        elif all(isinstance(metric, dict) for metric in metrics):\n            return [metric['name'] for metric in metrics], metrics\n        else:\n            raise ValueError(\"Invalid input format. Please provide either a list of strings or a list of dictionaries.\")\n    else:\n        raise ValueError(\"Invalid input format. Please provide a list.\")"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list):\n        if isinstance(metrics[0], str):\n            return metrics, [{} for _ in range(len(metrics))]\n        elif isinstance(metrics[0], dict):\n            return [m['name'] for m in metrics], metrics\n        else:\n            raise ValueError('Metrics must be either a list of strings or a list of dictionaries.')\n    else:\n        raise ValueError('Metrics must be a list.')"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, str):\n        metrics = [metrics]\n\n    if isinstance(metrics, list):\n        if all(isinstance(metric, str) for metric in metrics):\n            return metrics, [{} for _ in metrics]\n        elif all(isinstance(metric, dict) for metric in metrics):\n            return [metric['name'] for metric in metrics], metrics\n        else:\n            raise ValueError(\"Invalid metrics format. Please provide a list of strings or dictionaries.\")\n    else:\n        raise ValueError(\"Invalid metrics format. Please provide a list of strings or dictionaries.\")"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics[0], str):\n        return metrics, [{} for _ in metrics]\n\n    if isinstance(metrics[0], dict):\n        metrics = deepcopy(metrics)\n        names = [metric.pop('name') for metric in metrics]\n        return names, metrics"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics[0], str):\n        return metrics, [{} for _ in range(len(metrics))]\n\n    elif isinstance(metrics[0], dict):\n        return [m['name'] for m in metrics], metrics\n\n    else:\n        raise ValueError('Metrics must be either a list of strings or a list of dictionaries.')"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics[0], str):\n        return metrics, [{} for _ in metrics]\n    elif isinstance(metrics[0], dict):\n        return list(map(lambda x: list(x.keys())[0], metrics)), metrics\n    else:\n        raise ValueError(f'Unknown metrics format: {metrics}')"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list) and all(isinstance(item, str) for item in metrics):\n        return metrics, [{} for _ in metrics]\n    elif isinstance(metrics, list) and all(isinstance(item, dict) for item in metrics):\n        return [item['name'] for item in metrics], metrics\n    else:\n        raise ValueError(f\"Invalid input format for metrics: {metrics}\")"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list):\n        if isinstance(metrics[0], str):\n            return metrics, [{} for _ in metrics]\n        elif isinstance(metrics[0], dict):\n            return list(metrics[0].keys()), [deepcopy(metrics[0]) for _ in metrics]\n        else:\n            raise ValueError(\n                f'Metrics must be a list of strings or dictionaries, but got {type(metrics[0])}')\n    else:\n        raise ValueError(f'Metrics must be a list, but got {type(metrics)}')"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, str):\n        return [metrics], [{}]\n    elif isinstance(metrics, dict):\n        return [metrics['metric']], [metrics]\n    elif isinstance(metrics, list):\n        if all(isinstance(metric, str) for metric in metrics):\n            return metrics, [{} for _ in range(len(metrics))]\n        elif all(isinstance(metric, dict) for metric in metrics):\n            return [metric['metric'] for metric in metrics], metrics\n        else:\n            raise ValueError(\"Invalid input format for 'metrics'. Please provide either a string or a list of dictionaries.\")\n\n    raise ValueError(\"Invalid input format for 'metrics'. Please provide either a string or a list of dictionaries.\")"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics[0], str):\n        metrics = [{'name': m} for m in metrics]\n\n    metrics = [deepcopy(m) for m in metrics]\n\n    for m in metrics:\n        if 'name' not in m:\n            raise ValueError('Metric name not found in metric dictionary.')\n\n        if 'params' not in m:\n            m['params'] = {}\n\n        if 'params_to_cast' not in m:\n            m['params_to_cast'] = []\n\n        if 'params_to_cast' in m:\n            for key, value in m['params'].items():\n                if key in m['params_to_cast']:\n                    m['params'][key] = cast_embedding_model(key, value)\n\n    metric_names = [m['name'] for m in metrics]\n    metric_params = [m['params'] for m in metrics]\n\n    return metric_names, metric_params"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    # Initialize an empty list to store the metric names and a list to store the metric details\n    metric_names = []\n    metric_details = []\n\n    # Iterate over the input metrics\n    for metric in metrics:\n        # If the metric is a string, add it to the list of metric names\n        if isinstance(metric, str):\n            metric_names.append(metric)\n        # If the metric is a dictionary, extract its name and details and add them to the respective lists\n        elif isinstance(metric, dict):\n            name = metric.get('name')\n            if name is None:\n                raise ValueError(\"Metric details must contain a 'name' key.\")\n            metric_names.append(name)\n            metric_details.append(metric)\n        # If the metric is neither a string nor a dictionary, raise a ValueError\n        else:\n            raise ValueError(\"Metric must be a string or a dictionary.\")\n\n    # Return a tuple of the metric names and details\n    return metric_names, metric_details"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    if fn == contract:\n      fn_inv = inv_contract\n    elif fn == contract3_isoscale:\n      fn_inv = lambda s: 1 / s\n    else:\n      raise ValueError(f'fn={fn} not supported.')\n\n  t_to_s = lambda t: (fn(t) - t_near) / (t_far - t_near)\n  s_to_t = lambda s: fn_inv(s * (t_far - t_near) + t_near)\n\n  return t_to_s, s_to_t"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    if fn == contract:\n      fn_inv = inv_contract\n    else:\n      raise ValueError('fn_inv must be provided for non-contract functions.')\n\n  def t_to_s(t):\n    t = jnp.clip(t, t_near, t_far)\n    return (t - t_near) / (t_far - t_near)\n\n  def s_to_t(s):\n    return t_near + s * (t_far - t_near)\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    if fn == contract:\n      fn_inv = inv_contract\n    elif fn == contract3_isoscale:\n      fn_inv = lambda x: x / jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n    else:\n      raise ValueError(f'Cannot automatically determine inverse of {fn}.')\n\n  def t_to_s(t):\n    return (t - t_near) / (t_far - t_near)\n\n  def s_to_t(s):\n    return s * (t_far - t_near) + t_near\n\n  return t_to_s, s_to_t"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    if fn == contract:\n      fn_inv = inv_contract\n    elif fn == contract3_isoscale:\n      fn_inv = lambda s: 1.0 / s\n    else:\n      raise ValueError(f'fn_inv not provided and fn={fn} not supported.')\n\n  t_to_s = lambda t: fn_inv(jnp.clip(t, t_near, t_far))\n  s_to_t = lambda s: fn(s)\n  return t_to_s, s_to_t"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    if fn == contract:\n      fn_inv = inv_contract\n    else:\n      raise ValueError(f'fn_inv not specified for fn={fn}.')\n\n  def t_to_s(t):\n    \"\"\"\n    Maps metric distances to normalized distances in the range [0, 1].\n\n    Input-Output Arguments\n    :param t: Tensor. Represents the metric distances to be mapped.\n    :return: Tensor. The corresponding normalized distances in the range [0, 1].\n    \"\"\"\n    t_clipped = jnp.clip(t, t_near, t_far)\n    s = (t_clipped - t_near) / (t_far - t_near)\n    return fn(s)\n\n  def s_to_t(s):\n    \"\"\"\n    Maps normalized distances back to metric distances.\n\n    Input-Output Arguments\n    :param s: Tensor. Represents the normalized distances to be mapped back to metric distances.\n    :return: Tensor. The corresponding metric distances.\n    \"\"\"\n    s_clipped = jnp.clip(s, 0, 1)\n    t = fn_inv(s_clipped) * (t_far - t_near) + t_near\n    return t\n\n  return t_to_s, s_to_t"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Check if the provided `fn` and `fn_inv` are callable.\n  if not callable(fn):\n    raise TypeError(f'`fn` must be callable, but is of type {type(fn)}.')\n  if fn_inv is not None and not callable(fn_inv):\n    raise TypeError(f'`fn_inv` must be callable, but is of type {type(fn_inv)}.')\n\n  # Check if the provided `t_near` and `t_far` are tensors.\n  if not isinstance(t_near, jnp.ndarray):\n    raise TypeError(\n        f'`t_near` must be a jnp.ndarray, but is of type {type(t_near)}.'\n    )\n  if not isinstance(t_far, jnp.ndarray):\n    raise TypeError(\n        f'`t_far` must be a jnp.ndarray, but is of type {type(t_far)}.'\n    )\n\n  # Check if the shapes of `t_near` and `t_far` match.\n  if t_near.shape != t_far.shape:\n    raise ValueError(\n        f'`t_near` and `t_far` must have the same shape, but have shapes {t_near.shape} and {t_far.shape} respectively.'\n    )\n\n  # Check if `t_near` and `t_far` are 1D tensors.\n  if len(t_near.shape) != 1:\n    raise ValueError(\n        f'`t_near` and `t_far` must be 1D tensors, but have shapes {t_near.shape} and {t_far.shape} respectively.'\n    )\n\n  # Check if `t_near` and `t_far` are sorted in ascending order.\n  if not jnp.all(jnp.diff(t_near) >= 0):\n    raise ValueError(\n        f'`t_near` must be sorted in ascending order, but has values {t_near}.'\n    )\n  if"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Construct the forward mapping from metric to normalized distances.\n  t_to_s = lambda t: (t - t_near) / (t_far - t_near)\n\n  # Construct the backward mapping from normalized to metric distances.\n  if fn_inv is None:\n    # If the inverse function is not provided, try to automatically determine it.\n    if fn == contract:\n      # For the contract function, the inverse is the inverse of the contract\n      # function.\n      s_to_t = lambda s: contract(s)\n    elif fn == math.safe_sin:\n      # For the safe_sin function, the inverse is the safe_arcsin function.\n      s_to_t = lambda s: math.safe_arcsin(s)\n    else:\n      raise ValueError(\n          f'Cannot automatically determine the inverse function for fn={fn}.'\n      )\n  else:\n    s_to_t = fn_inv\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    if fn == contract:\n      fn_inv = inv_contract\n    elif fn == contract3_isoscale:\n      fn_inv = lambda s: jnp.where(s > 1, 1 / s, s)\n    else:\n      raise ValueError(f'fn_inv not provided for {fn}.')\n\n  def t_to_s(t):\n    \"\"\"\n    Maps metric distances to normalized distances in the range [0, 1].\n\n    Input-Output Arguments\n    :param t: Tensor. Represents the metric distances to be mapped.\n    :return: Tensor. The corresponding normalized distances in the range [0, 1].\n    \"\"\"\n    return jnp.clip((t - t_near) / (t_far - t_near), 0, 1)\n\n  def s_to_t(s):\n    \"\"\"\n    Maps normalized distances back to metric distances.\n\n    Input-Output Arguments\n    :param s: Tensor. Represents the normalized distances to be mapped back to metric distances.\n    :return: Tensor. The corresponding metric distances.\n    \"\"\"\n    return t_near + s * (t_far - t_near)\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    if fn == contract:\n      fn_inv = inv_contract\n    else:\n      raise ValueError(\n          f'fn_inv must be provided if fn is not contract, but got {fn}.'\n      )\n\n  # Clamp the distances to the range [t_near, t_far].\n  t_clamped = jnp.clip(fn(t_near), t_near, t_far)\n\n  # Compute the scaling factor that maps the clamped distances to the range [0, 1].\n  scale = (t_far - t_near) / (fn_inv(t_far) - fn_inv(t_near))\n\n  # Define the forward and backward mappings.\n  def t_to_s(t):\n    return (t - t_near) * scale\n\n  def s_to_t(s):\n    return s / scale + t_near\n\n  return t_to_s, s_to_t"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Define a mapping of functions to their inverses.\n  fn_inverse_map = {\n      'contract': 'inv_contract',\n      'sqrt': 'safe_sqrt',\n      'safe_sqrt': 'sqrt',\n      'safe_div': 'safe_div_inv',\n      'safe_div_inv': 'safe_div',\n      'safe_atan2': 'safe_atan2_inv',\n      'safe_atan2_inv': 'safe_atan2',\n      'safe_acos': 'safe_acos_inv',\n      'safe_acos_inv': 'safe_acos',\n      'safe_asin': 'safe_asin_inv',\n      'safe_asin_inv': 'safe_asin',\n      'safe_cos': 'safe_cos_inv',\n      'safe_cos_inv': 'safe_cos',\n      'safe_sin': 'safe_sin_inv',\n      'safe_sin_inv': 'safe_sin',\n      'safe_tan': 'safe_tan_inv',\n      'safe_tan_inv': 'safe_tan',\n      'safe_arccos': 'safe_arccos_inv',\n      'safe_arccos_inv': 'safe_arccos',\n      'safe_arcsin': 'safe_arcsin_inv',\n      'safe_arcsin_inv': 'safe_arcsin',\n      'safe_arctan': 'safe_arctan_inv',\n      'safe_arctan_inv': 'safe_arctan',\n      'safe_log': 'safe_exp',\n      'safe_exp': 'safe_log',\n      'safe_log1p': 'safe_expm1',\n      'safe_expm1': 'safe_log1p',\n      'safe_log2': 'safe_exp2',\n      'safe_exp2': 'safe_log2',\n      'safe_log10': 'safe_exp10',\n      'safe_exp10': 'safe_log10',\n      'safe_log1p': 'safe_"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    if fn == contract:\n      fn_inv = inv_contract\n    elif fn == contract3_isoscale:\n      fn_inv = lambda s: jnp.where(s > 0, jnp.exp(3 / 2 * jnp.log(s)), 0)\n    else:\n      raise ValueError(f'fn_inv not provided for {fn}.')\n\n  def t_to_s(t):\n    t = jnp.clip(t, t_near, t_far)\n    s = fn(t)\n    s = (s - t_near) / (t_far - t_near)\n    return s\n\n  def s_to_t(s):\n    s = jnp.clip(s, 0, 1)\n    t = s * (t_far - t_near) + t_near\n    t = fn_inv(s)\n    return t\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    fn_inv = {\n        jnp.sin: jnp.arcsin,\n        jnp.cos: jnp.arccos,\n        jnp.tan: jnp.arctan,\n        jnp.exp: jnp.log,\n        jnp.log: jnp.exp,\n        jnp.sqrt: lambda x: x**2,\n        jnp.square: jnp.sqrt,\n    }.get(fn, None)\n    if fn_inv is None:\n      raise ValueError(f'No inverse found for function {fn}.')\n\n  # Clamp the input to the specified bounds.\n  t_near = jnp.maximum(t_near, 0.0)\n  t_far = jnp.maximum(t_far, t_near)\n  t_to_s = lambda t: (fn(t) - t_near) / (t_far - t_near)\n  s_to_t = lambda s: t_near + (t_far - t_near) * fn_inv(s)\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    # Try to automatically determine the inverse of the provided function.\n    if fn == contract:\n      fn_inv = inv_contract\n    else:\n      raise ValueError(f'fn_inv={fn_inv} not implemented.')\n\n  # Clip t_near and t_far to be within the valid range [0, 1].\n  t_near = jnp.clip(t_near, 0, 1)\n  t_far = jnp.clip(t_far, 0, 1)\n\n  # Compute the scaling factor for the forward mapping.\n  t_to_s_scale = jnp.maximum(\n      jnp.finfo(jnp.float32).tiny, t_far - t_near\n  )  # Guard against division by zero.\n\n  # Compute the scaling factor for the backward mapping.\n  s_to_t_scale = jnp.maximum(\n      jnp.finfo(jnp.float32).tiny, t_near - t_far\n  )  # Guard against division by zero.\n\n  def t_to_s(t):\n    return (t - t_near) / t_to_s_scale\n\n  def s_to_t(s):\n    return s * s_to_t_scale + t_far\n\n  return t_to_s, s_to_t"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    # If no inverse is provided, try to automatically determine it.\n    if fn.__name__ == 'contract':\n      fn_inv = inv_contract\n    elif fn.__name__ == 'contract3_isoscale':\n      fn_inv = lambda s: s**3\n    else:\n      raise ValueError(f'fn={fn} not supported.')\n\n  # Clamp the input to the valid range [t_near, t_far].\n  t_clamped = jnp.clip(t_near, t_far, t_near)\n\n  # Compute the inverse of the function at the clamped values.\n  s_clamped = fn_inv(t_clamped)\n\n  # Compute the inverse of the function at the near and far plane values.\n  s_near = fn_inv(t_near)\n  s_far = fn_inv(t_far)\n\n  # Compute the scaling factor to normalize the input to the range [0, 1].\n  scale = (s_far - s_near) / (t_far - t_near)\n\n  # Define the forward mapping from metric to normalized distances.\n  t_to_s = lambda t: (fn(t) - s_near) / scale\n\n  # Define the backward mapping from normalized to metric distances.\n  s_to_t = lambda s: fn_inv(s * scale + s_near)\n\n  return t_to_s, s_to_t"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    # Try to automatically determine the inverse function.\n    if fn.__name__ == 'contract':\n      fn_inv = inv_contract\n    elif fn.__name__ == 'contract3_isoscale':\n      fn_inv = lambda s: s ** (3 / 2)\n    else:\n      raise ValueError(f'Cannot automatically determine inverse of {fn.__name__}.')\n\n  # Clip the input distances to the specified range.\n  t_clipped = jnp.clip(t_near, t_far, t_near + t_far)\n\n  # Compute the forward mapping from metric to normalized distances.\n  t_to_s = lambda t: (t - t_near) / (t_far - t_near)\n\n  # Compute the backward mapping from normalized to metric distances.\n  s_to_t = lambda s: t_near + (t_far - t_near) * fn_inv(s)\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    # Attempt to automatically determine the inverse of the provided function.\n    # This is a hardcoded mapping of functions to their inverses, and may not\n    # be suitable for all functions.\n    if fn.__name__ == 'contract':\n      fn_inv = inv_contract\n    else:\n      raise ValueError(\n          f'Cannot automatically determine inverse of function {fn.__name__}.'\n      )\n\n  def t_to_s(t):\n    \"\"\"\n    Maps metric distances `t` to normalized distances in the range [0, 1].\n\n    Input-Output Arguments\n    :param t: Tensor. The input metric distances to be mapped.\n    :return: Tensor. The mapped normalized distances in the range [0, 1].\n    \"\"\"\n    return jnp.maximum(0.0, jnp.minimum(1.0, (t - t_near) / (t_far - t_near)))\n\n  def s_to_t(s):\n    \"\"\"\n    Maps normalized distances `s` in the range [0, 1] to metric distances.\n\n    Input-Output Arguments\n    :param s: Tensor. The input normalized distances to be mapped.\n    :return: Tensor. The mapped metric distances.\n    \"\"\"\n    return s * (t_far - t_near) + t_near\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Construct the forward mapping from metric to normalized distances.\n  def t_to_s(t):\n    # Clip t to the valid range [t_near, t_far].\n    t = jnp.clip(t, t_near, t_far)\n\n    # Compute the scaling factor based on the function fn.\n    if fn_inv is None:\n      # If fn_inv is not provided, try to determine it automatically.\n      fn_inv_map = {\n          jnp.sin: jnp.arcsin,\n          jnp.cos: jnp.arccos,\n          jnp.tan: jnp.arctan,\n          jnp.exp: jnp.log,\n          jnp.log: jnp.exp,\n          jnp.sqrt: lambda x: x**2,\n          lambda x: 1 / jnp.sqrt(x): lambda x: x**2,\n          lambda x: 1 / jnp.sqrt(1 - x**2): lambda x: x**2,\n      }\n      if fn in fn_inv_map:\n        fn_inv = fn_inv_map[fn]\n      else:\n        raise ValueError(\n            f'fn_inv not provided and automatic detection failed for fn={fn}.'\n        )\n    s = (fn(t) - t_near) / (t_far - t_near)\n    return s\n\n  # Construct the backward mapping from normalized to metric distances.\n  def s_to_t(s):\n    # Clip s to the valid range [0, 1].\n    s = jnp.clip(s, 0, 1)\n\n    # Compute the scaling factor based on the inverse function fn_inv.\n    if fn_inv is None:\n      raise ValueError('fn_inv not provided.')\n    t = t_near + (t_far - t_near) * fn_inv(s)\n    return t\n\n  return t_to_s, s_to_t"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Check if the input arguments are valid.\n  if not callable(fn):\n    raise TypeError('fn must be a callable.')\n  if not callable(fn_inv) and fn_inv is not None:\n    raise TypeError('fn_inv must be a callable.')\n  if not isinstance(t_near, jnp.ndarray):\n    raise TypeError('t_near must be a tensor.')\n  if not isinstance(t_far, jnp.ndarray):\n    raise TypeError('t_far must be a tensor.')\n\n  # Check if the shapes of the input arguments are consistent.\n  if t_near.shape != t_far.shape:\n    raise ValueError('t_near and t_far must have the same shape.')\n\n  # Construct the forward mapping from metric to normalized distances.\n  def t_to_s(t):\n    return (t - t_near) / (t_far - t_near)\n\n  # Construct the backward mapping from normalized to metric distances.\n  if fn_inv is None:\n    # If fn_inv is not provided, try to automatically determine the inverse.\n    fn_inv = {\n        math.safe_sqrt: math.safe_sqrt,\n        math.safe_log: math.safe_exp,\n        math.safe_div: math.safe_div,\n        math.safe_div_zero_safe: math.safe_div_zero_safe,\n    }.get(fn, None)\n    if fn_inv is None:\n      raise ValueError(\n          f'Could not automatically determine inverse of {fn}. Please provide it as fn_inv.'\n      )\n  def s_to_t(s):\n    return fn_inv(s * (t_far - t_near) + t_near)\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Compute the inverse of the provided function, if not provided.\n  if fn_inv is None:\n    fn_inv = _invert_fn(fn)\n\n  # Construct the forward mapping from metric to normalized distances.\n  def t_to_s(t):\n    # Clip the input distances to the valid range.\n    t = jnp.clip(t, t_near, t_far)\n    # Transform the distances using the provided function.\n    s = fn(t)\n    # Normalize the transformed distances to the range [0, 1].\n    s = (s - t_near) / (t_far - t_near)\n    return s\n\n  # Construct the backward mapping from normalized to metric distances.\n  def s_to_t(s):\n    # Normalize the input distances to the range [0, 1].\n    s = jnp.clip(s, 0, 1)\n    # Denormalize the input distances to the original range.\n    s = s * (t_far - t_near) + t_near\n    # Transform the denormalized distances using the inverse of the provided function.\n    t = fn_inv(s)\n    return t\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Check if the provided function is one of the supported functions.\n  if fn not in [contract, contract3_isoscale]:\n    raise ValueError(\n        f'Function {fn} is not supported. Only contract and contract3_isoscale are supported.'\n    )\n\n  # Check if the provided inverse function is correct.\n  if fn_inv is not None and fn_inv != jnp.exp:\n    raise ValueError(\n        f'Function {fn_inv} is not the correct inverse for function {fn}.'\n    )\n\n  # Compute the inverse function if not provided.\n  if fn_inv is None:\n    if fn == contract:\n      fn_inv = jnp.exp\n    elif fn == contract3_isoscale:\n      fn_inv = lambda x: jnp.exp(x**3)\n    else:\n      raise ValueError(f'Function {fn} is not supported.')\n\n  # Construct the forward mapping from metric to normalized distances.\n  t_to_s = lambda t: fn_inv(fn(t) - t_near) / (t_far - t_near)\n\n  # Construct the backward mapping from normalized to metric distances.\n  s_to_t = lambda s: t_near + (t_far - t_near) * fn(t_near + s * (t_far - t_near))\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray\n\n"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  dt = jnp.diff(t)\n  dw = jnp.diff(w)\n  return jnp.sum(dt * (w[Ellipsis, :-1] + dw / 2), axis=-1)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  dt = jnp.diff(t)\n  dw = jnp.diff(w)\n  return jnp.sum(dt * (w[Ellipsis, :-1] + dw / 2), axis=-1)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  dt = jnp.diff(t)\n  dw = jnp.diff(w)\n  return jnp.sum(dt * (w[Ellipsis, :-1] + dw / 2), axis=-1)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  dt = jnp.diff(t)\n  dv = jnp.diff(w)\n  return jnp.sum(dt * (w[Ellipsis, :-1] + dv / 2), axis=-1)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  eps = jnp.finfo(jnp.float32).eps ** 2\n  dt = jnp.diff(t)\n  dw = jnp.diff(w)\n  return 0.5 * jnp.sum(dt[Ellipsis, :-1] * (w[Ellipsis, :-1] + w[Ellipsis, 1:]), axis=-1)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  eps = jnp.finfo(jnp.float32).eps ** 2\n  dt = jnp.diff(t)\n  dw = jnp.diff(w)\n  return jnp.sum(jnp.maximum(eps, dt) * (w[Ellipsis, :-1] + dw / 2), axis=-1)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  eps = jnp.finfo(jnp.float32).eps ** 2\n  dt = jnp.diff(t)\n  dv = jnp.diff(w)\n  return jnp.sum(jnp.maximum(eps, dt) * (w[Ellipsis, :-1] + dv / 2), axis=-1)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  eps = jnp.finfo(jnp.float32).eps ** 2\n  dt = jnp.diff(t)\n  dw = jnp.diff(w)\n  return jnp.sum(jnp.maximum(eps, dt) * (w[Ellipsis, :-1] + w[Ellipsis, 1:]) / 2, axis=-1)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n\n  dt = jnp.diff(t)\n  dw = jnp.diff(w)\n  return jnp.sum(dt[Ellipsis, :-1] * (w[Ellipsis, :-1] + dw / 2), axis=-1)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  check_zero_endpoints(w)\n  dt = jnp.diff(t)\n  w0 = jnp.concatenate([jnp.zeros_like(w[Ellipsis, :1]), w], axis=-1)\n  w1 = jnp.concatenate([w, jnp.zeros_like(w[Ellipsis, -1:])], axis=-1)\n  return jnp.sum(0.5 * dt * (w0 + w1), axis=-1)"}
{"namespace": "linspline.integrate", "completion": "  # Ensure the input data points are valid for a linear spline.\n  utils.assert_valid_linspline(t, w)\n\n  # Compute the integral using the trapezoid rule.\n  dt = jnp.diff(t)\n  dw = jnp.diff(w)\n  integral = jnp.sum(dt * (w[Ellipsis, :-1] + dw / 2), axis=-1)\n  return integral"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  check_zero_endpoints(w)\n  dt = jnp.diff(t)\n  dw = jnp.diff(w)\n  return jnp.cumsum(0.5 * dt * (w[Ellipsis, :-1] + dw), axis=-1)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  dt = jnp.diff(t)\n  dw = jnp.diff(w)\n  return jnp.sum(dt[Ellipsis, :-1] * (w[Ellipsis, :-1] + w[Ellipsis, 1:]), axis=-1) / 2"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  eps = jnp.finfo(jnp.float32).eps ** 2\n  dt = jnp.diff(t)\n  w0 = jnp.concatenate([jnp.zeros_like(w[Ellipsis, :1]), w], axis=-1)\n  w1 = jnp.concatenate([w, jnp.zeros_like(w[Ellipsis, -1:])], axis=-1)\n  return jnp.sum(jnp.maximum(eps, dt) * (w0[Ellipsis, :-1] + w1[Ellipsis, 1:]) / 2, axis=-1)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  return jnp.trapz(w, x=t)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  check_zero_endpoints(w)\n\n  # Integrate the spline using the trapezoid rule.\n  dt = jnp.diff(t)\n  dw = jnp.diff(w)\n  integral = jnp.sum(dt * (w[Ellipsis, :-1] + dw / 2), axis=-1)\n  return integral"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  eps = jnp.finfo(jnp.float32).eps ** 2\n  dt = jnp.diff(t)\n  dv = jnp.diff(w) / jnp.maximum(eps, dt)\n  return jnp.cumsum(dt[Ellipsis, :-1] * (w[Ellipsis, :-1] + dv), axis=-1)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  check_zero_endpoints(w)\n\n  # Integrate the spline using the trapezoid rule.\n  dt = jnp.diff(t)\n  dw = jnp.diff(w)\n  return jnp.sum(dt * (w[Ellipsis, :-1] + dw / 2), axis=-1)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  eps = jnp.finfo(jnp.float32).eps ** 2\n  dt = jnp.diff(t)\n  dv = jnp.diff(w)\n  # The integral has an ambiguous global offset here, which we set to 0.\n  return 0.5 * jnp.cumsum(dt * (w[Ellipsis, :-1] + w[Ellipsis, 1:]), axis=-1)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  check_zero_endpoints(w)\n\n  # Integrate the spline using the trapezoid rule.\n  dt = jnp.diff(t)\n  dw = jnp.diff(w)\n  integral = jnp.sum(dt[Ellipsis, :-1] * (w[Ellipsis, :-1] + dw / 2), axis=-1)\n  return integral"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    id_df = pd.DataFrame({f'id_{i}': id_list for i, id_list in enumerate(ids)})\n    score_df = pd.DataFrame({f'score_{i}': score_list for i, score_list in enumerate(scores)})\n    df = pd.concat([id_df, score_df], axis=1)\n\n    def cc_pure_apply(row):\n        ids_tuple = tuple(row[[f'id_{i}' for i in range(len(ids))]].values)\n        scores_tuple = tuple(row[[f'score_{i}' for i in range(len(scores))]].values)\n        return pd.Series(cc_pure(ids_tuple, scores_tuple, weights, top_k))\n\n    df[['cc_id', 'cc_score']] = df.apply(cc_pure_apply, axis=1)\n    return df['cc_id'].tolist(), df['cc_score'].tolist()\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": ""}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    id_df = pd.DataFrame({f'id_{i}': id_list for i, id_list in enumerate(ids)})\n    score_df = pd.DataFrame({f'score_{i}': score_list for i, score_list in enumerate(scores)})\n    df = pd.concat([id_df, score_df], axis=1)\n\n    def cc_pure_apply(row):\n        ids_tuple = tuple(row[[f'id_{i}' for i in range(len(ids))]].values)\n        scores_tuple = tuple(row[[f'score_{i}' for i in range(len(scores))]].values)\n        return pd.Series(cc_pure(ids_tuple, scores_tuple, weights, top_k))\n\n    df[['cc_id', 'cc_score']] = df.apply(cc_pure_apply, axis=1)\n    return df['cc_id'].tolist(), df['cc_score'].tolist()\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": ""}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": ""}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": ""}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": ""}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": ""}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    id_df = pd.DataFrame({f'id_{i}': id_list for i, id_list in enumerate(ids)})\n    score_df = pd.DataFrame({f'score_{i}': score_list for i, score_list in enumerate(scores)})\n    df = pd.concat([id_df, score_df], axis=1)\n\n    def cc_pure_apply(row):\n        ids_tuple = tuple(row[[f'id_{i}' for i in range(len(ids))]].values)\n        scores_tuple = tuple(row[[f'score_{i}' for i in range(len(scores))]].values)\n        return pd.Series(cc_pure(ids_tuple, scores_tuple, weights, top_k))\n\n    df[['cc_id', 'cc_score']] = df.apply(cc_pure_apply, axis=1)\n    return df['cc_id'].tolist(), df['cc_score'].tolist()"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    ids_list = []\n    scores_list = []\n    for i in range(len(ids)):\n        ids_list.extend(ids[i])\n        scores_list.extend(scores[i])\n\n    df = pd.DataFrame({'id': ids_list, 'score': scores_list})\n    df['weighted_score'] = df['score'] * weights[i]\n    df = df.groupby('id')['weighted_score'].sum().reset_index()\n    df = df.sort_values('weighted_score', ascending=False)\n    return df['id'].tolist()[:top_k], df['weighted_score'].tolist()[:top_k]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    id_df = pd.DataFrame({f'id_{i}': id_list for i, id_list in enumerate(ids)})\n    score_df = pd.DataFrame({f'score_{i}': score_list for i, score_list in enumerate(scores)})\n    df = pd.concat([id_df, score_df], axis=1)\n\n    df['weighted_score'] = df[[f'score_{i}' for i in range(len(scores))]].apply(\n        lambda x: sum(x * weights), axis=1)\n    df = df.sort_values(by='weighted_score', ascending=False).reset_index(drop=True)\n    return df['id_0'].tolist()[:top_k], df['weighted_score'].tolist()[:top_k]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    id_df = pd.DataFrame({f'id_{i}': id_list for i, id_list in enumerate(ids)})\n    score_df = pd.DataFrame({f'score_{i}': score_list for i, score_list in enumerate(scores)})\n    weight_df = pd.DataFrame({f'weight_{i}': weight for i, weight in enumerate(weights)})\n    df = pd.concat([id_df, score_df, weight_df], axis=1)\n\n    df['weighted_score'] = df[[f'score_{i}' for i in range(len(scores))]].mul(\n        df[[f'weight_{i}' for i in range(len(weights))]], axis=1).sum(axis=1)\n    df = df.sort_values('weighted_score', ascending=False).reset_index(drop=True)\n    return df['id_0'].tolist()[:top_k], df['weighted_score'].tolist()[:top_k]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    id_df = pd.DataFrame({f'id_{i}': id_list for i, id_list in enumerate(ids)})\n    score_df = pd.DataFrame({f'score_{i}': score_list for i, score_list in enumerate(scores)})\n    df = pd.concat([id_df, score_df], axis=1)\n\n    df['weighted_score'] = df[[f'score_{i}' for i in range(len(scores))]].mul(weights).sum(axis=1)\n    df.sort_values('weighted_score', ascending=False, inplace=True)\n    return df['id_0'].tolist()[:top_k], df['weighted_score'].tolist()[:top_k]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    id_df = pd.DataFrame({f'id_{i}': id_list for i, id_list in enumerate(ids)})\n    score_df = pd.DataFrame({f'score_{i}': score_list for i, score_list in enumerate(scores)})\n    weight_df = pd.DataFrame({f'weight_{i}': weight_list for i, weight_list in enumerate(weights)})\n    df = pd.concat([id_df, score_df, weight_df], axis=1)\n\n    df['weighted_score'] = df[[f'score_{i}' for i in range(len(scores))]].dot(\n        df[[f'weight_{i}' for i in range(len(weights))]])\n    df = df.sort_values('weighted_score', ascending=False).reset_index(drop=True)\n\n    return df['id_0'].tolist()[:top_k], df['weighted_score'].tolist()[:top_k]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Check the input arguments\n    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    # Calculate the weighted sum of scores for each ID\n    weighted_scores = {}\n    for i, id_list in enumerate(ids):\n        for j, id_ in enumerate(id_list):\n            if id_ not in weighted_scores:\n                weighted_scores[id_] = 0\n            weighted_scores[id_] += scores[i][j] * weights[i]\n\n    # Normalize the scores\n    total_weight = sum(weights)\n    normalized_scores = {id_: score / total_weight for id_, score in weighted_scores.items()}\n\n    # Sort the IDs by their weighted sum in descending order\n    sorted_ids = sorted(normalized_scores.keys(), key=lambda x: normalized_scores[x], reverse=True)\n\n    # Return the top K IDs and their corresponding scores\n    return sorted_ids[:top_k], [normalized_scores[id_] for id_ in sorted_ids[:top_k]]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    # Calculate the weighted sum of scores for each ID\n    weighted_scores = [sum([score * weight for score, weight in zip(scores_list, weights)]) for scores_list in scores]\n\n    # Normalize the scores\n    normalized_scores = [score / sum(weighted_scores) for score in weighted_scores]\n\n    # Sort the IDs and scores based on the normalized scores\n    sorted_indices = sorted(range(len(normalized_scores)), key=lambda i: normalized_scores[i], reverse=True)\n    sorted_ids = [ids[i] for i in sorted_indices]\n    sorted_scores = [normalized_scores[i] for i in sorted_indices]\n\n    # Return the top K IDs and their corresponding scores\n    return sorted_ids[:top_k], sorted_scores[:top_k]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    id_df = pd.DataFrame({f'id_{i}': id_list for i, id_list in enumerate(ids)})\n    score_df = pd.DataFrame({f'score_{i}': score_list for i, score_list in enumerate(scores)})\n    df = pd.concat([id_df, score_df], axis=1)\n\n    df['weighted_score'] = df[[f'score_{i}' for i in range(len(scores))]].apply(lambda x: x * weights[i], axis=1).sum(\n        axis=1)\n    df = df.sort_values(by='weighted_score', ascending=False)\n    df = df.head(top_k)\n    return df['id_0'].tolist(), df['weighted_score'].tolist()"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    # Calculate the weighted sum of scores for each ID\n    weighted_sums = []\n    for i in range(len(ids)):\n        id_list = ids[i]\n        score_list = scores[i]\n        weight = weights[i]\n        for j in range(len(id_list)):\n            id_ = id_list[j]\n            score = score_list[j]\n            weighted_sums.append((id_, score * weight))\n\n    # Normalize the scores\n    max_score = max([score for _, score in weighted_sums])\n    normalized_sums = [(id_, score / max_score) for id_, score in weighted_sums]\n\n    # Sort the IDs by their weighted sum in descending order\n    sorted_sums = sorted(normalized_sums, key=lambda x: x[1], reverse=True)\n\n    # Return the top K IDs and their corresponding scores\n    top_ids = [id_ for id_, _ in sorted_sums[:top_k]]\n    top_scores = [score for _, score in sorted_sums[:top_k]]\n    return top_ids, top_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    # Calculate the weighted sum of scores for each ID\n    weighted_sums = {id_: sum([score * weight for score, weight in zip(scores, weights)])\n                     for id_, scores in zip(ids, scores)}\n\n    # Normalize the weighted sums\n    max_weighted_sum = max(weighted_sums.values())\n    min_weighted_sum = min(weighted_sums.values())\n    normalized_weighted_sums = {id_: (weighted_sum - min_weighted_sum) / (max_weighted_sum - min_weighted_sum)\n                                for id_, weighted_sum in weighted_sums.items()}\n\n    # Sort the IDs based on their normalized weighted sums in descending order\n    sorted_ids = sorted(normalized_weighted_sums.keys(), key=lambda id_: normalized_weighted_sums[id_],\n                        reverse=True)\n\n    # Return the top K IDs and their corresponding normalized weighted sums\n    return sorted_ids[:top_k], [normalized_weighted_sums[id_] for id_ in sorted_ids[:top_k]]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Check that the lengths of the input tuples are the same\n    assert len(ids) == len(scores) == len(weights), \"The lengths of the input tuples must be the same.\"\n\n    # Check that the lengths of the lists within the input tuples are the same\n    for i in range(len(ids)):\n        assert len(ids[i]) == len(scores[i]), f\"The lengths of the lists in the {i}th position of the input tuples must be the same.\"\n\n    # Check that the sum of the weights is 1\n    assert sum(weights) == 1, \"The sum of the weights must be 1.\"\n\n    # Check that the top_k parameter is a positive integer\n    assert top_k > 0, \"The top_k parameter must be a positive integer.\"\n\n    # Calculate the weighted sum of scores for each ID\n    weighted_sums = []\n    for i in range(len(ids)):\n        weighted_sums.append([score * weights[i] for score in scores[i]])\n\n    # Normalize the scores\n    normalized_scores = [score / sum(weighted_sum) for score, weighted_sum in zip(weighted_sums, weighted_sums)]\n\n    # Combine the IDs and scores into a list of tuples\n    id_score_pairs = [(id, score) for ids_list, score_list in zip(ids, normalized_scores) for id, score in zip(ids_list, score_list)]\n\n    # Sort the ID-score pairs in descending order based on the scores\n    id_score_pairs.sort(key=lambda x: x[1], reverse=True)\n\n    # Return the top K IDs and their corresponding scores\n    return [id for id, _ in id_score_pairs[:top_k]], [score for _, score in id_score_pairs[:top_k]]"}
{"namespace": "coord.track_linearize", "completion": "  if mean.shape[:-1] != cov.shape[:-2]:\n    raise ValueError(\n        f'mean.shape[:-1] {mean.shape[:-1]} != cov.shape[:-2] {cov.shape[:-2]}.'\n    )\n  d = mean.shape[-1]\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n\n  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n  )\n\n  # The cube root of the determinant of the Jacobian is the geometric mean\n  # of the eigenvalues of the Jacobian, which gives us the isotropic scaling\n  # implied by `fn` at each mean that `cov` should be multiplied by.\n  eps = jnp.finfo(jnp.float32).tiny  # Guard against an inf gradient at 0.\n  abs_det = jnp.maximum(eps, jnp.abs(jnp.linalg.det(jac)))\n  # Special case d == 3 for speed's sake.\n  fn_cov = cov * (jnp.cbrt(abs_det) if d == 3 else abs_det ** (1 / d))\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  if mean.shape[:-1] != cov.shape[:-2]:\n    raise ValueError(\n        f'mean.shape[:-1] {mean.shape[:-1]} != cov.shape[:-2] {cov.shape[:-2]}.'\n    )\n  d = mean.shape[-1]\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n\n  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n  )\n\n  # The cube root of the determinant of the Jacobian is the geometric mean\n  # of the eigenvalues of the Jacobian, which gives us the isotropic scaling\n  # implied by `fn` at each mean that `cov` should be multiplied by.\n  eps = jnp.finfo(jnp.float32).tiny  # Guard against an inf gradient at 0.\n  abs_det = jnp.maximum(eps, jnp.abs(jnp.linalg.det(jac)))\n  # Special case d == 3 for speed's sake.\n  fn_cov = cov * (jnp.cbrt(abs_det) if d == 3 else abs_det ** (1 / d))\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  if mean.shape[:-1] != cov.shape[:-2]:\n    raise ValueError(\n        f'mean.shape[:-1] {mean.shape[:-1]} != cov.shape[:-2] {cov.shape[:-2]}.'\n    )\n  d = mean.shape[-1]\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n\n  if cov is not None:\n    # Compute the Jacobian of fn function at the locations of each mean.\n    jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n        jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n    )\n\n    # The cube root of the determinant of the Jacobian is the geometric mean\n    # of the eigenvalues of the Jacobian, which gives us the isotropic scaling\n    # implied by `fn` at each mean that `cov` should be multiplied by.\n    eps = jnp.finfo(jnp.float32).tiny  # Guard against an inf gradient at 0.\n    abs_det = jnp.maximum(eps, jnp.abs(jnp.linalg.det(jac)))\n    # Special case d == 3 for speed's sake.\n    fn_cov = cov * (jnp.cbrt(abs_det) if d == 3 else abs_det ** (1 / d))\n  else:\n    fn_cov = None\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  if mean.shape[:-1] != cov.shape[:-2]:\n    raise ValueError(\n        f'mean.shape[:-1] {mean.shape[:-1]} != cov.shape[:-2] {cov.shape[:-2]}.'\n    )\n  d = mean.shape[-1]\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n\n  if cov is not None:\n    # Compute the Jacobian of fn function at the locations of each mean.\n    jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n        jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n    )\n\n    # The cube root of the determinant of the Jacobian is the geometric mean\n    # of the eigenvalues of the Jacobian, which gives us the isotropic scaling\n    # implied by `fn` at each mean that `cov` should be multiplied by.\n    eps = jnp.finfo(jnp.float32).tiny  # Guard against an inf gradient at 0.\n    abs_det = jnp.maximum(eps, jnp.abs(jnp.linalg.det(jac)))\n    # Special case d == 3 for speed's sake.\n    fn_cov = cov * (jnp.cbrt(abs_det) if d == 3 else abs_det ** (1 / d))\n  else:\n    fn_cov = None\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  if mean.shape[:-1] != cov.shape[:-2]:\n    raise ValueError(\n        f'mean.shape[:-1] {mean.shape[:-1]} != cov.shape[:-2] {cov.shape[:-2]}.'\n    )\n  d = mean.shape[-1]\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n\n  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n  )\n\n  # The cube root of the determinant of the Jacobian is the geometric mean\n  # of the eigenvalues of the Jacobian, which gives us the isotropic scaling\n  # implied by `fn` at each mean that `cov` should be multiplied by.\n  eps = jnp.finfo(jnp.float32).tiny  # Guard against an inf gradient at 0.\n  abs_det = jnp.maximum(eps, jnp.abs(jnp.linalg.det(jac)))\n  # Special case d == 3 for speed's sake.\n  fn_cov = cov * (jnp.cbrt(abs_det) if d == 3 else abs_det ** (1 / d))\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  if mean.shape[:-1] != cov.shape[:-2]:\n    raise ValueError(\n        f'mean.shape[:-1] {mean.shape[:-1]} != cov.shape[:-2] {cov.shape[:-2]}.'\n    )\n  d = mean.shape[-1]\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n\n  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n  )\n\n  # The cube root of the determinant of the Jacobian is the geometric mean\n  # of the eigenvalues of the Jacobian, which gives us the isotropic scaling\n  # implied by `fn` at each mean that `cov` should be multiplied by.\n  eps = jnp.finfo(jnp.float32).tiny  # Guard against an inf gradient at 0.\n  abs_det = jnp.maximum(eps, jnp.abs(jnp.linalg.det(jac)))\n  # Special case d == 3 for speed's sake.\n  fn_cov = cov * (jnp.cbrt(abs_det) if d == 3 else abs_det ** (1 / d))\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  d = mean.shape[-1]\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n  )\n  # The cube root of the determinant of the Jacobian is the geometric mean\n  # of the eigenvalues of the Jacobian, which gives us the isotropic scaling\n  # implied by `fn` at each mean that `cov` should be multiplied by.\n  eps = jnp.finfo(jnp.float32).tiny  # Guard against an inf gradient at 0.\n  abs_det = jnp.maximum(eps, jnp.abs(jnp.linalg.det(jac)))\n  fn_cov = cov * (jnp.cbrt(abs_det) if d == 3 else abs_det ** (1 / d))\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  if mean.shape[:-1] != cov.shape[:-2]:\n    raise ValueError(\n        f'mean.shape[:-1] {mean.shape[:-1]} != cov.shape[:-2] {cov.shape[:-2]}.'\n    )\n  d = mean.shape[-1]\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n\n  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n  )\n\n  # Compute the Jacobian of the inverse of fn function at the locations of each\n  # mean.\n  inv_jac = jax.vmap(jax.jacfwd(jnp.linalg.inv), in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n  )\n\n  # Compute the Jacobian of the inverse of fn function at the locations of each\n  # mean.\n  inv_jac = jax.vmap(jax.jacfwd(jnp.linalg.inv), in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n  )\n\n  # Compute the Jacobian of the inverse of fn function at the locations of each\n  # mean.\n  inv_jac = jax.vmap(jax.jacfwd(jnp.linalg.inv), in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n  )\n\n  # Compute the Jacobian of the inverse of fn function at the locations of each\n "}
{"namespace": "coord.track_linearize", "completion": "  if mean.shape[:-1] != cov.shape[:-2]:\n    raise ValueError(\n        f'mean.shape[:-1] {mean.shape[:-1]} != cov.shape[:-2] {cov.shape[:-2]}.'\n    )\n  d = mean.shape[-1]\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n\n  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n  )\n\n  # Compute the linearized covariance matrix.\n  fn_cov = jnp.einsum('...ij,...jk,...lk->...il', jac, cov, jac)\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  if mean.shape[:-1] != cov.shape[:-2]:\n    raise ValueError(\n        f'mean.shape[:-1] {mean.shape[:-1]} != cov.shape[:-2] {cov.shape[:-2]}.'\n    )\n  d = mean.shape[-1]\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n\n  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n  )\n\n  # The cube root of the determinant of the Jacobian is the geometric mean\n  # of the eigenvalues of the Jacobian, which gives us the scale implied by\n  # `fn` at each mean that `cov` should be scaled by.\n  eps = jnp.finfo(jnp.float32).tiny  # Guard against an inf gradient at 0.\n  abs_det = jnp.maximum(eps, jnp.abs(jnp.linalg.det(jac)))\n  # Special case d == 3 for speed's sake.\n  fn_cov = cov * (jnp.cbrt(abs_det) if d == 3 else abs_det ** (1 / d))\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(jax.jacobian(fn), in_axes=-1, out_axes=-2)(mean)\n\n  # Compute the linearized function at each mean.\n  fn_mean = fn(mean)\n\n  # Compute the linearized covariance at each mean.\n  fn_cov = jnp.einsum('...ij,...jk,...lk->...il', jac, cov, jac)\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  if mean.shape[:-1] != cov.shape[:-2]:\n    raise ValueError(\n        f'mean.shape[:-1] {mean.shape[:-1]} != cov.shape[:-2] {cov.shape[:-2]}.'\n    )\n  d = mean.shape[-1]\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n\n  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n  )\n\n  # Compute the mean and covariance of the linearized function.\n  fn_mean = fn_mean\n  fn_cov = jnp.einsum('...ij,...jk,...kl->...il', jac, cov, jac)\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  if mean.shape[-1] != cov.shape[-2]:\n    raise ValueError(\n        f'mean.shape[-1] {mean.shape[-1]} != cov.shape[-2] {cov.shape[-2]}.'\n    )\n  d = mean.shape[-1]\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n  )\n  # The cube root of the determinant of the Jacobian is the geometric mean\n  # of the eigenvalues of the Jacobian, which gives us the scaling implied\n  # by `fn` at each mean that `cov` should be multiplied by.\n  eps = jnp.finfo(jnp.float32).tiny  # Guard against an inf gradient at 0.\n  abs_det = jnp.maximum(eps, jnp.abs(jnp.linalg.det(jac)))\n  fn_cov = cov * (jnp.cbrt(abs_det) if d == 3 else abs_det ** (1 / d))\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  if fn is None:\n    fn_mean = mean\n    fn_cov = cov\n  else:\n    fn_mean, lin_fn = jax.linearize(fn, mean)\n    jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n        jnp.broadcast_to(jnp.eye(mean.shape[-1]), mean.shape + (mean.shape[-1],))\n    )\n    fn_cov = cov + jnp.matmul(jnp.matmul(jac, cov), jac.transpose((0, 2, 1)))\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  if mean.shape[:-1] != cov.shape[:-2]:\n    raise ValueError(\n        f'mean.shape[:-1] {mean.shape[:-1]} != cov.shape[:-2] {cov.shape[:-2]}.'\n    )\n  d = mean.shape[-1]\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n\n  if cov is not None:\n    # Compute the Jacobian of fn function at the locations of each mean.\n    jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n        jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n    )\n\n    # The inverse of the Jacobian is the Jacobian of the inverse function.\n    inv_jac = jnp.linalg.inv(jac)\n\n    # The linearized covariance is the product of the linearized covariance and\n    # the inverse of the Jacobian.\n    fn_cov = jnp.einsum('...ij,...jk->...ik', cov, inv_jac)\n  else:\n    fn_cov = None\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  if mean.shape[:-1] != cov.shape[:-2]:\n    raise ValueError(\n        f'mean.shape[:-1] {mean.shape[:-1]} != cov.shape[:-2] {cov.shape[:-2]}.'\n    )\n  d = mean.shape[-1]\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n\n  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n  )\n\n  # Compute the linearized covariances.\n  fn_cov = cov + jnp.einsum(\n      '...ij,...jk,...lk->...il', jac, cov, jac, optimize='optimal'\n  )\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  if fn is None:\n    fn_fwd = lambda x: x\n    fn_inv = lambda x: x\n  else:\n    fn_fwd = fn\n    if fn_inv is None:\n      # A simple mapping from some functions to their inverse.\n      inv_mapping = {\n          'reciprocal': jnp.reciprocal,\n          'log': jnp.exp,\n          'exp': jnp.log,\n          'sqrt': jnp.square,\n          'square': jnp.sqrt,\n      }\n      fn_inv = inv_mapping[fn.__name__]\n\n  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(jax.jacfwd(fn_fwd), in_axes=-1, out_axes=-2)(mean)\n\n  # The cube root of the determinant of the Jacobian is the geometric mean\n  # of the eigenvalues of the Jacobian, which gives us the isotropic scaling\n  # implied by `fn` at each mean that `cov` should be multiplied by.\n  eps = jnp.finfo(jnp.float32).tiny  # Guard against an inf gradient at 0.\n  abs_det = jnp.maximum(eps, jnp.abs(jnp.linalg.det(jac)))\n  # Special case d == 3 for speed's sake.\n  fn_cov = cov * (jnp.cbrt(abs_det) if mean.shape[-1] == 3 else abs_det ** (1 / mean.shape[-1]))\n\n  # Compute the Jacobian of the inverse function at the locations of each mean.\n  jac_inv = jax.vmap(jax.jacfwd(fn_inv), in_axes=-1, out_axes=-2)(mean)\n\n  # The cube root of the determinant of the Jacobian is the geometric mean\n  # of the eigenvalues of the Jacobian, which gives us the isotropic scaling\n  # implied by `fn` at each mean that `cov` should be multi"}
{"namespace": "coord.track_linearize", "completion": "  if fn is None:\n    fn_mean = mean\n    fn_cov = cov\n  else:\n    fn_mean, lin_fn = jax.linearize(fn, mean)\n\n    # Compute the Jacobian of fn function at the locations of each mean.\n    jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n        jnp.broadcast_to(jnp.eye(mean.shape[-1]), mean.shape + (mean.shape[-1],))\n    )\n\n    # Compute the linearization of the covariance around each mean.\n    fn_cov = jnp.einsum('...ij,...jk,...lk->...il', jac, cov, jac)\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  if fn is None:\n    fn_fwd = lambda x: x\n    fn_inv = lambda x: x\n  else:\n    fn_fwd = fn\n    if fn_inv is None:\n      # A simple mapping from some functions to their inverse.\n      inv_mapping = {\n          'reciprocal': jnp.reciprocal,\n          'log': jnp.exp,\n          'exp': jnp.log,\n          'sqrt': jnp.square,\n          'square': jnp.sqrt,\n      }\n      fn_inv = inv_mapping[fn.__name__]\n\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n\n  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(jnp.eye(mean.shape[-1]), mean.shape + (mean.shape[-1],))\n  )\n\n  # Compute the Jacobian of fn_inv function at the locations of each mean.\n  jac_inv = jax.vmap(jax.jacfwd(fn_inv), in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(\n          jnp.eye(mean.shape[-1]), mean.shape + (mean.shape[-1],)\n      )\n  )\n\n  # Compute the linearized covariance using the Jacobian of fn.\n  fn_cov = jnp.einsum('...ij,...jk->...ik', jac, cov)\n\n  # Compute the inverse of the linearized covariance using the Jacobian of fn_inv.\n  inv_fn_cov = jnp.einsum('...ij,...jk->...ik', jac_inv, cov)\n\n  return fn_mean, fn_cov, inv_fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  if mean.shape[:-1] != cov.shape[:-2]:\n    raise ValueError(\n        f'mean.shape[:-1] {mean.shape[:-1]} != cov.shape[:-2] {cov.shape[:-2]}.'\n    )\n  d = mean.shape[-1]\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n\n  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n  )\n\n  # Compute the Jacobian of the linearized function at the locations of each mean.\n  jac_lin = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n  )\n\n  # Compute the Jacobian of the linearized function at the locations of each mean.\n  jac_lin_inv = jax.vmap(jnp.linalg.inv, in_axes=-1, out_axes=-1)(\n      jac_lin + jnp.eye(d)[None, Ellipsis] * 1e-5\n  )\n\n  # Compute the Jacobian of the linearized function at the locations of each mean.\n  jac_inv = jax.vmap(jnp.linalg.inv, in_axes=-1, out_axes=-1)(\n      jac + jnp.eye(d)[None, Ellipsis] * 1e-5\n  )\n\n  # Compute the Jacobian of the linearized function at the locations of each mean.\n  fn_cov = jnp.einsum('...ij,...jk,...k->...ij', jac_lin_inv, cov, jac_inv)\n  return fn_mean, fn_cov"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in range(len(x)):\n        if len(x[i]) > 1:\n            yield x[i][: len(x[i]) // 2], x[i][len(x[i]) // 2 :]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in range(len(x)):\n        if len(x[i]) > 1:\n            yield x[i][: len(x[i]) // 2], x[i][len(x[i]) // 2 :]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in range(len(x)):\n        if len(x[i]) > 1:\n            yield x[i][: len(x[i]) // 2], x[i][len(x[i]) // 2 :]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in range(len(x)):\n        if len(x[i]) > 1:\n            yield x[i][: len(x[i]) // 2], x[i][len(x[i]) // 2 :]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in range(len(x)):\n        if len(x[i]) > 1:\n            yield x[i][: len(x[i]) // 2], x[i][len(x[i]) // 2 :]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield i[: len(i) // 2], i[len(i) // 2 :]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in range(len(x)):\n        if len(x[i]) > 1:\n            yield [x[i][: len(x[i]) // 2], x[i][len(x[i]) // 2 :]]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[: len(i) // 2], i[len(i) // 2 :]]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield i[: len(i) // 2], i[len(i) // 2 :]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i, xi in enumerate(x):\n        if len(xi) > 1:\n            yield [xi[: len(xi) // 2], xi[len(xi) // 2 :]]\n        else:\n            yield [xi, xi]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i, xi in enumerate(x):\n        if len(xi) > 1:\n            yield [xi[: len(xi) // 2], xi[len(xi) // 2 :]]\n        else:\n            yield [xi, xi]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i, xi in enumerate(x):\n        if len(xi) > 1:\n            yield [xi[: len(xi) // 2], xi[len(xi) // 2 :]]\n        else:\n            yield [xi, xi]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for xi in x:\n        if len(xi) > 1:\n            yield [xi[: len(xi) // 2], xi[len(xi) // 2 :]]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for a in x:\n        if len(a) > 1:\n            yield [a[: len(a) // 2], a[len(a) // 2 :]]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for arr in x:\n        if len(arr) > 1:\n            yield [arr[: len(arr) // 2], arr[len(arr) // 2 :]]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for xi in x:\n        if xi.size > 1:\n            yield [xi[: xi.size // 2], xi[xi.size // 2 :]]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for x_i in x:\n        if x_i.size > 1:\n            yield x_i[: x_i.size // 2], x_i[x_i.size // 2 :]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i, xi in enumerate(x):\n        if xi.size > 1:\n            yield [xi[: len(xi) // 2], xi[len(xi) // 2 :]]\n    return"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for a in x:\n        if a.shape[0] > 1:\n            yield [a[: len(a) // 2], a[len(a) // 2 :]]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[: len(i) // 2], i[len(i) // 2 :]]\n    \"\"\"\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(f\"`x` must be a square matrix, got a {x.shape[0]}x{x.shape[1]} matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(f\"`x` must be a square matrix, got a {x.shape[0]}x{x.shape[1]} matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(f\"`x` must be a square matrix, got a {x.shape[0]}x{x.shape[1]} matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(f\"`x` must be a square matrix, got a {x.shape[0]}x{x.shape[1]} matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(f\"`x` must be a square matrix, got a {x.shape[0]}x{x.shape[1]} matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(f\"`x` must be a square matrix, got a {x.shape[0]}x{x.shape[1]} matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(f\"`x` must be a square matrix, got a {x.shape[0]}x{x.shape[1]} matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(f\"`x` must be a square matrix, got a {x.shape} matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(f\"`x` must be a square matrix, got a {x.shape[0]}x{x.shape[1]} matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(\"`x` must be a 2D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(f\"`x` must be a square matrix, got a {x.shape} matrix\")"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  encoded = jnp.sin(scaled_x)\n  if append_identity:\n    encoded = jnp.concatenate([encoded, x[Ellipsis, None, :]], axis=-1)\n  return encoded"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  encoded = jnp.concatenate(\n      [jnp.sin(scaled_x), jnp.cos(scaled_x)], axis=-1\n  )\n  if append_identity:\n    encoded = jnp.concatenate([x, encoded], axis=-1)\n  return encoded"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  encoded = jnp.concatenate(\n      [jnp.sin(scaled_x), jnp.cos(scaled_x)], axis=-1\n  )\n  if append_identity:\n    encoded = jnp.concatenate([x, encoded], axis=-1)\n  return encoded"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  encoded = jnp.concatenate([jnp.sin(scaled_x), jnp.cos(scaled_x)], axis=-1)\n  if append_identity:\n    encoded = jnp.concatenate([x[Ellipsis, None, :], encoded], axis=-1)\n  return encoded"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  sin_x = jnp.sin(scaled_x)\n  cos_x = jnp.cos(scaled_x)\n  if append_identity:\n    sin_x = jnp.concatenate([sin_x, x[Ellipsis, None]], axis=-1)\n    cos_x = jnp.concatenate([cos_x, jnp.ones_like(x[Ellipsis, None])], axis=-1)\n  return sin_x, cos_x"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  encoded = jnp.concatenate([jnp.sin(scaled_x), jnp.cos(scaled_x)], axis=-1)\n  if append_identity:\n    encoded = jnp.concatenate([x, encoded], axis=-1)\n  return encoded"}
{"namespace": "coord.pos_enc", "completion": "  if append_identity:\n    x = jnp.concatenate([x, jnp.ones_like(x[..., :1])], axis=-1)\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  return jnp.concatenate([jnp.sin(scaled_x), jnp.cos(scaled_x)], axis=-1)"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  sin_x = jnp.sin(scaled_x)\n  cos_x = jnp.cos(scaled_x)\n  if append_identity:\n    return jnp.concatenate([sin_x, cos_x, x[Ellipsis, None, :]], axis=-1)\n  else:\n    return jnp.concatenate([sin_x, cos_x], axis=-1)"}
{"namespace": "coord.pos_enc", "completion": "  x = jnp.expand_dims(x, -1)\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x * scales[:, None], shape)\n  encoded = jnp.concatenate(\n      [jnp.sin(scaled_x), jnp.cos(scaled_x)], axis=-1\n  )\n  if append_identity:\n    encoded = jnp.concatenate([x, encoded], axis=-1)\n  return encoded"}
{"namespace": "coord.pos_enc", "completion": "  if x.shape[-1] != 3:\n    raise ValueError(f'Inputs must be 3D, are {x.shape[-1]}D.')\n\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  encoded = jnp.concatenate(\n      [jnp.sin(scaled_x), jnp.cos(scaled_x)], axis=-1\n  )\n  if append_identity:\n    encoded = jnp.concatenate([x, encoded], axis=-1)\n  return encoded"}
{"namespace": "coord.pos_enc", "completion": "  # Generate the scales for the encoding.\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n\n  # Scale the input and apply the sine function.\n  scaled_x = x[Ellipsis, None, :] * scales[:, None]\n  sin_x = jnp.sin(scaled_x)\n\n  # Optionally append the original input to the encoded array.\n  if append_identity:\n    sin_x = jnp.concatenate([sin_x, x[Ellipsis, None, :]], axis=-1)\n\n  return sin_x"}
{"namespace": "coord.pos_enc", "completion": "  # Generate the scales for encoding\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n\n  # Scale the input by the scales and apply sine function\n  encoded = jnp.sin(x[..., None] * scales)\n\n  # Optionally append the original input to the encoded array\n  if append_identity:\n    encoded = jnp.concatenate([x[..., None], encoded], axis=-1)\n\n  return encoded"}
{"namespace": "coord.pos_enc", "completion": "  # Compute the number of degrees to encode\n  num_degrees = max_deg - min_deg\n\n  # Compute the scales for the encoding\n  scales = 2 ** jnp.arange(min_deg, max_deg)\n\n  # Scale the input and apply a sine function\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], x.shape[:-1] + (-1,))\n  encoded = jnp.sin(scaled_x)\n\n  # Optionally append the original input to the encoded array\n  if append_identity:\n    encoded = jnp.concatenate([encoded, x[Ellipsis, None]], axis=-1)\n\n  return encoded"}
{"namespace": "coord.pos_enc", "completion": "  # Generate a list of powers of 2 from min_deg to max_deg\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  # Reshape the input array to add a new axis at the end\n  x_reshaped = jnp.reshape(x, x.shape + (1,))\n  # Multiply the input array with the scales and concatenate the result with the original input\n  x_pos_enc = jnp.concatenate(\n      [x_reshaped * scale for scale in scales], axis=-1\n  )\n  # If append_identity is True, concatenate the original input with the encoded array\n  if append_identity:\n    x_pos_enc = jnp.concatenate([x_reshaped, x_pos_enc], axis=-1)\n  return x_pos_enc"}
{"namespace": "coord.pos_enc", "completion": "  # Generate the scales to be used in the positional encoding.\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n\n  # Reshape the input array to match the scales.\n  x_shape = x.shape\n  x = jnp.reshape(x, x_shape[:-1] + (-1,))\n\n  # Apply the positional encoding to the input array.\n  pos_enc = jnp.sin(x[..., None] * scales[None, :])\n\n  # Optionally append the original input to the positional encoding.\n  if append_identity:\n    pos_enc = jnp.concatenate([x, pos_enc], axis=-1)\n\n  return pos_enc"}
{"namespace": "coord.pos_enc", "completion": "  # Generate a list of scales based on the degree range\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  # Scale the input by the generated scales and apply the sine function\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  encoded = jnp.sin(scaled_x)\n\n  # Optionally append the original input to the encoded features\n  if append_identity:\n    encoded = jnp.concatenate([encoded, x[Ellipsis, None, :]], axis=-1)\n  return encoded"}
{"namespace": "coord.pos_enc", "completion": "  # Compute the scales for encoding\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n\n  # Scale the input and apply sine function\n  scaled_input = x[Ellipsis, None] * scales[None, Ellipsis]\n  encoded = jnp.sin(scaled_input)\n\n  # If specified, concatenate the original input with the encoded features\n  if append_identity:\n    encoded = jnp.concatenate([x[Ellipsis, None], encoded], axis=-1)\n\n  return encoded"}
{"namespace": "coord.pos_enc", "completion": "  # Compute the number of degrees of freedom in the encoding.\n  num_degrees = max_deg - min_deg + 1\n\n  # Compute the scales to use for encoding.\n  scales = 2.0 ** jnp.arange(min_deg, max_deg + 1)\n\n  # Scale the input array and apply a sine function.\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], x.shape[:-1] + (-1,))\n  encoded = jnp.sin(scaled_x)\n\n  # Optionally append the original input to the encoded array.\n  if append_identity:\n    encoded = jnp.concatenate([encoded, x[Ellipsis, None]], axis=-1)\n\n  return encoded"}
{"namespace": "coord.pos_enc", "completion": "  # Convert the input array to a float32 tensor\n  x = jnp.array(x, dtype=jnp.float32)\n\n  # Generate scales for encoding\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n\n  # Apply sine function to scaled input\n  encoded = jnp.sin(x[Ellipsis, None] * scales)\n\n  # If append_identity is True, concatenate the original input with the encoded features\n  if append_identity:\n    encoded = jnp.concatenate([x[Ellipsis, None], encoded], axis=-1)\n\n  return encoded"}
{"namespace": "coord.pos_enc", "completion": "  # Check if the input array is 2D or 3D\n  if len(x.shape) not in [2, 3]:\n    raise ValueError(\n        'Input array must be 2D or 3D. Got shape: {}'.format(x.shape)\n    )\n\n  # Generate the scales for encoding\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n\n  # Apply the sine function to the input array\n  encoded = jnp.sin(x[Ellipsis, None] * scales[None, :])\n\n  # Optionally concatenate the encoded array with the original input\n  if append_identity:\n    encoded = jnp.concatenate([encoded, x[Ellipsis, None]], axis=-1)\n\n  return encoded\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": ""}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": ""}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": ""}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": ""}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": ""}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": ""}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": ""}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": ""}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    "}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Check if len(field1) equals len(field2).\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(\n                    f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                    f\"resp. {values[field1][i].shape} and {values[field2][i].shape}\"\n                )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"\n        This function checks if two lists of numpy arrays (specified by field names) have the same length and if each corresponding pair of arrays within these lists has the same shape.\n\n        Input-Output Arguments\n        :param cls: type. The class type of the Pydantic model being validated.\n        :param values: Dict[str, List[np.ndarray]]. A dictionary of field names and their corresponding values.\n        :return: Dict[str, List[np.ndarray]]. The validated values if the check passes.\n\n        \"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Check if len(field1) equals len(field2) and if each corresponding pair of arrays within these lists has the same shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Check if all arrays in field1 have the same shape as the corresponding arrays in field2.\"\"\"\n        for arr1, arr2 in zip(values[field1], values[field2]):\n            if arr1.shape != arr2.shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Check if len(field1) equals len(field2) and that each pair of corresponding arrays have the same shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for array1, array2 in zip(values[field1], values[field2]):\n            if array1.shape != array2.shape:\n                raise ValueError(\n                    f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                    f\"resp. {array1.shape} and {array2.shape}\"\n                )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Check if len(field1) equals len(field2).\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for i, (arr1, arr2) in enumerate(zip(values[field1], values[field2])):\n            if arr1.shape != arr2.shape:\n                raise ValueError(\n                    f\"{cls.__name__}: {field1} and {field2} shape mismatch for index {i}.\"\n                    f\" Expected {arr1.shape}, got {arr2.shape}\"\n                )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n\n        \"\"\"\n        This function is a Pydantic model validator that checks if two lists of numpy arrays (specified by field names) have the same length and if each corresponding pair of arrays within these lists has the same shape.\n\n        Input-Output Arguments\n        :param cls: type. The class type of the Pydantic model.\n        :param values: Dict[str, List[np.ndarray]]. A dictionary of values to be validated. It should contain two fields, one for each of the lists of numpy arrays to be compared.\n        :return: Dict[str, List[np.ndarray]]. The validated values.\n\n        \"\"\"\n\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(\n                    f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                    f\"resp. {values[field1][i].shape} and {values[field2][i].shape}\"\n                )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n\n        \"\"\"\n        This function is a validator function that checks if two lists of numpy arrays (specified by field names) have the same length and if each corresponding pair of arrays within these lists has the same shape.\n\n        Input-Output Arguments\n        :param cls: type. The class type of the Pydantic model that is being validated.\n        :param values: Dict[str, List[np.ndarray]]. A dictionary of values to be validated. The keys of this dictionary should be the names of the fields that are being compared. The values should be lists of numpy arrays.\n        :return: Dict[str, List[np.ndarray]]. The validated values if the check passes.\n\n        \"\"\"\n\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for i, (arr1, arr2) in enumerate(zip(values[field1], values[field2])):\n            if arr1.shape != arr2.shape:\n                raise ValueError(\n                    f\"{cls.__name__}: {field1} and {field2} shape mismatch at index {i}, \"\n                    f\"resp. {arr1.shape} and {arr2.shape}\"\n                )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n\n        \"\"\"\n        This function performs the shape equality check between the arrays in the specified fields.\n\n        Input-Output Arguments\n        :param cls: type. The class type of the Pydantic model.\n        :param values: Dict[str, List[np.ndarray]]. A dictionary of field names and corresponding values.\n        :return: Dict[str, List[np.ndarray]]. The validated values if the check passes.\n\n        \"\"\"\n\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for i, (array1, array2) in enumerate(zip(values[field1], values[field2])):\n            if array1.shape != array2.shape:\n                raise ValueError(\n                    f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                    f\"resp. {array1.shape} and {array2.shape} at index {i}\"\n                )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"\n        This function validates the shape equality of two lists of numpy arrays.\n\n        Input-Output Arguments\n        :param cls: type. The class type of the Pydantic model.\n        :param values: Dict[str, List[np.ndarray]]. A dictionary of field names and corresponding values. It contains the two lists of numpy arrays to be compared.\n        :return: Dict[str, List[np.ndarray]]. The validated values after checking the shape equality.\n\n        \"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                             f\"resp. {len(values[field1])} and {len(values[field2])}\")\n\n        for i, (array1, array2) in enumerate(zip(values[field1], values[field2])):\n            if array1.shape != array2.shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch at index {i}, \"\n                                 f\"resp. {array1.shape} and {array2.shape}\")\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"\n        This function performs the actual validation check. It takes a class type and a dictionary of values, checks for shape equality between arrays in the specified fields, and returns the validated values if the check passes.\n\n        Input-Output Arguments\n        :param cls: type. The class type of the Pydantic model being validated.\n        :param values: Dict[str, List[np.ndarray]]. A dictionary of values to be validated. The keys are the field names, and the values are the corresponding lists of numpy arrays to be compared.\n        :return: Dict[str, List[np.ndarray]]. The validated values if the check passes.\n\n        \"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(\n                    f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                    f\"resp. {values[field1][i].shape} and {values[field2][i].shape}\"\n                )\n\n        return values\n\n    return __root_validator"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the context\n        eglctx.resize(camera.W, camera.H)\n\n        # Render the mesh\n        self.render(camera)\n\n        # Read the rendered image from the context\n        image = eglctx.read_pixels()\n        return image"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        self.resize_textures(camera.H, camera.W)\n        self.resize_buffers(len(self.verts), len(self.faces))\n        self.update_gl_buffers()\n        self.upload_gl_uniforms(camera)\n\n        # Render the mesh\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glUseProgram(self.point_program)\n            self.use_gl_program(self.point_program)\n        else:\n            gl.glUseProgram(self.mesh_program)\n            self.use_gl_program(self.mesh_program)\n\n        gl.glBindVertexArray(self.vao)\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glBindBuffer(gl.GL_ELEMENT_ARRAY_BUFFER, self.ebo)\n            gl.glDrawElements(gl.GL_LINES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of indices\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glBindBuffer(gl.GL_ELEMENT_ARRAY_BUFFER, self.ebo)\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of indices\n        elif self.render_type == Mesh.RenderType.QUADS:\n            gl.glBindBuffer(gl.GL_ELEMENT_ARRAY_BUFFER, self.ebo)\n            gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context\n        eglctx.resize(camera.W, camera.H)\n        eglctx.make_current()\n\n        # Render the mesh using the camera settings\n        self.render(camera)\n\n        # Read the rendered image from the framebuffer\n        image = eglctx.read_framebuffer(camera.W, camera.H)\n\n        # Return the rendered image\n        return image"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # TODO: Add more robust error handling\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple cameras"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        H, W = camera.H, camera.W\n        self.resize_textures(H, W)\n        eglctx.resize(H, W)\n        eglctx.make_current()\n        self.render(camera)\n        eglctx.swap_buffers()\n        eglctx.make_current()\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # log(yellow(f'Rendering mesh {self.name} with camera {camera.name}'))\n        eglctx.resize(camera.W, camera.H)\n        eglctx.make_current()\n        self.render(camera)\n        eglctx.done_current()\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the context to match the camera's width and height\n        eglctx.resize(camera.W, camera.H)\n\n        # Render the Mesh instance using the camera's settings\n        gl.glClearColor(0.0, 0.0, 0.0, 1.0)\n        gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)\n        self.render(camera)\n\n        # Read the rendered image from the OpenGL buffer\n        image = np.empty((camera.H, camera.W, 4), dtype=np.uint8)\n        gl.glReadPixels(0, 0, camera.W, camera.H, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, image)\n\n        # Resize the image to match the camera's width and height\n        image = cv2.resize(image, (camera.W, camera.H))\n\n        # Return the rendered image\n        return image"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context\n        eglctx.resize(camera.W, camera.H)\n\n        # Render the mesh\n        self.render(camera)\n\n        # Read the rendered image from the offscreen context\n        image = eglctx.read_pixels()\n        return image\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        gl.glViewport(0, 0, camera.W, camera.H)\n        gl.glClearColor(0.0, 0.0, 0.0, 0.0)\n        gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)\n\n        self.render(camera)\n\n        # Read the pixels\n        pixels = gl.glReadPixels(0, 0, camera.W, camera.H, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE)\n        pixels = np.frombuffer(pixels, dtype=np.uint8).reshape((camera.H, camera.W, 4))\n        pixels = np.flipud(pixels)\n        return pixels\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        if not self.visible: return\n\n        # Resize the rendering context\n        eglctx.resize(camera.W, camera.H)\n        gl.glViewport(0, 0, camera.W, camera.H)\n\n        # Render the mesh\n        self.render(camera)\n\n        # Read the rendered image\n        img = eglctx.read_pixels()\n        return img\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize(camera.W, camera.H)\n\n        # Clear the rendering context\n        gl.glClearColor(0.0, 0.0, 0.0, 1.0)\n        gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(camera)\n\n        # Read the rendered image from the rendering context\n        image = eglctx.read_pixels(camera.H, camera.W)\n\n        return image\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # log(f'Rendering mesh {self.name} with {len(self.verts)} verts and {len(self.faces)} faces')\n        gl.glViewport(0, 0, camera.W, camera.H)\n        gl.glClearColor(0, 0, 0, 0)\n        gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)\n        gl.glEnable(gl.GL_DEPTH_TEST)\n        gl.glDepthFunc(gl.GL_LESS)\n\n        gl.glEnable(gl.GL_BLEND)\n        gl.glBlendFunc(gl.GL_SRC_ALPHA, gl.GL_ONE_MINUS_SRC_ALPHA)\n\n        self.render(camera)\n\n        # gl.glFinish()\n        # gl.glFlush()\n        # gl.glFinish()\n        # gl.glFlush()\n        # gl.glFinish()\n        # gl.glFlush()\n\n        # gl.glFinish()\n        # gl.glFlush()\n        # gl.glFinish()\n        # gl.glFlush()\n        # gl.glFinish()\n        # gl.glFlush()\n\n        # gl.glFinish()\n        # gl.glFlush()\n        # gl.glFinish()\n        # gl.glFlush()\n        # gl.glFinish()\n        # gl.glFlush()\n\n        # gl.glFinish()\n        # gl.glFlush()\n        # gl.glFinish()\n        # gl.glFlush()\n        # gl.glFinish()\n        # gl.glFlush()\n\n        # gl.glFinish()\n        # gl.glFlush()\n        # gl.glFinish()\n        # gl.glFlush()\n        # gl.glFinish()\n        # gl.glFlush()\n\n        # gl.glFinish()\n        # gl.glFlush()\n        # gl."}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        if not self.visible: return\n        if not len(self.verts): return\n\n        # Resize the rendering context\n        eglctx.resize(camera.W, camera.H)\n\n        # Render the mesh\n        gl.glEnable(gl.GL_DEPTH_TEST)\n        gl.glClearColor(0.0, 0.0, 0.0, 1.0)\n        gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)\n        self.render(camera)\n        gl.glDisable(gl.GL_DEPTH_TEST)\n\n        # Read the rendered image from the framebuffer\n        image = eglctx.read_pixels(camera.W, camera.H)\n        return image\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Setup the OpenGL context\n        eglctx.make_current()\n        eglctx.resize(camera.W, camera.H)\n\n        # Render the mesh\n        self.render(camera)\n\n        # Read the rendered image\n        image = eglctx.read_framebuffer()\n        return image\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # TODO: This is not the correct way to do it, but it works for now\n        # The offscreen rendering is done by creating a new window and rendering to it\n        # This is not ideal, but it works for now\n        # This is not the correct way to do it, but it works for now\n        # The offscreen rendering is done by creating a new window and rendering to it\n        # This is not ideal, but it works for now\n        # This is not the correct way to do it, but it works for now\n        # The offscreen rendering is done by creating a new window and rendering to it\n        # This is not ideal, but it works for now\n        # This is not the correct way to do it, but it works for now\n        # The offscreen rendering is done by creating a new window and rendering to it\n        # This is not ideal, but it works for now\n        # This is not the correct way to do it, but it works for now\n        # The offscreen rendering is done by creating a new window and rendering to it\n        # This is not ideal, but it works for now\n        # This is not the correct way to do it, but it works for now\n        # The offscreen rendering is done by creating a new window and rendering to it\n        # This is not ideal, but it works for now\n        # This is not the correct way to do it, but it works for now\n        # The offscreen rendering is done by creating a new window and rendering to it\n        # This is not ideal, but it works for now\n        # This is not the correct way to do it, but it works for now\n        # The offscreen rendering is done by creating a new window and rendering to it\n        # This is not ideal, but it works for now\n        # This is not the correct way to do it, but it works for now\n        # The offscreen rendering is done by creating a new window and rendering to it\n        # This is not ideal, but it works for now\n        # This is not the correct way to do it, but it works for now\n        # The offscreen rendering is done by creating a new window and rendering to it\n        # This is not ideal, but it works for now\n        # This is not the correct"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Prepare camera\n        camera.resize(eglctx.width, eglctx.height)\n        camera.update_gl_buffers()\n\n        # Prepare rendering\n        gl.glViewport(0, 0, eglctx.width, eglctx.height)\n        gl.glClearColor(0.0, 0.0, 0.0, 1.0)\n        gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)\n\n        # Render the mesh\n        self.render(camera)\n\n        # Read the rendered image\n        image = np.empty((eglctx.height, eglctx.width, 3), dtype=np.uint8)\n        gl.glReadPixels(0, 0, eglctx.width, eglctx.height, gl.GL_RGB, gl.GL_UNSIGNED_BYTE, image)\n        image = image[::-1, :, :]  # flip the image vertically\n        return image\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        from easyvolcap.utils.opengl_utils import eglContextManager\n        eglctx.resize(camera.W, camera.H)\n\n        # Render the scene\n        gl.glViewport(0, 0, camera.W, camera.H)\n        gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)\n        gl.glEnable(gl.GL_DEPTH_TEST)\n        gl.glEnable(gl.GL_BLEND)\n        gl.glBlendFunc(gl.GL_SRC_ALPHA, gl.GL_ONE_MINUS_SRC_ALPHA)\n\n        self.render(camera)\n\n        # Read the pixels\n        pixels = gl.glReadPixels(0, 0, camera.W, camera.H, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE)\n        pixels = np.frombuffer(pixels, dtype=np.uint8).reshape((camera.H, camera.W, 4))\n        pixels = torch.from_numpy(pixels).permute(2, 0, 1)  # H, W, C\n        return pixels\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # TODO: This is a very slow offscreen rendering method, need to optimize it\n        with eglctx.gl_context(camera.W, camera.H) as ctx:\n            gl.glViewport(0, 0, camera.W, camera.H)\n            gl.glClearColor(0, 0, 0, 0)\n            gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)\n\n            # Render the mesh\n            self.render(camera)\n\n            # Read the rendered image\n            image = np.empty((camera.H, camera.W, 4), dtype=np.uint8)\n            gl.glReadPixels(0, 0, camera.W, camera.H, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, image)\n\n        return image\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # TODO: This is a hack, need to fix the egl context manager\n        self.resize_textures(camera.H, camera.W)\n\n        # Render to the texture\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)\n        self.render(camera)\n\n        # Read the texture back to the CPU\n        # TODO: This is a hack, need to fix the egl context manager\n        image = np.zeros((self.H, self.W, 4), dtype=np.uint8)\n        gl.glReadPixels(0, 0, self.W, self.H, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, image)\n        image = torch.from_numpy(image).permute(2, 0, 1).flip(0)  # H, W, C\n        return image\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the context\n        eglctx.resize(camera.W, camera.H)\n\n        # Render the mesh\n        self.render(camera)\n\n        # Read the framebuffer\n        # frame = eglctx.read_frame()\n        # frame = frame.transpose(1, 0, 2)  # HWC to CHW\n        # frame = torch.from_numpy(frame)\n        # frame = frame.permute(2, 0, 1)  # CHW to HWC\n        # frame = frame.contiguous()\n        # frame = frame.type(torch.uint8)\n        # frame = frame.flip(0)  # flip the image\n        # return frame\n"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    return NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        # The following attributes do not have a direct equivalent in BertConfig\n        # and are set to commonly used defaults for NomicBertConfig\n        pad_token_id=0,\n        position_embedding_type=\"absolute\",\n        use_cache=True,\n    )"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    return NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        # The following attributes do not have a direct equivalent in BertConfig\n        # and are set to commonly used defaults for GPT2Config\n        n_positions=bert_config.max_position_embeddings,\n        n_embd=bert_config.hidden_size,\n        n_layer=bert_config.num_hidden_layers,\n        n_head=bert_config.num_attention_heads,\n        n_inner=bert_config.intermediate_size,\n        activation_function=bert_config.hidden_act,\n        resid_pdrop=bert_config.hidden_dropout_prob,\n        attn_pdrop=bert_config.attention_probs_dropout_prob,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        initializer_range=bert_config.initializer_range,\n        type_vocab_size=bert_config.type_vocab_size,\n    )"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    return NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        # The following attributes do not have a direct equivalent in BertConfig\n        # and are set to commonly used defaults for GPT2Config\n        pad_token_id=0,\n        position_embedding_type=\"absolute\",\n        use_cache=True,\n    )"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    return NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        # The following attributes do not have a direct equivalent in BertConfig\n        # and are set to commonly used defaults for GPT2Config\n        n_positions=bert_config.max_position_embeddings,\n        n_embd=bert_config.hidden_size,\n        n_layer=bert_config.num_hidden_layers,\n        n_head=bert_config.num_attention_heads,\n        n_inner=bert_config.intermediate_size,\n        activation_function=bert_config.hidden_act,\n        resid_pdrop=bert_config.hidden_dropout_prob,\n        attn_pdrop=bert_config.attention_probs_dropout_prob,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        initializer_range=bert_config.initializer_range,\n        orig_vocab_size=bert_config.vocab_size,\n    )"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n    )\n\n    return nomic_config"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        pad_vocab_size_multiple=bert_config.pad_vocab_size_multiple,\n        last_layer_subset=bert_config.last_layer_subset,\n    )\n\n    return config"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    return NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        # The following attributes do not have a direct equivalent in BertConfig\n        # and are set to commonly used defaults for NomicBertConfig\n        pad_vocab_size_multiple=1,\n        pad_token_id=0,\n        position_embedding_type=\"absolute\",\n        use_cache=True,\n    )"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    return NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        # The following attributes do not have a direct equivalent in BertConfig\n        # and are set to commonly used defaults for GPT2Config\n        n_positions=bert_config.max_position_embeddings,\n        n_embd=bert_config.hidden_size,\n        n_layer=bert_config.num_hidden_layers,\n        n_head=bert_config.num_attention_heads,\n        n_inner=bert_config.intermediate_size,\n        activation_function=bert_config.hidden_act,\n        resid_pdrop=bert_config.hidden_dropout_prob,\n        attn_pdrop=bert_config.attention_probs_dropout_prob,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n    )"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    return NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        # The following attributes do not have a direct equivalent in BertConfig\n        # and are set to commonly used defaults for NomicBertConfig\n        n_positions=bert_config.max_position_embeddings,\n        n_embd=bert_config.hidden_size,\n        n_layer=bert_config.num_hidden_layers,\n        n_head=bert_config.num_attention_heads,\n        n_inner=bert_config.intermediate_size,\n        activation_function=bert_config.hidden_act,\n        resid_pdrop=bert_config.hidden_dropout_prob,\n        attn_pdrop=bert_config.attention_probs_dropout_prob,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        pad_vocab_size_multiple=1,\n        last_layer_subset=False,\n    )"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        # The following attributes do not have a direct equivalent in BertConfig\n        # and are set to commonly used defaults for GPT2Config\n        n_positions=bert_config.max_position_embeddings,\n        n_embd=bert_config.hidden_size,\n        n_layer=bert_config.num_hidden_layers,\n        n_head=bert_config.num_attention_heads,\n        n_inner=bert_config.intermediate_size,\n        activation_function=bert_config.hidden_act,\n        resid_pdrop=bert_config.hidden_dropout_prob,\n        attn_pdrop=bert_config.attention_probs_dropout_prob,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        initializer_range=bert_config.initializer_range,\n        type_vocab_size=bert_config.type_vocab_size,\n        pad_token_id=0,\n        position_embedding_type=\"absolute\",\n        use_cache=True,\n    )\n\n    return config"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    return NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        pad_vocab_size_multiple=bert_config.pad_vocab_size_multiple,\n        # The following attributes do not have a direct equivalent in BertConfig\n        # and are set to commonly used defaults for NomicBertConfig\n        n_positions=bert_config.max_position_embeddings,\n        n_embd=bert_config.hidden_size,\n        n_inner=bert_config.intermediate_size,\n        n_layer=bert_config.num_hidden_layers,\n        n_head=bert_config.num_attention_heads,\n        resid_pdrop=bert_config.hidden_dropout_prob,\n        attn_pdrop=bert_config.attention_probs_dropout_prob,\n        activation_function=bert_config.hidden_act,\n        orig_vocab_size=bert_config.vocab_size,\n        last_layer_subset=False,\n    )"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        # The following attributes do not have a direct equivalent in BertConfig\n        # and are set to commonly used defaults for NomicBertConfig\n        pad_vocab_size_multiple=1,\n        pad_token_id=0,\n        position_embedding_type=\"absolute\",\n        use_cache=True,\n    )\n    return config"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    return NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        # The following attributes do not have a direct equivalent in BertConfig\n        # and are set to commonly used defaults for NomicBertConfig\n        pad_vocab_size_multiple=1,\n        last_layer_subset=False,\n        n_positions=bert_config.max_position_embeddings,\n        n_embd=bert_config.hidden_size,\n        n_layer=bert_config.num_hidden_layers,\n        n_head=bert_config.num_attention_heads,\n        n_inner=bert_config.intermediate_size,\n        resid_pdrop=bert_config.hidden_dropout_prob,\n        attn_pdrop=bert_config.attention_probs_dropout_prob,\n        activation_function=bert_config.hidden_act,\n        initializer_std=bert_config.initializer_range,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        type_vocab_size=bert_config.type_vocab_size,\n        # The following attributes are not present in BertConfig\n        # and are set to commonly used defaults"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    return NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        # The following attributes do not have a direct equivalent in BertConfig\n        # and are set to commonly used defaults for NomicConfig\n        pad_vocab_size_multiple=1,\n        last_layer_subset=False,\n    )"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    return NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        # The following attributes do not have a direct equivalent in BertConfig\n        # and are set to commonly used defaults for NomicBertConfig\n        n_positions=bert_config.max_position_embeddings,\n        n_embd=bert_config.hidden_size,\n        n_layer=bert_config.num_hidden_layers,\n        n_head=bert_config.num_attention_heads,\n        n_inner=bert_config.intermediate_size,\n        resid_pdrop=bert_config.hidden_dropout_prob,\n        attn_pdrop=bert_config.attention_probs_dropout_prob,\n        activation_function=bert_config.hidden_act,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        last_layer_subset=False,\n        pad_vocab_size_multiple=1,\n        orig_vocab_size=bert_config.vocab_size,\n    )"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        pad_vocab_size_multiple=bert_config.pad_vocab_size_multiple,\n        last_layer_subset=bert_config.last_layer_subset,\n        # The following attributes do not have a direct equivalent in BertConfig\n        # and are set to commonly used defaults for GPT2Config\n        n_positions=bert_config.max_position_embeddings,\n        n_embd=bert_config.hidden_size,\n        n_layer=bert_config.num_hidden_layers,\n        n_head=bert_config.num_attention_heads,\n        n_inner=bert_config.intermediate_size,\n        activation_function=bert_config.hidden_act,\n        resid_pdrop=bert_config.hidden_dropout_prob,\n        attn_pdrop=bert_config.attention_probs_dropout_prob,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        initializer_range=bert_config.initializer_range,\n        scale_attn_weights=True,\n    )\n    return config"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        # The following attributes do not have a direct equivalent in BertConfig\n        # and are set to commonly used defaults for NomicBertConfig\n        n_positions=bert_config.max_position_embeddings,\n        n_embd=bert_config.hidden_size,\n        n_layer=bert_config.num_hidden_layers,\n        n_head=bert_config.num_attention_heads,\n        n_inner=bert_config.intermediate_size,\n        resid_pdrop=bert_config.hidden_dropout_prob,\n        attn_pdrop=bert_config.attention_probs_dropout_prob,\n        activation_function=bert_config.hidden_act,\n        orig_vocab_size=bert_config.vocab_size,\n        pad_vocab_size_multiple=1,\n    )\n    return config"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    return NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        # The following attributes do not have a direct equivalent in BertConfig\n        # and are set to commonly used defaults for NomicBertConfig\n        pad_token_id=0,\n        position_embedding_type=\"absolute\",\n        use_cache=True,\n        # The following attributes are not present in BertConfig, but are specific to NomicBertConfig\n        n_positions=bert_config.max_position_embeddings,\n        n_embd=bert_config.hidden_size,\n        n_layer=bert_config.num_hidden_layers,\n        n_head=bert_config.num_attention_heads,\n        n_inner=bert_config.intermediate_size,\n        activation_function=bert_config.hidden_act,\n        resid_pdrop=bert_config.hidden_dropout_prob,\n        attn_pdrop=bert_config.attention_probs_dropout_prob,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_epsilon=bert_config.layer"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Set the type_vocab_size attribute to 2 if it is not already present\n    if not hasattr(bert_config, \"type_vocab_size\"):\n        bert_config.type_vocab_size = 2\n\n    # Set the initializer_range attribute to 0.02 if it is not already present\n    if not hasattr(bert_config, \"initializer_range\"):\n        bert_config.initializer_range = 0.02\n\n    # Set the layer_norm_epsilon attribute to 1e-12 if it is not already present\n    if not hasattr(bert_config, \"layer_norm_epsilon\"):\n        bert_config.layer_norm_epsilon = 1e-12\n\n    # Set the pad_token_id attribute to 0 if it is not already present\n    if not hasattr(bert_config, \"pad_token_id\"):\n        bert_config.pad_token_id = 0\n\n    # Set the position_embedding_type attribute to \"absolute\" if it is not already present\n    if not hasattr(bert_config, \"position_embedding_type\"):\n        bert_config.position_embedding_type = \"absolute\"\n\n    # Set the use_cache attribute to True if it is not already present\n    if not hasattr(bert_config, \"use_cache\"):\n        bert_config.use_cache = True\n\n    # Create a new NomicBertConfig object with the adapted configuration\n    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        # The following attributes do not have a direct equivalent in BertConfig\n        # and are set to commonly used defaults for NomicConfig\n        pad_vocab_size_multiple=1,\n        n_positions=bert_config.max_position_embeddings,\n        n_embd=bert_config.hidden_size,\n        n_layer=bert_config.num_hidden_layers,\n        n_head=bert_config.num_attention_heads,\n        n_inner=bert_config.intermediate_size,\n        resid_pdrop=bert_config.hidden_dropout_prob,\n        attn_pdrop=bert_config.attention_probs_dropout_prob,\n        activation_function=bert_config.hidden_act,\n        # The following attribute is not present in BertConfig, but is required for NomicConfig\n        last_layer_subset=False,\n    )\n    return config"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.render_points(camera)\n        elif self.render_type == Mesh.RenderType.TRIS:\n            self.render_tris(camera)\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            self.render_strips(camera)\n        elif self.render_type == Mesh.RenderType.LINES:\n            self.render_lines(camera)\n        else:\n            raise NotImplementedError\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.render_points(camera)\n        elif self.render_type == Mesh.RenderType.LINES:\n            self.render_lines(camera)\n        elif self.render_type == Mesh.RenderType.TRIS:\n            self.render_tris(camera)\n        elif self.render_type == Mesh.RenderType.QUADS:\n            self.render_quads(camera)\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            self.render_strips(camera)\n        else:\n            raise NotImplementedError\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.render_points(camera)\n        elif self.render_type == Mesh.RenderType.LINES:\n            self.render_lines(camera)\n        elif self.render_type == Mesh.RenderType.TRIS:\n            self.render_tris(camera)\n        elif self.render_type == Mesh.RenderType.QUADS:\n            self.render_quads(camera)\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            self.render_strips(camera)\n        else:\n            raise NotImplementedError\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        if self.render_type == Mesh.RenderType.TRIS:\n            self.use_gl_program(self.mesh_program)\n            self.upload_gl_uniforms(camera)\n            gl.glBindVertexArray(self.vao)\n            if hasattr(self, 'faces'):\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces), gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n            else:\n                gl.glDrawArrays(gl.GL_TRIANGLES, 0, len(self.verts))\n            gl.glBindVertexArray(0)\n        elif self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n            self.upload_gl_uniforms(camera)\n            gl.glBindVertexArray(self.vao)\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n            gl.glBindVertexArray(0)\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            self.use_gl_program(self.mesh_program)\n            self.upload_gl_uniforms(camera)\n            gl.glBindVertexArray(self.vao)\n            gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))\n            gl.glBindVertexArray(0)\n        elif self.render_type == Mesh.RenderType.QUADS:\n            self.use_gl_program(self.mesh_program)\n            self.upload_gl_uniforms(camera)\n            gl.glBindVertexArray(self.vao)\n            gl.glDrawArrays(gl.GL_QUADS, 0, len(self.verts))\n            gl.glBindVertexArray(0)\n        else:\n            raise NotImplementedError\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            self.use_gl_program(self.mesh_program)\n\n        self.upload_gl_uniforms(camera)\n        gl.glBindVertexArray(self.vao)\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces), gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.QUADS:\n            gl.glDrawElements(gl.GL_QUADS, len(self.faces), gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces), gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        else:\n            raise NotImplementedError\n\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            self.use_gl_program(self.mesh_program)\n\n        self.upload_gl_uniforms(camera)\n        gl.glBindVertexArray(self.vao)\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.QUADS:\n            gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        else:\n            raise NotImplementedError(f'Unsupported render type: {self.render_type}')\n\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            self.use_gl_program(self.mesh_program)\n\n        self.upload_gl_uniforms(camera)\n\n        gl.glBindVertexArray(self.vao)\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces), gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of indices\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces), gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of indices\n        elif self.render_type == Mesh.RenderType.QUADS:\n            gl.glDrawElements(gl.GL_QUADS, len(self.faces), gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of indices\n        else:\n            raise NotImplementedError\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            self.use_gl_program(self.mesh_program)\n\n        self.upload_gl_uniforms(camera)\n\n        gl.glBindVertexArray(self.vao)\n        if self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces), gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces), gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawElements(gl.GL_LINES, len(self.faces), gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.LINE_STRIPS:\n            gl.glDrawElements(gl.GL_LINE_STRIP, len(self.faces), gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        else:\n            raise NotImplementedError\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        if self.render_type == Mesh.RenderType.TRIS:\n            self.use_gl_program(self.mesh_program)\n        elif self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            raise NotImplementedError\n\n        self.upload_gl_uniforms(camera)\n\n        gl.glBindVertexArray(self.vao)\n        if self.render_type == Mesh.RenderType.TRIS:\n            if len(self.faces):\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n            else:\n                gl.glDrawArrays(gl.GL_TRIANGLES, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))\n        else:\n            raise NotImplementedError\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            self.use_gl_program(self.mesh_program)\n\n        self.upload_gl_uniforms(camera)\n        self.bind_gl_buffers()\n        self.draw_gl_buffers()\n        self.unbind_gl_buffers()\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            self.use_gl_program(self.mesh_program)\n\n        self.upload_gl_uniforms(camera)\n\n        gl.glBindVertexArray(self.vao)\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.QUADS:\n            gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawElements(gl.GL_LINES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        else:\n            raise NotImplementedError\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.render_points(camera)\n        else:\n            self.render_mesh(camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.render_points(camera)\n        elif self.render_type == Mesh.RenderType.TRIS:\n            self.render_tris(camera)\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            self.render_strips(camera)\n        elif self.render_type == Mesh.RenderType.LINES:\n            self.render_lines(camera)\n        else:\n            raise NotImplementedError\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        # Use the program\n        if self.render_type == Mesh.RenderType.TRIS:\n            self.use_gl_program(self.mesh_program)\n        elif self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            raise NotImplementedError\n\n        # Upload uniforms\n        self.upload_gl_uniforms(camera)\n\n        # Bind the vertex array object\n        gl.glBindVertexArray(self.vao)\n\n        # Draw the mesh\n        if self.render_type == Mesh.RenderType.TRIS:\n            if len(self.faces):\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces), gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n            else:\n                gl.glDrawArrays(gl.GL_TRIANGLES, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.POINTS:\n            if len(self.faces):\n                gl.glDrawElements(gl.GL_POINTS, len(self.faces), gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n            else:\n                gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            if len(self.faces):\n                gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces), gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n            else:\n                gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))\n        else:\n            raise NotImplementedError\n\n        # Unbind the vertex array object\n        gl.glBindVertexArray"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n        if self.render_type == Mesh.RenderType.TRIS:\n            self.use_gl_program(self.mesh_program)\n        elif self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            raise NotImplementedError\n\n        self.upload_gl_uniforms(camera)\n\n        gl.glBindVertexArray(self.vao)\n        if self.render_type == Mesh.RenderType.TRIS:\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))\n        else:\n            raise NotImplementedError\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            self.use_gl_program(self.mesh_program)\n\n        self.upload_gl_uniforms(camera)\n        self.upload_gl_uniforms(camera)  # might be overwritten\n\n        gl.glBindVertexArray(self.vao)\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        else:\n            gl.glDrawElements(gl.GL_TRIANGLES if self.render_type == Mesh.RenderType.TRIS else gl.GL_QUADS, len(self.faces), gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            self.use_gl_program(self.mesh_program)\n\n        self.upload_gl_uniforms(camera)\n        gl.glBindVertexArray(self.vao)\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces), gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # use indexed drawing\n        elif self.render_type == Mesh.RenderType.QUADS:\n            gl.glDrawElements(gl.GL_QUADS, len(self.faces), gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # use indexed drawing\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # use indexed drawing\n        else:\n            raise NotImplementedError\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n        if self.render_type == Mesh.RenderType.TRIS:\n            self.render_triangles(camera)\n        elif self.render_type == Mesh.RenderType.POINTS:\n            self.render_points(camera)\n        elif self.render_type == Mesh.RenderType.LINES:\n            self.render_lines(camera)\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            self.render_strips(camera)\n        elif self.render_type == Mesh.RenderType.QUADS:\n            self.render_quads(camera)\n        else:\n            raise NotImplementedError\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        # Use the correct program\n        if self.render_type == Mesh.RenderType.POINTS:\n            program = self.point_program\n        else:\n            program = self.mesh_program\n\n        # Setup the uniforms\n        self.use_gl_program(program)\n        self.upload_gl_uniforms(camera)\n\n        # Draw the mesh\n        gl.glBindVertexArray(self.vao)\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        else:\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        if self.render_type == Mesh.RenderType.TRIS:\n            self.use_gl_program(self.mesh_program)\n            self.upload_gl_uniforms(camera)\n        elif self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n            self.upload_gl_uniforms(camera)\n        else:\n            raise NotImplementedError\n\n        gl.glBindVertexArray(self.vao)\n        if self.render_type == Mesh.RenderType.TRIS:\n            if len(self.faces) > 0:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # indexed\n            else:\n                gl.glDrawArrays(gl.GL_TRIANGLES, 0, len(self.verts))  # non-indexed\n        elif self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.LINE_STRIPS:\n            gl.glDrawArrays(gl.GL_LINE_STRIP, 0, len(self.verts))\n        else:\n            raise NotImplementedError\n        gl.glBindVertexArray(0)\n\n        # Some house keepings\n        gl.glUseProgram(0)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, 0)\n        gl.glBindBuffer(gl.GL_ARRAY_BUFFER, 0)\n        gl.glBindBuffer(gl.GL_ELEMENT_"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if not hasattr(self, 'cu_tex'):\n            self.init_texture()\n\n        # assert self.use_quad_cuda, \"Need to enable cuda-opengl interop to copy from device to device, check creation of this Quad\"\n        w = w or self.W\n        h = h or self.H\n        if ptr.shape[-1] == 3:\n            ptr = torch.cat([ptr, ptr.new_ones(ptr.shape[:-1] + (1,)) * 255], dim=-1)  # add alpha channel\n\n        from cuda import cudart\n        kind = cudart.cudaMemcpyKind.cudaMemcpyHostToDevice\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsMapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n        cu_tex_arr = CHECK_CUDART_ERROR(cudart.cudaGraphicsSubResourceGetMappedArray(self.cu_tex, 0, 0))\n\n        CHECK_CUDART_ERROR(cudart.cudaMemcpy2DToArrayAsync(cu_tex_arr,\n                                                           x * 4 * ptr.element_size(),\n                                                           y,\n                                                           ptr.data_ptr(),\n                                                           w * 4 * ptr.element_size(),  # differently sized\n                                                           w * 4 * ptr.element_size(),  # rgba, should do a composition first\n                                                           h,\n                                                           kind,\n                                                           torch.cuda.current_stream().cuda_stream))\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsUnmapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if ptr.shape[-1] == 3:\n            ptr = torch.cat([ptr, ptr.new_ones(ptr.shape[:-1] + (1,)) * 255], dim=-1)  # add alpha channel\n\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        w = w or self.W\n        h = h or self.H\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if self.use_quad_cuda:\n            self.copy_to_texture(ptr, x, y, w, h)\n            return\n\n        if not hasattr(self, 'cu_tex'):\n            self.init_texture()\n\n        # assert self.use_quad_cuda, \"Need to enable cuda-opengl interop to copy from device to device, check creation of this Quad\"\n        w = w or self.W\n        h = h or self.H\n        if ptr.shape[-1] == 3:\n            ptr = torch.cat([ptr, ptr.new_ones(ptr.shape[:-1] + (1,)) * 255], dim=-1)  # add alpha channel\n\n        from cuda import cudart\n        kind = cudart.cudaMemcpyKind.cudaMemcpyHostToDevice\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsMapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n        cu_tex_arr = CHECK_CUDART_ERROR(cudart.cudaGraphicsSubResourceGetMappedArray(self.cu_tex, 0, 0))\n\n        CHECK_CUDART_ERROR(cudart.cudaMemcpy2DToArrayAsync(cu_tex_arr,\n                                                           x * 4 * ptr.element_size(),\n                                                           y,\n                                                           ptr.data_ptr(),\n                                                           w * 4 * ptr.element_size(),  # differently sized\n                                                           w * 4 * ptr.element_size(),  # rgba, should do a composition first\n                                                           h,\n                                                           kind,\n                                                           torch.cuda.current_stream().cuda_stream))\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsUnmapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if not self.use_quad_cuda:\n            self.upload_to_texture(ptr, x, y, w, h)\n            return\n\n        w = w or self.W\n        h = h or self.H\n        if ptr.shape[-1] == 3:\n            ptr = torch.cat([ptr, ptr.new_ones(ptr.shape[:-1] + (1,)) * 255], dim=-1)  # add alpha channel\n\n        from cuda import cudart\n        kind = cudart.cudaMemcpyKind.cudaMemcpyHostToDevice\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsMapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n        cu_tex_arr = CHECK_CUDART_ERROR(cudart.cudaGraphicsSubResourceGetMappedArray(self.cu_tex, 0, 0))\n\n        CHECK_CUDART_ERROR(cudart.cudaMemcpy2DToArrayAsync(cu_tex_arr,\n                                                           x * 4 * ptr.element_size(),\n                                                           y,\n                                                           ptr.data_ptr(),\n                                                           w * 4 * ptr.element_size(),  # differently sized\n                                                           w * 4 * ptr.element_size(),  # rgba, should do a composition first\n                                                           h,\n                                                           kind,\n                                                           torch.cuda.current_stream().cuda_stream))\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsUnmapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        w = w or self.W\n        h = h or self.H\n        if ptr.shape[-1] == 3:\n            ptr = np.concatenate([ptr, ptr[..., -1:] * 255], axis=-1)  # add alpha channel\n\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if not hasattr(self, 'cu_tex'):\n            self.init_texture()\n\n        if not self.use_quad_cuda:\n            self.upload_to_texture(ptr)\n            return\n\n        w = w or self.W\n        h = h or self.H\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        from cuda import cudart\n        kind = cudart.cudaMemcpyKind.cudaMemcpyHostToDevice\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsMapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n        cu_tex_arr = CHECK_CUDART_ERROR(cudart.cudaGraphicsSubResourceGetMappedArray(self.cu_tex, 0, 0))\n        CHECK_CUDART_ERROR(cudart.cudaMemcpy2DToArrayAsync(cu_tex_arr,\n                                                           x * 4 * ptr.element_size(),\n                                                           y,\n                                                           ptr.data_ptr(),\n                                                           w * 4 * ptr.element_size(),  # differently sized\n                                                           w * 4 * ptr.element_size(),  # rgba, should do a composition first\n                                                           h,\n                                                           kind,\n                                                           torch.cuda.current_stream().cuda_stream))\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsUnmapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if ptr.shape[-1] == 3:\n            ptr = torch.cat([ptr, ptr.new_ones(ptr.shape[:-1] + (1,)) * 255], dim=-1)  # add alpha channel\n        ptr = ptr.detach().cpu().numpy()\n        ptr = np.asarray(ptr, dtype=np.float32, order='C')  # this should only be invoked once\n\n        w = w or self.W\n        h = h or self.H\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if self.use_quad_cuda:\n            from cuda import cudart\n            kind = cudart.cudaMemcpyKind.cudaMemcpyHostToDevice\n            CHECK_CUDART_ERROR(cudart.cudaGraphicsMapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n            cu_tex_arr = CHECK_CUDART_ERROR(cudart.cudaGraphicsSubResourceGetMappedArray(self.cu_tex, 0, 0))\n\n            if isinstance(ptr, torch.Tensor):\n                ptr = ptr.detach().cpu().numpy()\n\n            w = w or self.W\n            h = h or self.H\n            CHECK_CUDART_ERROR(cudart.cudaMemcpy2DToArrayAsync(cu_tex_arr,\n                                                               x * 4 * ptr.element_size(),\n                                                               y,\n                                                               ptr.data_ptr(),\n                                                               w * 4 * ptr.element_size(),  # differently sized\n                                                               w * 4 * ptr.element_size(),  # rgba, should do a composition first\n                                                               h,\n                                                               kind,\n                                                               torch.cuda.current_stream().cuda_stream))\n            CHECK_CUDART_ERROR(cudart.cudaGraphicsUnmapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n\n        else:\n            if isinstance(ptr, torch.Tensor):\n                ptr = ptr.detach().cpu().numpy()\n\n            w = w or self.W\n            h = h or self.H\n            gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n            gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if not hasattr(self, 'cu_tex'):\n            self.init_texture()\n\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n        if isinstance(ptr, np.ndarray):\n            ptr = ptr.astype(np.uint8)\n\n        w = w or self.W\n        h = h or self.H\n        if ptr.shape[-1] == 3:\n            ptr = np.concatenate([ptr, np.ones(ptr.shape[:-1] + (1,), dtype=np.uint8) * 255], axis=-1)  # add alpha channel\n\n        from cuda import cudart\n        kind = cudart.cudaMemcpyKind.cudaMemcpyHostToDevice\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsMapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n        cu_tex_arr = CHECK_CUDART_ERROR(cudart.cudaGraphicsSubResourceGetMappedArray(self.cu_tex, 0, 0))\n        CHECK_CUDART_ERROR(cudart.cudaMemcpy2DToArrayAsync(cu_tex_arr,\n                                                           x * 4 * ptr.element_size(),\n                                                           y,\n                                                           ptr.data_ptr(),\n                                                           w * 4 * ptr.element_size(),  # differently sized\n                                                           w * 4 * ptr.element_size(),  # rgba, should do a composition first\n                                                           h,\n                                                           kind,\n                                                           torch.cuda.current_stream().cuda_stream))\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsUnmapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if not hasattr(self, 'cu_tex'):\n            self.init_texture()\n\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n        elif isinstance(ptr, np.ndarray):\n            pass\n        else:\n            raise NotImplementedError\n\n        w = w or self.W\n        h = h or self.H\n        if ptr.shape[-1] == 3:\n            ptr = np.concatenate([ptr, np.ones_like(ptr[..., :1]) * 255], axis=-1)\n\n        from cuda import cudart\n        kind = cudart.cudaMemcpyKind.cudaMemcpyHostToDevice\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsMapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n        cu_tex_arr = CHECK_CUDART_ERROR(cudart.cudaGraphicsSubResourceGetMappedArray(self.cu_tex, 0, 0))\n        CHECK_CUDART_ERROR(cudart.cudaMemcpy2DToArrayAsync(cu_tex_arr,\n                                                           x * 4 * ptr.element_size(),\n                                                           y,\n                                                           ptr.data_ptr(),\n                                                           w * 4 * ptr.element_size(),  # differently sized\n                                                           w * 4 * ptr.element_size(),  # rgba, should do a composition first\n                                                           h,\n                                                           kind,\n                                                           torch.cuda.current_stream().cuda_stream))\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsUnmapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if ptr.dtype.name == 'float32':\n            dtype = gl.GL_FLOAT\n        elif ptr.dtype.name == 'uint8':\n            dtype = gl.GL_UNSIGNED_BYTE\n        else:\n            raise NotImplementedError\n\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.numpy()\n\n        w = w or self.W\n        h = h or self.H\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, dtype, ptr.ctypes.data)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if not hasattr(self, 'cu_tex'):\n            self.init_texture()\n\n        w = w or self.W\n        h = h or self.H\n\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        if ptr.dtype != np.uint8:\n            ptr = np.asarray(ptr, dtype=np.uint8, order='C')\n\n        if self.use_quad_cuda:\n            from cuda import cudart\n            CHECK_CUDART_ERROR(cudart.cudaGraphicsMapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n            cu_tex_arr = CHECK_CUDART_ERROR(cudart.cudaGraphicsSubResourceGetMappedArray(self.cu_tex, 0, 0))\n            CHECK_CUDART_ERROR(cudart.cudaMemcpy2DToArrayAsync(cu_tex_arr,\n                                                               x * 4 * ptr.element_size(),\n                                                               y,\n                                                               ptr.data_ptr(),\n                                                               w * 4 * ptr.element_size(),  # differently sized\n                                                               w * 4 * ptr.element_size(),  # rgba, should do a composition first\n                                                               h,\n                                                               cudart.cudaMemcpyKind.cudaMemcpyHostToDevice,\n                                                               torch.cuda.current_stream().cuda_stream))\n            CHECK_CUDART_ERROR(cudart.cudaGraphicsUnmapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n        else:\n            gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n            gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        w = w or self.W\n        h = h or self.H\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n        if ptr.shape[-1] == 3:\n            ptr = np.concatenate([ptr, ptr[..., -1:] * 255], axis=-1)  # add alpha channel\n\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        gl.glUseProgram(self.quad_program)  # use a different program\n        gl.glActiveTexture(gl.GL_TEXTURE0)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Upload the texture\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))\n        gl.glBindVertexArray(0)\n\n        # Some house keepings\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if self.use_quad_cuda:\n            from cuda import cudart\n            kind = cudart.cudaMemcpyKind.cudaMemcpyHostToDevice\n            CHECK_CUDART_ERROR(cudart.cudaGraphicsMapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n            cu_tex_arr = CHECK_CUDART_ERROR(cudart.cudaGraphicsSubResourceGetMappedArray(self.cu_tex, 0, 0))\n\n            if isinstance(ptr, torch.Tensor):\n                ptr = ptr.detach().cpu().numpy()\n\n            if ptr.shape[-1] == 3:\n                ptr = np.asarray(ptr, dtype=np.uint8, order='C')\n                ptr = np.concatenate([ptr, np.full_like(ptr[..., :1], 255)], axis=-1)  # add alpha channel\n\n            CHECK_CUDART_ERROR(cudart.cudaMemcpy2DToArrayAsync(cu_tex_arr,\n                                                               x * 4 * ptr.element_size(),\n                                                               y,\n                                                               ptr.data_ptr(),\n                                                               w * 4 * ptr.element_size(),  # differently sized\n                                                               w * 4 * ptr.element_size(),  # rgba, should do a composition first\n                                                               h,\n                                                               kind,\n                                                               torch.cuda.current_stream().cuda_stream))\n            CHECK_CUDART_ERROR(cudart.cudaGraphicsUnmapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n        else:\n            self.upload_to_texture(ptr.detach().cpu().numpy(), x, y, w, h)\n\n        \"\"\"\n        This function uploads a portion or the entirety of a numpy array or a PyTorch tensor to a texture in OpenGL. It is designed to update the texture content starting from a specified"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.numpy()\n\n        if isinstance(ptr, np.ndarray):\n            if ptr.dtype != np.uint8:\n                ptr = ptr.astype(np.uint8)\n\n        if ptr.ndim == 3 and ptr.shape[-1] == 3:\n            ptr = np.concatenate([ptr, np.full(ptr.shape[:-1] + (1,), 255, dtype=np.uint8)], axis=-1)  # add alpha channel\n\n        w = w or self.W\n        h = h or self.H\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.cpu().numpy()\n\n        if ptr.shape[-1] == 3:\n            ptr = np.asarray(ptr, dtype=np.uint8, order='C')\n            ptr = np.concatenate([ptr, np.ones_like(ptr[..., :1]) * 255], axis=-1)\n        elif ptr.shape[-1] == 4:\n            ptr = np.asarray(ptr, dtype=np.uint8, order='C')\n        else:\n            raise ValueError('Unsupported number of channels')\n\n        w = w or self.W\n        h = h or self.H\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n\n        if self.use_quad_draw:\n            gl.glBindVertexArray(self.vao)\n            gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))\n            gl.glBindVertexArray(0)\n\n        # Some house keepings\n        gl.glBindTexture(gl.GL_TEXTURE_2D, 0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if not hasattr(self, 'cu_tex'):\n            self.init_texture()\n\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n        if ptr.dtype == np.uint8:\n            ptr = ptr.astype(np.float32) / 255\n\n        w = w or self.W\n        h = h or self.H\n        if len(ptr.shape) == 3:\n            ptr = np.ascontiguousarray(ptr.transpose(2, 0, 1))  # HWC to CHW\n            ptr = np.ascontiguousarray(ptr[..., [2, 1, 0]])  # RGB to BGR\n\n        from cuda import cudart\n        kind = cudart.cudaMemcpyKind.cudaMemcpyHostToDevice\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsMapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n        cu_tex_arr = CHECK_CUDART_ERROR(cudart.cudaGraphicsSubResourceGetMappedArray(self.cu_tex, 0, 0))\n        CHECK_CUDART_ERROR(cudart.cudaMemcpy2DToArrayAsync(cu_tex_arr,\n                                                           x * 4 * ptr.element_size(),\n                                                           y,\n                                                           ptr.data_ptr(),\n                                                           w * 4 * ptr.element_size(),  # differently sized\n                                                           w * 4 * ptr.element_size(),  # rgba, should do a composition first\n                                                           h,\n                                                           kind,\n                                                           torch.cuda.current_stream().cuda_stream))\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsUnmapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n        if ptr.ndim == 3:\n            ptr = ptr.transpose(2, 0, 1)  # HWC -> CHW\n        if ptr.shape[0] == 3:\n            ptr = np.concatenate([ptr, np.ones_like(ptr[0:1, ...]) * 255], axis=0)  # add alpha channel\n        assert ptr.shape[0] == 4\n\n        w = w or self.W\n        h = h or self.H\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if ptr.dtype == np.float32:\n            ptr = ptr.astype(np.uint8)\n        elif ptr.dtype == np.float64:\n            ptr = ptr.astype(np.float32)\n        elif ptr.dtype == torch.float16:\n            ptr = ptr.float().numpy()\n        elif ptr.dtype == torch.float32:\n            ptr = ptr.float().numpy()\n        elif ptr.dtype == torch.float64:\n            ptr = ptr.float().numpy()\n        elif ptr.dtype == torch.uint8:\n            ptr = ptr.numpy()\n        else:\n            raise NotImplementedError\n\n        w = w or self.W\n        h = h or self.H\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, 0)\n\n        if self.use_quad_draw:\n            self.blit(x, y, w, h)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, np.ndarray):\n            ptr = torch.from_numpy(ptr)\n        elif isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu()\n        else:\n            raise TypeError('Input must be a numpy array or a PyTorch tensor.')\n\n        if ptr.shape[-1] == 3:\n            ptr = torch.cat([ptr, ptr.new_ones(ptr.shape[:-1] + (1,)) * 255], dim=-1)  # add alpha channel\n\n        if ptr.shape[0] == 3:\n            ptr = ptr.permute(1, 2, 0)  # H, W, C\n\n        # If the input tensor has a channel dimension, it is assumed to be in the last dimension.\n        # If the input tensor has no channel dimension, it is assumed to be a grayscale image.\n        # If the input tensor has more than one channel dimension, it is assumed to be a color image.\n        if ptr.shape[-1] == 1:\n            ptr = ptr.repeat(1, 1, 3)  # grayscale to RGB\n\n        w = w or self.W\n        h = h or self.H\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr.contiguous().data_ptr())\n\n        # If you're using CUDA, you can also use cudaGraphicsMapResources and cudaGraphicsSubResourceGetMappedArray to map the texture to a CUDA array and copy the data directly to the GPU.\n        # This will be slower than the CPU version, but it can be useful for more complex operations.\n        # https://pytorch.org/tutorials/advanced/cpp_frontend.html\n        # https://pytorch."}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched\n    batch_dim = R.shape[:-2]\n    R = R.view(-1, 3, 3)\n    tvec = tvec.view(-1, 3, 1)\n    camera_matrix = camera_matrix.view(-1, 3, 3)\n    image_size = image_size.view(-1, 2)\n\n    # Validate shapes and values\n    assert R.shape == (batch_dim + (3, 3)), \"R must be a batch of rotation matrices of size (*, 3, 3)\"\n    assert tvec.shape == (batch_dim + (3, 1)), \"tvec must be a batch of translation vectors of size (*, 3, 1)\"\n    assert camera_matrix.shape == (batch_dim + (3, 3)), \"camera_matrix must be a batch of camera intrinsic matrices of size (*, 3, 3)\"\n    assert image_size.shape == (batch_dim + (2,)), \"image_size must be a batch of image sizes of size (*, 2)\"\n    assert (camera_matrix[:, 0, 0] > 0).all(), \"camera_matrix must have positive focal lengths in the x direction\"\n    assert (camera_matrix[:, 1, 1] > 0).all(), \"camera_matrix must have positive focal lengths in the y direction\"\n    assert (image_size[:, 0] > 0).all(), \"image_size must have positive widths\"\n    assert (image_size[:, 1] > 0).all(), \"image_size must have positive heights\"\n\n    # Compute camera parameters\n    fx = camera_matrix[:, 0, 0]\n    fy = camera_matrix[:, 1, 1]\n    cx = camera_matrix[:, 0, 2]\n    cy = camera_matrix[:, 1, 2]\n    w = image_size[:, 0]\n    h = image_size[:, 1]\n    znear = znear\n    zfar = znear * 1000"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure inputs are batched\n    R = R.unsqueeze(0) if R.dim() == 2 else R\n    tvec = tvec.unsqueeze(0) if tvec.dim() == 1 else tvec\n    camera_matrix = camera_matrix.unsqueeze(0) if camera_matrix.dim() == 2 else camera_matrix\n    image_size = image_size.unsqueeze(0) if image_size.dim() == 1 else image_size\n\n    # Validate input shapes and values\n    assert R.ndim == 3 and R.shape[-2:] == (3, 3), \"R must be a batch of 3x3 matrices\"\n    assert tvec.ndim == 2 and tvec.shape[-1] == 3, \"tvec must be a batch of 3D vectors\"\n    assert camera_matrix.ndim == 3 and camera_matrix.shape[-2:] == (3, 3), \"camera_matrix must be a batch of 3x3 matrices\"\n    assert image_size.ndim == 2 and image_size.shape[-1] == 2, \"image_size must be a batch of 2D vectors\"\n\n    # Extract focal length and principal point from camera matrix\n    fx = camera_matrix[..., 0, 0]\n    fy = camera_matrix[..., 1, 1]\n    cx = camera_matrix[..., 0, 2]\n    cy = camera_matrix[..., 1, 2]\n\n    # Adjust focal length and principal point based on image size\n    fx = fx * image_size[..., 0] / image_size[..., 1]\n    fy = fy * image_size[..., 1] / image_size[..., 0]\n    cx = cx * image_size[..., 0] / image_size[..., 1] + (image_size[..., 0] - 1) / 2\n    cy = cy * image_size[..., 1] / image_size[..., 0] + (image"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Check input shapes and values\n    if not isinstance(R, torch.Tensor):\n        raise ValueError(\"R must be a torch.Tensor.\")\n    if not isinstance(tvec, torch.Tensor):\n        raise ValueError(\"tvec must be a torch.Tensor.\")\n    if not isinstance(camera_matrix, torch.Tensor):\n        raise ValueError(\"camera_matrix must be a torch.Tensor.\")\n    if not isinstance(image_size, torch.Tensor):\n        raise ValueError(\"image_size must be a torch.Tensor.\")\n\n    # Ensure inputs are batched\n    if R.ndim == 2:\n        R = R.unsqueeze(0)\n    if tvec.ndim == 1:\n        tvec = tvec.unsqueeze(0)\n    if camera_matrix.ndim == 2:\n        camera_matrix = camera_matrix.unsqueeze(0)\n    if image_size.ndim == 1:\n        image_size = image_size.unsqueeze(0)\n\n    # Validate shapes\n    if R.shape[-2:] != (3, 3):\n        raise ValueError(\"R must be a batch of rotation matrices of shape (*, 3, 3).\")\n    if tvec.shape[-1] != 3:\n        raise ValueError(\"tvec must be a batch of translation vectors of shape (*, 3).\")\n    if camera_matrix.shape[-2:] != (3, 3):\n        raise ValueError(\"camera_matrix must be a batch of camera intrinsic matrices of shape (*, 3, 3).\")\n    if image_size.shape[-1] != 2:\n        raise ValueError(\"image_size must be a batch of image sizes of shape (*, 2).\")\n\n    # Validate values\n    if not torch.all(torch.isfinite(R)):\n        raise ValueError(\"R must be a batch of finite rotation matrices.\")\n    if not torch.all(torch.isfinite(tvec)):\n        raise"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure that all inputs are batched\n    batch_dim = R.shape[:-2]\n    R = R.view(-1, 3, 3)\n    tvec = tvec.view(-1, 3, 1)\n    camera_matrix = camera_matrix.view(-1, 3, 3)\n    image_size = image_size.view(-1, 2)\n\n    # Validate shapes and values of inputs\n    assert R.shape == (batch_dim + (3, 3)), \"R must be a batch of rotation matrices of size (*, 3, 3)\"\n    assert tvec.shape == (batch_dim + (3, 1)), \"tvec must be a batch of translation vectors of size (*, 3, 1)\"\n    assert camera_matrix.shape == (batch_dim + (3, 3)), \"camera_matrix must be a batch of camera intrinsic matrices of size (*, 3, 3)\"\n    assert image_size.shape == (batch_dim + (2,)), \"image_size must be a batch of image sizes of size (*, 2)\"\n    assert znear > 0, \"znear must be a positive value\"\n\n    # Extract focal lengths from camera matrix\n    fx = camera_matrix[..., 0, 0]\n    fy = camera_matrix[..., 1, 1]\n\n    # Compute sensor width based on focal lengths and image size\n    sensor_width = fx * image_size[..., 0] / image_size[..., 1]\n\n    # Compute camera position from rotation and translation\n    camera_position = -R.mT @ tvec\n\n    # Compute rotation matrix from rotation vector\n    rotation_matrix = matrix_to_rotation_6d(R)\n\n    # Compute focal length from znear and sensor width\n    focal_length = znear * sensor_width / (2 * np.tan(0.5 * np.arctan(sensor_width / (2 * znear))))\n\n    # Compute camera parameters\n    camera_params = torch.cat([camera_position"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched\n    batch_dim = R.shape[:-2]\n    R = R.view(-1, 3, 3)\n    tvec = tvec.view(-1, 3, 1)\n    camera_matrix = camera_matrix.view(-1, 3, 3)\n    image_size = image_size.view(-1, 2)\n\n    # Validate shapes and values\n    assert R.shape == (batch_dim + (3, 3))\n    assert tvec.shape == (batch_dim + (3, 1))\n    assert camera_matrix.shape == (batch_dim + (3, 3))\n    assert image_size.shape == (batch_dim + (2,))\n    assert torch.all(image_size > 0)\n\n    # Extract camera parameters\n    fx = camera_matrix[..., 0, 0]\n    fy = camera_matrix[..., 1, 1]\n    cx = camera_matrix[..., 0, 2]\n    cy = camera_matrix[..., 1, 2]\n    w = image_size[..., 0]\n    h = image_size[..., 1]\n\n    # Calculate sensor width\n    sensor_width = (w / fx) * 2\n\n    # Calculate focal length\n    focal_length = torch.sqrt(fx * fy)\n\n    # Calculate principal point offsets\n    cx_offset = cx - w / 2\n    cy_offset = cy - h / 2\n\n    # Adjust focal length and principal point offsets\n    focal_length = focal_length / torch.sqrt(1 + (cx_offset ** 2 + cy_offset ** 2) / (w * h))\n    cx_offset = cx_offset / torch.sqrt(1 + (cx_offset ** 2 + cy_offset ** 2) / (w * h))\n    cy_offset = cy_offset / torch.sqrt(1 + (cx_offset ** 2 + cy_offset **"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure inputs are batched\n    batch_dim = R.shape[:-2]\n    R = R.view(-1, 3, 3)\n    tvec = tvec.view(-1, 3, 1)\n    camera_matrix = camera_matrix.view(-1, 3, 3)\n    image_size = image_size.view(-1, 2)\n\n    # Ensure inputs are valid\n    assert torch.all(torch.isfinite(R)), \"Input rotation matrix contains NaN or infinite values.\"\n    assert torch.all(torch.isfinite(tvec)), \"Input translation vector contains NaN or infinite values.\"\n    assert torch.all(torch.isfinite(camera_matrix)), \"Input camera matrix contains NaN or infinite values.\"\n    assert torch.all(torch.isfinite(image_size)), \"Input image size contains NaN or infinite values.\"\n\n    # Compute focal length from camera matrix\n    fx, fy = camera_matrix[..., 0, 0], camera_matrix[..., 1, 1]\n    fx = torch.where(torch.abs(fx - fy) / fy > 0.01, torch.mean(camera_matrix[..., 0, 0:1], dim=-1), fx)\n    fy = torch.where(torch.abs(fx - fy) / fy > 0.01, torch.mean(camera_matrix[..., 1, 1:2], dim=-1), fy)\n    warn_once_about_pulsar_fxfy()\n\n    # Compute principal point from camera matrix and image size\n    cx, cy = camera_matrix[..., 0, 2], camera_matrix[..., 1, 2]\n    cx = cx / image_size[..., 0] * 2 - 1\n    cy = cy / image_size[..., 1] * 2 - 1\n\n    # Compute sensor width from focal length and image size\n    sensor_"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure inputs are batched\n    R = R.unsqueeze(0) if R.ndim == 2 else R\n    tvec = tvec.unsqueeze(0) if tvec.ndim == 1 else tvec\n    camera_matrix = camera_matrix.unsqueeze(0) if camera_matrix.ndim == 2 else camera_matrix\n    image_size = image_size.unsqueeze(0) if image_size.ndim == 1 else image_size\n\n    # Validate shapes and values\n    assert R.ndim == tvec.ndim == camera_matrix.ndim == image_size.ndim == 2, \"Inputs must be batched\"\n    assert R.shape[0] == tvec.shape[0] == camera_matrix.shape[0] == image_size.shape[0], \"Batch sizes must match\"\n    assert R.shape[1:] == (3, 3), \"Rotation matrices must be 3x3\"\n    assert tvec.shape[1:] == (3,), \"Translation vectors must be 3D\"\n    assert camera_matrix.shape[1:] == (3, 3), \"Camera matrices must be 3x3\"\n    assert image_size.shape[1:] == (2,), \"Image sizes must be 2D\"\n\n    # Calculate focal lengths\n    fx, fy = camera_matrix[..., 0, 0], camera_matrix[..., 1, 1]\n    fx_avg = (fx + camera_matrix[..., 0, 1]) / 2\n    fy_avg = (fy + camera_matrix[..., 1, 0]) / 2\n\n    # Normalize focal lengths\n    fx_norm = fx_avg / image_size[..., 0]\n    fy_norm = fy_avg / image_size[..., 1]\n\n    # Calculate principal points\n    cx_norm = camera_matrix[..., 0, 2] / image_size[..., 0]\n    cy_norm = camera_"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure inputs are batched\n    batch_dim = R.shape[:-2]\n    R = R.view(-1, 3, 3)\n    tvec = tvec.view(-1, 3, 1)\n    camera_matrix = camera_matrix.view(-1, 3, 3)\n    image_size = image_size.view(-1, 2)\n\n    # Validate input shapes\n    assert R.shape == (\n        *batch_dim, 3, 3\n    ), \"R must be a batch of rotation matrices of size (*, 3, 3)\"\n    assert tvec.shape == (\n        *batch_dim, 3, 1\n    ), \"tvec must be a batch of translation vectors of size (*, 3, 1)\"\n    assert camera_matrix.shape == (\n        *batch_dim, 3, 3\n    ), \"camera_matrix must be a batch of camera intrinsic matrices of size (*, 3, 3)\"\n    assert image_size.shape == (\n        *batch_dim, 2\n    ), \"image_size must be a batch of image sizes of size (*, 2)\"\n\n    # Calculate focal lengths\n    focal_lengths = camera_matrix[..., 0, 0]\n    if (\n        torch.abs(\n            (focal_lengths[..., 0] - focal_lengths[..., 1])\n            / focal_lengths[..., 0]\n        )\n        > 0.01\n    ):\n        warn_once_about_pulsar_fxfy()\n    focal_lengths = (focal_lengths[..., 0] + focal_lengths[..., 1]) / 2\n\n    # Calculate principal points\n    principal_points = camera_matrix[..., 0, 2]\n    principal_points = principal_points / image_size[..., 0] * 2 - 1\n\n    # Calculate sensor width\n    sensor_width = camera_matrix[..., 1, 1]\n\n    # Calculate camera position"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Check input shapes and values\n    if not isinstance(R, torch.Tensor):\n        raise TypeError(\"R must be a torch.Tensor\")\n    if not isinstance(tvec, torch.Tensor):\n        raise TypeError(\"tvec must be a torch.Tensor\")\n    if not isinstance(camera_matrix, torch.Tensor):\n        raise TypeError(\"camera_matrix must be a torch.Tensor\")\n    if not isinstance(image_size, torch.Tensor):\n        raise TypeError(\"image_size must be a torch.Tensor\")\n    if not isinstance(znear, float):\n        raise TypeError(\"znear must be a float\")\n\n    # Check if inputs are batched\n    if len(R.shape) != len(tvec.shape) or len(R.shape) != len(camera_matrix.shape) or len(R.shape) != len(image_size.shape):\n        raise ValueError(\"All inputs must be batched\")\n\n    # Check if batch sizes match\n    if not torch.all(torch.eq(R.shape[:-2], tvec.shape[:-2])):\n        raise ValueError(\"Batch sizes of R and tvec must match\")\n    if not torch.all(torch.eq(R.shape[:-2], camera_matrix.shape[:-2])):\n        raise ValueError(\"Batch sizes of R and camera_matrix must match\")\n    if not torch.all(torch.eq(R.shape[:-2], image_size.shape[:-2])):\n        raise ValueError(\"Batch sizes of R and image_size must match\")\n\n    # Check if image size is a vector of size 2\n    if len(image_size.shape) != 2 or image_size.shape[-1] != 2:\n        raise ValueError(\"image_size must be a vector of size 2\")\n\n    # Check if znear is positive\n    if not torch.all(torch.gt(znear, 0)):\n        raise ValueError(\"znear must be positive\")"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure that all inputs are batched\n    batch_dim = R.shape[:-2]\n    R = R.view(-1, 3, 3)\n    tvec = tvec.view(-1, 3)\n    camera_matrix = camera_matrix.view(-1, 3, 3)\n    image_size = image_size.view(-1, 2)\n\n    # Validate shapes and values\n    assert R.shape == (batch_dim + (3, 3))\n    assert tvec.shape == (batch_dim + (3,))\n    assert camera_matrix.shape == (batch_dim + (3, 3))\n    assert image_size.shape == (batch_dim + (2,))\n    assert znear > 0\n\n    # Compute camera position\n    camera_position = -R.transpose(-1, -2) @ tvec[..., None]  # B, 3, 1\n    camera_position = camera_position[..., 0]  # B, 3\n\n    # Compute focal length\n    focal_length = camera_matrix[..., 0, 0] / camera_matrix[..., 0, 1]  # B\n\n    # Compute sensor width\n    sensor_width = image_size[..., 0] / camera_matrix[..., 0, 0]  # B\n\n    # Compute camera parameters\n    camera_parameters = torch.cat([camera_position, matrix_to_rotation_6d(R), focal_length[:, None], sensor_width[:, None]], dim=-1)\n\n    return camera_parameters\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure inputs are batched\n    R = R.unsqueeze(0) if R.dim() == 2 else R\n    tvec = tvec.unsqueeze(0) if tvec.dim() == 1 else tvec\n    camera_matrix = camera_matrix.unsqueeze(0) if camera_matrix.dim() == 2 else camera_matrix\n    image_size = image_size.unsqueeze(0) if image_size.dim() == 1 else image_size\n\n    # Validate input shapes and values\n    assert R.shape[-2:] == (3, 3), \"R should be a batch of rotation matrices of size (*, 3, 3)\"\n    assert tvec.shape[-1] == 3, \"tvec should be a batch of translation vectors of size (*, 3)\"\n    assert camera_matrix.shape[-2:] == (3, 3), \"camera_matrix should be a batch of camera intrinsic matrices of size (*, 3, 3)\"\n    assert image_size.shape[-1] == 2, \"image_size should be a batch of image sizes of size (*, 2)\"\n    assert znear > 0, \"znear should be a positive number\"\n\n    # Extract focal lengths and sensor width from camera matrix\n    fx, fy = camera_matrix[..., 0, 0], camera_matrix[..., 1, 1]\n    sensor_width = camera_matrix[..., 0, 2] * 2 / image_size[..., 0]\n\n    # Compute camera position and rotation\n    camera_position = -R.mT @ tvec\n    camera_rotation = matrix_to_rotation_6d(R)\n\n    # Compute focal length and sensor width for Pulsar\n    focal_length = torch.sqrt((fx + fy) / 2)\n    focal_length = focal_length.unsqueeze(-1)\n    sensor_width = sensor_width.unsqueeze(-1)\n\n    # Adjust focal length and sensor width for Pulsar\n    focal_length = focal_length * image_size["}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure inputs are batched\n    batch_dim = R.shape[:-2]\n    R = R.reshape(-1, 3, 3)\n    tvec = tvec.reshape(-1, 3)\n    camera_matrix = camera_matrix.reshape(-1, 3, 3)\n    image_size = image_size.reshape(-1, 2)\n\n    # Ensure inputs are valid\n    assert R.shape == (R.shape[0], 3, 3), \"R must be a batch of rotation matrices of shape (*, 3, 3)\"\n    assert tvec.shape == (tvec.shape[0], 3), \"tvec must be a batch of translation vectors of shape (*, 3)\"\n    assert camera_matrix.shape == (camera_matrix.shape[0], 3, 3), \"camera_matrix must be a batch of camera intrinsic matrices of shape (*, 3, 3)\"\n    assert image_size.shape == (image_size.shape[0], 2), \"image_size must be a batch of image sizes of shape (*, 2)\"\n\n    # Compute camera parameters\n    fx, fy = camera_matrix[..., 0, 0], camera_matrix[..., 1, 1]\n    cx, cy = camera_matrix[..., 0, 2], camera_matrix[..., 1, 2]\n    width, height = image_size[..., 0], image_size[..., 1]\n    sensor_width = fx * 2 / width\n    sensor_height = fy * 2 / height\n    f = torch.stack([fx, fy], dim=-1)\n    c = torch.stack([cx, cy], dim=-1)\n    n = torch.stack([znear, znear], dim=-1)\n    t = tvec\n    R = R.reshape(*batch_dim, 6)\n    return torch.stack([f, c, n, t, R, sensor_width, sensor_height], dim=-1)"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched\n    batch_dim = R.shape[:-2]\n    R = R.view(-1, 3, 3)\n    tvec = tvec.view(-1, 3, 1)\n    camera_matrix = camera_matrix.view(-1, 3, 3)\n    image_size = image_size.view(-1, 2)\n\n    # Validate shapes and values\n    assert R.shape[-2:] == (3, 3), \"R must be a (*, 3, 3) tensor\"\n    assert tvec.shape[-2:] == (3, 1), \"tvec must be a (*, 3, 1) tensor\"\n    assert camera_matrix.shape[-2:] == (3, 3), \"camera_matrix must be a (*, 3, 3) tensor\"\n    assert image_size.shape[-1] == 2, \"image_size must be a (*, 2) tensor\"\n    assert znear > 0, \"znear must be a positive value\"\n\n    # Extract intrinsic parameters\n    fx = (camera_matrix[..., 0, 0] + camera_matrix[..., 1, 1]) / 2\n    fy = fx\n    cx = camera_matrix[..., 0, 2]\n    cy = camera_matrix[..., 1, 2]\n    sensor_width = image_size[..., 0] / fx\n\n    # Adjust focal length and principal point offsets\n    fx = torch.sqrt(fx * fy)\n    fy = fx\n    cx = cx * image_size[..., 0] / fx - sensor_width / 2\n    cy = cy * image_size[..., 1] / fy - sensor_width / 2\n\n    # Normalize focal length\n    fx = fx / (znear * 2)\n    fy = fy / (znear * 2)\n\n    # Convert rotation matrix to 6D representation\n    rotation_6d = matrix_to_rotation"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure inputs are batched and valid\n    batch_dim = R.shape[:-2]\n    assert R.shape[-2:] == (3, 3), \"Rotation matrices must be 3x3\"\n    assert tvec.shape[-1] == 3, \"Translation vectors must be 3D\"\n    assert camera_matrix.shape[-2:] == (3, 3), \"Camera matrices must be 3x3\"\n    assert image_size.shape[-1] == 2, \"Image size must be 2D\"\n\n    # Compute focal lengths and principal points\n    fx = camera_matrix[..., 0, 0]\n    fy = camera_matrix[..., 1, 1]\n    cx = camera_matrix[..., 0, 2]\n    cy = camera_matrix[..., 1, 2]\n    fx = torch.where(fx == 0, torch.ones_like(fx), fx)\n    fy = torch.where(fy == 0, torch.ones_like(fy), fy)\n    cx = torch.where(cx == 0, torch.ones_like(cx) * image_size[..., 0] / 2, cx)\n    cy = torch.where(cy == 0, torch.ones_like(cy) * image_size[..., 1] / 2, cy)\n\n    # Compute sensor width\n    sensor_width = 2 * fx * znear / image_size[..., 0]\n\n    # Compute camera parameters\n    camera_params = torch.cat([\n        tvec,\n        matrix_to_rotation_6d(R),\n        fx.view(*batch_dim, 1),\n        fy.view(*batch_dim, 1),\n        cx.view(*batch_dim, 1),\n        cy.view(*batch_dim, 1),\n        sensor_width.view(*batch_dim, 1),\n    ], dim=-1)\n\n    return camera_params"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched\n    batch_dim = R.shape[:-2]\n    R = R.view(-1, 3, 3)\n    tvec = tvec.view(-1, 3)\n    camera_matrix = camera_matrix.view(-1, 3, 3)\n    image_size = image_size.view(-1, 2)\n\n    # Validate shapes and values\n    assert R.shape == (\n        *batch_dim, 3, 3\n    ), \"R must be a batch of rotation matrices of size (*, 3, 3)\"\n    assert tvec.shape == (\n        *batch_dim, 3\n    ), \"tvec must be a batch of translation vectors of size (*, 3)\"\n    assert camera_matrix.shape == (\n        *batch_dim, 3, 3\n    ), \"camera_matrix must be a batch of camera intrinsic matrices of size (*, 3, 3)\"\n    assert image_size.shape == (\n        *batch_dim, 2\n    ), \"image_size must be a batch of image sizes of size (*, 2)\"\n    assert znear > 0, \"znear must be a positive number\"\n\n    # Extract focal lengths from camera matrix\n    fx = camera_matrix[..., 0, 0]\n    fy = camera_matrix[..., 1, 1]\n\n    # Calculate sensor width from image size and focal lengths\n    sensor_width = (\n        (2 * fx / image_size[..., 0]) + (2 * fy / image_size[..., 1])\n    ) / 2\n\n    # Calculate camera position from translation vector\n    camera_position = -R.transpose(-1, -2) @ tvec[..., None]\n\n    # Calculate camera rotation from rotation matrix\n    camera_rotation = matrix_to_rotation_6d(R)\n\n    # Calculate focal length ratio\n    focal_length_ratio = fy / fx\n\n    # Calculate near and far clipping planes\n    near_clipping_plane ="}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure inputs are batched\n    batch_dim = R.shape[:-2]\n    R = R.view(-1, 3, 3)\n    tvec = tvec.view(-1, 3)\n    camera_matrix = camera_matrix.view(-1, 3, 3)\n    image_size = image_size.view(-1, 2)\n\n    # Validate shapes and values of inputs\n    assert R.shape == (batch_dim + (3, 3)), \"R should be (*, 3, 3)\"\n    assert tvec.shape == (batch_dim + (3,)), \"tvec should be (*, 3)\"\n    assert camera_matrix.shape == (batch_dim + (3, 3)), \"camera_matrix should be (*, 3, 3)\"\n    assert image_size.shape == (batch_dim + (2,)), \"image_size should be (*, 2)\"\n    assert (znear > 0).all(), \"znear should be positive\"\n\n    # Extract camera parameters\n    fx = camera_matrix[..., 0, 0] / (camera_matrix[..., 0, 0] / camera_matrix[..., 1, 1]).sqrt()\n    fy = camera_matrix[..., 1, 1] / (camera_matrix[..., 0, 0] / camera_matrix[..., 1, 1]).sqrt()\n    cx = camera_matrix[..., 0, 2]\n    cy = camera_matrix[..., 1, 2]\n    width = image_size[..., 0]\n    height = image_size[..., 1]\n    near = znear\n    far = near * 1000\n\n    # Convert rotation matrix to 6D representation\n    rot = matrix_to_rotation_6d(R)\n\n    # Calculate camera position\n    pos = -R.transpose(-1, -2) @ tvec[..., None]\n\n    # Calculate focal length\n    focal = (fx + fy) / "}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched and have the same batch dimension\n    batch_dim = R.shape[:-2]\n    R = R.view(-1, 3, 3)\n    tvec = tvec.view(-1, 3, 1)\n    camera_matrix = camera_matrix.view(-1, 3, 3)\n    image_size = image_size.view(-1, 2)\n\n    # Calculate focal lengths\n    focal_lengths = torch.stack([camera_matrix[..., 0, 0], camera_matrix[..., 1, 1]], dim=-1)\n    if focal_lengths.abs().max(dim=-1)[0].max() - focal_lengths.abs().min(dim=-1)[0].min() > 0.01:\n        warn_once_about_pulsar_fxfy()\n    focal_lengths = focal_lengths.mean(dim=-1, keepdim=True)\n\n    # Calculate principal points\n    principal_points = torch.stack([camera_matrix[..., 0, 2], camera_matrix[..., 1, 2]], dim=-1)\n\n    # Calculate sensor width\n    sensor_width = image_size[..., 1] / image_size[..., 0] * focal_lengths[..., 0]\n\n    # Calculate camera parameters\n    camera_params = torch.cat([\n        tvec[..., 0],\n        tvec[..., 1],\n        tvec[..., 2],\n        matrix_to_rotation_6d(R),\n        focal_lengths,\n        principal_points,\n        sensor_width,\n        torch.ones_like(focal_lengths) * znear,\n    ], dim=-1)\n\n    # Reshape the camera parameters to match the original batch dimension\n    camera_params = camera_params.view(*batch_dim, -1)\n\n    return camera_params"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched and have the same batch size\n    batch_dim = R.shape[:-2]\n    assert R.shape == tvec.shape == camera_matrix.shape == image_size.shape == batch_dim + (3, 3)\n    assert tvec.shape[-2] == 3\n    assert camera_matrix.shape[-2] == camera_matrix.shape[-1] == 3\n    assert image_size.shape[-2] == image_size.shape[-1] == 2\n\n    # Extract focal length and principal point from camera matrix\n    fx = camera_matrix[..., 0, 0]\n    fy = camera_matrix[..., 1, 1]\n    px = camera_matrix[..., 0, 2]\n    py = camera_matrix[..., 1, 2]\n\n    # Adjust focal length and principal point based on image size\n    fx = fx / image_size[..., 0]\n    fy = fy / image_size[..., 1]\n    px = px / image_size[..., 0]\n    py = py / image_size[..., 1]\n\n    # Compute sensor width from focal length and principal point\n    sensor_width = 2 * torch.atan(image_size[..., 0] / (2 * fx))\n\n    # Compute camera position from rotation and translation\n    tvec = tvec.view(-1, 3)\n    R = R.view(-1, 3, 3)\n    C = -R.mT @ tvec\n    C = C.view(*batch_dim + (3,))\n\n    # Compute rotation from rotation matrix\n    rotation_6d = matrix_to_rotation_6d(R)\n\n    # Compute camera parameters\n    camera_params = torch.cat([C, rotation_6d, fx[..., None], fy[..., None], px[..., None], py[..., None], sensor_width[..., None]], dim=-1)\n\n    # Return camera parameters"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched and validated\n    R = R.unsqueeze(0) if R.dim() == 2 else R\n    tvec = tvec.unsqueeze(0) if tvec.dim() == 1 else tvec\n    camera_matrix = camera_matrix.unsqueeze(0) if camera_matrix.dim() == 2 else camera_matrix\n    image_size = image_size.unsqueeze(0) if image_size.dim() == 1 else image_size\n\n    # Extract camera parameters\n    H, W = image_size.unbind(-1)\n    fx, fy = camera_matrix[..., 0, 0], camera_matrix[..., 1, 1]\n    cx, cy = camera_matrix[..., 0, 2], camera_matrix[..., 1, 2]\n    znear = znear\n\n    # Calculate camera position\n    tvec = tvec.unsqueeze(-1)  # B, 3, 1\n    Rt = torch.cat([R, tvec], dim=-1)  # B, 3, 4\n    C = -Rt[..., :3, -1]  # B, 3\n\n    # Calculate sensor width\n    sensor_width = (cx * 2) / W\n\n    # Calculate focal length\n    if fx != fy:\n        warn_once_about_pulsar_fxfy()\n    focal_length = (fx + fy) / 2\n\n    # Create camera parameters tensor\n    camera_params = torch.stack([C, R, focal_length, sensor_width, znear], dim=-1)  # B, 5\n\n    return camera_params"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure that all inputs are batched\n    batch_dim = R.shape[:-2]\n    R = R.view(-1, 3, 3)\n    tvec = tvec.view(-1, 3, 1)\n    camera_matrix = camera_matrix.view(-1, 3, 3)\n    image_size = image_size.view(-1, 2)\n\n    # Validate shapes and values\n    assert R.shape == (batch_dim + (3, 3)), \"Invalid rotation matrix shape\"\n    assert tvec.shape == (batch_dim + (3, 1)), \"Invalid translation vector shape\"\n    assert camera_matrix.shape == (batch_dim + (3, 3)), \"Invalid camera matrix shape\"\n    assert image_size.shape == (batch_dim + (2,)), \"Invalid image size shape\"\n    assert torch.all(image_size > 0), \"Image size must be positive\"\n    assert torch.all(znear > 0), \"znear must be positive\"\n\n    # Compute camera parameters\n    focal_length = torch.sqrt(camera_matrix[..., 0, 0] ** 2 + camera_matrix[..., 1, 1] ** 2) / 2\n    sensor_width = image_size[..., 0] / focal_length\n    principal_point = camera_matrix[..., 0:2, 2]\n    camera_position = -R.transpose(-2, -1) @ tvec\n    camera_rotation = matrix_to_rotation_6d(R)\n\n    # Adjust focal length and principal point to match OpenCV convention\n    focal_length = focal_length / image_size[..., 0]\n    principal_point = principal_point / image_size[..., 0]\n    sensor_width = sensor_width / image_size[..., 0]\n    focal_length = focal_length / image_size[..., 1]\n    principal_point = principal_point / image_size[..., 1]\n    sensor_width = sensor_width / image_"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        w = w or self.W\n        h = h or self.H\n        old = gl.glGetInteger(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        old_scissor = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n        gl.glScissor(x, y, w, h)\n\n        gl.glUseProgram(self.quad_program)\n        gl.glBindVertexArray(self.vao)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glBindVertexArray(0)\n        gl.glUseProgram(0)\n\n        gl.glViewport(*old)\n        gl.glScissor(*old_scissor)\n\n    "}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        w = w or self.W\n        h = h or self.H\n        old = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        old_scissor = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n        gl.glScissor(x, y, w, h)\n\n        # Prepare to render to textures\n        gl.glUseProgram(self.quad_program)\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glBindVertexArray(0)\n\n        gl.glViewport(*old)\n        gl.glScissor(*old_scissor)\n\n    "}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        w = w or self.W\n        h = h or self.H\n        old = gl.glGetInteger(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        old_scissor = gl.glGetInteger(gl.GL_SCISSOR_BOX)\n        gl.glScissor(x, y, w, h)\n\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        gl.glUseProgram(self.quad_program)\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glBindVertexArray(old_vao)\n        gl.glUseProgram(old_prog)\n\n        gl.glViewport(*old)\n        gl.glScissor(*old_scissor)\n\n        \"\"\"\n        The draw method in the Quad class is responsible for rendering a textured quadrilateral on the screen. It uses OpenGL conventions, specifically targeting the lower-left corner as the origin. If the use_quad_draw attribute is False, it falls back to a simpler blit method for drawing. Otherwise, it sets up a specific viewport and scissor box for rendering, activates a shader program, binds a texture, and draws the quadrilateral using vertex data. After drawing, it restores the viewport and scissor box to their original sizes.\n\n        Input-Output Arguments\n        :param self: Quad. An instance of the Quad class. It uses attributes like use_quad_draw, W, H, quad_program, tex, and vao for drawing operations.\n        :return: No return values. This method performs drawing operations and does not return any value."}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        w = w or self.W\n        h = h or self.H\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        gl.glUseProgram(self.quad_program)\n\n        # Prepare to render to textures\n        gl.glBindVertexArray(self.vao)\n        gl.glViewport(x, y, w, h)  # set the viewport\n        gl.glScissor(x, y, w, h)  # set the scissor box\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glBindVertexArray(old_vao)\n        gl.glUseProgram(old_prog)\n        gl.glViewport(0, 0, self.W, self.H)  # reset the viewport\n        gl.glScissor(0, 0, self.W, self.H)  # reset the scissor box\n\n        \"\"\"\n        The draw method in the Quad class is responsible for rendering a textured quadrilateral on the screen. It uses OpenGL conventions, specifically targeting the lower-left corner as the origin. If the use_quad_draw attribute is False, it falls back to a simpler blit method for drawing. Otherwise, it sets up a specific viewport and scissor box for rendering, activates a shader program, binds a texture, and draws the quadrilateral using vertex data. After drawing, it restores the viewport and scissor box to their original sizes.\n\n        Input-Output Arguments\n        :param self: Quad. An instance of the Quad class. It uses attributes like use_quad_draw, W, H, quad_program, tex, and vao for drawing operations.\n        :return: No return values. This"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        w = w or self.W\n        h = h or self.H\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        gl.glUseProgram(self.quad_program)\n        gl.glBindVertexArray(self.vao)\n\n        # Prepare to render to textures\n        gl.glViewport(x, y, w, h)  # the height is flipped\n        gl.glScissor(x, y, w, h)  # the height is flipped\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n\n        # Restore the original state\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n        gl.glBindVertexArray(old_vao)\n        gl.glUseProgram(old_prog)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        w = w or self.W\n        h = h or self.H\n\n        # Prepare for the actual rendering, previous operations could rebind the vertex array\n        gl.glViewport(x, y, w, h)  # set viewport\n        gl.glScissor(x, y, w, h)  # set scissor\n        gl.glUseProgram(self.quad_program)\n        gl.glBindVertexArray(self.vao)\n\n        # The actual drawing pass with render things out to the write_fbo\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n\n        # Restore states of things\n        gl.glViewport(0, 0, self.W, self.H)  # set viewport\n        gl.glScissor(0, 0, self.W, self.H)  # set scissor\n        gl.glBindVertexArray(0)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        w = w or self.W\n        h = h or self.H\n        old = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)  # set the viewport\n        gl.glScissor(x, y, w, h)  # set the scissor box\n        gl.glUseProgram(self.quad_program)\n        gl.glBindVertexArray(self.vao)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glBindVertexArray(0)\n        gl.glUseProgram(0)\n        gl.glViewport(*old)  # restore the viewport\n        gl.glScissor(0, 0, self.W, self.H)  # restore the scissor box\n\n    "}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        w = w or self.W\n        h = h or self.H\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        gl.glUseProgram(self.quad_program)\n\n        # Prepare to render to textures\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n        gl.glEnable(gl.GL_SCISSOR_TEST)\n        gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glBindVertexArray(old_vao)\n        gl.glDisable(gl.GL_SCISSOR_TEST)\n        gl.glUseProgram(old_prog)\n\n        # Restore the original state\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n        gl.glDisable(gl.GL_SCISSOR_TEST)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        w = w or self.W\n        h = h or self.H\n        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Prepare for the actual rendering, previous operations could rebind the vertex array\n        gl.glViewport(x, y, w, h)  # viewport\n        gl.glScissor(x, y, w, h)  # scissor\n        gl.glEnable(gl.GL_SCISSOR_TEST)\n        gl.glUseProgram(self.quad_program)\n        gl.glBindVertexArray(self.vao)\n\n        # The actual drawing pass with render things out to the write_fbo\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n\n        # Restore states of things\n        gl.glDisable(gl.GL_SCISSOR_TEST)\n        gl.glBindVertexArray(0)\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        w = w or self.W\n        h = h or self.H\n        gl.glViewport(x, y, w, h)  # lower left corner, width and height\n        gl.glScissor(x, y, w, h)  # lower left corner, width and height\n        gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)\n\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        gl.glUseProgram(self.quad_program)\n        self.upload_gl_uniforms(None)  # should be a noop\n\n        # Prepare to render to textures\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glBindVertexArray(old_vao)\n        gl.glUseProgram(old_prog)\n\n        # Restore the original state\n        gl.glViewport(0, 0, self.W, self.H)  # upper left corner, width and height\n        gl.glScissor(0, 0, self.W, self.H)  # upper left corner, width and height\n\n        \"\"\"\n        The draw method in the Quad class is responsible for rendering a textured quadrilateral on the screen. It uses OpenGL conventions, specifically targeting the lower-left corner as the origin. If the use_quad_draw attribute is False, it falls back to a simpler blit method for drawing. Otherwise, it sets up a specific viewport and scissor box for rendering, activates a shader program, binds a texture, and draws the quadrilateral using vertex data. After drawing, it restores the viewport and scissor"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        w = w or self.W\n        h = h or self.H\n\n        # Prepare for the actual rendering, previous operations could rebind the vertex array\n        gl.glBindVertexArray(self.vao)\n        gl.glUseProgram(self.quad_program)\n\n        # Set up the viewport and scissor box\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Bind the texture and draw the quadrilateral\n        gl.glActiveTexture(gl.GL_TEXTURE0)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n\n        # Restore the original state\n        gl.glBindVertexArray(0)\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Prepare for the actual rendering, previous operations could rebind the vertex array\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        gl.glUseProgram(self.quad_program)\n        self.upload_gl_uniforms(camera)\n\n        # Prepare to render to textures\n        gl.glBindVertexArray(self.vao)\n\n        # Setup the viewport and scissor box\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Draw the quadrilateral\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n\n        # Restore the original state\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n\n        # Restore the original state\n        gl.glBindVertexArray(old_vao)\n        gl.glUseProgram(old_prog)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        w = w or self.W\n        h = h or self.H\n        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Prepare for the actual rendering, previous operations could rebind the vertex array\n        gl.glBindVertexArray(self.vao)\n        gl.glViewport(x, y, w, h)  # set viewport\n        gl.glScissor(x, y, w, h)  # set scissor box\n        gl.glClear(gl.GL_COLOR_BUFFER_BIT)  # clear the screen\n        gl.glUseProgram(self.quad_program)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # The actual drawing pass with render things out to the write_fbo\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n\n        # Restore states of things\n        gl.glViewport(0, 0, self.W, self.H)  # set viewport\n        gl.glScissor(0, 0, self.W, self.H)  # set scissor box\n        gl.glBindVertexArray(0)\n        gl.glUseProgram(0)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, 0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        w = w or self.W\n        h = h or self.H\n        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Prepare for the actual rendering, previous operations could rebind the vertex array\n        gl.glViewport(x, y, w, h)  # set the viewport\n        gl.glScissor(x, y, w, h)  # set the scissor box\n        gl.glUseProgram(self.quad_program)\n        gl.glBindVertexArray(self.vao)\n\n        # The actual drawing pass with render things out to the write_fbo\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n\n        # Restore states of things\n        gl.glBindVertexArray(0)\n        gl.glViewport(0, 0, self.W, self.H)  # restore the viewport\n        gl.glScissor(0, 0, self.W, self.H)  # restore the scissor box\n\n        # gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, 0)  # restore the framebuffer\n\n        # gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)  # write buffer defaults to 0\n        # gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, 0)  # render the final content onto screen\n        # gl.glReadBuffer(gl.GL_COLOR_ATTACHMENT0)\n        # gl.glBlitFramebuffer(x, y, w, h, 0, 0, self.W, self.H, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        # gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, 0)\n\n        # gl.glViewport(0, 0, self.W, self.H)  # restore the viewport\n        # gl."}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Prepare to render to textures\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        gl.glUseProgram(self.quad_program)\n        gl.glBindVertexArray(self.vao)\n\n        # Setup the viewport and scissor box\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n        gl.glEnable(gl.GL_SCISSOR_TEST)\n\n        # Bind the texture and draw the quadrilateral\n        gl.glActiveTexture(gl.GL_TEXTURE0)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n\n        # Restore the original state\n        gl.glUseProgram(old_prog)\n        gl.glBindVertexArray(old_vao)\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glDisable(gl.GL_SCISSOR_TEST)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        w = w or self.W\n        h = h or self.H\n        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Prepare for the actual rendering, previous operations could rebind the vertex array\n        gl.glViewport(x, y, w, h)  # set the viewport to the lower left corner\n        gl.glScissor(x, y, w, h)  # set the scissor to the lower left corner\n        gl.glEnable(gl.GL_SCISSOR_TEST)  # enable the scissor test\n        gl.glUseProgram(self.quad_program)  # use a different program\n        gl.glBindVertexArray(self.vao)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n\n        # Restore states of things\n        gl.glBindVertexArray(0)\n        gl.glUseProgram(0)\n        gl.glDisable(gl.GL_SCISSOR_TEST)\n        gl.glViewport(0, 0, self.W, self.H)  # restore the viewport\n        gl.glScissor(0, 0, self.W, self.H)  # restore the scissor\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        w = w or self.W\n        h = h or self.H\n\n        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Prepare for the actual rendering, previous operations could rebind the vertex array\n        gl.glViewport(x, y, w, h)  # set the viewport\n        gl.glScissor(x, y, w, h)  # set the scissor box\n        gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)  # clear the screen\n        gl.glUseProgram(self.quad_program)\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glBindVertexArray(0)\n\n        # Restore states of things\n        gl.glViewport(0, 0, self.W, self.H)  # restore the viewport\n        gl.glScissor(0, 0, self.W, self.H)  # restore the scissor box\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, 0)  # restore the framebuffer\n\n        # gl.glDisable(gl.GL_SCISSOR_TEST)  # disable the scissor test\n        # gl.glDisable(gl.GL_DEPTH_TEST)  # disable the depth test\n        # gl.glDisable(gl.GL_CULL_FACE)  # disable the cull face\n        # gl.glDisable(gl.GL_BLEND)  # disable the blend\n        # gl.glDisable(gl.GL_DEPTH_CLAMP)  # disable the depth clamp\n        # gl.glDisable(gl.GL_STENCIL_TEST)  # disable the stencil test\n        # gl.glDisable(gl.GL_DITHER)  # disable the dither\n        #"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        w = w or self.W\n        h = h or self.H\n\n        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Prepare to render to textures\n        gl.glViewport(x, y, w, h)  # set the viewport\n        gl.glScissor(x, y, w, h)  # set the scissor box\n\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        gl.glUseProgram(self.quad_program)\n\n        # Bind the texture\n        gl.glActiveTexture(gl.GL_TEXTURE0)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Prepare to render to textures\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glBindVertexArray(old_vao)\n        gl.glUseProgram(old_prog)\n\n        # Restore the original viewport and scissor box\n        gl.glViewport(0, 0, self.W, self.H)  # restore the original viewport\n        gl.glScissor(0, 0, self.W, self.H)  # restore the original scissor box\n\n        # Restore the original state\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, 0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        x = x or 0\n        y = y or 0\n        w = w or self.W\n        h = h or self.H\n\n        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Prepare for the actual rendering, previous operations could rebind the vertex array\n        gl.glViewport(x, y, w, h)  # set the viewport\n        gl.glScissor(x, y, w, h)  # set the scissor box\n        gl.glUseProgram(self.quad_program)\n        gl.glBindVertexArray(self.vao)\n\n        # The actual drawing pass with render things out to the write_fbo\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n\n        # Restore states of things\n        gl.glViewport(0, 0, self.W, self.H)  # restore the viewport\n        gl.glScissor(0, 0, self.W, self.H)  # restore the scissor box\n        gl.glBindVertexArray(0)\n\n        # if self.compose:\n        #     gl.glReadBuffer(gl.GL_COLOR_ATTACHMENT0)\n        #     gl.glDrawBuffer(gl.GL_COLOR_ATTACHMENT0)\n        #     gl.glBlitFramebuffer(x, y, w, h,  # the height is flipped\n        #                          x, y, w, h,  # the height is flipped\n        #                          gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        #     gl.glDrawBuffer(gl.GL_BACK)\n        # else:\n        #     gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)  # write buffer defaults to 0\n        #     gl.glBlitFramebuffer(x, y, w, h,  # the height is flipped\n        #"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        w = w or self.W\n        h = h or self.H\n        old_fbo = gl.glGetIntegerv(gl.GL_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, self.fbo)  # read buffer defaults to 0\n        gl.glViewport(x, y, w, h)  # set the viewport\n        gl.glScissor(x, y, w, h)  # set the scissor box\n\n        gl.glUseProgram(self.quad_program)\n        gl.glBindVertexArray(self.vao)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glBindVertexArray(0)\n        gl.glUseProgram(0)\n\n        gl.glViewport(0, 0, self.W, self.H)  # restore the viewport\n        gl.glScissor(0, 0, self.W, self.H)  # restore the scissor box\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, old_fbo)  # restore the framebuffer\n\n        # \"\"\"\n        # This is a more complicated version of the above code, which is slower than the above version.\n        # It first sets up a specific viewport and scissor box for rendering, activates a shader program, binds a texture, and draws the quadrilateral using vertex data. After drawing, it restores the viewport and scissor box to their original sizes.\n        # \"\"\"\n        # gl.glUseProgram(self.quad_program)\n        # gl.glBindVertexArray(self.vao)\n        # gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.ver"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    K = get_ndc_perspective_matrix(K, H, W)\n    return H, W, K, R, T, C"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    K = get_ndc_perspective_matrix(K, H, W, -1, -1)  # B, 4, 4\n    return H, W, K, R, T, C"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    K = get_ndc_perspective_matrix(K, H, W, 1, 1).to(K.dtype)  # to opengl, remove last dim of n and f\n    return H, W, K, R, T, C"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    K = get_ndc_perspective_matrix(K, H, W, 1, 1).to(R.dtype)  # to opengl\n    R = R.mT  # to opengl\n    T = -T  # to opengl\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    K = get_ndc_perspective_matrix(K, H, W, C[..., 2], C[..., 2])  # B, 4, 4\n    return H, W, K, R, T, C"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    K = get_ndc_perspective_matrix(K, H, W)\n    R = R.mT  # B, 3, 3\n    T = -R @ T  # B, 3, 1\n    return H, W, K, R, T, C"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    K = get_ndc_perspective_matrix(K, H, W, torch.ones_like(K[..., 0, 0]), torch.ones_like(K[..., 0, 0]))\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    K = get_ndc_perspective_matrix(K, H, W, K[..., 0, 0], K[..., 1, 1])\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    K = get_ndc_perspective_matrix(K, H, W, K[..., 0, 0], K[..., 1, 1]).to(R.dtype)\n    K[..., 0, 1] = 0\n    K[..., 1, 0] = 0\n    K[..., 2, 0] = 0\n    K[..., 2, 1] = 0\n    return H, W, K, R, T, C"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    K = get_ndc_perspective_matrix(K, H, W, -1, -1)  # B, 4, 4\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    K = get_ndc_perspective_matrix(K, H, W, -K[..., 0, 0], K[..., 1, 1]).to(R.dtype)  # to opengl, remove last dim of n and f\n    return H, W, K, R, T, C"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    K = get_ndc_perspective_matrix(K, H, W, n=batch.n, f=batch.f)\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    # Adjust R and T\n    R = R.mT\n    T = -R @ T\n    # Recalculate K for NDC\n    K = get_ndc_perspective_matrix(K, H, W, 1, 1).to(R.dtype)\n    return H, W, K, R, T, C"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    K = get_ndc_perspective_matrix(K, H, W, 1, 1).to(R.dtype)  # to opengl, remove last dim of n and f\n    R = R.mT  # to pytorch3d\n    T = T * -1  # to pytorch3d\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    K = get_ndc_perspective_matrix(K, H, W, torch.tensor([1.0], device=K.device), torch.tensor([1.0], device=K.device))\n    return H, W, K, R, T, C"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n\n    # Adjust R and T\n    R = R.mT  # B, 3, 3\n    T = -T  # B, 3, 1\n\n    # Recalculate K for NDC\n    K = get_ndc_perspective_matrix(K, H, W).to(R.dtype)  # B, 4, 4\n\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    # R = batch.R.mT  # B, 3, 3\n    # T = -batch.R.mT @ batch.T  # B, 3, 1\n    # C = -batch.R.mT @ batch.T  # B, 3, 1\n    K = get_ndc_perspective_matrix(K, H, W, n=batch.meta.n[0], f=batch.meta.f[0]).to(R.dtype)  # B, 4, 4\n    return H, W, K, R, T, C"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n\n    # Adjust R and T\n    R = R.mT  # B, 3, 3\n    T = -T  # B, 3, 1\n\n    # Recalculate K for NDC\n    K = get_ndc_perspective_matrix(K, H, W, 0, 1).to(R.dtype)  # B, 4, 4\n\n    # Return height, width, K, R, T, C\n    return H, W, K, R, T, C"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    # R and T are applied on the right (requires a transposed R from OpenCV camera format)\n    # Coordinate system is different from that of OpenCV (cv: right down front, 3d: left up front)\n    # However, the correction has to be down on both T and R... (instead of just R)\n    R = R.mT\n    T = -R @ T\n    C = -R @ C\n    K = get_ndc_perspective_matrix(K, H, W, K[..., 0, 0], K[..., 1, 1])\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    K = get_ndc_perspective_matrix(K, H, W, n=torch.tensor([0, 0, 1], device=R.device), f=torch.tensor([1, 1, 1], device=R.device))\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)  # read buffer defaults to 0\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)  # now self.tex contains the content of the already rendered frame\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)  # read buffer defaults to 0\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)  # now self.tex contains the content of the already rendered frame\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glReadBuffer(gl.GL_COLOR_ATTACHMENT0)\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glReadBuffer(gl.GL_COLOR_ATTACHMENT0)\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glReadBuffer(gl.GL_COLOR_ATTACHMENT0)\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)  # now self.tex contains the content of the already rendered frame\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        # This function temporarily binds the Quad instance's framebuffer object (FBO) as the read framebuffer, performs the pixel copy operation, and then restores the previously bound read framebuffer. This ensures that the function's operations do not interfere with the current OpenGL state beyond the scope of the function.\n        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glReadBuffer(gl.GL_COLOR_ATTACHMENT0)\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glReadBuffer(gl.GL_COLOR_ATTACHMENT0)\n        gl.glBlitFramebuffer(0, 0, self.W, self.H, x, y, w, h, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n\n        # Some house keepings\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        # Note: This function temporarily binds the Quad instance's framebuffer object (FBO) as the read framebuffer, performs the pixel copy operation, and then restores the previously bound read framebuffer. This ensures that the function's operations do not interfere with the current OpenGL state beyond the scope of the function.\n        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glReadBuffer(gl.GL_COLOR_ATTACHMENT0)\n        gl.glBlitFramebuffer(x, y, w, h, x, y, w, h, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)  # read buffer defaults to 0\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)  # now self.tex contains the content of the already rendered frame\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        # Get the current read framebuffer\n        old_read_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n\n        # Bind the read framebuffer to the Quad instance's FBO\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n\n        # Copy the pixels from the read framebuffer to the destination framebuffer\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n\n        # Restore the original read framebuffer\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_read_fbo)\n\n        \"\"\"\n        The function blit copies a block of pixels from the source framebuffer to the destination framebuffer. It respects the OpenGL convention of using the lower left corner as the origin. This is typically used for operations like copying a texture, rendering to a texture, or performing a fast pixel transfer operation.\n\n        Input-Output Arguments\n        :param self: Quad. An instance of the Quad class, which presumably holds information about a framebuffer object (FBO) and its dimensions.\n        :param x: int, optional. The x-coordinate of the lower left corner where the copy starts. Defaults to 0.\n        :param y: int, optional. The y-coordinate of the lower left corner where the copy starts. Defaults to 0.\n        :param w: int, optional. The width of the pixel block to be copied. If 0, the width of the Quad instance (self.W) is used. Defaults to 0.\n        :param h: int, optional. The height of the pixel block to be copied. If 0, the height of the Quad instance (self.H) is used. Defaults to 0.\n        :return: No return values. This function performs operations directly on the framebuffer objects and does not return any value.\n        \"\"\"\n\n        # Restore the original state\n        gl.glBindFramebuffer(gl.GL"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)  # read buffer defaults to 0\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)  # now self.tex contains the content of the already rendered frame\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)\n\n        # Some house keepings\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n\n        gl.glUseProgram(self.quad_program)  # use a different program\n        gl.glActiveTexture(gl.GL_TEXTURE0)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))\n        gl.glBindVertexArray(0)\n\n        # Some house keepings\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n\n        \"\"\"\n        Note: This function temporarily binds the Quad instance's framebuffer object (FBO) as the read framebuffer, performs the pixel copy operation, and then restores the previously bound read framebuffer. This ensures that the function's operations do not interfere with the current OpenGL state beyond the scope of the function.\n        \"\"\"\n\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)\n        gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, 0)  # render the"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glReadBuffer(gl.GL_COLOR_ATTACHMENT0)\n\n        old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)  # now self.tex contains the content of the already rendered frame\n\n        gl.glViewport(*old_viewport)  # restore the viewport\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)\n\n        \"\"\"\n        The function blit copies a block of pixels from the source framebuffer to the destination framebuffer. It respects the OpenGL convention of using the lower left corner as the origin. This is typically used for operations like copying a texture, rendering to a texture, or performing a fast pixel transfer operation.\n\n        Input-Output Arguments\n        :param self: Quad. An instance of the Quad class, which presumably holds information about a framebuffer object (FBO) and its dimensions.\n        :param x: int, optional. The x-coordinate of the lower left corner where the copy starts. Defaults to 0.\n        :param y: int, optional. The y-coordinate of the lower left corner where the copy starts. Defaults to 0.\n        :param w: int, optional. The width of the pixel block to be copied. If 0, the width of the Quad instance (self.W) is used. Defaults to 0.\n        :param h: int, optional. The height of the pixel block to be copied. If 0, the height of the Quad instance (self.H) is used. Defaults to 0.\n        :return: No return values. This function performs operations directly on the framebuffer"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        # Get the current framebuffer and viewport dimensions\n        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n\n        # Set the destination framebuffer and viewport dimensions\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glReadBuffer(gl.GL_COLOR_ATTACHMENT0)\n        gl.glViewport(x, y, w, h)\n\n        # Blit the texture to the destination framebuffer\n        gl.glBlitFramebuffer(0, 0, self.W, self.H, x, y, w, h, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n\n        # Restore the original framebuffer and viewport dimensions\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)\n        gl.glViewport(*old_viewport)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        # Temporarily bind the read framebuffer to the Quad instance's framebuffer object\n        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n\n        # Blit the contents of the framebuffer to the screen\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n\n        # Restore the original read framebuffer\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)  # read buffer defaults to 0\n        gl.glReadBuffer(gl.GL_COLOR_ATTACHMENT0)  # read color attachment 0\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)  # now self.tex contains the content of the already rendered frame\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)\n\n        # Some house keepings\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, 0)\n        gl.glReadBuffer(gl.GL_BACK)\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, 0)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)  # read buffer defaults to 0\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)  # read buffer defaults to 0\n        gl.glReadBuffer(gl.GL_COLOR_ATTACHMENT0)\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)  # now self.tex contains the content of the already rendered frame\n\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n\n        # Some house keepings\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, 0)\n        gl.glReadBuffer(gl.GL_BACK)\n\n        # gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, 0)\n        # gl.glDrawBuffer(gl.GL_BACK)\n        # gl.glBlitFramebuffer(x, y, w, h,\n        #                      x, y, w, h,\n        #                      gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n\n        # gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, 0)\n        # gl.glReadBuffer(gl.GL_BACK)\n        # gl.glBlitFramebuffer(x, y, w, h,\n        #                      x, y, w, h,\n        #                      gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n\n        # gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, 0)\n        # gl.glDrawBuffer(gl.GL_BACK)\n       "}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        # Prepare for the actual rendering, previous operations could rebind the vertex array\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, 0)  # render the final content onto screen\n        gl.glReadBuffer(gl.GL_COLOR_ATTACHMENT0)\n        gl.glDrawBuffer(gl.GL_BACK)\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n\n        # Some house keepings\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, 0)\n\n        \"\"\"\n        The function blit copies a block of pixels from the source framebuffer to the destination framebuffer. It respects the OpenGL convention of using the lower left corner as the origin. This is typically used for operations like copying a texture, rendering to a texture, or performing a fast pixel transfer operation.\n\n        Input-Output Arguments\n        :param self: Quad. An instance of the Quad class, which presumably holds information about a framebuffer object (FBO) and its dimensions.\n        :param x: int, optional. The x-coordinate of the lower left corner where the copy starts. Defaults to 0.\n        :param y: int, optional. The y-coordinate of the lower left corner where the copy starts. Defaults to 0.\n        :param w: int, optional. The width of the pixel block to be copied. If 0, the width of the Quad instance (self.W) is used. Defaults to 0.\n        :param h: int, optional. The height of the pixel block to be copied. If 0, the height of the Quad instance (self.H) is used. Defaults to 0.\n        :return: No return values. This function performs operations directly on the framebuffer objects and does not return any value.\n        \"\"\"\n\n        # gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, "}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        # Check if the Quad instance has a valid framebuffer object (FBO)\n        if not hasattr(self, 'fbo'):\n            log(red('Quad has no valid framebuffer object (FBO), cannot blit.'))\n            return\n\n        # Get the current read and draw framebuffer objects\n        read_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        draw_fbo = gl.glGetIntegerv(gl.GL_DRAW_FRAMEBUFFER_BINDING)\n\n        # Bind the Quad instance's framebuffer object (FBO) as the read framebuffer\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n\n        # Perform the pixel copy operation\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n\n        # Restore the previously bound read framebuffer\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, read_fbo)\n\n        # Restore the previously bound draw framebuffer\n        gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, draw_fbo)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        # Get the current framebuffer and viewport dimensions\n        old_fbo = gl.glGetIntegerv(gl.GL_FRAMEBUFFER_BINDING)\n        old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n\n        # Set the destination framebuffer to the read framebuffer\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n\n        # Set the source framebuffer to the draw framebuffer\n        gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, old_fbo)\n\n        # Set the viewport to the destination framebuffer dimensions\n        gl.glViewport(x, y, w, h)\n\n        # Blit the source framebuffer to the destination framebuffer\n        gl.glBlitFramebuffer(x, y, w, h, x, y, w, h, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n\n        # Restore the original framebuffer and viewport\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, old_fbo)\n        gl.glViewport(*old_viewport)\n\n        # Note: This function performs operations directly on the framebuffer objects and does not return any value.\n        # The function temporarily binds the Quad instance's framebuffer object (FBO) as the read framebuffer, performs the pixel copy operation, and then restores the previously bound read framebuffer. This ensures that the function's operations do not interfere with the current OpenGL state beyond the scope of the function.\n        \"\"\"\n\n        # Set the source framebuffer to the read framebuffer\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n\n        # Set the destination framebuffer to the draw framebuffer\n        gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, old_fbo)\n\n        # Blit the source framebuffer to the destination framebuffer\n        gl.glBlitFramebuffer(x, y, w, h, x, y, w, h, gl.GL_COLOR_"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # t0 = t0.view(-1, 1)\n    # t1 = t1.view(-1, 1)\n    # y1 = y1.view(-1, 1)\n\n    # t0 = t0.view(-1, 1)\n    # t1 = t1.view(-1, 1)\n    # y1 = y1.view(-1, 1)\n\n    # t0 = t0.view(-1, 1)\n    # t1 = t1.view(-1, 1)\n    # y1 = y1.view(-1, 1)\n\n    # t0 = t0.view(-1, 1)\n    # t1 = t1.view(-1, 1)\n    # y1 = y1.view(-1, 1)\n\n    # t0 = t0.view(-1, 1)\n    # t1 = t1.view(-1, 1)\n    # y1 = y1.view(-1, 1)\n\n    # t0 = t0.view(-1, 1)\n    # t1 = t1.view(-1, 1)\n    # y1 = y1.view(-1, 1)\n\n    # t0 = t0.view(-1, 1)\n    # t1 = t1.view(-1, 1)\n    # y1 = y1.view(-1, 1)\n\n    # t0 = t0.view(-1, 1)\n    # t1 = t1.view(-1, 1)\n    # y1 = y1.view(-1, 1)\n\n    # t0 = t0.view(-1, 1)\n    # t1 = t1.view(-1, 1)\n    # y1 = y1.view(-1, 1)\n\n    # t0 = t0.view(-1, 1)\n    # t1 = t1.view(-1, 1)\n    # y1 = y1.view(-1, 1)\n\n    # t0 = t0.view"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # t0, t1, y1 = matchup_channels(t0, t1, y1)\n    t0, t1 = matchup_channels(t0, t1)\n    t0, t1 = t0.flatten(), t1.flatten()\n    y1 = y1.flatten()\n    t0_idx = searchsorted(t0, t1)\n    y0 = y1[t0_idx]\n    y1 = y1[t0_idx + 1]\n    inner = (t1 - t0[t0_idx]) * y0 + (t0[t0_idx + 1] - t1) * y1\n    outer = (t0[t0_idx + 1] - t0[t0_idx]) * y0 + (t0[t0_idx + 1] - t1) * y1\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    t0 = t0.view(-1, 1)\n    t1 = t1.view(-1, 1)\n    y1 = y1.view(-1, 1)\n    t0_idx = searchsorted(t1, t0)\n    t0_idx = torch.clamp(t0_idx, 0, t1.shape[0] - 1)\n    t0_idx_1 = torch.clamp(t0_idx + 1, 0, t1.shape[0] - 1)\n    y0 = y1[t0_idx]\n    y1 = y1[t0_idx_1]\n    return y0, y1\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    t0, t1, y1 = matchup_channels(t0, t1, y1)\n    # The outer measure is the cumulative sum of the inner measure.\n    t1, y1 = matchup_channels(t0, t1, y1)\n    # The inner measure is the cumulative sum of the inner measure.\n    t1, y1 = matchup_channels(t0, t1, y1)\n    # The outer measure is the cumulative sum of the inner measure.\n    t1, y1 = matchup_channels(t0, t1, y1)\n    # The inner measure is the cumulative sum of the inner measure.\n    t1, y1 = matchup_channels(t0, t1, y1)\n    # The outer measure is the cumulative sum of the inner measure.\n    t1, y1 = matchup_channels(t0, t1, y1)\n    # The inner measure is the cumulative sum of the inner measure.\n    t1, y1 = matchup_channels(t0, t1, y1)\n    # The outer measure is the cumulative sum of the inner measure.\n    t1, y1 = matchup_channels(t0, t1, y1)\n    # The inner measure is the cumulative sum of the inner measure.\n    t1, y1 = matchup_channels(t0, t1, y1)\n    # The outer measure is the cumulative sum of the inner measure.\n    t1, y1 = matchup_channels(t0, t1, y1)\n    # The inner measure is the cumulative sum of the inner measure.\n    t1, y1 = matchup_channels(t0, t1, y1)\n    # The outer measure is the cumulative sum of the inner measure.\n    t1, y1 = matchup_channels(t0, t1, y1)\n    # The inner measure is the cumulative sum of the inner measure.\n    t1, y1 = matchup_channels(t0, t1, y1)\n    # The outer measure"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Find the indices of the elements in t1 that are less than or equal to t0\n    t1_idx = searchsorted(t1, t0)\n\n    # Compute the inner and outer measures\n    inner = torch.sum(y1[:, :-1] * (t1_idx[:, 1:] - t1_idx[:, :-1]), dim=-1)\n    outer = torch.sum(y1[:, 1:] * (t1_idx[:, 1:] - t1_idx[:, :-1]), dim=-1)\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    t0, t1, y1 = matchup_channels(t0, t1, y1)\n    # The cumulative sum of the values at the source times.\n    y1_cumsum = torch.cumsum(y1, dim=-1)\n    # The cumulative sum of the values at the target times.\n    y0_cumsum = torch.cumsum(y1_cumsum * (t0[..., 1:] - t0[..., :-1]), dim=-1)\n    # The cumulative sum of the values at the target times, with the first value\n    # set to zero.\n    y0_cumsum = torch.cat([torch.zeros_like(y0_cumsum[..., :1]), y0_cumsum], dim=-1)\n    # The cumulative sum of the values at the source times, with the last value\n    # set to zero.\n    y1_cumsum = torch.cat([y1_cumsum, torch.zeros_like(y1_cumsum[..., -1:])], dim=-1)\n    # The inner measure is the difference between the cumulative sums at the\n    # target times.\n    y0_inner = y0_cumsum[..., 1:] - y0_cumsum[..., :-1]\n    # The outer measure is the difference between the cumulative sums at the\n    # source times.\n    y1_outer = y1_cumsum[..., 1:] - y1_cumsum[..., :-1]\n    return y0_inner, y1_outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # !: CUDA IMPLEMENTATION OF SORT IS EXTREMELY SLOW\n    # old_device = t0.device\n    # t0 = t0.cpu()\n    # t1 = t1.cpu()\n    # y1 = y1.cpu()\n    t0 = t0.view(-1, t0.shape[-1])\n    t1 = t1.view(-1, t1.shape[-1])\n    y1 = y1.view(-1, y1.shape[-1])\n    # t0 = t0.to(old_device)\n    # t1 = t1.to(old_device)\n    # y1 = y1.to(old_device)\n    t0_idx = searchsorted(t0, t1)\n    t0_idx = t0_idx.view(-1, t0_idx.shape[-1])\n    t1_idx = t0_idx - 1\n    t0_idx = torch.minimum(torch.tensor(t0.shape[-1] - 1), t0_idx)\n    t1_idx = torch.maximum(torch.tensor(0), t1_idx)\n    y0 = y1.take_along_dim(t0_idx, dim=-1)\n    y1 = y1.take_along_dim(t1_idx, dim=-1)\n    inner = (y0 + y1) / 2\n    outer = (y0 - y1).abs() / 2\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Find the indices of the elements in t1 that are less than or equal to each element in t0.\n    # This is done using the searchsorted function from the torch library.\n    # The resulting indices are then used to select the corresponding elements from y1.\n    # The resulting tensor is then returned.\n    idx = searchsorted(t1, t0)\n    inner = y1.take_along_dim(idx, dim=-1)\n\n    # Find the indices of the elements in t1 that are strictly greater than each element in t0.\n    # This is done using the searchsorted function from the torch library.\n    # The resulting indices are then used to select the corresponding elements from y1.\n    # The resulting tensor is then returned.\n    idx = searchsorted(t1, t0, side='right')\n    outer = y1.take_along_dim(idx, dim=-1)\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Find the indices of the first and last non-zero elements in the source time (t1)\n    first_nonzero = torch.nonzero(t1 > 0).flatten()[0]\n    last_nonzero = torch.nonzero(t1 > 0).flatten()[-1]\n\n    # Find the indices of the first and last non-zero elements in the target time (t0)\n    first_nonzero_t0 = torch.nonzero(t0 > 0).flatten()[0]\n    last_nonzero_t0 = torch.nonzero(t0 > 0).flatten()[-1]\n\n    # Extract the values of the source time and the corresponding values (y1)\n    t1_nonzero = t1[first_nonzero:last_nonzero + 1]\n    y1_nonzero = y1[first_nonzero:last_nonzero + 1]\n\n    # Extract the values of the target time and the corresponding values (y1)\n    t0_nonzero = t0[first_nonzero_t0:last_nonzero_t0 + 1]\n    y0_nonzero = y1[first_nonzero_t0:last_nonzero_t0 + 1]\n\n    # Compute the inner measure\n    inner = torch.trapz(y1_nonzero, t1_nonzero)\n\n    # Compute the outer measure\n    outer = torch.trapz(y0_nonzero, t0_nonzero)\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    t0, t1 = matchup_channels(t0, t1)\n\n    # Find the indices of the closest elements in t1 to each element in t0.\n    # t1_idx[i] is the index of the closest element in t1 to t0[i].\n    t1_idx = searchsorted(t1, t0)\n\n    # Find the corresponding values in y1 for each element in t0.\n    # y1_inner[i] is the value in y1 corresponding to t1_idx[i].\n    y1_inner = y1.take_along_dim(t1_idx, dim=-1)\n\n    # Find the indices of the elements in t1 that are greater than each element in t0.\n    # t1_outer_idx[i] is the index of the first element in t1 that is greater than t0[i].\n    t1_outer_idx = searchsorted(t1, t0, side='right')\n\n    # Find the corresponding values in y1 for each element in t0.\n    # y1_outer[i] is the value in y1 corresponding to t1_outer_idx[i].\n    y1_outer = y1.take_along_dim(t1_outer_idx, dim=-1)\n\n    return y1_inner, y1_outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Find the indices of the elements in t1 that are less than or equal to t0\n    t0_idx = searchsorted(t1, t0)\n\n    # Calculate the inner measure as the cumulative sum of the corresponding elements in y1\n    inner = torch.cumsum(y1[t0_idx], dim=-1)\n\n    # Calculate the outer measure as the cumulative sum of the corresponding elements in y1,\n    # starting from the end of the tensor\n    outer = torch.cumsum(y1[t0_idx], dim=-1)[..., ::-1]\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Find the indices of the first and last non-zero elements in the cumulative sum.\n    t1_idx = searchsorted(t1, t0)\n    t0_idx = t1_idx - 1\n\n    # Extract the values at the indices.\n    y1_start = y1.take_along_dim(t0_idx[..., None], dim=-1)\n    y1_end = y1.take_along_dim(t1_idx[..., None], dim=-1)\n\n    # Compute the inner and outer measures.\n    inner = y1_start\n    outer = y1_end - y1_start\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # t0, t1, y1 = matchup_channels(t0, t1, y1)\n    t0, t1 = matchup_channels(t0, t1)\n    # t0, t1 = t0[..., :-1], t1[..., 1:]\n    # y1 = y1[..., :-1]\n\n    # find the indices of the outer and inner points\n    outer_idx = searchsorted(t0, t1)\n    inner_idx = outer_idx - 1\n\n    # outer_idx = torch.where(t0 >= t1, outer_idx, torch.zeros_like(outer_idx))\n    # inner_idx = torch.where(t0 < t1, inner_idx, torch.zeros_like(inner_idx))\n\n    # outer_idx = torch.where(t0 >= t1, outer_idx, torch.zeros_like(outer_idx))\n    # inner_idx = torch.where(t0 < t1, inner_idx, torch.zeros_like(inner_idx))\n\n    # outer_idx = torch.where(t0 >= t1, outer_idx, torch.zeros_like(outer_idx))\n    # inner_idx = torch.where(t0 < t1, inner_idx, torch.zeros_like(inner_idx))\n\n    # outer_idx = torch.where(t0 >= t1, outer_idx, torch.zeros_like(outer_idx))\n    # inner_idx = torch.where(t0 < t1, inner_idx, torch.zeros_like(inner_idx))\n\n    # outer_idx = torch.where(t0 >= t1, outer_idx, torch.zeros_like(outer_idx))\n    # inner_idx = torch.where(t0 < t1, inner_idx, torch.zeros_like(inner_idx))\n\n    # outer_idx = torch.where(t0 >= t1, outer_idx, torch."}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # t0, t1, y1 = matchup_channels(t0, t1, y1)\n    t0, t1 = matchup_channels(t0, t1)\n    # y1 = y1.squeeze(-1)\n    # t0, t1, y1 = matchup_channels(t0, t1, y1)\n\n    # TODO: use searchsorted instead of np.searchsorted\n    # t1_idx = np.searchsorted(t1, t0)\n    t1_idx = searchsorted(t1, t0)\n    t1_idx = torch.clamp(t1_idx, min=0, max=t1.shape[-1] - 1)\n    # t1_idx = torch.clamp(t1_idx, min=0, max=t1.shape[-1] - 1)\n    # t1_idx = torch.clamp(t1_idx, min=0, max=t1.shape[-1] - 1)\n    # t1_idx = torch.clamp(t1_idx, min=0, max=t1.shape[-1] - 1)\n    # t1_idx = torch.clamp(t1_idx, min=0, max=t1.shape[-1] - 1)\n    # t1_idx = torch.clamp(t1_idx, min=0, max=t1.shape[-1] - 1)\n    # t1_idx = torch.clamp(t1_idx, min=0, max=t1.shape[-1] - 1)\n    # t1_idx = torch.clamp(t1_idx, min=0, max=t1.shape[-1] - 1)\n    # t1_idx = torch.clamp(t1_idx, min=0, max=t1.shape[-1] - 1)\n    # t1_idx = torch.clamp(t1_idx, min=0, max=t1.shape[-1] - "}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Find the indices of the elements in t0 that are within the range of t1.\n    t1_lo = t1[:, 0]\n    t1_hi = t1[:, -1]\n    t0_lo = t0[:, 0]\n    t0_hi = t0[:, -1]\n\n    # Find the indices of the elements in t1 that are within the range of t0.\n    t0_lo_idx = searchsorted(t1, t0_lo)\n    t0_hi_idx = searchsorted(t1, t0_hi)\n\n    # Compute the inner and outer measures.\n    inner = torch.sum(y1[:, t0_lo_idx] * (t0_hi - t0_lo), dim=1)\n    outer = torch.sum(y1[:, t0_hi_idx] * (t0_hi - t0_lo), dim=1)\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # t0, t1, y1 = matchup_channels(t0, t1, y1)\n    t0, t1, y1 = matchup_channels(t0, t1)\n    t0, t1 = matchup_channels(t0, t1)\n    t0, t1 = t0.squeeze(), t1.squeeze()\n    y1 = y1.squeeze()\n    # t0, t1, y1 = t0.squeeze(), t1.squeeze(), y1.squeeze()\n    t0_idx = searchsorted(t1, t0)\n    t1_idx = searchsorted(t1, t0, side='right')\n    y0 = y1.take_along_dim(t0_idx, dim=-1)\n    y1 = y1.take_along_dim(t1_idx, dim=-1)\n    y0 = torch.cat([y0, y0[-1:]], dim=-1)\n    y1 = torch.cat([y0[0:1], y1], dim=-1)\n    return y0, y1\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Find the indices of the elements in t1 that are less than or equal to t0.\n    i = searchsorted(t1, t0)\n\n    # Compute the inner measure.\n    inner = torch.cumsum(y1, dim=-1)\n\n    # Compute the outer measure.\n    outer = torch.cumsum(y1, dim=-1)\n    outer = torch.cat([torch.zeros_like(outer[..., :1]), outer], dim=-1)\n\n    # Compute the inner and outer measures for each element in t0.\n    inner = inner[..., i]\n    outer = outer[..., i + 1]\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    t0, t1 = matchup_channels(t0, t1)\n    t1_sort, t1_idx = torch.sort(t1)\n    y1_sort = y1.take_along_dim(t1_idx, dim=-1)\n    t0_sort, t0_idx = torch.sort(t0)\n    t0_idx_rev = torch.argsort(t0_idx)\n    y0_sort = torch.zeros_like(y1_sort)\n    y0_sort = y0_sort.index_put_(tuple([t0_idx_rev]), y1_sort)\n    t1_cumsum = torch.cumsum(y1_sort, dim=-1)\n    t0_cumsum = torch.cumsum(y0_sort, dim=-1)\n    t1_cumsum = torch.cat([torch.zeros_like(t1_cumsum[..., :1]), t1_cumsum], dim=-1)\n    t0_cumsum = torch.cat([torch.zeros_like(t0_cumsum[..., :1]), t0_cumsum], dim=-1)\n    t1_cumsum = t1_cumsum[..., :-1]\n    t0_cumsum = t0_cumsum[..., 1:]\n    inner = t0_cumsum - t1_cumsum\n    outer = t0_cumsum - t1_cumsum\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    t0, t1 = matchup_channels(t0, t1)\n\n    # Find the indices of the first and last times in t1 that are less than or equal to t0.\n    t1_start_idx = searchsorted(t1, t0, side='left')\n    t1_end_idx = searchsorted(t1, t0, side='right')\n\n    # Extract the corresponding values from y1.\n    y1_start = y1.take_along_dim(t1_start_idx[..., None], dim=-1)\n    y1_end = y1.take_along_dim(t1_end_idx[..., None], dim=-1)\n\n    # Compute the inner measure as the sum of the values in y1_start.\n    inner = torch.sum(y1_start, dim=-1)\n\n    # Compute the outer measure as the difference between the sum of the values in y1_end and the sum of the values in y1_start.\n    outer = torch.sum(y1_end, dim=-1) - torch.sum(y1_start, dim=-1)\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    t0, t1, y1 = torch.broadcast_tensors(t0, t1, y1)\n    t0 = t0.flatten(end_dim=-2)\n    t1 = t1.flatten(end_dim=-2)\n    y1 = y1.flatten(end_dim=-2)\n    # t0_idx = torch.searchsorted(t0, t1)\n    t0_idx = searchsorted(t0, t1)\n    t0_idx = torch.clamp(t0_idx, 0, t0.shape[-1] - 1)\n    t0_idx = torch.cat([t0_idx, t0.shape[-1] - 1], dim=-1)\n    t0_idx = torch.cat([torch.zeros_like(t0_idx[..., :1]), t0_idx], dim=-1)\n    t0_idx = torch.cat([t0_idx, t0.shape[-1] - 1], dim=-1)\n    t0_idx = torch.cat([t0_idx, t0.shape[-1] - 1], dim=-1)\n    t0_idx = t0_idx.unsqueeze(-1).expand(-1, -1, y1.shape[-1])\n    t0_idx = t0_idx.flatten(end_dim=-2)\n    t1_idx = t0_idx + 1\n    t0 = t0.flatten(end_dim=-2)\n    t1 = t1.flatten(end_dim=-2)\n    y1 = y1.flatten(end_dim=-2)\n    y0 = y1.take_along_dim(t0_idx, dim=-1)\n    y1 = y1.take_along_dim(t1_idx, dim=-1)\n    return y0, y1\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    t, w = matchup_channels(t, w)\n    t_env, w_env = matchup_channels(t_env, w_env)\n\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], eps)\n\n    t_, w_ = blur_stepfun(t, w_normalize, 1.)\n    w_ = torch.clip(w_, min=0.)\n    assert (w_ >= 0.0).all()\n\n    # piecewise linear pdf to piecewise quadratic cdf\n    area = 0.5 * (w_[..., 1:] + w_[..., :-1]) * (t_[..., 1:] - t_[..., :-1])\n\n    cdf = torch.cat([torch.zeros_like(area[..., :1]), torch.cumsum(area, dim=-1)], dim=-1)\n\n    # query piecewise quadratic interpolation\n    cdf_interp = sorted_interp_quad(t_env, t_, w_, cdf)\n    # difference between adjacent interpolated values\n    w_s = torch.diff(cdf_interp, dim=-1)\n\n    return ((w_s - w_env).clip(0.).pow(2) / (w_env + eps)).mean()\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n    # w_env.shape[-1] = t_env.shape[-1] - 1\n\n    t, w = matchup_channels(t, w)\n    t_env, w_env = matchup_channels(t_env, w_env)\n\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], eps)\n\n    t_, w_ = blur_stepfun(t, w_normalize, 0.05)\n    w_ = torch.clip(w_, min=0.)\n    assert (w_ >= 0.0).all()\n\n    # piecewise linear pdf to piecewise quadratic cdf\n    area = 0.5 * (w_[..., 1:] + w_[..., :-1]) * (t_[..., 1:] - t_[..., :-1])\n\n    cdf = torch.cat([torch.zeros_like(area[..., :1]), torch.cumsum(area, dim=-1)], dim=-1)\n\n    # query piecewise quadratic interpolation\n    cdf_interp = sorted_interp_quad(t_env, t_, w_, cdf)\n    # difference between adjacent interpolated values\n    w_s = torch.diff(cdf_interp, dim=-1)\n\n    return ((w_s - w_env).clip(0.).pow(2) / (w_env + eps)).mean()\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    w_outer, w_inner = inner_outer(t, t_env, w_env)\n    w_outer = torch.clip(w_outer, min=0.)\n    w_inner = torch.clip(w_inner, min=0.)\n    return ((w - w_outer).clip(0.)**2 / (w_outer + eps)).mean() + ((w_inner - w).clip(0.)**2 / (w_inner + eps)).mean()\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n\n    # assert t.shape[-1] == w.shape[-1] + 1\n    # assert t_env.shape[-1] == w_env.shape[-1] + 1\n\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], eps)\n\n    t_, w_ = blur_stepfun(t, w_normalize, 0.01)\n    w_ = torch.clip(w_, min=0.)\n    assert (w_ >= 0.0).all()\n\n    # piecewise linear pdf to piecewise quadratic cdf\n    area = 0.5 * (w_[..., 1:] + w_[..., :-1]) * (t_[..., 1:] - t_[..., :-1])\n    cdf = torch.cat([torch.zeros_like(area[..., :1]), torch.cumsum(area, dim=-1)], dim=-1)\n\n    # query piecewise quadratic interpolation\n    cdf_interp = sorted_interp_quad(t_env, t_, w_, cdf)\n    # difference between adjacent interpolated values\n    w_s = torch.diff(cdf_interp, dim=-1)\n\n    return ((w_s - w_env).clip(0.).pow(2) / (w_env + eps)).mean()\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n    # w.shape[-1] = w_env.shape[-1]\n\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], eps)\n\n    t_, w_ = blur_stepfun(t, w_normalize, 1.)\n    w_ = torch.clip(w_, min=0.)\n    assert (w_ >= 0.0).all()\n\n    # piecewise linear pdf to piecewise quadratic cdf\n    area = 0.5 * (w_[..., 1:] + w_[..., :-1]) * (t_[..., 1:] - t_[..., :-1])\n\n    cdf = torch.cat([torch.zeros_like(area[..., :1]), torch.cumsum(area, dim=-1)], dim=-1)\n\n    # query piecewise quadratic interpolation\n    cdf_interp = sorted_interp_quad(t_env, t_, w_, cdf)\n    # difference between adjacent interpolated values\n    w_s = torch.diff(cdf_interp, dim=-1)\n\n    return ((w_s - w_env).clip(0.).pow(2) / (w_env + eps)).mean()\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], eps)\n    w_env_normalize = w_env / torch.clamp_min(t_env[..., 1:] - t_env[..., :-1], eps)\n\n    t_, w_ = blur_stepfun(t, w_normalize, 0.001)\n    w_ = torch.clip(w_, min=0.)\n    assert (w_ >= 0.0).all()\n\n    # piecewise linear pdf to piecewise quadratic cdf\n    area = 0.5 * (w_[..., 1:] + w_[..., :-1]) * (t_[..., 1:] - t_[..., :-1])\n\n    cdf = torch.cat([torch.zeros_like(area[..., :1]), torch.cumsum(area, dim=-1)], dim=-1)\n\n    # query piecewise quadratic interpolation\n    cdf_interp = sorted_interp_quad(t_env, t_, w_, cdf)\n    # difference between adjacent interpolated values\n    w_s = torch.diff(cdf_interp, dim=-1)\n\n    return ((w_s - w_env_normalize).clip(0.).pow(2) / (w_env_normalize + eps)).mean()\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    w_env, t_env = matchup_channels(w_env, t_env)\n    w, t = matchup_channels(w, t)\n    w_env_inner, w_env_outer = inner_outer(t, t_env, w_env)\n    w_inner, w_outer = inner_outer(t, t_env, w)\n    w_outer = torch.maximum(w_outer, w_env_outer)\n    loss = torch.square(w_inner - w_env_inner) + torch.square(w_outer - w_env_outer)\n    return loss.mean()\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # t: 64\n    # w: 64\n    # t_env: 64\n    # w_env: 64\n\n    # t, w = matchup_channels(t, w)\n    # t_env, w_env = matchup_channels(t_env, w_env)\n\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], eps)\n\n    t_, w_ = blur_stepfun(t, w_normalize, 0.001)\n    w_ = torch.clip(w_, min=0.)\n    assert (w_ >= 0.0).all()\n\n    # piecewise linear pdf to piecewise quadratic cdf\n    area = 0.5 * (w_[..., 1:] + w_[..., :-1]) * (t_[..., 1:] - t_[..., :-1])\n\n    cdf = torch.cat([torch.zeros_like(area[..., :1]), torch.cumsum(area, dim=-1)], dim=-1)\n\n    # query piecewise quadratic interpolation\n    cdf_interp = sorted_interp_quad(t_env, t_, w_, cdf)\n    # difference between adjacent interpolated values\n    w_s = torch.diff(cdf_interp, dim=-1)\n\n    return ((w_s - w_env).clip(0.).pow(2) / (w_env + eps)).mean()"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    w_env_outer, _ = inner_outer(t, t_env, w_env)\n    w_outer, _ = inner_outer(t, t, w)\n    loss = (w_outer - w_env_outer).clamp(min=0.)\n    loss = (loss**2).mean() / (w_env_outer + eps)\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    w, t = matchup_channels(w, t)\n    w_env, t_env = matchup_channels(w_env, t_env)\n\n    # Compute the upper envelope weights\n    w_env_outer, _ = inner_outer(t_env, t, w_env)\n\n    # Compute the difference between the target weights and the upper envelope weights\n    w_diff = w - w_env_outer\n\n    # Compute the scaled half-quadratic loss\n    loss = torch.mean(torch.clamp_min(w_diff**2, eps) / (w_env + eps))\n\n    return loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    w_env, w_env = matchup_channels(w_env, w_env)\n    w, w = matchup_channels(w, w)\n\n    t_env, t_env = matchup_channels(t_env, t_env)\n    t, t = matchup_channels(t, t)\n\n    w_env_outer, w_env_inner = inner_outer(t, t_env, w_env)\n    w_outer, w_inner = inner_outer(t, t, w)\n\n    w_env_outer = torch.clip(w_env_outer, min=0.)\n    w_outer = torch.clip(w_outer, min=0.)\n\n    return ((w_outer - w_env_outer).clip(0.) / (w_env_outer + eps)).mean()\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # t: 64, w: 64\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], eps)  # 64\n    # t_env: 64, w_env: 64\n    w_env_normalize = w_env / torch.clamp_min(t_env[..., 1:] - t_env[..., :-1], eps)  # 64\n\n    w_normalize = w_normalize.unsqueeze(0)  # 1, 64\n    w_env_normalize = w_env_normalize.unsqueeze(0)  # 1, 64\n\n    # t: 64, w: 64\n    w_normalize = torch.cat([torch.zeros_like(w_normalize[..., :1]), w_normalize], dim=-1)  # 1, 65\n    w_env_normalize = torch.cat([torch.zeros_like(w_env_normalize[..., :1]), w_env_normalize], dim=-1)  # 1, 65\n\n    # t: 65, w: 65\n    w_normalize = torch.cumsum(w_normalize, dim=-1)  # 1, 65\n    w_env_normalize = torch.cumsum(w_env_normalize, dim=-1)  # 1, 65\n\n    # t: 65, w: 65\n    w_normalize = torch.cat([torch.zeros_like(w_normalize[..., :1]), w_normalize], dim=-1)  # 1, 66\n    w_env_normalize = torch.cat([torch.zeros_like(w_env_normalize[..., :1]), w_env_normalize], dim=-1)  # 1, "}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # Calculate the upper envelope weights and positions.\n    w_env_outer, _ = inner_outer(t, t_env, w_env)\n\n    # Calculate the inner and outer measures of the target weights and positions.\n    w_inner, w_outer = inner_outer(t, t, w)\n\n    # Calculate the loss based on the difference between the inner and outer measures.\n    loss = (w_inner - w_env_outer).clip(0.)\n    loss = loss.pow(2) / (w_env + eps)\n    return loss.mean()\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    w_env_outer, w_env_inner = inner_outer(t, t_env, w_env)\n    w_outer, w_inner = inner_outer(t, t, w)\n    w_outer = w_outer.clamp_min(eps)\n    w_inner = w_inner.clamp_min(eps)\n    w_env_outer = w_env_outer.clamp_min(eps)\n    w_env_inner = w_env_inner.clamp_min(eps)\n\n    loss_outer = (w_inner / w_outer).pow(2) * (w_outer - w_env_outer) / w_env_outer + (\n        w_inner / w_outer).pow(2) * (w_outer - w_env_inner) / w_env_inner\n    return loss_outer.mean()\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # t: [1, 2, 3, ..., n]\n    # w: [0, 1, 2, ..., n]\n    # t_env: [1, 2, 3, ..., n]\n    # w_env: [0, 1, 2, ..., n]\n\n    # t: [1, 2, 3, ..., n]\n    # w: [0, 1, 2, ..., n]\n    # t_env: [1, 2, 3, ..., n]\n    # w_env: [0, 1, 2, ..., n]\n\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], eps)\n\n    # t: [1, 2, 3, ..., n]\n    # w: [0, 1, 2, ..., n]\n    # t_env: [1, 2, 3, ..., n]\n    # w_env: [0, 1, 2, ..., n]\n\n    t_, w_ = blur_stepfun(t, w_normalize, 0.5)\n    w_ = torch.clip(w_, min=0.)\n    assert (w_ >= 0.0).all()\n\n    # t: [1, 2, 3, ..., n]\n    # w: [0, 1, 2, ..., n]\n    # t_env: [1, 2, 3, ..., n]\n    # w_env: [0, 1, 2, ..., n]\n\n    # piecewise linear pdf to piecewise quadratic cdf\n    area = 0.5 * (w_[..., 1:] + w_[..., :-1]) * (t_[..., 1:] - t_[..., :-1])\n\n    cdf = torch.cat([torch.zeros_like(area[..., :1]), torch.cumsum(area, dim=-1)"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # Calculate the upper envelope weights using the target positions and weights.\n    w_env_outer, _ = inner_outer(t, t_env, w_env)\n    w_env_outer = torch.maximum(w_env_outer, w)\n\n    # Calculate the difference between the target weights and the upper envelope weights.\n    w_diff = w - w_env_outer\n\n    # Compute the loss using the scaled half-quadratic loss function.\n    loss = (w_diff**2 / (w_env_outer + eps)).mean()\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    w_env, t_env = matchup_channels(w_env, t_env)\n    w, t = matchup_channels(w, t)\n    w_env = torch.clamp(w_env, min=eps)\n    w = torch.clamp(w, min=eps)\n    t_env = torch.clamp(t_env, min=eps)\n    t = torch.clamp(t, min=eps)\n    w_env = w_env.detach()\n    t_env = t_env.detach()\n\n    y0_inner, y0_outer = inner_outer(t, w, t_env)\n    y0_outer = torch.maximum(y0_outer, w_env)\n    y0_inner = torch.maximum(y0_inner, w)\n    return ((y0_outer - y0_inner)**2 / (y0_outer + eps)).mean()\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    w_env, w_env_inner, w_env_outer = inner_outer(t, t_env, w_env)\n    w_inner, w_outer = inner_outer(t, t, w)\n\n    # 128\n    w_env_inner = torch.clip(w_env_inner, min=eps)\n    w_env_outer = torch.clip(w_env_outer, min=eps)\n    w_inner = torch.clip(w_inner, min=eps)\n    w_outer = torch.clip(w_outer, min=eps)\n\n    # 128\n    w_env_inner = torch.log(w_env_inner)\n    w_env_outer = torch.log(w_env_outer)\n    w_inner = torch.log(w_inner)\n    w_outer = torch.log(w_outer)\n\n    # 128\n    w_env_inner = w_env_inner - w_env_outer\n    w_inner = w_inner - w_outer\n\n    # 128\n    w_env_inner = w_env_inner.pow(2)\n    w_inner = w_inner.pow(2)\n\n    # 128\n    w_env_inner = w_env_inner / 2\n    w_inner = w_inner / 2\n\n    # 128\n    w_env_inner = w_env_inner.sum(-1)\n    w_inner = w_inner.sum(-1)\n\n    # 128\n    w_env_inner = w_env_inner / (w_env_inner + w_inner)\n\n    # 128\n    w_env_inner = w_env_inner.mean()\n\n    return w_env_inner\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # Calculate the inner and outer measures for the target positions.\n    y0_inner, y0_outer = inner_outer(t, t, w)\n\n    # Calculate the inner and outer measures for the environment positions.\n    y1_inner, y1_outer = inner_outer(t, t_env, w_env)\n\n    # Calculate the difference between the outer measures of the target and environment.\n    y0_outer = torch.clip(y0_outer, min=eps)\n    y1_outer = torch.clip(y1_outer, min=eps)\n    y1_outer_inv = 1 / y1_outer\n\n    # Calculate the loss based on the difference between the outer measures.\n    loss = y0_outer * y1_outer_inv\n    loss = torch.sum(loss, dim=-1)\n\n    # Calculate the loss based on the inner measures.\n    y0_inner = torch.clip(y0_inner, min=eps)\n    y0_inner_inv = 1 / y0_inner\n    y0_inner_inv_sq = y0_inner_inv**2\n    y1_inner = torch.clip(y1_inner, min=eps)\n    y1_inner_inv = 1 / y1_inner\n    y1_inner_inv_sq = y1_inner_inv**2\n    loss += y0_inner_inv_sq * y1_inner_inv_sq\n\n    # Return the final loss.\n    return torch.mean(loss)"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # The code calculates the difference between the target and environment weights, and calculates the upper envelope weights by subtracting the environment weights from the target weights.\n    # The function then calculates the difference between the target weights and the upper envelope weights, and calculates the scaled half-quadratic loss based on the difference.\n    # The loss is calculated by multiplying the difference by itself, dividing by two, and summing over the target positions.\n    # The loss is returned as a tensor.\n    w_env, w = matchup_channels(w_env, w)\n    t, w = matchup_channels(t, w)\n    # The code calculates the difference between the target and environment weights, and calculates the upper envelope weights by subtracting the environment weights from the target weights.\n    # The function then calculates the difference between the target weights and the upper envelope weights, and calculates the scaled half-quadratic loss based on the difference.\n    # The loss is calculated by multiplying the difference by itself, dividing by two, and summing over the target positions.\n    # The loss is returned as a tensor.\n\n    w_env, w = matchup_channels(w_env, w)\n    t, w = matchup_channels(t, w)\n    w_env, w = matchup_channels(w_env, w)\n    t, w = matchup_channels(t, w)\n    w_env, w = matchup_channels(w_env, w)\n    t, w = matchup_channels(t, w)\n\n    w_env, w = matchup_channels(w_env, w)\n    t, w = matchup_channels(t, w)\n    w_env, w = matchup_channels(w_env, w)\n    t, w = matchup_channels(t, w)\n    w_env, w = matchup_channels(w_env, w)\n    t, w = matchup_channels(t, w)\n\n    w_env, w = matchup_channels(w_env, w)\n    t, w = match"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # loss_inter = lossfun_outer(t, w, t_env, w_env)\n    loss_inter = lossfun_zip_outer(t, w, t_env, w_env, pulse_width=0.05)\n    loss_intra = lossfun_outer(t, w, t, w)\n    return loss_inter + loss_intra"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # Compute the inter-interval losses.\n    inter_loss = lossfun_outer(t, w, t, w)\n\n    # Compute the intra-interval losses.\n    intra_loss = lossfun_zip_outer(t, w, t, w, pulse_width)\n\n    # Combine the inter-interval and intra-interval losses.\n    distortion_loss = inter_loss + intra_loss\n\n    return distortion_loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # Compute the inter-interval loss\n    inter_loss = lossfun_outer(t, w, t_env, w_env)\n    # Compute the intra-interval loss\n    intra_loss = lossfun_zip_outer(t, w, t_env, w_env, pulse_width)\n    # Combine the inter-interval and intra-interval losses\n    loss = inter_loss + intra_loss\n    return loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # accepts t.shape[-1] = w.shape[-1] + 1\n    t, w = matchup_channels(t, w)\n\n    # Compute the inter-interval distortion loss.\n    distortion_inter = interval_distortion(t[:, :-1], t[:, 1:], t[:, :-1], t[:, 1:]).mean()\n\n    # Compute the intra-interval distortion loss.\n    distortion_intra = interval_distortion(t[:, :-1], t[:, 1:], t[:, 1:], t[:, 2:]).mean()\n\n    # Combine the inter- and intra-interval distortion losses.\n    distortion = distortion_inter + distortion_intra\n\n    return distortion"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # Compute the inter-interval losses\n    t_lo, t_hi = t[..., :-1], t[..., 1:]\n    w_lo, w_hi = w[..., :-1], w[..., 1:]\n    loss_inter = lossfun_outer(t_lo, w_lo, t_hi, w_hi)\n\n    # Compute the intra-interval losses\n    t_mid, w_mid = (t[..., :-1] + t[..., 1:]) / 2, (w[..., :-1] + w[..., 1:]) / 2\n    loss_intra = lossfun_outer(t_mid, w_mid, t_mid, w_mid)\n\n    # Combine the losses\n    loss = loss_inter + loss_intra\n\n    return loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n\n    # t_env, w_env = matchup_channels(t_env, w_env)\n\n    # w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], 1e-6)\n\n    # t_, w_ = blur_stepfun(t, w_normalize, 0.05)\n    # w_ = torch.clip(w_, min=0.)\n    # assert (w_ >= 0.0).all()\n\n    # # piecewise linear pdf to piecewise quadratic cdf\n    # area = 0.5 * (w_[..., 1:] + w_[..., :-1]) * (t_[..., 1:] - t_[..., :-1])\n\n    # cdf = torch.cat([torch.zeros_like(area[..., :1]), torch.cumsum(area, dim=-1)], dim=-1)\n\n    # # query piecewise quadratic interpolation\n    # cdf_interp = sorted_interp_quad(t_env, t_, w_, cdf)\n    # # difference between adjacent interpolated values\n    # w_s = torch.diff(cdf_interp, dim=-1)\n\n    # inter_loss = ((w_s - w_env).clip(0.).pow(2) / (w_env + 1e-6)).mean()\n\n    # # intra_loss = lossfun_distortion(t, w)\n\n    # # return inter_loss + intra_loss\n    # return inter_loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # Compute the inter-interval distortion loss\n    t_diff = torch.diff(t, dim=-1)  # (B, N)\n    w_diff = torch.diff(w, dim=-1)  # (B, N)\n    inter_interval_loss = torch.mean(torch.square(t_diff - w_diff))\n\n    # Compute the intra-interval distortion loss\n    t_diff = torch.diff(t, dim=-1)  # (B, N)\n    w_diff = torch.diff(w, dim=-1)  # (B, N)\n    intra_interval_loss = torch.mean(torch.square(t_diff - w_diff))\n\n    # Combine the inter-interval and intra-interval losses\n    distortion_loss = inter_interval_loss + intra_interval_loss\n\n    return distortion_loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # Compute the inter-interval distortion loss.\n    t, w = matchup_channels(t, w)\n    t_ = t[..., 1:]\n    t_ = torch.cat([t[..., :1], t_], dim=-1)\n    t_ = (t_ + t[..., :-1]) / 2\n    t_ = torch.cat([t[..., :1], t_], dim=-1)\n    t_ = torch.cat([t_, t[..., -1:]], dim=-1)\n    t_ = t_.view(t.shape[0], -1)\n    t_ = t_.view(t.shape[0], -1)\n    t_ = t_.view(t.shape[0], -1)\n    t_ = t_.view(t.shape[0], -1)\n    t_ = t_.view(t.shape[0], -1)\n    t_ = t_.view(t.shape[0], -1)\n    t_ = t_.view(t.shape[0], -1)\n    t_ = t_.view(t.shape[0], -1)\n    t_ = t_.view(t.shape[0], -1)\n    t_ = t_.view(t.shape[0], -1)\n    t_ = t_.view(t.shape[0], -1)\n    t_ = t_.view(t.shape[0], -1)\n    t_ = t_.view(t.shape[0], -1)\n    t_ = t_.view(t.shape[0], -1)\n    t_ = t_.view(t.shape[0], -1)\n    t_ = t_.view(t.shape[0], -1)\n    t_ = t_.view(t.shape[0], -1)\n    t_ = t_.view(t.shape[0], -1)\n    t_ = t_.view(t.shape[0], -1)\n    t_ = t_.view(t.shape[0], -1)\n    t"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # Compute the inter-interval losses\n    t_diff = torch.diff(t, dim=-1)\n    w_diff = torch.diff(w, dim=-1)\n    t_diff = t_diff[..., 1:]\n    w_diff = w_diff[..., 1:]\n    inter_interval_loss = (t_diff * w_diff).sum(dim=-1)\n\n    # Compute the intra-interval losses\n    t_intra = torch.diff(t, dim=-1)\n    w_intra = torch.diff(w, dim=-1)\n    t_intra = t_intra[..., 1:]\n    w_intra = w_intra[..., 1:]\n    intra_interval_loss = (t_intra * w_intra).sum(dim=-1)\n\n    # Combine the inter-interval and intra-interval losses\n    distortion_loss = inter_interval_loss + intra_interval_loss\n\n    return distortion_loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n    # t.shape[-1] = w.shape[-1] + 1\n\n    # Interval distortion.\n    t_lo = t[..., :-1]\n    t_hi = t[..., 1:]\n    d_inter = interval_distortion(t_lo, t_hi, t_lo, t_hi)\n\n    # Intra-interval distortion.\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], 1e-6)\n    d_intra = interval_distortion(t_lo, t_hi, t_lo, t_hi + w_normalize)\n\n    return d_inter + d_intra"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n    t_outer, w_outer = inner_outer(t, t, w)\n    t_inner, w_inner = inner_outer(t, t_outer, w_outer)\n\n    # Distortion when the intervals do not overlap.\n    d_disjoint = torch.abs((t_inner + t_outer) / 2 - (t_inner + t_outer) / 2)\n\n    # Distortion when the intervals overlap.\n    d_overlap = (2 *\n                 (torch.minimum(t_inner, t_outer)**3 - torch.maximum(t_inner, t_outer)**3) +\n                 3 * (t_outer * t_inner * torch.abs(t_outer - t_inner) +\n                      t_outer * t_outer * torch.abs(t_outer - t_outer) + t_outer * t_inner *\n                      (t_inner - t_outer) + t_outer * t_outer *\n                      (t_outer - t_inner))) / (6 * (t_inner - t_outer) * (t_outer - t_inner))\n\n    # Are the two intervals not overlapping?\n    are_disjoint = (t_inner > t_outer) | (t_outer > t_inner)\n\n    return torch.where(are_disjoint, d_disjoint, d_overlap).sum(dim=-1)"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # accepts t.shape[-1] = w.shape[-1] + 1\n    t, w = matchup_channels(t, w)\n\n    # Compute the inter-interval losses\n    t_inner, t_outer = inner_outer(t, t, w)\n    t_outer = t_outer.clamp_min(1e-6)\n    t_inner = t_inner.clamp_min(1e-6)\n    loss_inter = (t_outer - t_inner).pow(2) / t_outer\n\n    # Compute the intra-interval losses\n    t_inner, t_outer = inner_outer(t, t, w)\n    t_outer = t_outer.clamp_min(1e-6)\n    t_inner = t_inner.clamp_min(1e-6)\n    loss_intra = (t_outer - t_inner).pow(2) / t_outer\n\n    # Combine the inter- and intra-interval losses\n    loss = loss_inter + loss_intra\n\n    return loss.mean()"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # accepts t.shape[-1] = w.shape[-1] + 1\n    t, w = matchup_channels(t, w)\n    t_env, w_env = matchup_channels(t_env, w_env)\n\n    # inter-interval loss\n    # distortion_inter = torch.mean(lossfun_outer(t, w, t_env, w_env))\n    distortion_inter = torch.mean(lossfun_zip_outer(t, w, t_env, w_env, pulse_width))\n\n    # intra-interval loss\n    distortion_intra = torch.mean(lossfun_outer(t, w, t, w))\n\n    # total loss\n    distortion = distortion_inter + distortion_intra\n\n    return distortion"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # t: (B, N)\n    # w: (B, N-1)\n\n    # Compute the inter-interval losses\n    inter_loss = lossfun_outer(t, w, t, w)\n\n    # Compute the intra-interval losses\n    intra_loss = lossfun_zip_outer(t, w, t, w, pulse_width=0.01)\n\n    # Combine the inter-interval and intra-interval losses\n    loss = inter_loss + intra_loss\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n    t_env, w_env = matchup_channels(t_env, w_env)\n    \"\"\"\n    Computes the distortion loss function for a given tensor of targets and weights. The function calculates both the inter-interval and intra-interval losses based on the provided tensors and combines them to produce the total distortion loss.\n\n    Input-Output Arguments\n    :param t: torch.Tensor. The target tensor for which the distortion loss is to be calculated. It is expected that the last dimension of 't' is one more than that of 'w'.\n    :param w: torch.Tensor. The weights tensor, used to weight the distortion loss calculation. The last dimension of 'w' should be one less than that of 't'.\n    :return: torch.Tensor. The calculated distortion loss as a tensor. This combines both inter-interval and intra-interval losses.\n    \"\"\"\n    # The inter-interval loss is the weighted average of the squared distance between the target and the environment.\n    inter_interval_loss = lossfun_outer(t, w, t_env, w_env).mean()\n\n    # The intra-interval loss is the weighted average of the squared distance between the target and the average of the target and the environment.\n    intra_interval_loss = lossfun_zip_outer(t, w, t_env, w_env, pulse_width=0.01).mean()\n\n    # The total distortion loss is the sum of the inter-interval and intra-interval losses.\n    return inter_interval_loss + intra_interval_loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n\n    # Compute the inter-interval losses.\n    w_inner, w_outer = inner_outer(t, t, w)\n    w_inner = w_inner.clip(0.)\n    w_outer = w_outer.clip(0.)\n    w_inner_loss = lossfun_outer(t, w_inner, t, w_inner)\n    w_outer_loss = lossfun_outer(t, w_outer, t, w_outer)\n    inter_loss = w_inner_loss + w_outer_loss\n\n    # Compute the intra-interval losses.\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], 1e-6)\n    t_, w_ = blur_stepfun(t, w_normalize, 0.01)\n    w_ = torch.clip(w_, min=0.)\n    assert (w_ >= 0.0).all()\n    intra_loss = lossfun_outer(t_, w_, t_, w_)\n\n    # Combine the losses.\n    loss = inter_loss + intra_loss\n\n    return loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # accepts t.shape[-1] = w.shape[-1] + 1\n    t, w = matchup_channels(t, w)\n    # t_env, w_env = matchup_channels(t_env, w_env)\n\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], 1e-6)\n    # w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], 1e-6)\n    # w_normalize = w / (t[..., 1:] - t[..., :-1])\n\n    t_, w_ = blur_stepfun(t, w_normalize, 0.1)\n    w_ = torch.clip(w_, min=0.)\n    assert (w_ >= 0.0).all()\n\n    # piecewise linear pdf to piecewise quadratic cdf\n    area = 0.5 * (w_[..., 1:] + w_[..., :-1]) * (t_[..., 1:] - t_[..., :-1])\n    cdf = torch.cat([torch.zeros_like(area[..., :1]), torch.cumsum(area, dim=-1)], dim=-1)\n\n    # query piecewise quadratic interpolation\n    cdf_interp = sorted_interp_quad(t, t_, w_, cdf)\n    # difference between adjacent interpolated values\n    w_s = torch.diff(cdf_interp, dim=-1)\n\n    # inter-interval loss\n    loss_inter = torch.mean(lossfun_outer(t, w, t_, w_, 0.1))\n    # intra-interval loss\n    loss_intra = torch.mean(lossfun_outer(t, w, t_, w_, 0.01))\n\n    return loss_inter + loss_intra"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], 1e-6)\n    w_normalize = torch.cat([torch.zeros_like(w_normalize[..., :1]), w_normalize], dim=-1)\n\n    w_inter = torch.abs(w_normalize[..., 1:] - w_normalize[..., :-1])\n    w_intra = torch.abs(w_normalize[..., 1:] - w_normalize[..., :-1])\n\n    loss_inter = (w_inter * (t[..., 1:] - t[..., :-1])).sum()\n    loss_intra = (w_intra * (t[..., 1:] - t[..., :-1])).sum()\n    return loss_inter, loss_intra\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n    t_env, w_env = matchup_channels(t_env, w_env)\n\n    # The distortion loss is the sum of two losses:\n    # 1. The inter-interval loss, which penalizes the difference between the\n    #    proposal and environment weights at the midpoints of the intervals.\n    # 2. The intra-interval loss, which penalizes the difference between the\n    #    proposal and environment weights at the ends of the intervals.\n\n    # Compute the inter-interval loss.\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], 1e-6)\n    w_normalize = torch.cat([w_normalize, torch.zeros_like(w_normalize[..., :1])], dim=-1)\n    w_inter = (w_normalize[..., 1:] + w_normalize[..., :-1]) / 2\n    t_inter = (t[..., 1:] + t[..., :-1]) / 2\n    loss_inter = lossfun_outer(t_inter, w_inter, t_env, w_env)\n\n    # Compute the intra-interval loss.\n    loss_intra = lossfun_outer(t, w, t_env, w_env)\n\n    return loss_inter + loss_intra"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # The first element of 't' is assumed to be 0.\n    t, w = matchup_channels(t, w)\n    # t_env = torch.cat([t[..., :1], t[..., 1:]], dim=-1)\n    # w_env = torch.cat([w[..., :1], w[..., 1:]], dim=-1)\n    t_env, w_env = matchup_channels(t, w)\n\n    # The first element of 't' is assumed to be 0.\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], 1e-6)\n\n    # The first element of 't' is assumed to be 0.\n    # The first element of 't' is assumed to be 0.\n    # The first element of 't' is assumed to be 0.\n    # The first element of 't' is assumed to be 0.\n    # The first element of 't' is assumed to be 0.\n    # The first element of 't' is assumed to be 0.\n    # The first element of 't' is assumed to be 0.\n    # The first element of 't' is assumed to be 0.\n    # The first element of 't' is assumed to be 0.\n    # The first element of 't' is assumed to be 0.\n    # The first element of 't' is assumed to be 0.\n    # The first element of 't' is assumed to be 0.\n    # The first element of 't' is assumed to be 0.\n    # The first element of 't' is assumed to be 0.\n    # The first element of 't' is assumed to be 0.\n    # The first element of 't' is assumed to be 0.\n    # The first element of 't' is assumed to be 0.\n    # The first element of 't' is assumed to be 0.\n    # The first element of 't' is assumed to be 0.\n   "}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, device=t.device), cw, t)"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, device=t.device), cw, t)"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps), cw, t)"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps), cw, t)"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, dtype=t.dtype, device=t.device), cw, t)"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, device=t.device), cw[..., :-1], t)"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, dtype=w.dtype, device=w.device), cw, t)"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, dtype=t.dtype, device=t.device), cw, t)"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, device=t.device), cw, t)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, dtype=w.dtype, device=w.device), cw, t)"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, device=t.device, dtype=t.dtype), cw, t)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, device=t.device), cw[..., :-1], t[..., :-1])\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps), cw, t)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, device=t.device), cw, t)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, device=t.device), cw, t)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    ps = torch.tensor(ps, dtype=cw.dtype, device=cw.device)\n    return interpolate(ps, cw[..., :-1], t[..., :-1])\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, device=t.device), cw, t)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n\n    # Integrate the weights to get the CDF.\n    cw = integrate_weights(w)\n\n    # Interpolate into the CDF to get the weighted percentiles.\n    return interpolate(torch.tensor(ps), cw[..., :-1], t[..., :-1])\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, dtype=t.dtype, device=t.device), cw, t)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    ps = torch.tensor(ps, dtype=t.dtype, device=t.device)\n    return interpolate(ps, cw, t)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n\n    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n\n    # Generate samples from the CDF.\n    u = torch.rand(num_samples, device=t.device)\n    t_new = invert_cdf(u, t, w)\n\n    if perturb:\n        # Perturb the samples to avoid sample clustering at bin boundaries.\n        if single_jitter:\n            jitter = torch.rand_like(t_new)\n        else:\n            jitter = torch.rand_like(t_new)\n        t_new = t_new + jitter * (cw[..., 1:] - cw[..., :-1])\n\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n    # Sample uniformly from [0, 1) according to the PDF.\n    u = torch.rand(num_samples, device=t.device)\n    if perturb:\n        # Perturb the samples to avoid sample clustering at bin boundaries.\n        u = u + (torch.rand(num_samples, device=t.device) - 0.5) / cw.shape[-1]\n    u = u.clip(0, 1)\n    # Interpolate into the inverse CDF.\n    t_new = interpolate(u, cw, t)\n    if single_jitter:\n        # Add a small amount of jitter to each sample.\n        t_new = t_new + torch.rand_like(t_new) * 1e-5\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n\n    # Generate samples from the PDF.\n    u = torch.rand(num_samples, *w.shape[:-1], device=t.device)\n    t_new = invert_cdf(u, t, w)\n\n    # Apply perturbation to the samples.\n    if perturb:\n        t_new = t_new + torch.rand_like(t_new) / num_samples\n\n    # Apply jitter to the samples.\n    if single_jitter:\n        jitter = torch.rand_like(t_new) / num_samples\n        t_new = t_new + jitter\n    else:\n        jitter = torch.rand_like(t_new) / num_samples\n        t_new = t_new + jitter\n\n    return t_new"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # 1. Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n\n    # 2. Generate uniform samples in [0, 1).\n    u = torch.rand(num_samples, *cw.shape[:-1], device=cw.device)\n\n    # 3. Invert the CDF to obtain samples from the PDF.\n    t_new = invert_cdf(u, t, w)\n\n    # 4. Optionally, perturb the samples to avoid sample clustering at bin boundaries.\n    if perturb:\n        if single_jitter:\n            t_new += torch.rand_like(t_new)\n        else:\n            t_new += torch.rand_like(t_new) * (cw[..., 1:] - cw[..., :-1])\n\n    return t_new"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n    # Sample uniformly from [0, 1)\n    u = torch.rand(num_samples, device=t.device)\n\n    # If we're perturbing, then we need to add a small amount of noise to the\n    # samples to avoid numerical issues.\n    if perturb:\n        u = u + torch.rand_like(u) * 1e-5\n\n    # If we're jittering, then we need to add a small amount of noise to the\n    # samples to avoid numerical issues.\n    if single_jitter:\n        u = u + torch.rand_like(u) * 1e-5\n    else:\n        u = u + torch.rand_like(u) * 1e-5\n\n    # Invert the CDF to get the samples.\n    t_new = invert_cdf(u, t, w)\n\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n    w = w / torch.sum(w, dim=-1, keepdim=True)\n    # Perturb the sampling points to avoid sample clustering at bin boundaries.\n    if perturb:\n        u = torch.rand(t.shape[:-1] + (num_samples,), device=t.device)\n        if single_jitter:\n            u = u.mean(dim=-1, keepdim=True)\n    else:\n        u = torch.linspace(0, 1, num_samples, device=t.device)\n        u = u.expand(t.shape[:-1] + (num_samples,))\n\n    # Invert the CDF to get the sampling points.\n    t_new = invert_cdf(u, t, w)\n\n    return t_new"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Transfer weights to probabilities.\n    w = w / torch.sum(w, dim=-1, keepdim=True)\n\n    # Sample uniformly in [0, 1) and invert the CDF.\n    u = torch.rand(t.shape[:-1] + (num_samples,), device=t.device)\n    t_new = invert_cdf(u, t, w)\n\n    # Perturb the samples.\n    if perturb:\n        t_new = t_new + torch.rand_like(t_new) / t_new.shape[-1]\n\n    # Jitter the samples.\n    if single_jitter:\n        t_new = t_new + torch.rand_like(t_new) / t_new.shape[-1]\n    else:\n        t_new = t_new + torch.rand_like(t_new) / t_new.shape[-1]\n\n    return t_new"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n    # Sample uniformly from [0, 1)\n    u = torch.rand(num_samples, device=t.device)\n    if perturb:\n        # Perturb the samples to avoid sample clustering at bin boundaries.\n        u = u + torch.rand_like(u) * (1 / num_samples)\n    if single_jitter:\n        # Apply the same jitter to every sample along each dimension.\n        u = u + torch.rand_like(u) * (1 / num_samples)\n    else:\n        # Apply independent jitter to each sample.\n        u = u + torch.rand_like(u) * (1 / num_samples)\n    # Invert the CDF to get the sample points.\n    t_new = invert_cdf(u, t, w)\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Ensure that the CDF starts with exactly 0 and ends with exactly 1.\n    cw = integrate_weights(w)\n\n    # Draw uniform samples from the CDF.\n    u = torch.rand(num_samples, device=t.device)\n    t_new = invert_cdf(u, t, w)\n\n    # Perturb the samples to avoid sample clustering at bin boundaries.\n    if perturb:\n        t_new = t_new + torch.rand_like(t_new) / num_samples\n\n    # Jitter the samples to avoid numerical issues.\n    if single_jitter:\n        t_new = t_new + torch.rand_like(t_new) / num_samples\n    else:\n        t_new = t_new + torch.rand_like(t_new) / num_samples**0.5\n\n    # Clip the samples to the valid range.\n    t_new = torch.clamp(t_new, t[..., 0], t[..., -1])\n\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Check if t and w are sorted\n    if not torch.all(torch.diff(t) >= 0):\n        raise ValueError(\"t must be sorted\")\n\n    # Check if w sums to 1\n    if not torch.allclose(torch.sum(w, dim=-1), torch.ones_like(w[..., 0])):\n        raise ValueError(\"w must sum to 1\")\n\n    # Check if t and w have the same shape\n    if t.shape != w.shape:\n        raise ValueError(\"t and w must have the same shape\")\n\n    # Check if the last dimension of t and w is 1\n    if t.shape[-1] != 1:\n        raise ValueError(\"the last dimension of t and w must be 1\")\n\n    # Check if num_samples is positive\n    if num_samples <= 0:\n        raise ValueError(\"num_samples must be positive\")\n\n    # Check if the last dimension of t and w is 1\n    if t.shape[-1] != 1:\n        raise ValueError(\"the last dimension of t and w must be 1\")\n\n    # Check if the last dimension of t and w is 1\n    if t.shape[-1] != 1:\n        raise ValueError(\"the last dimension of t and w must be 1\")\n\n    # Check if the last dimension of t and w is 1\n    if t.shape[-1] != 1:\n        raise ValueError(\"the last dimension of t and w must be 1\")\n\n    # Check if the last dimension of t and w is 1\n    if t.shape[-1] != 1:\n        raise ValueError(\"the last dimension of t and w must be 1\")\n\n    # Check if the last dimension of t and w is 1\n    if t.shape[-1] != 1:\n        raise ValueError(\"the last dimension of t and w must be 1\")\n\n    # Check if the last dimension of t and w is 1\n    if t.shape[-1] != 1:\n        raise ValueError(\"the last dimension of t"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n    # Sample uniformly from [0, 1)\n    u = torch.rand(num_samples, device=t.device)\n    if perturb:\n        # Perturb the samples to avoid sample clustering at bin boundaries.\n        u = u + (torch.rand(num_samples, device=t.device) - 0.5) / num_samples\n    if single_jitter:\n        # Apply the same jitter to every sample along each dimension.\n        u = u + torch.rand_like(u) / num_samples\n    else:\n        # Apply independent jitter to each sample.\n        u = u + torch.rand_like(u) / num_samples\n    u = u.clip(0, 1)\n    # Invert the CDF to sample from the PDF.\n    t_new = invert_cdf(u, t, w)\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n    \"\"\"\n    Performs importance sampling on a piecewise-constant probability density function (PDF) defined by bin endpoints and weights.\n\n    Input-Output Arguments\n    :param t: torch.Tensor. The tensor containing bin endpoint coordinates, which must be sorted. It represents the domain of the PDF.\n    :param w: torch.Tensor. The tensor containing weights for each bin, used to define the piecewise-constant PDF.\n    :param num_samples: int. The number of samples to generate from the PDF.\n    :param perturb: bool, optional. If True, applies perturbation to the sampling process to avoid sample clustering at bin boundaries. Defaults to True.\n    :param single_jitter: bool, optional. If True, applies the same jitter to every sample along each dimension. If False, applies independent jitter to each sample. Defaults to False.\n    :return: torch.Tensor. A tensor of samples drawn from the specified PDF. The shape of the tensor is determined by the input tensors and the number of samples.\n    \"\"\"\n    # Sample uniformly from [0, 1]\n    u = torch.rand(num_samples, device=t.device)\n    # Sample from the PDF\n    t_new = invert_cdf(u, t, w)\n    # Apply jitter\n    if perturb:\n        if single_jitter:\n            jitter = torch.rand_like(t_new)\n        else:\n            jitter = torch.rand(t_new.shape, device=t.device)\n        t_new = t_new + jitter / w.shape[-1]\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Ensure that the CDF starts with exactly 0 and ends with exactly 1.\n    cw = integrate_weights(w)\n    # Compute the PDF and CDF for each weight vector.\n    p = cw[..., 1:] - cw[..., :-1]\n    cdf = torch.cumsum(p, dim=-1)\n    # Compute the PDF and CDF for the entire set of weights.\n    p_total = cw[..., -1]\n    cdf_total = torch.cumsum(p_total, dim=-1)\n    # Draw uniform samples from the entire set of weights.\n    u = torch.rand(num_samples, device=t.device)\n    # Invert the CDF to get the samples.\n    t_new = invert_cdf(u, t, w)\n    # Perturb the samples to avoid sample clustering at bin boundaries.\n    if perturb:\n        if single_jitter:\n            jitter = torch.rand_like(t_new)\n        else:\n            jitter = torch.rand_like(t_new).unsqueeze(-2)\n        t_new = t_new + jitter\n        t_new = torch.clamp(t_new, t[..., 0], t[..., -1])\n    return t_new"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n\n    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n    cdf = cw[..., :-1]\n    pdf = cdf[..., 1:] - cdf[..., :-1]\n\n    # Sample uniformly from [0, 1)\n    u = torch.rand(num_samples, *cdf.shape[:-1], device=cdf.device)\n\n    # Invert the CDF to get the sampling distribution\n    t_new = invert_cdf(u, t, w)\n\n    # Perturb the samples to avoid sample clustering at bin boundaries\n    if perturb:\n        if single_jitter:\n            jitter = torch.rand_like(t_new)\n        else:\n            jitter = torch.rand_like(t_new).unsqueeze(-2)\n        t_new = t_new + jitter\n\n    return t_new"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n    \"\"\"\n    Performs importance sampling on a piecewise-constant probability density function (PDF) defined by bin endpoints and weights. It generates samples according to the specified PDF, with options for perturbation and jittering of samples.\n\n    Input-Output Arguments\n    :param t: torch.Tensor. The tensor containing bin endpoint coordinates, which must be sorted. It represents the domain of the PDF.\n    :param w: torch.Tensor. The tensor containing weights for each bin, used to define the piecewise-constant PDF.\n    :param num_samples: int. The number of samples to generate from the PDF.\n    :param perturb: bool, optional. If True, applies perturbation to the sampling process to avoid sample clustering at bin boundaries. Defaults to True.\n    :param single_jitter: bool, optional. If True, applies the same jitter to every sample along each dimension. If False, applies independent jitter to each sample. Defaults to False.\n    :return: torch.Tensor. A tensor of samples drawn from the specified PDF. The shape of the tensor is determined by the input tensors and the number of samples.\n    \"\"\"\n\n    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n\n    # Sample uniformly in [0, 1) according to the CDF.\n    u = torch.rand(num_samples, device=t.device)\n    u = u.reshape([-1, 1])\n    t_new = invert_cdf(u, t, w)\n\n    # Optionally perturb the samples.\n    if perturb:\n        # Apply jitter to the samples to avoid clustering at bin boundaries.\n        if single_jitter:\n            t_new = t_new + torch.rand_like(t_new) / t.shape[-1]\n        else:\n            t_new = t_new + torch.rand_like(t_new) / t.shape[-1]\n\n        # Clamp the samples to the valid range.\n        t_new = t_new.clip(t["}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n    # Compute the CDF of the PDF.\n    ccdf = torch.cumsum(cw, dim=-1)\n    # Draw uniform samples from the CDF of the PDF.\n    u = torch.rand(list(cw.shape[:-1]) + [num_samples], device=cw.device)\n    if perturb:\n        # Perturb the samples to avoid sample clustering at bin boundaries.\n        u = u + torch.rand_like(u) / cw.shape[-1]\n    u = u.clip(0, 1)\n    # Invert the CDF to get the sample points.\n    t_new = invert_cdf(u, t, w)\n    # Add jitter to the samples to avoid numerical issues.\n    if single_jitter:\n        t_new = t_new + torch.rand_like(t_new) / cw.shape[-1]\n    else:\n        t_new = t_new + torch.rand_like(t_new) / cw.shape[-1]\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n\n    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n\n    # Sample uniformly from the CDF.\n    u = torch.rand(num_samples, device=t.device)\n    t_new = invert_cdf(u, t, w)\n\n    # Perturb the samples to avoid sample clustering at bin boundaries.\n    if perturb:\n        # Get the PDF value at the newly-sampled points.\n        p = torch.interpolate(cw, t_new, align_corners=False)\n        # Compute the tau values.\n        tau = torch.sqrt(p) * (1 - p)\n        # Perturb the samples.\n        if single_jitter:\n            t_new = t_new + torch.randn_like(t_new) * tau\n        else:\n            t_new = t_new + torch.randn_like(t_new) * tau[..., None]\n\n    return t_new"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n\n    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n\n    # Draw uniform samples in [0, 1)\n    u = torch.rand(num_samples, *t.shape[:-1], device=t.device)\n\n    # Invert the CDF to get a sample within each interval.\n    t_new = invert_cdf(u, t, w)\n\n    # Perturb the samples to avoid grid-like artifacts.\n    if perturb:\n        if single_jitter:\n            jitter = torch.rand_like(u)\n        else:\n            jitter = torch.rand_like(u).split(1, dim=-1)\n        t_new += jitter\n\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # t, w = matchup_channels(t, w)\n    t, w = t.to(torch.float32), w.to(torch.float32)\n    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n    # Draw uniform samples from the CDF.\n    u = torch.rand(t.shape[:-1] + (num_samples,), device=t.device)\n    # Invert the CDF to get the sample points.\n    t_new = invert_cdf(u, t, w)\n    # Optionally perturb the samples to avoid clustering at bin boundaries.\n    if perturb:\n        # Perturb the samples to avoid clustering at bin boundaries.\n        t_new = t_new + torch.rand_like(t_new) / cw.shape[-1]\n        # If we are jittering each sample along each dimension, do so.\n        if single_jitter:\n            t_new = t_new + torch.rand_like(t_new) / cw.shape[-1]\n        # Otherwise, jitter the samples along each dimension independently.\n        else:\n            t_new = t_new + torch.rand_like(t_new) / cw.shape[-1]\n    return t_new"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Check input tensors\n    if t.ndim != 2:\n        raise ValueError(\n            f\"t must have shape (batch_size, num_bins), got {t.shape}\")\n    if w.ndim != 2:\n        raise ValueError(\n            f\"w must have shape (batch_size, num_bins), got {w.shape}\")\n    if t.shape != w.shape:\n        raise ValueError(\n            f\"t and w must have the same shape, got {t.shape} and {w.shape}\")\n    if t.shape[0] != num_samples:\n        raise ValueError(\n            f\"t and w must have the same batch size as num_samples, got {t.shape[0]} and {num_samples}\")\n\n    # Check the shape of the input tensors\n    if t.shape[-1] != w.shape[-1] + 1:\n        raise ValueError(\n            f\"t must have shape (batch_size, num_bins + 1), got {t.shape}\")\n\n    # Ensure that the CDF starts with exactly 0 and ends with exactly 1.\n    cw = integrate_weights(w)\n\n    # Generate samples from the CDF\n    u = torch.rand(num_samples, device=t.device)\n    t_new = invert_cdf(u, t, w)\n\n    # Perturb the samples to avoid sample clustering at bin boundaries\n    if perturb:\n        if single_jitter:\n            t_new += torch.rand_like(t_new)\n        else:\n            t_new += torch.rand_like(t_new) / num_samples\n\n    # Clip the samples to ensure they are within the valid range\n    t_new = t_new.clip(t[..., 0], t[..., -1])\n\n    return t_new"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, p = matchup_channels(t, w)\n    t_dilate = torch.max_pool1d(t[..., None], dilation, stride=1, padding=dilation // 2).squeeze(-1)\n    p_dilate = torch.max_pool1d(p[..., None], dilation, stride=1, padding=dilation // 2).squeeze(-1)\n    t_dilate = torch.clamp(t_dilate, domain[0], domain[1])\n    return t_dilate, p_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Perform max-pooling on the time steps to dilate them.\n    t_dilate = torch.nn.functional.max_pool1d(t.unsqueeze(-1), dilation, stride=1, padding=dilation // 2).squeeze(-1)\n\n    # Clip the dilated time steps to the specified domain.\n    t_dilate = torch.clamp(t_dilate, min=domain[0], max=domain[1])\n\n    # Compute the dilated weights by dividing the original weights by the dilated time steps.\n    w_dilate = w / (t_dilate[..., 1:] - t_dilate[..., :-1]).clip(eps)\n\n    return t_dilate, w_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute the dilated time steps.\n    t_dilate = t * dilation\n\n    # Clip the dilated time steps to the specified domain.\n    t_dilate = torch.clamp(t_dilate, *domain)\n\n    # Compute the dilated weights.\n    w_dilate = torch.zeros_like(t_dilate)\n    w_dilate[..., 1:] = torch.max_pool1d(w, dilation, stride=1, padding=dilation - 1)\n\n    return t_dilate, w_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n\n    # Pad the time steps and weights with a dilation of 1 on either side.\n    t_pad = torch.cat([t[..., :1] - dilation, t, t[..., -1:] + dilation], dim=-1)\n    w_pad = torch.cat([w[..., :1], w, w[..., -1:]], dim=-1)\n\n    # Compute the dilated time steps and weights.\n    t_dilate = torch.max_pool1d(t_pad, kernel_size=3, stride=1, padding=1).squeeze(-1)\n    w_dilate = torch.max_pool1d(w_pad, kernel_size=3, stride=1, padding=1).squeeze(-1)\n\n    # Clip the dilated time steps to the specified domain.\n    t_dilate = torch.clamp(t_dilate, min=domain[0], max=domain[1])\n\n    # Adjust the weights to match the dilated time steps.\n    w_dilate = w_dilate * (t_dilate[..., 1:] - t_dilate[..., :-1])\n\n    return t_dilate, w_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n\n    # Dilate the CDF.\n    cw_dilate = torch.max_pool1d(cw, kernel_size=dilation, stride=dilation, padding=0)\n\n    # Clip the dilated CDF to the specified domain.\n    cw_dilate = torch.clamp(cw_dilate, min=domain[0], max=domain[1])\n\n    # Compute the dilated PDF from the dilated CDF.\n    p_dilate = (cw_dilate[..., 1:] - cw_dilate[..., :-1]) / (t[..., 1:] - t[..., :-1]).clip(eps)\n\n    # Compute the dilated time steps from the dilated CDF.\n    t_dilate = interpolate(cw_dilate[..., 1:], cw_dilate, t)\n\n    return t_dilate, p_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n\n    # Dilate the CDF.\n    cw_dilate = torch.max_pool1d(cw, dilation, stride=1, padding=dilation // 2)\n\n    # Invert the dilated CDF to get the dilated time steps.\n    t_dilate = invert_cdf(cw_dilate, t, w)\n\n    # Clip the dilated time steps to the specified domain.\n    t_dilate = torch.clamp(t_dilate, min=domain[0], max=domain[1])\n\n    # Compute the dilated PDF.\n    cw_dilate = integrate_weights(t_dilate, w)\n    p_dilate = cw_dilate[..., 1:] - cw_dilate[..., :-1]\n\n    return t_dilate, p_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n\n    # Pad the time steps and weights with a value of 0 at the beginning and end.\n    t = torch.cat([t.new_zeros(t.shape[:-1] + (1,)), t, t.new_zeros(t.shape[:-1] + (1,))], dim=-1)\n    w = torch.cat([w.new_zeros(w.shape[:-1] + (1,)), w, w.new_zeros(w.shape[:-1] + (1,))], dim=-1)\n\n    # Compute the dilated time steps.\n    t_dilate = torch.max_pool1d(t, kernel_size=dilation, stride=1, padding=dilation // 2)\n\n    # Compute the dilated weights.\n    w_dilate = torch.max_pool1d(w, kernel_size=dilation, stride=1, padding=dilation // 2)\n\n    # Clip the dilated time steps to the specified domain.\n    t_dilate = t_dilate.clip(*domain)\n\n    # Compute the dilated weights.\n    w_dilate = w_dilate.clip(eps)\n\n    return t_dilate, w_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, p = matchup_channels(t, w)\n    t_dilate = torch.cat([t[..., :1], t[..., 1:] - dilation], dim=-1)\n    t_dilate = torch.max(torch.min(t_dilate, torch.as_tensor(domain[1], device=t.device, dtype=t.dtype)),\n                         torch.as_tensor(domain[0], device=t.device, dtype=t.dtype))\n    p_dilate = torch.max(torch.zeros_like(t_dilate), torch.diff(t_dilate, dim=-1))\n    return t_dilate, p_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, p = matchup_channels(t, w)\n    # t, p = matchup_channels(t, p)\n    # t_dilate = torch.cat([t[..., :1], t[..., 1:] - dilation, t[..., -1:]], dim=-1)\n    t_dilate = torch.cat([t[..., :1], t[..., 1:] - dilation], dim=-1)\n    t_dilate = torch.clamp(t_dilate, min=domain[0], max=domain[1])\n    p_dilate = torch.nn.functional.max_pool1d(p[..., None], kernel_size=2, stride=1, padding=0)[..., 0]\n    return t_dilate, p_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, p = matchup_channels(t, w)\n    # Compute the dilated time steps by finding the maximum time step within each interval.\n    t_dilate = torch.max(t[..., :-1] + dilation, t[..., 1:] - dilation)\n    # Clip the dilated time steps to the specified domain.\n    t_dilate = torch.clamp(t_dilate, min=domain[0], max=domain[1])\n    # Compute the dilated weights by dividing the original weights by the width of each interval.\n    p_dilate = p / (t[..., 1:] - t[..., :-1]).clip(eps)\n    return t_dilate, p_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n\n    # Pad the time steps and weights with a dilation of 1 at the beginning and end.\n    t_padded = torch.cat([t[..., :1] - dilation, t, t[..., -1:] + dilation], dim=-1)\n    w_padded = torch.cat([w[..., :1], w, w[..., -1:]], dim=-1)\n\n    # Compute the maximum value of the weights within each interval.\n    w_max = torch.max_pool1d(w_padded, kernel_size=2, stride=1, padding=0)\n\n    # Compute the dilated time steps by subtracting the dilation from the original time steps.\n    t_dilate = t_padded[..., 1:-1] - dilation\n\n    # Clip the dilated time steps to the specified domain.\n    t_dilate = torch.clamp(t_dilate, domain[0], domain[1])\n\n    # Compute the dilated weights by dividing the original weights by the maximum value within each interval.\n    w_dilate = w_max[..., 1:-1] / w_padded[..., 1:-1]\n\n    return t_dilate, w_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, p = matchup_channels(t, w)\n    # Dilate the time steps\n    t_dilate = t * dilation\n\n    # Clip the dilated time steps to the specified domain\n    t_dilate = torch.clamp(t_dilate, *domain)\n\n    # Compute the dilated weights as the maximum value of the weights within each interval\n    p_dilate = torch.zeros_like(t_dilate)\n    for i in range(t_dilate.shape[-1] - 1):\n        p_dilate[..., i] = torch.max(p[..., i:i + 2])\n\n    # Normalize the dilated weights to sum to 1\n    p_dilate = p_dilate / torch.sum(p_dilate, dim=-1, keepdim=True)\n\n    return t_dilate, p_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, p = matchup_channels(t, w)\n\n    # Dilate the time steps.\n    t_dilate = t[..., 1:] + (t[..., 1:] - t[..., :-1]) * dilation\n\n    # Clip the dilated time steps to the specified domain.\n    t_dilate = torch.clamp(t_dilate, *domain)\n\n    # Compute the dilated weights by finding the maximum values of the weights within each interval.\n    p_dilate = torch.max_pool1d(p.unsqueeze(-1), 2, stride=1, padding=1).squeeze(-1)\n\n    return t_dilate, p_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, p = matchup_channels(t, w)\n    # The dilation is performed by finding the maximum value within a window of size dilation.\n    dilated_t = torch.max_pool1d(t, kernel_size=dilation, stride=1, padding=dilation // 2)\n    # The dilated time steps are then clipped to the specified domain.\n    dilated_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n    # The weights are adjusted to match the dilated time steps.\n    dilated_p = query(dilated_t, t, p)\n    return dilated_t, dilated_p"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, p = matchup_channels(t, w)\n\n    # Perform max-pooling on the PDF.\n    p_dilate = torch.nn.functional.max_pool1d(\n        p[..., None], dilation, stride=1, padding=dilation // 2,\n        ceil_mode=True).squeeze(-1)\n\n    # Compute the dilated time steps from the dilated PDF.\n    t_dilate = invert_cdf(torch.linspace(0, 1, p_dilate.shape[-1], device=p.device, dtype=p.dtype), t, p_dilate)\n\n    # Clip the dilated time steps to the specified domain.\n    t_dilate = t_dilate.clip(*domain)\n\n    # Compute the dilated PDF from the dilated time steps.\n    p_dilate = weight_to_pdf(t_dilate, pdf_to_weight(t, p))\n\n    return t_dilate, p_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # t, w = matchup_channels(t, w)\n\n    # Pad the time steps and weights with a left and right boundary.\n    t = torch.cat([t.new_full((*t.shape[:-1], 1), -torch.inf), t, t.new_full((*t.shape[:-1], 1), torch.inf)], dim=-1)\n    w = torch.cat([torch.zeros_like(t[..., :1]), w, torch.zeros_like(t[..., :1])], dim=-1)\n\n    # Compute the dilated time steps.\n    t_dilate = torch.max_pool1d(t, kernel_size=2, stride=1, padding=0).squeeze(-1)\n\n    # Compute the dilated weights.\n    w_dilate = torch.max_pool1d(w, kernel_size=2, stride=1, padding=0).squeeze(-1)\n\n    # Clip the dilated time steps to the specified domain.\n    t_dilate = torch.clamp(t_dilate, min=domain[0], max=domain[1])\n\n    # Compute the dilated weights.\n    w_dilate = w_dilate / (t_dilate[..., 1:] - t_dilate[..., :-1]).clip(eps)\n\n    # Remove the left and right boundaries.\n    t_dilate = t_dilate[..., 1:-1]\n    w_dilate = w_dilate[..., 1:-1]\n\n    return t_dilate, w_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, p = matchup_channels(t, w)\n    p = weight_to_pdf(t, p)\n\n    # The dilated time steps are the maxima of the original time steps with a\n    # dilation factor.\n    dilated_t = torch.max_pool1d(t[..., None], dilation, stride=1, padding=dilation // 2)[..., 0]\n\n    # The dilated weights are the maxima of the original weights with a dilation\n    # factor.\n    dilated_p = torch.max_pool1d(p[..., None], dilation, stride=1, padding=dilation // 2)[..., 0]\n\n    # Clip the dilated time steps to the specified domain.\n    dilated_t = torch.clamp(dilated_t, domain[0], domain[1])\n\n    # Adjust the dilated weights to match the dilated time steps.\n    dilated_p = dilated_p * (dilated_t[..., 1:] - dilated_t[..., :-1]).clip(eps)\n\n    return dilated_t, dilated_p"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Pad the time steps with an extra value at the beginning and end.\n    t = torch.cat([t[..., :1], t, t[..., -1:]], dim=-1)\n    w = torch.cat([w[..., :1], w, w[..., -1:]], dim=-1)\n\n    # Compute the dilated time steps.\n    t_dilate = t * dilation\n\n    # Clip the dilated time steps to the specified domain.\n    t_dilate = torch.clamp(t_dilate, *domain)\n\n    # Compute the dilated weights.\n    w_dilate = torch.max_pool1d(w, dilation, stride=1, padding=dilation // 2)\n\n    # Compute the dilated weights and time steps.\n    t_dilate, w_dilate = matchup_channels(t_dilate, w_dilate)\n\n    return t_dilate, w_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, p = matchup_channels(t, w)\n    # Pad the time steps and weights with a dilation of 1 to ensure that the\n    # dilated time steps are always within the domain.\n    t_pad = torch.cat([t - dilation, t, t + dilation], dim=-1)\n    p_pad = torch.cat([p, p, p], dim=-1)\n    # Compute the dilated time steps by taking the maximum value within the\n    # window of size 2dilation + 1 centered at each time step.\n    t_dilate = torch.nn.functional.max_pool1d(t_pad, kernel_size=2 * dilation + 1, stride=1, padding=dilation)\n    # Compute the dilated weights by taking the maximum value within the window\n    # of size 2dilation + 1 centered at each time step.\n    p_dilate = torch.nn.functional.max_pool1d(p_pad, kernel_size=2 * dilation + 1, stride=1, padding=dilation)\n    # Clip the dilated time steps to the specified domain.\n    t_dilate = torch.clamp(t_dilate, domain[0], domain[1])\n    # Compute the dilated weights by dividing the dilated weights by the\n    # difference between the dilated time steps and the original time steps.\n    p_dilate = p_dilate / (t_dilate[..., 1:] - t_dilate[..., :-1]).clip(eps)\n    return t_dilate, p_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Ensure t is sorted\n    t, w = matchup_channels(t, w)\n    assert torch.all(t[..., 1:] >= t[..., :-1]), \"t must be sorted\"\n\n    # Ensure the dilation is positive\n    assert dilation > 0, \"dilation must be positive\"\n\n    # Perform max-pooling on the weights to dilate the time steps\n    dilated_weights = torch.nn.functional.max_pool1d(w.unsqueeze(-1), dilation, stride=dilation, padding=0).squeeze(-1)\n\n    # Clip the dilated time steps to the specified domain\n    dilated_time_steps = torch.clamp(t * dilation, min=domain[0], max=domain[1])\n\n    # Compute the adjusted weights\n    adjusted_weights = dilated_weights / torch.sum(dilated_weights, dim=-1, keepdim=True)\n\n    return dilated_time_steps, adjusted_weights"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # "}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Ensure the inputs are tensors.\n    tq = torch.as_tensor(tq)\n    t = torch.as_tensor(t)\n    y = torch.as_tensor(y)\n\n    # Ensure the inputs have the same shape.\n    tq.shape.assert_is_compatible_with(tq.shape)\n    t.shape.assert_is_compatible_with(t)\n    y.shape.assert_is_compatible_with(t)\n\n    # Ensure the inputs have the same dtype.\n    tq.dtype.assert_is_compatible_with(t)\n    t.dtype.assert_is_compatible_with(t)\n    y.dtype.assert_is_compatible_with(t)\n\n    # Ensure the inputs have the same device.\n    tq.device.assert_is_compatible_with(t)\n    t.device.assert_is_compatible_with(t)\n    y.device.assert_is_compatible_with(t)\n\n    # Ensure the inputs have the same device.\n    tq.device.assert_is_compatible_with(t)\n    t.device.assert_is_compatible_with(t)\n    y.device.assert_is_compatible_with(t)\n\n    # Ensure the inputs have the same device.\n    tq.device.assert_is_compatible_with(t)\n    t.device.assert_is_compatible_with(t)\n    y.device.assert_is_compatible_with(t)\n\n    # Ensure the inputs have the same device.\n    tq.device.assert_is_compatible_with(t)\n    t.device.assert_is_compatible_with(t)\n    y.device.assert_is_compatible_with(t)\n\n    # Ensure the inputs have the same device.\n    tq.device.assert_is_compatible_with(t)\n    t.device.assert_is_compatible_with(t)\n    y.device.assert_is_compatible_with(t)\n\n    # Ensure"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check if the query times are outside the range of the step function\n    outside = tq < t[..., 0]\n    outside = outside | (tq > t[..., -1])\n\n    # Interpolate the values at the query times\n    yq = interpolate(tq, t, y)\n\n    # Set the outside values to the outside value\n    yq = torch.where(outside, outside_value, yq)\n\n    return yq"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Match up the channels of tq, t, and y\n    tq, t, y = matchup_channels(tq, t, y)\n\n    # Find the indices of the intervals in t that contain tq\n    idx_lo, idx_hi = searchsorted(t, tq)\n\n    # Interpolate the values of y at tq using the indices of the intervals in t\n    yq = interpolate(tq, t[..., idx_lo], y[..., idx_lo])\n\n    # If tq is exactly equal to a time in t, return the outside value\n    yq = torch.where(tq == t[..., idx_lo], outside_value, yq)\n\n    return yq"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    tq = tq.flatten()\n    t = t.flatten()\n    y = y.flatten()\n\n    # Check if the input tensors have the same shape\n    if t.shape != y.shape:\n        raise ValueError(\"'t' and 'y' must have the same shape.\")\n\n    # Check if the query times are within the range of the step function\n    if torch.any(tq < t[0]) or torch.any(tq > t[-1]):\n        raise ValueError(\"Query times must be within the range of the step function.\")\n\n    # Check if the step function is monotonically increasing\n    if torch.any(torch.diff(t) < 0):\n        raise ValueError(\"Step function must be monotonically increasing.\")\n\n    # Find the indices of the times that are less than or equal to the query times\n    idx = torch.searchsorted(t, tq)\n\n    # Check if the query times exactly match a step change time\n    if torch.any(tq == t[idx]):\n        return outside_value\n\n    # Interpolate the values at the query times\n    yq = torch.interp(tq, t, y)\n\n    return yq"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Pad the step function with a value at the beginning and end.\n    t = torch.cat([t.new_zeros(t.shape[:-1] + (1,)), t, t.new_ones(t.shape[:-1] + (1,))], dim=-1)\n    y = torch.cat([y.new_zeros(y.shape[:-1] + (1,)), y, y.new_zeros(y.shape[:-1] + (1,))], dim=-1)\n\n    # Find the indices of the query times in the step function.\n    idx_lo, idx_hi = searchsorted(t, tq)\n\n    # Interpolate the values at the query times.\n    t_lo = t[..., idx_lo]\n    t_hi = t[..., idx_hi]\n    y_lo = y[..., idx_lo]\n    y_hi = y[..., idx_hi]\n    t_diff = t_hi - t_lo\n    t_diff = torch.where(t_diff == 0, torch.ones_like(t_diff), t_diff)\n    y_diff = y_hi - y_lo\n    y_diff = torch.where(y_diff == 0, torch.ones_like(y_diff), y_diff)\n    yq = y_lo + (tq - t_lo) * y_diff / t_diff\n\n    # If the query times exactly match a step change, return the outside value.\n    yq = torch.where(t_lo == tq, outside_value, yq)\n\n    return yq"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Ensure that the query times are in the correct range\n    tq = tq.clip(t[0], t[-1])\n\n    # Find the indices of the step function values that bracket the query times\n    i = torch.searchsorted(t, tq, right=True) - 1\n\n    # If the query time exactly matches a step change time, return the outside value\n    outside = torch.eq(tq, t[i])\n    yq = outside_value * outside + (1 - outside) * y[i]\n\n    # Interpolate the step function values at the query times\n    yq = yq + (tq - t[i]) * (y[i + 1] - y[i]) / (t[i + 1] - t[i])\n\n    return yq\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Match the shape of tq to the shape of t\n    if tq.ndim == t.ndim - 1:\n        tq = tq[..., None]\n\n    # Find the indices of the closest time values in t to tq\n    indices = torch.searchsorted(t, tq, side='right') - 1\n\n    # Clip the indices to the valid range\n    indices = torch.clamp(indices, 0, t.shape[-1] - 1)\n\n    # Look up the values of y at the indices\n    yq = y.gather(-1, indices[..., None])\n\n    # If tq matches a step change time, return the outside value\n    yq = torch.where(tq == t[..., indices], outside_value, yq)\n\n    # Return the interpolated or outside values\n    return yq[..., 0]"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Convert tq to a tensor if it is not already one.\n    tq = torch.as_tensor(tq)\n\n    # Flatten the input tensors to 1D.\n    t = t.flatten()\n    y = y.flatten()\n\n    # Find the indices of the times in 't' that are less than or equal to 'tq'.\n    indices = torch.searchsorted(t, tq, side='right') - 1\n\n    # Clip the indices to the valid range.\n    indices = torch.clip(indices, 0, len(t) - 1)\n\n    # Check if the query times exactly match a step change time.\n    exact_match = torch.eq(tq, t[indices])\n\n    # If the query times exactly match a step change time, return the outside value.\n    if torch.any(exact_match):\n        return torch.where(exact_match, outside_value, y[indices])\n\n    # Otherwise, interpolate the values at the query times.\n    return torch.lerp(y[indices], y[indices + 1], (tq - t[indices]) / (t[indices + 1] - t[indices]))"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Pad the time and value vectors with a value at the start and end to handle edge cases\n    t = torch.cat([t.new_zeros(t.shape[:-1] + (1,)), t, t.new_zeros(t.shape[:-1] + (1,))], dim=-1)\n    y = torch.cat([y.new_zeros(y.shape[:-1] + (1,)), y, y.new_zeros(y.shape[:-1] + (1,))], dim=-1)\n\n    # Find the indices of the time values that bracket the query times\n    tq_indices = torch.searchsorted(t, tq, right=True)\n\n    # If the query time exactly matches a step change time, return the outside value\n    if torch.allclose(t[..., tq_indices], tq):\n        return outside_value\n\n    # Otherwise, interpolate the value at the query time based on the step function\n    y_interp = torch.lerp(y[..., tq_indices - 1], y[..., tq_indices], (tq - t[..., tq_indices - 1]) / (t[..., tq_indices] - t[..., tq_indices - 1]))\n\n    return y_interp"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Pad the time and value arrays with a value at the beginning and end of the step function.\n    t = torch.cat([t.new_zeros(1), t, t.new_ones(1)])\n    y = torch.cat([y.new_zeros(1), y, y.new_zeros(1)])\n\n    # Find the indices of the time values that are less than or equal to the query times.\n    idx = torch.searchsorted(t, tq, right=True) - 1\n\n    # If the query time exactly matches a step change time, return the outside value.\n    outside = idx == t.shape[-1] - 1\n    if outside.any():\n        tq = tq[outside]\n        idx = idx[outside]\n\n    # Interpolate the values at the query times using the indices of the closest step change times.\n    yq = (y[idx + 1] - y[idx]) / (t[idx + 1] - t[idx]) * (tq - t[idx]) + y[idx]\n\n    return yq"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Step 1: Find the indices of the time intervals in which each query time falls\n    # Step 2: Interpolate the values of the step function at the query times using the indices found in Step 1\n    # Step 3: Return the interpolated values or outside values depending on whether the query time exactly matches a step change time\n\n    # Step 1: Find the indices of the time intervals in which each query time falls\n    indices = torch.searchsorted(t, tq, right=True) - 1\n\n    # Step 2: Interpolate the values of the step function at the query times using the indices found in Step 1\n    # If the query time exactly matches a step change time, return the outside value\n    if outside_value is None:\n        outside_value = y[..., -1]\n    yq = torch.where(\n        indices == t.shape[-1] - 1,\n        outside_value,\n        y[..., indices] + (tq - t[..., indices]) / (t[..., indices + 1] - t[..., indices]) * (y[..., indices + 1] - y[..., indices])\n    )\n\n    # Step 3: Return the interpolated values or outside values depending on whether the query time exactly matches a step change time\n    return yq"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Pad the endpoints of the step function with a value outside the range.\n    t = torch.cat([t[..., :1], t, t[..., -1:]], dim=-1)\n    y = torch.cat([y[..., :1], y, y[..., -1:]], dim=-1)\n\n    # Find the indices of the left and right endpoints of the intervals in which the query times fall.\n    i0, i1 = searchsorted(t, tq)\n\n    # If the query time exactly matches a step change time, return the outside value.\n    mask = i0 == i1\n    if torch.any(mask):\n        yq = torch.where(mask, outside_value, y[..., i0])\n    else:\n        yq = y[..., i0]\n\n    # Interpolate the values at the query times.\n    yq = yq * (t1 - tq) / (t1 - t0) + y[..., i1] * (tq - t0) / (t1 - t0)\n\n    return yq\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    if tq.ndim == t.ndim - 1:\n        tq = tq[None]\n    if t.ndim == y.ndim + 1:\n        y = y[..., 0]\n    # Find the indices of the time points in 't' that are closest to 'tq'\n    indices = torch.searchsorted(t, tq)\n\n    # Clip the indices to be within the valid range\n    indices = torch.clamp(indices, 0, t.shape[-1] - 1)\n\n    # Check if the query times exactly match a step change time\n    exact_matches = torch.eq(tq, t[..., indices])\n\n    # Interpolate the values at the query times\n    yq = torch.lerp(y[..., indices - 1], y[..., indices], tq - t[..., indices - 1])\n\n    # Set the outside values to the specified value\n    yq = torch.where(exact_matches, outside_value, yq)\n\n    return yq"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Convert tq to a 2D tensor\n    if tq.ndim == 0:\n        tq = tq.unsqueeze(0)\n\n    # Convert t to a 2D tensor\n    if t.ndim == 1:\n        t = t.unsqueeze(0)\n\n    # Convert y to a 2D tensor\n    if y.ndim == 1:\n        y = y.unsqueeze(0)\n\n    # Check that tq and t have the same number of dimensions\n    if tq.ndim != t.ndim:\n        raise ValueError(\n            f\"tq and t must have the same number of dimensions, but got {tq.ndim} and {t.ndim} respectively.\")\n\n    # Check that tq and y have the same number of dimensions\n    if tq.ndim != y.ndim:\n        raise ValueError(\n            f\"tq and y must have the same number of dimensions, but got {tq.ndim} and {y.ndim} respectively.\")\n\n    # Check that tq and y have the same shape\n    if tq.shape != y.shape:\n        raise ValueError(\n            f\"tq and y must have the same shape, but got {tq.shape} and {y.shape} respectively.\")\n\n    # Check that tq and y have the same number of steps\n    if tq.shape[0] != t.shape[0]:\n        raise ValueError(\n            f\"tq and t must have the same number of steps, but got {tq.shape[0]} and {t.shape[0]} respectively.\")\n\n    # Check that tq and y have the same number of dimensions\n    if tq.ndim != y.ndim:\n        raise ValueError(\n            f\"tq and y must have the same number of dimensions, but got {tq.ndim} and {y.ndim} respectively.\")\n\n    # Check that tq and y have the same number of dimensions\n    if tq.ndim != t.ndim:\n        raise ValueError(\n           "}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Convert tq to a tensor if it's not already one.\n    tq = torch.as_tensor(tq)\n\n    # Find the indices of the time points that are less than or equal to tq.\n    idx = torch.searchsorted(t, tq)\n\n    # If the query time exactly matches a time point, return the outside value.\n    if torch.any(idx == len(t) - 1):\n        return outside_value\n\n    # If the query time is less than the first time point, return the first value.\n    if torch.any(idx == 0):\n        return y[0]\n\n    # Interpolate the value at the query time using the time points before and after the query time.\n    return torch.lerp(y[idx - 1], y[idx], (tq - t[idx - 1]) / (t[idx] - t[idx - 1]))\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Ensure that tq is a 1D tensor.\n    tq = torch.as_tensor(tq)\n    if tq.ndim != 1:\n        raise ValueError(f\"tq must be a 1D tensor, but got shape {tq.shape}\")\n\n    # Ensure that t and y are 1D tensors with the same length.\n    t = torch.as_tensor(t)\n    y = torch.as_tensor(y)\n    if t.ndim != 1 or y.ndim != 1 or t.shape[0] != y.shape[0]:\n        raise ValueError(f\"t and y must be 1D tensors with the same length, but got shapes {t.shape} and {y.shape}\")\n\n    # Find the indices of the time points in t that are greater than or equal to the query times in tq.\n    idx = torch.searchsorted(t, tq)\n\n    # Check if any query times exactly match a time point in t.\n    exact_match = torch.any(tq == t)\n\n    # If any query times exactly match a time point in t, return the corresponding value from y.\n    if exact_match:\n        return y[idx]\n\n    # Otherwise, interpolate the values between the time points in t that are immediately before and after the query times in tq.\n    y_left = y[idx - 1]\n    y_right = y[idx]\n    t_left = t[idx - 1]\n    t_right = t[idx]\n    return (y_left * (t_right - tq) + y_right * (tq - t_left)) / (t_right - t_left)"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check if tq is a scalar\n    if torch.is_tensor(tq):\n        tq = tq.flatten()\n        is_scalar = False\n    else:\n        is_scalar = True\n\n    # Check if t and y have the same shape\n    if t.shape != y.shape:\n        raise ValueError(\"t and y must have the same shape.\")\n\n    # Check if tq is within the range of t\n    if torch.any(tq < t[0]) or torch.any(tq > t[-1]):\n        raise ValueError(\"tq must be within the range of t.\")\n\n    # Compute the indices of the nearest time values in t to the query times\n    indices = torch.searchsorted(t, tq)\n\n    # Compute the interpolated values at the query times\n    yq = torch.where(\n        indices == 0,\n        outside_value,\n        torch.where(\n            indices == t.shape[0],\n            outside_value,\n            torch.where(\n                indices == t.shape[0] - 1,\n                outside_value,\n                y[indices - 1] + (tq - t[indices - 1]) * (y[indices] - y[indices - 1]) / (t[indices] - t[indices - 1])\n            )\n        )\n    )\n\n    # Reshape yq to match the shape of tq\n    if is_scalar:\n        yq = yq.reshape(())\n\n    return yq"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    tq = torch.as_tensor(tq)\n    t = torch.as_tensor(t)\n    y = torch.as_tensor(y)\n\n    # Check if the inputs are one-dimensional and of the same length\n    if tq.ndim != 1 or t.ndim != 1 or y.ndim != 1 or tq.shape != t.shape or tq.shape != y.shape:\n        raise ValueError(\"Inputs must be one-dimensional and of the same length\")\n\n    # Check if the inputs are sorted\n    if not torch.all(t[:-1] <= t[1:]):\n        raise ValueError(\"Inputs must be sorted\")\n\n    # Check if the inputs contain at least two distinct values\n    if torch.all(t == t[0]) or torch.all(y == y[0]):\n        raise ValueError(\"Inputs must contain at least two distinct values\")\n\n    # Check if the query times are within the range of the step function\n    if torch.any(tq < t[0]) or torch.any(tq > t[-1]):\n        raise ValueError(\"Query times must be within the range of the step function\")\n\n    # Check if the step function is monotonic\n    if not torch.all(torch.diff(y) >= 0):\n        raise ValueError(\"Step function must be monotonic\")\n\n    # Find the indices of the query times in the step function\n    idx = torch.searchsorted(t, tq)\n\n    # Check if the query times exactly match a step change time\n    if torch.any(tq == t[idx]):\n        return outside_value\n\n    # Interpolate the values at the query times\n    return torch.interp(tq, t, y)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check input shapes and dimensions\n    if tq.ndim != t.ndim or tq.ndim != y.ndim or tq.shape[-1] != t.shape[-1] or tq.shape[-1] != y.shape[-1]:\n        raise ValueError(\"Input tensors must have the same shape and length in the last dimension.\")\n\n    # Convert t and tq to tensors\n    tq = torch.as_tensor(tq)\n    t = torch.as_tensor(t)\n    y = torch.as_tensor(y)\n\n    # Check if tq is a scalar\n    if tq.ndim == 0:\n        tq = tq.unsqueeze(0)\n\n    # Check if t is a scalar\n    if t.ndim == 0:\n        t = t.unsqueeze(0)\n\n    # Check if y is a scalar\n    if y.ndim == 0:\n        y = y.unsqueeze(0)\n\n    # Find the indices of the closest time points in t to tq\n    idx = torch.searchsorted(t, tq)\n\n    # Check if tq is outside the range of t\n    idx = torch.clamp(idx, 0, t.shape[-1] - 1)\n\n    # Check if tq is an exact match to a time point in t\n    idx_exact = torch.where(torch.eq(tq, t[idx]))\n\n    # If tq is an exact match to a time point in t, return the corresponding value from y\n    if idx_exact[0].nelement() > 0:\n        yq = y[idx_exact]\n\n    # If tq is not an exact match to a time point in t, interpolate the value from y\n    else:\n        yq = torch.lerp(y[idx - 1], y[idx], (tq - t[idx - 1]) / (t[idx] - t[idx - 1]))\n\n    # If tq is an exact match to a time point in t"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    w_anneal = w * torch.sigmoid(anneal_slope * (1 - train_frac))\n    w_anneal = w_anneal / torch.sum(w_anneal, dim=-1, keepdim=True).clip(eps)\n    return t, w_anneal"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    w = w.clip(eps)\n    w_anneal = w * torch.sigmoid(anneal_slope * (1 - train_frac))\n    w_anneal = w_anneal / torch.sum(w_anneal, dim=-1, keepdim=True).clip(eps)\n    return t, w_anneal"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    t = t[..., :-1]\n    w = w.clip(eps)\n    w_anneal = w * torch.sigmoid(anneal_slope * (1 - train_frac))\n    w_anneal /= torch.sum(w_anneal, dim=-1, keepdim=True).clip(eps)\n    return t, w_anneal"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # Calculate the annealing factor using Schlick's bias function\n    anneal_factor = torch.sigmoid(anneal_slope * (1 - train_frac))\n\n    # Adjust the weights by multiplying with the annealing factor\n    w_annealed = w * anneal_factor\n\n    # Handle cases where adjacent intervals have zero distance\n    w_annealed = torch.where(torch.abs(t[..., 1:] - t[..., :-1]) < eps, torch.zeros_like(w_annealed), w_annealed)\n\n    # Prevent NaN values by using a softmax operation on the adjusted weights\n    w_annealed = torch.softmax(w_annealed, dim=-1)\n\n    return w_annealed"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n\n    # Compute the annealing factor based on the training fraction and slope\n    anneal_factor = torch.sigmoid(anneal_slope * (train_frac - 0.5))\n\n    # Compute the annealed weights by multiplying the original weights with the annealing factor\n    w_annealed = w * anneal_factor\n\n    # Ensure that the weights sum to 1\n    w_annealed /= torch.sum(w_annealed, dim=-1, keepdim=True).clip(eps)\n\n    return t, w_annealed"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    # t.shape[-1] = w.shape[-1] + 1\n\n    # Calculate the annealing factor using Schlick's bias function\n    anneal_factor = torch.sigmoid(anneal_slope * (train_frac - 0.5))\n\n    # Adjust the weights based on the annealing factor\n    w_annealed = w * anneal_factor\n\n    # Ensure stability in the computation by handling cases where adjacent intervals have zero distance\n    w_annealed = torch.where(t[..., 1:] - t[..., :-1] == 0, torch.zeros_like(w_annealed), w_annealed)\n\n    # Prevent NaN values by using a softmax operation on the adjusted weights\n    w_annealed = torch.softmax(w_annealed, dim=-1)\n\n    # Normalize the weights to sum to 1\n    w_annealed /= torch.sum(w_annealed, dim=-1, keepdim=True).clip(eps)\n\n    return w_annealed"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n\n    # Calculate the annealing factor\n    anneal_factor = torch.sigmoid(anneal_slope * (1 - train_frac))\n\n    # Adjust the weights by the annealing factor\n    w_annealed = w * anneal_factor\n\n    # Ensure stability in the computation by handling cases where adjacent intervals have zero distance\n    w_annealed = torch.where(t[..., 1:] - t[..., :-1] == 0, torch.zeros_like(w_annealed), w_annealed)\n\n    # Prevent NaN values by using a softmax operation on the adjusted weights\n    w_annealed = torch.softmax(w_annealed, dim=-1)\n\n    return t, w_annealed"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    w = w.clip(eps)\n\n    # Calculate the bias factor based on the training fraction and annealing slope\n    bias = torch.sigmoid(anneal_slope * (train_frac - 0.5))\n\n    # Adjust the weights using the bias factor\n    w_adjusted = bias * w\n\n    # Normalize the adjusted weights to sum to 1\n    w_adjusted = w_adjusted / torch.sum(w_adjusted, dim=-1, keepdim=True).clip(eps)\n\n    return w_adjusted"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n\n    # Adjust the weights based on the training fraction and annealing slope using Schlick's bias function.\n    w_adjusted = w * (1 - torch.sigmoid(anneal_slope * (1 - train_frac)))\n\n    # Prevent NaN values by using a softmax operation on the adjusted weights.\n    w_adjusted = torch.softmax(w_adjusted, dim=-1)\n\n    # Handle cases where adjacent intervals have zero distance, setting their weight to zero.\n    w_adjusted = torch.where(torch.abs(t[..., 1:] - t[..., :-1]) > eps, w_adjusted, 0)\n\n    return t, w_adjusted"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n\n    # Calculate the annealing factor using Schlick's bias function\n    anneal_factor = torch.clamp(1 - torch.exp(-anneal_slope * train_frac), min=0, max=1)\n\n    # Adjust the weights using the annealing factor\n    adjusted_weights = w * anneal_factor\n\n    # Ensure stability in the computation by handling cases where adjacent intervals have zero distance\n    # and setting their weight to zero\n    adjusted_weights = torch.where(torch.abs(t[..., 1:] - t[..., :-1]) < eps, 0, adjusted_weights)\n\n    # Normalize the adjusted weights using a softmax operation\n    normalized_weights = torch.softmax(adjusted_weights, dim=-1)\n\n    return normalized_weights"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    t = t[..., :-1]\n    w = w[..., :-1]\n    w_anneal = torch.max(w, eps)\n    w_anneal = torch.exp(-anneal_slope * (1 - train_frac) * (t - t[..., 0]) + torch.log(w_anneal))\n    w_anneal = w_anneal / torch.sum(w_anneal, dim=-1, keepdim=True).clip(eps)\n    return w_anneal"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    # Ensure that adjacent intervals have nonzero distance.\n    t = torch.cat([t[..., :1], t[..., :-1]], dim=-1)\n    w = torch.cat([w[..., :1], w[..., :-1]], dim=-1)\n    # Ensure that the weights are nonnegative.\n    w = w.clip(min=0)\n    # Adjust the weights using Schlick's bias function.\n    w_adjusted = w * (1 - torch.exp(-anneal_slope * train_frac))\n    # Normalize the adjusted weights to sum to 1.\n    w_adjusted = w_adjusted / torch.sum(w_adjusted, dim=-1, keepdim=True).clip(eps)\n    return t, w_adjusted\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # assuming sampling in s space\n    if t.shape[-1] != w.shape[-1] + 1:\n        t = torch.cat([t, torch.ones_like(t[..., -1:])], dim=-1)\n\n    # calculate bias\n    bias = torch.sigmoid(anneal_slope * (1 - train_frac))\n\n    # calculate annealed weights\n    w_annealed = w * bias\n\n    # ensure stability by handling cases where adjacent intervals have zero distance\n    w_annealed = torch.where(t[..., 1:] - t[..., :-1] == 0, torch.zeros_like(w_annealed), w_annealed)\n\n    # prevent NaN values by using a softmax operation on the adjusted weights\n    w_annealed = torch.nn.functional.softmax(w_annealed, dim=-1)\n\n    # renormalize weights\n    w_annealed = w_annealed / torch.sum(w_annealed, dim=-1, keepdim=True).clip(eps)\n\n    return w_annealed"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    t = t[..., :-1]\n\n    # Calculate the bias based on the training fraction and anneal slope\n    bias = torch.sigmoid(anneal_slope * (1 - train_frac))\n\n    # Adjust the weights using the bias\n    adjusted_weights = torch.where(\n        t[..., 1:] - t[..., :-1] == 0,\n        torch.zeros_like(t[..., 1:] - t[..., :-1]),\n        w * bias\n    )\n\n    # Ensure stability by using a softmax operation on the adjusted weights\n    adjusted_weights = torch.softmax(adjusted_weights, dim=-1)\n\n    # Return the adjusted weights\n    return adjusted_weights"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    t = t[..., :-1]\n\n    # Compute the bias based on the training fraction and annealing slope\n    bias = torch.exp(-anneal_slope * train_frac)\n\n    # Apply the bias to the weights\n    w_annealed = torch.softmax(w * bias, dim=-1)\n\n    # Ensure that adjacent intervals have non-zero distance\n    w_annealed = torch.where(torch.abs(t[..., 1:] - t[..., :-1]) > eps, w_annealed, 0)\n\n    return w_annealed"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    \"\"\"\n    This function anneals the weights based on the training fraction and an annealing slope using Schlick's bias function. It adjusts the weights of a tensor based on the progression of training, making the weight adjustment more dynamic as training progresses. It ensures stability in the computation by handling cases where adjacent intervals have zero distance, setting their weight to zero, and preventing NaN values by using a softmax operation on the adjusted weights.\n\n    Input-Output Arguments\n    :param t: torch.Tensor. The tensor representing time or another sequential dimension, used to align with the weights tensor.\n    :param w: torch.Tensor. The weights tensor that will be adjusted based on the training fraction and anneal slope.\n    :param train_frac: float. The fraction of training completed, used to calculate the annealing effect on weights.\n    :param anneal_slope: float, optional. The slope of the annealing function, determining how sharply weights are adjusted. Defaults to 10.0.\n    :param eps: torch.float32, optional. A very small number added to prevent division by zero and log of zero in computations. Defaults to a small epsilon value squared.\n    :return: torch.Tensor. The adjusted weights tensor after applying the annealing process.\n    \"\"\"\n    # t, w = matchup_channels(t, w)\n\n    # Calculate the annealing factor based on the training fraction and anneal slope\n    # anneal_factor = torch.sigmoid(anneal_slope * (train_frac - 0.5))\n    anneal_factor = torch.sigmoid(anneal_slope * (train_frac - 0.5))\n\n    # Calculate the annealed weights using Schlick's bias function\n    # w_annealed = w * (anneal_factor + (1 - anneal_factor) * torch.exp(-torch.log(w + eps) / (anneal_slope * train_frac)))\n    w_annealed = w"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n\n    # Calculate the bias factor using Schlick's bias function\n    bias = torch.maximum(\n        torch.tensor(1.0 - train_frac, device=t.device, dtype=t.dtype),\n        torch.tensor(0.0, device=t.device, dtype=t.dtype)\n    )\n    bias = torch.pow(bias, anneal_slope)\n\n    # Adjust the weights based on the bias factor\n    w_adj = torch.maximum(w * bias, torch.tensor(0.0, device=t.device, dtype=t.dtype))\n\n    # Normalize the adjusted weights using softmax\n    w_adj = torch.softmax(w_adj, dim=-1)\n\n    # Ensure that the weights sum to 1\n    w_adj /= torch.sum(w_adj, dim=-1, keepdim=True).clip(eps)\n\n    return w_adj"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # Check if the input tensors have the same shape\n    assert t.shape == w.shape, \"t and w must have the same shape\"\n\n    # Check if the last dimension of t is one more than the last dimension of w\n    assert t.shape[-1] == w.shape[-1] + 1, \"The last dimension of t must be one more than the last dimension of w\"\n\n    # Check if the last dimension of t is greater than 1\n    assert t.shape[-1] > 1, \"The last dimension of t must be greater than 1\"\n\n    # Check if the last dimension of w is greater than 0\n    assert w.shape[-1] > 0, \"The last dimension of w must be greater than 0\"\n\n    # Check if the last dimension of t is monotonically increasing\n    assert torch.all(torch.diff(t[..., :-1]) >= 0), \"The last dimension of t must be monotonically increasing\"\n\n    # Check if the last dimension of w is non-negative\n    assert torch.all(w >= 0), \"The last dimension of w must be non-negative\"\n\n    # Check if the last dimension of t is within the domain\n    assert torch.all(t[..., 0] <= t[..., -1]), \"The last dimension of t must be within the domain\"\n\n    # Check if the last dimension of w is within the domain\n    assert torch.all(w[..., 0] <= w[..., -1]), \"The last dimension of w must be within the domain\"\n\n    # Check if the last dimension of t is within the domain\n    assert torch.all(t[..., 0] <= t[..., -1]), \"The last dimension of t must be within the domain\"\n\n    # Check if the last dimension of w is within the domain\n    assert torch.all(w[..., 0] <= w[..., -1]), \"The last dimension of w must be within the domain\"\n\n    # Check if the last dimension of t is within the domain\n    assert torch.all(t[..., 0] <= t"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    # Ensure that the last bin has zero weight.\n    w = w.clip(min=0.0)\n    w_sum = torch.sum(w, dim=-1, keepdim=True)\n    w_sum = w_sum.clip(min=eps)\n    w_sum_inv = 1 / w_sum\n    w_sum_inv = w_sum_inv.clip(min=eps)\n    w_sum_inv_sq = w_sum_inv ** 2\n    w_sum_inv_sq = w_sum_inv_sq.clip(min=eps)\n    w_sum_inv_sq_m1 = w_sum_inv_sq - 1\n    w_sum_inv_sq_m1 = w_sum_inv_sq_m1.clip(min=eps)\n    w_sum_inv_sq_m1_slope = w_sum_inv_sq_m1 * anneal_slope\n    w_sum_inv_sq_m1_slope = w_sum_inv_sq_m1_slope.clip(min=eps)\n    w_sum_inv_sq_m1_slope_sq = w_sum_inv_sq_m1_slope ** 2\n    w_sum_inv_sq_m1_slope_sq = w_sum_inv_sq_m1_slope_sq.clip(min=eps)\n    w_sum_inv_sq_m1_slope_sq_m1 = w_sum_inv_sq_m1_slope_sq - 1\n    w_sum_inv_sq_m1_slope_sq_m1 = w_sum_inv_sq_m1_slope_sq_m1.clip(min=eps)\n    w_sum_inv_sq_m1_slope_sq_m1_slope = w_sum_inv_sq_m1_slope_sq_m1 * anneal_slope\n    w_sum_inv_sq_m1_"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n\n    # Ensure that adjacent intervals have nonzero distance.\n    t = torch.cat([t, t.new_zeros(t.shape[:-1] + (1,))], dim=-1)\n    w = torch.cat([w, torch.zeros_like(w[..., :1])], dim=-1)\n\n    # Calculate the annealing bias.\n    bias = torch.sigmoid(anneal_slope * (1 - train_frac))\n\n    # Adjust the weights using the annealing bias.\n    adjusted_weights = torch.softmax(w * bias, dim=-1)\n\n    # Handle cases where adjacent intervals have zero distance.\n    adjusted_weights = torch.where(\n        (t[..., 1:] - t[..., :-1]) == 0,\n        torch.zeros_like(adjusted_weights),\n        adjusted_weights\n    )\n\n    # Prevent NaN values due to log of zero.\n    adjusted_weights = torch.where(\n        adjusted_weights == 0,\n        torch.ones_like(adjusted_weights) * eps,\n        adjusted_weights\n    )\n\n    return adjusted_weights"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    else:  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    else:  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    else:  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    else:  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    else:  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    else:  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    else:  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    else:\n        pass  # do nothing here, used for typed in to_cuda for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    else:\n        pass  # do nothing here, used for typed in to_cuda for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    else:  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    else:  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        if not ignore_list:\n            batch = batch.to(device, non_blocking=True)\n    else:  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    # if isinstance(batch, (tuple, list)):\n    #     batch = [to_cuda(b, device, ignore_list) for b in batch]\n    # elif isinstance(batch, dict):\n    #     batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    # elif isinstance(batch, torch.Tensor):\n    #     batch = batch.to(device, non_blocking=True)\n    # elif isinstance(batch, np.ndarray):  # numpy and others\n    #     batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    # else:\n    #     pass  # do nothing here, used for typed in to_x for methods\n    #     # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    # return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    elif isinstance(batch, np.ndarray):\n        batch = torch.from_numpy(batch).to(device, non_blocking=True)\n    else:\n        pass  # do nothing here, used for typed in to_cuda for methods\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        if not ignore_list:\n            batch = batch.to(device, non_blocking=True)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        if not ignore_list:\n            batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    else:\n        pass  # do nothing here, used for typed in to_cuda for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    else:  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    # if isinstance(batch, (tuple, list)):\n    #     batch = [to_cuda(b, device, ignore_list) for b in batch]\n    # elif isinstance(batch, dict):\n    #     batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    # elif isinstance(batch, torch.Tensor):\n    #     batch = batch.to(device, non_blocking=True)\n    # else:  # numpy and others\n    #     batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    # return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    # if isinstance(batch, (tuple, list)):\n    #     batch = [to_cuda(b, device, ignore_list) for b in batch]\n    # elif isinstance(batch, dict):\n    #     batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    # elif isinstance(batch, torch.Tensor):\n    #     batch = batch.to(device, non_blocking=True)\n    # elif isinstance(batch, np.ndarray):  # numpy and others\n    #     batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    # else:\n    #     pass  # do nothing here, used for typed in to_x for methods\n    #     # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    # return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    # if isinstance(batch, (tuple, list)):\n    #     batch = [to_cuda(b, device, ignore_list) for b in batch]\n    # elif isinstance(batch, dict):\n    #     batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    # elif isinstance(batch, torch.Tensor):\n    #     batch = batch.to(device, non_blocking=True)\n    # else:  # numpy and others\n    #     batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    # return batch\n\n    if isinstance(batch, (tuple, list)):\n        batch = [to_x(b, device) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_x(v, device) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # Gather the value at the -2th dim of values, augment index shape on the back\n    # Example: values: B, P, 3, index: B, N, -> B, N, 3\n\n    # index will first be augmented to match the values' dimentionality at the back\n    # take care of batch dimension of, and acts like a linear indexing in the target dimention\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    # we assume that the values's"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # v: B, P, 3\n    # f: B, F, 3\n\n    # v: B, P, 3\n    # f: B, F, 3\n\n    # v: B, P, 3\n    # f: B, F, 3\n\n    # v: B, P, 3\n    # f: B, F, 3\n\n    # v: B, P, 3\n    # f: B, F, 3\n\n    # v: B, P, 3\n    # f: B, F, 3\n\n    # v: B, P, 3\n    # f: B, F, 3\n\n    # v: B, P, 3\n    # f: B, F, 3\n\n    # v: B, P, 3\n    # f: B, F, 3\n\n    # v: B, P, 3\n    # f: B, F, 3\n\n    # v: B, P, 3\n    # f: B, F, 3\n\n    # v: B, P, 3\n    # f: B, F, 3\n\n    # v: B, P, 3\n    # f: B, F, 3\n\n    # v: B, P, 3\n    # f: B, F, 3\n\n    # v: B, P, 3\n    # f: B, F, 3\n\n    # v: B, P, 3\n    # f: B, F, 3\n\n    # v: B, P, 3\n    # f: B, F, 3\n\n    # v: B, P, 3\n    # f: B, F, 3\n\n    # v: B, P, 3\n    # f: B, F, 3\n\n    # v: B, P, 3\n    # f: B, F, 3\n\n    # v: B, P, 3\n    # f: B, F, 3\n\n    # v: B, P, 3\n    # f: B"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # v: B, P, 3, f: B, F, 3 -> B, F, 3, 3\n    # index will first be augmented to match the values' dimentionality at the back\n    # take care of batch dimension of, and acts like a linear indexing in the target dimention\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    # we also assume that the values's last dimension is the dimension to be gathered from\n    # we also assume that the values's last dimension is the dimension to be gathered from\n    # we also assume that the values's last dimension is the dimension to be gathered from\n    # we also assume that the values's last dimension is the dimension to be gathered from\n    # we also assume that the values's last dimension is the dimension to be gathered from\n    # we also assume that the values's last dimension is the dimension to be gathered from\n    # we also assume that the values's last dimension is the dimension to be gathered from\n    # we also assume that the values's last dimension is the dimension to be gathered from\n    # we also assume that the values's last dimension is the dimension to be gathered from\n    # we also assume that the values's last dimension is the dimension to be gathered from\n    # we also assume that the values's last dimension is the dimension to be gathered from\n    # we also assume that the values's last dimension is the dimension to be gathered from\n    # we also assume that the values's last dimension is the dimension to be gathered from\n    # we also assume that the values's last dimension is the dimension to be gathered from\n    # we also assume that the values's last dimension is the dimension to be gathered from\n    # we also assume that the values's last dimension is the dimension to be gathered from\n    # we also assume that the values's last dimension is the dimension to be gathered from\n    # we also assume that the values's last dimension is the dimension to be gathered from\n    # we also assume that the values's last dimension is the dimension to be gathered from\n    # we also assume that the values's last dimension is the dimension to be gathered from\n    # we"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # if v.ndim == 3: v = v[:, None, :, :]\n    # if f.ndim == 2: f = f[:, None, :]\n    # if f.shape[-1] == 3: f = f[:, :, :, None]\n    # if f.shape[-1] == 4: f = f[:, :, :, :, None]\n    # if f.shape[-1] == 5: f = f[:, :, :, :, :, None]\n    # if f.shape[-1] == 6: f = f[:, :, :, :, :, :, None]\n    # if f.shape[-1] == 7: f = f[:, :, :, :, :, :, :, None]\n    # if f.shape[-1] == 8: f = f[:, :, :, :, :, :, :, :, None]\n    # if f.shape[-1] == 9: f = f[:, :, :, :, :, :, :, :, :, None]\n    # if f.shape[-1] == 10: f = f[:, :, :, :, :, :, :, :, :, :, None]\n    # if f.shape[-1] == 11: f = f[:, :, :, :, :, :, :, :, :, :, :, None]\n    # if f.shape[-1] == 12: f = f[:, :, :, :, :, :, :, :, :, :, :, :, None]\n    # if f.shape[-1] == 13: f = f[:, :, :, :, :, :, :, :, :, :, :, :, :, None]\n    # if f.shape[-1] == 14: f = f[:, :, :, :, :, :, :, :, :, :, :, :, :, :, None]\n    # if f.shape[-1] == 15: f = f[:, :, :, :, :, :, :, :,"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # f: B, N, 3\n    # v: B, P, 3\n    # return: B, N, 3, 3\n\n    # f = f.unsqueeze(-1).expand(-1, -1, 3)  # B, N, 3\n    # f = f.permute(0, 2, 1)  # B, 3, N\n    # v = v.unsqueeze(1).expand(-1, f.shape[1], -1, -1)  # B, 3, N, 3\n    # v = v.permute(0, 2, 1, 3)  # B, N, 3, 3\n    # return v - f  # B, N, 3, 3\n\n    # f: B, N, 3\n    # v: B, P, 3\n    # return: B, N, 3, 3\n\n    # f = f.unsqueeze(-1).expand(-1, -1, 3)  # B, N, 3\n    # f = f.permute(0, 2, 1)  # B, 3, N\n    # v = v.unsqueeze(1).expand(-1, f.shape[1], -1, -1)  # B, 3, N, 3\n    # v = v.permute(0, 2, 1, 3)  # B, N, 3, 3\n    # return v - f  # B, N, 3, 3\n\n    # f: B, N, 3\n    # v: B, P, 3\n    # return: B, N, 3, 3\n\n    # f = f.unsqueeze(-1).expand(-1, -1, 3)  # B, N, 3\n    # f = f.permute(0, 2, 1)  # B, 3, N\n    # v = v.unsqueeze(1).expand(-1, f.shape[1], -1"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # v: B, P, 3\n    # f: B, F, 3\n    # out: B, F, 3, 3\n    # f: B, F, 3 -> B, F, 3, 3\n    # v: B, P, 3 -> B, F, 3, 3, 3\n    # f: B, F, 3, 3, 3 -> B, F, 3, 3\n    # v: B, F, 3, 3 -> B, F, 3, 3\n    # v: B, F, 3, 3 -> B, F, 3, 3, 3 -> B, F, 3, 3\n    # v: B, F, 3, 3 -> B, F, 3, 3, 3 -> B, F, 3, 3\n    # v: B, F, 3, 3 -> B, F, 3, 3, 3 -> B, F, 3, 3\n    # v: B, F, 3, 3 -> B, F, 3, 3, 3 -> B, F, 3, 3\n    # v: B, F, 3, 3 -> B, F, 3, 3, 3 -> B, F, 3, 3\n    # v: B, F, 3, 3 -> B, F, 3, 3, 3 -> B, F, 3, 3\n    # v: B, F, 3, 3 -> B, F, 3, 3, 3 -> B, F, 3, 3\n    # v: B, F, 3, 3 -> B, F, 3, 3, 3 -> B, F, 3, 3\n    # v: B, F, 3, 3 -> B, F, 3, 3, 3 -> B, F, 3, 3\n    # v: B, F, 3, "}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # expand the faces tensor to match the batch dimension of the vertices tensor\n    if v.ndim != f.ndim:\n        f = f[(slice(None),) * (v.ndim - f.ndim)]\n    # reshape the faces tensor to maintain the original structure with additional dimensions for batch processing\n    f = f.reshape(-1, 3, *f.shape[dim + 1:])\n    # gather the vertices corresponding to the faces\n    return multi_gather(v, f, dim)\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # expand the faces tensor to match the batch dimension of the vertices tensor\n    f = f.expand(v.shape[0], *f.shape[1:]) if f.shape[0] != v.shape[0] else f\n\n    # gather the vertices corresponding to the faces\n    v1 = multi_gather(v, f[:, 0], dim)\n    v2 = multi_gather(v, f[:, 1], dim)\n    v3 = multi_gather(v, f[:, 2], dim)\n\n    # compute the face normals\n    n = torch.cross(v2 - v1, v3 - v1, dim=dim)\n    n = n / (torch.norm(n, dim=dim, keepdim=True) + 1e-8)  # normalize the normals\n\n    # reshape the result to maintain the original faces tensor structure\n    n = n.reshape(*f.shape[:-1], 3)\n\n    return n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # check if f is a batch of faces\n    if f.ndim == v.ndim:\n        f = f.unsqueeze(dim)\n    # check if f is a batch of vertices\n    if f.ndim == v.ndim + 1:\n        f = f.unsqueeze(-1)\n    # check if f is a batch of faces and vertices\n    if f.ndim == v.ndim + 2:\n        f = f.unsqueeze(-2)\n    # check if f is a batch of faces and vertices\n    if f.ndim == v.ndim + 3:\n        f = f.unsqueeze(-3)\n    # check if f is a batch of faces and vertices\n    if f.ndim == v.ndim + 4:\n        f = f.unsqueeze(-4)\n\n    # check if f is a batch of faces and vertices\n    if f.ndim == v.ndim + 5:\n        f = f.unsqueeze(-5)\n    # check if f is a batch of faces and vertices\n    if f.ndim == v.ndim + 6:\n        f = f.unsqueeze(-6)\n    # check if f is a batch of faces and vertices\n    if f.ndim == v.ndim + 7:\n        f = f.unsqueeze(-7)\n    # check if f is a batch of faces and vertices\n    if f.ndim == v.ndim + 8:\n        f = f.unsqueeze(-8)\n    # check if f is a batch of faces and vertices\n    if f.ndim == v.ndim + 9:\n        f = f.unsqueeze(-9)\n    # check if f is a batch of faces and vertices\n    if f.ndim == v.ndim + 10:\n        f = f.unsqueeze(-10)\n    # check if f is a batch of faces and vertices\n    if f.ndim == v.ndim + 11:\n        f = f.unsqueeze(-11)\n    # check if f is a batch"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # f: B, P, 3\n    # v: B, N, 3\n    # return: B, P, 3, 3\n    # f = f.view(-1, 3)\n    # v = v.view(-1, 3)\n    f = multi_indexing(f, v.shape, dim)\n    v = v.view(-1, 3)\n    return v[f].view(*f.shape, 3, 3)\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # Adjust the dimensions of the faces tensor to match the batch dimension of the vertices tensor\n    if v.ndim > f.ndim:\n        f = f[(slice(None),) * (v.ndim - f.ndim)]\n    # Gather the vertices at the specified indices\n    v1 = multi_gather(v, f[:, 0], dim)\n    v2 = multi_gather(v, f[:, 1], dim)\n    v3 = multi_gather(v, f[:, 2], dim)\n    # Compute the normals of the triangles\n    v12 = v2 - v1\n    v13 = v3 - v1\n    n = torch.cross(v12, v13, dim=dim)\n    # Reshape the result to maintain the original faces tensor structure\n    n = n.reshape(*f.shape[:dim], 3, *f.shape[dim + 1:])\n    return n\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # Adjust the dimensions of the faces tensor to match the batch dimension of the vertices tensor\n    if f.ndim < v.ndim:\n        f = f.expand(*(v.shape[:-2] + f.shape))\n\n    # Gather the vertices corresponding to the faces\n    f = multi_indexing(f, v.shape, dim)\n    f = f.reshape(-1, 3, 3)\n    v = v.reshape(-1, 3)\n\n    # Compute the normals of the faces\n    normals = torch.cross(f[:, 1] - f[:, 0], f[:, 2] - f[:, 0], dim=-1)\n\n    # Reshape the result to maintain the original faces tensor structure with additional dimensions for batch processing\n    normals = normals.reshape(*(f.shape[:-2] + normals.shape[1:]))\n    return normals"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # dim: in terms of the index (mask)\n    # f: B, N, 3\n    # v: B, P, 3\n    # return: B, N, 3, 3\n    # 1. expand f to match v's dim\n    # 2. gather v with f\n    # 3. reshape to maintain the original f's structure\n    # 4. return\n    f = f.expand(*v.shape[:-1], -1)\n    f = multi_gather(v, f, dim)  # B, N, 3, 3\n    f = f.reshape(*f.shape[:-2], -1)  # B, N, 9\n    return f"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # expand f to match the batch dimension of v\n    if f.ndim < v.ndim: f = f.expand(v.shape[:-1])\n    # gather the vertices that form the faces\n    v1 = multi_gather(v, f[..., 0], dim=dim)\n    v2 = multi_gather(v, f[..., 1], dim=dim)\n    v3 = multi_gather(v, f[..., 2], dim=dim)\n    # compute the cross product of the vectors v2-v1 and v3-v1\n    e1 = v2 - v1\n    e2 = v3 - v1\n    n = torch.cross(e1, e2, dim=dim)\n    # normalize the normals\n    n = n / torch.norm(n, dim=dim, keepdim=True)\n    # reshape the result to maintain the original faces tensor structure\n    return n.reshape(*f.shape[:-1], *n.shape[dim + 1:])"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust the shape of the faces tensor to match the batch dimension of the vertices tensor\n    if f.ndim > v.ndim:\n        f = f[..., :v.shape[dim]]\n    elif f.ndim < v.ndim:\n        f = f.expand(v.shape[0], *f.shape)\n\n    # gather the vertices corresponding to the face indices\n    v0 = multi_gather(v, f[:, 0], dim)\n    v1 = multi_gather(v, f[:, 1], dim)\n    v2 = multi_gather(v, f[:, 2], dim)\n\n    # compute the face normals\n    n = torch.cross(v1 - v0, v2 - v0, dim=dim)\n    n = n / (torch.norm(n, dim=dim, keepdim=True) + 1e-6)\n\n    # reshape the result to maintain the original faces tensor structure with additional dimensions for batch processing\n    return n.reshape(*f.shape, *n.shape[1:])"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # v: B, P, 3\n    # f: B, F, 3\n    # return: B, F, 3, 3\n    # we will expand f to match the batch dimension of v\n    # then we will gather the vertices with the index of f\n    # then we will reshape the result to maintain the original structure of f\n\n    # expand f to match the batch dimension of v\n    f = f.expand(v.shape[0], *f.shape[1:])\n    # gather the vertices with the index of f\n    v = v.gather(dim, f)\n    # reshape the result to maintain the original structure of f\n    v = v.reshape(v.shape[0], *f.shape, v.shape[-1])\n    return v\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # f = f.expand(v.shape[0], -1, -1)\n    f = f.expand(v.shape[0], *f.shape[1:])\n    f = f.reshape(v.shape[0], -1, 3)\n    # f = f.reshape(-1, 3)\n    # v = v.reshape(-1, 3)\n    v = v.reshape(v.shape[0], -1, 3)\n    v1, v2, v3 = multi_gather(v, f, dim=dim)\n    v1 = v1 - v3\n    v2 = v2 - v3\n    n = torch.cross(v1, v2, dim=-1)\n    n = n / torch.norm(n, dim=-1, keepdim=True)\n    return n\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust the shape of the faces tensor to match the batch dimension of the vertices tensor\n    if v.shape[0] != f.shape[0]:\n        # if the batch dimension of the vertices tensor is not the same as the faces tensor, expand the faces tensor to match the batch dimension\n        f = f.expand(v.shape[0], *f.shape[1:])\n\n    # gather the vertices at the specified indices in the faces tensor\n    v1 = multi_gather(v, f[:, 0], dim)  # B, P, 3\n    v2 = multi_gather(v, f[:, 1], dim)\n    v3 = multi_gather(v, f[:, 2], dim)\n\n    # compute the normals of the triangles using the gathered vertices\n    n = torch.cross(v2 - v1, v3 - v1, dim=dim)  # B, P, 3\n    n = n / (torch.norm(n, dim=dim, keepdim=True) + 1e-8)  # B, P, 3\n\n    # reshape the result to maintain the original faces tensor structure with additional dimensions for batch processing\n    return n.reshape(f.shape)\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # assert v.shape[0] == f.shape[0]\n    # assert f.ndim == v.ndim\n    # assert f.shape[-1] == 3\n    # assert dim == -2\n    # assert v.ndim >= 3\n\n    # if f.shape[0] != v.shape[0]:\n    #     # expand the faces to match the batch dimension of the vertices\n    #     f = f.expand(v.shape[0], -1, -1)\n\n    # # reshape the faces tensor to have the same batch dimension as the vertices tensor\n    # f = f.reshape(-1, *f.shape[2:])\n\n    # # gather the vertices corresponding to the faces\n    # v = v.reshape(-1, *v.shape[2:])\n    # v = v.gather(dim, f)\n\n    # # reshape the vertices tensor back to the original shape\n    # v = v.reshape(*f.shape[:-1], 3, *v.shape[1:])\n\n    # # compute the normals of the faces\n    # v1 = v[:, :, 1] - v[:, :, 0]\n    # v2 = v[:, :, 2] - v[:, :, 0]\n    # normals = torch.cross(v1, v2)\n\n    # # reshape the normals tensor back to the original shape\n    # normals = normals.reshape(*f.shape[:-1], *normals.shape[1:])\n\n    # return normals\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # v: B, P, 3\n    # f: B, F, 3\n    # return: B, F, 3, 3\n    # compute faces normals w.r.t the vertices (considering batch dimension)\n\n    # expand faces to match the batch dimension of the vertices\n    if f.ndim > v.ndim:\n        for _ in range(f.ndim - v.ndim):\n            f = f.unsqueeze(0)\n    # broadcast faces to match the vertices' shape\n    if f.shape[dim] != v.shape[dim]:\n        f = f.expand(*v.shape[:dim], f.shape[dim], *v.shape[dim + 1:])\n    # gather vertices\n    v1 = multi_gather(v, f[:, :, 0], dim)\n    v2 = multi_gather(v, f[:, :, 1], dim)\n    v3 = multi_gather(v, f[:, :, 2], dim)\n    # compute face normals\n    e1 = v2 - v1\n    e2 = v3 - v1\n    n = torch.cross(e1, e2, dim=dim)\n    n = n / torch.norm(n, dim=dim, keepdim=True)\n    # return face normals\n    return n\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None]\n    else:  # numpy and others\n        batch = np.array([batch])\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None]\n    else:  # numpy and others\n        batch = np.array(batch)[None]\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, (torch.Tensor, np.ndarray)):  # numpy and others\n        batch = batch[None]\n    else:\n        batch = torch.as_tensor(batch)[None]\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None]\n    else:  # numpy and others\n        batch = np.array([batch])\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None]\n    else:  # numpy and others\n        batch = np.array([batch])\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None]\n    else:  # numpy and others\n        batch = np.expand_dims(batch, axis=0)\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None]\n    else:  # numpy and others\n        batch = np.expand_dims(batch, 0)\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, (torch.Tensor, np.ndarray)):  # numpy and others\n        batch = batch[None]\n    else:\n        batch = torch.as_tensor(batch)[None]\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, (torch.Tensor, np.ndarray)):  # numpy and others\n        batch = np.expand_dims(batch, axis=0)\n    else:\n        batch = torch.as_tensor(batch)[None]\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, (torch.Tensor, np.ndarray)):  # numpy and others\n        batch = batch[None]\n    else:\n        batch = torch.as_tensor(batch)\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, (torch.Tensor, np.ndarray)):  # numpy and others\n        batch = batch[None]\n    else:\n        batch = torch.as_tensor(batch)\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None]\n    else:  # numpy and others\n        batch = np.array(batch)[None]\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None]\n    else:  # numpy and others\n        batch = np.array(batch)[None]\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, (torch.Tensor, np.ndarray)):  # numpy and others\n        batch = batch[None]\n    else:\n        batch = torch.as_tensor(batch)[None]\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, (torch.Tensor, np.ndarray)):  # numpy and others\n        batch = np.expand_dims(batch, axis=0)\n    else:\n        batch = torch.as_tensor(batch)\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = batch[None]\n    else:\n        batch = torch.as_tensor(batch)[None]\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.unsqueeze(0)\n    else:  # numpy and others\n        batch = np.expand_dims(batch, 0)\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, (torch.Tensor, np.ndarray)):  # numpy and others\n        batch = np.expand_dims(batch, axis=0)\n    else:\n        batch = np.expand_dims(np.array(batch), axis=0)\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.unsqueeze(0)\n    else:  # numpy and others\n        batch = np.expand_dims(batch, 0)\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None]\n    else:  # numpy and others\n        batch = np.array([batch])\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H = self.H\n        batch.W = self.W\n        batch.K = self.K\n        batch.R = self.R\n        batch.T = self.T\n        batch.n = self.n\n        batch.f = self.f\n        batch.t = self.t\n        batch.v = self.v\n        batch.bounds = self.bounds\n\n        batch.meta = dotdict()\n        batch.meta.H = self.H\n        batch.meta.W = self.W\n        batch.meta.K = self.K\n        batch.meta.R = self.R\n        batch.meta.T = self.T\n        batch.meta.n = self.n\n        batch.meta.f = self.f\n        batch.meta.t = self.t\n        batch.meta.v = self.v\n        batch.meta.bounds = self.bounds\n        return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        batch.meta = dotdict(batch)\n        return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H = self.H\n        batch.W = self.W\n        batch.K = self.K\n        batch.R = self.R\n        batch.T = self.T\n        batch.n = self.n\n        batch.f = self.f\n        batch.t = self.t\n        batch.v = self.v\n        batch.bounds = self.bounds\n\n        batch.mass = self.mass\n        batch.moment_of_inertia = self.moment_of_inertia\n        batch.movement_force = self.movement_force\n        batch.movement_torque = self.movement_torque\n        batch.movement_speed = self.movement_speed\n        batch.origin = self.origin\n        batch.world_up = self.world_up\n\n        batch.meta = dotdict(batch)\n        return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H = self.H\n        batch.W = self.W\n        batch.K = self.K\n        batch.R = self.R\n        batch.T = self.T\n        batch.n = self.n\n        batch.f = self.f\n        batch.t = self.t\n        batch.v = self.v\n        batch.bounds = self.bounds\n\n        batch.mass = self.mass\n        batch.moment_of_inertia = self.moment_of_inertia\n        batch.movement_force = self.movement_force\n        batch.movement_torque = self.movement_torque\n        batch.movement_speed = self.movement_speed\n        batch.origin = self.origin\n        batch.world_up = self.world_up\n\n        batch.meta = dotdict(batch)\n        return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        batch.meta = dotdict()\n        batch.meta.H, batch.meta.W, batch.meta.K, batch.meta.R, batch.meta.T, batch.meta.n, batch.meta.f, batch.meta.t, batch.meta.v, batch.meta.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        batch.meta.origin = self.origin\n        batch.meta.world_up = self.world_up\n        batch.meta.movement_speed = self.movement_speed\n        batch.meta.movement_force = self.movement_force\n        batch.meta.drag_coeff_mult = self.drag_coeff_mult\n        batch.meta.constant_drag = self.constant_drag\n        batch.meta.mass = self.mass\n        batch.meta.moment_of_inertia = self.moment_of_inertia\n        batch.meta.movement_torque = self.movement_torque\n        batch.meta.angular_friction = self.angular_friction\n        batch.meta.constant_torque = self.constant_torque\n        batch.meta.min_interval = self.min_interval\n        batch.meta.pause_physics = self.pause_physics\n        return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H = self.H\n        batch.W = self.W\n        batch.K = self.K\n        batch.R = self.R\n        batch.T = self.T\n        batch.n = self.n\n        batch.f = self.f\n        batch.t = self.t\n        batch.v = self.v\n        batch.bounds = self.bounds\n\n        batch.mass = self.mass\n        batch.moment_of_inertia = self.moment_of_inertia\n        batch.movement_force = self.movement_force\n        batch.movement_torque = self.movement_torque\n        batch.movement_speed = self.movement_speed\n        batch.origin = self.origin\n        batch.world_up = self.world_up\n\n        batch.meta = batch\n        return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H = self.H\n        batch.W = self.W\n        batch.K = self.K\n        batch.R = self.R\n        batch.T = self.T\n        batch.n = self.n\n        batch.f = self.f\n        batch.t = self.t\n        batch.v = self.v\n        batch.bounds = self.bounds\n\n        batch.mass = self.mass\n        batch.moment_of_inertia = self.moment_of_inertia\n        batch.movement_force = self.movement_force\n        batch.movement_torque = self.movement_torque\n        batch.movement_speed = self.movement_speed\n        batch.origin = self.origin\n        batch.world_up = self.world_up\n\n        batch.meta = batch\n        return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Batch (network input parameters)\n        batch = dotdict()\n        batch.H = torch.as_tensor(self.H, dtype=torch.float)\n        batch.W = torch.as_tensor(self.W, dtype=torch.float)\n        batch.K = torch.as_tensor(self.K, dtype=torch.float)\n        batch.R = torch.as_tensor(self.R, dtype=torch.float)\n        batch.T = torch.as_tensor(self.T, dtype=torch.float)\n        batch.n = torch.as_tensor(self.n, dtype=torch.float)\n        batch.f = torch.as_tensor(self.f, dtype=torch.float)\n        batch.t = torch.as_tensor(self.t, dtype=torch.float)\n        batch.v = torch.as_tensor(self.v, dtype=torch.float)\n        batch.bounds = torch.as_tensor(self.bounds, dtype=torch.float)\n\n        # Other configurables\n        batch.origin = torch.as_tensor(self.origin, dtype=torch.float)\n        batch.world_up = torch.as_tensor(self.world_up, dtype=torch.float)\n        batch.movement_speed = torch.as_tensor(self.movement_speed, dtype=torch.float)\n        batch.movement_force = torch.as_tensor(self.movement_force, dtype=torch.float)\n        batch.drag_coeff_mult = torch.as_tensor(self.drag_coeff_mult, dtype=torch.float)\n        batch.constant_drag = torch.as_tensor(self.constant_drag, dtype=torch.float)\n        batch.mass = torch.as_tensor(self.mass, dtype=torch.float)\n        batch.moment_of_inertia = torch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H = torch.as_tensor(self.H)\n        batch.W = torch.as_tensor(self.W)\n        batch.K = torch.as_tensor(self.K)\n        batch.R = torch.as_tensor(self.R)\n        batch.T = torch.as_tensor(self.T)\n        batch.n = torch.as_tensor(self.n)\n        batch.f = torch.as_tensor(self.f)\n        batch.t = torch.as_tensor(self.t)\n        batch.v = torch.as_tensor(self.v)\n        batch.bounds = torch.as_tensor(self.bounds)\n        batch.meta = dotdict()\n        batch.meta.H = self.H\n        batch.meta.W = self.W\n        batch.meta.K = self.K\n        batch.meta.R = self.R\n        batch.meta.T = self.T\n        batch.meta.n = self.n\n        batch.meta.f = self.f\n        batch.meta.t = self.t\n        batch.meta.v = self.v\n        batch.meta.bounds = self.bounds\n        return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H = self.H\n        batch.W = self.W\n        batch.K = self.K\n        batch.R = self.R\n        batch.T = self.T\n        batch.n = self.n\n        batch.f = self.f\n        batch.t = self.t\n        batch.v = self.v\n        batch.bounds = self.bounds\n\n        batch.meta = dotdict()\n        batch.meta.H = self.H\n        batch.meta.W = self.W\n        batch.meta.K = self.K\n        batch.meta.R = self.R\n        batch.meta.T = self.T\n        batch.meta.n = self.n\n        batch.meta.f = self.f\n        batch.meta.t = self.t\n        batch.meta.v = self.v\n        batch.meta.bounds = self.bounds\n\n        batch.meta.origin = self.origin\n        batch.meta.world_up = self.world_up\n        batch.meta.movement_speed = self.movement_speed\n        batch.meta.movement_force = self.movement_force\n        batch.meta.drag_coeff_mult = self.drag_coeff_mult\n        batch.meta.constant_drag = self.constant_drag\n        batch.meta.mass = self.mass\n        batch.meta.moment_of_inertia = self.moment_of_inertia\n        batch.meta.movement_torque = self.movement_torque\n        batch.meta.angular_friction = self.angular_friction\n        batch.meta.constant_torque = self.constant_torque\n        batch.meta.min_interval = self.min_interval\n        batch.meta.pause_physics = self.pause_physics\n\n        batch.meta.torque = self.torque\n        batch.meta.speed = self.speed\n        batch.meta.acc = self.acc\n        batch.meta.angular_speed = self.angular_speed\n        batch.meta."}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        batch.meta = batch.copy()\n        return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Batch (network input parameters)\n        batch = dotdict()\n        batch.H = self.H\n        batch.W = self.W\n        batch.K = self.K\n        batch.R = self.R\n        batch.T = self.T\n        batch.n = self.n\n        batch.f = self.f\n        batch.t = self.t\n        batch.v = self.v\n        batch.bounds = self.bounds\n\n        batch.mass = self.mass\n        batch.moment_of_inertia = self.moment_of_inertia\n        batch.movement_force = self.movement_force\n        batch.movement_torque = self.movement_torque\n        batch.movement_speed = self.movement_speed\n        batch.origin = self.origin\n        batch.world_up = self.world_up\n\n        batch.meta = dotdict()\n        batch.meta.H = self.H\n        batch.meta.W = self.W\n        batch.meta.K = self.K\n        batch.meta.R = self.R\n        batch.meta.T = self.T\n        batch.meta.n = self.n\n        batch.meta.f = self.f\n        batch.meta.t = self.t\n        batch.meta.v = self.v\n        batch.meta.bounds = self.bounds\n\n        batch.meta.mass = self.mass\n        batch.meta.moment_of_inertia = self.moment_of_inertia\n        batch.meta.movement_force = self.movement_force\n        batch.meta.movement_torque = self.movement_torque\n        batch.meta.movement_speed = self.movement_speed\n        batch.meta.origin = self.origin\n        batch.meta.world_up = self.world_up\n\n        return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Batch (network input parameters)\n        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        batch.meta = dotdict()\n        batch.meta.H, batch.meta.W, batch.meta.K, batch.meta.R, batch.meta.T, batch.meta.n, batch.meta.f, batch.meta.t, batch.meta.v, batch.meta.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H = self.H\n        batch.W = self.W\n        batch.K = self.K\n        batch.R = self.R\n        batch.T = self.T\n        batch.n = self.n\n        batch.f = self.f\n        batch.t = self.t\n        batch.v = self.v\n        batch.bounds = self.bounds\n\n        # Other configurables\n        batch.mass = self.mass\n        batch.moment_of_inertia = self.moment_of_inertia\n        batch.movement_force = self.movement_force\n        batch.movement_torque = self.movement_torque\n        batch.movement_speed = self.movement_speed\n        batch.origin = self.origin\n        batch.world_up = self.world_up\n\n        batch.meta = dotdict()\n        batch.meta.H = self.H\n        batch.meta.W = self.W\n        batch.meta.K = self.K\n        batch.meta.R = self.R\n        batch.meta.T = self.T\n        batch.meta.n = self.n\n        batch.meta.f = self.f\n        batch.meta.t = self.t\n        batch.meta.v = self.v\n        batch.meta.bounds = self.bounds\n\n        # Other configurables\n        batch.meta.mass = self.mass\n        batch.meta.moment_of_inertia = self.moment_of_inertia\n        batch.meta.movement_force = self.movement_force\n        batch.meta.movement_torque = self.movement_torque\n        batch.meta.movement_speed = self.movement_speed\n        batch.meta.origin = self.origin\n        batch.meta.world_up = self.world_up\n\n        return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n\n        # Batch (network input parameters)\n        batch.mass = self.mass\n        batch.moment_of_inertia = self.moment_of_inertia\n        batch.movement_force = self.movement_force\n        batch.movement_torque = self.movement_torque\n        batch.movement_speed = self.movement_speed\n        batch.origin = self.origin\n        batch.world_up = self.world_up\n\n        # Meta\n        batch.meta = dotdict()\n        batch.meta.H, batch.meta.W, batch.meta.K, batch.meta.R, batch.meta.T, batch.meta.n, batch.meta.f, batch.meta.t, batch.meta.v, batch.meta.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        batch.meta.mass = self.mass\n        batch.meta.moment_of_inertia = self.moment_of_inertia\n        batch.meta.movement_force = self.movement_force\n        batch.meta.movement_torque = self.movement_torque\n        batch.meta.movement_speed = self.movement_speed\n        batch.meta.origin = self.origin\n        batch.meta.world_up = self.world_up\n\n        return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Batch (network input parameters)\n        batch = dotdict()\n        batch.H = self.H\n        batch.W = self.W\n        batch.K = self.K\n        batch.R = self.R\n        batch.T = self.T\n        batch.n = self.n\n        batch.f = self.f\n        batch.t = self.t\n        batch.v = self.v\n        batch.bounds = self.bounds\n\n        batch.meta = dotdict()\n        batch.meta.H = self.H\n        batch.meta.W = self.W\n        batch.meta.K = self.K\n        batch.meta.R = self.R\n        batch.meta.T = self.T\n        batch.meta.n = self.n\n        batch.meta.f = self.f\n        batch.meta.t = self.t\n        batch.meta.v = self.v\n        batch.meta.bounds = self.bounds\n        batch.meta.origin = self.origin\n        batch.meta.world_up = self.world_up\n        batch.meta.movement_speed = self.movement_speed\n        batch.meta.movement_force = self.movement_force\n        batch.meta.drag_coeff_mult = self.drag_coeff_mult\n        batch.meta.constant_drag = self.constant_drag\n        batch.meta.mass = self.mass\n        batch.meta.moment_of_inertia = self.moment_of_inertia\n        batch.meta.movement_torque = self.movement_torque\n        batch.meta.angular_friction = self.angular_friction\n        batch.meta.constant_torque = self.constant_torque\n        batch.meta.min_interval = self.min_interval\n        batch.meta.pause_physics = self.pause_physics\n\n        return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H = torch.tensor([self.H], dtype=torch.float)\n        batch.W = torch.tensor([self.W], dtype=torch.float)\n        batch.K = torch.tensor(self.K, dtype=torch.float)  # 3, 3\n        batch.R = torch.tensor(self.R, dtype=torch.float)  # 3, 3\n        batch.T = torch.tensor(self.T, dtype=torch.float)  # 3,\n        batch.n = torch.tensor([self.n], dtype=torch.float)\n        batch.f = torch.tensor([self.f], dtype=torch.float)\n        batch.t = torch.tensor([self.t], dtype=torch.float)\n        batch.v = torch.tensor([self.v], dtype=torch.float)\n        batch.bounds = torch.tensor(self.bounds, dtype=torch.float)  # 2, 3\n\n        # Batch (network input parameters)\n        batch.meta = dotdict()\n        batch.meta.H = self.H\n        batch.meta.W = self.W\n        batch.meta.K = self.K\n        batch.meta.R = self.R\n        batch.meta.T = self.T\n        batch.meta.n = self.n\n        batch.meta.f = self.f\n        batch.meta.t = self.t\n        batch.meta.v = self.v\n        batch.meta.bounds = self.bounds\n\n        return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Batch (network input parameters)\n        batch = dotdict()\n        batch.H = self.H\n        batch.W = self.W\n        batch.K = self.K\n        batch.R = self.R\n        batch.T = self.T\n        batch.n = self.n\n        batch.f = self.f\n        batch.t = self.t\n        batch.v = self.v\n        batch.bounds = self.bounds\n\n        # Other configurables\n        batch.origin = self.origin\n        batch.world_up = self.world_up\n        batch.movement_speed = self.movement_speed\n        batch.movement_force = self.movement_force\n        batch.drag_coeff_mult = self.drag_coeff_mult\n        batch.constant_drag = self.constant_drag\n        batch.mass = self.mass\n        batch.moment_of_inertia = self.moment_of_inertia\n        batch.movement_torque = self.movement_torque\n        batch.angular_friction = self.angular_friction\n        batch.constant_torque = self.constant_torque\n        batch.min_interval = self.min_interval\n        batch.pause_physics = self.pause_physics\n\n        # Other states\n        batch.is_dragging = self.is_dragging\n        batch.about_origin = self.about_origin\n        batch.is_panning = self.is_panning\n        batch.lock_fx_fy = self.lock_fx_fy\n        batch.drag_start = self.drag_start\n\n        # Internal states to facilitate moving with mass\n        batch.force = self.force\n        batch.speed = self.speed\n        batch.acc = self.acc\n        batch.torque = self.torque\n        batch.angular_speed = self.angular_speed\n        batch.angular_acc = self.angular_acc\n\n        batch.meta = batch\n        return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H = self.H\n        batch.W = self.W\n        batch.K = torch.as_tensor(self.K).float()  # 3, 3\n        batch.R = torch.as_tensor(self.R).float()  # 3, 3\n        batch.T = torch.as_tensor(self.T).float()  # 3,\n        batch.n = self.n\n        batch.f = self.f\n        batch.t = self.t\n        batch.v = self.v\n        batch.bounds = torch.as_tensor(self.bounds).float()  # 2, 3\n\n        # Batch (network input parameters)\n        batch.meta = dotdict()\n        batch.meta.H = self.H\n        batch.meta.W = self.W\n        batch.meta.K = self.K\n        batch.meta.R = self.R\n        batch.meta.T = self.T\n        batch.meta.n = self.n\n        batch.meta.f = self.f\n        batch.meta.t = self.t\n        batch.meta.v = self.v\n        batch.meta.bounds = self.bounds\n        return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n\n        batch.meta = dotdict()\n        batch.meta.H, batch.meta.W, batch.meta.K, batch.meta.R, batch.meta.T, batch.meta.n, batch.meta.f, batch.meta.t, batch.meta.v, batch.meta.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n\n        # Other configurables\n        batch.origin = self.origin\n        batch.world_up = self.world_up\n        batch.movement_speed = self.movement_speed\n        batch.movement_force = self.movement_force\n        batch.drag_coeff_mult = self.drag_coeff_mult\n        batch.constant_drag = self.constant_drag\n        batch.mass = self.mass\n        batch.moment_of_inertia = self.moment_of_inertia\n        batch.movement_torque = self.movement_torque\n        batch.angular_friction = self.angular_friction\n        batch.constant_torque = self.constant_torque\n\n        batch.min_interval = self.min_interval\n        batch.pause_physics = self.pause_physics\n\n        return batch"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            similarity_threshold = self.calculate_similarity_threshold()\n            closest_agent = None\n            max_similarity = -np.inf\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n                similarity = cosine_similarity([agent.purpose_embedding], [purpose_embedding])[0][0]\n                if similarity > max_similarity and similarity > similarity_threshold:\n                    closest_agent = agent\n                    max_similarity = similarity\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            similarities = [cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0] for agent in self.agents]\n            if similarities:\n                max_similarity = max(similarities)\n                max_similarity_index = similarities.index(max_similarity)\n                return self.agents[max_similarity_index], max_similarity\n            else:\n                return None, -np.inf\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            similarity_threshold = self.calculate_similarity_threshold()\n            closest_agent = None\n            max_similarity = -np.inf\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n                similarity = cosine_similarity([agent.purpose_embedding], [purpose_embedding])[0][0]\n                if similarity > similarity_threshold and similarity > max_similarity:\n                    closest_agent = agent\n                    max_similarity = similarity\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            similarities = [cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0] for agent in self.agents]\n            if not similarities:\n                return None, -np.inf\n            max_similarity = max(similarities)\n            max_similarity_index = similarities.index(max_similarity)\n            return self.agents[max_similarity_index], max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            similarities = [cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0] for agent in self.agents]\n            if similarities:\n                max_similarity = max(similarities)\n                max_index = similarities.index(max_similarity)\n                return self.agents[max_index], max_similarity\n            else:\n                logger.exception(\"No agents found\")\n                raise ValueError(\"No agents found\")\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            similarities = [cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0] for agent in self.agents]\n            max_similarity = max(similarities)\n            max_index = similarities.index(max_similarity)\n            return self.agents[max_index], max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            return None, -np.inf"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            similarities = [cosine_similarity([e], [purpose_embedding])[0][0] for e in self.agents]\n            if similarities:\n                max_similarity = max(similarities)\n                max_index = similarities.index(max_similarity)\n                return self.agents[max_index], max_similarity\n            else:\n                return None, -np.inf\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if not self.agents:\n                return None, -np.inf\n\n            similarities = [cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0] for agent in self.agents]\n            best_agent_index = np.argmax(similarities)\n            return self.agents[best_agent_index], similarities[best_agent_index]\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if len(self.agents) == 0:\n                return None, -np.inf\n\n            similarities = [cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0] for agent in self.agents]\n            max_similarity = max(similarities)\n            max_index = similarities.index(max_similarity)\n            return self.agents[max_index], max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if not self.agents:\n                return None, -np.inf\n\n            similarities = [cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0] for agent in self.agents]\n            max_similarity = max(similarities)\n            if max_similarity < self.calculate_similarity_threshold():\n                return None, -np.inf\n\n            closest_agent_index = similarities.index(max_similarity)\n            return self.agents[closest_agent_index], max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if len(self.agents) < 250:\n                return None, -np.inf\n\n            similarities = [cosine_similarity([e], [purpose_embedding])[0][0] for e in [agent.purpose_embedding for agent in self.agents]]\n            max_similarity = max(similarities)\n            if max_similarity < self.calculate_similarity_threshold():\n                return None, -np.inf\n            max_similarity_index = similarities.index(max_similarity)\n            return self.agents[max_similarity_index], max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            similarity_threshold = self.calculate_similarity_threshold()\n            if similarity_threshold <= 0.999:\n                return None, -np.inf\n\n            closest_agent = None\n            max_similarity = -np.inf\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n                if similarity > max_similarity and similarity >= similarity_threshold:\n                    closest_agent = agent\n                    max_similarity = similarity\n\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if not isinstance(purpose_embedding, np.ndarray):\n                logger.exception(\"Invalid purpose embedding type\")\n                raise ValueError(\"Invalid purpose embedding type\")\n\n            closest_agent = None\n            max_similarity = -np.inf\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n                similarity = cosine_similarity([agent.purpose_embedding], [purpose_embedding])[0][0]\n                if similarity > max_similarity:\n                    closest_agent = agent\n                    max_similarity = similarity\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if not isinstance(purpose_embedding, np.ndarray):\n                raise ValueError(\"Invalid purpose embedding format. Expected numpy array.\")\n\n            if len(self.agents) == 0:\n                return None, -np.inf\n\n            max_similarity = -np.inf\n            closest_agent = None\n\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([agent.purpose_embedding], [purpose_embedding])[0][0]\n                if similarity > max_similarity:\n                    max_similarity = similarity\n                    closest_agent = agent\n\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if not isinstance(purpose_embedding, np.ndarray) or purpose_embedding.shape != (1, 1536):\n                logger.exception(\"Invalid purpose embedding format\")\n                raise ValueError(\"Invalid purpose embedding format\")\n\n            similarity_threshold = self.calculate_similarity_threshold()\n            logger.info(f\"Similarity threshold: {similarity_threshold}\")\n            closest_agent = None\n            max_similarity = -np.inf\n\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n                if similarity > max_similarity and similarity >= similarity_threshold:\n                    closest_agent = agent\n                    max_similarity = similarity\n\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if not isinstance(purpose_embedding, np.ndarray):\n                raise TypeError(\"Purpose embedding must be a numpy array.\")\n\n            if len(purpose_embedding) != 1536:\n                raise ValueError(\"Purpose embedding must have 1536 dimensions.\")\n\n            similarities = [cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0] for agent in self.agents]\n            if similarities:\n                max_similarity = max(similarities)\n                if max_similarity > 0.999:\n                    return self.agents[similarities.index(max_similarity)], max_similarity\n\n            return None, -np.inf\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            closest_agent = None\n            highest_similarity = -np.inf\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n                if similarity > highest_similarity:\n                    closest_agent = agent\n                    highest_similarity = similarity\n            return closest_agent, highest_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")\n\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if not purpose_embedding.any():\n                return None, -np.inf\n\n            closest_agent = None\n            max_similarity = -np.inf\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n                if similarity > max_similarity:\n                    closest_agent = agent\n                    max_similarity = similarity\n\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")\n\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if not purpose_embedding.any():\n                raise ValueError(\"Purpose embedding is empty\")\n\n            similarities = [cosine_similarity([e], [purpose_embedding])[0][0] for e in self.agents]\n            closest_agent_index = np.argmax(similarities)\n            closest_agent = self.agents[closest_agent_index]\n            similarity_score = similarities[closest_agent_index]\n\n            return closest_agent, similarity_score\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            closest_agent = None\n            max_similarity = -np.inf\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n                similarity = cosine_similarity([agent.purpose_embedding], [purpose_embedding])[0][0]\n                if similarity > max_similarity:\n                    closest_agent = agent\n                    max_similarity = similarity\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")\n\n\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.prime = True\n        prime_agent.unspecified = True\n        self.agents.append(prime_agent)"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        self.agents.append(MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper, prime=True, weight=PRIME_AGENT_WEIGHT))"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 1, self, self.openai_wrapper)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.prime = True\n        prime_agent.unspecified = True\n        self.agents.append(prime_agent)\n\n    "}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        self.agents.append(MicroAgent(PRIME_PROMPT, PRIME_NAME, 1, self, self.openai_wrapper, weight=PRIME_AGENT_WEIGHT, prime=True))"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        self.agents.append(MicroAgent(PRIME_PROMPT, PRIME_NAME, 1, self, self.openai_wrapper, prime=True, weight=PRIME_AGENT_WEIGHT))\n\n    "}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        self.agents.append(MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper, prime=True))\n\n    "}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.prime = True\n        prime_agent.unspecified = True\n        self.agents.append(prime_agent)"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 1, self, self.openai_wrapper)\n        prime_agent.prime = True\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.unspecified = True\n        self.agents.append(prime_agent)"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        self.agents.append(MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper, prime_agent=True, weight=PRIME_AGENT_WEIGHT))"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.is_prime = True\n        prime_agent.unspecified = True\n        self.agents.append(prime_agent)\n\n    "}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper, prime=True)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.flags = {\"unspecified\": True}\n        self.agents.append(prime_agent)"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper, prime=True)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.working_agent = True\n        self.agents.append(prime_agent)\n\n    "}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper, prime=True)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.flags = [\"prime\"]\n        self.agents.append(prime_agent)"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper, prime=True)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.flags.append(\"prime\")\n        prime_agent.flags.append(\"unspecified\")\n        self.agents.append(prime_agent)"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 1, self, self.openai_wrapper)\n        prime_agent.is_prime = True\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.working_agent = True\n        self.agents.append(prime_agent)"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        self.agents.append(MicroAgent(PRIME_PROMPT, PRIME_NAME, 1, self, self.openai_wrapper, prime=True, unspecified=True, weight=PRIME_AGENT_WEIGHT))"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        self.agents = []\n        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 1, self, self.openai_wrapper)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.prime = True\n        prime_agent.unspecified = True\n        self.agents.append(prime_agent)\n\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        try:\n            prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper)\n            prime_agent.is_prime = True\n            prime_agent.weight = PRIME_AGENT_WEIGHT\n            prime_agent.is_unspecified = True\n            prime_agent.usage_count = 0\n            self.agents.append(prime_agent)\n        except Exception as e:\n            logger.exception(f\"Error in creating prime agent: {e}\")\n            raise\n\n    "}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        if self.agents:\n            return\n\n        prime_prompt = PRIME_PROMPT\n        prime_name = PRIME_NAME\n        prime_weight = PRIME_AGENT_WEIGHT\n        prime_flags = [\n            \"prime\",\n            \"unspecified\"\n        ]\n        prime_agent = MicroAgent(prime_prompt, prime_name, 0, self, self.openai_wrapper)\n        prime_agent.weight = prime_weight\n        prime_agent.flags = prime_flags\n        self.agents.append(prime_agent)\n\n    "}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        try:\n            prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper)\n            prime_agent.weight = PRIME_AGENT_WEIGHT\n            prime_agent.is_prime = True\n            prime_agent.is_unspecified = True\n            self.agents.append(prime_agent)\n        except Exception as e:\n            logger.exception(f\"Error creating prime agent: {e}\")\n            raise\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:\n            "}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agent:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agent:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agent:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        serialized_agents = self.persistence.fetch_all_agents()\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        serialized_agents = self.persistence.fetch_all_agents()\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        serialized_agents = self.persistence.fetch_all_agents()\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        serialized_agents = self.persistence.fetch_all_agents()\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent is not None:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        # Load all agents from the database and return a list of these agents if they are successfully loaded.\n        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise\n\n    "}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agent_lifecycle.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agent_lifecycle.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agent_lifecycle.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agent_lifecycle.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agent_lifecycle.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agent_lifecycle.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agent_lifecycle.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agent_lifecycle.cleanup_agents()\n        return self.agent_lifecycle.agents\n\n    "}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agent_lifecycle.cleanup_agents()\n        return self.agent_lifecycle.agents\n\n    "}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agent_lifecycle.cleanup_agents()\n        return self.agent_lifecycle.agents\n\n    "}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agent_lifecycle.cleanup_agents()\n        return self.agent_lifecycle.agents\n\n    "}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents\n\n    "}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agent_lifecycle.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agent_lifecycle.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agent_lifecycle.cleanup_agents()\n        return self.agent_lifecycle.agents\n    \n    "}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        logger.info(\"Cleaning up agents...\")\n        self.agent_lifecycle.cleanup_agents()\n        logger.info(\"Agents cleaned up.\")\n        return self.agent_lifecycle.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        logger.info(\"Cleaning up agents...\")\n        self.agent_lifecycle.cleanup_agents()\n        logger.info(\"Agents cleaned up successfully.\")\n        return self.agent_lifecycle.agents"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": ""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": ""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n            goal=goal,\n            system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n            examples=EXAMPLES,\n            sample_input=sample_input\n        )\n\n        try:\n            return self.openai_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n            goal=goal,\n            examples=EXAMPLES,\n            system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n            sample_input=sample_input\n        )\n\n        try:\n            response = self.openai_wrapper.get_chat_completion(prompt)\n            return response\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n            goal=goal,\n            examples=EXAMPLES,\n            system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n            sample_input=sample_input\n        )\n\n        try:\n            chat_completion = self.openai_wrapper.get_chat_completion(prompt)\n            return chat_completion\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n            goal=goal,\n            examples=EXAMPLES,\n            prompt_engineering_system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n            sample_input=sample_input\n        )\n\n        try:\n            return self.openai_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n            goal=goal,\n            system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n            examples=EXAMPLES,\n            sample_input=sample_input\n        )\n        try:\n            return self.openai_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n            purpose=goal,\n            examples=EXAMPLES,\n            system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n            sample_input=sample_input\n        )\n\n        try:\n            return self.openai_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n            goal=goal,\n            examples=EXAMPLES,\n            prompt_engineering_system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n            sample_input=sample_input\n        )\n        try:\n            return self.openai_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n            goal=goal,\n            examples=EXAMPLES,\n            system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n            sample_input=sample_input,\n        )\n\n        try:\n            chat_completion = self.openai_wrapper.get_chat_completion(prompt)\n            return chat_completion\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n            goal=goal,\n            examples=EXAMPLES,\n            system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n            sample_input=sample_input\n        )\n        return prompt"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n            goal=goal,\n            examples=EXAMPLES,\n            sample_input=sample_input,\n            system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT\n        )\n\n        try:\n            response = self.openai_wrapper.get_chat_completion(prompt)\n            return response\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        goal_prompt = PROMPT_ENGINEERING_TEMPLATE.format(goal=goal, examples=EXAMPLES)\n        prompt = PROMPT_ENGINEERING_SYSTEM_PROMPT + goal_prompt + sample_input\n        try:\n            return self.openai_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n            goal=goal,\n            examples=EXAMPLES,\n            system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n            sample_input=sample_input\n        )\n\n        try:\n            response = self.openai_wrapper.get_chat_completion(prompt)\n            return response.strip()\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n                system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n                examples=EXAMPLES,\n                goal=goal,\n                sample_input=sample_input,\n            )\n            return self.openai_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n            system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n            goal=goal,\n            example=EXAMPLES,\n            sample_input=sample_input\n        )\n\n        try:\n            return self.openai_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        prompt = PROMPT_ENGINEERING_TEMPLATE.format(goal=goal, examples=EXAMPLES, sample_input=sample_input)\n        try:\n            response = self.openai_wrapper.get_chat_completion(prompt, PROMPT_ENGINEERING_SYSTEM_PROMPT)\n            return response\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n            goal=goal,\n            examples=EXAMPLES,\n            system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n            sample_input=sample_input,\n        )\n\n        try:\n            response = self.openai_wrapper.get_chat_completion(prompt)\n            return response.strip()\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        prompt = PROMPT_ENGINEERING_TEMPLATE.format(goal=goal, examples=EXAMPLES, sample_input=sample_input)\n        try:\n            response = self.openai_wrapper.get_chat_completion(PROMPT_ENGINEERING_SYSTEM_PROMPT, prompt)\n            return response.message.content\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n                system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n                examples=EXAMPLES,\n                goal=goal,\n                sample_input=sample_input\n            )\n            llm_response = self.openai_wrapper.get_chat_completion(prompt)\n            return llm_response\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"REPLACE INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))\n\n    "}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))\n\n    "}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict[\"id\"], agent_dict[\"purpose\"], json.dumps(agent_dict)))\n\n    "}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"REPLACE INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict[\"id\"], agent_dict[\"purpose\"], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"REPLACE INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))\n            conn.commit()"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"REPLACE INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))\n            conn.commit()"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"REPLACE INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict[\"id\"], agent_dict[\"purpose\"], json.dumps(agent_dict)))\n\n    "}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"REPLACE INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"REPLACE INTO agents (id, purpose, data) VALUES (?, ?, ?)\",\n                         (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"REPLACE INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            data = json.dumps(agent_dict)\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], data))\n\n    "}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"REPLACE INTO agents (id, purpose, data) VALUES (?, ?, ?)\",\n                           (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))\n            conn.commit()\n\n    "}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            data = json.dumps(agent_dict)\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict[\"id\"], agent_dict[\"purpose\"], data))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict[\"id\"], agent_dict[\"purpose\"], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"INSERT OR REPLACE INTO agents (id, purpose, data) VALUES (?, ?, ?)\",\n                           (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))\n            conn.commit()"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n        return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n        return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n        return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n        return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row is not None:\n                return json.loads(row[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row is not None:\n                return json.loads(row[0])\n        return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,))\n            result = cursor.fetchone()\n            if result:\n                return json.loads(result[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row is not None:\n                return json.loads(row[0])\n        return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row is not None:\n                return json.loads(row[0])\n            return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            result = cursor.fetchone()\n            if result:\n                return json.loads(result[0])\n        return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row is None:\n                return None\n            return json.loads(row[0])"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row is not None:\n                return json.loads(row[0])\n            return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,))\n            result = cursor.fetchone()\n            if result:\n                return json.loads(result[0])\n        return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row is not None:\n                return json.loads(row[0])\n        return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,))\n            result = cursor.fetchone()\n            if result:\n                return json.loads(result[0])\n            else:\n                return None\n\n    "}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row is None:\n                return None\n            return json.loads(row[0])"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            purposes = cursor.fetchall()\n            return [purpose[0] for purpose in purposes]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            purposes = cursor.fetchall()\n            return [purpose[0] for purpose in purposes]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            purposes = cursor.fetchall()\n            return [purpose[0] for purpose in purposes]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            purposes = cursor.fetchall()\n            return [purpose[0] for purpose in purposes]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            return [row[0] for row in cursor.fetchall()]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            return [row[0] for row in cursor.fetchall()]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            purposes = cursor.fetchall()\n            return [purpose[0] for purpose in purposes]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            return [row[0] for row in cursor.fetchall()]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            return [row[0] for row in cursor.fetchall()]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            purposes = [row[0] for row in cursor.fetchall()]\n            return purposes"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"SELECT result FROM cache WHERE hash = ?\",\n            (arg_hash,)\n        )\n        result = cursor.fetchone()\n        if result is not None:\n            return json.loads(result[0])\n        return None"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"SELECT result FROM cache WHERE hash = ?\",\n            (arg_hash,)\n        )\n        result = cursor.fetchone()\n        if result is not None:\n            return json.loads(result[0])\n        return None"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"SELECT result FROM cache WHERE hash = ?\",\n            (arg_hash,)\n        )\n        result = cursor.fetchone()\n        if result is not None:\n            return json.loads(result[0])\n        return None"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"SELECT result FROM cache WHERE hash = ?\",\n            (arg_hash,)\n        )\n        result = cursor.fetchone()\n        if result is not None:\n            return json.loads(result[0])\n        return None"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"SELECT result FROM cache WHERE hash = ?\",\n            (arg_hash,)\n        )\n        row = cursor.fetchone()\n        if row is not None:\n            return json.loads(row[0])\n        return None"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"SELECT result FROM cache WHERE hash = ?\",\n            (arg_hash,)\n        )\n        row = cursor.fetchone()\n        if row is not None:\n            return json.loads(row[0])\n        return None"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"SELECT result FROM cache WHERE hash = ?\",\n            (arg_hash,)\n        )\n        row = cursor.fetchone()\n        if row is not None:\n            return json.loads(row[0])\n        return None"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n        result = cursor.fetchone()\n        if result is not None:\n            return json.loads(result[0])\n        return None"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n        result = cursor.fetchone()\n        if result is not None:\n            return json.loads(result[0])\n        return None"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n        result = cursor.fetchone()\n        if result is not None:\n            return json.loads(result[0])\n        return None"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"SELECT result FROM cache WHERE hash = ?\",\n            (arg_hash,)\n        )\n        row = cursor.fetchone()\n        if row is None:\n            return None\n        return json.loads(row[0])"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"SELECT result FROM cache WHERE hash = ?\",\n            (arg_hash,)\n        )\n        row = cursor.fetchone()\n        if row is None:\n            return None\n        return json.loads(row[0])"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"SELECT result FROM cache WHERE hash = ?\",\n            (arg_hash,)\n        )\n        row = cursor.fetchone()\n        if row is None:\n            return None\n        return json.loads(row[0])"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"SELECT result FROM cache WHERE hash = ?\",\n            (arg_hash,)\n        )\n        result = cursor.fetchone()\n        if result is None:\n            return None\n        return json.loads(result[0])"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n        row = cursor.fetchone()\n        if row is None:\n            return None\n        return json.loads(row[0])"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"SELECT result FROM cache WHERE hash = ?\", (arg_hash,)\n        )\n        row = cursor.fetchone()\n        if row is None:\n            return None\n        return json.loads(row[0])"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"SELECT result FROM cache WHERE hash = ?\", (arg_hash,)\n        )\n        row = cursor.fetchone()\n        if row is None:\n            return None\n        return json.loads(row[0])"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash=?\", (arg_hash,))\n        row = cursor.fetchone()\n        if row is not None:\n            return json.loads(row[0])\n        return None"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n        row = cursor.fetchone()\n        if row is not None:\n            return json.loads(row[0])\n\n        return None"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash=?\", (arg_hash,))\n        row = cursor.fetchone()\n        if row is not None:\n            return json.loads(row[0])"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT OR REPLACE INTO cache VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT OR REPLACE INTO cache VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT OR REPLACE INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT OR REPLACE INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"INSERT INTO cache VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"INSERT INTO cache VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"INSERT INTO cache VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"INSERT INTO cache VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT OR REPLACE INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT OR REPLACE INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT OR REPLACE INTO cache VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT OR REPLACE INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT OR REPLACE INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT OR REPLACE INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"INSERT INTO cache VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"INSERT INTO cache (hash, result) VALUES (?, ?)\", (arg_hash, json.dumps(result)))\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT INTO cache VALUES (?, ?)\", (arg_hash, json.dumps(result))\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        self.connection.execute(\n            \"INSERT OR REPLACE INTO cache VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT OR REPLACE INTO cache VALUES (?, ?)\", (arg_hash, json.dumps(result))\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        # Serialize the result to JSON\n        result_json = json.dumps(result)\n\n        # Insert the result into the cache table\n        cursor = self.connection.cursor()\n        cursor.execute(\"INSERT OR REPLACE INTO cache VALUES (?, ?)\", (arg_hash, result_json))\n        self.connection.commit()"}
{"namespace": "run.execute_command_line_process", "completion": "    if args.quiet:\n        with open(\"output.txt\", \"w\") as f:\n            with redirect_stdout(f):\n                start_command_line(vars(args))\n    else:\n        start_command_line(vars(args))"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_config(args)\n\n    # Start the command line interface with the provided arguments\n    start_command_line(vars(args))"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_config_from_args(args)\n\n    # Start the command line interface\n    start_command_line(vars(args))"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_config_from_args(args)\n\n    # Start the command line interface with the provided arguments\n    start_command_line(vars(args))"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update(deepcopy(ARGS))\n    CONFIG.update(vars(args))\n\n    # If quiet mode is enabled, redirect output to a file\n    if quiet_mode:\n        with open(\"output.txt\", \"w\") as f:\n            with redirect_stdout(f):\n                start_command_line(vars(args))\n    else:\n        start_command_line(vars(args))"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_config_from_args(args)\n\n    # If quiet mode is enabled, redirect output to a file\n    if quiet_mode:\n        with open(os.path.join(CONFIG.record_dir, 'output.txt'), 'w') as f:\n            with redirect_stdout(f):\n                start_command_line(vars(args))\n    else:\n        start_command_line(vars(args))"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_config(\n        task=args.task,\n        upload_files=args.upload_files,\n        model=args.model,\n        record_dir=args.record_dir,\n        mode=args.mode,\n        max_subtask_chain_length=args.max_subtask_chain_length,\n        enable_ask_human_for_help=args.enable_ask_human_for_help,\n        max_plan_refine_chain_length=args.max_plan_refine_chain_length,\n        max_plan_tree_depth=args.max_plan_tree_depth,\n        max_plan_tree_width=args.max_plan_tree_width,\n        max_retry_times=args.max_retry_times,\n    )\n\n    # If quiet mode is enabled, redirect output to a file\n    if quiet_mode:\n        with open(os.path.join(CONFIG.record_dir, 'output.txt'), 'w') as f:\n            with redirect_stdout(f):\n                start_command_line(deepcopy(ARGS))\n    else:\n        start_command_line(deepcopy(ARGS))"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_config(args)\n\n    # If quiet mode is enabled, redirect output to a file\n    if quiet_mode:\n        with open('output.txt', 'w') as f:\n            with redirect_stdout(f):\n                start_command_line(deepcopy(ARGS))\n    else:\n        # If quiet mode is not enabled, print output to the terminal\n        start_command_line(deepcopy(ARGS))"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_config(\n        model=args.model,\n        record_dir=args.record_dir,\n        max_subtask_chain_length=args.max_subtask_chain_length,\n        enable_ask_human_for_help=args.enable_ask_human_for_help,\n        max_plan_refine_chain_length=args.max_plan_refine_chain_length,\n        max_plan_tree_depth=args.max_plan_tree_depth,\n        max_plan_tree_width=args.max_plan_tree_width,\n        max_retry_times=args.max_retry_times,\n    )\n\n    # If quiet mode is enabled, redirect standard output to a file\n    if quiet_mode:\n        with open(\"log.txt\", \"w\") as f:\n            with redirect_stdout(f):\n                start_command_line(vars(args))\n    else:\n        start_command_line(vars(args))"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update the global configuration parameters with the provided arguments\n    CONFIG.update(ARGS)\n    CONFIG.update(vars(args))\n\n    # If quiet mode is enabled, redirect the standard output to a file\n    if quiet_mode:\n        with open(\"output.txt\", \"w\") as f:\n            with redirect_stdout(f):\n                start_command_line(vars(args))\n    else:\n        start_command_line(vars(args))"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update(deepcopy(ARGS))\n    CONFIG.update(vars(args))\n\n    # Start the command line interface with the updated arguments\n    start_command_line(CONFIG)"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_config_from_args(args)\n\n    # If quiet mode is enabled, redirect the standard output to a file instead of printing it to the terminal\n    if quiet_mode:\n        with open(\"output.txt\", \"w\") as f:\n            with redirect_stdout(f):\n                start_command_line(deepcopy(ARGS))\n    else:\n        start_command_line(deepcopy(ARGS))"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_params(\n        task=args.task,\n        upload_files=args.upload_files,\n        model=args.model,\n        record_dir=args.record_dir,\n        mode=args.mode,\n        max_subtask_chain_length=args.max_subtask_chain_length,\n        enable_ask_human_for_help=args.enable_ask_human_for_help,\n        max_plan_refine_chain_length=args.max_plan_refine_chain_length,\n        max_plan_tree_depth=args.max_plan_tree_depth,\n        max_plan_tree_width=args.max_plan_tree_width,\n        max_retry_times=args.max_retry_times,\n    )\n\n    # Redirect output to a file if quiet mode is enabled\n    if quiet_mode:\n        with open(CONFIG.record_dir + \"/log.txt\", \"w\") as f:\n            with redirect_stdout(f):\n                start_command_line(deepcopy(ARGS))\n    else:\n        start_command_line(deepcopy(ARGS))"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_config(args)\n\n    # If quiet mode is enabled, redirect the standard output to a file\n    if quiet_mode:\n        with open(CONFIG.get('record_dir') + \"/output.txt\", \"w\") as f:\n            with redirect_stdout(f):\n                start_command_line(deepcopy(ARGS))\n    else:\n        start_command_line(deepcopy(ARGS))"}
{"namespace": "run.execute_command_line_process", "completion": "    if quiet_mode:\n        with open('logs/command_line_process.log', 'w') as f:\n            with redirect_stdout(f):\n                start_command_line(args.__dict__)\n    else:\n        start_command_line(args.__dict__)"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_params(args)\n\n    # Create a copy of the original configuration parameters\n    original_config = deepcopy(CONFIG)\n\n    # Start the command line interface\n    start_command_line(vars(args))\n\n    # Restore the original configuration parameters\n    CONFIG.update_params(original_config)"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update(ARGS, args)\n\n    # If quiet mode is enabled, redirect standard output to a file\n    if quiet_mode:\n        with open(\"logs/log.txt\", \"w\") as f:\n            with redirect_stdout(f):\n                start_command_line(deepcopy(vars(args)))\n    else:\n        # If quiet mode is not enabled, simply call the function with the provided arguments\n        start_command_line(deepcopy(vars(args)))"}
{"namespace": "run.execute_command_line_process", "completion": "    global CONFIG\n    config_file = args.config_file\n    CONFIG = deepcopy(CONFIG)\n    CONFIG.update(args)\n\n    if quiet_mode:\n        with open(os.path.join(CONFIG['record_dir'], 'output.txt'), 'w') as f:\n            with redirect_stdout(f):\n                start_command_line(vars(args))\n    else:\n        start_command_line(vars(args))"}
{"namespace": "run.execute_command_line_process", "completion": "    if quiet_mode:\n        with open(CONFIG.get('record_dir', '') + '/' + CONFIG.get('record_file', ''), 'w') as f:\n            with redirect_stdout(f):\n                start_command_line(vars(args))\n    else:\n        start_command_line(vars(args))"}
{"namespace": "run.execute_command_line_process", "completion": "    global CONFIG\n\n    # Update global configuration parameters with the provided arguments\n    CONFIG.update(**vars(args))\n\n    if quiet_mode:\n        # Redirect output to a file\n        with open('output.txt', 'w') as f:\n            with redirect_stdout(f):\n                start_command_line(deepcopy(vars(args)))\n    else:\n        # Print output to the terminal\n        start_command_line(deepcopy(vars(args)))"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_ap"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_ap"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n        except BadRequestError as e:\n            if \"maximum context length\" in e.user_message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n                chatcompletion_kwargs.update(kwargs)\n                chatcompletion_kwargs.pop(\"schema_error_retry\", None)\n\n                response = openai"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n        except BadRequestError as e:\n            if \"maximum context length\" in e.user_message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n                chatcompletion_kwargs.update(kwargs)\n                chatcompletion_kwargs.pop(\"schema_error_retry\", None)\n\n                response = openai"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n        except BadRequestError as e:\n            if \"maximum context length\" in e.user_message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n                chatcompletion_kwargs.update(kwargs)\n                chatcompletion_kwargs.pop(\"schema_error_retry\", None)\n\n                response = openai"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n        except BadRequestError as e:\n            if \"maximum context length\" in e.user_message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n                chatcompletion_kwargs.update(kwargs)\n                chatcompletion_kwargs.pop(\"schema_error_retry\", None)\n\n                response = openai"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n                chatcompletion_kwargs.update(kwargs)\n                chatcompletion_kwargs.pop(\"schema_error_retry\", None)\n\n                response = openai.Chat"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n        except BadRequestError as e:\n            if \"maximum context length\" in e.user_message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n                chatcompletion_kwargs.update(kwargs)\n                chatcompletion_kwargs.pop(\"schema_error_retry\", None)\n\n                response = open"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._last_time is None or time() - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._last_time is None or time() - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._last_time is None or (time() - self._last_time) > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._last_time is None or time() - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (\n            self._last_time is not None and time() - self._last_time > self._refetch_interval\n        ):\n            self._create_client()\n            self._last_time = time()\n\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (\n            self._last_time is not None and time() - self._last_time > self._refetch_interval\n        ):\n            self._create_client()\n            self._last_time = time()\n\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (\n            self._last_time is not None and time() - self._last_time > self._refetch_interval\n        ):\n            self._create_client()\n            self._last_time = time()\n\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if not self._client or (\n            self._last_time and time() - self._last_time > self._refetch_interval\n        ):\n            self._create_client()\n            self._last_time = time()\n\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._last_time is None or time() - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._last_time is None or time() - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._last_time is None or time() - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._last_time is None or (time() - self._last_time) > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (self._last_time and time() - self._last_time > self._refetch_interval):\n            self._create_client()\n            self._last_time = time()\n\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if not self._client or time() - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (\n            self._last_time is not None and time() - self._last_time > self._refetch_interval\n        ):\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if not self._client or (\n            self._last_time and time() - self._last_time > self._refetch_interval\n        ):\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if not self._client or self._last_time is None or time() - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._last_time is None or time() - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (self._last_time is not None and time() - self._last_time > self._refetch_interval):\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._last_time is None or time() - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n\n        return self._client"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise ValueError(\"The `state_dict` method cannot be called from a DataLoader worker process.\")\n\n        if self.item_loader:\n            item_loader_state = self.item_loader.state_dict()\n        else:\n            item_loader_state = None\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": item_loader_state,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise ValueError(\"The `state_dict` method should not be called from a DataLoader worker process.\")\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n    "}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The `state_dict` method cannot be called from a DataLoader worker process.\")\n\n        if self.item_loader:\n            item_loader_state = self.item_loader.state_dict()\n        else:\n            item_loader_state = None\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": item_loader_state,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise ValueError(\"`state_dict` can only be called from the main process.\")\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The `state_dict` method should be called from the main process.\")\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise ValueError(\"The `state_dict` method cannot be called from a DataLoader worker process.\")\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n        return state"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise ValueError(\"The `state_dict` method cannot be called from a DataLoader worker process.\")\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"shuffle\": self.shuffle,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        if self.item_loader:\n            state[\"item_loader\"] = self.item_loader.state_dict()\n\n        return state"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The state_dict method should not be called from a DataLoader worker process.\")\n\n        if self.item_loader:\n            item_loader_state = self.item_loader.state_dict()\n        else:\n            item_loader_state = None\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": item_loader_state,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\n                \"The `state_dict` method should not be called from a DataLoader worker process. \"\n                \"Please use the `load_state_dict` method instead.\"\n            )\n\n        if self.item_loader:\n            item_loader_state = self.item_loader.state_dict()\n        else:\n            item_loader_state = None\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": item_loader_state,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return state"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The state_dict method should be called from the main process.\")\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n        return state"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"`state_dict` can only be called from a non-worker process.\")\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The state_dict method is not supported within a DataLoader worker process.\")\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"shuffle\": self.shuffle,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n        }\n\n        if self.item_loader:\n            state[\"item_loader\"] = self.item_loader.state_dict()\n\n        return state"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise ValueError(\"You cannot call `state_dict` from within a DataLoader worker.\")\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return state"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\n                \"The `state_dict` method should not be called from a DataLoader worker process. \"\n                \"Instead, it should be called from the main process that is responsible for creating the dataset.\"\n            )\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise ValueError(\n                \"The state_dict method should not be called from a DataLoader worker process. \"\n                \"This method is intended to be called from the main process.\"\n            )\n\n        self._state_dict = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return self._state_dict"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise ValueError(\n                \"You're operating within a DataLoader worker process. \"\n                \"Please call `state_dict` from the main process.\"\n            )\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"shuffle\": self.shuffle,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n        }\n\n        if self.item_loader:\n            state[\"item_loader\"] = self.item_loader.state_dict()\n\n        self._state_dict = state\n\n        return state"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\n                \"The `state_dict` method should only be called from the main process. \"\n                \"If you are using a distributed setup, please use the `load_state_dict` method instead.\"\n            )\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return state"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise ValueError(\n                \"The state_dict method cannot be called from a DataLoader worker process. \"\n                \"Please use the load_state_dict method to reload the state of the dataset.\"\n            )\n\n        if self.item_loader is not None:\n            item_loader_state = self.item_loader.state_dict()\n        else:\n            item_loader_state = None\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": item_loader_state,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise ValueError(\n                \"The `state_dict` method should only be called from the main process. \"\n                \"Please check that you're not using the `StreamingDataLoader` with `num_workers > 1`.\"\n            )\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"shuffle\": self.shuffle,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n        }\n\n        if self.item_loader:\n            state[\"item_loader\"] = self.item_loader.state_dict()\n\n        return state\n\n    "}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\n                \"The `state_dict` method is only supported when used outside a DataLoader worker process. \"\n                \"Please ensure that the `StreamingDataset` is not used within a DataLoader.\"\n            )\n\n        self._validate_state_dict()\n\n        state: Dict[str, Any] = self._state_dict\n        state[\"num_samples_yielded\"] = num_samples_yielded\n        state[\"num_workers\"] = num_workers\n        state[\"batch_size\"] = batch_size\n        return state\n\n    "}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._validate_state_dict()\n        self._state_dict = state_dict\n\n    "}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        if self._state_dict is not None:\n            raise ValueError(\"The method `load_state_dict` should only be called once.\")\n\n        self._state_dict = state_dict\n        self._validate_state_dict()\n\n    "}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._state_dict = state_dict\n        self._validate_state_dict()\n\n        # replay sampling from each worker / chunks using the batch size\n        workers_chunks, workers_intervals = _associate_chunks_to_workers(\n            state_dict[\"num_workers\"], self.worker_env, self.worker_chunks, self.worker_intervals\n        )\n        indexes = _replay_sampling(state_dict[\"num_samples_yielded\"], state_dict[\"batch_size\"], state_dict[\"num_workers\"])\n        chunks_index, indexes = _replay_chunks_sampling(workers_intervals, indexes)\n\n        # select the chunks and intervals associated to this worker\n        worker_rank = self.worker_env.rank\n        self.num_chunks = len(workers_intervals[worker_rank])\n        self.chunk_index = chunks_index[worker_rank]\n        self.worker_chunks = workers_chunks[worker_rank]\n        self.worker_intervals = workers_intervals[worker_rank]\n\n        # replay the indexes for the current chunks\n        interval = self.worker_intervals[self.chunk_index]\n        current_indexes = np.arange(interval[0], interval[1])\n\n        # re-shuffle the indexes\n        current_indexes = self.shuffler(current_indexes, self.num_chunks, self.current_epoch, self.chunk_index)\n\n        # skip any indexes already consumed\n        current_indexes = current_indexes[indexes[worker_rank] :]\n        self.current_indexes = current_indexes\n\n        self.global_index = state_dict[\"num_samples_yielded\"]\n\n        # bump the chunk_index\n        self.chunk_index += 1"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        if self._state_dict is not None:\n            raise ValueError(\"The state dict has already been loaded.\")\n\n        self._state_dict = state_dict\n        self._validate_state_dict()"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict\n        self._validate_state_dict()\n\n        self.current_epoch = state_dict[\"current_epoch\"]\n\n        # TODO: Implement elastic sampling where the number of workers, ranks can change.\n        num_samples_yielded = state_dict[\"num_samples_yielded\"]\n        num_workers = state_dict[\"num_workers\"]\n        batch_size = state_dict[\"batch_size\"]\n\n        # replay sampling from each worker / chunks using the batch size\n        workers_chunks, workers_intervals = _associate_chunks_to_workers(\n            num_workers, self.worker_env, self.worker_chunks, self.worker_intervals\n        )\n        indexes = _replay_sampling(num_samples_yielded, batch_size, num_workers)\n        chunks_index, indexes = _replay_chunks_sampling(workers_intervals, indexes)\n\n        # select the chunks and intervals associated to this worker\n        worker_rank = self.worker_env.rank\n        self.num_chunks = len(workers_intervals[worker_rank])\n        self.chunk_index = chunks_index[worker_rank]\n        self.worker_chunks = workers_chunks[worker_rank]\n        self.worker_intervals = workers_intervals[worker_rank]\n\n        # replay the indexes for the current chunks\n        interval = self.worker_intervals[self.chunk_index]\n        current_indexes = np.arange(interval[0], interval[1])\n\n        # re-shuffle the indexes\n        current_indexes = self.shuffler(current_indexes, self.num_chunks, self.current_epoch, self.chunk_index)\n\n        # skip any indexes already consumed\n        current_indexes = current_indexes[indexes[worker_rank] :]\n        self.current_indexes = current_indexes\n\n        self.global_index = num_samples_y"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._state_dict = state_dict\n        self._validate_state_dict()\n\n        # TODO: Implement elastic sampling where the number of workers, ranks can change.\n        num_workers = state_dict[\"num_workers\"]\n        batch_size = state_dict[\"batch_size\"]\n\n        # replay sampling from each worker / chunks using the batch size\n        workers_chunks, workers_intervals = _associate_chunks_to_workers(\n            num_workers, self.worker_env, self.worker_chunks, self.worker_intervals\n        )\n        indexes = _replay_sampling(state_dict[\"num_samples_yielded\"], batch_size, num_workers)\n        chunks_index, indexes = _replay_chunks_sampling(workers_intervals, indexes)\n\n        # select the chunks and intervals associated to this worker\n        worker_rank = self.worker_env.rank\n        self.num_chunks = len(workers_intervals[worker_rank])\n        self.chunk_index = chunks_index[worker_rank]\n        self.worker_chunks = workers_chunks[worker_rank]\n        self.worker_intervals = workers_intervals[worker_rank]\n\n        # replay the indexes for the current chunks\n        interval = self.worker_intervals[self.chunk_index]\n        current_indexes = np.arange(interval[0], interval[1])\n\n        assert self.shuffler is not None\n        assert self.num_chunks is not None\n        current_indexes = self.shuffler(current_indexes, self.num_chunks, self.current_epoch, self.chunk_index)\n\n        # re-shuffle the indexes\n        current_indexes = current_indexes[indexes[worker_rank] :]\n        self.current_indexes = current_index"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict\n        self.worker_env = _WorkerEnv.detect()\n        self.distributed_env = _DistributedEnv.detect()\n\n        self._validate_state_dict()\n\n        self.cache = self._create_cache(worker_env=self.worker_env)\n        self.shuffler = self._create_shuffler(self.cache)\n\n        state: Dict[str, Any] = self._state_dict\n\n        num_workers = state[\"num_workers\"]\n        batch_size = state[\"batch_size\"]\n\n        # TODO: Implement elastic sampling where the number of workers, ranks can change.\n        num_samples_yielded = self._state_dict[\"num_samples_yielded\"]\n\n        # replay sampling from each worker / chunks using the batch size\n        workers_chunks, workers_intervals = _associate_chunks_to_workers(\n            num_workers, self.worker_env, self.worker_chunks, self.worker_intervals\n        )\n        indexes = _replay_sampling(num_samples_yielded, batch_size, num_workers)\n        chunks_index, indexes = _replay_chunks_sampling(workers_intervals, indexes)\n\n        # select the chunks and intervals associated to this worker\n        worker_rank = self.worker_env.rank\n        self.num_chunks = len(workers_intervals[worker_rank])\n        self.chunk_index = chunks_index[worker_rank]\n        self.worker_chunks = workers_chunks[worker_rank]\n        self.worker_intervals = workers_intervals[worker_rank]\n\n        # replay the indexes for the current chunks\n        interval = self.worker_intervals[self.chunk_index]\n        current_indexes = np.arange(interval[0], interval[1])\n\n        # re-shuffle the indexes\n        current_indexes = self.shuffler(current_indexes, self.num_"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._validate_state_dict()\n\n        self._state_dict = state_dict\n        self.worker_env = _WorkerEnv.detect()\n        self.cache = self._create_cache(worker_env=self.worker_env)\n        self.shuffler = self._create_shuffler(self.cache)\n\n        num_workers = state_dict[\"num_workers\"]\n        batch_size = state_dict[\"batch_size\"]\n\n        # TODO: Implement elastic sampling where the number of workers, ranks can change.\n        num_samples_yielded = self._state_dict[\"num_samples_yielded\"]\n\n        # replay sampling from each worker / chunks using the batch size\n        workers_chunks, workers_intervals = _associate_chunks_to_workers(\n            num_workers, self.worker_env, self.worker_chunks, self.worker_intervals\n        )\n        indexes = _replay_sampling(num_samples_yielded, batch_size, num_workers)\n        chunks_index, indexes = _replay_chunks_sampling(workers_intervals, indexes)\n\n        # select the chunks and intervals associated to this worker\n        worker_rank = self.worker_env.rank\n        self.num_chunks = len(workers_intervals[worker_rank])\n        self.chunk_index = chunks_index[worker_rank]\n        self.worker_chunks = workers_chunks[worker_rank]\n        self.worker_intervals = workers_intervals[worker_rank]\n\n        # replay the indexes for the current chunks\n        interval = self.worker_intervals[self.chunk_index]\n        current_indexes = np.arange(interval[0], interval[1])\n\n        # re-shuffle the indexes\n        current_indexes = self.shuffler(current_indexes, self.num_chunks, self.current_epoch, self.chunk_index)\n\n        # skip any indexes already consumed\n        current_indexes"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if self._state_dict is not None:\n            raise RuntimeError(\"The method `load_state_dict` should only be called once.\")\n\n        self._validate_state_dict()\n\n        self._state_dict = state_dict\n        self.current_epoch = state_dict[\"current_epoch\"]\n        self.worker_env = _WorkerEnv.detect()\n        self.cache = self._create_cache(worker_env=self.worker_env)\n        self.shuffler = self._create_shuffler(self.cache)\n\n        num_workers = state_dict[\"num_workers\"]\n        batch_size = state_dict[\"batch_size\"]\n\n        # TODO: Implement elastic sampling where the number of workers, ranks can change.\n        num_samples_yielded = state_dict[\"num_samples_yielded\"]\n\n        # replay sampling from each worker / chunks using the batch size\n        workers_chunks, workers_intervals = _associate_chunks_to_workers(\n            num_workers, self.worker_env, self.worker_chunks, self.worker_intervals\n        )\n        indexes = _replay_sampling(num_samples_yielded, batch_size, num_workers)\n        chunks_index, indexes = _replay_chunks_sampling(workers_intervals, indexes)\n\n        # select the chunks and intervals associated to this worker\n        worker_rank = self.worker_env.rank\n        self.num_chunks = len(workers_intervals[worker_rank])\n        self.chunk_index = chunks_index[worker_rank]\n        self.worker_chunks = workers_chunks[worker_rank]\n        self.worker_intervals = workers_intervals[worker_rank]\n\n        # replay the indexes for the current chunks\n        interval = self.worker_intervals[self.chunk_index]\n        current_indexes = np.arange(interval[0], interval[1])\n\n        # re-shuffle the indexes\n        current_"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict\n        self.worker_env = _WorkerEnv.detect()\n        self._validate_state_dict()\n\n        self.cache = self._create_cache(worker_env=self.worker_env)\n        self.shuffler = self._create_shuffler(self.cache)\n\n        # Handle restart\n        state: Dict[str, Any] = self._state_dict\n        self.current_epoch = state[\"current_epoch\"]\n\n        chunks_per_replica, intervals_per_replica = self.shuffler.get_chunks_and_intervals_per_ranks(\n            self.distributed_env, self.current_epoch\n        )\n        chunks_replica = chunks_per_replica[self.distributed_env.global_rank % self.distributed_env.world_size]\n        intervals_replica = intervals_per_replica[self.distributed_env.global_rank % self.distributed_env.world_size]\n\n        self._resume(chunks_replica, intervals_replica)"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._state_dict = state_dict\n        self._validate_state_dict()\n\n        self.worker_env = _WorkerEnv.detect()\n        self.cache = self._create_cache(worker_env=self.worker_env)\n        self.shuffler = self._create_shuffler(self.cache)\n\n        state: Dict[str, Any] = self._state_dict\n\n        num_workers = state[\"num_workers\"]\n        batch_size = state[\"batch_size\"]\n        num_samples_yielded = self._state_dict[\"num_samples_yielded\"]\n\n        # replay sampling from each worker / chunks using the batch size\n        workers_chunks, workers_intervals = _associate_chunks_to_workers(\n            num_workers, self.worker_env, self.cache._get_chunks_indexes(), self.cache._get_chunks_intervals()\n        )\n        indexes = _replay_sampling(num_samples_yielded, batch_size, num_workers)\n        chunks_index, indexes = _replay_chunks_sampling(workers_intervals, indexes)\n\n        # select the chunks and intervals associated to this worker\n        worker_rank = self.worker_env.rank\n        self.num_chunks = len(workers_intervals[worker_rank])\n        self.chunk_index = chunks_index[worker_rank]\n        self.worker_chunks = workers_chunks[worker_rank]\n        self.worker_intervals = workers_intervals[worker_rank]\n\n        # replay the indexes for the current chunks\n        interval = self.worker_intervals[self.chunk_index]\n        current_indexes = np.arange(interval[0], interval[1])\n\n        # re-shuffle the indexes\n        current_indexes = self."}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict\n\n        # Validate the state dict\n        self._validate_state_dict()\n\n        # replay sampling from each worker / chunks using the batch size\n        num_workers = state_dict[\"num_workers\"]\n        batch_size = state_dict[\"batch_size\"]\n        num_samples_yielded = state_dict[\"num_samples_yielded\"]\n\n        chunks_replica = state_dict[\"chunks_replica\"]\n        intervals_replica = state_dict[\"intervals_replica\"]\n\n        workers_chunks, workers_intervals = _associate_chunks_to_workers(\n            num_workers, self.worker_env, chunks_replica, intervals_replica\n        )\n        indexes = _replay_sampling(num_samples_yielded, batch_size, num_workers)\n        chunks_index, indexes = _replay_chunks_sampling(workers_intervals, indexes)\n\n        # select the chunks and intervals associated to this worker\n        worker_rank = self.worker_env.rank\n        self.num_chunks = len(workers_intervals[worker_rank])\n        self.chunk_index = chunks_index[worker_rank]\n        self.worker_chunks = workers_chunks[worker_rank]\n        self.worker_intervals = workers_intervals[worker_rank]\n\n        # replay the indexes for the current chunks\n        interval = self.worker_intervals[self.chunk_index]\n        current_indexes = np.arange(interval[0], interval[1])\n\n        assert self.shuffler is not None\n        assert self.num_chunks is not None\n        self.current_indexes = self.shuffler(current_indexes, self.num_chunks, self.current_epoch, self.chunk_index)\n\n        # skip any indexes already consumed\n        self.current_indexes = self.current_indexes[indexes[worker_rank] :]\n\n        self"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._state_dict = state_dict\n\n        if self._state_dict is None:\n            return\n\n        self.distributed_env = _DistributedEnv.detect()\n        self.worker_env = _WorkerEnv.detect()\n\n        self.cache = self._create_cache(worker_env=self.worker_env)\n\n        self.shuffler = self._create_shuffler(self.cache)\n\n        self._validate_state_dict()\n\n        state: Dict[str, Any] = self._state_dict\n\n        num_workers = state[\"num_workers\"]\n        batch_size = state[\"batch_size\"]\n\n        # TODO: Implement elastic sampling where the number of workers, ranks can change.\n        num_samples_yielded = self._state_dict[\"num_samples_yielded\"]\n\n        # replay sampling from each worker / chunks using the batch size\n        workers_chunks, workers_intervals = _associate_chunks_to_workers(\n            num_workers, self.worker_env, self.worker_chunks, self.worker_intervals\n        )\n        indexes = _replay_sampling(num_samples_yielded, batch_size, num_workers)\n        chunks_index, indexes = _replay_chunks_sampling(workers_intervals, indexes)\n\n        # select the chunks and intervals associated to this worker\n        worker_rank = self.worker_env.rank\n        self.num_chunks = len(workers_intervals[worker_rank])\n        self.chunk_index = chunks_index[worker_rank]\n        self.worker_chunks = workers_chunks[worker_rank]\n        self.worker_intervals = workers_intervals[worker_rank]\n\n        # replay the indexes for the current chunks\n        interval = self.worker_intervals[self."}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict\n        self.worker_env = _WorkerEnv.detect()\n        self.cache = self._create_cache(worker_env=self.worker_env)\n        self.shuffler = self._create_shuffler(self.cache)\n\n        self._validate_state_dict()\n\n        self.current_epoch = state_dict[\"current_epoch\"]\n        self.num_chunks = state_dict[\"num_workers\"]\n        self.chunk_index = state_dict[\"num_samples_yielded\"] // state_dict[\"batch_size\"]\n        self.has_triggered_download = True\n        self.global_index = state_dict[\"num_samples_yielded\"]\n\n        self.worker_chunks = []\n        self.worker_intervals = []\n\n        for i, (chunk_index, chunk_interval) in enumerate(zip(self.cache._chunks, self.cache._intervals)):\n            if i % self.worker_env.world_size != self.worker_env.rank:\n                continue\n            self.worker_chunks.append(chunk_index)\n            self.worker_intervals.append(chunk_interval)\n\n        self.num_chunks = len(self.worker_chunks)\n\n        self.current_indexes = []\n\n        interval = self.worker_intervals[self.chunk_index]\n        current_indexes = np.arange(interval[0], interval[1])\n\n        assert self.shuffler is not None\n        assert self.num_chunks is not None\n        self.current_indexes = self.shuffler(current_indexes, self.num_chunks, self.current_epoch, self.chunk_index)\n\n        self.chunk_index += 1"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        if state_dict is None:\n            raise ValueError(\"The provided state_dict is None.\")\n\n        self._state_dict = state_dict\n\n        self.distributed_env = _DistributedEnv.detect()\n        self.worker_env = _WorkerEnv.detect()\n\n        self.input_dir.path = state_dict[\"input_dir_path\"]\n        self.input_dir.url = state_dict[\"input_dir_url\"]\n\n        self.shuffle = state_dict[\"shuffle\"]\n        self.drop_last = state_dict[\"drop_last\"]\n        self.seed = state_dict[\"seed\"]\n        self.current_epoch = state_dict[\"current_epoch\"]\n\n        self.cache = self._create_cache(worker_env=self.worker_env)\n        self.shuffler = self._create_shuffler(self.cache)\n\n        self.item_loader = BaseItemLoader.from_state_dict(state_dict[\"item_loader\"])\n\n        self._validate_state_dict()"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict\n        self._validate_state_dict()\n\n        self.distributed_env = _DistributedEnv(state_dict[\"world_size\"])\n        self.worker_env = _WorkerEnv(state_dict[\"num_workers\"])\n        self.shuffle = state_dict[\"shuffle\"]\n        self.seed = state_dict[\"seed\"]\n        self.drop_last = state_dict[\"drop_last\"]\n        self.current_epoch = state_dict[\"current_epoch\"]\n\n        self.input_dir = Dir(\n            path=state_dict[\"input_dir_path\"],\n            url=state_dict[\"input_dir_url\"],\n        )\n\n        if state_dict[\"item_loader\"]:\n            self.item_loader = BaseItemLoader.from_state_dict(state_dict[\"item_loader\"])\n\n        self.cache = self._create_cache(worker_env=self.worker_env)\n        self.shuffler = self._create_shuffler(self.cache)\n\n        self.num_chunks = len(self.worker_chunks)\n\n        self.has_triggered_download = False\n\n        self.last_time = time()\n\n        return self"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict\n        self._validate_state_dict()\n\n        self.distributed_env = _DistributedEnv(\n            rank=self._state_dict[\"rank\"],\n            local_rank=self._state_dict[\"local_rank\"],\n            world_size=self._state_dict[\"world_size\"],\n        )\n\n        self.worker_env = _WorkerEnv(\n            rank=self._state_dict[\"rank\"],\n            local_rank=self._state_dict[\"local_rank\"],\n            world_size=self._state_dict[\"world_size\"],\n        )\n\n        self.cache = self._create_cache(worker_env=self.worker_env)\n        self.shuffler = self._create_shuffler(self.cache)\n\n        self.current_epoch = self._state_dict[\"current_epoch\"]\n\n        # Handle restart\n        if self._state_dict:\n            self._resume(\n                chunks_replica=self._state_dict[\"chunks_replica\"],\n                intervals_replica=self._state_dict[\"intervals_replica\"],\n            )\n\n        self.has_triggered_download = False\n        self.last_time = time()"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        if self._state_dict is not None:\n            self._validate_state_dict()\n\n        self._state_dict = state_dict\n\n        self.worker_env = _WorkerEnv.detect()\n        self.distributed_env = _DistributedEnv.detect()\n\n        self.cache = self._create_cache(worker_env=self.worker_env)\n        self.shuffler = self._create_shuffler(self.cache)\n\n        self.min_items_per_replica = self.shuffler.get_min_items_per_ranks(\n            self.distributed_env, self.current_epoch\n        )\n\n        self.random_state = np.random.RandomState(seed=self.seed)\n\n        self.worker_chunks = []\n        self.worker_intervals = []\n\n        for i, (chunk_index, chunk_interval) in enumerate(zip(self._state_dict[\"chunks\"], self._state_dict[\"intervals\"])):\n            if i % self.worker_env.world_size != self.worker_env.rank:\n                continue\n            self.worker_chunks.append(chunk_index)\n            self.worker_intervals.append(chunk_interval)\n\n        self.num_chunks = len(self.worker_chunks)\n\n        self.current_indexes = []\n        self.chunk_index = 0\n        self.global_index = 0\n        self.index = 0\n\n        self.has_triggered_download = False\n        self.last_time = time()"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._state_dict = state_dict\n\n        # Validate the state dict\n        self._validate_state_dict()\n\n        # Update the input dir\n        state: Dict[str, Any] = self._state_dict\n        self.input_dir.path = state[\"input_dir_path\"]\n        self.input_dir.url = state[\"input_dir_url\"]\n\n        # Update the item loader\n        self.item_loader = BaseItemLoader.from_state_dict(state[\"item_loader\"]) if state[\"item_loader\"] else None\n\n        # Update the shuffle\n        self.shuffle = state[\"shuffle\"]\n\n        # Update the drop last\n        self.drop_last = state[\"drop_last\"]\n\n        # Update the seed\n        self.seed = state[\"seed\"]\n\n        # Update the distributed env\n        self.distributed_env = _DistributedEnv(state[\"world_size\"])\n\n        # Update the current epoch\n        self.current_epoch = state[\"current_epoch\"]\n\n        # Update the min items per replica\n        self.min_items_per_replica = None\n\n        # Update the random state\n        self.random_state = None\n\n        # Update the shuffler\n        self.shuffler = None\n\n        # Update the serializers\n        self.serializers = None\n\n        # Update the cache\n        self.cache = None\n\n        # Update the worker env\n        self.worker_env = _WorkerEnv(state[\"num_workers\"])\n\n        # Update the worker chunks\n        self.worker_chunks = []\n\n        # Update the worker intervals\n        self.worker_intervals = []\n\n        # Update the current indexes\n        self.current_indexes = []\n\n        # Update the chunk index\n        self.chunk_index = 0\n\n        # Update the num chunks\n        self.num_chunks = None\n\n        # Update"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._state_dict = state_dict\n\n        self.input_dir = Dir(\n            path=state_dict[\"input_dir_path\"] if state_dict[\"input_dir_path\"] else state_dict[\"input_dir_url\"]\n        )\n        self.item_loader = BaseItemLoader.from_state_dict(state_dict[\"item_loader\"])\n        self.shuffle = state_dict[\"shuffle\"]\n        self.drop_last = state_dict[\"drop_last\"]\n        self.seed = state_dict[\"seed\"]\n        self.distributed_env = _DistributedEnv(world_size=state_dict[\"world_size\"])\n\n        self.worker_env = _WorkerEnv.detect()\n        self.cache = self._create_cache(worker_env=self.worker_env)\n        self.shuffler = self._create_shuffler(self.cache)\n\n        self._validate_state_dict()\n\n        # Handle restart\n        if self._state_dict:\n            self._resume(self.cache._reader._chunks_per_replica, self.cache._reader._intervals_per_replica)\n\n        self.current_epoch = state_dict[\"current_epoch\"]\n\n    "}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        assert self._state_dict\n        assert self.worker_env\n        assert self.cache\n        assert self.shuffler\n\n        state: Dict[str, Any] = self._state_dict\n\n        num_workers = state[\"num_workers\"]\n        batch_size = state[\"batch_size\"]\n\n        # TODO: Implement elastic sampling where the number of workers, ranks can change.\n        num_samples_yielded = self._state_dict[\"num_samples_yielded\"]\n\n        # replay sampling from each worker / chunks using the batch size\n        workers_chunks, workers_intervals = _associate_chunks_to_workers(\n            num_workers, self.worker_env, self.worker_chunks, self.worker_intervals\n        )\n        indexes = _replay_sampling(num_samples_yielded, batch_size, num_workers)\n        chunks_index, indexes = _replay_chunks_sampling(workers_intervals, indexes)\n\n        # select the chunks and intervals associated to this worker\n        worker_rank = self.worker_env.rank\n        self.num_chunks = len(workers_intervals[worker_rank])\n        self.chunk_index = chunks_index[worker_rank]\n        self.worker_chunks = workers_chunks[worker_rank]\n        self.worker_intervals = workers_intervals[worker_rank]\n\n        # replay the indexes for the current chunks\n        interval = self.worker_intervals[self.chunk_index]\n        current_indexes = np.arange(interval[0], interval[1])\n\n        # re-shuffle the indexes\n        current_indexes = self.shuffler(current_indexes, self.num_chunks, self.current_epoch, self.chunk_index)\n\n        # skip any indexes already consumed\n        current_indexes = current_indexes[indexes[worker_rank] :]\n        self.current_indexes = current_indexes\n\n        self.global_index"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        assert self._state_dict\n        assert self.worker_env\n        assert self.cache\n        assert self.shuffler\n\n        state: Dict[str, Any] = self._state_dict\n\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\n                f\"The number of workers changed from {state['num_workers']} to {self.worker_env.world_size}.\"\n            )\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(f\"The input directory path changed from {state['input_dir_path']} to {self.input_dir.path}.\")\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(f\"The input directory URL changed from {state['input_dir_url']} to {self.input_dir.url}.\")\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(f\"The seed changed from {state['seed']} to {self.seed}.\")\n\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(f\"The drop_last flag changed from {state['drop_last']} to {self.drop_last}.\")\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(f\"The shuffle flag changed from {state['shuffle']} to {self.shuffle}.\")\n\n        if state[\"item_loader\"] != self.item_loader.state_dict():\n            raise ValueError(\n                f\"The item loader state changed from {state['item_loader']} to {self.item_loader.state_dict()}.\"\n            )"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        assert self._state_dict\n        assert self.worker_env\n        assert self.cache\n        assert self.shuffler\n\n        state: Dict[str, Any] = self._state_dict\n\n        num_workers = state[\"num_workers\"]\n        batch_size = state[\"batch_size\"]\n        input_dir_path = state[\"input_dir_path\"]\n        input_dir_url = state[\"input_dir_url\"]\n        drop_last = state[\"drop_last\"]\n        seed = state[\"seed\"]\n        shuffle = state[\"shuffle\"]\n        world_size = state[\"world_size\"]\n\n        if num_workers != self.worker_env.world_size:\n            raise ValueError(f\"The number of workers changed from {num_workers} to {self.worker_env.world_size}\")\n\n        if batch_size != self.worker_env.batch_size:\n            raise ValueError(f\"The batch size changed from {batch_size} to {self.worker_env.batch_size}\")\n\n        if input_dir_path != self.input_dir.path:\n            raise ValueError(f\"The input dir path changed from {input_dir_path} to {self.input_dir.path}\")\n\n        if input_dir_url != self.input_dir.url:\n            raise ValueError(f\"The input dir url changed from {input_dir_url} to {self.input_dir.url}\")\n\n        if drop_last != self.drop_last:\n            raise ValueError(f\"The drop_last flag changed from {drop_last} to {self.drop_last}\")\n\n        if seed != self.seed:\n            raise ValueError(f\"The seed changed from {seed} to {self.seed}\")\n\n        if shuffle != self.shuffle:\n            raise ValueError(f\"The shuffle flag changed from {shuffle} to {self.shuffle}\")\n\n        if world_size != self.distributed_env.world_size:\n            raise ValueError(f\"The world"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        assert self._state_dict\n        assert self.worker_env\n        assert self.cache\n        assert self.shuffler\n\n        state: Dict[str, Any] = self._state_dict\n\n        num_workers = state[\"num_workers\"]\n        batch_size = state[\"batch_size\"]\n        input_dir_path = state[\"input_dir_path\"]\n        input_dir_url = state[\"input_dir_url\"]\n        drop_last = state[\"drop_last\"]\n        seed = state[\"seed\"]\n        shuffle = state[\"shuffle\"]\n        world_size = state[\"world_size\"]\n\n        if (\n            num_workers != self.worker_env.world_size\n            or batch_size != self.worker_env.batch_size\n            or input_dir_path != self.input_dir.path\n            or input_dir_url != self.input_dir.url\n            or drop_last != self.drop_last\n            or seed != self.seed\n            or shuffle != self.shuffle\n            or world_size != self.distributed_env.world_size\n        ):\n            raise ValueError(\n                f\"The state_dict is not consistent with the current state of the StreamingDataset. \"\n                f\"num_workers: {num_workers} != {self.worker_env.world_size} \"\n                f\"batch_size: {batch_size} != {self.worker_env.batch_size} \"\n                f\"input_dir_path: {input_dir_path} != {self.input_dir.path} \"\n                f\"input_dir_url: {input_dir_url} != {self.input_dir.url} \"\n                f\"drop_last: {drop_last} != {self.drop_last} \"\n                f\"seed: {seed} != {self.seed} \"\n                f\"shuffle: {shuffle} != {self.shuffle} \"\n                f\"world_size: {world_size} != {self.distributed"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            raise ValueError(\"The state dict is not set.\")\n\n        state: Dict[str, Any] = self._state_dict\n\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\"The number of workers does not match.\")\n\n        if state[\"batch_size\"] != self.worker_env.batch_size:\n            raise ValueError(\"The batch size does not match.\")\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\"The input directory path does not match.\")\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\"The input directory URL does not match.\")\n\n        if state[\"item_loader\"] is not None:\n            if self.item_loader is None:\n                raise ValueError(\"The item loader is not set.\")\n            if state[\"item_loader\"] != self.item_loader.state_dict():\n                raise ValueError(\"The item loader state does not match.\")\n\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(\"The drop last flag does not match.\")\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(\"The seed does not match.\")\n\n        if state[\"world_size\"] != self.distributed_env.world_size:\n            raise ValueError(\"The world size does not match.\")\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\"The shuffle flag does not match.\")"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        state: Dict[str, Any] = self._state_dict\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"The StreamingDataset was created with shuffle={self.shuffle} but the state_dict was created with shuffle={state['shuffle']}.\"\n            )\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\n                f\"The StreamingDataset was created with num_workers={self.worker_env.world_size} but the state_dict was created with num_workers={state['num_workers']}.\"\n            )\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The StreamingDataset was created with input_dir_path={self.input_dir.path} but the state_dict was created with input_dir_path={state['input_dir_path']}.\"\n            )\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The StreamingDataset was created with input_dir_url={self.input_dir.url} but the state_dict was created with input_dir_url={state['input_dir_url']}.\"\n            )\n        if state[\"seed\"] != self.seed:\n            raise ValueError(\n                f\"The StreamingDataset was created with seed={self.seed} but the state_dict was created with seed={state['seed']}.\"\n            )\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                f\"The StreamingDataset was created with drop_last={self.drop_last} but the state_dict was created with drop_last={state['drop_last']}.\"\n            )\n        if state[\"item_loader\"] is not None:\n            if self.item_loader is None:\n                raise ValueError(\n                    f\"The StreamingDataset was created with item_loader"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            raise ValueError(\"The state_dict is not set.\")\n\n        state: Dict[str, Any] = self._state_dict\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(f\"The shuffle is not the same. Expected: {self.shuffle}, found: {state['shuffle']}\")\n\n        if state[\"world_size\"] != self.distributed_env.world_size:\n            raise ValueError(\n                f\"The world_size is not the same. Expected: {self.distributed_env.world_size}, found: {state['world_size']}\"\n            )\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The input_dir_path is not the same. Expected: {self.input_dir.path}, found: {state['input_dir_path']}\"\n            )\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The input_dir_url is not the same. Expected: {self.input_dir.url}, found: {state['input_dir_url']}\"\n            )\n\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(f\"The drop_last is not the same. Expected: {self.drop_last}, found: {state['drop_last']}\")\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(f\"The seed is not the same. Expected: {self.seed}, found: {state['seed']}\")\n\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\n                f\"The num_workers is not the same. Expected: {self.worker_env.world_size}, found: {state['num_workers']}\"\n            )\n\n        if state[\"batch_size\"] != self.worker_env.batch_size:"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            raise ValueError(\"The state_dict is not initialized.\")\n\n        state: Dict[str, Any] = self._state_dict\n\n        num_workers = state[\"num_workers\"]\n        batch_size = state[\"batch_size\"]\n\n        if self.distributed_env.world_size != state[\"world_size\"]:\n            raise ValueError(\"The world size is not the same.\")\n\n        if self.shuffle != state[\"shuffle\"]:\n            raise ValueError(\"The shuffle is not the same.\")\n\n        if self.seed != state[\"seed\"]:\n            raise ValueError(\"The seed is not the same.\")\n\n        if self.drop_last != state[\"drop_last\"]:\n            raise ValueError(\"The drop_last is not the same.\")\n\n        if self.input_dir.path != state[\"input_dir_path\"]:\n            raise ValueError(\"The input_dir_path is not the same.\")\n\n        if self.input_dir.url != state[\"input_dir_url\"]:\n            raise ValueError(\"The input_dir_url is not the same.\")\n\n        if self.item_loader and state[\"item_loader\"] is not None:\n            self.item_loader.load_state_dict(state[\"item_loader\"])"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        state: Dict[str, Any] = self._state_dict\n\n        if state[\"world_size\"] != self.distributed_env.world_size:\n            raise ValueError(\n                f\"The world size of the dataset ({state['world_size']}) is different from the world size of the current process ({self.distributed_env.world_size}).\"\n            )\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(f\"The shuffle state of the dataset ({state['shuffle']}) is different from the current process ({self.shuffle}).\")\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(f\"The seed of the dataset ({state['seed']}) is different from the current process ({self.seed}).\")\n\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                f\"The drop_last state of the dataset ({state['drop_last']}) is different from the current process ({self.drop_last}).\"\n            )\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(f\"The input_dir_path of the dataset ({state['input_dir_path']}) is different from the current process ({self.input_dir.path}).\")\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(f\"The input_dir_url of the dataset ({state['input_dir_url']}) is different from the current process ({self.input_dir.url}).\")\n\n        if state[\"item_loader\"] is not None and self.item_loader is not None:\n            self.item_loader.load_state_dict(state[\"item_loader\"])\n        elif state[\"item_loader\"] is not None and self.item_loader is None:\n            raise ValueError(f\"The item_loader of the dataset ({state['item_loader']}) is different from the current process ({self.item_loader}).\")\n        elif state[\"item_"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            raise ValueError(\"The state dict is empty.\")\n\n        state: Dict[str, Any] = self._state_dict\n\n        if self.worker_env is None:\n            self.worker_env = _WorkerEnv.detect()\n\n        if self.cache is None:\n            self.cache = self._create_cache(worker_env=self.worker_env)\n\n        if self.shuffler is None:\n            self.shuffler = self._create_shuffler(self.cache)\n\n        if self.shuffle != state[\"shuffle\"]:\n            raise ValueError(f\"The shuffle flag is different. Found {self.shuffle} vs {state['shuffle']}\")\n\n        if self.distributed_env.world_size != state[\"world_size\"]:\n            raise ValueError(f\"The world size is different. Found {self.distributed_env.world_size} vs {state['world_size']}\")\n\n        if self.input_dir.path != state[\"input_dir_path\"]:\n            raise ValueError(f\"The input dir path is different. Found {self.input_dir.path} vs {state['input_dir_path']}\")\n\n        if self.input_dir.url != state[\"input_dir_url\"]:\n            raise ValueError(f\"The input dir url is different. Found {self.input_dir.url} vs {state['input_dir_url']}\")\n\n        if self.item_loader is None and state[\"item_loader\"] is not None:\n            raise ValueError(f\"The item loader is different. Found {self.item_loader} vs {state['item_loader']}\")\n\n        if self.item_loader is not None and state[\"item_loader\"] is None:\n            raise ValueError(f\"The item loader is different. Found {self.item_loader} vs {state['item_loader']}\")\n\n        if self.item_loader is not None and state[\"item_loader\"] is not None:\n            self.item_loader.load_state"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        state: Dict[str, Any] = self._state_dict\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(f\"The provided state_dict is inconsistent with the shuffle value. {state['shuffle']} != {self.shuffle}\")\n        if state[\"world_size\"] != self.distributed_env.world_size:\n            raise ValueError(\n                f\"The provided state_dict is inconsistent with the world_size value. {state['world_size']} != {self.distributed_env.world_size}\"\n            )\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The provided state_dict is inconsistent with the input_dir_path value. {state['input_dir_path']} != {self.input_dir.path}\"\n            )\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The provided state_dict is inconsistent with the input_dir_url value. {state['input_dir_url']} != {self.input_dir.url}\"\n            )\n        if state[\"seed\"] != self.seed:\n            raise ValueError(f\"The provided state_dict is inconsistent with the seed value. {state['seed']} != {self.seed}\")\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                f\"The provided state_dict is inconsistent with the drop_last value. {state['drop_last']} != {self.drop_last}\"\n            )\n        if state[\"item_loader\"] != self.item_loader.state_dict():\n            raise ValueError(\n                f\"The provided state_dict is inconsistent with the item_loader value. {state['item_loader']} != {self.item_loader.state_dict()}\"\n            )"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            return\n        if self.worker_env is None:\n            self.worker_env = _WorkerEnv.detect()\n        if self.cache is None:\n            self.cache = self._create_cache(worker_env=self.worker_env)\n        if self.shuffler is None:\n            self.shuffler = self._create_shuffler(self.cache)\n\n        state: Dict[str, Any] = self._state_dict\n        if state[\"world_size\"] != self.distributed_env.world_size:\n            raise ValueError(\n                f\"The world size of the state dict ({state['world_size']}) does not match the current world size ({self.distributed_env.world_size}).\"\n            )\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(f\"The shuffle state of the state dict ({state['shuffle']}) does not match the current shuffle state ({self.shuffle}).\")\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                f\"The drop_last state of the state dict ({state['drop_last']}) does not match the current drop_last state ({self.drop_last}).\"\n            )\n        if state[\"seed\"] != self.seed:\n            raise ValueError(f\"The seed of the state dict ({state['seed']}) does not match the current seed ({self.seed}).\")\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The input directory path of the state dict ({state['input_dir_path']}) does not match the current input directory path ({self.input_dir.path}).\"\n            )\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The input directory URL of the state dict ({state['input_dir_url']}) does not match the current input directory URL ({self.input_dir.url"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        state: Dict[str, Any] = self._state_dict\n\n        if self.worker_env is None:\n            self.worker_env = _WorkerEnv.detect()\n\n        if self.cache is None:\n            self.cache = self._create_cache(worker_env=self.worker_env)\n\n        if self.shuffler is None:\n            self.shuffler = self._create_shuffler(self.cache)\n\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\"The number of workers is different from the state.\")\n\n        if state[\"batch_size\"] != self.worker_env.batch_size:\n            raise ValueError(\"The batch size is different from the state.\")\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\"The input directory path is different from the state.\")\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\"The input directory url is different from the state.\")\n\n        if state[\"item_loader\"] is not None and self.item_loader is not None:\n            if state[\"item_loader\"][\"type\"] != self.item_loader.type:\n                raise ValueError(\"The item loader type is different from the state.\")\n            if state[\"item_loader\"][\"state\"] != self.item_loader.state_dict():\n                raise ValueError(\"The item loader state is different from the state.\")\n\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(\"The drop_last flag is different from the state.\")\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(\"The seed is different from the state.\")\n\n        if state[\"world_size\"] != self.distributed_env.world_size:\n            raise ValueError(\"The world size is different from the state.\")\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\"The shuffle flag is different from the state.\")"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `_validate_state_dict` should only be called in the main process.\")\n\n        if self._state_dict is None:\n            raise ValueError(\"The state dictionary is empty.\")\n\n        state = self._state_dict\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"The shuffle state is different between the state dictionary and the current state of the StreamingDataset instance. \"\n                f\"Expected: {self.shuffle}, found: {state['shuffle']}.\"\n            )\n\n        if state[\"world_size\"] != self.distributed_env.world_size:\n            raise ValueError(\n                f\"The world size is different between the state dictionary and the current state of the StreamingDataset instance. \"\n                f\"Expected: {self.distributed_env.world_size}, found: {state['world_size']}.\"\n            )\n\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                f\"The drop_last state is different between the state dictionary and the current state of the StreamingDataset instance. \"\n                f\"Expected: {self.drop_last}, found: {state['drop_last']}.\"\n            )\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(\n                f\"The seed is different between the state dictionary and the current state of the StreamingDataset instance. \"\n                f\"Expected: {self.seed}, found: {state['seed']}.\"\n            )\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The input directory path is different between the state dictionary and the current state of the StreamingDataset instance. \"\n                f\"Expected: {self.input_dir.path}, found: {state['input_dir_path']}.\"\n            )\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\""}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        state = self._state_dict\n        if state[\"world_size\"] != self.distributed_env.world_size:\n            raise ValueError(\"The world size of the state dictionary does not match the current world size.\")\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\"The shuffle state of the state dictionary does not match the current shuffle state.\")\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\"The input directory path of the state dictionary does not match the current input directory path.\")\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\"The input directory URL of the state dictionary does not match the current input directory URL.\")\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(\"The seed of the state dictionary does not match the current seed.\")\n\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(\"The drop_last state of the state dictionary does not match the current drop_last state.\")\n\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\"The number of workers of the state dictionary does not match the current number of workers.\")\n\n        if state[\"item_loader\"] is not None and self.item_loader is not None:\n            if state[\"item_loader\"][\"class_name\"] != self.item_loader.__class__.__name__:\n                raise ValueError(\"The item_loader class name of the state dictionary does not match the current item_loader class name.\")\n\n            if state[\"item_loader\"][\"state_dict\"] != self.item_loader.state_dict():\n                raise ValueError(\"The item_loader state dictionary of the state dictionary does not match the current item_loader state dictionary.\")\n\n        if state[\"batch_size\"] != self.worker_env.batch_size:\n            raise ValueError(\"The batch size of the state dictionary does not match the current batch size.\")"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            return\n\n        state: Dict[str, Any] = self._state_dict\n\n        # check that the state dict is valid\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\n                f\"The number of workers in the state dict ({state['num_workers']}) does not match the current number of workers ({self.worker_env.world_size}).\"\n            )\n\n        if state[\"batch_size\"] != self.worker_env.batch_size:\n            raise ValueError(\n                f\"The batch size in the state dict ({state['batch_size']}) does not match the current batch size ({self.worker_env.batch_size}).\"\n            )\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The input directory path in the state dict ({state['input_dir_path']}) does not match the current input directory path ({self.input_dir.path}).\"\n            )\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The input directory URL in the state dict ({state['input_dir_url']}) does not match the current input directory URL ({self.input_dir.url}).\"\n            )\n\n        if state[\"item_loader\"] and self.item_loader and state[\"item_loader\"] != self.item_loader.state_dict():\n            raise ValueError(\n                f\"The item loader state in the state dict ({state['item_loader']}) does not match the current item loader state ({self.item_loader.state_dict()}).\"\n            )\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(f\"The seed in the state dict ({state['seed']}) does not match the current seed ({self.seed}).\")\n\n        if state[\"world_size\"] != self.distributed_env.world_size:\n            raise ValueError(\n               "}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `_validate_state_dict` should only be called in the main process.\")\n\n        assert self._state_dict\n        assert self.worker_env\n        assert self.cache\n        assert self.shuffler\n\n        state: Dict[str, Any] = self._state_dict\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(f\"The state dict contains shuffle={state['shuffle']}, but the StreamingDataset is not.\")\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(f\"The state dict contains seed={state['seed']}, but the StreamingDataset is not.\")\n\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(f\"The state dict contains drop_last={state['drop_last']}, but the StreamingDataset is not.\")\n\n        if state[\"world_size\"] != self.distributed_env.world_size:\n            raise ValueError(\n                f\"The state dict contains world_size={state['world_size']}, but the StreamingDataset is not.\"\n            )\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The state dict contains input_dir_path={state['input_dir_path']}, but the StreamingDataset is not.\"\n            )\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The state dict contains input_dir_url={state['input_dir_url']}, but the StreamingDataset is not.\"\n            )\n\n        if state[\"item_loader\"] is not None and self.item_loader is not None:\n            self.item_loader.load_state_dict(state[\"item_loader\"])"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            return\n\n        state: Dict[str, Any] = self._state_dict\n        if self.worker_env is None:\n            self.worker_env = _WorkerEnv.detect()\n\n        if self.cache is None:\n            self.cache = self._create_cache(worker_env=self.worker_env)\n\n        if self.shuffler is None:\n            self.shuffler = self._create_shuffler(self.cache)\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The input directory path provided to the StreamingDataset `{state['input_dir_path']}` \"\n                f\"doesn't match the current input directory path `{self.input_dir.path}`.\"\n            )\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The input directory URL provided to the StreamingDataset `{state['input_dir_url']}` \"\n                f\"doesn't match the current input directory URL `{self.input_dir.url}`.\"\n            )\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"The shuffle parameter provided to the StreamingDataset `{state['shuffle']}` \"\n                f\"doesn't match the current shuffle parameter `{self.shuffle}`.\"\n            )\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(\n                f\"The seed parameter provided to the StreamingDataset `{state['seed']}` \"\n                f\"doesn't match the current seed parameter `{self.seed}`.\"\n            )\n\n        if state[\"world_size\"] != self.distributed_env.world_size:\n            raise ValueError(\n                f\"The world size parameter provided to the StreamingDataset `{state['world_size']}` \"\n                f\"doesn't match the current world"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            raise ValueError(\"The state dict is not initialized.\")\n\n        state: Dict[str, Any] = self._state_dict\n\n        if self.worker_env is None:\n            self.worker_env = _WorkerEnv.detect()\n        if self.cache is None:\n            self.cache = self._create_cache(worker_env=self.worker_env)\n        if self.shuffler is None:\n            self.shuffler = self._create_shuffler(self.cache)\n\n        if self.cache.input_dir.path != state[\"input_dir_path\"]:\n            raise ValueError(\n                f\"The input_dir_path in the state dict {state['input_dir_path']} \"\n                f\"doesn't match the current input_dir_path {self.cache.input_dir.path}\"\n            )\n\n        if self.cache.input_dir.url != state[\"input_dir_url\"]:\n            raise ValueError(\n                f\"The input_dir_url in the state dict {state['input_dir_url']} \"\n                f\"doesn't match the current input_dir_url {self.cache.input_dir.url}\"\n            )\n\n        if self.shuffle != state[\"shuffle\"]:\n            raise ValueError(f\"The shuffle in the state dict {state['shuffle']} \" f\"doesn't match the current shuffle\")\n\n        if self.seed != state[\"seed\"]:\n            raise ValueError(f\"The seed in the state dict {state['seed']} \" f\"doesn't match the current seed\")\n\n        if self.drop_last != state[\"drop_last\"]:\n            raise ValueError(\n                f\"The drop_last in the state dict {state['drop_last']} \" f\"doesn't match the current drop_last\"\n            )\n\n        if self.distributed_env.world_size != state[\"world_size\"]:\n            raise ValueError(\n                f\"The world_size in the state"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `_validate_state_dict` should only be called in the main process.\")\n\n        state: Dict[str, Any] = self._state_dict\n        if not isinstance(state, dict):\n            raise ValueError(f\"The state dict must be a dict. Found {state}\")\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The input_dir_path in the state_dict must be equal to the input_dir_path in the StreamingDataset. \"\n                f\"Found {state['input_dir_path']} != {self.input_dir.path}\"\n            )\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The input_dir_url in the state_dict must be equal to the input_dir_url in the StreamingDataset. \"\n                f\"Found {state['input_dir_url']} != {self.input_dir.url}\"\n            )\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(f\"The seed in the state_dict must be equal to the seed in the StreamingDataset. \" f\"Found {state['seed']} != {self.seed}\")\n\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                f\"The drop_last in the state_dict must be equal to the drop_last in the StreamingDataset. \"\n                f\"Found {state['drop_last']} != {self.drop_last}\"\n            )\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"The shuffle in the state_dict must be equal to the shuffle in the StreamingDataset. \"\n                f\"Found {state['shuffle']} != {self.shuffle}\"\n            )\n\n        if state[\"world_size\"] != self.distributed_env.world"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    hashed_input_dir = hashlib.md5(input_dir.encode()).hexdigest()\n    cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hashed_input_dir)\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    return cache_dir"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None or input_dir == \"\":\n        input_dir = \"\"\n\n    cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hashlib.md5(input_dir.encode(\"utf-8\")).hexdigest())\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir, exist_ok=True)\n\n    return cache_dir"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None or input_dir == \"\":\n        return None\n\n    cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hashlib.sha256(input_dir.encode()).hexdigest())\n\n    if os.path.exists(cache_dir):\n        return cache_dir\n\n    os.makedirs(cache_dir, exist_ok=True)\n\n    return cache_dir"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    cache_dir = os.environ.get(\"DATA_OPTIMIZER_CACHE_DIR\", _DEFAULT_CACHE_DIR)\n    cache_dir = os.path.join(cache_dir, hashlib.md5(input_dir.encode(\"utf-8\")).hexdigest())\n\n    if os.path.exists(cache_dir):\n        return cache_dir\n\n    try:\n        os.makedirs(cache_dir)\n        return cache_dir\n    except Exception:\n        return None"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    hash_object = hashlib.md5(input_dir.encode())\n    hash_str = hash_object.hexdigest()\n    cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hash_str)\n\n    if os.path.exists(cache_dir):\n        return cache_dir\n\n    if os.getenv(\"DATA_OPTIMIZER_GLOBAL_RANK\") is None:\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n\n    if os.getenv(\"DATA_OPTIMIZER_GLOBAL_RANK\") == \"0\":\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n\n    return None"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    if os.getenv(\"DATA_OPTIMIZER_GLOBAL_RANK\") is None:\n        if os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\") is not None:\n            cache_dir = os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\")\n        else:\n            cache_dir = _DEFAULT_CACHE_DIR\n\n        if not os.path.exists(cache_dir):\n            os.makedirs(cache_dir)\n\n        cache_dir = os.path.join(cache_dir, hashlib.md5(input_dir.encode()).hexdigest())\n\n        if not os.path.exists(cache_dir):\n            os.makedirs(cache_dir)\n\n        return cache_dir\n\n    return None"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    hash_object = hashlib.md5(input_dir.encode())\n    cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hash_object.hexdigest())\n\n    if os.path.exists(cache_dir):\n        return cache_dir\n\n    if os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\") is not None:\n        cache_dir = os.path.join(os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"), hash_object.hexdigest())\n\n    if os.path.exists(cache_dir):\n        return cache_dir\n\n    os.makedirs(cache_dir, exist_ok=True)\n    return cache_dir"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    hash_str = hashlib.sha1(input_dir.encode()).hexdigest()\n    cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hash_str)\n\n    if os.path.exists(cache_dir):\n        return cache_dir\n\n    if os.getenv(\"DATA_OPTIMIZER_GLOBAL_RANK\") is None:\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n\n    if os.getenv(\"DATA_OPTIMIZER_GLOBAL_RANK\") is not None:\n        if os.getenv(\"DATA_OPTIMIZER_GLOBAL_RANK\") == \"0\":\n            os.makedirs(cache_dir, exist_ok=True)\n            return cache_dir\n\n    return None"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    cache_dir = os.path.join(\n        _DEFAULT_CACHE_DIR,\n        hashlib.sha256(input_dir.encode(\"utf-8\")).hexdigest(),\n    )\n\n    if os.path.exists(cache_dir):\n        return cache_dir\n\n    if os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"):\n        cache_dir = os.path.join(os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"), hashlib.sha256(input_dir.encode(\"utf-8\")).hexdigest())\n\n    if os.getenv(\"DATA_OPTIMIZER_GLOBAL_RANK\"):\n        cache_dir = os.path.join(\n            os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"),\n            os.getenv(\"DATA_OPTIMIZER_GLOBAL_RANK\"),\n            hashlib.sha256(input_dir.encode(\"utf-8\")).hexdigest(),\n        )\n\n    os.makedirs(cache_dir, exist_ok=True)\n    return cache_dir"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if not input_dir:\n        input_dir = \"\"\n\n    hash_obj = hashlib.md5(input_dir.encode())\n    hash_obj.update(str(time()).encode())\n    cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hash_obj.hexdigest())\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    return cache_dir"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # If the environment variables are not set, create the cache directory in a default location.\n    if not os.getenv(\"DATA_OPTIMIZER_GLOBAL_RANK\") and not os.getenv(\"DATA_OPTIMIZER_RANK\"):\n        cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hashlib.md5(input_dir.encode()).hexdigest())\n        if not os.path.exists(cache_dir):\n            os.makedirs(cache_dir)\n        return cache_dir\n\n    # If the environment variables are set, create the cache directory in a specified location.\n    cache_dir = os.path.join(os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\", \"\"), hashlib.md5(input_dir.encode()).hexdigest())\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n    return cache_dir"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n    if _should_replace_path(input_dir):\n        cache_dir = _DEFAULT_CACHE_DIR\n        if os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"):\n            cache_dir = os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\")\n\n        cache_dir = os.path.join(cache_dir, hashlib.md5(input_dir.encode()).hexdigest())\n\n        if not os.path.exists(cache_dir):\n            os.makedirs(cache_dir)\n        return cache_dir\n\n    return None"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None or input_dir == \"\":\n        input_dir = \"\"\n\n    hash_input = hashlib.md5(input_dir.encode()).hexdigest()\n\n    if os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"):\n        cache_dir = os.path.join(os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"), hash_input)\n    elif os.getenv(\"DATA_OPTIMIZER_CACHE_DIR_PATH\"):\n        cache_dir = os.path.join(os.getenv(\"DATA_OPTIMIZER_CACHE_DIR_PATH\"), hash_input)\n    else:\n        cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hash_input)\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    return cache_dir"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    if not os.path.exists(_DEFAULT_CACHE_DIR):\n        os.makedirs(_DEFAULT_CACHE_DIR)\n\n    if not os.path.exists(input_dir):\n        raise ValueError(f\"The provided input_dir `{input_dir}` doesn't exist.\")\n\n    hash_obj = hashlib.sha1(input_dir.encode())\n    cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hash_obj.hexdigest())\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    return cache_dir"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    if not os.path.exists(input_dir):\n        raise ValueError(f\"The input directory {input_dir} does not exist.\")\n\n    if input_dir == \"\":\n        input_dir = hashlib.sha256(input_dir.encode()).hexdigest()\n\n    cache_dir = os.path.join(_DEFAULT_CACHE_DIR, input_dir)\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    return cache_dir"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None or input_dir == \"\":\n        input_dir = \"\"\n\n    cache_dir_name = hashlib.sha256(input_dir.encode(\"utf-8\")).hexdigest()\n    cache_dir_path = os.path.join(_DEFAULT_CACHE_DIR, cache_dir_name)\n\n    if os.path.exists(cache_dir_path):\n        return cache_dir_path\n\n    # If the cache directory already exists, do not create a new one.\n    # Instead, return the existing directory path.\n    if os.path.exists(cache_dir_path):\n        return cache_dir_path\n\n    # If the cache directory doesn't exist, create it.\n    os.makedirs(cache_dir_path, exist_ok=True)\n\n    return cache_dir_path"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    hash_object = hashlib.md5(input_dir.encode())\n    hash_object.hexdigest()\n    cache_dir = hash_object.hexdigest()\n\n    cache_dir = os.path.join(_DEFAULT_CACHE_DIR, cache_dir)\n\n    if os.path.exists(cache_dir):\n        return cache_dir\n\n    # Create the cache directory\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n    except OSError as e:\n        logger.error(f\"Failed to create cache directory: {e}\")\n        return None\n\n    return cache_dir"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    if os.getenv(\"DATA_OPTIMIZER_GLOBAL_RANK\"):\n        cache_dir = os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\")\n    else:\n        cache_dir = os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\", _DEFAULT_CACHE_DIR)\n\n    if cache_dir is None:\n        cache_dir = _DEFAULT_CACHE_DIR\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    # Create a unique directory name for the cache by hashing the input directory.\n    cache_dir = os.path.join(cache_dir, hashlib.md5(input_dir.encode()).hexdigest())\n\n    # If the directory already exists, return it.\n    if os.path.exists(cache_dir):\n        return cache_dir\n\n    # Create the cache directory.\n    os.makedirs(cache_dir)\n\n    # Return the path of the created cache directory.\n    return cache_dir"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # We hash the input dir to create a unique cache dir name\n    cache_dir_name = hashlib.md5(input_dir.encode()).hexdigest()\n\n    # We create the cache dir in the default location if the env vars are not set\n    cache_dir = os.path.join(_DEFAULT_CACHE_DIR, cache_dir_name)\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    return cache_dir"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Generate a unique directory name by hashing the input directory.\n    cache_dir = hashlib.sha256(input_dir.encode(\"utf-8\")).hexdigest()\n\n    # Create the cache directory if it doesn't exist.\n    if os.path.exists(cache_dir):\n        return cache_dir\n\n    os.makedirs(cache_dir, exist_ok=True)\n\n    # Get the default cache directory from the environment variable.\n    default_cache_dir = os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\")\n    if default_cache_dir is None:\n        default_cache_dir = _DEFAULT_CACHE_DIR\n\n    # Create the default cache directory if it doesn't exist.\n    if not os.path.exists(default_cache_dir):\n        os.makedirs(default_cache_dir, exist_ok=True)\n\n    # Move the cache directory to the default location.\n    shutil.move(cache_dir, default_cache_dir)\n\n    return default_cache_dir + \"/\" + cache_dir"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"The provided remote_path doesn't use the 's3' scheme: {remote_filepath}\")\n\n        if not os.path.exists(local_filepath):\n            if self._s5cmd_available:\n                s5cmd_cmd = f\"s5cmd cp --force {remote_filepath} {local_filepath}\"\n                subprocess.run(s5cmd_cmd, shell=True, check=True)\n            else:\n                with FileLock(local_filepath + \".lock\", timeout=10):\n                    self._client.download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"The provided remote_filepath {remote_filepath} doesn't have an s3 scheme.\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        parsed_url = parse.urlparse(remote_filepath)\n        bucket = parsed_url.netloc\n        key = parsed_url.path.lstrip(\"/\")\n\n        if self._s5cmd_available:\n            s5cmd_cmd = f\"s5cmd --endpoint-url {self._client.endpoint_url} cp s3://{bucket}/{key} {local_filepath}\"\n            subprocess.run(s5cmd_cmd, shell=True, check=True)\n        else:\n            self._client.download_file(bucket, key, local_filepath)"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"The provided remote_path doesn't start with s3://: {remote_filepath}\")\n\n        parsed_url = parse.urlparse(remote_filepath)\n        bucket = parsed_url.netloc\n        remote_filepath = parsed_url.path\n\n        if os.path.exists(local_filepath):\n            return\n\n        if self._s5cmd_available:\n            cmd = f\"s5cmd cp s3://{bucket}{remote_filepath} {local_filepath}\"\n            subprocess.check_call(cmd, shell=True)\n        else:\n            with FileLock(local_filepath + \".lock\", timeout=10):\n                self._client.download_file(bucket, remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"The provided remote_filepath doesn't use the s3 scheme: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        parsed_url = parse.urlparse(remote_filepath)\n        bucket = parsed_url.netloc\n        key = parsed_url.path.lstrip(\"/\")\n\n        lock_filepath = os.path.join(self._cache_dir, f\"{key}.lock\")\n\n        with FileLock(lock_filepath, timeout=10):\n            if os.path.exists(local_filepath):\n                return\n\n            if self._s5cmd_available:\n                cmd = f\"s5cmd cp s3://{bucket}/{key} {local_filepath}\"\n                subprocess.check_call(cmd, shell=True)\n            else:\n                self._client.download_file(bucket, key, local_filepath)"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"The provided remote_filepath doesn't use the `s3` scheme: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        with FileLock(local_filepath + \".lock\", timeout=5):\n            parsed_url = parse.urlparse(remote_filepath)\n            bucket = parsed_url.netloc\n            key = parsed_url.path.lstrip(\"/\")\n\n            if self._s5cmd_available:\n                subprocess.run([\"s5cmd\", \"cp\", f\"s3://{bucket}/{key}\", local_filepath], check=True)\n            else:\n                self._client.download_file(bucket, key, local_filepath)"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not str(remote_filepath).startswith(\"s3://\"):\n            raise ValueError(f\"The provided remote_filepath doesn't use the `s3` scheme: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        s3_url = parse.urlparse(remote_filepath)\n        bucket = s3_url.netloc\n        key = s3_url.path.lstrip(\"/\")\n\n        if self._s5cmd_available:\n            cmd = [\"s5cmd\", \"cp\", f\"s3://{bucket}/{key}\", local_filepath]\n            subprocess.run(cmd, check=True)\n        else:\n            self._client.download_file(bucket, key, local_filepath)"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"The provided remote_filepath doesn't use the 's3' scheme: {remote_filepath}\")\n\n        parsed_url = parse.urlparse(remote_filepath)\n        bucket_name = parsed_url.netloc\n        key = parsed_url.path.lstrip(\"/\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        with FileLock(local_filepath + \".lock\", timeout=5):\n            if not os.path.exists(local_filepath):\n                if self._s5cmd_available:\n                    s5cmd_cmd = f\"s5cmd --endpoint-url {self._client.endpoint_url} cp s3://{bucket_name}/{key} {local_filepath}\"\n                    subprocess.run(s5cmd_cmd, shell=True, check=True)\n                else:\n                    self._client.download_file(bucket_name, key, local_filepath)"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"The provided remote_filepath doesn't start with s3://: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        parsed_url = parse.urlparse(remote_filepath)\n        bucket = parsed_url.netloc\n        key = parsed_url.path[1:]\n\n        if self._s5cmd_available:\n            cmd = f\"s5cmd --endpoint-url {self._client.endpoint_url} cp s3://{bucket}/{key} {local_filepath}\"\n            subprocess.run(cmd, shell=True, check=True)\n        else:\n            lock_path = os.path.join(self._cache_dir, f\"{key}.lock\")\n            with FileLock(lock_path, timeout=10):\n                self._client.download_file(bucket, key, local_filepath)"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"The provided remote_filepath doesn't use the 's3' scheme: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        lock_filepath = local_filepath + \".lock\"\n\n        with FileLock(lock_filepath, timeout=10):\n            if os.path.exists(local_filepath):\n                return\n\n            parsed = parse.urlparse(remote_filepath)\n            bucket_name = parsed.netloc\n            key = parsed.path.lstrip(\"/\")\n\n            if self._s5cmd_available:\n                cmd = f\"s5cmd --no-sign-request cp s3://{bucket_name}/{key} {local_filepath}\"\n                subprocess.run(cmd, shell=True, check=True)\n            else:\n                self._client.download_file(bucket_name, key, local_filepath)"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"The provided `remote_filepath` {remote_filepath} doesn't start with `s3://`.\")\n\n        parsed_url = parse.urlparse(remote_filepath)\n        bucket = parsed_url.netloc\n        key = parsed_url.path[1:]\n\n        if os.path.exists(local_filepath):\n            return\n\n        lock_filepath = local_filepath + \".lock\"\n        with FileLock(lock_filepath, timeout=5):\n            if not os.path.exists(local_filepath):\n                if self._s5cmd_available:\n                    cmd = f\"s5cmd --endpoint-url {self._client.endpoint_url} cp {remote_filepath} {local_filepath}\"\n                    subprocess.run(cmd, shell=True, check=True)\n                else:\n                    self._client.download_file(bucket, key, local_filepath)"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        # Check if the remote file path is an S3 URL\n        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"The provided remote_path doesn't use the 's3' scheme: {remote_filepath}\")\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            return\n\n        # Get the bucket and key from the S3 URL\n        parsed_url = parse.urlparse(remote_filepath)\n        bucket = parsed_url.netloc\n        key = parsed_url.path.lstrip(\"/\")\n\n        # Create the cache directory if it doesn't exist\n        os.makedirs(os.path.dirname(local_filepath), exist_ok=True)\n\n        # Use s5cmd to download the file if available\n        if self._s5cmd_available:\n            s5cmd_cmd = f\"s5cmd cp s3://{bucket}/{key} {local_filepath}\"\n            subprocess.run(s5cmd_cmd, shell=True, check=True)\n\n        # Use boto3 to download the file if s5cmd is not available\n        else:\n            self._client.download_file(bucket, key, local_filepath)"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"The provided remote_filepath doesn't use the `s3` scheme: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        with FileLock(local_filepath + \".lock\", timeout=10):\n            if os.path.exists(local_filepath):\n                return\n\n            if self._s5cmd_available:\n                self._download_file_with_s5cmd(remote_filepath, local_filepath)\n            else:\n                self._download_file_with_boto3(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"The provided remote_filepath doesn't use the `s3://` scheme: {remote_filepath}\")\n\n        if not os.path.exists(local_filepath):\n            parsed_url = parse.urlparse(remote_filepath)\n            bucket_name = parsed_url.netloc\n            remote_path = parsed_url.path\n\n            if self._s5cmd_available:\n                self._download_with_s5cmd(bucket_name, remote_path, local_filepath)\n            else:\n                self._download_with_boto3(bucket_name, remote_path, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"The provided remote_filepath {remote_filepath} doesn't start with s3://.\")\n\n        parsed_url = parse.urlparse(remote_filepath)\n\n        if not parsed_url.netloc:\n            raise ValueError(f\"The provided remote_filepath {remote_filepath} doesn't have a netloc.\")\n\n        if not parsed_url.path:\n            raise ValueError(f\"The provided remote_filepath {remote_filepath} doesn't have a path.\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        if self._s5cmd_available:\n            s5cmd_cmd = f\"s5cmd cp {remote_filepath} {local_filepath}\"\n            subprocess.run(s5cmd_cmd, shell=True, check=True)\n        else:\n            with FileLock(local_filepath + \".lock\", timeout=10):\n                if os.path.exists(local_filepath):\n                    return\n\n                self._client.download_file(\n                    parsed_url.netloc,\n                    parsed_url.path.lstrip(\"/\"),\n                    local_filepath,\n                )"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"The provided remote_path doesn't start with s3://: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        parsed = parse.urlparse(remote_filepath)\n        bucket = parsed.netloc\n        key = parsed.path.lstrip(\"/\")\n\n        lock_filepath = local_filepath + \".lock\"\n        lock = FileLock(lock_filepath, timeout=10)\n\n        try:\n            with lock:\n                if os.path.exists(local_filepath):\n                    return\n\n                if self._s5cmd_available:\n                    subprocess.check_call(\n                        [\"s5cmd\", \"cp\", \"--quiet\", f\"s3://{bucket}/{key}\", local_filepath],\n                        stderr=subprocess.DEVNULL,\n                    )\n                else:\n                    self._client.download_file(bucket, key, local_filepath)\n        except Timeout:\n            raise Timeout(f\"Could not acquire lock for {local_filepath}\")"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not str(remote_filepath).startswith(\"s3://\"):\n            raise ValueError(f\"The provided remote_filepath doesn't start with 's3://': {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        if not self._s5cmd_available:\n            self._client.download_file(remote_filepath, local_filepath)\n            return\n\n        with FileLock(f\"{local_filepath}.lock\", timeout=5):\n            if os.path.exists(local_filepath):\n                return\n\n            s3_url = parse.urlparse(remote_filepath)\n            bucket = s3_url.netloc\n            key = s3_url.path[1:]\n\n            cmd = f\"s5cmd --no-sign-request --endpoint-url=https://{bucket}.s3.amazonaws.com cp s3://{bucket}/{key} {local_filepath}\"\n            subprocess.run(cmd, shell=True, check=True)"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"The provided remote_filepath doesn't use the 's3' scheme: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        parsed_url = parse.urlparse(remote_filepath)\n        bucket = parsed_url.netloc\n        key = parsed_url.path.lstrip(\"/\")\n\n        if self._s5cmd_available:\n            self._download_file_with_s5cmd(bucket, key, local_filepath)\n        else:\n            self._download_file_with_boto3(bucket, key, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"The provided `remote_filepath` {remote_filepath} doesn't start with `s3://`\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        parsed_url = parse.urlparse(remote_filepath)\n        bucket = parsed_url.netloc\n        key = parsed_url.path.lstrip(\"/\")\n\n        if self._s5cmd_available:\n            self._download_s5cmd(bucket, key, local_filepath)\n        else:\n            self._download_boto3(bucket, key, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"The provided remote_filepath doesn't use the 's3' scheme: {remote_filepath}\")\n\n        if not os.path.exists(local_filepath):\n            parsed_url = parse.urlparse(remote_filepath)\n            bucket = parsed_url.netloc\n            key = parsed_url.path.lstrip(\"/\")\n\n            lock_path = local_filepath + \".lock\"\n            lock = FileLock(lock_path, timeout=10)\n\n            try:\n                with lock.acquire(timeout=10):\n                    if self._s5cmd_available:\n                        self._download_file_s5cmd(bucket, key, local_filepath)\n                    else:\n                        self._download_file_boto3(bucket, key, local_filepath)\n            except Timeout:\n                raise Timeout(f\"Couldn't acquire lock for file {local_filepath}\")\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"The provided remote_path doesn't start with s3://: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        # parse the S3 URL\n        parsed_url = parse.urlparse(remote_filepath)\n        bucket_name = parsed_url.netloc\n        object_key = parsed_url.path.lstrip(\"/\")\n\n        # create a lock file for the local file\n        lock_file = f\"{local_filepath}.lock\"\n        lock = FileLock(lock_file, timeout=5)\n\n        # attempt to acquire the lock\n        try:\n            with lock:\n                # check if the local file exists after acquiring the lock\n                if os.path.exists(local_filepath):\n                    return\n\n                # use s5cmd to download the file if available\n                if self._s5cmd_available:\n                    s5cmd_cmd = f\"s5cmd --no-sign-request cp s3://{bucket_name}/{object_key} {local_filepath}\"\n                    subprocess.run(s5cmd_cmd, shell=True, check=True)\n                else:\n                    # use boto3 to download the file\n                    self._client.download_file(bucket_name, object_key, local_filepath)\n        except Timeout:\n            # handle the timeout exception\n            raise Timeout(f\"Timeout while trying to download {remote_filepath} to {local_filepath}\")"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {}\n    workers_intervals = {}\n\n    for worker_idx in range(num_workers):\n        workers_chunks[worker_idx] = []\n        workers_intervals[worker_idx] = []\n\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_idx = i % worker_env.world_size\n        workers_chunks[worker_idx].append(chunk_index)\n        workers_intervals[worker_idx].append(chunk_interval)\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {}\n    workers_intervals = {}\n\n    for worker_idx in range(num_workers):\n        workers_chunks[worker_idx] = []\n        workers_intervals[worker_idx] = []\n\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_idx = i % worker_env.world_size\n        workers_chunks[worker_idx].append(chunk_index)\n        workers_intervals[worker_idx].append(chunk_interval)\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {}\n    workers_intervals = {}\n\n    for worker_idx in range(num_workers):\n        workers_chunks[worker_idx] = []\n        workers_intervals[worker_idx] = []\n\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_idx = i % worker_env.world_size\n        workers_chunks[worker_idx].append(chunk_index)\n        workers_intervals[worker_idx].append(chunk_interval)\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {}\n    workers_intervals = {}\n\n    for worker_idx in range(num_workers):\n        workers_chunks[worker_idx] = []\n        workers_intervals[worker_idx] = []\n\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_idx = i % worker_env.world_size\n        workers_chunks[worker_idx].append(chunk_index)\n        workers_intervals[worker_idx].append(chunk_interval)\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {}\n    workers_intervals = {}\n\n    for worker_idx in range(num_workers):\n        workers_chunks[worker_idx] = []\n        workers_intervals[worker_idx] = []\n\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_idx = i % worker_env.world_size\n        workers_chunks[worker_idx].append(chunk_index)\n        workers_intervals[worker_idx].append(chunk_interval)\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {}\n    workers_intervals = {}\n\n    for worker_idx in range(num_workers):\n        workers_chunks[worker_idx] = []\n        workers_intervals[worker_idx] = []\n\n    for chunk_idx, interval in zip(chunks_replica, intervals_replica):\n        worker_idx = chunk_idx % worker_env.world_size\n        workers_chunks[worker_idx].append(chunk_idx)\n        workers_intervals[worker_idx].append(interval)\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {}\n    workers_intervals = {}\n\n    for worker_idx in range(num_workers):\n        workers_chunks[worker_idx] = []\n        workers_intervals[worker_idx] = []\n\n    for worker_idx, (chunk_index, interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        workers_chunks[worker_idx % worker_env.world_size].append(chunk_index)\n        workers_intervals[worker_idx % worker_env.world_size].append(interval)\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {}\n    workers_intervals = {}\n\n    for i in range(num_workers):\n        workers_chunks[i] = []\n        workers_intervals[i] = []\n\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_index = i % worker_env.world_size\n        workers_chunks[worker_index].append(chunk_index)\n        workers_intervals[worker_index].append(chunk_interval)\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {}\n    workers_intervals = {}\n\n    for worker_idx in range(num_workers):\n        workers_chunks[worker_idx] = []\n        workers_intervals[worker_idx] = []\n\n    for chunk_index, interval in zip(chunks_replica, intervals_replica):\n        worker_idx = worker_env.rank % worker_env.world_size\n        workers_chunks[worker_idx].append(chunk_index)\n        workers_intervals[worker_idx].append(interval)\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {}\n    workers_intervals = {}\n\n    for worker_idx in range(num_workers):\n        workers_chunks[worker_idx] = []\n        workers_intervals[worker_idx] = []\n\n    # distribute chunks and intervals\n    for chunk_index, interval in zip(chunks_replica, intervals_replica):\n        worker_idx = chunk_index % worker_env.world_size\n        workers_chunks[worker_idx].append(chunk_index)\n        workers_intervals[worker_idx].append(interval)\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {}\n    workers_intervals = {}\n    for worker_idx in range(num_workers):\n        workers_chunks[worker_idx] = []\n        workers_intervals[worker_idx] = []\n\n    # distribute chunks and intervals across workers\n    for worker_idx in range(num_workers):\n        for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n            if i % worker_env.world_size != worker_env.rank:\n                continue\n            workers_chunks[worker_idx].append(chunk_index)\n            workers_intervals[worker_idx].append(chunk_interval)\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {}\n    workers_intervals = {}\n\n    for worker_idx in range(num_workers):\n        workers_chunks[worker_idx] = []\n        workers_intervals[worker_idx] = []\n\n    # distribute the chunks and intervals across the workers\n    for chunk_idx, interval in zip(chunks_replica, intervals_replica):\n        worker_idx = chunk_idx % worker_env.world_size\n        workers_chunks[worker_idx].append(chunk_idx)\n        workers_intervals[worker_idx].append(interval)\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    assert len(chunks_replica) == len(intervals_replica)\n\n    workers_chunks = {}\n    workers_intervals = {}\n\n    for worker_idx in range(num_workers):\n        workers_chunks[worker_idx] = []\n        workers_intervals[worker_idx] = []\n\n    for chunk_idx, chunk_interval in zip(chunks_replica, intervals_replica):\n        worker_idx = chunk_idx % worker_env.world_size\n        workers_chunks[worker_idx].append(chunk_idx)\n        workers_intervals[worker_idx].append(chunk_interval)\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # TODO: Implement elastic sampling where the number of workers, ranks can change.\n    workers_chunks = {}\n    workers_intervals = {}\n\n    for worker_idx in range(num_workers):\n        workers_chunks[worker_idx] = []\n        workers_intervals[worker_idx] = []\n\n    for chunk_index, interval in zip(chunks_replica, intervals_replica):\n        worker_idx = worker_idx % worker_env.world_size\n        workers_chunks[worker_idx].append(chunk_index)\n        workers_intervals[worker_idx].append(interval)\n        worker_idx += 1\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    chunks_per_worker = {}\n    intervals_per_worker = {}\n\n    for i in range(num_workers):\n        chunks_per_worker[i] = []\n        intervals_per_worker[i] = []\n\n    # Distribute chunks and intervals based on the worker index and world size\n    for i, (chunk_index, interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_index = i % worker_env.world_size\n        chunks_per_worker[worker_index].append(chunk_index)\n        intervals_per_worker[worker_index].append(interval)\n\n    return chunks_per_worker, intervals_per_worker"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # TODO: Implement elastic sampling where the number of workers, ranks can change.\n\n    workers_chunks = {}\n    workers_intervals = {}\n\n    for i in range(num_workers):\n        workers_chunks[i] = []\n        workers_intervals[i] = []\n\n    # distribute the chunks and intervals to the workers\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_index = i % worker_env.world_size\n        workers_chunks[worker_index].append(chunk_index)\n        workers_intervals[worker_index].append(chunk_interval)\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # TODO: Implement elastic sampling where the number of workers, ranks can change.\n    # TODO: Implement the logic for the first worker to be the last worker.\n    # TODO: Implement the logic for the last worker to be the first worker.\n    workers_chunks = {}\n    workers_intervals = {}\n    for worker_idx in range(num_workers):\n        workers_chunks[worker_idx] = []\n        workers_intervals[worker_idx] = []\n\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        if i % worker_env.world_size != worker_env.rank:\n            continue\n        workers_chunks[worker_idx].append(chunk_index)\n        workers_intervals[worker_idx].append(chunk_interval)\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Initialize an empty dictionary to store the chunks and intervals for each worker\n    workers_chunks = {}\n    workers_intervals = {}\n\n    # Iterate over each worker index\n    for worker_idx in range(num_workers):\n        # Calculate the number of chunks that each worker should get\n        num_chunks_per_worker = len(chunks_replica) // num_workers\n\n        # Calculate the starting and ending indices for the chunks assigned to the current worker\n        start_idx = worker_idx * num_chunks_per_worker\n        end_idx = start_idx + num_chunks_per_worker\n\n        # If the worker index is the last one, assign the remaining chunks to the worker\n        if worker_idx == num_workers - 1:\n            end_idx = len(chunks_replica)\n\n        # Get the chunks and intervals for the current worker\n        workers_chunks[worker_idx] = chunks_replica[start_idx:end_idx]\n        workers_intervals[worker_idx] = intervals_replica[start_idx:end_idx]\n\n    # Return the dictionary of chunks and intervals for each worker\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Create a dictionary to store the chunks and intervals for each worker\n    workers_chunks = {}\n    workers_intervals = {}\n\n    # Iterate over the number of workers\n    for worker_idx in range(num_workers):\n        # Determine the chunks and intervals for the current worker based on the worker index and world size\n        chunks = []\n        intervals = []\n        for i in range(worker_idx, len(chunks_replica), worker_env.world_size):\n            chunks.append(chunks_replica[i])\n            intervals.append(intervals_replica[i])\n        # Store the chunks and intervals for the current worker\n        workers_chunks[worker_idx] = chunks\n        workers_intervals[worker_idx] = intervals\n\n    # Return the dictionaries containing the chunks and intervals for each worker\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Initialize empty dictionaries to store the chunks and intervals for each worker\n    workers_chunks: Dict[int, List[int]] = {}\n    workers_intervals: Dict[int, List[Any]] = {}\n\n    # Iterate over the number of workers\n    for worker_idx in range(num_workers):\n        # Initialize empty lists to store the chunks and intervals for the current worker\n        worker_chunks: List[int] = []\n        worker_intervals: List[Any] = []\n\n        # Determine the start and end indices for the current worker based on its index and the world size\n        start_idx = worker_idx % worker_env.world_size\n        end_idx = start_idx + worker_env.world_size\n\n        # Iterate over the chunk indices and intervals\n        for chunk_idx, interval in zip(chunks_replica, intervals_replica):\n            # Check if the chunk index falls within the range of the current worker\n            if start_idx <= chunk_idx < end_idx:\n                # Add the chunk index and interval to the current worker's lists\n                worker_chunks.append(chunk_idx)\n                worker_intervals.append(interval)\n\n        # Add the current worker's chunks and intervals to the overall dictionary\n        workers_chunks[worker_idx] = worker_chunks\n        workers_intervals[worker_idx] = worker_intervals\n\n    # Return the dictionary containing the chunks and intervals for each worker\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if str(remote_filepath).startswith(\"local:\"):\n            remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if str(remote_filepath).startswith(\"local:\"):\n            remote_filepath = remote_filepath.replace(\"local:\", \"\")\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if not remote_filepath.startswith(\"local:\"):\n            super().download_file(remote_filepath, local_filepath)\n            return\n\n        remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        if not os.path.exists(remote_filepath):\n            raise FileNotFoundError(f\"The provided remote_path doesn't exist: {remote_filepath}\")\n\n        if remote_filepath != local_filepath and not os.path.exists(local_filepath):\n            shutil.copy(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if not remote_filepath.startswith(\"local:\"):\n            super().download_file(remote_filepath, local_filepath)\n            return\n\n        remote_filepath = remote_filepath[len(\"local:\") :]\n        if not os.path.exists(remote_filepath):\n            raise FileNotFoundError(f\"The provided remote_path doesn't exist: {remote_filepath}\")\n\n        if remote_filepath != local_filepath and not os.path.exists(local_filepath):\n            shutil.copy(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if not str(remote_filepath).startswith(\"local:\"):\n            return super().download_file(remote_filepath, local_filepath)\n\n        remote_filepath = str(remote_filepath).replace(\"local:\", \"\")\n        if not os.path.exists(remote_filepath):\n            raise FileNotFoundError(f\"The provided remote_path doesn't exist: {remote_filepath}\")\n\n        if remote_filepath != local_filepath and not os.path.exists(local_filepath):\n            shutil.copy(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if not os.path.exists(remote_filepath):\n            raise FileNotFoundError(f\"The provided remote_path doesn't exist: {remote_filepath}\")\n\n        if remote_filepath != local_filepath and not os.path.exists(local_filepath):\n            if remote_filepath.startswith(\"local:\"):\n                remote_filepath = remote_filepath[len(\"local:\") :]\n            shutil.copy(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if not str(remote_filepath).startswith(\"local:\"):\n            return super().download_file(remote_filepath, local_filepath)\n\n        remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        if not os.path.exists(remote_filepath):\n            raise FileNotFoundError(f\"The provided remote_path doesn't exist: {remote_filepath}\")\n\n        if remote_filepath != local_filepath and not os.path.exists(local_filepath):\n            shutil.copy(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if not str(remote_filepath).startswith(\"local:\"):\n            return super().download_file(remote_filepath, local_filepath)\n\n        remote_filepath = remote_filepath.replace(\"local:\", \"\")\n\n        if not os.path.exists(remote_filepath):\n            raise FileNotFoundError(f\"The provided remote_path doesn't exist: {remote_filepath}\")\n\n        if remote_filepath != local_filepath and not os.path.exists(local_filepath):\n            shutil.copy(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if not str(remote_filepath).startswith(\"local:\"):\n            raise ValueError(f\"Expected `remote_filepath` to start with `local:`, instead, got {remote_filepath}\")\n        remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if not os.path.exists(remote_filepath):\n            raise FileNotFoundError(f\"The provided remote_path doesn't exist: {remote_filepath}\")\n\n        if remote_filepath != local_filepath and not os.path.exists(local_filepath):\n            if not os.path.exists(local_filepath):\n                shutil.copy(remote_filepath, local_filepath)\n            else:\n                shutil.copy(local_filepath, remote_filepath)\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        width, height = item.size\n        mode = item.mode\n        mode_size = len(mode)\n        data = [np.uint32(width).tobytes()]\n        data.append(np.uint32(height).tobytes())\n        data.append(np.uint32(mode_size).tobytes())\n        data.append(mode.encode(\"utf-8\"))\n        data.append(item.tobytes())\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        width, height = item.size\n        mode = item.mode\n        mode_size = len(mode)\n        size = width, height\n        raw = item.tobytes()\n        data = [np.uint32(width).tobytes(), np.uint32(height).tobytes(), np.uint32(mode_size).tobytes()]\n        data.append(mode.encode(\"utf-8\"))\n        data.append(raw)\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        buff = io.BytesIO()\n        item.save(buff, format=\"PNG\")\n        buff.seek(0)\n        return buff.read(), None\n\n    "}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            if not hasattr(item, \"filename\"):\n                raise ValueError(\n                    \"The JPEG Image's filename isn't defined. HINT: Open the image in your Dataset __getitem__ method.\"\n                )\n            if item.filename and os.path.isfile(item.filename):\n                # read the content of the file directly\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                item_bytes = io.BytesIO()\n                item.save(item_bytes, format=\"JPEG\")\n                item_bytes = item_bytes.getvalue()\n                return item_bytes, None\n\n        if isinstance(item, (PngImageFile, WebPImageFile, GifImageFile, Image.Image)):\n            buff = io.BytesIO()\n            item.convert(\"RGB\").save(buff, quality=100, format=\"JPEG\")\n            buff.seek(0)\n            return buff.read(), None\n\n        raise TypeError(f\"The provided item should be of type {JpegImageFile}. Found {item}.\")"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        width, height = item.size\n        mode = item.mode\n        mode_size = len(mode)\n        size = width, height\n        raw = item.tobytes()\n        data = [np.uint32(width).tobytes(), np.uint32(height).tobytes(), np.uint32(mode_size).tobytes(), mode.encode(\"utf-8\")]\n        data.append(raw)\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        width, height = item.size\n        mode = item.mode\n        mode_size = len(mode)\n        data = [np.uint32(width).tobytes()]\n        data.append(np.uint32(height).tobytes())\n        data.append(np.uint32(mode_size).tobytes())\n        data.append(mode.encode(\"utf-8\"))\n        data.append(item.tobytes())\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        if not isinstance(item, Image.Image):\n            raise TypeError(f\"The provided item should be of type {Image.Image}. Found {item}.\")\n\n        width, height = item.size\n        mode = item.mode\n        mode_size = len(mode)\n        data = [np.uint32(width).tobytes()]\n        data.append(np.uint32(height).tobytes())\n        data.append(np.uint32(mode_size).tobytes())\n        data.append(mode.encode(\"utf-8\"))\n        data.append(item.tobytes(mode))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        if not hasattr(item, \"filename\"):\n            raise ValueError(\n                \"The PIL Image's filename isn't defined. HINT: Open the image in your Dataset __getitem__ method.\"\n            )\n        if item.filename and os.path.isfile(item.filename):\n            # read the content of the file directly\n            with open(item.filename, \"rb\") as f:\n                return f.read(), None\n        else:\n            item_bytes = io.BytesIO()\n            item.save(item_bytes, format=\"PNG\")\n            item_bytes = item_bytes.getvalue()\n            return item_bytes, None"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        data = [np.uint32(item.width).tobytes()]\n        data.append(np.uint32(item.height).tobytes())\n        data.append(np.uint32(len(item.mode)).tobytes())\n        data.append(item.mode.encode(\"utf-8\"))\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n\n    "}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Note: The image's mode is encoded in UTF-8\n        mode = item.mode\n        mode_size = len(mode)\n        data = [np.uint32(item.width).tobytes(), np.uint32(item.height).tobytes(), np.uint32(mode_size).tobytes()]\n        data.append(mode.encode(\"utf-8\"))\n        data.append(item.tobytes())\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # serialize the image data\n        width, height = item.size\n        mode = item.mode\n        mode_size = len(mode.encode(\"utf-8\"))\n        data = [np.uint32(width).tobytes(), np.uint32(height).tobytes(), np.uint32(mode_size).tobytes(), mode.encode(\"utf-8\")]\n        data.append(item.tobytes())\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        if not isinstance(item, Image.Image):\n            raise TypeError(f\"The provided item should be of type {Image.Image}. Found {item}.\")\n        if item.width == 0 or item.height == 0:\n            raise ValueError(\"The provided item should have a valid width and height.\")\n\n        mode_bytes = item.mode.encode(\"utf-8\")\n        mode_size = len(mode_bytes)\n        data = [np.uint32(item.width).tobytes()]\n        data.append(np.uint32(item.height).tobytes())\n        data.append(np.uint32(mode_size).tobytes())\n        data.append(mode_bytes)\n        data.append(item.tobytes(mode_bytes))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        if not isinstance(item, Image.Image):\n            raise TypeError(f\"The provided item should be of type {Image.Image}. Found {item}.\")\n        if not item.size:\n            raise ValueError(\"The image size should not be empty.\")\n        if not item.mode:\n            raise ValueError(\"The image mode should not be empty.\")\n\n        width, height = item.size\n        mode = item.mode\n        mode_size = len(mode)\n        data = [np.uint32(width).tobytes()]\n        data.append(np.uint32(height).tobytes())\n        data.append(np.uint32(mode_size).tobytes())\n        data.append(mode.encode(\"utf-8\"))\n        data.append(item.tobytes(mode))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        width, height = item.size\n        mode = item.mode\n        mode_size = len(mode)\n        data = [np.uint32(width).tobytes(), np.uint32(height).tobytes(), np.uint32(mode_size).tobytes()]\n        data.append(mode.encode(\"utf-8\"))\n        data.append(item.tobytes())\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # TODO: Add support for a better serialization mechanism for images\n        # Note: The start position of the shape value: 4 (dtype + shape length) + 4 * shape_idx\n        idx = 3 * 4\n        width, height = item.size\n        mode = item.mode\n        mode_size = len(mode)\n        size = width, height\n        raw = item.tobytes()\n        data = bytearray(idx + mode_size + len(raw))\n        np.array([width, height, mode_size], dtype=np.uint32).tofile(data)\n        data[idx:idx + mode_size] = mode.encode(\"utf-8\")\n        data[idx + mode_size:] = raw\n        return bytes(data), None"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # The serialized image data will be a bytes object containing the image's width, height, mode length as a bytes object, followed by the image mode encoded in UTF-8, and finally the raw pixel data.\n        width, height = item.size\n        mode = item.mode\n        mode_size = len(mode)\n        data = [np.uint32(width).tobytes(), np.uint32(height).tobytes(), np.uint32(mode_size).tobytes()]\n        data.append(mode.encode(\"utf-8\"))\n        data.append(item.tobytes())\n        return b\"\".join(data), None\n\n    "}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Encode the image's mode as a UTF-8 string\n        mode = item.mode\n        mode_bytes = mode.encode(\"utf-8\")\n        mode_size = len(mode_bytes)\n\n        # Serialize the image's dimensions and mode size\n        data = [np.uint32(item.width).tobytes(), np.uint32(item.height).tobytes(), np.uint32(mode_size).tobytes()]\n        # Append the mode bytes and the raw pixel data\n        data.append(mode_bytes)\n        data.append(item.tobytes())\n\n        # Return the serialized data as a bytes object\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        if not isinstance(item, Image.Image):\n            raise TypeError(f\"The provided item should be of type {Image.Image}. Found {type(item)}.\")\n\n        # Note: We use the PIL Image.size property to get the image's dimensions.\n        width, height = item.size\n        mode = item.mode\n        mode_size = len(mode)\n        # Note: We use the PIL Image.tobytes() method to get the image's raw pixel data.\n        raw = item.tobytes()\n        data = [np.uint32(width).tobytes()]\n        data.append(np.uint32(height).tobytes())\n        data.append(np.uint32(mode_size).tobytes())\n        data.append(mode.encode(\"utf-8\"))\n        data.append(raw)\n        return b\"\".join(data), None\n\n    "}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Serialize the image's dimensions, mode, and raw pixel data into a bytes object\n        # Note: The first 3 * 4 bytes are the width, height, and mode_size.\n        # Note: The mode_size is the length of the mode encoded in UTF-8.\n        # Note: The raw pixel data is appended after the mode_size.\n        # Note: The serialized data is a bytes object containing the image's width, height, mode length as a bytes object, followed by the image mode encoded in UTF-8, and finally the raw pixel data.\n        mode_size = len(item.mode.encode(\"utf-8\"))\n        size = item.size[0], item.size[1], mode_size\n        raw = item.tobytes()\n        data = []\n        for s in size:\n            data.append(np.uint32(s).tobytes())\n        data.append(item.mode.encode(\"utf-8\"))\n        data.append(raw)\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Convert the PIL Image object to bytes using the tobytes() method.\n        # The tobytes() method returns a bytes object containing the raw pixel data of the image.\n        # The mode and dimensions of the image are encoded in the bytes object.\n        # The mode is encoded in UTF-8 and the dimensions are encoded as 32-bit unsigned integers.\n        # The raw pixel data is appended to the end of the bytes object.\n        # The resulting bytes object can be used to store or transmit the image data in a standardized format.\n\n        if not isinstance(item, Image.Image):\n            raise TypeError(f\"The provided item should be of type {Image.Image}. Found {item}.\")\n\n        data = io.BytesIO()\n        item.save(data, format=\"PNG\")\n        data.seek(0)\n        return data.read(), None\n\n    "}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            filename = item.filename\n            if filename and os.path.exists(filename):\n                with open(filename, \"rb\") as f:\n                    return f.read(), None\n\n        if _TORCH_VISION_AVAILABLE:\n            return decode_jpeg(item).numpy().tobytes(), None\n\n        with io.BytesIO() as f:\n            item.save(f, format=\"JPEG\")\n            return f.getvalue(), None\n\n    "}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n        if _TORCH_VISION_AVAILABLE:\n            array = torch.frombuffer(item.tobytes(), dtype=torch.uint8)\n            return decode_jpeg(array).numpy().tobytes(), None\n        return item.tobytes(), None"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n\n        if isinstance(item, Image.Image):\n            with io.BytesIO() as f:\n                item.save(f, format=\"JPEG\")\n                return f.getvalue(), None\n\n        raise TypeError(f\"Unsupported image type: {type(item)}\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if not _PIL_AVAILABLE:\n            raise ModuleNotFoundError(\"PIL is required. Run `pip install Pillow`\")\n\n        if not _TORCH_VISION_AVAILABLE:\n            raise ModuleNotFoundError(\"torchvision is required. Run `pip install torchvision`\")\n\n        if isinstance(item, JpegImageFile):\n            return item.tobytes(), None\n        return pil_to_tensor(item).numpy().tobytes(), None"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            return item.read(), None\n\n        if _TORCH_VISION_AVAILABLE:\n            return decode_jpeg(pil_to_tensor(item)).numpy().tobytes(), None\n\n        img_bytes = io.BytesIO()\n        item.save(img_bytes, format=\"JPEG\")\n        return img_bytes.getvalue(), None"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            if isinstance(item, JpegImageFile):\n                filename = item.filename\n                if filename and os.path.exists(filename):\n                    with open(filename, \"rb\") as f:\n                        return f.read(), None\n\n            array = torch.frombuffer(item.tobytes(), dtype=torch.uint8)\n            try:\n                return decode_jpeg(array), None\n            except RuntimeError:\n                # Note: Some datasets like Imagenet contains some PNG images with JPEG extension, so we fallback to PIL\n                pass\n\n        img = PILSerializer.serialize(item)\n        if _TORCH_VISION_AVAILABLE:\n            img = pil_to_tensor(img)\n        return img\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            if isinstance(item, JpegImageFile):\n                return item.read_bytes(), None\n            return decode_jpeg(pil_to_tensor(item)).numpy().tobytes(), None\n\n        if isinstance(item, Image.Image):\n            with io.BytesIO() as f:\n                item.save(f, format=\"JPEG\")\n                return f.getvalue(), None\n\n        raise TypeError(f\"{item} is not a supported image type.\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            with open(item.filename, \"rb\") as f:\n                return f.read(), None\n\n        if not isinstance(item, Image.Image):\n            raise TypeError(\"Only PIL Image is supported\")\n\n        if item.mode != \"RGB\":\n            item = item.convert(\"RGB\")\n\n        with io.BytesIO() as output:\n            item.save(output, format=\"JPEG\")\n            return output.getvalue(), None\n\n    "}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            if item.filename:\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n\n        if isinstance(item, Image.Image):\n            img_bytes = io.BytesIO()\n            item.save(img_bytes, format=\"JPEG\")\n            return img_bytes.getvalue(), None\n\n        raise TypeError(f\"{item} is not supported\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            # If the item is a JPEG and has a defined filename that exists, read the file directly\n            if item.filename and os.path.isfile(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                raise TypeError(\"JPEGSerializer can only serialize JPEG images with a defined filename.\")\n\n        if isinstance(item, Image.Image):\n            # Convert the item into JPEG format in memory\n            with io.BytesIO() as f:\n                item.save(f, format=\"JPEG\")\n                return f.getvalue(), None\n        else:\n            raise TypeError(\"JPEGSerializer can only serialize JPEG images.\")\n\n    "}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise ModuleNotFoundError(\"torchvision is required. Run `pip install torchvision`\")\n\n        if not isinstance(item, JpegImageFile):\n            raise TypeError(\"JPEGSerializer can only serialize JPEG images. Got: {}\".format(type(item)))\n\n        if item.filename and os.path.exists(item.filename):\n            with open(item.filename, \"rb\") as f:\n                return f.read(), None\n\n        with io.BytesIO() as output:\n            item.save(output, format=\"JPEG\")\n            return output.getvalue(), None\n\n    "}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if not isinstance(item, Image.Image):\n            raise TypeError(f\"{type(item)} is not a supported image type.\")\n\n        if not isinstance(item, JpegImageFile):\n            with io.BytesIO() as f:\n                item.save(f, format=\"JPEG\")\n                return f.getvalue(), None\n\n        if isinstance(item, JpegImageFile):\n            if item.filename and os.path.isfile(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                with io.BytesIO() as f:\n                    item.save(f, format=\"JPEG\")\n                    return f.getvalue(), None\n\n    "}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            if isinstance(item, JpegImageFile):\n                if os.path.isfile(item.filename):\n                    with open(item.filename, \"rb\") as f:\n                        return f.read(), None\n                else:\n                    # Note: Some datasets like Imagenet contains some PNG images with JPEG extension, so we fallback to PIL\n                    pass\n\n        if _PIL_AVAILABLE:\n            if isinstance(item, Image.Image):\n                with io.BytesIO() as f:\n                    item.save(f, format=\"JPEG\")\n                    return f.getvalue(), None\n\n        raise TypeError(f\"Expected `item` to be an instance of `Image`, got {type(item)}.\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            if item.filename and os.path.isfile(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                raise ValueError(f\"JPEG image file {item.filename} does not exist.\")\n\n        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(item).tobytes(), None\n            except RuntimeError:\n                pass\n\n        if _PIL_AVAILABLE:\n            img = pil_to_tensor(item)\n            return img.tobytes(), None\n\n        raise TypeError(f\"Unsupported image type {type(item)}\")\n\n    "}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            filename = item.filename\n            if not filename:\n                raise ValueError(\"The filename is not defined.\")\n            if not os.path.exists(filename):\n                raise FileNotFoundError(f\"The file {filename} does not exist.\")\n            with open(filename, \"rb\") as f:\n                return f.read(), None\n\n        if _TORCH_VISION_AVAILABLE:\n            return decode_jpeg(item).numpy().tobytes(), None\n\n        # Note: Some datasets like Imagenet contains some PNG images with JPEG extension, so we fallback to PIL\n        if _PIL_AVAILABLE:\n            img = PILSerializer.serialize(item)[0]\n            return img, None\n\n        raise TypeError(\"The item is not a supported image type.\")\n\n    "}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if not isinstance(item, Image.Image):\n            raise TypeError(\"Only PIL Image is supported.\")\n\n        if isinstance(item, JpegImageFile):\n            return item.read(), None\n\n        # If the item is not a JPEG image, convert it to JPEG format in memory\n        buffer = io.BytesIO()\n        item.save(buffer, format=\"JPEG\")\n        return buffer.getvalue(), None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise ModuleNotFoundError(\"torchvision is required. Run `pip install torchvision`\")\n\n        if isinstance(item, JpegImageFile):\n            if item.filename and os.path.isfile(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                raise FileNotFoundError(f\"{item.filename} not found.\")\n\n        if not isinstance(item, Image.Image):\n            raise TypeError(f\"Expected PIL Image, got {type(item)}.\")\n\n        if item.mode != \"RGB\":\n            item = item.convert(\"RGB\")\n\n        img_buffer = io.BytesIO()\n        item.save(img_buffer, format=\"JPEG\")\n        return img_buffer.getvalue(), None\n\n    "}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            if isinstance(item, JpegImageFile):\n                # TODO: we should check the file exists\n                return item.read(), None\n            else:\n                array = torch.frombuffer(item.tobytes(), dtype=torch.uint8)\n                return decode_jpeg(array).numpy().tobytes(), None\n\n        if isinstance(item, Image.Image):\n            # TODO: we should check the file exists\n            return item.tobytes(), None\n\n        raise TypeError(f\"The item {item} is not a supported image type.\")\n\n    "}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                raise ValueError(\"The JPEG image item has no filename or the filename does not exist.\")\n\n        if _TORCH_VISION_AVAILABLE:\n            if item.format == \"JPEG\":\n                return item.tobytes(), None\n            return decode_jpeg(item.tobytes()).tobytes(), None\n\n        if _PIL_AVAILABLE:\n            if item.format == \"JPEG\":\n                with io.BytesIO() as f:\n                    item.save(f, format=\"JPEG\")\n                    return f.getvalue(), None\n            return item.tobytes(), None\n\n        raise TypeError(f\"Unsupported image type: {type(item)}\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            # Note: JpegImageFile is a subclass of Image, so we can not use isinstance to check it\n            # Note: Some datasets like Imagenet contains some PNG images with JPEG extension, so we fallback to PIL\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n\n        # Note: Some datasets like Imagenet contains some PNG images with JPEG extension, so we fallback to PIL\n        if _TORCH_VISION_AVAILABLE:\n            array = torch.frombuffer(item.tobytes(), dtype=torch.uint8)\n            try:\n                return decode_jpeg(array).tobytes(), None\n            except RuntimeError:\n                pass\n\n        img = PILSerializer.serialize(item)\n        if _TORCH_VISION_AVAILABLE:\n            img = pil_to_tensor(img)\n        return img, None\n\n    "}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width = np.frombuffer(data[0:4], np.uint32).item()\n        height = np.frombuffer(data[4:8], np.uint32).item()\n        mode_size = np.frombuffer(data[8:12], np.uint32).item()\n        mode = data[12:12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size:]\n        return Image.frombytes(mode, (width, height), raw)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = np.frombuffer(data[0:12], np.uint32)\n        mode = data[12:12 + mode_size]\n        raw = data[12 + mode_size:]\n        return Image.frombytes(mode.decode(\"utf-8\"), (width, height), raw)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = np.frombuffer(data[0:12], np.uint32)\n        mode = data[12:12 + mode_size]\n        raw = data[12 + mode_size:]\n        return Image.frombytes(mode.decode(\"utf-8\"), (width, height), raw)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width = np.frombuffer(data[0:4], np.uint32).item()\n        height = np.frombuffer(data[4:8], np.uint32).item()\n        mode_size = np.frombuffer(data[8:12], np.uint32).item()\n        mode = data[12:12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size:]\n        return Image.frombytes(mode, (width, height), raw)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width = np.frombuffer(data[0:4], np.uint32).item()\n        height = np.frombuffer(data[4:8], np.uint32).item()\n        mode_size = np.frombuffer(data[8:12], np.uint32).item()\n        mode = data[12:12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size:]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = np.frombuffer(data[0:12], np.uint32)\n        mode = data[12:12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size:]\n        return Image.frombuffer(mode, (width, height), raw, \"raw\")\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = np.frombuffer(data[0:12], np.uint32)\n        mode = data[12:12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size:]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = np.frombuffer(data[0:12], np.uint32)\n        mode = data[12:12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size :]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[0:12], np.uint32)\n        width, height, mode_size = ints[0], ints[1], ints[2]\n        mode = data[12:12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size:]\n        return Image.frombuffer(mode, (width, height), raw, \"raw\", mode, 0, 1)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[0:12], np.uint32)\n        width, height, mode_size = ints[0], ints[1], ints[2]\n        mode = data[12 : 12 + mode_size]\n        raw = data[12 + mode_size :]\n        return Image.frombuffer(\"RGB\", (width, height), raw, \"raw\", mode, 0, 1)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = np.frombuffer(data[0:12], np.uint32)\n        mode = data[12:12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size:]\n        return Image.frombuffer(mode, (width, height), raw, \"raw\", mode, 0, 1)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[0:12], np.uint32)\n        mode = data[12:12 + ints[2]].decode(\"utf-8\")\n        width, height, _ = ints\n        raw = data[12 + ints[2]:]\n        return Image.frombuffer(mode, (width, height), raw, \"raw\")"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width = np.frombuffer(data[0:4], np.uint32).item()\n        height = np.frombuffer(data[4:8], np.uint32).item()\n        mode_size = np.frombuffer(data[8:12], np.uint32).item()\n        mode = data[12:12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size:]\n        return Image.frombuffer(mode, (width, height), raw, \"raw\", mode, 0, 1)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[0:12], np.uint32)\n        width, height, mode_size = ints[0], ints[1], ints[2]\n        mode = data[12:12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size:]\n        return Image.frombuffer(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[0:12], np.uint32)\n        width, height, mode_size = ints[0], ints[1], ints[2]\n        mode = data[12 : 12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size :]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width = np.frombuffer(data[0:4], np.uint32).item()\n        height = np.frombuffer(data[4:8], np.uint32).item()\n        mode_size = np.frombuffer(data[8:12], np.uint32).item()\n        mode = data[12:12 + mode_size]\n        raw = data[12 + mode_size:]\n        return Image.frombytes(mode.decode(\"utf-8\"), (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[0:12], np.uint32)\n        width, height, mode_size = ints[0], ints[1], ints[2]\n        mode = data[12:12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size:]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = np.frombuffer(data[0:12], np.uint32)\n        mode = data[12 : 12 + mode_size]\n        item = Image.frombytes(mode.decode(\"utf-8\"), (width, height), data[12 + mode_size :])\n        return item\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = np.frombuffer(data[0:12], np.uint32)\n        mode = data[12 : 12 + mode_size]\n        raw = data[12 + mode_size :]\n        return Image.frombuffer(mode.decode(\"utf-8\"), (width, height), raw, \"raw\", mode.decode(\"utf-8\"), 0, 1)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width = np.frombuffer(data[0:4], np.uint32).item()\n        height = np.frombuffer(data[4:8], np.uint32).item()\n        mode_size = np.frombuffer(data[8:12], np.uint32).item()\n        mode = data[12 : 12 + mode_size]\n        raw = data[12 + mode_size : len(data)]\n        return Image.frombuffer(\"RGB\", (width, height), raw, \"raw\", mode)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return torch.from_numpy(tensor)\n        return torch.from_numpy(np.reshape(tensor, shape))"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return torch.from_numpy(tensor)\n        return torch.from_numpy(np.reshape(tensor, shape))"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the tensor bytes\n        tensor = torch.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return tensor.reshape(shape)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n\n    "}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the tensor bytes\n        tensor = torch.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return tensor.reshape(shape)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return torch.from_numpy(tensor)\n        return torch.from_numpy(np.reshape(tensor, shape))"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the tensor bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return torch.from_numpy(tensor)\n        return torch.from_numpy(np.reshape(tensor, shape))\n\n    "}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # extract the data type index from the first 4 bytes of the input byte array\n        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n\n        # extract the shape size from the next 4 bytes of the input byte array\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the tensor bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return torch.from_numpy(tensor)\n        return torch.from_numpy(np.reshape(tensor, shape))\n\n    "}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None\n\n    "}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        return pil_to_tensor(Image.open(io.BytesIO(data)))"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        return pil_to_tensor(Image.open(io.BytesIO(data)))"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        return pil_to_tensor(Image.open(io.BytesIO(data)))"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        return pil_to_tensor(Image.open(io.BytesIO(data)))"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        return pil_to_tensor(Image.open(io.BytesIO(data)))"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        if _PIL_AVAILABLE:\n            return pil_to_tensor(Image.open(io.BytesIO(data)))\n\n        raise TypeError(f\"The provided item should be of type {JpegImageFile}. Found {type(data)}.\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        try:\n            return pil_to_tensor(Image.open(io.BytesIO(data)))\n        except OSError:\n            pass\n\n        raise TypeError(f\"The provided data should be of type {JpegImageFile}. Found {data}.\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        try:\n            return pil_to_tensor(Image.open(io.BytesIO(data)))\n        except Exception:\n            raise TypeError(f\"The provided item should be of type {JpegImageFile}. Found {Image.open(io.BytesIO(data))}.\")\n\n    "}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        try:\n            return pil_to_tensor(Image.open(io.BytesIO(data)))\n        except OSError:\n            pass\n\n        raise TypeError(f\"The provided data should be of type {JpegImageFile}. Found {data}.\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise ModuleNotFoundError(\"torchvision is required. Run `pip install torchvision`\")\n\n        try:\n            return decode_jpeg(data)\n        except RuntimeError:\n            # fallback to PIL\n            return pil_to_tensor(Image.open(io.BytesIO(data)))\n\n    "}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        try:\n            return pil_to_tensor(Image.open(io.BytesIO(data)))\n        except Exception:\n            raise TypeError(\"The provided data is not a valid JPEG image.\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        try:\n            return pil_to_tensor(Image.open(io.BytesIO(data)))\n        except OSError:\n            pass\n\n        raise ValueError(f\"The provided data is not a valid JPEG image. Found {data}.\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        try:\n            return pil_to_tensor(Image.open(io.BytesIO(data)))\n        except Exception:\n            pass\n\n        raise ValueError(\"The data is not a valid JPEG image.\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        try:\n            return pil_to_tensor(Image.open(io.BytesIO(data)))\n        except Exception:\n            pass\n\n        raise ValueError(\"The data is not a JPEG image.\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n        try:\n            return pil_to_tensor(Image.open(io.BytesIO(data)))\n        except Exception:\n            raise ValueError(\n                \"The provided data is not a valid JPEG image. HINT: Open the image in your Dataset __getitem__ method.\"\n            )\n\n    "}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE and _AV_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        if _PIL_AVAILABLE:\n            return pil_to_tensor(Image.open(io.BytesIO(data)))\n\n        raise ValueError(\"Unable to deserialize the image.\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                jpeg_image = decode_jpeg(data)\n                if isinstance(jpeg_image, torch.Tensor):\n                    return jpeg_image\n            except RuntimeError:\n                pass\n\n        if _PIL_AVAILABLE:\n            image = Image.open(io.BytesIO(data))\n            if _TORCH_VISION_AVAILABLE:\n                return pil_to_tensor(image)\n            return image\n        raise TypeError(\"The provided data is not a valid JPEG image.\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE and isinstance(data, bytes):\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                # this is a PNG with a JPEG extension\n                pass\n\n        if _PIL_AVAILABLE:\n            return pil_to_tensor(Image.open(io.BytesIO(data)))\n\n        raise TypeError(f\"The provided data should be of type {bytes}. Found {type(data)}.\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError as e:\n                if \"Invalid JPEG data\" in str(e):\n                    pass\n                else:\n                    raise e\n\n        try:\n            return pil_to_tensor(Image.open(io.BytesIO(data)))\n        except Exception as e:\n            raise RuntimeError(f\"Unable to deserialize the image. {e}\")\n\n    "}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            if _TORCH_VISION_AVAILABLE:\n                # try to decode the data as a JPEG image\n                # if the decoding fails, it will raise an exception\n                decoded_jpeg = decode_jpeg(data)\n                if _TORCH_VISION_AVAILABLE:\n                    return decoded_jpeg\n        except RuntimeError:\n            # if the decoding fails, try to use PIL to deserialize the data\n            pil_image = Image.open(io.BytesIO(data))\n            if _TORCH_VISION_AVAILABLE:\n                # convert the PIL image to a PyTorch tensor\n                return pil_to_tensor(pil_image)\n            else:\n                return pil_image\n\n        raise ValueError(\"Unable to decode the data as a JPEG image.\")"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\"\n\n    "}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n\n    "}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        return np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype).reshape(shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        return tensor.reshape(shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        array = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if array.shape == shape:\n            return array\n        return np.reshape(array, shape)\n\n    "}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = np.array(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n\n    "}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        array = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if array.shape == shape:\n            return array\n        return array.reshape(shape)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n\n    "}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n\n    "}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n\n    "}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            state = {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            state = {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        return state\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            state = {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            state = {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n        return state\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_streaming\n            if isinstance(self.dataset, StreamingDataset)\n            else self._num_samples_yielded_combined,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n\n        if isinstance(self.dataset, StreamingDataset):\n            state[\"dataset\"] = self.dataset.state_dict()\n\n        return state"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            state = {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            state = {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": deepcopy(self._num_samples_yielded_combined),\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n        return state"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n        return {\n            \"dataset\": self.dataset.state_dict(),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_combined,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE or not _AV_AVAILABLE:\n            raise ImportError(\"torchvision and av are required to deserialize video data.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.seek(0)\n            video, _, _ = torchvision.io.read_video(f.name)\n        return video"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise ImportError(\"torchvision and av are required to deserialize video data.\")\n        from torchvision.io import read_video\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.seek(0)\n            video, _, _ = read_video(f.name)\n        return video"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE or not _AV_AVAILABLE:\n            raise RuntimeError(\"torchvision and av are required to deserialize video.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.seek(0)\n            video, _, _ = torchvision.io.read_video(f.name, pts_unit=\"sec\")\n        return video"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise RuntimeError(\"torchvision is not installed. Please install torchvision to use this function.\")\n        if not _AV_AVAILABLE:\n            raise RuntimeError(\"av is not installed. Please install av to use this function.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.seek(0)\n            video, _, _ = torchvision.io.read_video(f.name, pts_unit=\"sec\")\n        return video"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise RuntimeError(\"The av library is not installed. Please install it with `pip install av`.\")\n        if not _TORCH_VISION_AVAILABLE:\n            raise RuntimeError(\"The torchvision library is not installed. Please install it with `pip install torchvision`.\")\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.flush()\n            video, _, _ = torchvision.io.read_video(f.name)\n        return video"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE or not _AV_AVAILABLE:\n            raise RuntimeError(\"torchvision and av must be installed to use the VideoSerializer.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.seek(0)\n            video, _, _ = torchvision.io.read_video(f.name)\n        return video"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise ImportError(\"The `av` library is not installed. Please install it with `pip install av`.\")\n        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\"The `torchvision` library is not installed. Please install it with `pip install torchvision`.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.seek(0)\n            return torchvision.io.read_video(f.name)"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise ModuleNotFoundError(\"torchvision and av are required to deserialize video data.\")\n        import av\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.flush()\n            container = av.open(f.name)\n            video = container.streams.video[0]\n            return video\n\n    "}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\"torchvision is not installed. Please install it with `pip install torchvision`.\")\n\n        if not _AV_AVAILABLE:\n            raise ImportError(\"av is not installed. Please install it with `pip install av`.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".tmp\") as f:\n            f.write(data)\n            f.seek(0)\n            video, _, _ = torchvision.io.read_video(f.name)\n        return video\n\n    "}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE or not _AV_AVAILABLE:\n            raise RuntimeError(\n                \"The torchvision and av libraries are required to deserialize videos. Please install them with `pip install torchvision av`.\"\n            )\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.seek(0)\n            video, _, _ = torchvision.io.read_video(f.name, pts_unit=\"sec\")\n        return video"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise RuntimeError(\n                \"The `av` library is required to deserialize video data. Please install it with `pip install av`.\"\n            )\n        if not _TORCH_VISION_AVAILABLE:\n            raise RuntimeError(\n                \"The `torchvision` library is required to deserialize video data. Please install it with `pip install torchvision`.\"\n            )\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.seek(0)\n            return read_video(f.name)"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\"The torchvision library is required to deserialize video data.\")\n        if not _AV_AVAILABLE:\n            raise ImportError(\"The av library is required to deserialize video data.\")\n        with tempfile.NamedTemporaryFile(delete=False) as f:\n            f.write(data)\n            f.flush()\n            video, _, _ = torchvision.io.read_video(f.name)\n        os.remove(f.name)\n        return video"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE or not _AV_AVAILABLE:\n            raise RuntimeError(\"torchvision and av are required to use VideoSerializer.\")\n\n        with tempfile.NamedTemporaryFile(\"wb\") as f:\n            f.write(data)\n            f.seek(0)\n            video, _, _ = torchvision.io.read_video(f.name)\n        return video\n\n    "}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise RuntimeError(\"torchvision and av must be installed to deserialize video data.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as f:\n            f.write(data)\n            video, _, _ = torchvision.io.read_video(f.name, pts_unit=\"sec\")\n            os.unlink(f.name)\n        return video"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise ModuleNotFoundError(\"torchvision and av are required to use VideoSerializer.\")\n\n        with tempfile.NamedTemporaryFile(delete=False) as f:\n            f.write(data)\n            video, _, _ = torchvision.io.read_video(f.name)\n\n        return video"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE or not _TORCH_VISION_AVAILABLE:\n            raise ModuleNotFoundError(\"The required libraries are not installed. Please install torchvision and av.\")\n\n        from torchvision.io import read_video\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.seek(0)\n            video, _, _ = read_video(f.name, pts_unit=\"sec\")\n        return video\n\n    "}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\n                \"torchvision is required to deserialize video. Please install torchvision to use this feature.\"\n            )\n        if not _AV_AVAILABLE:\n            raise ImportError(\"av is required to deserialize video. Please install av to use this feature.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as tmp_file:\n            tmp_file.write(data)\n            tmp_file.seek(0)\n            video, _, _ = torchvision.io.read_video(tmp_file.name)\n        return video"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE or not _TORCH_VISION_AVAILABLE:\n            raise ModuleNotFoundError(\n                \"torchvision and av are required to deserialize video files. Please install them using `pip install av torchvision`.\"\n            )\n        import av\n        import torchvision\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.seek(0)\n            return torchvision.io.read_video(f.name)"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise ImportError(\"The torchvision library is required for video deserialization. Please install it.\")\n        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\"The torchvision library is required for video deserialization. Please install it.\")\n\n        # write the data to a temporary file\n        with tempfile.NamedTemporaryFile(delete=False) as f:\n            f.write(data)\n            filepath = f.name\n\n        # use torchvision's read_video function to deserialize the video file into a video object\n        video, _, _ = torchvision.io.read_video(filepath, pts_unit=\"sec\")\n        return video"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise ModuleNotFoundError(\n                \"The torchvision library is required for deserializing video. Please install it with `pip install torchvision`.\"\n            )\n        if not _PIL_AVAILABLE:\n            raise ModuleNotFoundError(\n                \"The PIL library is required for deserializing video. Please install it with `pip install Pillow`.\"\n            )\n\n        with tempfile.NamedTemporaryFile(\"wb\") as f:\n            f.write(data)\n            f.flush()\n            video, _, _ = torchvision.io.read_video(f.name)\n        return video"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        while self._should_write():\n            self.write_chunk()\n        self.write_chunks_index()\n        self._is_done = True\n        return [os.path.join(self._cache_dir, self.get_chunk_filename())]"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self.filled:\n            return []\n\n        if self._should_write():\n            self.write_chunk(on_done=True)\n\n        self.write_chunks_index()\n        self._is_done = True\n        return [os.path.join(self._cache_dir, self.get_chunk_filename())]"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        while self._should_write():\n            self.write_chunk()\n\n        self._is_done = True\n        return self.write_chunks_index()"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return []\n\n        # Write any remaining chunks\n        while self._should_write():\n            self.write_chunk()\n\n        # Write the chunks index\n        self.write_chunks_index()\n        self._is_done = True\n        return [os.path.join(self._cache_dir, self.get_chunk_filename())]"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return []\n\n        self._is_done = True\n\n        if self._should_write():\n            self.write_chunk(on_done=True)\n\n        return self.write_chunks_index()"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        # Write the remaining chunks\n        while self._should_write():\n            self.write_chunk()\n\n        # Write the chunks index\n        self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        # Return the file paths of the written chunks\n        return [os.path.join(self._cache_dir, self.get_chunk_filename())]"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self.filled:\n            return []\n\n        # Write the remaining chunks\n        while self._should_write():\n            self.write_chunk()\n\n        # Write the index file\n        self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        # Return the list of written chunk file paths\n        return [os.path.join(self._cache_dir, self.get_chunk_filename())]"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return []\n\n        while not self._should_write():\n            sleep(0.01)\n\n        filepaths = []\n        while self._should_write():\n            filepaths.append(self.write_chunk())\n\n        self.write_chunks_index()\n        self._is_done = True\n        return filepaths"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        while self._should_write():\n            self.write_chunk()\n\n        self._is_done = True\n\n        return self.write_chunks_index()"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self.filled:\n            return []\n\n        # Write any remaining chunks\n        while self._should_write():\n            self.write_chunk()\n\n        # Write the chunks index\n        self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        # Return the list of written chunk file paths\n        return [os.path.join(self._cache_dir, self.get_chunk_filename())]"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return []\n\n        # Write the remaining chunks\n        while self._should_write():\n            self.write_chunk()\n\n        # Write the index file\n        self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        # Return the list of file paths to the written chunks\n        return [os.path.join(self._cache_dir, f) for f in os.listdir(self._cache_dir) if f.endswith(\".bin\")]"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return []\n\n        # Write any remaining chunks\n        while self._should_write():\n            self.write_chunk()\n\n        # Write the chunks index\n        self.write_chunks_index()\n\n        # Mark the process as done\n        self._is_done = True\n\n        # Return the list of written chunks\n        return [os.path.join(self._cache_dir, self.get_chunk_filename())]"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self.filled:\n            return []\n\n        while not self.filled:\n            if self._should_write():\n                self.write_chunk()\n            sleep(0.01)\n\n        self._is_done = True\n\n        return [os.path.join(self._cache_dir, self.get_chunk_filename())]"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return []\n\n        while not self.filled:\n            sleep(0.01)\n\n        self._is_done = True\n\n        if self._serialized_items:\n            self.write_chunk(on_done=True)\n\n        return self.write_chunks_index()"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        while self._should_write():\n            self.write_chunk()\n\n        if not self.filled:\n            raise RuntimeError(\n                \"The writer is not done, but the index file was not created. This shouldn't have happened.\"\n                f\" Found {self._pretty_serialized_items()}.\"\n            )\n\n        self.write_chunks_index()\n        self._is_done = True\n        return [os.path.join(self._cache_dir, self.get_chunk_filename())]"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return []\n\n        # If the writer is not filled, then wait for it to be filled.\n        while not self.filled:\n            sleep(0.01)\n\n        # Write any remaining chunks\n        if self._should_write():\n            self.write_chunk(on_done=True)\n\n        # Write the index file\n        self.write_chunks_index()\n\n        # Mark the writer as done\n        self._is_done = True\n\n        # Return the list of chunk file paths\n        return [os.path.join(self._cache_dir, f\"chunk-{self.rank}-{i}.bin\") for i in range(self._chunk_index)]"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return []\n\n        # If there are no items, we can't write anything\n        if not self._serialized_items:\n            return []\n\n        # If the writer is not filled, we should write the chunk\n        if not self.filled:\n            self._should_write()\n            self.write_chunk()\n\n        # Write the index file\n        self.write_chunks_index()\n        self._is_done = True\n        return [os.path.join(self._cache_dir, self.get_chunk_filename())]"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        # If the writer is already done, return the chunks that have been written.\n        if self._is_done:\n            return [os.path.join(self._cache_dir, f) for f in os.listdir(self._cache_dir) if f.endswith(\".bin\")]\n\n        # If there are no serialized items, return an empty list.\n        if not self._serialized_items:\n            return []\n\n        # If there are serialized items, write the remaining chunks and return the file paths.\n        while self._should_write():\n            self.write_chunk()\n\n        # Write the chunks index file and mark the writer as done.\n        self.write_chunks_index()\n        self._is_done = True\n\n        # Return the file paths of the written chunks.\n        return [os.path.join(self._cache_dir, f) for f in os.listdir(self._cache_dir) if f.endswith(\".bin\")]"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        # TODO: This is a hack to make sure the chunk is written before the index file is written.\n        # This is a temporary solution and should be removed once the index file is written before the chunk.\n        while not self._should_write():\n            sleep(0.01)\n\n        # Write the chunk if there is any\n        if self._should_write():\n            self.write_chunk(on_done=True)\n\n        # Write the index file\n        self.write_chunks_index()\n\n        # Mark the writer as done\n        self._is_done = True\n\n        # Return the list of chunks written\n        return [os.path.join(self._cache_dir, self.get_chunk_filename())]"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        # TODO: This is a temporary fix until we have a better way to handle the case when the user doesn't have\n        #  enough workers.\n        if self.rank == 0:\n            while not self.filled:\n                sleep(0.01)\n\n        if self.filled:\n            # Write the last chunk\n            if self._should_write():\n                self.write_chunk(on_done=True)\n\n            # Write the index file\n            self.write_chunks_index()\n\n        # Mark the writer as done\n        self._is_done = True\n\n        # Return the file paths of the chunks\n        files = os.listdir(self._cache_dir)\n        return [os.path.join(self._cache_dir, f) for f in files if f.endswith(\".bin\")]"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.restore = True\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.restore = True\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.restore = True\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\")\n\n        self.restore = True\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.restore = True\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.restore = True\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.restore = True\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.restore = True\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self._worker_idx_iter = iter(self._worker_idx)\n        self.restore = True\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.restore = True"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.restore = True"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n        self.restore = True\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.restore = True\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.restore = True\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self._worker_idx_iter = iter(self._worker_idx)\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self._worker_idx_iter = iter(self._worker_idx)\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.restore = True\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.restore = True"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self._worker_idx_iter = iter(self._worker_idx)\n        self.restore = True"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.restore = True\n        self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n        self._worker_idx_iter = iter(self._worker_idx)"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.current_epoch = obj[\"current_epoch\"]\n        self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n        self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        self.restore = True"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n            self._worker_idx_iter = iter(self._worker_idx)\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n            self._worker_idx_iter = iter(self._worker_idx)\n        else:\n            raise RuntimeError(\"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\")\n\n        self.restore = True"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.restore = True"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self._worker_idx_iter = iter(self._worker_idx)\n            self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n            self.restore = True\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self._worker_idx_iter = iter(self._worker_idx)\n            self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n            self.restore = True\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self._worker_idx_iter = iter(self._worker_idx)\n            self.restore = True\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self._worker_idx_iter = iter(self._worker_idx)\n            self.restore = True\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            return {}\n\n        if num_samples_yielded is not None:\n            return self._iterator.state_dict(num_workers, batch_size)\n\n        return _state_dict(self._datasets, self._num_samples_yielded, num_workers, batch_size)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            return {}\n\n        if num_samples_yielded is not None:\n            return self._iterator.state_dict(num_workers, batch_size)\n\n        return _state_dict(self._datasets, self._num_samples_yielded, num_workers, batch_size)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            return {}\n\n        if num_samples_yielded is not None:\n            return self._iterator.state_dict(num_workers, batch_size)\n\n        return {\n            \"dataset\": _state_dict(self._datasets, self._num_samples_yielded, num_workers, batch_size),\n            \"num_samples_yielded\": self._num_samples_yielded,\n        }"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            return {}\n\n        if num_samples_yielded is not None:\n            return self._iterator.state_dict(num_workers=num_workers, batch_size=batch_size)\n\n        return _state_dict(self._datasets, self._num_samples_yielded, num_workers, batch_size)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            return {}\n\n        if num_samples_yielded is not None:\n            return self._iterator.state_dict(num_workers=num_workers, batch_size=batch_size)\n\n        return _state_dict(self._datasets, self._num_samples_yielded, num_workers, batch_size)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            return {}\n\n        if num_samples_yielded is not None:\n            return self._iterator.state_dict(num_workers=num_workers, batch_size=batch_size)\n\n        return {\n            \"dataset\": _state_dict(self._datasets, self._num_samples_yielded, num_workers, batch_size),\n            \"num_samples_yielded\": self._num_samples_yielded,\n        }"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is None:\n            return _state_dict(self._datasets, num_samples_yielded, num_workers, batch_size)\n\n        return self._iterator.state_dict(num_workers, batch_size)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            return {}\n\n        if self._use_streaming_dataloader:\n            return {\n                \"dataset\": self._iterator.state_dict(num_workers=num_workers, batch_size=batch_size),\n                \"num_samples_yielded\": self._num_samples_yielded,\n            }\n\n        return self._iterator.state_dict(num_workers=num_workers, batch_size=batch_size)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            return {}\n\n        if self._use_streaming_dataloader:\n            return {\n                \"dataset\": self._iterator.state_dict(num_workers=num_workers, batch_size=batch_size),\n                \"num_samples_yielded\": self._num_samples_yielded,\n            }\n\n        return self._iterator.state_dict(num_workers=num_workers, batch_size=batch_size)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            return {}\n\n        if num_samples_yielded is not None:\n            self._num_samples_yielded = num_samples_yielded\n\n        return self._iterator.state_dict(num_workers, batch_size)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            return {}\n\n        if self._use_streaming_dataloader:\n            return self._iterator.state_dict(num_workers, batch_size)\n\n        if num_samples_yielded is None:\n            return {}\n\n        return {\n            \"dataset\": _state_dict(self._datasets, num_samples_yielded, num_workers, batch_size),\n            \"num_samples_yielded\": num_samples_yielded,\n        }"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            return {}\n\n        if num_samples_yielded is not None:\n            return self._iterator.state_dict(num_workers, batch_size)\n\n        if self._use_streaming_dataloader:\n            self._num_samples_yielded = self._iterator._num_samples_yielded\n\n        return _state_dict(self._datasets, self._num_samples_yielded, num_workers, batch_size)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            return {}\n\n        if self._use_streaming_dataloader:\n            return self._iterator.state_dict(num_workers, batch_size)\n\n        if num_samples_yielded is not None:\n            return _state_dict(self._datasets, num_samples_yielded, num_workers, batch_size)\n\n        return _state_dict(self._datasets, self._num_samples_yielded, num_workers, batch_size)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            return {}\n\n        if num_samples_yielded is not None:\n            return self._iterator.state_dict(num_workers=num_workers, batch_size=batch_size)\n\n        return _state_dict(self._datasets, self._num_samples_yielded, num_workers=num_workers, batch_size=batch_size)\n\n    "}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is not None:\n            return self._iterator.state_dict(num_workers=num_workers, batch_size=batch_size)\n\n        return _state_dict(self._datasets, num_samples_yielded, num_workers=num_workers, batch_size=batch_size)\n\n    "}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            return {}\n\n        if num_samples_yielded is not None:\n            return self._iterator.state_dict(num_workers, batch_size)\n\n        return {\n            \"dataset\": {\n                str(dataset_idx): dataset.state_dict(num_samples_yielded=num_samples_yielded[dataset_idx])\n                for dataset_idx, dataset in enumerate(self._datasets)\n            },\n            \"num_samples_yielded\": num_samples_yielded,\n        }"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._use_streaming_dataloader:\n            return {\n                \"dataset\": self._iterator.state_dict(num_workers=num_workers, batch_size=batch_size),\n                \"num_samples_yielded\": self._num_samples_yielded,\n            }\n\n        return {\n            \"dataset\": _state_dict(self._datasets, self._num_samples_yielded, num_workers=num_workers, batch_size=batch_size)\n        }\n\n    "}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._use_streaming_dataloader:\n            return self._iterator.state_dict(num_workers, batch_size)\n\n        return {\n            \"dataset\": _state_dict(\n                self._datasets, self._num_samples_yielded, num_workers, batch_size\n            ),\n            \"num_samples_yielded\": self._num_samples_yielded,\n        }"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            return {}\n\n        if self._use_streaming_dataloader:\n            return self._iterator.state_dict(num_workers=num_workers, batch_size=batch_size)\n\n        if num_samples_yielded is not None:\n            return {\n                \"dataset\": _state_dict(self._datasets, num_samples_yielded, num_workers, batch_size),\n                \"num_samples_yielded\": num_samples_yielded,\n            }\n\n        return {\n            \"dataset\": _state_dict(self._datasets, self._num_samples_yielded, num_workers, batch_size),\n            \"num_samples_yielded\": self._num_samples_yielded,\n        }"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            return {}\n\n        if num_samples_yielded is not None:\n            return self._iterator.state_dict(num_workers, batch_size)\n\n        if self._use_streaming_dataloader:\n            self._num_samples_yielded = self._iterator.state_dict(num_workers, batch_size)[__NUM_SAMPLES_YIELDED_KEY__]\n\n        return _state_dict(self._datasets, self._num_samples_yielded, num_workers, batch_size)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        for dataset_idx, dataset in enumerate(self._datasets):\n            dataset.load_state_dict(state_dict[str(dataset_idx)])\n\n        if self._use_streaming_dataloader:\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        for dataset_idx, dataset in enumerate(self._datasets):\n            dataset.load_state_dict(state_dict[str(dataset_idx)])\n\n        if self._use_streaming_dataloader:\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        for dataset in self._datasets:\n            dataset.load_state_dict(state_dict[str(id(dataset))])\n\n        if self._use_streaming_dataloader:\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        for dataset in self._datasets:\n            dataset.load_state_dict(state_dict[str(self._datasets.index(dataset))])\n\n        if self._use_streaming_dataloader:\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if not state_dict:\n            return\n\n        for dataset_idx, dataset in enumerate(self._datasets):\n            dataset.load_state_dict(state_dict[str(dataset_idx)])\n\n        if self._use_streaming_dataloader:\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if state_dict is None:\n            return\n\n        for dataset_idx, dataset in enumerate(self._datasets):\n            dataset_state_dict = state_dict.get(str(dataset_idx))\n            if dataset_state_dict is not None:\n                dataset.load_state_dict(dataset_state_dict)\n\n        if __NUM_SAMPLES_YIELDED_KEY__ in state_dict:\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if __SAMPLES_KEY__ in state_dict:\n            for dataset_idx, dataset in enumerate(self._datasets):\n                dataset.load_state_dict(state_dict[str(dataset_idx)])\n\n        if __NUM_SAMPLES_YIELDED_KEY__ in state_dict:\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self._check_datasets(self._datasets)\n\n        for dataset_idx, dataset in enumerate(self._datasets):\n            dataset.load_state_dict(state_dict[str(dataset_idx)])\n\n        if self._use_streaming_dataloader:\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if __NUM_SAMPLES_YIELDED_KEY__ not in state_dict:\n            return\n\n        num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n\n        for dataset_idx, dataset in enumerate(self._datasets):\n            dataset.load_state_dict(state_dict[str(dataset_idx)])\n\n        if self._use_streaming_dataloader:\n            self._num_samples_yielded = num_samples_yielded"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if state_dict is None:\n            return\n\n        for dataset_idx, dataset in enumerate(self._datasets):\n            dataset.load_state_dict(state_dict[str(dataset_idx)])\n\n        if self._use_streaming_dataloader:\n            self._num_samples_yielded = state_dict.get(__NUM_SAMPLES_YIELDED_KEY__)\n\n        if self._num_samples_yielded is not None:\n            for _ in range(sum(self._num_samples_yielded)):\n                self._rng.choices(self._dataset_indexes, weights=self._weights, k=1)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self._num_samples_yielded = None\n\n        for dataset_idx, dataset in enumerate(self._datasets):\n            dataset.load_state_dict(state_dict[str(dataset_idx)])\n\n        if self._use_streaming_dataloader:\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        # Update the state of each dataset within the CombinedStreamingDataset\n        for dataset in self._datasets:\n            dataset.load_state_dict(state_dict[str(self._datasets.index(dataset))])\n\n        # If the CombinedStreamingDataset is used with the StreamingDataLoader, update the number of samples yielded\n        if self._use_streaming_dataloader:\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if __NUM_SAMPLES_YIELDED_KEY__ in state_dict:\n            num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n            del state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n        else:\n            num_samples_yielded = None\n\n        for dataset in self._datasets:\n            dataset.load_state_dict(state_dict[str(id(dataset))])\n\n        if num_samples_yielded is not None:\n            self._num_samples_yielded = num_samples_yielded"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if state_dict is None:\n            return\n\n        # Update the state of each dataset within the CombinedStreamingDataset\n        for dataset_idx, dataset_state in state_dict.items():\n            dataset = self._datasets[int(dataset_idx)]\n            dataset.load_state_dict(dataset_state)\n\n        # Update the number of samples yielded by the streaming dataloader to avoid repeating samples\n        if self._use_streaming_dataloader:\n            self._num_samples_yielded = state_dict.get(__NUM_SAMPLES_YIELDED_KEY__)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if state_dict is None:\n            return\n\n        for dataset_idx, dataset_state_dict in state_dict.items():\n            dataset = self._datasets[int(dataset_idx)]\n            dataset.load_state_dict(dataset_state_dict)\n\n        if self._use_streaming_dataloader:\n            num_samples_yielded = state_dict.get(__NUM_SAMPLES_YIELDED_KEY__)\n            if num_samples_yielded is not None:\n                self._num_samples_yielded = num_samples_yielded"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self._check_datasets(state_dict)\n        self._num_samples_yielded = state_dict.get(__NUM_SAMPLES_YIELDED_KEY__)\n\n        for dataset_idx, dataset in enumerate(self._datasets):\n            dataset.load_state_dict(state_dict[str(dataset_idx)])"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        # Iterate over the datasets and load their state\n        for dataset_idx, dataset in enumerate(self._datasets):\n            dataset_state = state_dict.get(str(dataset_idx), None)\n            if dataset_state is not None:\n                dataset.load_state_dict(dataset_state)\n\n        # Update the number of samples yielded by the streaming dataloader\n        if self._use_streaming_dataloader:\n            self._num_samples_yielded = state_dict.get(__NUM_SAMPLES_YIELDED_KEY__, None)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        # Load the state of each dataset in the CombinedStreamingDataset\n        for dataset in self._datasets:\n            dataset.load_state_dict(state_dict[str(id(dataset))])\n\n        # Update the number of samples yielded by the streaming dataloader\n        if self._use_streaming_dataloader:\n            for dataset_idx, dataset in enumerate(self._datasets):\n                dataset.set_num_samples_yielded(state_dict[str(id(dataset))][__NUM_SAMPLES_YIELDED_KEY__])"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if state_dict is None:\n            return\n\n        for dataset_idx, dataset_state in state_dict.items():\n            dataset = self._datasets[int(dataset_idx)]\n            dataset.load_state_dict(dataset_state)\n\n        if self._use_streaming_dataloader:\n            if state_dict:\n                self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n            else:\n                self._num_samples_yielded = None\n\n    "}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if state_dict is None:\n            return\n\n        # Update the state of each dataset within the CombinedStreamingDataset\n        for dataset_idx, dataset in enumerate(self._datasets):\n            dataset.load_state_dict(state_dict[str(dataset_idx)])\n\n        # Update the number of samples yielded by the streaming dataloader to avoid repeating samples\n        if self._use_streaming_dataloader:\n            num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n            for dataset_idx, dataset in enumerate(self._datasets):\n                dataset.load_state_dict(state_dict[str(dataset_idx)])\n\n        if self._iterator is not None:\n            self._iterator.load_state_dict(state_dict)"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, str):\n        dir_path = Dir(path=dir_path)\n\n    if dir_path.path is None:\n        return dir_path\n\n    if dir_path.path.startswith(\"s3://\"):\n        return _resolve_s3(dir_path)\n\n    if dir_path.path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path.path, target_name=None, target_id=None)\n\n    if dir_path.path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path.path, target_name=None, target_id=None)\n\n    if dir_path.path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path.path, target_name=None, target_id=None)\n\n    if dir_path.path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path.path, target_name=None, target_id=None)\n\n    if dir_path.path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path.path, target_name=None, target_id=None)\n\n    if dir_path.path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path.path, target_name=None, target_id=None)\n\n    if dir_path.path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path.path, target_name=None, target_id=None)\n\n    if dir_path.path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path.path, target_name=None, target_id=None)\n\n    if dir_path.path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path.path, target_name=None,"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path.startswith(\"s3://\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio("}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, str):\n        dir_path = Dir(path=dir_path)\n\n    if dir_path.path is None:\n        return dir_path\n\n    if dir_path.path.startswith(\"s3://\"):\n        return _resolve_s3(dir_path.path)\n\n    if dir_path.path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path.path, target_name=None, target_id=None)\n\n    if dir_path.path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path.path, target_name=None, target_id=None)\n\n    if dir_path.path.startswith(\"datasets://\"):\n        return _resolve_datasets(dir_path.path)\n\n    if dir_path.path.startswith(\"s3-connections://\"):\n        return _resolve_s3_connections(dir_path.path)\n\n    if dir_path.path.startswith(\"~\"):\n        return Dir(path=os.path.expanduser(dir_path.path))\n\n    if dir_path.path.startswith(\"/\"):\n        return Dir(path=dir_path.path)\n\n    return Dir(path=os.path.join(os.getcwd(), dir_path.path))\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, target_name=None, target_id=None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, target_name=None, target_id=None)\n\n    if dir_path.startswith(\"datasets://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"s3-connections://\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n\n    return Dir(path=os.path.join(os.getcwd(), dir_path))"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if os.path.isabs(dir_path):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, target_name=None, target_id=None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, target_name=None, target_id=None)\n\n    if dir_path.startswith(\"s3-connection://\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"datasets://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"~\"):\n        return Dir(path=os.path.expanduser(dir_path))\n\n    if dir_path.startswith(\".\"):\n        return Dir(path=os.path.abspath(dir_path))\n\n    return Dir(path=dir_path)"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if os.path.isabs(dir_path):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"projects/\"):\n        return _resolve_project_dir(dir_path)\n\n    if dir_path.startswith(\"studio/\"):\n        return _resolve_studio(dir_path, target_name=None, target_id=None)\n\n    if dir_path.startswith(\"s3-connections/\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"datasets/\"):\n        return _resolve_datasets(dir_path)\n\n    return Dir(path=dir_path)\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if os.path.isabs(dir_path):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"~/\"):\n        return Dir(path=os.path.expanduser(dir_path))\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.join(os.getcwd(), dir_path))\n\n    if dir_path.startswith(\"../\"):\n        return Dir(path=os.path.join(os.getcwd(), dir_path))\n\n    if dir_path.startswith(\"~/\"):\n        return Dir(path=os.path.expanduser(dir_path))\n\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"s3-connections://\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"datasets://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, str):\n        dir_path = Dir(path=dir_path)\n\n    if dir_path.url is not None:\n        return dir_path\n\n    if dir_path.path is None:\n        return dir_path\n\n    if dir_path.path.startswith(\"s3://\"):\n        return Dir(url=dir_path.path)\n\n    if dir_path.path.startswith(\"./\"):\n        return Dir(path=os.path.abspath(dir_path.path))\n\n    if dir_path.path.startswith(\"../\"):\n        return Dir(path=os.path.abspath(os.path.join(os.getcwd(), dir_path.path)))\n\n    if dir_path.path.startswith(\"./\"):\n        return Dir(path=os.path.abspath(os.path.join(os.getcwd(), dir_path.path)))\n\n    if dir_path.path.startswith(\"~/\"):\n        return Dir(path=os.path.expanduser(dir_path.path))\n\n    if dir_path.path.startswith(\"/\"):\n        return Dir(path=dir_path.path)\n\n    if dir_path.path.startswith(\"./\"):\n        return Dir(path=os.path.abspath(os.path.join(os.getcwd(), dir_path.path)))\n\n    if dir_path.path.startswith(\"./\"):\n        return Dir(path=os.path.abspath(os.path.join(os.getcwd(), dir_path.path)))\n\n    if dir_path.path.startswith(\"./\"):\n        return Dir(path=os.path.abspath(os.path.join(os.getcwd(), dir_path.path)))\n\n    if dir_path.path.startswith(\"./\"):\n        return Dir(path=os.path.abspath(os.path"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir(path=None, url=None)\n\n    if isinstance(dir_path, str):\n        dir_path = Dir(path=dir_path)\n\n    if dir_path.path is None:\n        return dir_path\n\n    if dir_path.path.startswith(\"s3://\"):\n        return _resolve_s3(dir_path)\n\n    if dir_path.path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path.path, None, None)\n\n    if dir_path.path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path.path, None, None)\n\n    if dir_path.path.startswith(\"s3-connections://\"):\n        return _resolve_s3_connections(dir_path.path)\n\n    if dir_path.path.startswith(\"datasets://\"):\n        return _resolve_datasets(dir_path.path)\n\n    return dir_path\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if not isinstance(dir_path, str):\n        raise ValueError(f\"The provided dir_path isn't a str. Found {type(dir_path)}.\")\n\n    if os.path.exists(dir_path):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"datasets/\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"data_connections/\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"studios/\"):\n        target_name = None\n        target_id = None\n\n        if \"/\" in dir_path:\n            target_name = dir_path.split(\"/\")[1]\n            target_id = dir_path.split(\"/\")[2]\n\n        return _resolve_studio(dir_path, target_name, target_id)\n\n    return Dir(path=dir_path)"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    dir_path = str(dir_path)\n\n    if dir_path.startswith(\"s3://\"):\n        return _resolve_s3_url(dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return _resolve_lightning_url(dir_path)\n\n    if dir_path.startswith(\"datasets://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"s3-connections://\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path)\n\n    return Dir(path=dir_path)\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    dir_path = dir_path.rstrip(\"/\")\n\n    if dir_path.startswith(\"s3://\"):\n        return _resolve_s3_url(dir_path)\n\n    if dir_path.startswith(\"datasets/\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"data_connections/\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"studio/\"):\n        target_name = dir_path.split(\"/\")[1]\n        target_id = dir_path.split(\"/\")[2] if len(dir_path.split(\"/\")) > 2 else None\n        return _resolve_studio(dir_path, target_name, target_id)\n\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n\n    if Path(dir_path).exists():\n        return Dir(path=dir_path)\n\n    raise ValueError(f\"The provided dir_path `{dir_path}` is not a valid path.\")\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir(path=None, url=None)\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path.startswith(\"s3://\"):\n        return _resolve_s3(dir_path)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path)\n\n    if dir_path.startswith(\"s3-connections://\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"datasets://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"projects://\"):\n        return _resolve_projects(dir_path)\n\n    if os.path.exists(dir_path):\n        return Dir(path=dir_path)\n\n    return Dir(path=dir_path)\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, str):\n        dir_path = Dir(path=dir_path)\n\n    if dir_path.path is None:\n        return dir_path\n\n    # If it is a local path\n    if os.path.exists(dir_path.path):\n        return dir_path\n\n    # If it is a s3 url\n    obj = parse.urlparse(dir_path.path)\n\n    if obj.scheme == \"s3\":\n        return Dir(path=dir_path.path, url=dir_path.path)\n\n    # If it is a project path\n    if obj.scheme == \"project\":\n        return _resolve_project(dir_path.path)\n\n    # If it is a studio path\n    if obj.scheme == \"studio\":\n        return _resolve_studio(dir_path.path, target_name=obj.netloc, target_id=obj.path.lstrip(\"/\"))\n\n    # If it is a s3 connection path\n    if obj.scheme == \"s3-connection\":\n        return _resolve_s3_connections(dir_path.path)\n\n    # If it is a datasets path\n    if obj.scheme == \"datasets\":\n        return _resolve_datasets(dir_path.path)\n\n    raise ValueError(f\"The provided path `{dir_path.path}` isn't supported.\")\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if not isinstance(dir_path, str):\n        raise ValueError(\"The provided dir_path isn't a string or a Dir Object.\")\n\n    if dir_path.startswith(\"s3://\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"datasets://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"studio://\"):\n        target_name = dir_path.split(\"studio://\")[1].split(\"/\")[0]\n        target_id = dir_path.split(\"studio://\")[1].split(\"/\")[1] if len(dir_path.split(\"studio://\")[1].split(\"/\")) > 1 else None\n        return _resolve_studio(dir_path, target_name, target_id)\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"~\"):\n        return Dir(path=os.path.expanduser(dir_path))\n\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n\n    return Dir(path=os.path.join(os.getcwd(), dir_path))"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir()\n\n    if not isinstance(dir_path, str):\n        raise ValueError(f\"The provided dir_path should be a string, found {dir_path}.\")\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"s3-connections://\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"datasets://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"project://\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"~\"):\n        return Dir(path=os.path.expanduser(dir_path))\n\n    if os.path.exists(dir_path):\n        return Dir(path=dir_path)\n\n    raise ValueError(f\"The provided dir_path `{dir_path}` is not a valid path.\")"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if os.path.isabs(dir_path):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        if dir_path.startswith(\"lightning://studio\"):\n            return _resolve_studio(dir_path, target_name=None, target_id=None)\n\n        if dir_path.startswith(\"lightning://datasets\"):\n            return _resolve_datasets(dir_path)\n\n        if dir_path.startswith(\"lightning://s3_connections\"):\n            return _resolve_s3_connections(dir_path)\n\n        raise ValueError(f\"We don't support the provided lightning path {dir_path}.\")\n\n    if dir_path.startswith(\"~/\"):\n        dir_path = os.path.expanduser(dir_path)\n\n    if dir_path.startswith(\"./\"):\n        dir_path = os.path.abspath(dir_path)\n\n    if dir_path.startswith(\"../\"):\n        dir_path = os.path.abspath(os.path.join(os.getcwd(), dir_path))\n\n    return Dir(path=dir_path)"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if os.path.isabs(dir_path):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        if not _LIGHTNING_SDK_AVAILABLE:\n            raise ModuleNotFoundError(\"The `lightning_sdk` is required.\")\n\n        if dir_path.startswith(\"lightning://datasets/\"):\n            return _resolve_datasets(dir_path)\n\n        if dir_path.startswith(\"lightning://studio/\"):\n            target_name = dir_path.split(\"/\")[3]\n            target_id = None\n            if len(dir_path.split(\"/\")) > 4:\n                target_id = dir_path.split(\"/\")[4]\n\n            return _resolve_studio(dir_path, target_name, target_id)\n\n        if dir_path.startswith(\"lightning://s3/\"):\n            return _resolve_s3_connections(dir_path)\n\n        raise ValueError(f\"The provided path `{dir_path}` is not supported.\")\n\n    # TODO: Add support for local paths\n    raise ValueError(f\"The provided path `{dir_path}` is not supported.\")\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir()\n\n    if not isinstance(dir_path, str):\n        raise ValueError(f\"The provided dir_path `{dir_path}` isn't a string.\")\n\n    if os.path.exists(dir_path):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return _resolve_lightning_datasets(dir_path)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"s3-connection://\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"datasets://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"~\"):\n        return Dir(path=os.path.expanduser(dir_path))\n\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.join(os.getcwd(), dir_path))\n\n    if dir_path.startswith(\"../\"):\n        return Dir(path=os.path.join(os.getcwd(), dir_path))\n\n    if dir_path.startswith(\"data/\"):\n        return Dir(path=os.path.join(os.getcwd(), dir_path))\n\n    if dir_path.startswith(\"data/\"):\n        return Dir(path=os.path.join(os.getc"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, str):\n        dir_path = Dir(path=dir_path)\n\n    if dir_path.url is not None:\n        return dir_path\n\n    if dir_path.path is None:\n        raise ValueError(\"The dir_path should either be a string or a Dir object.\")\n\n    # If the path starts with s3://, we assume it's a S3 URL\n    if dir_path.path.startswith(\"s3://\"):\n        return Dir(url=dir_path.path)\n\n    # If the path is a relative path, we assume it's a local path\n    if not Path(dir_path.path).is_absolute():\n        return Dir(path=dir_path.path)\n\n    # If the path starts with lightning://, we assume it's a Lightning project path\n    if dir_path.path.startswith(\"lightning://\"):\n        return _resolve_project_path(dir_path.path)\n\n    # If the path starts with studio://, we assume it's a Lightning Studio path\n    if dir_path.path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path.path, None, None)\n\n    # If the path starts with s3_connections://, we assume it's a Lightning Data Connection path\n    if dir_path.path.startswith(\"s3_connections://\"):\n        return _resolve_s3_connections(dir_path.path)\n\n    # If the path starts with datasets://, we assume it's a Lightning Datasets path\n    if dir_path.path.startswith(\"datasets://\"):\n        return _resolve_datasets(dir_path.path)\n\n    # If the path starts with lightning://, we assume it's a Lightning project path\n    if dir_path.path.startswith(\"lightning://\"):\n        return _resolve_project_path(dir_path.path)\n\n    # If the path starts"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    # No files are found in this folder\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    # Check the index file exists\n    try:\n        s3.head_object(Bucket=obj.netloc, Key=os.path.join(prefix, \"index.json\"))\n        has_index_file = True\n    except botocore.exceptions.ClientError:\n        has_index_file = False\n\n    if has_index_file:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an optimized immutable datasets.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    bucket_name = obj.netloc\n    s3 = boto3.resource(\"s3\")\n    for obj in s3.Bucket(bucket_name).objects.filter(Prefix=prefix):\n        s3.Object(bucket_name, obj.key).delete()"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    # No files are found in this folder\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    # Check the index file exists\n    try:\n        s3.head_object(Bucket=obj.netloc, Key=os.path.join(prefix, \"index.json\"))\n        has_index_file = True\n    except botocore.exceptions.ClientError:\n        has_index_file = False\n\n    if has_index_file:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an optimized immutable datasets.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    bucket_name = obj.netloc\n    s3 = boto3.resource(\"s3\")\n    for obj in s3.Bucket(bucket_name).objects.filter(Prefix=prefix):\n        s3.Object(bucket_name, obj.key).delete()"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    # No files are found in this folder\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    # Check the index file exists\n    try:\n        s3.head_object(Bucket=obj.netloc, Key=os.path.join(prefix, \"index.json\"))\n        has_index_file = True\n    except botocore.exceptions.ClientError:\n        has_index_file = False\n\n    if has_index_file:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an optimized immutable datasets.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    if append:\n        raise NotImplementedError(\"Append is not implemented yet.\")\n\n    if overwrite:\n        raise NotImplementedError(\"Overwrite is not implemented yet.\")\n\n    bucket_name = obj.netloc\n    s3 = boto3.resource(\"s3\")\n    for obj in s3.Bucket(bucket_name).objects.filter(Prefix=prefix):\n        s3.Object(bucket_name, obj.key).delete()"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data. \"\n            \"HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data. HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    # No files are found in this folder\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    # Check the index file exists\n    try:\n        s3.head_object(Bucket=obj.netloc, Key=os.path.join(prefix, \"index.json\"))\n        has_index_file = True\n    except botocore.exceptions.ClientError:\n        has_index_file = False\n\n    if has_index_file:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an optimized immutable datasets.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    if append:\n        raise NotImplementedError(\"Append is not implemented yet.\")\n\n    if overwrite:\n        raise NotImplementedError(\"Overwrite is not implemented yet.\")\n\n    bucket_name = obj.netloc\n    s3 = boto3.resource(\"s3\")\n    for obj in s3.Bucket(bucket_name).objects.filter(Prefix=prefix):\n        s3.Object(bucket_name, obj.key).delete()"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(f\"The provided output_dir `{output_dir.path}` already contains data.\")"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    # No files are found in this folder\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    if not append and not overwrite:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data. HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    bucket_name = obj.netloc\n    s3 = boto3.resource(\"s3\")\n    for obj in s3.Bucket(bucket_name).objects.filter(Prefix=prefix):\n        s3.Object(bucket_name, obj.key).delete()"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(f\"The provided output_dir `{output_dir.path}` already contains data.\")"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    if objects[\"KeyCount\"] != 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data. HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    # No files are found in this folder\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    # Check if the folder is empty\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data. HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    bucket_name = obj.netloc\n    s3 = boto3.resource(\"s3\")\n    for obj in s3.Bucket(bucket_name).objects.filter(Prefix=prefix):\n        s3.Object(bucket_name, obj.key).delete()"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    # No files are found in this folder\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    # Check the index file exists\n    try:\n        s3.head_object(Bucket=obj.netloc, Key=os.path.join(prefix, \"index.json\"))\n        has_index_file = True\n    except botocore.exceptions.ClientError:\n        has_index_file = False\n\n    if has_index_file:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an optimized immutable datasets.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    if append or overwrite:\n        raise NotImplementedError(\"Append and overwrite are not yet implemented.\")\n\n    bucket_name = obj.netloc\n    s3 = boto3.resource(\"s3\")\n    for obj in s3.Bucket(bucket_name).objects.filter(Prefix=prefix):\n        s3.Object(bucket_name, obj.key).delete()"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    # No files are found in this folder\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    # Check the index file exists\n    try:\n        s3.head_object(Bucket=obj.netloc, Key=os.path.join(prefix, \"index.json\"))\n        has_index_file = True\n    except botocore.exceptions.ClientError:\n        has_index_file = False\n\n    if has_index_file:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an optimized immutable datasets.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    if append or overwrite:\n        raise NotImplementedError(\"Append and Overwrite are not implemented yet.\")\n\n    bucket_name = obj.netloc\n    s3 = boto3.resource(\"s3\")\n    for obj in s3.Bucket(bucket_name).objects.filter(Prefix=prefix):\n        s3.Object(bucket_name, obj.key).delete()"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    if append or overwrite:\n        raise NotImplementedError(\"Append or overwrite is not implemented yet.\")"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data. HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    if append:\n        raise RuntimeError(\"Append is not yet supported.\")\n\n    if overwrite:\n        raise RuntimeError(\"Overwrite is not yet supported.\")"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    # No files are found in this folder\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    # Check the index file exists\n    try:\n        s3.head_object(Bucket=obj.netloc, Key=os.path.join(prefix, \"index.json\"))\n        has_index_file = True\n    except botocore.exceptions.ClientError:\n        has_index_file = False\n\n    if has_index_file:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an optimized immutable datasets.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    if append:\n        raise NotImplementedError(\"Appending data is not supported in this version.\")\n\n    if overwrite:\n        raise NotImplementedError(\"Overwriting data is not supported in this version.\")\n\n    bucket_name = obj.netloc\n    s3 = boto3.resource(\"s3\")\n    for obj in s3.Bucket(bucket_name).objects.filter(Prefix=prefix):\n        s3.Object(bucket_name, obj.key).delete()"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    # No files are found in this folder\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    if append or overwrite:\n        raise RuntimeError(\n            \"The provided output_dir is not empty. Appending or overwriting is not supported yet.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    bucket_name = obj.netloc\n    s3 = boto3.resource(\"s3\")\n    for obj in s3.Bucket(bucket_name).objects.filter(Prefix=prefix):\n        s3.Object(bucket_name, obj.key).delete()"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    # No files are found in this folder\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    # Check the index file exists\n    try:\n        s3.head_object(Bucket=obj.netloc, Key=os.path.join(prefix, \"index.json\"))\n        has_index_file = True\n    except botocore.exceptions.ClientError:\n        has_index_file = False\n\n    if has_index_file:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an optimized immutable datasets.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    if append or overwrite:\n        raise ValueError(\n            \"The `append` and `overwrite` arguments are not supported for the `output_dir` argument. \"\n            \"Please remove the `output_dir` argument or set the `append` and `overwrite` arguments to False.\"\n        )\n\n    bucket_name = obj.netloc\n    s3 = boto3.resource(\"s3\")\n    for obj in s3.Bucket(bucket_name).objects.filter(Prefix=prefix):\n        s3.Object(bucket_name, obj.key).delete()"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # We aren't alloweing to add more data\n    # TODO: Add support for `append` and `overwrite`.\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    # We aren't alloweing to add more data\n    # TODO: Add support for `append` and `overwrite`.\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    # We aren't alloweing to add more data\n    # TODO: Add support for `append` and `overwrite`.\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    # We"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if \"Contents\" in objects and any(obj[\"Key\"].endswith(\"index.json\") for obj in objects[\"Contents\"]):\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an index file named 'index.json'.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # We aren't alloweing to add more data\n    # TODO: Add support for `append` and `overwrite`.\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if \"Contents\" not in objects:\n        return\n\n    for object in objects[\"Contents\"]:\n        if object[\"Key\"].endswith(\"index.json\"):\n            raise RuntimeError(\n                f\"The provided output_dir `{output_dir.path}` already contains an index file named 'index.json'. \"\n                \"HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n            )\n\n    # Delete all objects in the bucket\n    for object in objects[\"Contents\"]:\n        s3.delete_object(Bucket=obj.netloc, Key=object[\"Key\"])"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    for object in objects[\"Contents\"]:\n        if object[\"Key\"].endswith(\"index.json\"):\n            raise RuntimeError(\n                f\"The provided output_dir `{output_dir.path}` already contains an index file named 'index.json'.\"\n                \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n            )\n\n    if objects[\"KeyCount\"] > 0:\n        s3.delete_objects(\n            Bucket=obj.netloc,\n            Delete={\"Objects\": [{\"Key\": obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\" + object[\"Key\"]} for object in objects[\"Contents\"]]},\n        )"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    for obj in objects[\"Contents\"]:\n        if obj[\"Key\"].endswith(\"index.json\"):\n            raise RuntimeError(\n                f\"The provided output_dir `{output_dir.path}` already contains an index file named 'index.json'.\"\n                \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n            )\n\n    # Delete all objects within the specified prefix in the bucket.\n    for obj in objects[\"Contents\"]:\n        s3.delete_object(Bucket=obj[\"Bucket\"], Key=obj[\"Key\"])"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if \"Contents\" not in objects:\n        return\n\n    for obj in objects[\"Contents\"]:\n        if obj[\"Key\"].endswith(\"index.json\"):\n            raise ValueError(\n                f\"The provided output_dir `{output_dir.path}` already contains an index file named 'index.json'.\"\n                \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n            )\n\n    # TODO: Add support for `append` and `overwrite`.\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if \"Contents\" not in objects:\n        return\n\n    for content in objects[\"Contents\"]:\n        if content[\"Key\"].endswith(\"index.json\"):\n            raise RuntimeError(\n                f\"The provided output_dir `{output_dir.path}` already contains an index file named 'index.json'.\"\n                \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n            )\n\n    if \"Contents\" not in objects:\n        return\n\n    for content in objects[\"Contents\"]:\n        if content[\"Key\"].endswith(\"index.json\"):\n            s3.delete_objects(\n                Bucket=obj.netloc,\n                Delete={\"Objects\": [{\"Key\": content[\"Key\"]}]},\n            )"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if \"Contents\" not in objects:\n        return\n\n    for content in objects[\"Contents\"]:\n        if content[\"Key\"] == \"index.json\":\n            raise ValueError(\n                f\"The provided output_dir `{output_dir.path}` already contains an index file named 'index.json'.\"\n            )\n\n    if \"Contents\" in objects:\n        for content in objects[\"Contents\"]:\n            s3.delete_object(Bucket=obj.netloc, Key=content[\"Key\"])"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if \"Contents\" not in objects:\n        return\n\n    for object in objects[\"Contents\"]:\n        if object[\"Key\"].endswith(\"index.json\"):\n            raise RuntimeError(\n                f\"The provided output_dir `{output_dir.path}` already contains an index file named `index.json`.\"\n                \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n            )\n\n    if \"Contents\" not in objects:\n        return\n\n    for object in objects[\"Contents\"]:\n        s3.delete_object(Bucket=obj.netloc, Key=object[\"Key\"])\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if \"Contents\" not in objects:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` doesn't contain any data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    for obj in objects[\"Contents\"]:\n        if obj[\"Key\"].endswith(\"index.json\"):\n            raise RuntimeError(\n                f\"The provided output_dir `{output_dir.path}` already contains an index file named 'index.json'.\"\n                \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n            )\n\n    if objects[\"KeyCount\"] > 0:\n        s3.delete_objects(\n            Bucket=obj.netloc,\n            Delete={\n                \"Objects\": [{\"Key\": obj[\"Key\"]} for obj in objects[\"Contents\"]],\n            },\n        )"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if \"Contents\" not in objects:\n        return\n\n    index_file_found = False\n    for file in objects[\"Contents\"]:\n        if file[\"Key\"].endswith(\"index.json\"):\n            index_file_found = True\n            break\n\n    if not index_file_found:\n        raise ValueError(\n            f\"The provided output_dir `{output_dir.path}` does not contain an index file named 'index.json'.\"\n        )\n\n    if \"Contents\" not in objects:\n        return\n\n    for file in objects[\"Contents\"]:\n        if file[\"Key\"] != \"index.json\":\n            s3.delete_object(Bucket=obj.netloc, Key=file[\"Key\"])"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if \"Contents\" not in objects:\n        return\n\n    if any(obj[\"Key\"].endswith(\"index.json\") for obj in objects[\"Contents\"]):\n        raise ValueError(\n            f\"The provided output_dir `{output_dir.path}` already contains an index.json file. \"\n            \"HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    # delete all objects within the specified prefix in the bucket\n    for obj in objects[\"Contents\"]:\n        s3.delete_object(Bucket=obj[\"Key\"].split(\"/\")[0], Key=obj[\"Key\"])"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # If there is already an index file, delete all objects within the specified prefix in the bucket.\n    if \"CommonPrefixes\" in objects and \"index.json\" in [\n        prefix[\"Prefix\"].split(\"/\")[-2] for prefix in objects[\"CommonPrefixes\"]\n    ]:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if \"Contents\" in objects:\n        for content in objects[\"Contents\"]:\n            if content[\"Key\"].endswith(\"index.json\"):\n                raise RuntimeError(\n                    f\"The provided output_dir `{output_dir.path}` already contains an index file named \"\n                    \"index.json. Please remove the file before running the operator.\"\n                )\n\n    if \"CommonPrefixes\" in objects:\n        for prefix in objects[\"CommonPrefixes\"]:\n            if prefix[\"Prefix\"].endswith(\"index.json\"):\n                raise RuntimeError(\n                    f\"The provided output_dir `{output_dir.path}` already contains an index file named \"\n                    \"index.json. Please remove the file before running the operator.\"\n                )"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if \"Contents\" in objects:\n        for obj in objects[\"Contents\"]:\n            if obj[\"Key\"].endswith(\"index.json\"):\n                raise RuntimeError(\n                    \"The provided output_dir already contains an index file named 'index.json'. \"\n                    \"Please remove it or use a different output_dir.\"\n                )\n\n    # Delete all objects within the specified prefix in the bucket\n    if \"Contents\" in objects:\n        for obj in objects[\"Contents\"]:\n            s3.delete_object(Bucket=obj[\"Key\"].split(\"/\")[0], Key=obj[\"Key\"])\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # If the directory is not an S3 bucket, we can't check for the presence of an index file.\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    # If the directory already contains an index file, we raise an error.\n    if \"index.json\" in [obj[\"Key\"] for obj in objects.get(\"Contents\", [])]:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an index file named 'index.json'. \"\n            \"HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    # If the directory does not contain an index file, we delete all objects within the specified prefix.\n    if objects[\"KeyCount\"] > 0:\n        s3.delete_objects(\n            Bucket=obj.netloc,\n            Delete={\"Objects\": [{\"Key\": obj[\"Key\"]} for obj in objects.get(\"Contents\", [])]},\n        )"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # We aren't alloweing to add more data\n    # TODO: Add support for `append` and `overwrite`.\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    index_files = [obj for obj in objects[\"Contents\"] if obj[\"Key\"].endswith(\"/index.json\")]\n\n    if index_files:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an index file named 'index.json'. \"\n            \"HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    # Delete all objects within the specified prefix\n    for obj in objects.get(\"Contents\", []):\n        s3.delete_object(Bucket=obj[\"Bucket\"], Key=obj[\"Key\"])"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if objects[\"KeyCount\"] > 0:\n        # Delete all objects within the specified prefix\n        for obj in objects[\"Contents\"]:\n            s3.delete_object(Bucket=obj.get(\"Bucket\"), Key=obj.get(\"Key\"))\n\n    # Check if an index file named \"index.json\" exists within the specified prefix\n    index_file = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/index.json\",\n    )\n\n    if \"Contents\" in index_file and index_file[\"Contents\"]:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an index file named 'index.json'.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if \"CommonPrefixes\" in objects:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if \"Contents\" in objects:\n        index_file_exists = False\n        for content in objects[\"Contents\"]:\n            if content[\"Key\"].endswith(\"index.json\"):\n                index_file_exists = True\n                break\n\n        if not index_file_exists:\n            raise RuntimeError(\n                f\"The provided output_dir `{output_dir.path}` does not contain an index.json file and datasets are meant to be immutable.\"\n                \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n            )\n\n    return "}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.rank\n\n        if node_rank != 0:\n            while True:\n                files = os.listdir(self._cache_dir)\n                index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n                if len(index_files) == num_workers:\n                    break\n                sleep(0.1)\n\n            self._merge_no_wait(node_rank)\n\n        else:\n            while True:\n                files = os.listdir(self._cache_dir)\n                index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n                if len(index_files) == num_workers:\n                    break\n                sleep(0.1)\n\n            self._merge_no_wait()"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait until all the workers have written their own index\n        while len(os.listdir(self._cache_dir)) < num_workers:\n            sleep(0.1)\n\n        if node_rank is None:\n            self._merge_no_wait()\n        else:\n            self._merge_no_wait(node_rank)"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.rank\n\n        if node_rank != 0:\n            while True:\n                files = os.listdir(self._cache_dir)\n                index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n                if len(index_files) == num_workers:\n                    break\n                sleep(1)\n\n            self._merge_no_wait(node_rank)\n        else:\n            self._merge_no_wait()"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = 0\n\n        if node_rank != 0:\n            while not self.filled:\n                sleep(0.1)\n            return\n\n        self._merge_no_wait(node_rank)"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.rank\n\n        if node_rank == 0:\n            self._merge()\n        else:\n            self._merge_no_wait(node_rank)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.rank\n\n        if node_rank == 0:\n            self._merge(num_workers)\n        else:\n            self._merge_no_wait(node_rank)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.rank\n\n        if node_rank != 0:\n            # Wait until the master node (rank 0) has written down the index\n            while True:\n                files = os.listdir(self._cache_dir)\n                index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n                if len(index_files) == num_workers:\n                    break\n                sleep(1)\n            return\n\n        # Merge the index files\n        files = os.listdir(self._cache_dir)\n        index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n        chunks_info = []\n        config = None\n        for index_filename in sorted(index_files):\n            chunk_path = os.path.join(self._cache_dir, index_filename)\n            with open(chunk_path) as f:\n                data = json.load(f)\n\n                if config is None:\n                    config = data[\"config\"]\n\n                elif config != data[\"config\"]:\n                    raise Exception(\n                        \"The config isn't consistent between chunks. This shouldn't have happened.\"\n                        f\"Found {config} {data['config']}.\"\n                    )\n\n                chunks_info.extend(data[\"chunks\"])\n\n            os.remove(chunk_path)\n\n        with open(os.path.join(self._cache_dir, _INDEX_FILENAME), \"w\") as f:\n            json.dump({\"chunks\": chunks_info, \"config\": config}, f, sort_keys=True)"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = get_worker_rank()\n\n        # Wait until all the workers have written their own index\n        while len(os.listdir(self._cache_dir)) < num_workers:\n            sleep(1)\n\n        # If the node is not the master node, wait until the merged index file is available\n        if node_rank != 0:\n            while not os.path.exists(os.path.join(self._cache_dir, _INDEX_FILENAME)):\n                sleep(1)\n\n        # If the node is the master node, perform the actual merging\n        if node_rank == 0:\n            self._merge_no_wait(node_rank)"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = get_worker_rank()\n\n        if node_rank == 0:\n            self._merge_no_wait(node_rank)\n        else:\n            self._merge_wait(num_workers, node_rank)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait for all parts to be available\n        while True:\n            files = os.listdir(self._cache_dir)\n            index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n            if len(index_files) == num_workers:\n                break\n            sleep(0.5)\n\n        # If we are not the master node, wait until the merged index file is available\n        if node_rank is not None and node_rank != 0:\n            while True:\n                files = os.listdir(self._cache_dir)\n                index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n                if len(index_files) == 1:\n                    break\n                sleep(0.5)\n\n        # Merge the index files into a single index file\n        self._merge_no_wait(node_rank)"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = 0\n\n        if node_rank != 0:\n            # Wait until the master node (rank 0) writes down the index.\n            while True:\n                files = os.listdir(self._cache_dir)\n                index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n                if len(index_files) == num_workers:\n                    break\n                sleep(0.1)\n\n            # Wait until the master node (rank 0) writes down the index.\n            while True:\n                files = os.listdir(self._cache_dir)\n                index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n                if len(index_files) == num_workers:\n                    break\n                sleep(0.1)\n\n            self._merge_no_wait(node_rank)\n\n        else:\n            # Wait until all the workers have written their own index.\n            while True:\n                files = os.listdir(self._cache_dir)\n                index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n                if len(index_files) == num_workers:\n                    break\n                sleep(0.1)\n\n            self._merge_no_wait()\n\n    "}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.rank\n\n        if node_rank != 0:\n            # Wait for the master node to write the merged index file\n            while not os.path.exists(os.path.join(self._cache_dir, _INDEX_FILENAME)):\n                sleep(1)\n            return\n\n        # Wait for all parts to be available\n        while len(os.listdir(self._cache_dir)) < num_workers:\n            sleep(1)\n\n        # Merge the index files\n        self._merge_no_wait(node_rank)\n\n        # Wait for the merged index file to be available\n        while not os.path.exists(os.path.join(self._cache_dir, _INDEX_FILENAME)):\n            sleep(1)\n\n    "}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait for all workers to write their own index\n        while True:\n            files = os.listdir(self._cache_dir)\n            index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n\n            if len(index_files) == num_workers:\n                if node_rank is None:\n                    self._merge_no_wait()\n                else:\n                    self._merge_no_wait(node_rank)\n                break\n\n            sleep(0.1)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.rank\n\n        # Wait until all parts are available\n        while True:\n            files = os.listdir(self._cache_dir)\n            index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n\n            if len(index_files) == num_workers:\n                break\n            sleep(0.1)\n\n        # Only the master node (rank 0) should merge the index\n        if node_rank == 0:\n            self._merge_no_wait(node_rank=node_rank)\n\n    "}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # If we are not the master node, we wait for the merged index file to be available.\n        if node_rank is not None and node_rank != 0:\n            while not os.path.exists(os.path.join(self._cache_dir, f\"{node_rank}-{_INDEX_FILENAME}\")):\n                sleep(0.1)\n\n        # If we are the master node, we wait for all index files to be available.\n        while len(os.listdir(self._cache_dir)) < num_workers:\n            sleep(0.1)\n\n        self._merge_no_wait(node_rank)"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            worker_env = _WorkerEnv.detect()\n            node_rank = worker_env.rank\n\n        if node_rank != 0:\n            # Wait for the master node to write down the merged index file\n            while True:\n                files = os.listdir(self._cache_dir)\n                index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n                if len(index_files) == num_workers:\n                    break\n                sleep(1)\n        else:\n            # Wait for all index parts to be available\n            while True:\n                files = os.listdir(self._cache_dir)\n                index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n                if len(index_files) == num_workers:\n                    break\n                sleep(1)\n\n        self._merge_no_wait(node_rank=node_rank)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.rank\n\n        if node_rank != 0:\n            self._merge_no_wait(node_rank)\n            return\n\n        # Wait until all parts of the index are available\n        while not self.filled:\n            sleep(0.1)\n\n        # Only proceed if this is the master node (rank 0)\n        if node_rank != 0:\n            return\n\n        files = os.listdir(self._cache_dir)\n        index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n        chunks_info = []\n        config = None\n        for index_filename in sorted(index_files):\n            chunk_path = os.path.join(self._cache_dir, index_filename)\n            with open(chunk_path) as f:\n                data = json.load(f)\n\n                if config is None:\n                    config = data[\"config\"]\n\n                elif config != data[\"config\"]:\n                    raise Exception(\n                        \"The config isn't consistent between chunks. This shouldn't have happened.\"\n                        f\"Found {config} {data['config']}.\"\n                    )\n\n                chunks_info.extend(data[\"chunks\"])\n\n            os.remove(chunk_path)\n\n        with open(os.path.join(self._cache_dir, _INDEX_FILENAME), \"w\") as f:\n            json.dump({\"chunks\": chunks_info, \"config\": config}, f, sort_keys=True)\n\n    "}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = 0\n\n        # If this is the master node, wait for all the workers to write their own index files.\n        if node_rank == 0:\n            while len(os.listdir(self._cache_dir)) < num_workers:\n                sleep(0.5)\n\n            self._merge_no_wait(node_rank)\n\n        # If this is not the master node, wait for the merged index file to be available.\n        else:\n            while not os.path.exists(os.path.join(self._cache_dir, f\"{node_rank}-{_INDEX_FILENAME}\")):\n                sleep(0.5)\n\n        # If this is the master node, merge all the index files into a single index.\n        if node_rank == 0:\n            self._merge_no_wait()"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # If the user provides the rank, we assume the user is handling the distributed environment\n        if node_rank is not None:\n            self._distributed_env = _DistributedEnv(node_rank=node_rank, world_size=num_workers)\n\n        # Wait for all the workers to write their own index\n        while not self.filled:\n            sleep(0.1)\n\n        # Once all the workers have written their own index, the merge function is responsible to read and merge them\n        # into a single index.\n        self._merge_no_wait(node_rank=node_rank)\n\n    "}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.rank\n\n        if node_rank != 0:\n            # Wait until the master node is done writing down the index.\n            # This is a special case for non-master nodes.\n            # In this case, we don't wait for all the workers to finish,\n            # but only for the master node to finish.\n            while not os.path.exists(os.path.join(self._cache_dir, _INDEX_FILENAME)):\n                sleep(0.01)\n            return\n\n        # Wait until all the workers have written down their index\n        while len(os.listdir(self._cache_dir)) < num_workers:\n            sleep(0.01)\n\n        # Once all the workers have written their own index, the merge function is responsible to read and merge them\n        # into a single index.\n        self._merge_no_wait(node_rank)\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ModuleNotFoundError(\"The `lightning_sdk` package is required to execute this function.\")\n\n    if machine is None:\n        machine = Machine(cpu=1, gpu=0, mem=\"2Gi\")\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run app {os.getcwd()} --env {os.environ}\"\n\n    job = Studio.execute(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n    )\n\n    print(f\"Job URL: {_get_lightning_cloud_url()}/studio/jobs/{job.id}\")\n\n    while job.status != \"succeeded\":\n        sleep(1)\n        job.refresh()\n\n    if job.status == \"failed\":\n        raise RuntimeError(f\"Job {job.id} failed with error: {job.error}\")\n\n    print(f\"Job {job.id} succeeded\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ImportError(\"The Lightning SDK is not installed.\")\n\n    if machine is None:\n        machine = Machine(cpu=1, gpu=0, gpu_type=\"any\")\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run --app {os.getcwd()} --env-vars {os.environ}\"\n\n    job = Studio.create_data_job(name, machine=machine, command=command)\n\n    print(f\"Job URL: {job.url}\")\n\n    while job.status not in [\"failed\", \"succeeded\"]:\n        sleep(5)\n        job = Studio.get_data_job(job.job_id)\n\n    if job.status == \"failed\":\n        raise RuntimeError(\"The job failed.\")\n\n    print(f\"Job URL: {job.url}\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ImportError(\n            \"The Lightning SDK is not installed. Please install it with `pip install lightning-sdk`.\"\n        )\n\n    if machine is None:\n        machine = Machine(cpu=num_nodes, gpu=0, machine_type=\"cpu\")\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run {os.getcwd()} --env-vars {os.environ}\"\n\n    job = Studio.execute(\n        name=name,\n        command=command,\n        machine=machine,\n        cloud_compute=CloudCompute(\"cpu-small\"),\n    )\n\n    job_url = f\"{_get_lightning_cloud_url()}/studio/jobs/{job.job_id}\"\n    print(f\"Job URL: {job_url}\")\n\n    while job.status not in [\"SUCCEEDED\", \"FAILED\"]:\n        job = Studio.get_job(job_id=job.job_id)\n        print(f\"Job status: {job.status}\")\n        sleep(5)\n\n    if job.status == \"FAILED\":\n        raise RuntimeError(f\"Job failed with error: {job.error_message}\")\n\n    print(f\"Job succeeded with output: {job.output}\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\"The Lightning SDK is not installed. Please install it to use this function.\")\n\n    if not _BOTO3_AVAILABLE:\n        raise RuntimeError(\"The boto3 library is not installed. Please install it to use this function.\")\n\n    if machine is None:\n        machine = Machine(cpu=1, memory=\"1Gi\")\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run {sys.argv[0]} --cloud --env-vars {os.environ}\"\n\n    job = Studio.create_job(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n    )\n\n    job_url = f\"{_get_lightning_cloud_url()}/studio/jobs/{job.job_id}\"\n\n    print(f\"Job URL: {job_url}\")\n\n    while job.status != \"succeeded\":\n        if job.status in [\"failed\", \"cancelled\"]:\n            raise RuntimeError(f\"Job {job.job_id} failed with status {job.status}.\")\n        sleep(1)\n        job = Studio.get_job(job.job_id)\n\n    print(f\"Job {job.job_id} succeeded.\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ModuleNotFoundError(\"The Lightning SDK is not installed.\")\n\n    if machine is None:\n        machine = Machine(cpu=1, gpu=0, memory=\"1Gi\")\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.run app.py --entrypoint={sys.argv[0]} --arg={sys.argv[1]}\"\n\n    job = Studio.execute(\n        name=name,\n        cloud_compute=machine,\n        command=command,\n        num_nodes=num_nodes,\n    )\n\n    job_id = job.job_id\n\n    while True:\n        job_status = Studio.get_job(job_id=job_id).status\n        if job_status == \"running\":\n            print(f\"Job URL: {_get_lightning_cloud_url()}/studio/jobs/{job_id}\")\n            break\n        elif job_status == \"failed\":\n            raise RuntimeError(f\"Job failed with the following error: {job.error_message}\")\n        else:\n            sleep(5)"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ImportError(\"The `lightning` package is required to execute this operator.\")\n\n    if machine is None:\n        machine = Machine(cpu=1, gpu=0, memory=\"2Gi\")\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run --app {os.getcwd()} --env-file {os.getenv('LIGHTNING_ENV_FILE')}\"\n\n    studio = Studio(cloud=True)\n    studio.start()\n    studio.connect()\n\n    job = studio.launch(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n        stream_output=True,\n    )\n\n    print(f\"Job URL: {job.get_url()}\")\n\n    while job.status not in [\"SUCCEEDED\", \"FAILED\"]:\n        sleep(5)\n\n    if job.status == \"FAILED\":\n        raise RuntimeError(\"The job failed.\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ImportError(\n            \"The `lightning` package is required in order to use this feature. Please install it with `pip install lightning`.\"\n        )\n\n    if machine is None:\n        machine = Machine(cpu=1, memory=\"2Gi\")\n\n    if command is None:\n        command = f\"cd {os.getcwd()} && env\"\n\n    job = Studio.execute(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n        cloud=\"lightning\",\n    )\n\n    print(f\"Job URL: {_get_lightning_cloud_url()}/studio/projects/this_project/jobs/{job.job_id}\")\n\n    while job.status not in [\"SUCCEEDED\", \"FAILED\", \"CANCELLED\"]:\n        sleep(1)\n        job = Studio.get_job(job_id=job.job_id)\n\n    if job.status == \"FAILED\":\n        raise RuntimeError(f\"Job failed with the following error: {job.error_message}\")\n\n    if job.status == \"CANCELLED\":\n        raise RuntimeError(f\"Job was cancelled with the following error: {job.error_message}\")\n\n    print(f\"Job succeeded. Output: {job.output}\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ModuleNotFoundError(\"The Lightning SDK is not installed.\")\n\n    if not _BOTO3_AVAILABLE:\n        raise ModuleNotFoundError(\"The boto3 package is not installed.\")\n\n    if machine is None:\n        machine = Machine(cpu=1, memory=\"1Gi\")\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run --app {os.getcwd()} --env-vars {os.environ}\"\n\n    studio = Studio(lightning_cloud_url=_get_lightning_cloud_url())\n\n    job = studio.execute_job(\n        name=name,\n        command=command,\n        num_nodes=num_nodes,\n        machine=machine,\n    )\n\n    job_status = job.status\n    job_url = job.url\n\n    print(f\"Job URL: {job_url}\")\n\n    while job_status not in [\"COMPLETED\", \"FAILED\"]:\n        sleep(5)\n        job_status = job.status\n\n    if job_status == \"FAILED\":\n        raise RuntimeError(f\"The job failed with the following error: {job.error_message}\")\n\n    print(\"Job completed successfully.\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ImportError(\"The `lightning` SDK is required to execute this operator.\")\n\n    if machine is None:\n        machine = Machine(cpu=2, gpu=0, gpu_type=\"A100\", cloud_compute_type=\"GPU\")\n\n    if command is None:\n        command = f\"cd {os.getcwd()} && env\"\n\n    studio = Studio(cloud=True)\n    job = studio.create_data_job(\n        name=name,\n        machine=machine,\n        command=command,\n        num_nodes=num_nodes,\n    )\n\n    job.start()\n\n    while job.status not in [\"PENDING\", \"RUNNING\", \"COMPLETED\", \"FAILED\"]:\n        print(f\"Job status: {job.status}\")\n        sleep(5)\n\n    if job.status == \"FAILED\":\n        raise RuntimeError(\"The job failed.\")\n\n    print(f\"Job URL: {job.url}\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\n            \"The `lightning_sdk` package is required to execute this operator. Please install it with `pip install lightning-sdk`.\"\n        )\n\n    if machine is None:\n        machine = Machine(cpu=4, gpu=0)\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.run app run {os.getcwd()} --env-vars {os.getenv('LIGHTNING_ENV_VARS')}\"\n\n    # Create a job\n    job = Studio.create_data_job(\n        name=name,\n        machine=machine,\n        command=command,\n        num_nodes=num_nodes,\n        cloud_compute_type=\"cpu-small\",\n    )\n\n    # Print the job URL\n    print(f\"Job URL: {_get_lightning_cloud_url()}/studio/jobs/{job.job_id}\")\n\n    # Wait for the job to start\n    while job.status not in [\"PENDING\", \"RUNNING\"]:\n        sleep(5)\n        job = Studio.get_data_job(job_id=job.job_id)\n\n    # Print the job URL\n    print(f\"Job URL: {_get_lightning_cloud_url()}/studio/jobs/{job.job_id}\")\n\n    # Check if the job failed\n    if job.status == \"FAILED\":\n        raise RuntimeError(f\"Job failed with message: {job.message}\")\n\n    # Wait for the job to finish\n    while job.status not in [\"COMPLETED\", \"FAILED\"]:\n        sleep(5)\n        job = Studio.get_data_job(job_id=job.job_id)\n\n    # Check if the job failed\n    if job.status == \"FAILED\":\n        raise RuntimeError(f\"Job failed with message: {job.message}\")\n\n    # Print the job URL\n    print(f\"Job URL: {_get_light"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\n            \"The `lightning_sdk` package is required to use this function. Please install it with `pip install lightning-sdk`.\"\n        )\n\n    if machine is None:\n        machine = Machine(cpu=4, memory=16, gpu=0)\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run {os.getcwd()} --env-vars\"\n\n    studio = Studio(\n        cloud_compute=machine,\n        cloud_build_config={\"build_steps\": [{\"name\": \"python\", \"command\": command}]},\n    )\n\n    job = studio.start_training(name=name, num_nodes=num_nodes)\n\n    job_url = f\"{_get_lightning_cloud_url()}/studio/jobs/{job.job_id}\"\n\n    print(f\"Job URL: {job_url}\")\n\n    while job.status not in [\"succeeded\", \"failed\"]:\n        sleep(1)\n        job.refresh()\n\n    if job.status == \"failed\":\n        raise RuntimeError(f\"Job failed with the following message: {job.error_message}\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\n            \"The `lightning_sdk` package is not installed. Please install it with `pip install lightning-sdk`.\"\n        )\n\n    if machine is None:\n        machine = Machine(cpu=4, memory=\"16Gi\")\n\n    if command is None:\n        command = f\"python -m lightning_utilities.datasets.prepare_datasets --path {os.getcwd()} --env-vars {','.join(os.environ.keys())}\"\n\n    job = Studio.execute(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n        cloud_compute=Studio.CloudCompute.CLOUD_COMPUTE_GPU_MULTI,\n    )\n\n    job.wait()\n\n    if job.status != \"succeeded\":\n        raise RuntimeError(f\"The job {job.name} failed with status {job.status}.\")\n\n    print(f\"Job {job.name} succeeded. Check it out at: {job.url}\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ImportError(\"The Lightning SDK is not installed. Please install it to use this function.\")\n\n    if machine is None:\n        machine = Machine(cpu=1, gpu=0, cloud_compute_type=\"cpu-medium\")\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run --app {os.getcwd()} --env-vars {os.environ}\"\n\n    if not Studio.running_in_cloud():\n        raise RuntimeError(\n            \"This function can only be used when running in the Lightning Cloud. Please run this command in the Lightning Cloud.\"\n        )\n\n    client = LightningClient()\n\n    project_id = os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", None)\n    if project_id is None:\n        raise RuntimeError(\"The `project_id` couldn't be found from the environement variables.\")\n\n    cloud_space_id = os.getenv(\"LIGHTNING_CLOUD_SPACE_ID\", None)\n    if cloud_space_id is None:\n        raise RuntimeError(\"The `cloud_space_id` couldn't be found from the environement variables.\")\n\n    cloud_space = client.cloud_space_service_get_cloud_space(\n        project_id=project_id, cloud_space_id=cloud_space_id\n    )\n\n    if cloud_space.state != \"RUNNING\":\n        raise RuntimeError(\n            \"The current cloud space is not in the RUNNING state. Please make sure the cloud space is running.\"\n        )\n\n    job_id = client.job_service_create_job(\n        project_id=project_id,\n        cloud_space_id=cloud_space_id,\n        name=name,\n        machine=machine,\n        command=command,\n        num_nodes=num_nodes,\n    ).job_id\n\n    job_url = f\"{_get_lightning_cloud_url()}/project"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ModuleNotFoundError(\n            \"The Lightning SDK is not installed. Please install it by running `pip install lightning-sdk`.\"\n        )\n\n    if not Studio.running_in_cloud():\n        raise RuntimeError(\n            \"This function can only be used when running in the Lightning Cloud. Please make sure you are running in the Lightning Cloud.\"\n        )\n\n    if not Studio.running_in_studio():\n        raise RuntimeError(\n            \"This function can only be used when running in the Lightning Studio. Please make sure you are running in the Lightning Studio.\"\n        )\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run app {os.getcwd()} --env-vars {os.environ}\"\n\n    if machine is None:\n        machine = Machine(cpu=1, memory=\"1Gi\")\n\n    job_id = Studio.current_job_id()\n    if job_id is None:\n        raise RuntimeError(\"This function can only be used when running in the Lightning Studio.\")\n\n    client = LightningClient(max_tries=2)\n    client.job_service_create_job(\n        project_id=Studio.current_project_id(),\n        job_id=job_id,\n        job_name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n    )\n\n    while True:\n        job = client.job_service_get_job(project_id=Studio.current_project_id(), job_id=job_id)\n        if job.status == \"running\":\n            print(f\"Job {job.name} is running: {_get_lightning_cloud_url()}/studio/jobs/{job.id}\")\n        elif job.status == \"failed\":\n            raise RuntimeError(f\"Job {job.name} failed: {job.message}\")\n        elif job.status == \"succeeded\":\n            print(f\"Job {job"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\"The `lightning` SDK is required in order to execute this command.\")\n\n    if machine is None:\n        machine = Machine(cpu=1, gpu=0, ram=\"1Gi\")\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run app {os.getcwd()} --env-file {os.getenv('LIGHTNING_ENV_FILE')}\"\n\n    if not isinstance(machine, Machine):\n        raise ValueError(\"The `machine` argument must be of type `Machine`.\")\n\n    if not isinstance(command, str):\n        raise ValueError(\"The `command` argument must be of type `str`.\")\n\n    studio = Studio(cloud=True)\n\n    job = studio.execute(name, command, machine, num_nodes=num_nodes)\n\n    job_url = job.job_url\n    job_id = job.job_id\n\n    print(f\"Job URL: {job_url}\")\n\n    while True:\n        status = studio.get_job_status(job_id)\n\n        if status in [\"FAILED\", \"CANCELLED\"]:\n            raise RuntimeError(f\"The job failed with status: {status}.\")\n\n        if status == \"COMPLETED\":\n            break\n\n        sleep(1)\n\n    print(f\"Job URL: {job_url}\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ValueError(\n            \"This function requires the Lightning SDK to be installed. Please install it by running `pip install lightning-sdk`.\"\n        )\n\n    if machine is None:\n        machine = Machine(cpu=1, gpu=0, machine_type=\"cpu\")\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run {os.getcwd()} --env-vars {os.environ}\"\n\n    job = Studio.create_job(\n        name=name,\n        machine=machine,\n        num_nodes=num_nodes,\n        command=command,\n    )\n\n    print(f\"The job {job.name} has been submitted. You can track its progress at {job.url}\")\n\n    while job.status != \"succeeded\" and job.status != \"failed\":\n        sleep(5)\n        job = Studio.get_job(job.id)\n\n    if job.status == \"failed\":\n        raise RuntimeError(f\"The job {job.name} has failed. Please check its status at {job.url}.\")\n\n    print(f\"The job {job.name} has been successfully completed. You can check its status at {job.url}.\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    # Check if the required SDK is available\n    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ImportError(\"The `lightning` SDK is not available.\")\n\n    # Check if the Studio object is provided\n    if machine is None:\n        studio = Studio.get_from_environment()\n        if studio is None:\n            raise ValueError(\"The `machine` argument is required if the Studio object is not provided.\")\n        machine = studio.get_machine_by_name(\"data-preparation\")\n        if machine is None:\n            raise ValueError(\"The machine with the name `data-preparation` is not found.\")\n\n    # Get the current working directory\n    cwd = os.getcwd()\n\n    # Get the environment variables\n    env = os.environ.copy()\n\n    # Check if the command is provided\n    if command is None:\n        command = f\"python -m lightning.app.cli.run app={cwd} --env\"\n\n    # Create a job\n    job = machine.create_job(name=name, num_nodes=num_nodes, command=command, env=env)\n\n    # Print the job URL\n    print(f\"Job URL: {job.url}\")\n\n    # Wait for the job to complete\n    while job.status not in [\"SUCCEEDED\", \"FAILED\"]:\n        job.refresh()\n        sleep(1)\n\n    # Raise an exception if the job fails\n    if job.status == \"FAILED\":\n        raise RuntimeError(f\"The job failed with the following error: {job.error}\")\n\n    # Print the job URL\n    print(f\"Job URL: {job.url}\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\n            \"The Lightning SDK is not installed. Please install the Lightning SDK to use this function.\"\n        )\n\n    if machine is None:\n        machine = Machine(cpu=1, memory=\"10Gi\")\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.run app.py --env-vars={os.environ} --cwd={os.getcwd()}\"\n\n    studio = Studio(cloud_compute=machine)\n\n    job = studio.start_job(\n        name=name,\n        command=command,\n        num_nodes=num_nodes,\n        cloud_compute=machine,\n        cloud_url=_get_lightning_cloud_url(),\n    )\n\n    while job.status not in [\"PENDING\", \"RUNNING\"]:\n        sleep(5)\n        job.refresh()\n\n    if job.status == \"PENDING\":\n        print(f\"The job `{job.name}` is pending. Please wait for it to start.\")\n\n    if job.status == \"RUNNING\":\n        print(f\"The job `{job.name}` is running. You can follow the logs at: {job.url}\")\n\n    job.wait()\n\n    if job.status == \"FAILED\":\n        raise RuntimeError(f\"The job `{job.name}` failed. You can follow the logs at: {job.url}\")\n\n    if job.status == \"SUCCEEDED\":\n        print(f\"The job `{job.name}` succeeded. You can follow the logs at: {job.url}\")\n\n    job.stop()"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\n            \"The `lightning_sdk` package is required to execute the operator. Please install it by running `pip install lightning-sdk`.\"\n        )\n\n    if machine is None:\n        machine = Machine(cpu=1, gpu=0, machine_type=\"cpu\", cloud_compute_type=\"cpu-small\")\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run {os.getcwd()} --envs {os.getenv('LIGHTNING_ENVIRONMENT_VARIABLES')}\"\n\n    studio = Studio(\n        cloud_compute=machine,\n        num_nodes=num_nodes,\n        cloud_build_config=None,\n        project_name=name,\n        storage=None,\n        run_name=name,\n        command=command,\n        project_root=os.getcwd(),\n        run_config=None,\n        storage_config=None,\n        storage_type=None,\n        cloud_build_id=None,\n        cloud_build_job_id=None,\n        cloud_build_job_name=None,\n        cloud_build_job_type=None,\n        cloud_build_job_url=None,\n        cloud_build_logs_url=None,\n        cloud_build_status=None,\n        cloud_build_url=None,\n        cloud_compute_id=None,\n        cloud_compute_type=None,\n        cloud_run_id=None,\n        cloud_run_name=None,\n        cloud_run_status=None,\n        cloud_run_url=None,\n        cloud_run_workflow_id=None,\n        cloud_run_workflow_name=None,\n        cloud_run_workflow_status=None,\n        cloud_run_workflow_url=None,\n        cloud_run_workflow_version=None,\n        cloud_run_workflow_version_id=None,\n        cloud_run_workflow_version"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\n            \"To execute the current operator remotely, the Lightning SDK must be installed. \"\n            \"Please install the SDK by running `pip install lightning-sdk`.\"\n        )\n\n    if not Studio.running_in_cloud():\n        raise RuntimeError(\n            \"To execute the current operator remotely, the operator must be executed in the Lightning Cloud. \"\n            \"Please run the operator in the Lightning Cloud by setting `studio.run()` in the `__init__.py` file.\"\n        )\n\n    studio = Studio.from_current_environment()\n\n    if studio.studio_id is None:\n        raise RuntimeError(\n            \"To execute the current operator remotely, the Studio ID must be set in the `__init__.py` file. \"\n            \"Please set the Studio ID by running `studio.studio_id = <STUDIO_ID>` in the `__init__.py` file.\"\n        )\n\n    if studio.studio_id.startswith(\"studio-\"):\n        studio_id = studio.studio_id\n    else:\n        studio_id = f\"studio-{studio.studio_id}\"\n\n    if machine is None:\n        machine = Machine(cpu=1, gpu=0)\n\n    if command is None:\n        command = f\"python -m lightning_data_prep.cli.cli run --studio-id {studio_id}\"\n\n    job_id = studio.start_job(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n    )\n\n    job_url = f\"{_get_lightning_cloud_url()}/studio/{studio_id}/jobs/{job_id}\"\n\n    print(f\"Job URL: {job_url}\")\n\n    while True:\n        job_status = studio.get_job_status(job_id)\n        if job_status == \"running\":\n            print(f\"Job URL:"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n\n    "}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n\n    "}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n\n    "}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n\n    "}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n\n    "}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n\n    "}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n\n    "}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n\n    "}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n\n    "}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n\n    "}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n\n    "}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n\n    "}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n\n    "}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n\n    "}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n\n    "}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config is not None:\n            return self._config\n\n        if not os.path.exists(os.path.join(self._cache_dir, \"index.json\")):\n            return None\n\n        self._config = ChunksConfig(\n            cache_dir=self._cache_dir,\n            remote_dir=self._remote_input_dir,\n            compressor=self._compression,\n            serializers=self._serializers,\n            item_loader=self._item_loader,\n        )\n\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config is not None:\n            return self._config\n\n        # Load the config containing the index\n        index_filepath = os.path.join(self._cache_dir, \"index.json\")\n        if not os.path.exists(index_filepath):\n            return None\n\n        self._config = ChunksConfig.from_index_file(\n            index_filepath,\n            self._cache_dir,\n            self._remote_input_dir,\n            self._compression,\n            self._serializers,\n            self._item_loader,\n        )\n\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if not os.path.exists(self._cache_dir):\n            raise FileNotFoundError(f\"The provided cache_dir `{self._cache_dir}` doesn't exist.\")\n\n        # Load the config containing the index\n        index_file = os.path.join(self._cache_dir, \"index.json\")\n        if not os.path.exists(index_file):\n            return None\n\n        self._config = ChunksConfig.from_index_file(\n            index_file,\n            self._cache_dir,\n            self._serializers,\n            self._remote_input_dir,\n            self._compression,\n            self._item_loader,\n        )\n\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config is not None:\n            return self._config\n\n        # Check whether the index file exists\n        index_file = os.path.join(self._cache_dir, \"index.json\")\n        if not os.path.exists(index_file):\n            return None\n\n        # Load the config\n        self._config = ChunksConfig.load(\n            self._cache_dir,\n            self._serializers,\n            self._remote_input_dir,\n            self._compression,\n            self._item_loader,\n            self._max_cache_size,\n        )\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config is not None:\n            return self._config\n\n        # Check whether the index file is available\n        index_filepath = os.path.join(self._cache_dir, \"index.json\")\n        if not os.path.exists(index_filepath):\n            return None\n\n        # Load the config\n        self._config = ChunksConfig.from_index_file(\n            index_filepath,\n            cache_dir=self._cache_dir,\n            remote_dir=self._remote_input_dir,\n            compression=self._compression,\n            serializers=self._serializers,\n            item_loader=self._item_loader,\n        )\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._remote_input_dir is not None:\n            self._config = ChunksConfig.from_remote_dir(\n                self._cache_dir,\n                self._remote_input_dir,\n                self._serializers,\n                self._compression,\n                self._item_loader,\n                self._max_cache_size,\n            )\n        else:\n            self._config = ChunksConfig.from_local_dir(\n                self._cache_dir,\n                self._serializers,\n                self._compression,\n                self._item_loader,\n                self._max_cache_size,\n            )\n\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config is not None:\n            return self._config\n\n        # Load the config\n        config_path = os.path.join(self._cache_dir, \"index.json\")\n        if os.path.exists(config_path):\n            self._config = ChunksConfig.load(config_path)\n            self._config._remote_dir = self._remote_input_dir\n            self._config._compressor = self._compression\n            self._config._serializers = self._serializers\n            return self._config\n\n        return None"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config is not None:\n            return self._config\n\n        # Check if the index files are available\n        if not os.path.exists(os.path.join(self._cache_dir, \"indexes.json\")):\n            return None\n\n        # Load the config\n        self._config = ChunksConfig.load(\n            self._cache_dir,\n            self._serializers,\n            self._remote_input_dir,\n            self._compression,\n            self._item_loader,\n            self._distributed_env,\n            self._rank,\n        )\n\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        try:\n            # Load the config containing the index\n            config = ChunksConfig.load(\n                self._cache_dir,\n                self._serializers,\n                self._remote_input_dir,\n                self._compression,\n                self._item_loader,\n                self._distributed_env,\n                self.rank,\n            )\n        except FileNotFoundError:\n            return None\n\n        self._config = config\n        return config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config is not None:\n            return self._config\n\n        index_file = os.path.join(self._cache_dir, \"index.txt\")\n        if not os.path.exists(index_file):\n            return None\n\n        # Load the config containing the index\n        self._config = ChunksConfig(\n            index_file=index_file,\n            cache_dir=self._cache_dir,\n            serializers=self._serializers,\n            remote_input_dir=self._remote_input_dir,\n            compression=self._compression,\n            item_loader=self._item_loader,\n        )\n\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if not self._remote_input_dir:\n            return None\n\n        # Load the index file\n        index_filepath = os.path.join(self._cache_dir, \"index.json\")\n        if not os.path.exists(index_filepath):\n            raise FileNotFoundError(f\"The provided index_filepath `{index_filepath}` doesn't exist.\")\n\n        # Load the config containing the index\n        self._config = ChunksConfig.load_from_index_file(\n            index_filepath=index_filepath,\n            cache_dir=self._cache_dir,\n            remote_dir=self._remote_input_dir,\n            serializers=self._serializers,\n            compression=self._compression,\n            item_loader=self._item_loader,\n        )\n\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Load the config containing the index\n        if self._config is not None:\n            return self._config\n\n        index_files = [\n            os.path.join(self._cache_dir, f)\n            for f in os.listdir(self._cache_dir)\n            if f.endswith(\".index\") and f.startswith(f\"{self.rank:04d}\")\n        ]\n\n        if not index_files:\n            return None\n\n        self._config = ChunksConfig(\n            index_files,\n            self._serializers,\n            self._remote_input_dir,\n            self._compression,\n            self._item_loader,\n            self._cache_dir,\n        )\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Load the config containing the index\n        if self._config is not None:\n            return self._config\n\n        index_files = [\n            os.path.join(self._cache_dir, f)\n            for f in os.listdir(self._cache_dir)\n            if f.endswith(\".index\") and not f.endswith(f\"{self.rank}.index\")\n        ]\n\n        if not index_files:\n            return None\n\n        index_files.sort()\n        index_file = index_files[-1]\n        self._config = ChunksConfig.load(index_file, self._serializers, self._item_loader, self._compression)\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if not self._config:\n            # If the index files are available, load the configuration\n            if os.path.exists(os.path.join(self._cache_dir, \"indexes.json\")):\n                self._config = ChunksConfig(\n                    self._cache_dir,\n                    self._remote_input_dir,\n                    self._compression,\n                    self._serializers,\n                    self._item_loader,\n                )\n                return self._config\n\n        return None"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._remote_input_dir:\n            if self._rank is None:\n                self._worker_env = _WorkerEnv.detect()\n                self._rank = self._distributed_env.global_rank * self._worker_env.world_size + self._worker_env.rank\n            index_filepath = os.path.join(self._cache_dir, f\"{self._rank}.index\")\n            if not os.path.exists(index_filepath):\n                return None\n            self._config = ChunksConfig.from_index_file(\n                index_filepath=index_filepath,\n                cache_dir=self._cache_dir,\n                remote_dir=self._remote_input_dir,\n                compression=self._compression,\n                serializers=self._serializers,\n            )\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Check whether the index file exists\n        index_file = os.path.join(self._cache_dir, \"index.json\")\n        if not os.path.exists(index_file):\n            return None\n\n        # Load the config\n        self._config = ChunksConfig.from_json(\n            index_file,\n            remote_input_dir=self._remote_input_dir,\n            compression=self._compression,\n            serializers=self._serializers,\n        )\n\n        # Check whether the index file is valid\n        if self._config.num_bytes == 0:\n            raise ValueError(\"The index file is empty.\")\n\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Load the config containing the index\n        if self._config is not None:\n            return self._config\n\n        if not os.path.exists(self._cache_dir):\n            raise FileNotFoundError(f\"The provided cache_dir `{self._cache_dir}` doesn't exist.\")\n\n        index_files = [\n            os.path.join(self._cache_dir, filename)\n            for filename in os.listdir(self._cache_dir)\n            if filename.endswith(\".index\")\n        ]\n\n        if not index_files:\n            raise FileNotFoundError(f\"No index files found in the provided cache_dir `{self._cache_dir}`.\")\n\n        self._config = ChunksConfig(\n            cache_dir=self._cache_dir,\n            remote_dir=self._remote_input_dir,\n            index_files=index_files,\n            serializers=self._serializers,\n            compressor=self._compression,\n            item_loader=self._item_loader,\n        )\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if not os.path.exists(self._cache_dir):\n            raise FileNotFoundError(f\"The provided cache_dir `{self._cache_dir}` doesn't exist.\")\n\n        index_files = [\n            os.path.join(self._cache_dir, file)\n            for file in os.listdir(self._cache_dir)\n            if file.startswith(\"index_\") and file.endswith(\".json\")\n        ]\n\n        if len(index_files) == 0:\n            return None\n\n        index_files.sort(key=lambda x: int(os.path.basename(x).split(\"_\")[1].split(\".\")[0]))\n\n        # Load the config containing the index\n        self._config = ChunksConfig.from_index_files(\n            index_files,\n            self._serializers,\n            self._cache_dir,\n            self._remote_input_dir,\n            self._compression,\n            self._item_loader,\n        )\n\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config:\n            return self._config\n\n        if self._intervals is None:\n            self._intervals = self._get_intervals()\n\n        if self._intervals is None:\n            return None\n\n        self._config = ChunksConfig(\n            self._cache_dir,\n            self._remote_input_dir,\n            self._compression,\n            self._intervals,\n            self._serializers,\n            self._item_loader,\n        )\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        try:\n            self._config = ChunksConfig.load_from_index(\n                cache_dir=self._cache_dir,\n                remote_input_dir=self._remote_input_dir,\n                compression=self._compression,\n                serializers=self._serializers,\n                item_loader=self._item_loader,\n            )\n            return self._config\n        except FileNotFoundError:\n            logger.warning(\n                f\"The index files for the dataset are not available at `{self._cache_dir}`. \"\n                \"You can use the `BinaryWriter` to generate them.\"\n            )\n            return None"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n\n    "}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n\n    "}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n\n    "}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n\n    "}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n\n    "}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n\n    "}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n        return self._config\n\n    "}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n        return self._config\n\n    "}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        return self._config\n\n    "}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        return self._config\n\n    "}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The config should be defined before accessing it.\")\n        return self._config\n\n    "}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The config is not set.\")\n        return self._config\n\n    "}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration should be defined before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The config should be defined before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The config should be defined before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration should be defined before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration should be defined before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index should be a ChunkedIndex.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Load the chunk\n        chunk_index = index.chunk_index\n        chunk_filepath, _, _ = self.config[index]\n\n        if not os.path.exists(chunk_filepath):\n            # Download the chunk\n            self._prepare_thread.download([chunk_index])\n\n        # Load the item\n        return self._item_loader.load(index, chunk_filepath)\n\n    "}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index should be an instance of ChunkedIndex.\")\n\n        # Check if the chunk is already loaded\n        chunk_index = index.chunk_index\n        chunk_filepath, _, _ = self.config[index]\n        if not os.path.exists(chunk_filepath):\n            # Load the config containing the index\n            if self._config is None and self._try_load_config() is None:\n                raise Exception(\"The reader index isn't defined.\")\n\n            # Load the chunk\n            self._config.download_chunk_from_index(chunk_index)\n\n        # Load the item\n        return self._item_loader.load(index, chunk_filepath)"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index should be a ChunkedIndex.\")\n\n        if self._prepare_thread is None:\n            raise Exception(\"The prepare thread should be defined.\")\n\n        assert self._prepare_thread is not None\n\n        chunk_index = index.chunk_index\n        chunk_filepath, _, _ = self.config[index]\n\n        # Check if the chunk is already downloaded\n        if not os.path.exists(chunk_filepath):\n            # Download the chunk\n            self._prepare_thread.download([chunk_index])\n\n        # Load the chunk\n        return self._item_loader.load(index, chunk_filepath)\n\n    "}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The index `{index}` is not a valid ChunkedIndex.\")\n\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index from the index\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Check if the chunk is already downloaded\n        chunk_filepath, _, _ = self.config[index]\n        if not os.path.exists(chunk_filepath):\n            self._prepare_thread.download([chunk_index])\n\n        # Load the chunk\n        return self._item_loader.load(index, chunk_filepath)"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index should be an instance of ChunkedIndex.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index from the index\n        chunk_index = index.chunk_index\n\n        # Check if the chunk is available\n        if chunk_index not in self.config:\n            # If not, download the chunk\n            self._prepare_thread.download([chunk_index])\n\n        # Get the chunk filepath\n        chunk_filepath, _, _ = self.config[ChunkedIndex(index=-1, chunk_index=chunk_index)]\n\n        # Get the item index within the chunk\n        item_index = index.index\n\n        # Load the chunk\n        return self._item_loader.load(chunk_index, chunk_filepath, item_index)\n\n    "}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index should be a ChunkedIndex.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index from the provided index\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Get the chunk filepath and the chunk size\n        chunk_filepath, chunk_size, _ = self.config[index]\n\n        # Get the item index from the provided index\n        item_index = index.index - self.config.get_chunk_start_index(chunk_index)\n\n        # Load the chunk if it's not already loaded\n        if chunk_index not in self._item_loader.loaded_chunks:\n            self._item_loader.load_chunk(chunk_index, chunk_filepath, chunk_size, self._compression)\n\n        # Get the item from the chunk\n        item = self._item_loader.get_item(chunk_index, item_index)\n\n        # Delete the chunk if it's not the last one\n        if chunk_index != self._last_chunk_index:\n            self._last_chunk_index = chunk_index\n            self._item_loader.delete_chunk(chunk_index)\n\n        return item\n\n    "}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index should be a ChunkedIndex.\")\n\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        chunk_index = index.chunk_index\n        if chunk_index == -1:\n            chunk_index = self._get_chunk_index_from_index(index.index)\n\n        chunk_filepath, _, _ = self.config[index]\n\n        # If the chunk is not in memory, we need to download it\n        if not self._item_loader.is_chunk_loaded(chunk_index):\n            if self._prepare_thread is None:\n                self._prepare_thread = PrepareChunksThread(\n                    self.config,\n                    self._item_loader,\n                    self._distributed_env,\n                    self._max_cache_size,\n                    max_pre_download=2,\n                )\n                self._prepare_thread.start()\n\n            # If the chunk is not in memory, we need to download it\n            if not self._item_loader.is_chunk_loaded(chunk_index):\n                self._prepare_thread.download([chunk_index])\n\n        # If the chunk is not in memory, we need to download it\n        if not self._item_loader.is_chunk_loaded(chunk_index):\n            raise Exception(\"The chunk is not loaded.\")\n\n        # If the chunk is not in memory, we need to download it\n        if not self._item_loader.is_chunk_loaded(chunk_index):\n            raise Exception(\"The chunk is not loaded.\")\n\n        # If the chunk is not in memory, we need to download it\n        if not self._item_loader.is_chunk_loaded(chunk_index):\n            raise Exception(\"The chunk is not loaded.\")\n\n        # If the chunk is not in memory, we need to download it\n        if not self._item_loader.is_chunk_loaded(chunk_index):\n            raise Exception(\"The chunk is not loaded"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Check whether the chunk is already downloaded\n        chunk_index = index.chunk_index\n        if chunk_index not in self.config:\n            raise Exception(f\"The chunk {chunk_index} is not available.\")\n\n        # Load the chunk\n        chunk_filepath, chunk_start_index, chunk_end_index = self.config[index.chunk_index]\n        assert self._prepare_thread is not None\n        if chunk_index != self._last_chunk_index:\n            self._last_chunk_index = chunk_index\n            self._prepare_thread.download([chunk_index])\n\n        # Get the item from the chunk\n        item = self._item_loader.load(chunk_filepath, chunk_start_index, chunk_end_index, index.index)\n\n        return item\n\n    "}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index should be an instance of ChunkedIndex.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index from the provided index\n        chunk_index = index.chunk_index\n\n        # Get the chunk filepath and the item index within the chunk\n        chunk_filepath, item_index, item_size = self.config[index]\n\n        # Check whether the chunk is already loaded\n        if not self._item_loader.is_loaded(chunk_index):\n            # If not, load it\n            self._item_loader.load(chunk_index, chunk_filepath)\n\n        # Get the item from the loaded chunk\n        item = self._item_loader.get_item(chunk_index, item_index)\n\n        # Delete the chunk if it is not needed anymore\n        if self._last_chunk_index is not None and chunk_index != self._last_chunk_index:\n            self._item_loader.delete(self._last_chunk_index, chunk_filepath)\n\n        # Update the last chunk index\n        self._last_chunk_index = chunk_index\n\n        return item"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index should be an instance of ChunkedIndex.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Load the chunk\n        chunk_filepath, chunk_index, chunk_offset = self._config[index]\n\n        # Initialize the prepare thread if needed\n        if self._prepare_thread is None:\n            self._prepare_thread = PrepareChunksThread(\n                self.config,\n                self._item_loader,\n                self._distributed_env,\n                max_cache_size=self._max_cache_size,\n                max_pre_download=self._distributed_env.num_nodes,\n            )\n            self._prepare_thread.start()\n\n        # Check if the chunk is already loaded\n        if chunk_index != self._last_chunk_index:\n            # Download the chunk if needed\n            self._prepare_thread.download([chunk_index])\n\n            # Delete the previous chunk if needed\n            if self._last_chunk_index is not None:\n                self._prepare_thread.delete([self._last_chunk_index])\n\n            self._last_chunk_index = chunk_index\n\n        # Load the chunk\n        return self._item_loader.load_chunk(chunk_index, chunk_filepath, chunk_offset, self._compression)"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The index should be a ChunkedIndex. Got {type(index)}.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Check if the chunk is available\n        chunk_index = index.chunk_index\n        chunk_filepath, chunk_metadata, _ = self.config[index]\n        if not os.path.exists(chunk_filepath):\n            # If the chunk is not available, download it\n            if self._prepare_thread is None:\n                raise Exception(\"The prepare thread should be defined.\")\n            self._prepare_thread.download([chunk_index])\n\n        # Load the item from the chunk\n        item = self._item_loader.load(chunk_filepath, chunk_metadata, index.index)\n\n        # Delete the chunk if the item has been fully consumed\n        if self._last_chunk_index != chunk_index:\n            self._last_chunk_index = chunk_index\n            if self._prepare_thread is None:\n                raise Exception(\"The prepare thread should be defined.\")\n            self._prepare_thread.delete([chunk_index])\n\n        return item"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index should be a ChunkedIndex.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index from the provided index\n        chunk_index = index.chunk_index\n\n        # Check if the chunk is available\n        chunk_filepath, _, _ = self.config[index]\n        if not os.path.exists(chunk_filepath):\n            # If not, download it\n            self._prepare_thread.download([chunk_index])\n\n        # Load the chunk\n        return self._item_loader.load(chunk_filepath)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The provided index should be of type ChunkedIndex, got {type(index)}.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index from the index\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Get the chunk filepath\n        chunk_filepath, _, _ = self.config[index]\n\n        # Check whether the chunk is available locally\n        if not os.path.exists(chunk_filepath):\n            # Check whether the prepare thread is available\n            if self._prepare_thread is None:\n                raise Exception(\"The prepare thread is not defined.\")\n\n            # Download the chunk\n            self._prepare_thread.download([chunk_index])\n\n        # Load the chunk\n        return self._item_loader.load(index, chunk_filepath)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The provided index is not an instance of ChunkedIndex.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        chunk_index = index.chunk_index\n\n        # Load the chunk\n        chunk_filepath, _, _ = self._config[index]\n        if not os.path.exists(chunk_filepath):\n            raise FileNotFoundError(f\"The chunk `{chunk_filepath}` doesn't exist.\")\n\n        # Get the chunk index\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Load the chunk\n        item = self._item_loader.load(chunk_index, chunk_filepath, self._compression)\n\n        # Delete the chunk if it is the last one to be consumed\n        if self._last_chunk_index is not None and chunk_index == self._last_chunk_index:\n            self._last_chunk_index = None\n            self._prepare_thread.delete([chunk_index])\n\n        return item"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The provided index `{index}` should be an instance of ChunkedIndex.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index from the index\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # If the chunk is already in memory, return the item\n        if chunk_index in self.config._chunks_in_memory:\n            return self._item_loader.load(chunk_index, index.chunk_index, self.config._chunks_in_memory[chunk_index])\n\n        # If the chunk is not in memory, check if it is already downloaded\n        if chunk_index in self.config._chunks_downloaded:\n            return self._item_loader.load(chunk_index, index.chunk_index, self.config._chunks_downloaded[chunk_index])\n\n        # If the chunk is not in memory or downloaded, download it\n        self._prepare_thread.download([chunk_index])\n\n        # Wait for the chunk to be downloaded\n        while chunk_index not in self.config._chunks_downloaded:\n            pass\n\n        # Return the item\n        return self._item_loader.load(chunk_index, index.chunk_index, self.config._chunks_downloaded[chunk_index])\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The provided index is not an instance of ChunkedIndex.\")\n\n        chunk_index = index.chunk_index\n        if chunk_index is None:\n            chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Check whether the chunk is available\n        chunk_filepath, _, _ = self.config[index]\n        if not os.path.exists(chunk_filepath):\n            # If not, start the download\n            if self._prepare_thread is None:\n                raise Exception(\"The prepare thread should be defined.\")\n            self._prepare_thread.download([chunk_index])\n\n        # Load the item\n        return self._item_loader.load(index, chunk_filepath)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The index should be a ChunkedIndex. Got {type(index)}\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index\n        chunk_index = index.chunk_index\n\n        # Check if the chunk is available\n        if chunk_index not in self._config:\n            # If not, download it\n            self._prepare_thread.download([chunk_index])\n\n        # Load the chunk\n        chunk_filepath, chunk_index, chunk_offset = self._config[index]\n        return self._item_loader.load_chunk_item(chunk_index, chunk_filepath, chunk_offset)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The provided index `{index}` is not a ChunkedIndex.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index from the index\n        chunk_index = index.chunk_index\n\n        # Check if the chunk is already loaded\n        if chunk_index in self._item_loader.loaded_chunks:\n            return self._item_loader.read(index)\n\n        # Get the chunk filepath from the index\n        chunk_filepath, _, _ = self.config[index]\n\n        # Check if the chunk is already downloaded\n        if not os.path.exists(chunk_filepath):\n            # Check if the chunk is already being downloaded\n            if chunk_index in self._prepare_thread._to_download_queue.queue:\n                return None\n\n            # Download the chunk\n            self._prepare_thread.download([chunk_index])\n\n        # Wait for the chunk to be loaded\n        while chunk_index not in self._item_loader.loaded_chunks:\n            pass\n\n        # Load the chunk\n        self._item_loader.load_chunk(chunk_index, chunk_filepath)\n\n        # Get the item from the chunk\n        return self._item_loader.read(index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index should be a ChunkedIndex.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index from the index\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Load the chunk if it's not already loaded\n        if chunk_index not in self.config._loaded_chunks:\n            self._load_chunk(chunk_index)\n\n        # Get the chunk from the loaded chunks\n        chunk = self.config._loaded_chunks[chunk_index]\n\n        # Get the item index from the index\n        item_index = index.index - chunk.index\n\n        # Load the item from the chunk\n        return self._item_loader.load(chunk.filepath, item_index, chunk.serializer)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The provided index `{index}` is not a ChunkedIndex.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index from the provided index\n        chunk_index = index.chunk_index\n\n        # Get the chunk filepath\n        chunk_filepath, chunk_offset, chunk_size = self.config[index]\n\n        # Check whether the chunk is already downloaded\n        if not self._item_loader.is_chunk_loaded(chunk_index):\n            # If not, download the chunk\n            self._item_loader.download_chunk(chunk_index, chunk_filepath, chunk_offset, chunk_size, self._compression)\n\n        # Load the chunk\n        self._item_loader.load_chunk(chunk_index, chunk_filepath)\n\n        # Get the item from the chunk\n        item = self._item_loader.get_item(index)\n\n        # If this is the last chunk, delete the chunk\n        if index.chunk_index == self._config.num_chunks - 1:\n            self._item_loader.delete(chunk_index, chunk_filepath)\n\n        return item\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n    return _ImmutableDistributedMap().set_and_get(key, obj)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n    return _ImmutableDistributedMap().set_and_get(key, obj)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n    return _ImmutableDistributedMap().set_and_get(key, obj)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n\n    distributed_map = _ImmutableDistributedMap()\n    return distributed_map.set_and_get(key, obj)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n\n    distributed_map = _ImmutableDistributedMap()\n    return distributed_map.set_and_get(key, obj)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n\n    distributed_map = _ImmutableDistributedMap()\n    return distributed_map.set_and_get(key, obj)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n\n    distributed_map = _ImmutableDistributedMap()\n    return distributed_map.set_and_get(key, obj)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n\n    distributed_map = _ImmutableDistributedMap()\n    return distributed_map.set_and_get(key, obj)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n    return _ImmutableDistributedMap().set_and_get(key, obj)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n\n    distributed_map = _ImmutableDistributedMap()\n    return distributed_map.set_and_get(key, obj)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n\n    distributed_map = _ImmutableDistributedMap()\n    return distributed_map.set_and_get(key, obj)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n\n    distributed_map = _ImmutableDistributedMap()\n    return distributed_map.set_and_get(key, obj)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n\n    return _ImmutableDistributedMap().set_and_get(key, obj)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n    distributed_map = _ImmutableDistributedMap()\n    return distributed_map.set_and_get(key, obj)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n    distributed_map = _ImmutableDistributedMap()\n    return distributed_map.set_and_get(key, obj)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n\n    map = _ImmutableDistributedMap()\n    return map.set_and_get(key, obj)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n    return _ImmutableDistributedMap().set_and_get(key, obj)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n\n    if os.getenv(\"LIGHTNING_APP_STATE_URL\") is None:\n        raise RuntimeError(\"The `LIGHTNING_APP_STATE_URL` should be set.\")\n\n    distributed_map = _ImmutableDistributedMap()\n    return distributed_map.set_and_get(key, obj)"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    np.random.seed(seed + current_epoch)\n    shuffled_chunks_per_ranks = [\n        np.random.permutation(chunks) for chunks in chunks_per_ranks\n    ]\n    return [chunk for chunks in shuffled_chunks_per_ranks for chunk in chunks]"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    rng = np.random.default_rng(seed + current_epoch)\n    shuffled_chunks: List[int] = []\n    for rank in range(distributed_env.world_size):\n        rng.shuffle(chunks_per_ranks[rank])\n        shuffled_chunks.extend(chunks_per_ranks[rank])\n    return shuffled_chunks"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Generate a random permutation of chunk indexes\n    permutation = np.random.RandomState(seed + current_epoch).permutation(\n        sum(chunks_per_ranks, [])\n    )\n\n    # 2. Split the permutation into chunks based on the number of chunks assigned to each rank\n    chunks_per_ranks = [\n        permutation[sum(chunks_per_ranks[:i], []): sum(chunks_per_ranks[:i + 1], [])]\n        for i in range(distributed_env.world_size)\n    ]\n\n    # 3. Flatten the chunks into a single list\n    flattened_chunks = [chunk for chunks in chunks_per_ranks for chunk in chunks]\n\n    return flattened_chunks"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Shuffle the chunk indexes for each rank\n    for rank in range(distributed_env.world_size):\n        np.random.seed(seed + current_epoch * distributed_env.world_size + rank)\n        np.random.shuffle(chunks_per_ranks[rank])\n\n    # 2. Flatten the shuffled chunk indexes\n    flattened_chunks_per_ranks = [\n        chunk for rank_chunks in chunks_per_ranks for chunk in rank_chunks\n    ]\n\n    return flattened_chunks_per_ranks"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Compute the number of chunks per node\n    num_chunks_per_node = [len(chunks) for chunks in chunks_per_ranks]\n\n    # 2. Compute the number of chunks per node for each rank\n    num_chunks_per_node_per_rank = [\n        num_chunks // distributed_env.world_size + num_chunks % distributed_env.world_size\n        if rank == distributed_env.world_size - 1\n        else num_chunks // distributed_env.world_size\n        for rank, num_chunks in enumerate(num_chunks_per_node)\n    ]\n\n    # 3. Compute the number of chunks per node for each rank\n    num_chunks_per_rank = [\n        sum(num_chunks_per_node_per_rank[:rank + 1])\n        for rank in range(distributed_env.world_size)\n    ]\n\n    # 4. Compute the number of chunks per node for each rank\n    num_chunks_per_rank = [\n        sum(num_chunks_per_node_per_rank[:rank + 1])\n        for rank in range(distributed_env.world_size)\n    ]\n\n    # 5. Compute the number of chunks per node for each rank\n    num_chunks_per_rank = [\n        sum(num_chunks_per_node_per_rank[:rank + 1])\n        for rank in range(distributed_env.world_size)\n    ]\n\n    # 6. Compute the number of chunks per node for each rank\n    num_chunks_per_rank = [\n        sum(num_chunks_per_node_per_rank[:rank + 1])\n        for rank in range(distributed_env.world_size)\n    ]\n\n    # 7. Compute the number of chunks per node for each rank\n    num_chunks_per_rank = [\n        sum(num_chunks_per_node_per_rank[:rank + 1])\n       "}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Flatten the chunks_per_ranks list\n    flattened_chunks_per_ranks = [\n        chunk_index for chunks in chunks_per_ranks for chunk_index in chunks\n    ]\n\n    # 2. Calculate the seed for the current epoch\n    current_seed = seed + current_epoch\n\n    # 3. Shuffle the flattened chunks using the current seed\n    np.random.seed(current_seed)\n    np.random.shuffle(flattened_chunks_per_ranks)\n\n    # 4. Return the shuffled flattened chunks\n    return flattened_chunks_per_ranks"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Create a random generator with the provided seed and current epoch\n    rng = np.random.default_rng(seed + current_epoch)\n\n    # 2. Shuffle the chunk indexes for each rank\n    shuffled_chunk_indexes: List[int] = []\n    for chunk_indexes in chunks_per_ranks:\n        rng.shuffle(chunk_indexes)\n        shuffled_chunk_indexes.extend(chunk_indexes)\n\n    # 3. Shuffle the shuffled chunk indexes across all nodes\n    rng.shuffle(shuffled_chunk_indexes)\n\n    return shuffled_chunk_indexes"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Get the number of chunks per node\n    num_chunks_per_node = [len(chunks) for chunks in chunks_per_ranks]\n\n    # 2. Create a list of chunk indexes for each node\n    chunks_per_node = [[] for _ in range(distributed_env.world_size)]\n    for rank, chunks in enumerate(chunks_per_ranks):\n        chunks_per_node[rank].extend(chunks)\n\n    # 3. Shuffle the chunk indexes for each node\n    for rank, chunks in enumerate(chunks_per_node):\n        np.random.seed(seed + current_epoch * distributed_env.world_size + rank)\n        np.random.shuffle(chunks)\n\n    # 4. Flatten the list of shuffled chunk indexes\n    flattened_chunks = [chunk for chunks in chunks_per_node for chunk in chunks]\n\n    return flattened_chunks"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Generate a random permutation of chunk indexes\n    chunk_indexes = np.arange(len(chunks_per_ranks))\n    rng = np.random.default_rng(seed + current_epoch)\n    rng.shuffle(chunk_indexes)\n\n    # 2. Broadcast the random permutation to all nodes\n    chunk_indexes = distributed_env.broadcast(chunk_indexes)\n\n    # 3. Shuffle the chunk indexes based on the random permutation\n    shuffled_chunk_indexes = [\n        chunk_indexes[chunk_index] for chunk_indexes_per_node in chunk_indexes for chunk_index in chunk_indexes_per_node\n    ]\n\n    return shuffled_chunk_indexes"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Generate random numbers for each rank\n    rng = np.random.default_rng(seed + current_epoch)\n    random_numbers = rng.random(size=len(chunks_per_ranks))\n\n    # 2. Sort the chunks based on the random numbers\n    sorted_chunks = [\n        chunk for _, chunk in sorted(zip(random_numbers, chunks_per_ranks), key=lambda x: x[0])\n    ]\n\n    # 3. Flatten the sorted chunks and return\n    return [chunk for chunks in sorted_chunks for chunk in chunks]"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Generate a random permutation of chunk indexes\n    chunk_indexes = np.arange(len(chunks_per_ranks))\n    permutation = np.random.RandomState(seed + current_epoch).permutation(chunk_indexes)\n\n    # 2. Shuffle the chunks per rank based on the permutation\n    shuffled_chunks_per_ranks = [\n        [chunks_per_ranks[i] for i in permutation] for chunks_per_ranks in chunks_per_ranks\n    ]\n\n    # 3. Flatten the shuffled chunks per rank into a single list\n    flattened_shuffled_chunks = [\n        chunk for rank_chunks in shuffled_chunks_per_ranks for chunk in rank_chunks\n    ]\n\n    return flattened_shuffled_chunks"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Get the chunk indexes assigned to each rank\n    chunk_indexes = [\n        chunk_index\n        for chunks in chunks_per_ranks\n        for chunk_index in chunks\n    ]\n\n    # 2. Shuffle the chunk indexes based on the provided seed and current epoch\n    np.random.seed(seed + current_epoch)\n    np.random.shuffle(chunk_indexes)\n\n    # 3. Distribute the shuffled chunk indexes across all nodes\n    chunk_indexes_per_node = np.array_split(chunk_indexes, distributed_env.world_size)\n\n    # 4. Flatten the list of shuffled chunk indexes\n    shuffled_chunk_indexes = [\n        chunk_index\n        for chunk_indexes in chunk_indexes_per_node\n        for chunk_index in chunk_indexes\n    ]\n\n    return shuffled_chunk_indexes"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Generate a random permutation of chunk indexes for each node\n    chunk_indexes_per_rank = [\n        np.random.RandomState(seed + current_epoch + rank).permutation(chunks_per_rank)\n        for rank, chunks_per_rank in enumerate(chunks_per_ranks)\n    ]\n\n    # 2. Concatenate the chunk indexes from all nodes\n    flattened_chunk_indexes = np.concatenate(chunk_indexes_per_rank)\n\n    # 3. Shuffle the flattened chunk indexes using the same seed and epoch\n    np.random.RandomState(seed + current_epoch).shuffle(flattened_chunk_indexes)\n\n    return flattened_chunk_indexes"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Calculate the total number of chunks across all nodes\n    total_chunks = sum([len(chunks) for chunks in chunks_per_ranks])\n\n    # 2. Generate a random permutation of chunk indexes based on the provided seed and epoch\n    chunk_indexes = np.random.RandomState(seed + current_epoch).permutation(total_chunks)\n\n    # 3. Assign the shuffled chunk indexes to each node\n    shuffled_chunk_indexes: List[int] = []\n    index = 0\n    for chunks in chunks_per_ranks:\n        shuffled_chunk_indexes.extend(chunk_indexes[index : index + len(chunks)])\n        index += len(chunks)\n\n    # 4. Return the flattened list of shuffled chunk indexes\n    return shuffled_chunk_indexes"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Generate a random permutation of chunk indexes for each rank\n    np.random.seed(seed + current_epoch)\n    chunk_indexes_per_ranks: List[List[int]] = [\n        np.random.permutation(chunks_per_rank).tolist() for chunks_per_rank in chunks_per_ranks\n    ]\n\n    # 2. Flatten the list of lists into a single list\n    flattened_chunk_indexes = [\n        chunk_index for chunk_indexes in chunk_indexes_per_ranks for chunk_index in chunk_indexes\n    ]\n\n    # 3. Shuffle the flattened list using the same random seed and epoch\n    np.random.seed(seed + current_epoch)\n    np.random.shuffle(flattened_chunk_indexes)\n\n    return flattened_chunk_indexes"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Create a random permutation of chunk indexes for each rank\n    rank = distributed_env.rank\n    rng = np.random.default_rng(seed + current_epoch)\n    shuffled_chunk_indexes = rng.permutation(chunks_per_ranks[rank])\n\n    # 2. Shuffle the chunk indexes across all nodes\n    shuffled_chunk_indexes = distributed_env.all_gather(shuffled_chunk_indexes)\n\n    # 3. Flatten the shuffled chunk indexes\n    flattened_shuffled_chunk_indexes = [\n        chunk_index for rank_shuffled_chunk_indexes in shuffled_chunk_indexes for chunk_index in rank_shuffled_chunk_indexes\n    ]\n\n    return flattened_shuffled_chunk_indexes"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Get the node rank\n    rank = distributed_env.node_rank\n\n    # 2. Get the number of chunks assigned to the current node\n    num_chunks = len(chunks_per_ranks[rank])\n\n    # 3. Generate a random permutation of chunk indexes using the provided seed and current epoch\n    np.random.seed(seed + current_epoch)\n    permutation = np.random.permutation(num_chunks)\n\n    # 4. Reorder the chunk indexes based on the permutation\n    shuffled_chunks_per_ranks = [\n        [chunks_per_ranks[rank][i] for i in permutation] for rank in range(distributed_env.world_size)\n    ]\n\n    # 5. Flatten the shuffled chunk indexes and return the result\n    return [chunk for sublist in shuffled_chunks_per_ranks for chunk in sublist]"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Calculate the number of items per rank\n    num_items_per_ranks: List[int] = [\n        sum([(interval[-1] - interval[0]) for interval in chunks_per_ranks[rank]])\n        for rank in range(distributed_env.world_size)\n    ]\n\n    # 2. Calculate the seed for each rank\n    seeds_per_ranks: List[int] = [\n        seed + rank + current_epoch * distributed_env.world_size\n        for rank in range(distributed_env.world_size)\n    ]\n\n    # 3. Shuffle the chunk indexes for each rank\n    shuffled_chunks_per_ranks: List[List[int]] = [\n        np.random.RandomState(seeds_per_ranks[rank]).permutation(\n            np.array(chunks_per_ranks[rank]).flatten()\n        ).tolist()\n        for rank in range(distributed_env.world_size)\n    ]\n\n    # 4. Flatten the shuffled chunk indexes and return the result\n    return [\n        shuffled_chunks_per_ranks[rank][i]\n        for rank in range(distributed_env.world_size)\n        for i in range(num_items_per_ranks[rank])\n    ]\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Get the number of chunks per rank\n    num_chunks_per_rank = [len(chunks) for chunks in chunks_per_ranks]\n\n    # 2. Compute the total number of chunks\n    total_num_chunks = sum(num_chunks_per_rank)\n\n    # 3. Generate a random permutation of chunk indexes based on the seed and epoch\n    rng = np.random.default_rng(seed + current_epoch)\n    permuted_chunk_indexes = rng.permutation(total_num_chunks)\n\n    # 4. Re-arrange the permuted chunk indexes to match the original chunk indexes per rank\n    shuffled_chunks_per_rank = [[] for _ in range(distributed_env.world_size)]\n    current_index = 0\n    for rank, num_chunks in enumerate(num_chunks_per_rank):\n        shuffled_chunks_per_rank[rank] = permuted_chunk_indexes[\n            current_index : current_index + num_chunks\n        ].tolist()\n        current_index += num_chunks\n\n    # 5. Flatten the shuffled chunks per rank to get a single list of shuffled chunk indexes\n    shuffled_chunk_indexes = [\n        chunk_index for rank_chunks in shuffled_chunks_per_rank for chunk_index in rank_chunks\n    ]\n\n    return shuffled_chunk_indexes"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    num_chunks_per_node = [len(chunks) for chunks in chunks_per_ranks]\n    num_chunks_per_node_per_rank = [\n        [len(chunks) for chunks in chunks_per_ranks] for _ in range(distributed_env.world_size)\n    ]\n\n    # 1. Generate a random permutation for each rank\n    rng = np.random.default_rng(seed + current_epoch)\n    permutations = [\n        rng.permutation(num_chunks_per_node) for _ in range(distributed_env.world_size)\n    ]\n\n    # 2. Create a list of permutations for each rank\n    permutations_per_rank: List[List[int]] = [[] for _ in range(distributed_env.world_size)]\n\n    for rank in range(distributed_env.world_size):\n        for chunk_index in permutations[rank]:\n            permutations_per_rank[rank].extend(\n                [chunk_index] * num_chunks_per_node_per_rank[rank][chunk_index]\n            )\n\n    # 3. Flatten the list of permutations for each rank\n    flattened_permutations = [\n        permutation\n        for rank_permutations in permutations_per_rank\n        for permutation in rank_permutations\n    ]\n\n    return flattened_permutations"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) == 1:\n        return os.path.dirname(list(indexed_paths.values())[0])\n\n    if len(indexed_paths) == 2:\n        first_path = list(indexed_paths.values())[0]\n        second_path = list(indexed_paths.values())[1]\n\n        if os.path.dirname(first_path) == os.path.dirname(second_path):\n            return os.path.dirname(first_path)\n\n    raise ValueError(\n        f\"The provided inputs should contain at least a valid file path. Found {indexed_paths.values()}.\"\n    )"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) == 1:\n        return os.path.dirname(list(indexed_paths.values())[0])\n\n    if len(indexed_paths) == 2:\n        return os.path.dirname(os.path.commonpath(list(indexed_paths.values())))\n\n    raise ValueError(f\"The provided inputs should contain at least a valid filepath. Found {inputs}.\")"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) == 1:\n        return os.path.dirname(list(indexed_paths.values())[0])\n\n    first_path = list(indexed_paths.values())[0]\n    second_path = list(indexed_paths.values())[1]\n\n    if first_path == second_path:\n        return first_path\n\n    if os.path.dirname(first_path) == os.path.dirname(second_path):\n        return os.path.dirname(first_path)\n\n    raise ValueError(\n        f\"The provided inputs should have consistent file paths. \"\n        f\"Found {first_path} and {second_path}.\"\n    )"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) > 2:\n        raise ValueError(\"The provided inputs should contain at most 2 valid file paths.\")\n\n    if len(indexed_paths) == 1:\n        return os.path.dirname(list(indexed_paths.values())[0])\n\n    if indexed_paths[0] != indexed_paths[1]:\n        raise ValueError(\"The provided inputs should contain the same file path.\")\n\n    return os.path.dirname(indexed_paths[0])"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) > 2:\n        raise ValueError(f\"The provided inputs should be at most two file paths. Found {inputs}.\")\n\n    if len(indexed_paths) == 2:\n        if indexed_paths[0] != indexed_paths[1]:\n            raise ValueError(f\"The provided inputs should be the same file path. Found {inputs}.\")\n\n    input_dir = os.path.dirname(indexed_paths[0])\n\n    if input_dir == \".\":\n        return None\n\n    return input_dir"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) == 1:\n        return Path(list(indexed_paths.values())[0]).parent.absolute()\n\n    first_file_path = list(indexed_paths.values())[0]\n    second_file_path = list(indexed_paths.values())[1]\n\n    if first_file_path != second_file_path:\n        raise ValueError(\n            f\"The provided inputs should be valid file paths. Found {first_file_path} and {second_file_path}.\"\n        )\n\n    return Path(first_file_path).parent.absolute()"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) == 1:\n        return Path(list(indexed_paths.values())[0]).parent.absolute()\n\n    if len(indexed_paths) > 1:\n        first_path = Path(list(indexed_paths.values())[0]).parent.absolute()\n        second_path = Path(list(indexed_paths.values())[1]).parent.absolute()\n\n        if first_path != second_path:\n            raise ValueError(\n                \"The provided inputs are inconsistent. Found two different paths: \"\n                f\"{first_path} and {second_path}.\"\n            )\n\n        return first_path"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) == 1:\n        return str(Path(next(iter(indexed_paths.values()))).parent)\n\n    if len(indexed_paths) > 1:\n        first_file_path = next(iter(indexed_paths.values()))\n        second_file_path = next(iter(indexed_paths.values()))\n\n        if first_file_path != second_file_path:\n            raise ValueError(\n                f\"The provided inputs contain inconsistent file paths. Found {first_file_path} and {second_file_path}.\"\n            )\n\n    return str(Path(first_file_path).parent)"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) == 1:\n        return os.path.dirname(list(indexed_paths.values())[0])\n\n    if len(indexed_paths) > 1:\n        raise ValueError(\n            f\"The provided inputs should be non empty sequence of filepaths. Found {inputs}.\"\n            f\" HINT: The provided inputs should be non empty sequence of filepaths. Found {inputs}.\"\n        )"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) == 1:\n        return indexed_paths[0]\n\n    if len(indexed_paths) == 2:\n        return os.path.dirname(os.path.commonpath(list(indexed_paths.values())))\n\n    raise ValueError(\n        f\"The provided inputs should contain at most two non empty file paths. Found {indexed_paths}.\"\n    )"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) == 1:\n        return os.path.dirname(next(iter(indexed_paths.values())))\n\n    input_dir = None\n\n    for index, path in indexed_paths.items():\n        if index == 0:\n            input_dir = os.path.dirname(path)\n            continue\n\n        if input_dir != os.path.dirname(path):\n            raise ValueError(f\"The provided inputs should be from the same directory. Found {input_dir} and {path}.\")\n\n    return input_dir"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) == 1:\n        return os.path.dirname(list(indexed_paths.values())[0])\n\n    if len(indexed_paths) == 2:\n        if indexed_paths[0] == indexed_paths[1]:\n            return os.path.dirname(indexed_paths[0])\n\n    raise ValueError(f\"The provided inputs should contain at least two different files. Found {inputs}.\")"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) > 2:\n        raise ValueError(\n            f\"The provided inputs should contain at most 2 valid file paths. Found {len(indexed_paths)}.\"\n        )\n\n    if len(indexed_paths) == 2:\n        if not os.path.dirname(indexed_paths[0]) == os.path.dirname(indexed_paths[1]):\n            raise ValueError(\n                f\"The provided inputs should have the same directory. \"\n                f\"Found {os.path.dirname(indexed_paths[0])} and {os.path.dirname(indexed_paths[1])}.\"\n            )\n\n    return os.path.dirname(indexed_paths[0])"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) > 2:\n        raise ValueError(\n            f\"The provided inputs should contain at most two file paths. Found {len(indexed_paths)}.\"\n        )\n\n    if len(indexed_paths) == 1:\n        return os.path.dirname(list(indexed_paths.values())[0])\n\n    file_path_1, file_path_2 = list(indexed_paths.values())\n\n    if not os.path.exists(file_path_1) or not os.path.exists(file_path_2):\n        raise FileNotFoundError(f\"The provided inputs should contain valid file paths. Found {inputs}.\")\n\n    if os.path.dirname(file_path_1) != os.path.dirname(file_path_2):\n        raise ValueError(\n            f\"The provided inputs should contain file paths that share the same directory. Found {inputs}.\"\n        )\n\n    return os.path.dirname(file_path_1)"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if not inputs:\n        return None\n\n    indexed_paths = _get_indexed_paths(inputs)\n\n    if not indexed_paths:\n        return None\n\n    if len(indexed_paths) == 1:\n        return str(Path(indexed_paths[0]).parent.absolute())\n\n    if len(indexed_paths) > 1:\n        for index, path in indexed_paths.items():\n            if index > 0 and indexed_paths[index - 1] != path:\n                raise ValueError(\n                    f\"The provided inputs should contain consistent file paths. \"\n                    f\"Found {indexed_paths[index - 1]} and {path}.\"\n                )\n\n    return str(Path(indexed_paths[0]).parent.absolute())"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) == 1:\n        return str(Path(list(indexed_paths.values())[0]).parent)\n\n    if len(indexed_paths) == 2:\n        return str(Path(list(indexed_paths.values())[0]).parent)\n\n    raise ValueError(\n        f\"The provided inputs should contain at most two file paths. Found {len(indexed_paths)} paths.\"\n        \" HINT: You can either provide a single file path or a sequence of file paths.\"\n    )"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if len(inputs) < 2:\n        raise ValueError(f\"The provided inputs should be at least 2. Found {inputs}.\")\n\n    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        raise ValueError(f\"The provided inputs should contain at least 2 valid file paths. Found {inputs}.\")\n\n    if len(indexed_paths) == 1:\n        return str(Path(list(indexed_paths.values())[0]).parent)\n\n    first_index, first_path = list(indexed_paths.items())[0]\n    second_index, second_path = list(indexed_paths.items())[1]\n\n    if first_index > second_index:\n        first_index, second_index = second_index, first_index\n        first_path, second_path = second_path, first_path\n\n    if first_path != second_path:\n        raise ValueError(\n            f\"The provided inputs should contain consistent file paths. \"\n            f\"Found {first_path} and {second_path} at index {first_index} and {second_index} respectively.\"\n        )\n\n    return str(Path(first_path).parent)"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) > 2:\n        raise ValueError(\n            f\"The provided inputs should contain at most 2 file paths. Found {indexed_paths}.\"\n            \" HINT: You can either provide a single file path or a list of file paths.\"\n        )\n\n    if len(indexed_paths) == 1:\n        return os.path.dirname(list(indexed_paths.values())[0])\n\n    first_path = list(indexed_paths.values())[0]\n    second_path = list(indexed_paths.values())[1]\n\n    if not os.path.isdir(first_path):\n        raise ValueError(f\"The provided input {first_path} isn't a valid directory.\")\n\n    if not os.path.isdir(second_path):\n        raise ValueError(f\"The provided input {second_path} isn't a valid directory.\")\n\n    if not os.path.commonpath([first_path, second_path]):\n        raise ValueError(\n            f\"The provided inputs {first_path} and {second_path} are not in the same directory.\"\n            \" HINT: You can either provide a single file path or a list of file paths.\"\n        )\n\n    return first_path"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if not isinstance(inputs, Sequence):\n        raise ValueError(f\"The provided inputs should be a sequence. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    indexed_paths = _get_indexed_paths(inputs)\n    if len(indexed_paths) == 0:\n        raise ValueError(\"The provided inputs should contain at least a valid file path.\")\n\n    if len(indexed_paths) > 2:\n        raise ValueError(\n            \"The provided inputs should contain at most two valid file paths. \"\n            f\"Found {len(indexed_paths)} valid file paths.\"\n        )\n\n    if len(indexed_paths) == 1:\n        return indexed_paths[0]\n\n    if not Path(indexed_paths[0]).is_absolute():\n        raise ValueError(f\"The provided path {indexed_paths[0]} isn't absolute.\")\n\n    if not Path(indexed_paths[1]).is_absolute():\n        raise ValueError(f\"The provided path {indexed_paths[1]} isn't absolute.\")\n\n    if not Path(indexed_paths[0]).samefile(Path(indexed_paths[1])):\n        raise ValueError(\n            \"The provided inputs should contain the same file path. \"\n            f\"Found {indexed_paths[0]} and {indexed_paths[1]}.\"\n        )\n\n    return indexed_paths[0]"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if len(inputs) == 0:\n        return None\n\n    if len(inputs) == 1:\n        if isinstance(inputs[0], str):\n            return os.path.dirname(inputs[0])\n        if isinstance(inputs[0], Dir):\n            return inputs[0].path\n        if isinstance(inputs[0], Path):\n            return str(inputs[0].parent)\n        if isinstance(inputs[0], list):\n            return _get_input_dir(inputs[0])\n        if isinstance(inputs[0], tuple):\n            return _get_input_dir(inputs[0])\n\n    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        raise ValueError(\n            f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\"\n        )\n\n    if len(indexed_paths) == 1:\n        return os.path.dirname(list(indexed_paths.values())[0])\n\n    if len(indexed_paths) > 1:\n        first_path = list(indexed_paths.values())[0]\n        for path in list(indexed_paths.values())[1:]:\n            if not os.path.commonpath([first_path, path]):\n                raise ValueError(\n                    f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\"\n                )\n        return os.path.commonpath(list(indexed_paths.values()))\n\n    return None"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if not _IS_IN_STUDIO:\n        yield\n        return\n\n    if enable:\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns(True)'\"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501\n\n    try:\n        yield\n    finally:\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns(False)'\"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if not _IS_IN_STUDIO:\n        yield\n        return\n\n    if enable:\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns(True)'\"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501\n\n    try:\n        yield\n    finally:\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns(False)'\"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if not _IS_IN_STUDIO:\n        yield\n        return\n\n    if enable:\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns(True)'\"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501\n\n    try:\n        yield\n    finally:\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns(False)'\"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if not _IS_IN_STUDIO:\n        yield\n        return\n\n    if enable:\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns(True)'\"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501\n\n    try:\n        yield\n    finally:\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns(False)'\"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if not _IS_IN_STUDIO:\n        yield\n        return\n\n    if enable:\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns(True)'\"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501\n\n    try:\n        yield\n    finally:\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns(False)'\"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if not _IS_IN_STUDIO:\n        yield\n        return\n\n    if enable:\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns(True)'\"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501\n\n    try:\n        yield\n    finally:\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns(False)'\"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if not _IS_IN_STUDIO:\n        yield\n        return\n\n    if enable:\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns(True)'\"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501\n\n    try:\n        yield\n    finally:\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns(False)'\"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if not _IS_IN_STUDIO:\n        yield\n        return\n\n    with open(\"/etc/resolv.conf\") as f:\n        lines = f.readlines()\n\n    write_lines = []\n    for line in lines:\n        if \"nameserver 127\" in line:\n            if enable:\n                write_lines.append(\"nameserver 127.0.0.1\\n\")\n            else:\n                write_lines.append(\"nameserver 127.0.0.53\\n\")\n        else:\n            write_lines.append(line)\n\n    with open(\"/etc/resolv.conf\", \"w\") as f:\n        for line in write_lines:\n            f.write(line)\n\n    try:\n        yield\n    finally:\n        with open(\"/etc/resolv.conf\") as f:\n            lines = f.readlines()\n\n        write_lines = []\n        for line in lines:\n            if \"nameserver 127\" in line:\n                if enable:\n                    write_lines.append(\"nameserver 127.0.0.53\\n\")\n                else:\n                    write_lines.append(\"nameserver 127.0.0.1\\n\")\n            else:\n                write_lines.append(line)\n\n        with open(\"/etc/resolv.conf\", \"w\") as f:\n            for line in write_lines:\n                f.write(line)"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if not _IS_IN_STUDIO:\n        yield\n        return\n\n    with open(\"/etc/resolv.conf\") as f:\n        lines = f.readlines()\n\n    write_lines = []\n    for line in lines:\n        if \"nameserver 127\" in line:\n            if enable:\n                write_lines.append(\"nameserver 127.0.0.1\\n\")\n            else:\n                write_lines.append(\"nameserver 127.0.0.53\\n\")\n        else:\n            write_lines.append(line)\n\n    with open(\"/etc/resolv.conf\", \"w\") as f:\n        for line in write_lines:\n            f.write(line)\n\n    try:\n        yield\n    finally:\n        with open(\"/etc/resolv.conf\") as f:\n            lines = f.readlines()\n\n        write_lines = []\n        for line in lines:\n            if \"nameserver 127\" in line:\n                write_lines.append(\"nameserver 127.0.0.53\\n\")\n            else:\n                write_lines.append(line)\n\n        with open(\"/etc/resolv.conf\", \"w\") as f:\n            for line in write_lines:\n                f.write(line)"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if not _IS_IN_STUDIO:\n        yield\n        return\n\n    with open(\"/etc/resolv.conf\") as f:\n        lines = f.readlines()\n\n    if (enable and any(\"127.0.0.53\" in line for line in lines)) or (\n        not enable and any(\"127.0.0.1\" in line for line in lines)\n    ):\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns({enable})'\"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501\n\n    try:\n        yield\n    finally:\n        with open(\"/etc/resolv.conf\") as f:\n            lines = f.readlines()\n\n        write_lines = []\n        for line in lines:\n            if \"nameserver 127\" in line:\n                if enable:\n                    write_lines.append(\"nameserver 127.0.0.1\\n\")\n                else:\n                    write_lines.append(\"nameserver 127.0.0.53\\n\")\n            else:\n                write_lines.append(line)\n\n        with open(\"/etc/resolv.conf\", \"w\") as f:\n            for line in write_lines:\n                f.write(line)"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if not _IS_IN_STUDIO:\n        yield\n        return\n\n    with open(\"/etc/resolv.conf\") as f:\n        lines = f.readlines()\n\n    if (enable and any(\"127.0.0.53\" in line for line in lines)) or (\n        not enable and any(\"127.0.0.1\" in line for line in lines)\n    ):\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns({enable})'\"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501\n\n    try:\n        yield\n    finally:\n        with open(\"/etc/resolv.conf\") as f:\n            lines = f.readlines()\n\n        write_lines = []\n        for line in lines:\n            if \"nameserver 127\" in line:\n                if enable:\n                    write_lines.append(\"nameserver 127.0.0.1\\n\")\n                else:\n                    write_lines.append(\"nameserver 127.0.0.53\\n\")\n            else:\n                write_lines.append(line)\n\n        with open(\"/etc/resolv.conf\", \"w\") as f:\n            for line in write_lines:\n                f.write(line)"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if not _IS_IN_STUDIO:\n        yield\n        return\n\n    with open(\"/etc/resolv.conf\") as f:\n        lines = f.readlines()\n\n    if (enable and any(\"127.0.0.53\" in line for line in lines)) or (\n        not enable and any(\"127.0.0.1\" in line for line in lines)\n    ):\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns({enable})'\"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501\n\n    try:\n        yield\n    finally:\n        with open(\"/etc/resolv.conf\") as f:\n            lines = f.readlines()\n\n        write_lines = []\n        for line in lines:\n            if \"nameserver 127\" in line:\n                if enable:\n                    write_lines.append(\"nameserver 127.0.0.1\\n\")\n                else:\n                    write_lines.append(\"nameserver 127.0.0.53\\n\")\n            else:\n                write_lines.append(line)\n\n        with open(\"/etc/resolv.conf\", \"w\") as f:\n            for line in write_lines:\n                f.write(line)"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if not _IS_IN_STUDIO:\n        yield\n        return\n\n    with open(\"/etc/resolv.conf\") as f:\n        lines = f.readlines()\n\n    if (enable and any(\"127.0.0.53\" in line for line in lines)) or (\n        not enable and any(\"127.0.0.1\" in line for line in lines)\n    ):\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns({enable})'\"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501\n\n    try:\n        yield\n    finally:\n        with open(\"/etc/resolv.conf\") as f:\n            lines = f.readlines()\n\n        write_lines = []\n        for line in lines:\n            if \"nameserver 127\" in line:\n                if enable:\n                    write_lines.append(\"nameserver 127.0.0.1\\n\")\n                else:\n                    write_lines.append(\"nameserver 127.0.0.53\\n\")\n            else:\n                write_lines.append(line)\n\n        with open(\"/etc/resolv.conf\", \"w\") as f:\n            for line in write_lines:\n                f.write(line)"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        optimize_dns(enable)\n        yield\n    finally:\n        optimize_dns(False)"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if not _IS_IN_STUDIO:\n        yield\n        return\n\n    with open(\"/etc/resolv.conf\") as f:\n        lines = f.readlines()\n\n    if (enable and any(\"127.0.0.53\" in line for line in lines)) or (\n        not enable and any(\"127.0.0.1\" in line for line in lines)\n    ):\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns({enable})'\"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501\n\n    try:\n        yield\n    finally:\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns(False)' \"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        optimize_dns(enable)\n        yield\n    finally:\n        optimize_dns(False)"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if not _IS_IN_STUDIO:\n        yield\n        return\n\n    with open(\"/etc/resolv.conf\") as f:\n        lines = f.readlines()\n\n    if (enable and any(\"127.0.0.53\" in line for line in lines)) or (\n        not enable and any(\"127.0.0.1\" in line for line in lines)\n    ):\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns({enable})'\"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501\n\n    try:\n        yield\n    finally:\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns(False)' \"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if not _IS_IN_STUDIO:\n        yield\n        return\n\n    if enable:\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns(True)' \"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501\n    try:\n        yield\n    finally:\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns(False)' \"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if not _IS_IN_STUDIO:\n        yield\n        return\n\n    try:\n        if enable:\n            cmd = (\n                f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n                f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns({enable})'\"\n            )\n            Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501\n        yield\n    finally:\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns(False)' \"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if not _IS_IN_STUDIO:\n        yield\n        return\n\n    if enable:\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns(True)'\"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501\n    else:\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns(False)'\"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501\n\n    yield\n\n    if enable:\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns(False)'\"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501\n    else:\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns(True)'\"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    \"\"\"\n    This function distributes chunks and their corresponding intervals across different ranks in a distributed environment. It calculates the number of items each rank should process based on the total items and the world size of the distributed environment, taking into account whether to drop the last items or not. Then, it assigns chunks and their intervals to each rank accordingly.\n\n    Input-Output Arguments\n    :param distributed_env: _DistributedEnv. The distributed environment configuration, including the world size.\n    :param indexes: Any. A list or array of chunk indexes that need to be distributed among the ranks.\n    :param chunk_intervals: Any. A list or array of tuples, where each tuple represents the start and end of a chunk interval.\n    :param drop_last: Bool. A flag indicating whether to drop the last items to make the distribution even across all ranks.\n    :return: Tuple[List[List[int]], List[Any]]. A tuple containing two elements. The first element is a list of lists, where each sublist contains the chunk indexes assigned to each rank. The second element is a list of lists of lists, where each sublist contains the intervals of chunks assigned to each rank.\n    \"\"\"\n\n    \"\"\"\n    This function distributes chunks and their corresponding intervals across different ranks in a distributed environment. It calculates the number of items each rank should process based on the total items and the world size of the distributed environment, taking into account whether to drop the last items or not. Then, it assigns chunks and their intervals to each rank accordingly.\n\n    Input-Output Arguments\n    :param distributed_env: _DistributedEnv. The distributed environment configuration, including the world size.\n    :param indexes: Any. A list or array of chunk indexes that need to be distributed among the ranks.\n    :param chunk_intervals: Any. A list or array of tuples, where each tuple represents the start and end of a chunk interval.\n    :param drop_last: Bool. A flag indicating whether to drop the last items to make the distribution even across all ranks.\n    :return: Tuple[List[List[int]], List[Any]]. A tuple containing two elements. The first element is a list of lists, where each"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    \"\"\"\n    This function distributes chunks and their corresponding intervals across different ranks in a distributed environment. It calculates the number of items each rank should process based on the total items and the world size of the distributed environment, taking into account whether to drop the last items or not. Then, it assigns chunks and their intervals to each rank accordingly.\n\n    Input-Output Arguments\n    :param distributed_env: _DistributedEnv. The distributed environment configuration, including the world size.\n    :param indexes: Any. A list or array of chunk indexes that need to be distributed among the ranks.\n    :param chunk_intervals: Any. A list or array of tuples, where each tuple represents the start and end of a chunk interval.\n    :param drop_last: Bool. A flag indicating whether to drop the last items to make the distribution even across all ranks.\n    :return: Tuple[List[List[int]], List[Any]]. A tuple containing two elements. The first element is a list of lists, where each sublist contains the chunk indexes assigned to each rank. The second element is a list of lists of lists, where each sublist contains the intervals of chunks assigned to each rank.\n    \"\"\"\n\n    \"\"\"\n    This function distributes chunks and their corresponding intervals across different ranks in a distributed environment. It calculates the number of items each rank should process based on the total items and the world size of the distributed environment, taking into account whether to drop the last items or not. Then, it assigns chunks and their intervals to each rank accordingly.\n\n    Input-Output Arguments\n    :param distributed_env: _DistributedEnv. The distributed environment configuration, including the world size.\n    :param indexes: Any. A list or array of chunk indexes that need to be distributed among the ranks.\n    :param chunk_intervals: Any. A list or array of tuples, where each tuple represents the start and end of a chunk interval.\n    :param drop_last: Bool. A flag indicating whether to drop the last items to make the distribution even across all ranks.\n    :return: Tuple[List[List[int]], List[Any]]. A tuple containing two elements. The first element is a list of lists, where each"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    \"\"\"\n    This function distributes chunks and their corresponding intervals across different ranks in a distributed environment. It calculates the number of items each rank should process based on the total items and the world size of the distributed environment, taking into account whether to drop the last items or not. Then, it assigns chunks and their intervals to each rank accordingly.\n\n    Input-Output Arguments\n    :param distributed_env: _DistributedEnv. The distributed environment configuration, including the world size.\n    :param indexes: Any. A list or array of chunk indexes that need to be distributed among the ranks.\n    :param chunk_intervals: Any. A list or array of tuples, where each tuple represents the start and end of a chunk interval.\n    :param drop_last: Bool. A flag indicating whether to drop the last items to make the distribution even across all ranks.\n    :return: Tuple[List[List[int]], List[Any]]. A tuple containing two elements. The first element is a list of lists, where each sublist contains the chunk indexes assigned to each rank. The second element is a list of lists of lists, where each sublist contains the intervals of chunks assigned to each rank.\n    \"\"\"\n\n    \"\"\"\n    This function distributes chunks and their corresponding intervals across different ranks in a distributed environment. It calculates the number of items each rank should process based on the total items and the world size of the distributed environment, taking into account whether to drop the last items or not. Then, it assigns chunks and their intervals to each rank accordingly.\n\n    Input-Output Arguments\n    :param distributed_env: _DistributedEnv. The distributed environment configuration, including the world size.\n    :param indexes: Any. A list or array of chunk indexes that need to be distributed among the ranks.\n    :param chunk_intervals: Any. A list or array of tuples, where each tuple represents the start and end of a chunk interval.\n    :param drop_last: Bool. A flag indicating whether to drop the last items to make the distribution even across all ranks.\n    :return: Tuple[List[List[int]], List[Any]]. A tuple containing two elements. The first element is a list of lists, where each"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    \"\"\"\n    This function distributes chunks and their corresponding intervals across different ranks in a distributed environment. It calculates the number of items each rank should process based on the total items and the world size of the distributed environment, taking into account whether to drop the last items or not. Then, it assigns chunks and their intervals to each rank accordingly.\n\n    Input-Output Arguments\n    :param distributed_env: _DistributedEnv. The distributed environment configuration, including the world size.\n    :param indexes: Any. A list or array of chunk indexes that need to be distributed among the ranks.\n    :param chunk_intervals: Any. A list or array of tuples, where each tuple represents the start and end of a chunk interval.\n    :param drop_last: Bool. A flag indicating whether to drop the last items to make the distribution even across all ranks.\n    :return: Tuple[List[List[int]], List[Any]]. A tuple containing two elements. The first element is a list of lists, where each sublist contains the chunk indexes assigned to each rank. The second element is a list of lists of lists, where each sublist contains the intervals of chunks assigned to each rank.\n    \"\"\"\n\n    \"\"\"\n    This function distributes chunks and their corresponding intervals across different ranks in a distributed environment. It calculates the number of items each rank should process based on the total items and the world size of the distributed environment, taking into account whether to drop the last items or not. Then, it assigns chunks and their intervals to each rank accordingly.\n\n    Input-Output Arguments\n    :param distributed_env: _DistributedEnv. The distributed environment configuration, including the world size.\n    :param indexes: Any. A list or array of chunk indexes that need to be distributed among the ranks.\n    :param chunk_intervals: Any. A list or array of tuples, where each tuple represents the start and end of a chunk interval.\n    :param drop_last: Bool. A flag indicating whether to drop the last items to make the distribution even across all ranks.\n    :return: Tuple[List[List[int]], List[Any]]. A tuple containing two elements. The first element is a list of lists, where each"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    \"\"\"\n    This function distributes chunks and their corresponding intervals across different ranks in a distributed environment. It calculates the number of items each rank should process based on the total items and the world size of the distributed environment, taking into account whether to drop the last items or not. Then, it assigns chunks and their intervals to each rank accordingly.\n\n    Input-Output Arguments\n    :param distributed_env: _DistributedEnv. The distributed environment configuration, including the world size.\n    :param indexes: Any. A list or array of chunk indexes that need to be distributed among the ranks.\n    :param chunk_intervals: Any. A list or array of tuples, where each tuple represents the start and end of a chunk interval.\n    :param drop_last: Bool. A flag indicating whether to drop the last items to make the distribution even across all ranks.\n    :return: Tuple[List[List[int]], List[Any]]. A tuple containing two elements. The first element is a list of lists, where each sublist contains the chunk indexes assigned to each rank. The second element is a list of lists of lists, where each sublist contains the intervals of chunks assigned to each rank.\n    \"\"\"\n\n    \"\"\"\n    This function distributes chunks and their corresponding intervals across different ranks in a distributed environment. It calculates the number of items each rank should process based on the total items and the world size of the distributed environment, taking into account whether to drop the last items or not. Then, it assigns chunks and their intervals to each rank accordingly.\n\n    Input-Output Arguments\n    :param distributed_env: _DistributedEnv. The distributed environment configuration, including the world size.\n    :param indexes: Any. A list or array of chunk indexes that need to be distributed among the ranks.\n    :param chunk_intervals: Any. A list or array of tuples, where each tuple represents the start and end of a chunk interval.\n    :param drop_last: Bool. A flag indicating whether to drop the last items to make the distribution even across all ranks.\n    :return: Tuple[List[List[int]], List[Any]]. A tuple containing two elements. The first element is a list of lists, where each"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    \"\"\"\n    This function distributes chunks and their corresponding intervals across different ranks in a distributed environment. It calculates the number of items each rank should process based on the total items and the world size of the distributed environment, taking into account whether to drop the last items or not. Then, it assigns chunks and their intervals to each rank accordingly.\n\n    Input-Output Arguments\n    :param distributed_env: _DistributedEnv. The distributed environment configuration, including the world size.\n    :param indexes: Any. A list or array of chunk indexes that need to be distributed among the ranks.\n    :param chunk_intervals: Any. A list or array of tuples, where each tuple represents the start and end of a chunk interval.\n    :param drop_last: Bool. A flag indicating whether to drop the last items to make the distribution even across all ranks.\n    :return: Tuple[List[List[int]], List[Any]]. A tuple containing two elements. The first element is a list of lists, where each sublist contains the chunk indexes assigned to each rank. The second element is a list of lists of lists, where each sublist contains the intervals of chunks assigned to each rank.\n    \"\"\"\n\n    \"\"\"\n    This function distributes chunks and their corresponding intervals across different ranks in a distributed environment. It calculates the number of items each rank should process based on the total items and the world size of the distributed environment, taking into account whether to drop the last items or not. Then, it assigns chunks and their intervals to each rank accordingly.\n\n    Input-Output Arguments\n    :param distributed_env: _DistributedEnv. The distributed environment configuration, including the world size.\n    :param indexes: Any. A list or array of chunk indexes that need to be distributed among the ranks.\n    :param chunk_intervals: Any. A list or array of tuples, where each tuple represents the start and end of a chunk interval.\n    :param drop_last: Bool. A flag indicating whether to drop the last items to make the distribution even across all ranks.\n    :return: Tuple[List[List[int]], List[Any]]. A tuple containing two elements. The first element is a list of lists, where each"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    "}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    "}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    \"\"\"\n    This function distributes chunks and their corresponding intervals across different ranks in a distributed environment. It calculates the number of items each rank should process based on the total items and the world size of the distributed environment, taking into account whether to drop the last items or not. Then, it assigns chunks and their intervals to each rank accordingly.\n\n    Input-Output Arguments\n    :param distributed_env: _DistributedEnv. The distributed environment configuration, including the world size.\n    :param indexes: Any. A list or array of chunk indexes that need to be distributed among the ranks.\n    :param chunk_intervals: Any. A list or array of tuples, where each tuple represents the start and end of a chunk interval.\n    :param drop_last: Bool. A flag indicating whether to drop the last items to make the distribution even across all ranks.\n    :return: Tuple[List[List[int]], List[Any]]. A tuple containing two elements. The first element is a list of lists, where each sublist contains the chunk indexes assigned to each rank. The second element is a list of lists of lists, where each sublist contains the intervals of chunks assigned to each rank.\n    \"\"\"\n\n    "}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items each rank should process\n    num_items_per_rank = (len(indexes) + distributed_env.world_size - 1) // distributed_env.world_size\n    if drop_last and distributed_env.rank == distributed_env.world_size - 1:\n        num_items_per_rank -= len(indexes) % distributed_env.world_size\n\n    # assign chunks and their intervals to each rank\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    chunk_intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    for i in range(len(indexes)):\n        rank = i % distributed_env.world_size\n        chunks_per_ranks[rank].append(indexes[i])\n        chunk_intervals_per_ranks[rank].append(chunk_intervals[i])\n\n    return chunks_per_ranks, chunk_intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    num_items = len(indexes)\n    num_items_per_rank = num_items // distributed_env.world_size\n    if drop_last:\n        num_items_per_rank = num_items_per_rank - (num_items % distributed_env.world_size)\n\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    chunks_intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n\n    for rank in range(distributed_env.world_size):\n        start_index = rank * num_items_per_rank\n        end_index = (rank + 1) * num_items_per_rank\n        if drop_last and rank == distributed_env.world_size - 1:\n            end_index = num_items\n\n        chunks_per_ranks[rank] = indexes[start_index:end_index]\n        chunks_intervals_per_ranks[rank] = chunk_intervals[start_index:end_index]\n\n    return chunks_per_ranks, chunks_intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items each rank should process\n    num_items_per_rank = _calculate_num_items_per_rank(\n        distributed_env, len(indexes), drop_last=drop_last\n    )\n\n    # distribute the chunks and their intervals to the ranks\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    chunks_intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    for rank, num_items in enumerate(num_items_per_rank):\n        chunks_per_ranks[rank] = indexes[:num_items]\n        chunks_intervals_per_ranks[rank] = chunk_intervals[:num_items]\n        indexes = indexes[num_items:]\n        chunk_intervals = chunk_intervals[num_items:]\n\n    return chunks_per_ranks, chunks_intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items to process per rank\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    if drop_last:\n        num_items_per_rank -= len(indexes) % distributed_env.world_size\n\n    # calculate the number of items to process per rank\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    if drop_last:\n        num_items_per_rank -= len(indexes) % distributed_env.world_size\n\n    # assign chunks and intervals to each rank\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    chunks_intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    for rank in range(distributed_env.world_size):\n        start_index = rank * num_items_per_rank\n        end_index = (rank + 1) * num_items_per_rank\n        if drop_last and rank == distributed_env.world_size - 1:\n            end_index = len(indexes)\n        chunks_per_ranks[rank] = indexes[start_index:end_index]\n        chunks_intervals_per_ranks[rank] = chunk_intervals[start_index:end_index]\n\n    return chunks_per_ranks, chunks_intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items to process per rank\n    num_items_per_rank = _calculate_num_items_per_rank(\n        num_items=len(indexes), world_size=distributed_env.world_size, drop_last=drop_last\n    )\n\n    # distribute the chunks and their corresponding intervals to the ranks\n    chunks_per_ranks: List[List[int]] = [[] for _ in range(distributed_env.world_size)]\n    intervals_per_ranks: List[List[Any]] = [[] for _ in range(distributed_env.world_size)]\n    for rank, (start, end) in enumerate(chunk_intervals):\n        chunks_per_ranks[rank].extend(indexes[start:end])\n        intervals_per_ranks[rank].extend(chunk_intervals[start:end])\n\n    return chunks_per_ranks, intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items per rank\n    items_per_rank = _calculate_items_per_rank(distributed_env, len(indexes), drop_last)\n\n    # associate chunks and intervals to each rank\n    chunks_per_ranks, chunk_intervals_per_ranks = [], []\n    for rank, items in enumerate(items_per_rank):\n        chunks_per_ranks.append(indexes[rank * items : (rank + 1) * items])\n        chunk_intervals_per_ranks.append(chunk_intervals[rank * items : (rank + 1) * items])\n\n    return chunks_per_ranks, chunk_intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items to process per rank\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    if drop_last:\n        num_items_per_rank -= len(indexes) % distributed_env.world_size\n\n    # calculate the number of items to process on each rank\n    num_items_per_ranks = [num_items_per_rank] * distributed_env.world_size\n    if drop_last:\n        for i in range(len(indexes) % distributed_env.world_size):\n            num_items_per_ranks[i] += 1\n\n    # assign chunks and their intervals to each rank\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    chunk_intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    for rank, num_items in enumerate(num_items_per_ranks):\n        for i in range(num_items):\n            chunks_per_ranks[rank].append(indexes[i])\n            chunk_intervals_per_ranks[rank].append(chunk_intervals[indexes[i]])\n\n    return chunks_per_ranks, chunk_intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    assert len(indexes) == len(chunk_intervals), \"indexes and chunk_intervals must have the same length\"\n\n    # calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    if drop_last:\n        num_items_per_rank = num_items_per_rank * distributed_env.world_size\n\n    # distribute the chunks and intervals across the ranks\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    for rank in range(distributed_env.world_size):\n        start = rank * num_items_per_rank\n        end = (rank + 1) * num_items_per_rank\n        chunks_per_ranks[rank].extend(indexes[start:end])\n        intervals_per_ranks[rank].extend(chunk_intervals[start:end])\n\n    return chunks_per_ranks, intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items per rank\n    num_items_per_rank = _calculate_num_items_per_rank(distributed_env, len(indexes), drop_last)\n\n    # distribute the chunks and intervals to the ranks\n    chunks_per_ranks = _distribute_chunks_and_intervals_to_ranks(indexes, chunk_intervals, num_items_per_rank)\n\n    # shuffle the chunks and intervals per node\n    chunks_per_ranks = _intra_node_chunk_shuffle(distributed_env, chunks_per_ranks, seed=0, current_epoch=0)\n\n    return chunks_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items per rank\n    num_items_per_rank = _calculate_num_items_per_rank(\n        num_items=len(indexes),\n        world_size=distributed_env.world_size,\n        drop_last=drop_last,\n    )\n\n    # calculate the number of chunks per rank\n    num_chunks_per_rank = _calculate_num_chunks_per_rank(\n        num_chunks=len(indexes),\n        num_items_per_rank=num_items_per_rank,\n    )\n\n    # calculate the number of items per rank\n    num_items_per_rank = _calculate_num_items_per_rank(\n        num_items=len(indexes),\n        world_size=distributed_env.world_size,\n        drop_last=drop_last,\n    )\n\n    # calculate the number of chunks per rank\n    num_chunks_per_rank = _calculate_num_chunks_per_rank(\n        num_chunks=len(indexes),\n        num_items_per_rank=num_items_per_rank,\n    )\n\n    # assign chunks and intervals to each rank\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    for rank, (num_chunks, num_items) in enumerate(zip(num_chunks_per_rank, num_items_per_rank)):\n        for chunk_index in indexes[:num_chunks]:\n            chunks_per_ranks[rank].append(chunk_index)\n            intervals_per_ranks[rank].append(chunk_intervals[chunk_index])\n        indexes = indexes[num_chunks:]\n\n    return chunks_per_ranks, intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    assert len(indexes) == len(chunk_intervals)\n    assert len(indexes) > 0\n\n    # if we drop the last items, we need to make sure that the number of items is divisible by the world size\n    if drop_last:\n        assert len(indexes) % distributed_env.world_size == 0\n\n    # calculate the number of items to process per rank\n    items_per_rank = len(indexes) // distributed_env.world_size\n    # calculate the number of items to process on the last rank\n    items_last_rank = len(indexes) % distributed_env.world_size\n\n    # distribute the items across the ranks\n    chunks_per_ranks = [\n        indexes[i * items_per_rank : (i + 1) * items_per_rank]\n        for i in range(distributed_env.world_size)\n    ]\n\n    # if we drop the last items, we need to remove the last items from the last rank\n    if drop_last:\n        chunks_per_ranks[-1] = chunks_per_ranks[-1][:items_last_rank]\n\n    # distribute the intervals across the ranks\n    intervals_per_ranks = [\n        chunk_intervals[i * items_per_rank : (i + 1) * items_per_rank]\n        for i in range(distributed_env.world_size)\n    ]\n\n    # if we drop the last items, we need to remove the last intervals from the last rank\n    if drop_last:\n        intervals_per_ranks[-1] = intervals_per_ranks[-1][:items_last_rank]\n\n    return chunks_per_ranks, intervals_per_ranks"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            self._find_device()\n\n        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, device=self._device, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir, device=self._device)"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            self._find_device()\n\n        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, device=self._device, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir, device=self._device)"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            self._find_device()\n\n        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, device=self._device, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir, device=self._device)"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            self._find_device()\n\n        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, device=self._device, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir, device=self._device)"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            self._find_device()\n\n        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last, device=self._device)\n        else:\n            self._fn(item_metadata, output_dir, device=self._device)"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            self._find_device()\n\n        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, device=self._device, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir, device=self._device)\n\n    "}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            self._find_device()\n\n        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, device=self._device, is_last=is_last)  # type: ignore\n        else:\n            self._fn(item_metadata, output_dir, device=self._device)  # type: ignore"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            self._find_device()\n            if self._device is not None:\n                item_metadata = item_metadata.to(self._device)\n\n        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir)"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if isinstance(self._fn, partial):\n            self._fn(item_metadata, output_dir, is_last=is_last)\n\n        elif isinstance(self._fn, FunctionType):\n            if inspect.isgeneratorfunction(self._fn):\n                self._fn(item_metadata, output_dir, is_last=is_last)\n            else:\n                self._fn(item_metadata, output_dir, is_last=is_last)\n        elif callable(self._fn):\n            if inspect.isgeneratorfunction(self._fn.__call__):  # type: ignore\n                self._fn.__call__(item_metadata, output_dir, is_last=is_last)  # type: ignore\n            else:\n                self._fn.__call__(item_metadata, output_dir, is_last=is_last)  # type: ignore\n        else:\n            raise ValueError(f\"The provided {self._fn} isn't supported.\")\n\n    "}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            self._find_device()\n\n        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, device=self._device, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir, device=self._device)\n\n    "}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            self._find_device()\n            self._fn(item_metadata, output_dir, device=self._device)  # type: ignore\n        elif self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last)  # type: ignore\n        else:\n            self._fn(item_metadata, output_dir)  # type: ignore"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            self._find_device()\n\n        if isinstance(self._fn, partial):\n            self._fn(item_metadata, output_dir, device=self._device, is_last=is_last)\n\n        elif isinstance(self._fn, FunctionType):\n            self._fn(item_metadata, output_dir, device=self._device, is_last=is_last)\n        elif callable(self._fn):\n            self._fn.__call__(item_metadata, output_dir, device=self._device, is_last=is_last)  # type: ignore\n        else:\n            raise ValueError(f\"The provided {self._fn} isn't supported.\")\n\n    "}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            self._find_device()\n            item_metadata[\"device\"] = self._device\n\n        if self._contains_is_last:\n            item_metadata[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir)"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if isinstance(self._fn, partial):\n            self._fn(output_dir, item_metadata, device=self._device, is_last=is_last)\n\n        elif isinstance(self._fn, FunctionType):\n            if inspect.isgeneratorfunction(self._fn):\n                self._fn(output_dir, item_metadata, device=self._device, is_last=is_last)\n            else:\n                self._fn(output_dir, item_metadata, device=self._device, is_last=is_last)\n        elif callable(self._fn):\n            if inspect.isgeneratorfunction(self._fn.__call__):  # type: ignore\n                self._fn.__call__(output_dir, item_metadata, device=self._device, is_last=is_last)  # type: ignore\n            else:\n                self._fn.__call__(output_dir, item_metadata, device=self._device, is_last=is_last)  # type: ignore\n        else:\n            raise ValueError(f\"The provided {self._fn} isn't supported.\")\n\n    "}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            self._find_device()\n\n        if isinstance(self._fn, partial):\n            self._fn(item_metadata, output_dir, device=self._device, is_last=is_last)  # type: ignore\n\n        elif isinstance(self._fn, FunctionType):\n            if inspect.isgeneratorfunction(self._fn):\n                for item in self._fn(item_metadata, output_dir, device=self._device, is_last=is_last):  # type: ignore\n                    yield item\n            else:\n                yield self._fn(item_metadata, output_dir, device=self._device, is_last=is_last)  # type: ignore\n        elif callable(self._fn):\n            if inspect.isgeneratorfunction(self._fn.__call__):  # type: ignore\n                for item in self._fn.__call__(item_metadata, output_dir, device=self._device, is_last=is_last):  # type: ignore\n                    yield item\n            else:\n                yield self._fn.__call__(item_metadata, output_dir, device=self._device, is_last=is_last)  # type: ignore\n        else:\n            raise ValueError(f\"The provided {self._fn} isn't supported.\")"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            self._find_device()\n\n        if isinstance(self._fn, partial):\n            self._fn(output_dir, item_metadata, device=self._device, is_last=is_last)\n        elif isinstance(self._fn, FunctionType):\n            self._fn(output_dir, item_metadata, device=self._device, is_last=is_last)\n        elif callable(self._fn):\n            self._fn.__call__(output_dir, item_metadata, device=self._device, is_last=is_last)  # type: ignore\n        else:\n            raise ValueError(f\"The provided {self._fn} isn't supported.\")\n\n    "}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            if self._device is None:\n                self._find_device()\n\n            item_metadata.update({\"device\": self._device})\n\n        if self._contains_is_last:\n            item_metadata.update({\"is_last\": is_last})\n\n        self._fn(output_dir, item_metadata)"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            self._find_device()\n            if self._device is None:\n                raise ValueError(\"The provided device isn't valid.\")\n\n        if isinstance(self._fn, partial):\n            self._fn(output_dir, item_metadata, device=self._device, is_last=is_last)  # type: ignore\n        elif isinstance(self._fn, FunctionType):\n            self._fn(output_dir, item_metadata, device=self._device, is_last=is_last)  # type: ignore\n        elif callable(self._fn):\n            self._fn.__call__(output_dir, item_metadata, device=self._device, is_last=is_last)  # type: ignore\n        else:\n            raise ValueError(f\"The provided {self._fn} isn't supported.\")"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if isinstance(self._fn, partial):\n            self._fn(item_metadata, output_dir, is_last=is_last)  # type: ignore\n\n        elif isinstance(self._fn, FunctionType):\n            if inspect.isgeneratorfunction(self._fn):\n                self._fn(item_metadata, output_dir, is_last=is_last)  # type: ignore\n            else:\n                self._fn(item_metadata, output_dir, is_last=is_last)  # type: ignore\n        elif callable(self._fn):\n            if inspect.isgeneratorfunction(self._fn.__call__):  # type: ignore\n                self._fn.__call__(item_metadata, output_dir, is_last=is_last)  # type: ignore\n            else:\n                self._fn.__call__(item_metadata, output_dir, is_last=is_last)  # type: ignore\n        else:\n            raise ValueError(f\"The provided {self._fn} isn't supported.\")\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if isinstance(self._fn, partial):\n            self._fn(item_metadata, output_dir)\n\n        elif isinstance(self._fn, FunctionType):\n            if inspect.isgeneratorfunction(self._fn):\n                for item in self._fn(item_metadata, output_dir):\n                    yield item\n            else:\n                yield self._fn(item_metadata, output_dir)\n        elif callable(self._fn):\n            if inspect.isgeneratorfunction(self._fn.__call__):  # type: ignore\n                for item in self._fn.__call__(item_metadata, output_dir):  # type: ignore\n                    yield item\n            else:\n                yield self._fn.__call__(item_metadata, output_dir)  # type: ignore\n        else:\n            raise ValueError(f\"The provided {self._fn} isn't supported.\")\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n                continue\n            raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n                continue\n            raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n                continue\n            raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n                continue\n            raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n                continue\n            raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n                continue\n            raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n                continue\n            raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n                continue\n            raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n                continue\n            raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n                continue\n            raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n                continue\n            raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n                continue\n            raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    try:\n        s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"404\":\n            sleep(sleep_time)\n            _wait_for_file_to_exist(s3, obj, sleep_time)\n        else:\n            raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    try:\n        s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"404\":\n            sleep(sleep_time)\n            _wait_for_file_to_exist(s3, obj, sleep_time)\n        else:\n            raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    try:\n        s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"404\":\n            sleep(sleep_time)\n            _wait_for_file_to_exist(s3, obj, sleep_time)\n        else:\n            raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    try:\n        s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"404\":\n            sleep(sleep_time)\n            return _wait_for_file_to_exist(s3, obj, sleep_time)\n        else:\n            raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    try:\n        s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n        return\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"404\":\n            sleep(sleep_time)\n            return _wait_for_file_to_exist(s3, obj, sleep_time)\n        else:\n            raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n                continue\n            else:\n                raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            break\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir("}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir("}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir("}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir("}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir="}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir="}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir="}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir="}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default_num_workers(),\n            fast_dev_run=fast_dev_run,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_files=reorder_files,\n            weights=weights,"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not _IS_IN_STUDIO:\n        print(\"This method is optimized to run on https://lightning.ai/. Don't use it otherwise.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default_num_workers(),\n            fast_dev_run=fast_dev_run,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_files=reorder_files,\n            weights=weights,\n            reader=reader,\n        )\n        with optimize_dns_context(True):\n            return data_processor.run(\n                LambdaDataChunkRecipe(fn, inputs, chunk_size, chunk_bytes, compression)\n            )\n    return _execute(\n        f\"data"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default_num_workers(),\n            fast_dev_run=fast_dev_run,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_files=reorder_files,\n            weights=weights,\n            reader=reader,\n        )\n        with optimize_dns_context(True):\n            return data_processor.run(\n                LambdaDataChunkRecipe(\n                    fn,\n                    inputs,\n                    chunk_size=chunk_size,\n                    chunk_bytes=chunk"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to process your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to process your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(input"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to process your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(input"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to process your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(input"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to process your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(input"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to process your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(input"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to process your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir)\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to process your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir)\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to process your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir)\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to map your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to map your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to process your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir)\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to process your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to map your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir)\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to process your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir, error_when_not_empty)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir"}
{"namespace": "litdata.processing.functions.map", "completion": "    if not _IS_IN_STUDIO:\n        print(\"This method is optimized to run on https://lightning.ai/. Don't use it otherwise.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if num_workers is None:\n        num_workers = _get_default_num_workers()\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir)\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        # 1. Collect download task\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Check if the files are already downloaded\n        for path in paths:\n            if input_dir:\n                if not path.startswith(cache_dir) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if os.path.exists(path):\n                    continue\n\n            elif os.path.exists(path):\n                continue\n\n            # 4. Download the file\n            if input_dir.url:\n                obj = parse.urlparse(path)\n                s3 = S3Client()\n                _wait_for_file_to_exist(s3, obj)\n                with open(path, \"wb\") as f:\n                    s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n            elif input_dir.path:\n                shutil.copyfile(path, os.path.join(cache_dir, os.path.basename(path)))\n            else:\n                raise ValueError(f\"The provided {input_dir.path} isn't supported.\")\n\n        # 5. Signal completion\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        # 1. Collect the task\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(input_dir.path) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if os.path.exists(path):\n                    continue\n\n                if input_dir.url:\n                    obj = parse.urlparse(path)\n                    if obj.scheme == \"s3\":\n                        s3 = S3Client()\n                        _wait_for_file_to_exist(s3, obj)\n                        s3.client.download_file(\n                            obj.netloc,\n                            obj.path.lstrip(\"/\"),\n                            path,\n                        )\n                    else:\n                        shutil.copyfile(path, path)\n                else:\n                    shutil.copyfile(path, path)\n\n            elif os.path.exists(path) and \"s3_connections\" not in path:\n                os.makedirs(os.path.dirname(path), exist_ok=True)\n                shutil.copyfile(path, path)\n\n        # 4. Signal the task completion\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        # 1. Collect the task\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(input_dir.path) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if os.path.exists(path):\n                    continue\n\n                if input_dir.url:\n                    obj = parse.urlparse(path)\n                    if obj.scheme == \"s3\":\n                        s3 = S3Client()\n                        _wait_for_file_to_exist(s3, obj)\n                        s3.client.download_file(\n                            obj.netloc,\n                            obj.path.lstrip(\"/\"),\n                            path,\n                        )\n                    else:\n                        shutil.copyfile(path, path)\n                else:\n                    shutil.copyfile(path, path)\n\n            elif os.path.exists(path):\n                continue\n\n            else:\n                raise ValueError(f\"The provided {path} isn't supported.\")\n\n        # 4. Signal the task is done\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        # 1. Collect the download task\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(input_dir.path) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if os.path.exists(path):\n                    continue\n\n                if input_dir.url:\n                    obj = parse.urlparse(path)\n                    if obj.scheme == \"s3\":\n                        s3 = S3Client()\n                        _wait_for_file_to_exist(s3, obj)\n                        s3.client.download_file(obj.netloc, obj.path.lstrip(\"/\"), path)\n                    elif input_dir.path:\n                        shutil.copyfile(path, path.replace(input_dir.path, cache_dir))\n                    continue\n\n            elif os.path.exists(path):\n                continue\n\n            shutil.copyfile(path, path.replace(input_dir.path, cache_dir))\n\n        # 4. Signal the completion of the task\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        # 1. Collect paths\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(input_dir.path) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if os.path.exists(path):\n                    continue\n\n                if input_dir.url:\n                    obj = parse.urlparse(path)\n                    if obj.scheme == \"s3\":\n                        s3 = S3Client()\n                        _wait_for_file_to_exist(s3, obj)\n                        s3.client.download_file(obj.netloc, obj.path.lstrip(\"/\"), path)\n                    else:\n                        raise ValueError(f\"The provided {input_dir.url} isn't supported.\")\n                else:\n                    shutil.copyfile(path, path.replace(input_dir.path, cache_dir))\n\n            elif os.path.exists(path) and \"s3_connections\" not in path:\n                shutil.copyfile(path, path.replace(input_dir.path, cache_dir))\n\n        # 4. Signal the main thread that the task is done\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    cache_dir = cache_dir.rstrip(\"/\")\n    while True:\n        # 1. Collect the download task\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(input_dir.path) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if os.path.exists(path):\n                    continue\n\n                if input_dir.url:\n                    obj = parse.urlparse(path)\n                    s3 = S3Client()\n                    _wait_for_file_to_exist(s3, obj)\n                    s3.client.download_file(obj.netloc, obj.path.lstrip(\"/\"), path)\n                else:\n                    shutil.copyfile(path, path.replace(input_dir.path, cache_dir))\n\n            elif os.path.exists(path):\n                shutil.copyfile(path, path.replace(input_dir.path, cache_dir))\n            else:\n                raise ValueError(f\"The provided {path} isn't supported.\")\n\n        # 4. Signal the completion of the task\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        # 1. Collect the task\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(input_dir.path) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if os.path.exists(path):\n                    continue\n\n                obj = parse.urlparse(path)\n                if obj.scheme == \"s3\":\n                    s3 = S3Client()\n                    _wait_for_file_to_exist(s3, obj)\n                    os.makedirs(os.path.dirname(path), exist_ok=True)\n                    with open(path, \"wb\") as f:\n                        s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n                else:\n                    shutil.copyfile(path, path)\n\n            elif os.path.exists(path) and \"s3_connections\" not in path:\n                os.makedirs(os.path.dirname(path), exist_ok=True)\n                shutil.copyfile(path, path)\n            else:\n                raise ValueError(f\"The provided {path} isn't supported.\")\n\n        # 4. Signal the completion of the task\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        # 1. Collect the task\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(input_dir.path) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if os.path.exists(path):\n                    continue\n\n                obj = parse.urlparse(path)\n                if obj.scheme == \"s3\":\n                    s3 = S3Client()\n                    _wait_for_file_to_exist(s3, obj)\n                    os.makedirs(os.path.dirname(path), exist_ok=True)\n                    with open(path, \"wb\") as f:\n                        s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n                elif input_dir.path and os.path.isdir(input_dir.path):\n                    shutil.copyfile(path, path.replace(input_dir.path, cache_dir))\n            else:\n                if os.path.exists(path):\n                    continue\n                shutil.copyfile(path, path.replace(cache_dir, input_dir.path))\n\n        # 4. Signal that the task is done\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        # 1. Collect the download task\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(input_dir.path) and input_dir.url is not None:\n                    path = path.replace(input_dir.path, input_dir.url)\n\n                obj = parse.urlparse(path)\n                if obj.scheme == \"s3\":\n                    s3 = S3Client()\n                    _wait_for_file_to_exist(s3, obj)\n                    s3.client.download_file(obj.netloc, obj.path.lstrip(\"/\"), os.path.join(cache_dir, path.split(\"/\")[-1]))\n\n            elif os.path.exists(path):\n                shutil.copyfile(path, os.path.join(cache_dir, path.split(\"/\")[-1]))\n\n        # 4. Signal the completion of the download task\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        # 1. Collect the download task\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(input_dir.path) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if os.path.exists(path):\n                    continue\n\n                if input_dir.url:\n                    obj = parse.urlparse(path)\n                    if obj.scheme == \"s3\":\n                        s3 = S3Client()\n                        _wait_for_file_to_exist(s3, obj)\n                        with open(path, \"wb\") as f:\n                            s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n                    else:\n                        shutil.copyfile(path, path)\n                else:\n                    os.makedirs(os.path.dirname(path), exist_ok=True)\n                    shutil.copyfile(path, path)\n\n            elif os.path.exists(path):\n                continue\n            else:\n                raise ValueError(f\"The provided {path} isn't supported.\")\n\n        # 4. Signal the completion of the download task\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        # 1. Collect the task\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Check if the files are already downloaded\n        for path in paths:\n            if input_dir:\n                if not path.startswith(cache_dir) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if os.path.exists(path):\n                    continue\n\n            elif os.path.exists(path) and \"s3_connections\" not in path:\n                continue\n\n        # 4. Download the files\n        obj = parse.urlparse(input_dir.url if input_dir.url else input_dir.path)\n        if obj.scheme == \"s3\":\n            s3 = S3Client()\n            for path in paths:\n                if input_dir:\n                    if not path.startswith(cache_dir) and input_dir.path is not None:\n                        path = path.replace(input_dir.path, cache_dir)\n\n                    if not os.path.exists(path):\n                        output_filepath = os.path.join(cache_dir, os.path.basename(path))\n                        _wait_for_file_to_exist(s3, obj)\n                        s3.client.download_file(\n                            obj.netloc,\n                            obj.path.lstrip(\"/\") + \"/\" + os.path.basename(path),\n                            output_filepath,\n                        )\n\n        elif input_dir.path:\n            for path in paths:\n                if not path.startswith(cache_dir) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if not os.path.exists(path):\n                    output_filepath = os.path.join(cache_dir, os.path.basename(path))\n                    os.makedirs(os.path.dirname(output"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        # 1. Collect paths\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(cache_dir) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if os.path.exists(path):\n                    continue\n\n            elif os.path.exists(path):\n                continue\n\n            obj = parse.urlparse(path)\n\n            if obj.scheme == \"s3\":\n                s3 = S3Client()\n                _wait_for_file_to_exist(s3, obj)\n                s3.client.download_file(obj.netloc, obj.path.lstrip(\"/\"), path)\n\n            elif input_dir.path:\n                shutil.copyfile(path, os.path.join(cache_dir, os.path.basename(path)))\n            else:\n                raise ValueError(f\"The provided {input_dir.path} isn't supported.\")\n\n        # 4. Signal the main process that the task is done.\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Collect the download task\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(input_dir.path) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if os.path.exists(path):\n                    continue\n\n                if input_dir.url:\n                    obj = parse.urlparse(path)\n                    _wait_for_file_to_exist(s3, obj)\n\n                    os.makedirs(os.path.dirname(path), exist_ok=True)\n                    with open(path, \"wb\") as f:\n                        s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n\n            elif os.path.exists(path):\n                if not path.startswith(cache_dir):\n                    path = os.path.join(cache_dir, path)\n\n                os.makedirs(os.path.dirname(path), exist_ok=True)\n                shutil.copyfile(path, path)\n\n            else:\n                raise ValueError(f\"The provided {path} isn't supported.\")\n\n        # 4. Inform the processor that the task is done\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client()\n    while True:\n        # 1. Collect the download task\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(input_dir.path) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if os.path.exists(path):\n                    continue\n\n                if input_dir.url and input_dir.url.startswith(\"s3://\"):\n                    obj = parse.urlparse(path)\n                    _wait_for_file_to_exist(s3, obj)\n                    with open(path, \"wb\") as f:\n                        s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n                else:\n                    shutil.copyfile(path, path.replace(input_dir.path, cache_dir))\n\n            elif os.path.exists(path):\n                continue\n\n            elif input_dir.url and input_dir.url.startswith(\"s3://\"):\n                obj = parse.urlparse(path)\n                _wait_for_file_to_exist(s3, obj)\n                with open(path, \"wb\") as f:\n                    s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n            else:\n                shutil.copyfile(path, path.replace(input_dir.path, cache_dir))\n\n        # 4. Signal completion\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        # 1. Collect paths\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(input_dir.path) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if os.path.exists(path):\n                    continue\n\n                if input_dir.url:\n                    obj = parse.urlparse(path)\n                    if obj.scheme == \"s3\":\n                        s3 = S3Client()\n                        _wait_for_file_to_exist(s3, obj)\n                        os.makedirs(os.path.dirname(path), exist_ok=True)\n                        s3.client.download_file(obj.netloc, obj.path.lstrip(\"/\"), path)\n                    elif input_dir.path and os.path.isdir(input_dir.path):\n                        shutil.copyfile(path, path.replace(input_dir.path, cache_dir))\n            else:\n                if not os.path.exists(path):\n                    raise RuntimeError(f\"The provided path {path} doesn't exist.\")\n\n        # 4. Signal completion to the main process\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        # 1. Collect download task\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and check if the file is already downloaded\n        for path in paths:\n            if input_dir:\n                if not path.startswith(input_dir.path) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if os.path.exists(path):\n                    continue\n\n            elif os.path.exists(path) and \"s3_connections\" not in path:\n                continue\n\n            # 4. Download the file\n            obj = parse.urlparse(path)\n            if obj.scheme == \"s3\":\n                s3 = S3Client()\n                s3.client.download_file(\n                    obj.netloc,\n                    obj.path.lstrip(\"/\"),\n                    os.path.join(cache_dir, os.path.basename(path)),\n                )\n\n            elif input_dir.path and os.path.isdir(input_dir.path):\n                shutil.copyfile(path, os.path.join(cache_dir, os.path.basename(path)))\n\n            else:\n                raise ValueError(f\"The provided {input_dir.path} isn't supported.\")\n\n        # 5. Signal completion\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    cache = Cache(cache_dir)\n\n    while True:\n        # 1. Collect paths\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(input_dir.path) and input_dir.url is not None:\n                    path = path.replace(input_dir.path, input_dir.url)\n\n                if input_dir.url and input_dir.url.startswith(\"s3://\"):\n                    obj = parse.urlparse(path)\n                    _wait_for_file_to_exist(S3Client(), obj)\n\n            elif not os.path.exists(path):\n                raise RuntimeError(f\"The provided file {path} doesn't exist.\")\n\n            # 4. Check if the file is already downloaded\n            if cache.exists(path):\n                continue\n\n            # 5. Download the file\n            cache.add_item(path)\n\n        # 6. Signal that the files for the index are available\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Wait for the input dir to be available\n    if input_dir.url:\n        obj = parse.urlparse(input_dir.url)\n        if obj.scheme == \"s3\":\n            s3 = S3Client()\n            _wait_for_file_to_exist(s3, obj)\n\n    if input_dir.path:\n        _wait_for_disk_usage_higher_than_threshold(input_dir.path)\n\n    while True:\n        # 1. Collect the task\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir.path is not None:\n                if not path.startswith(input_dir.path):\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if os.path.exists(path):\n                    continue\n\n            if input_dir.url:\n                obj = parse.urlparse(path)\n                if obj.scheme == \"s3\":\n                    s3 = S3Client()\n                    _wait_for_file_to_exist(s3, obj)\n\n                    # Download the file\n                    s3.client.download_file(\n                        obj.netloc,\n                        obj.path.lstrip(\"/\"),\n                        path,\n                    )\n\n                elif input_dir.path:\n                    raise ValueError(f\"The provided {input_dir.path} isn't supported.\")\n\n        # 4. Signal the completion of the task\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # Wait for the disk to have more than 25GB free space\n    if input_dir.path and input_dir.path.startswith(\"/teamspace\"):\n        _wait_for_disk_usage_higher_than_threshold(input_dir.path)\n\n    while True:\n        # 1. Collect paths\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(cache_dir) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if os.path.exists(path):\n                    continue\n\n                obj = parse.urlparse(path)\n                if obj.scheme == \"s3\":\n                    s3 = S3Client()\n                    _wait_for_file_to_exist(s3, obj)\n                    s3.client.download_file(obj.netloc, obj.path.lstrip(\"/\"), path)\n\n            elif os.path.exists(path):\n                continue\n\n            else:\n                raise ValueError(f\"The provided {path} isn't supported.\")\n\n        # 4. Signal the completion of the download\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    cache_dir = cache_dir.rstrip(\"/\")\n\n    if input_dir.url:\n        s3 = S3Client()\n        obj = parse.urlparse(input_dir.url)\n        if obj.scheme == \"s3\":\n            _wait_for_file_to_exist(s3, obj)\n\n    while True:\n        # 1. Collect task\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(cache_dir) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if os.path.exists(path):\n                    continue\n\n            if input_dir.url:\n                if obj.scheme == \"s3\":\n                    obj = parse.urlparse(path)\n                    _wait_for_file_to_exist(s3, obj)\n\n                    with open(path, \"wb\") as f:\n                        s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n                elif input_dir.path and os.path.isdir(input_dir.path):\n                    shutil.copyfile(path, os.path.join(cache_dir, os.path.basename(path)))\n\n        # 4. Signal the main thread that the files are available\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        # 1. Collect paths\n        data = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if data is None:\n            queue_out.put(None)\n            return\n\n        # 3. Unpack\n        if isinstance(data, str):\n            local_path = data\n            remote_path = data.replace(cache_dir, output_dir.path if output_dir.path else output_dir.url)\n        else:\n            local_path, remote_path = data\n\n        # 4. Upload the file\n        if output_dir.url:\n            obj = parse.urlparse(remote_path)\n            s3 = S3Client()\n            s3.client.upload_file(local_path, obj.netloc, obj.path.lstrip(\"/\"))\n        elif output_dir.path and os.path.isdir(output_dir.path):\n            shutil.copyfile(local_path, remote_path)\n\n        # 5. Inform the worker the current files are available\n        queue_out.put(remote_path)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        # 1. Fetch from the queue\n        r: Optional[Union[str, Tuple[str, str]]] = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if r is None:\n            remove_queue.put(None)\n            return\n\n        # 3. Unpack\n        if isinstance(r, str):\n            local_path = r\n            if input_dir.path:\n                local_path = local_path.replace(input_dir.path, cache_dir)\n\n            if input_dir.url:\n                local_path = local_path.replace(input_dir.path, input_dir.url)\n\n            obj = parse.urlparse(local_path)\n\n            if obj.scheme == \"s3\":\n                dirpath = os.path.dirname(local_path)\n\n                os.makedirs(dirpath, exist_ok=True)\n\n                with open(local_path, \"wb\") as f:\n                    s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n\n            elif os.path.isfile(local_path):\n                if not local_path.startswith(\"/teamspace/studios/this_studio\"):\n                    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n                    shutil.copyfile(local_path, local_path)\n            else:\n                raise ValueError(f\"The provided {input_dir.url} isn't supported.\")\n\n        elif isinstance(r, tuple):\n            local_path, remote_path = r\n            obj = parse.urlparse(remote_path)\n\n            if obj.scheme == \"s3\":\n                dirpath = os.path.dirname(remote_path)\n\n                os.makedirs(dirpath, exist_ok=True)\n\n                with open(local_path, \"rb\") as f:\n                    s3.client.upload_fileobj(f, obj.netloc, obj.path"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        # 1. Fetch from the queue\n        r: Optional[Union[str, Tuple[str, str]]] = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if r is None:\n            remove_queue.put(None)\n            return\n\n        # 3. Unpack\n        if isinstance(r, str):\n            local_path = r\n            remote_path = r.replace(cache_dir, output_dir.path if output_dir.path else output_dir.url)\n        else:\n            local_path, remote_path = r\n\n        # 4. Upload\n        obj = parse.urlparse(remote_path)\n\n        if obj.scheme == \"s3\":\n            s3 = S3Client()\n            s3.client.upload_file(local_path, obj.netloc, obj.path.lstrip(\"/\"))\n\n        elif output_dir.path and os.path.isdir(output_dir.path):\n            shutil.copyfile(local_path, remote_path)\n\n        # 5. Inform the worker the current files are available\n        remove_queue.put(local_path)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        # 1. Collect paths\n        r = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if r is None:\n            remove_queue.put(None)\n            return\n\n        # 3. Unpack\n        if isinstance(r, str):\n            local_filepath = r\n            remote_filepath = local_filepath\n        else:\n            local_filepath = r[-1]\n            remote_filepath = r[0]\n\n        # 4. Upload the file\n        obj = parse.urlparse(output_dir.url if output_dir.url else output_dir.path)\n        if obj.scheme == \"s3\":\n            s3 = S3Client()\n            s3.client.upload_file(\n                local_filepath, obj.netloc, os.path.join(str(obj.path).lstrip(\"/\"), os.path.basename(remote_filepath))\n            )\n        elif output_dir.path and os.path.isdir(output_dir.path):\n            shutil.copyfile(local_filepath, os.path.join(output_dir.path, os.path.basename(remote_filepath)))\n\n        # 5. Remove the file\n        if remove_queue:\n            remove_queue.put(local_filepath)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    # 1. Initialize the upload queue\n    upload_queue = Queue()\n\n    # 2. Initialize the remove queue\n    remove_queue = Queue()\n\n    # 3. Initialize the uploader\n    uploader = Process(\n        target=_upload_fn,\n        args=(\n            upload_queue,\n            remove_queue,\n            cache_dir,\n            output_dir,\n        ),\n    )\n\n    # 4. Start the uploader\n    uploader.start()\n\n    # 5. Wait for the uploader to finish\n    uploader.join()\n\n    # 6. Terminate the uploader\n    uploader.terminate()\n\n    # 7. Terminate the uploader\n    uploader.join()"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Fetch from the queue\n        r: Optional[Union[str, Tuple[str, str]]] = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if r is None:\n            remove_queue.put(None)\n            return\n\n        # 3. Unpack\n        if isinstance(r, str):\n            local_filepath = r\n        else:\n            local_filepath, remote_filepath = r\n\n        # 4. Upload the file to the remote directory\n        obj = parse.urlparse(remote_filepath)\n        if obj.scheme == \"s3\":\n            dirpath = os.path.dirname(remote_filepath)\n            if dirpath:\n                s3.client.put_object(Bucket=obj.netloc, Key=dirpath.lstrip(\"/\"), Body=b\"\")\n\n            with open(local_filepath, \"rb\") as f:\n                s3.client.upload_fileobj(f, obj.netloc, obj.path.lstrip(\"/\"))\n\n        elif output_dir.path and os.path.isdir(output_dir.path):\n            shutil.copyfile(local_filepath, os.path.join(output_dir.path, os.path.basename(local_filepath)))\n\n        # 5. Inform the worker the current file is available\n        remove_queue.put(local_filepath)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        # 1. Collect paths\n        data = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if data is None:\n            queue_out.put(None)\n            return\n\n        # 3. Unpack\n        if isinstance(data, str):\n            assert os.path.exists(data)\n            local_path = data\n            remote_path = data.replace(cache_dir, output_dir.path) if output_dir.path else data.replace(\n                cache_dir, output_dir.url\n            )\n        else:\n            assert os.path.exists(data[-1])\n            local_path = data[-1]\n            remote_path = data[-1].replace(cache_dir, output_dir.path) if output_dir.path else data[-1].replace(\n                cache_dir, output_dir.url\n            )\n\n        # 4. Upload the file\n        obj = parse.urlparse(remote_path)\n        if obj.scheme == \"s3\":\n            s3 = S3Client()\n            s3.client.upload_file(local_path, obj.netloc, obj.path.lstrip(\"/\"))\n\n        elif output_dir.path and os.path.isdir(output_dir.path):\n            shutil.copyfile(local_path, remote_path)\n\n        # 5. Inform the worker the current files are available\n        queue_out.put(data)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        # 1. Collect paths\n        data = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if data is None:\n            queue_out.put(None)\n            return\n\n        # 3. Unpack\n        if isinstance(data, str):\n            local_path = data\n            remote_path = data\n        else:\n            local_path, remote_path = data\n\n        # 4. Upload the file\n        obj = parse.urlparse(remote_path)\n\n        if obj.scheme == \"s3\":\n            dirpath = os.path.dirname(remote_path)\n\n            if input_dir.path:\n                local_path = local_path.replace(input_dir.path, cache_dir)\n\n            with open(local_path, \"rb\") as f:\n                s3.client.upload_fileobj(f, obj.netloc, obj.path.lstrip(\"/\"))\n\n        elif output_dir.path and os.path.isdir(output_dir.path):\n            shutil.copyfile(local_path, remote_path)\n        else:\n            raise ValueError(f\"The provided {output_dir.url} isn't supported.\")\n\n        # 5. Remove the file from the local cache\n        if remove:\n            remove_queue.put(local_path)\n\n        # 6. Inform the worker the current files are available\n        queue_out.put(remote_path)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        # 1. Collect paths\n        data = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if data is None:\n            queue_out.put(None)\n            return\n\n        # 3. Unpack\n        if isinstance(data, str):\n            if not os.path.exists(data):\n                raise ValueError(f\"The provided {data} doesn't exist.\")\n\n            local_path = data\n            if input_dir.path and not input_dir.path.startswith(\"/teamspace/studios/this_studio\"):\n                local_path = data.replace(cache_dir, input_dir.path)\n\n            if output_dir.path:\n                dirpath = os.path.dirname(local_path)\n                os.makedirs(dirpath, exist_ok=True)\n                shutil.copyfile(data, local_path)\n\n            if output_dir.url:\n                obj = parse.urlparse(local_path)\n                if obj.scheme == \"s3\":\n                    s3 = S3Client()\n                    s3.client.upload_file(local_path, obj.netloc, obj.path.lstrip(\"/\"))\n\n        elif isinstance(data, tuple):\n            if not os.path.exists(data[1]):\n                raise ValueError(f\"The provided {data[1]} doesn't exist.\")\n\n            local_path = data[1]\n            if input_dir.path and not input_dir.path.startswith(\"/teamspace/studios/this_studio\"):\n                local_path = data[1].replace(cache_dir, input_dir.path)\n\n            if output_dir.path:\n                dirpath = os.path.dirname(local_path)\n                os.makedirs(dirpath, exist_ok=True)\n                shutil.copyfile(data[1], local_path)\n\n            if output_dir.url:\n                obj = parse.urlparse(local_path)\n                if obj.scheme == \"s3"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        # 1. Fetch from the queue\n        data: Optional[Union[str, Tuple[str, str]]] = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if data is None:\n            remove_queue.put(None)\n            return\n\n        # 3. Unpack\n        if isinstance(data, str):\n            local_filepath = data\n            if not local_filepath.startswith(cache_dir):\n                raise ValueError(\n                    f\"The provided filepath {local_filepath} isn't in the cache directory {cache_dir}.\"\n                )\n\n            # 4. Upload the file\n            obj = parse.urlparse(output_dir.url if output_dir.url else output_dir.path)\n            if obj.scheme == \"s3\":\n                s3 = S3Client()\n                s3.client.upload_file(\n                    local_filepath, obj.netloc, os.path.join(str(obj.path).lstrip(\"/\"), os.path.basename(local_filepath))\n                )\n\n            elif output_dir.path and os.path.isdir(output_dir.path):\n                shutil.copyfile(local_filepath, os.path.join(output_dir.path, os.path.basename(local_filepath)))\n\n        elif isinstance(data, tuple):\n            local_dirpath, local_filepath = data\n            if not local_filepath.startswith(cache_dir):\n                raise ValueError(\n                    f\"The provided filepath {local_filepath} isn't in the cache directory {cache_dir}.\"\n                )\n\n            # 4. Upload the file\n            obj = parse.urlparse(output_dir.url if output_dir.url else output_dir.path)\n            if obj.scheme == \"s3\":\n                s3 = S3Client()\n                s3.client.upload_file(\n                    local_filepath,\n                    obj.netloc,\n                    os.path.join(str"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        # 1. Collect the paths\n        data = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if data is None:\n            queue_out.put(None)\n            return\n\n        # 3. Unpack\n        if isinstance(data, str):\n            local_path = data\n            remote_path = data.replace(cache_dir, output_dir.path) if output_dir.path else data\n        else:\n            local_path, remote_path = data\n\n        # 4. Upload the file to the cloud storage\n        obj = parse.urlparse(remote_path)\n        if obj.scheme == \"s3\":\n            s3 = S3Client()\n            s3.client.upload_file(local_path, obj.netloc, obj.path.lstrip(\"/\"))\n        elif output_dir.path and os.path.isdir(output_dir.path):\n            shutil.copyfile(local_path, remote_path)\n\n        # 5. Remove the file from the local cache\n        remove_queue.put([local_path])\n\n        # 6. Inform the worker the current files are available\n        queue_out.put(remote_path)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        # 1. Collect paths\n        r: Optional[Union[str, Tuple[str, str]]] = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if r is None:\n            queue_out.put(None)\n            return\n\n        # 3. Unpack\n        if isinstance(r, tuple):\n            local_dir, local_path = r\n            local_path = os.path.join(local_dir, local_path)\n        else:\n            local_path = r\n\n        # 4. Check whether the file is already uploaded\n        if output_dir.path and os.path.exists(local_path.replace(cache_dir, output_dir.path)):\n            queue_out.put(local_path)\n            continue\n\n        # 5. Upload the file\n        obj = parse.urlparse(output_dir.url if output_dir.url else output_dir.path)\n        if obj.scheme == \"s3\":\n            dirpath = os.path.dirname(local_path)\n            if dirpath:\n                dirpath = dirpath.replace(cache_dir, \"\")\n\n            with open(local_path, \"rb\") as f:\n                s3.client.upload_fileobj(f, obj.netloc, os.path.join(str(obj.path).lstrip(\"/\"), dirpath))\n\n        elif output_dir.path and os.path.isdir(output_dir.path):\n            shutil.copyfile(local_path, local_path.replace(cache_dir, output_dir.path))\n\n        # 6. Inform the worker the current file is uploaded\n        queue_out.put(local_path)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        # 1. Collect paths\n        r: Optional[Union[str, Tuple[str, str]]] = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if r is None:\n            remove_queue.put(None)\n            return\n\n        # 3. Unpack\n        if isinstance(r, str):\n            local_path = r\n            if input_dir.path:\n                local_path = r.replace(input_dir.path, cache_dir)\n        else:\n            local_path, path = r\n\n        # 4. Upload the file\n        obj = parse.urlparse(output_dir.url if output_dir.url else output_dir.path)\n\n        if obj.scheme == \"s3\":\n            if not path.startswith(\"/teamspace/studios/this_studio\"):\n                with open(local_path, \"rb\") as f:\n                    s3.client.upload_fileobj(f, obj.netloc, obj.path.lstrip(\"/\"))\n        elif output_dir.path and os.path.isdir(output_dir.path):\n            shutil.copyfile(local_path, path)\n\n        # 5. Remove the file\n        if remove:\n            remove_queue.put([path])"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n    while True:\n        # 1. Fetch from the queue\n        r: Optional[Union[str, Tuple[str, str]]] = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if r is None:\n            remove_queue.put(None)\n            return\n\n        # 3. Upload the file to the remote directory\n        if isinstance(r, str):\n            local_path = r\n            obj = parse.urlparse(output_dir.url if output_dir.url else output_dir.path)\n            if obj.scheme == \"s3\":\n                with open(local_path, \"rb\") as f:\n                    s3.client.upload_fileobj(f, obj.netloc, obj.path.lstrip(\"/\"))\n            elif output_dir.path and os.path.isdir(output_dir.path):\n                shutil.copyfile(local_path, os.path.join(output_dir.path, os.path.basename(local_path)))\n            else:\n                raise ValueError(f\"The provided {output_dir.url} isn't supported.\")\n\n        elif isinstance(r, tuple):\n            local_path, remote_path = r\n            obj = parse.urlparse(output_dir.url if output_dir.url else output_dir.path)\n            if obj.scheme == \"s3\":\n                with open(local_path, \"rb\") as f:\n                    s3.client.upload_fileobj(f, obj.netloc, remote_path)\n            elif output_dir.path and os.path.isdir(output_dir.path):\n                shutil.copyfile(local_path, os.path.join(output_dir.path, os.path.basename(local_path)))\n            else:\n                raise ValueError(f\"The provided {output_dir.url} isn't supported.\")\n\n        # 4. Inform the worker the current files are available\n        remove_queue.put(local_path)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    # 1. Create the upload queue\n    upload_queue = Queue()\n\n    # 2. Start the uploader\n    p = Process(\n        target=_upload_fn_target,\n        args=(\n            upload_queue,\n            remove_queue,\n            cache_dir,\n            output_dir,\n        ),\n    )\n    p.start()\n\n    # 3. Wait for the uploader to finish\n    p.join()\n\n    # 4. Terminate the uploader\n    upload_queue.put(None)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        # 1. Collect paths\n        data = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if data is None:\n            queue_out.put(None)\n            return\n\n        # 3. Unpack\n        if isinstance(data, str):\n            assert os.path.exists(data), data\n            dirpath = os.path.dirname(data)\n            local_path = os.path.basename(data)\n        else:\n            dirpath, local_path = data\n            assert os.path.exists(os.path.join(dirpath, local_path)), (dirpath, local_path)\n\n        # 4. Upload the file to the output directory\n        obj = parse.urlparse(output_dir.url if output_dir.url else output_dir.path)\n\n        if obj.scheme == \"s3\":\n            s3 = S3Client()\n            s3.client.upload_file(\n                os.path.join(dirpath, local_path),\n                obj.netloc,\n                os.path.join(str(obj.path).lstrip(\"/\"), local_path),\n            )\n\n        elif output_dir.path and os.path.isdir(output_dir.path):\n            shutil.copyfile(\n                os.path.join(dirpath, local_path),\n                os.path.join(output_dir.path, local_path),\n            )\n\n        # 5. Remove the file from the cache directory\n        remove_queue.put([os.path.join(dirpath, local_path)])"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        # 1. Fetch from the queue\n        r = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if r is None:\n            remove_queue.put(None)\n            return\n\n        # 3. Unpack\n        if isinstance(r, str):\n            local_path = r\n            if not local_path.startswith(cache_dir):\n                raise ValueError(f\"The provided path {local_path} isn't in the cache dir.\")\n\n            if output_dir.path:\n                dirpath = os.path.dirname(local_path.replace(cache_dir, output_dir.path))\n                os.makedirs(dirpath, exist_ok=True)\n                shutil.copyfile(local_path, local_path.replace(cache_dir, output_dir.path))\n            elif output_dir.url:\n                obj = parse.urlparse(local_path.replace(cache_dir, output_dir.url))\n                s3 = S3Client()\n                s3.client.upload_file(local_path, obj.netloc, obj.path.lstrip(\"/\"))\n            else:\n                raise ValueError(\"The provided output_dir isn't supported.\")\n\n            remove_queue.put(local_path)\n        else:\n            local_dir, local_path = r\n            if not local_path.startswith(cache_dir):\n                raise ValueError(f\"The provided path {local_path} isn't in the cache dir.\")\n\n            if output_dir.path:\n                dirpath = os.path.dirname(local_path.replace(cache_dir, output_dir.path))\n                os.makedirs(dirpath, exist_ok=True)\n                shutil.copyfile(local_path, local_path.replace(cache_dir, output_dir.path))\n            elif output_dir.url:\n                obj = parse.urlparse(local_path.replace(cache_dir, output_dir.url))\n                s3 = S3Client()\n                s"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    # 1. Wait for the uploaders to catch up\n    _wait_for_disk_usage_higher_than_threshold(cache_dir, 25)\n\n    while True:\n        # 2. Fetch from the queue\n        r: Optional[Union[str, Tuple[str, str]]] = upload_queue.get()\n\n        # 3. Terminate the process if we received a termination signal\n        if r is None:\n            remove_queue.put(None)\n            return\n\n        # 4. Unpack\n        if isinstance(r, str):\n            filepath = r\n        else:\n            dirpath, filepath = r\n\n        # 5. Upload the file\n        obj = parse.urlparse(output_dir.url if output_dir.url else output_dir.path)\n        if obj.scheme == \"s3\":\n            s3 = S3Client()\n            s3.client.upload_file(\n                filepath,\n                obj.netloc,\n                os.path.join(str(obj.path).lstrip(\"/\"), os.path.basename(filepath)),\n            )\n        elif output_dir.path and os.path.isdir(output_dir.path):\n            shutil.copyfile(filepath, os.path.join(output_dir.path, os.path.basename(filepath)))\n\n        # 6. Remove the file\n        if dirpath:\n            if os.path.exists(dirpath):\n                shutil.rmtree(dirpath)\n        else:\n            if os.path.exists(filepath):\n                os.remove(filepath)\n\n        # 7. Inform the worker the current file is available\n        remove_queue.put(filepath)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    # 1. Initialize the upload queue\n    upload_queue: Queue = upload_queue\n    remove_queue: Queue = remove_queue\n\n    # 2. Initialize the S3 client\n    s3 = S3Client()\n\n    while True:\n        # 3. Fetch from the queue\n        data = upload_queue.get()\n\n        # 4. Terminate the process if we received a termination signal\n        if data is None:\n            upload_queue.put(None)\n            return\n\n        # 5. Unpack\n        if isinstance(data, str):\n            local_path = data\n        else:\n            local_path = data[-1]\n\n        # 6. Upload the file to the cloud\n        obj = parse.urlparse(output_dir.url if output_dir.url else output_dir.path)\n\n        if obj.scheme == \"s3\":\n            if not local_path.startswith(cache_dir):\n                local_path = local_path.replace(input_dir.path, cache_dir)\n\n            dirpath = os.path.dirname(local_path)\n            filename = os.path.basename(local_path)\n\n            if os.path.exists(local_path):\n                s3.client.upload_file(local_path, obj.netloc, os.path.join(obj.path.lstrip(\"/\"), filename))\n\n        # 7. Inform the worker the current files are available\n        upload_queue.put(data)\n\n        # 8. Remove the file from the local disk\n        if remove_queue:\n            remove_queue.put([local_path])"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    # 1. Start the main loop\n    while True:\n        # 2. Collect paths\n        paths = upload_queue.get()\n\n        # 3. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 4. Upload the files to the remote directory\n        if output_dir.url:\n            s3 = S3Client()\n            obj = parse.urlparse(output_dir.url)\n            for path in paths:\n                if not path.startswith(cache_dir):\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if os.path.isfile(path):\n                    local_filepath = path\n                    remote_filepath = path.replace(cache_dir, \"\")\n                else:\n                    local_filepath = path\n                    remote_filepath = path\n\n                s3.client.upload_file(local_filepath, obj.netloc, os.path.join(obj.path.lstrip(\"/\"), remote_filepath))\n\n        # 5. Delete the local files\n        if not isinstance(paths, str):\n            for path in paths:\n                if os.path.exists(path):\n                    os.remove(path)\n\n        # 6. Inform the worker the current files are available\n        remove_queue.put(paths)"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if file_size:\n        print(f\"Worker {str(_get_node_rank() * num_workers + worker_index)} is processing {len(worker_items)} items.\")\n    else:\n        print(f\"Worker {str(_get_node_rank() * num_workers + worker_index)} is processing {sum(worker_weights)} items.\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Print the distribution details\n    if file_size:\n        print(f\"Worker {str(node_rank * num_workers)}: {len(worker_items[node_rank * num_workers])} items.\")\n        print(\n            f\"Worker {str(node_rank * num_workers)}: {round(sum(worker_weights[node_rank * num_workers]) / 1000 / 1000, 2)} MB.\"\n        )\n        for i in range(num_workers):\n            print(\n                f\"Worker {str(node_rank * num_workers + i)}: {len(worker_items[node_rank * num_workers + i])} items.\"\n            )\n            print(\n                f\"Worker {str(node_rank * num_workers + i)}: {round(sum(worker_weights[node_rank * num_workers + i]) / 1000 / 1000, 2)} MB.\"\n            )\n    else:\n        print(f\"Worker {str(node_rank * num_workers)}: {len(worker_items[node_rank * num_workers])} items.\")\n        print(f\"Worker {str(node_rank * num_workers)}: {sum(worker_weights[node_rank * num_workers])} weights.\")\n        for i in range(num_workers):\n            print(f\"Worker {str(node_rank * num_workers + i)}: {len(worker_items[node_rank * num_workers + i])} items.\")\n            print(f\"Worker {str(node_rank * num_workers + i)}: {sum(worker_weights[node_rank * num_workers + i])} weights.\")\n\n    # Shuffle the items for each worker\n    for worker_index in worker_ids_this_node:\n        np.random.shuffle(worker_items[worker_index])\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if file_size:\n        print(f\"Worker {str(node_rank)}:\")\n        for i, (worker_index, worker_weight) in enumerate(zip(worker_ids_this_node, worker_weights)):\n            print(f\"\\tWorker {worker_index}: {round(worker_weight / 1000 / 1000, 2)} MB\")\n    else:\n        print(f\"Worker {str(node_rank)}:\")\n        for i, (worker_index, worker_weight) in enumerate(zip(worker_ids_this_node, worker_weights)):\n            print(f\"\\tWorker {worker_index}: {worker_weight}\")\n\n    # Shuffle the items for each worker\n    for worker_items_this_node in worker_items:\n        random.shuffle(worker_items_this_node)\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if file_size:\n        print(f\"Worker {str(node_rank)} is distributing {len(worker_items)} items to {num_workers} workers:\")\n        for i, worker_item in enumerate(worker_items):\n            print(f\"\\tWorker {str(i)}: {len(worker_item)} items ({sum(worker_weights[i])} MB)\")\n    else:\n        print(f\"Worker {str(node_rank)} is distributing {len(worker_items)} items to {num_workers} workers:\")\n        for i, worker_item in enumerate(worker_items):\n            print(f\"\\tWorker {str(i)}: {len(worker_item)} items ({sum(worker_weights[i])} weight)\")\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if file_size:\n        print(f\"Worker {str(node_rank)} is distributing {len(worker_items)} items across {num_workers} workers.\")\n        print(f\"Worker {str(node_rank)} is distributing {round(sum(worker_weights) / 1000 / 1000, 2)} MB across {num_workers} workers.\")\n    else:\n        print(f\"Worker {str(node_rank)} is distributing {len(worker_items)} items across {num_workers} workers.\")\n        print(f\"Worker {str(node_rank)} is distributing {sum(worker_weights)} items across {num_workers} workers.\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if file_size:\n        # Print the distribution details for workers on the current node\n        for worker_id in worker_ids_this_node:\n            worker_items_this_node = worker_items[worker_id]\n            worker_weights_this_node = worker_weights[worker_id]\n            total_weight = sum(worker_weights_this_node)\n            print(\n                f\"Worker {worker_id} will process {len(worker_items_this_node)} items with a total weight of \"\n                f\"{total_weight / 1000 / 1000} MB.\"\n            )\n    else:\n        # Print the distribution details for workers on the current node\n        for worker_id in worker_ids_this_node:\n            worker_items_this_node = worker_items[worker_id]\n            worker_weights_this_node = worker_weights[worker_id]\n            total_weight = sum(worker_weights_this_node)\n            print(\n                f\"Worker {worker_id} will process {len(worker_items_this_node)} items with a total weight of {total_weight}.\"\n            )\n\n    # Shuffle the items for each worker\n    worker_items_shuffled = []\n    for worker_id in worker_ids_this_node:\n        worker_items_this_node = worker_items[worker_id]\n        worker_weights_this_node = worker_weights[worker_id]\n        worker_items_shuffled.append(\n            [item for _, item in sorted(zip(worker_weights_this_node, worker_items_this_node), reverse=True)]\n        )\n\n    return worker_items_shuffled\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if file_size:\n        print(f\"Worker {str(node_rank)}:\")\n        for i, w in enumerate(worker_weights):\n            print(f\"\\t{w} MB: {len(worker_items[i])}\")\n    else:\n        print(f\"Worker {str(node_rank)}:\")\n        for i, w in enumerate(worker_weights):\n            print(f\"\\t{w} items: {len(worker_items[i])}\")\n\n    # Shuffle the items\n    for i in range(len(worker_items)):\n        np.random.shuffle(worker_items[i])\n\n    # Shuffle the workers\n    np.random.shuffle(worker_ids_this_node)\n\n    # Shuffle the items within the workers\n    for i in worker_ids_this_node:\n        np.random.shuffle(worker_items[i])\n\n    return [worker_items[i] for i in worker_ids_this_node]\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if file_size:\n        print(f\"Worker {str(node_rank)} will process {len(worker_items)} items.\")\n        print(f\"Worker {str(node_rank)} will process {sum(worker_weights) / 1000000} MB of data.\")\n    else:\n        print(f\"Worker {str(node_rank)} will process {len(worker_items)} items.\")\n        print(f\"Worker {str(node_rank)} will process {sum(worker_weights)} of weight.\")\n\n    # Shuffle the items to avoid biasing the workers.\n    worker_items = [item for _, item in sorted(zip(worker_weights, worker_items))]\n    worker_weights = sorted(worker_weights)\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if file_size:\n        print(f\"Worker {str(node_rank)} is processing {len(worker_items)} items.\")\n    else:\n        print(f\"Worker {str(node_rank)} is processing {sum(worker_weights)} items.\")\n\n    # Shuffle the items and weights for each worker\n    for worker_id in worker_ids_this_node:\n        shuffled_worker_items = worker_items[worker_id]\n        shuffled_worker_weights = worker_weights[worker_id]\n        worker_items[worker_id] = [shuffled_worker_items[i] for i in np.random.permutation(len(shuffled_worker_items))]\n        worker_weights[worker_id] = [\n            shuffled_worker_weights[i] for i in np.random.permutation(len(shuffled_worker_weights))\n        ]\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Print the distribution details for the workers on the current node\n    print(f\"Worker {str(_get_node_rank() * num_workers)} is distributing items to {num_workers} workers.\")\n\n    # Print the distribution details for the workers on the current node\n    for worker_id, worker_items in enumerate(worker_items):\n        if file_size:\n            print(\n                f\"Worker {str(_get_node_rank() * num_workers)} is distributing {len(worker_items)} items to worker {worker_id}.\"\n                f\" Total size: {round(sum(worker_weights[worker_id]) / 1000 / 1000, 2)} MB.\"\n            )\n        else:\n            print(\n                f\"Worker {str(_get_node_rank() * num_workers)} is distributing {len(worker_items)} items to worker {worker_id}.\"\n                f\" Total weight: {sum(worker_weights[worker_id])}.\"\n            )\n\n    # Shuffle the items for each worker\n    for worker_id, worker_items in enumerate(worker_items):\n        np.random.shuffle(worker_items)\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    worker_items_per_node = [[] for _ in range(num_nodes)]\n    for worker_index, worker_item in enumerate(worker_items):\n        node_index = worker_index // num_workers\n        worker_items_per_node[node_index].append(worker_item)\n\n    for worker_index, worker_item in enumerate(worker_items):\n        node_index = worker_index // num_workers\n        if file_size:\n            print(\n                f\"Worker {node_index * num_workers + worker_index} has {len(worker_item)} items \"\n                f\"with a total size of {round(sum(worker_weights[i] for i in worker_item) / 1000 / 1000, 2)} MB.\"\n            )\n        else:\n            print(f\"Worker {node_index * num_workers + worker_index} has {len(worker_item)} items.\")\n\n    return worker_items_per_node\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if file_size:\n        print(f\"Worker {str(_get_node_rank() * num_workers + worker_index)} is distributing {len(worker_items)} items.\")\n    else:\n        print(f\"Worker {str(_get_node_rank() * num_workers + worker_index)} is distributing {sum(worker_weights)} items.\")\n\n    for worker_item, worker_weight in zip(worker_items, worker_weights):\n        worker_item.sort(key=lambda x: weights[x], reverse=True)\n        worker_item.sort(key=lambda x: os.path.getsize(user_items[x]) if file_size else 0, reverse=True)\n        worker_item.sort(key=lambda x: worker_weight, reverse=True)\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Get the total weight of the items\n    total_weight = sum(weights)\n\n    # Print the distribution details for workers on the current node\n    for worker_id in worker_ids_this_node:\n        worker_items_this_node = worker_items[worker_id]\n        worker_weights_this_node = worker_weights[worker_id]\n\n        if file_size:\n            worker_items_this_node_sizes = [\n                os.path.getsize(item) / (1024 * 1024) for item in worker_items_this_node\n            ]\n            worker_items_this_node_sizes_str = \", \".join(\n                [f\"{round(size, 2)} MB\" for size in worker_items_this_node_sizes]\n            )\n            print(\n                f\"Worker {worker_id} (node {node_rank}): {len(worker_items_this_node)} items, \"\n                f\"total size: {round(sum(worker_items_this_node_sizes), 2)} MB, \"\n                f\"items: {worker_items_this_node_sizes_str}\"\n            )\n        else:\n            worker_items_this_node_sizes_str = \", \".join(\n                [f\"{round(weight / total_weight * 100, 2)} %\" for weight in worker_weights_this_node]\n            )\n            print(\n                f\"Worker {worker_id} (node {node_rank}): {len(worker_items_this_node)} items, \"\n                f\"total weight: {round(sum(worker_weights_this_node) / total_weight * 100, 2)} %, \"\n                f\"items: {worker_items_this_node_sizes_str}\"\n            )\n\n    # Shuffle the items for each worker\n    for worker_id in worker_ids_this_node:\n        worker_items_this_node = worker_items[worker_id]\n        worker_weights_this_node"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if file_size:\n        print(f\"Worker {str(node_rank)} is distributing {len(worker_items)} items.\")\n    else:\n        print(f\"Worker {str(node_rank)} is distributing {sum(worker_weights)} items.\")\n\n    # Shuffle the items to avoid biasing the distribution.\n    for i, (item, weight) in enumerate(zip(worker_items, worker_weights)):\n        if file_size:\n            print(\n                f\"Worker {str(node_rank)} is distributing {len(item)} items with a total size of {round(weight / 1000 / 1000, 2)} MB.\"\n            )\n        else:\n            print(f\"Worker {str(node_rank)} is distributing {len(item)} items with a total weight of {weight}.\")\n\n        random.Random(i).shuffle(item)\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if file_size:\n        print(f\"Worker {node_rank}: {len(worker_items)} items to process, {sum(worker_weights) / 1000 / 1000} MB\")\n    else:\n        print(f\"Worker {node_rank}: {len(worker_items)} items to process, {sum(worker_weights)}\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if file_size:\n        print(f\"Worker {str(node_rank)} is distributing {len(worker_items)} items.\")\n        print(f\"Worker {str(node_rank)} is distributing {sum(worker_weights)} items.\")\n    else:\n        print(f\"Worker {str(node_rank)} is distributing {len(worker_items)} items.\")\n        print(f\"Worker {str(node_rank)} is distributing {sum(worker_weights)} bytes.\")\n\n    # Shuffle the items to avoid having the same items on the same worker.\n    worker_items = [\n        worker_items[i]\n        for i in np.random.RandomState(seed=random.randint(0, 100000000)).choice(\n            len(worker_items), len(worker_items), replace=False\n        )\n    ]\n\n    # Shuffle the weights to avoid having the same items on the same worker.\n    worker_weights = [\n        worker_weights[i]\n        for i in np.random.RandomState(seed=random.randint(0, 100000000)).choice(\n            len(worker_weights), len(worker_weights), replace=False\n        )\n    ]\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if file_size:\n        print(f\"Worker {str(node_rank)}: {len(worker_items)} items ({sum(worker_weights) / 1000 / 1000:.2f} MB)\")\n    else:\n        print(f\"Worker {str(node_rank)}: {len(worker_items)} items ({sum(worker_weights)} weights)\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Print the distribution details for workers on the current node\n    print(f\"Worker {str(node_rank)} is processing {len(worker_items)} items.\")\n\n    if file_size:\n        total_size = sum([sum([os.path.getsize(path) for path in item]) for item in worker_items])\n        print(f\"Total size: {total_size / 1000 / 1000} MB\")\n    else:\n        total_weight = sum(worker_weights)\n        print(f\"Total weight: {total_weight}\")\n\n    # Shuffle the items for each worker\n    for worker_item in worker_items:\n        random.Random(42).shuffle(worker_item)\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Create a list of tuples containing the worker index and the number of items assigned to it\n    worker_items_and_counts = [(i, len(worker_items[i])) for i in range(len(worker_items))]\n\n    # Sort the list of tuples based on the number of items assigned to each worker\n    worker_items_and_counts.sort(key=lambda x: x[1], reverse=True)\n\n    # Create a list of lists, where each sublist contains the items assigned to a worker\n    worker_items = [worker_items[i] for i in range(len(worker_items))]\n\n    # Create a list of lists, where each sublist contains the weights associated with the items assigned to a worker\n    worker_weights = [worker_weights[i] for i in range(len(worker_weights))]\n\n    # Shuffle the items within each worker\n    for worker_items_and_count in worker_items_and_counts:\n        worker_index, count = worker_items_and_count\n        worker_items[worker_index] = np.random.choice(worker_items[worker_index], count, p=worker_weights[worker_index])\n\n    # Shuffle the workers\n    np.random.shuffle(worker_items)\n\n    # Print the distribution details for workers on the current node\n    if file_size:\n        for worker_index in worker_ids_this_node:\n            print(f\"Worker {worker_index} has {len(worker_items[worker_index])} items.\")\n            print(f\"Worker {worker_index} has {sum(worker_weights[worker_index]) / 1000 / 1000} MB.\")\n    else:\n        for worker_index in worker_ids_this_node:\n            print(f\"Worker {worker_index} has {len(worker_items[worker_index])} items.\")\n            print(f\"Worker {worker_index} has {sum(worker_weights[worker_index])} total weight.\")\n\n    return worker_items"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # TODO: Consider using a dictionary to map the items to the workers.\n    # This will allow us to avoid the shuffle.\n    worker_items = _map_items_to_workers_sequentially(num_workers, user_items)\n\n    # TODO: Consider using a dictionary to map the items to the workers.\n    # This will allow us to avoid the shuffle.\n    worker_weights = _map_items_to_workers_sequentially(num_workers, weights)\n\n    # TODO: Consider using a dictionary to map the items to the workers.\n    # This will allow us to avoid the shuffle.\n    worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n    # Shuffle the items to distribute the work evenly\n    if file_size:\n        worker_items = [sorted(items, key=lambda x: os.path.getsize(x), reverse=True) for items in worker_items]\n    else:\n        worker_items = [sorted(items, key=lambda x: weights[x], reverse=True) for items in worker_items]\n\n    # Print the distribution details for workers on the current node\n    for worker_id, items, weights in zip(worker_ids_this_node, worker_items, worker_weights):\n        print(f\"Worker {worker_id} has {len(items)} items with a total weight of {sum(weights)}\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    # 1. Calculate the number of items each worker should process\n    items_per_worker = len(user_items) // world_size\n    remainder = len(user_items) % world_size\n\n    # 2. Distribute the remainder among the last workers\n    items_per_worker_with_remainder = [items_per_worker + 1] * remainder + [items_per_worker] * (\n        world_size - remainder\n    )\n\n    # 3. Calculate the start and end indices for each worker's items\n    start_indices = np.cumsum([0] + items_per_worker_with_remainder)\n    end_indices = np.cumsum(items_per_worker_with_remainder)\n\n    # 4. Ensure the output list has a length equal to the number of workers\n    if len(start_indices) != num_workers:\n        raise RuntimeError(\"The output list should have a length equal to the number of workers.\")\n\n    # 5. Return a list of lists, where each sublist contains the items assigned to a worker\n    return [user_items[start_indices[i] : end_indices[i]] for i in range(num_workers)]\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    if num_workers <= 0:\n        raise ValueError(\"The number of workers should be greater than 0.\")\n\n    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    # 1. Calculate the number of items to process per worker\n    num_items_per_worker = (len(user_items) + world_size - 1) // world_size\n\n    # 2. Calculate the start and end indices for each worker's items\n    start_indices = np.arange(0, len(user_items), num_items_per_worker)\n    end_indices = np.minimum(start_indices + num_items_per_worker, len(user_items))\n\n    # 3. Create a list of lists, where each sublist contains the items assigned to a worker\n    worker_items = [\n        user_items[start_indices[i] : end_indices[i]] for i in range(world_size) if start_indices[i] < end_indices[i]\n    ]\n\n    # 4. Ensure the output list has a length equal to the number of workers\n    if len(worker_items) != world_size:\n        raise RuntimeError(\n            f\"The number of workers ({world_size}) doesn't match the number of items ({len(user_items)}).\"\n        )\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    # Distribute the items evenly among the workers\n    num_items_per_worker = int(len(user_items) / world_size)\n    remainder = len(user_items) % world_size\n\n    worker_items = []\n    for worker_id in range(world_size):\n        start = num_items_per_worker * worker_id\n        end = start + num_items_per_worker\n\n        if worker_id < remainder:\n            end += 1\n\n        worker_items.append(user_items[start:end])\n\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\"The number of worker items doesn't match the number of workers.\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n    num_items = len(user_items)\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = num_items // world_size\n    remainder = num_items % world_size\n\n    # Distribute the remainder among the last workers\n    num_items_per_worker_with_remainder = [num_items_per_worker + 1] * remainder + [num_items_per_worker] * (\n        world_size - remainder\n    )\n\n    # Calculate the start and end indices for each worker's items\n    start_indices = np.cumsum([0] + num_items_per_worker_with_remainder[:-1])\n    end_indices = np.cumsum(num_items_per_worker_with_remainder)\n\n    # Create the list of worker items\n    worker_items = [user_items[start:end] for start, end in zip(start_indices, end_indices)]\n\n    # Ensure the output list has a length equal to the number of workers\n    if len(worker_items) != world_size:\n        raise RuntimeError(\n            f\"The output list has a length of {len(worker_items)} but the number of workers is {world_size}.\"\n        )\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n    num_items = len(user_items)\n    num_items_per_worker = num_items // world_size\n    remainder = num_items % world_size\n\n    # 1. Distribute the items to the workers\n    worker_items = [\n        user_items[i * num_items_per_worker : (i + 1) * num_items_per_worker] for i in range(world_size)\n    ]\n\n    # 2. Distribute the remainder to the last workers\n    for i in range(remainder):\n        worker_items[i + num_items_per_worker * world_size].append(user_items[num_items - remainder + i])\n\n    # 3. Ensure the output list has a length equal to the number of workers\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\n            f\"The output list has a length of {len(worker_items)} but the number of workers is {num_workers}.\"\n        )\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n    num_items = len(user_items)\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = num_items // world_size\n    num_items_remainder = num_items % world_size\n\n    # Distribute the remainder among the workers starting from the end of the list\n    worker_items = []\n    for worker_id in range(world_size):\n        start_index = num_items_per_worker * worker_id\n        end_index = start_index + num_items_per_worker\n        if worker_id < num_items_remainder:\n            end_index += 1\n        worker_items.append(user_items[start_index:end_index])\n\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\"The number of workers should match the number of worker items.\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n    num_items_per_worker = len(user_items) // world_size\n    num_items_per_worker_remainder = len(user_items) % world_size\n\n    worker_items = [[] for _ in range(num_workers)]\n\n    for worker_id in range(world_size):\n        worker_items[worker_id % num_workers].extend(\n            user_items[\n                (worker_id * num_items_per_worker)\n                + min(worker_id, num_items_per_worker_remainder) : (worker_id + 1) * num_items_per_worker\n                + min(worker_id + 1, num_items_per_worker_remainder)\n            ]\n        )\n\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\n            f\"The number of workers ({num_workers}) doesn't match the number of workers per node ({world_size}).\"\n        )\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    num_items = len(user_items)\n    num_items_per_worker = num_items // num_nodes\n    remainder = num_items % num_nodes\n\n    # 1. Calculate the number of items each worker should process\n    if num_workers > num_nodes:\n        num_items_per_worker = num_items // num_workers\n        remainder = num_items % num_workers\n    else:\n        num_items_per_worker = num_items_per_worker // num_workers\n        remainder = remainder // num_workers\n\n    # 2. Adjust for any remainder\n    if remainder > 0:\n        num_items_per_worker += 1\n        remainder -= 1\n\n    # 3. Distribute the items to the workers\n    workers_user_items = []\n    start = 0\n    for worker_index in range(num_workers):\n        end = start + num_items_per_worker\n        if remainder > 0:\n            end += 1\n            remainder -= 1\n        workers_user_items.append(user_items[start:end])\n        start = end\n\n    # 4. Ensure the output list has a length equal to the number of workers\n    if len(workers_user_items) != num_workers:\n        raise RuntimeError(\"The output list has a length different from the number of workers.\")\n\n    return workers_user_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    # Determine how many items each worker should process\n    num_items_per_worker = len(user_items) // world_size\n    remainder = len(user_items) % world_size\n    num_items_per_worker_list = [num_items_per_worker] * world_size\n\n    # Distribute the remainder to the last workers\n    for i in range(remainder):\n        num_items_per_worker_list[i] += 1\n\n    # Calculate the start and end indices for each worker's items\n    start_indices = np.cumsum([0] + num_items_per_worker_list[:-1])\n    end_indices = np.cumsum(num_items_per_worker_list)\n\n    # Create a list of lists, where each sublist contains the items assigned to a worker\n    worker_items = [\n        user_items[start_indices[node_rank * num_workers + i] : end_indices[node_rank * num_workers + i]]\n        for i in range(num_workers)\n    ]\n\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\n            f\"The number of workers ({num_workers}) doesn't match the number of workers per node ({world_size}).\"\n        )\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // world_size\n    num_items_remainder = len(user_items) % world_size\n    num_items_per_worker_with_remainder = num_items_per_worker + 1\n\n    # Distribute the remainder among the last workers\n    num_items_per_worker_with_remainder_per_worker = [\n        num_items_per_worker_with_remainder if i < num_items_remainder else num_items_per_worker\n        for i in range(world_size)\n    ]\n\n    # Calculate the start and end indices for each worker's items\n    worker_start_indices = np.cumsum([0] + num_items_per_worker_with_remainder_per_worker[:-1])\n    worker_end_indices = np.cumsum(num_items_per_worker_with_remainder_per_worker)\n\n    # Check that the output list has the expected length\n    assert len(worker_start_indices) == len(worker_end_indices) == world_size\n\n    # Split the user items into sublists based on the start and end indices\n    worker_user_items = [\n        user_items[worker_start_indices[worker_id] : worker_end_indices[worker_id]]\n        for worker_id in range(world_size)\n    ]\n\n    # Ensure the output list has the expected length\n    assert len(worker_user_items) == world_size\n\n    return worker_user_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n    num_items_per_worker = len(user_items) // world_size\n    num_items_remainder = len(user_items) % world_size\n    num_items_per_worker_with_remainder = num_items_per_worker + 1 if num_items_remainder > 0 else num_items_per_worker\n\n    # Distribute the remainder items to the last workers\n    if num_items_remainder > 0:\n        num_items_per_worker_with_remainder = num_items_per_worker + 1\n        num_items_remainder_per_worker = num_items_remainder // num_items_remainder\n        num_items_remainder_per_worker_with_remainder = (\n            num_items_remainder_per_worker + 1 if num_items_remainder % num_items_remainder > 0 else num_items_remainder\n        )\n        for worker_index in range(num_items_remainder):\n            start = num_items_per_worker * world_size + worker_index * num_items_remainder_per_worker\n            end = start + num_items_remainder_per_worker_with_remainder\n            user_items[start:end]\n\n    # Distribute the items to the workers based on number of nodes and node rank.\n    worker_items = []\n    for worker_index in range(world_size):\n        start = worker_index * num_items_per_worker_with_remainder\n        end = start + num_items_per_worker_with_remainder\n        worker_items.append(user_items[start:end])\n\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\n            f\"The number of workers {len(worker_items)} is different from the expected number of workers {num_workers}.\"\n       "}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    if len(user_items) < world_size:\n        raise RuntimeError(\n            f\"The number of items is lower than the number of workers per node ({num_workers}) times the number of nodes ({num_nodes}).\"\n        )\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // world_size\n    num_items_remainder = len(user_items) % world_size\n\n    # Distribute the remainder to the last workers\n    num_items_per_worker_with_remainder = num_items_per_worker + 1 if num_items_remainder > 0 else num_items_per_worker\n\n    # Calculate the start and end indices for each worker\n    worker_start_indices = np.cumsum([0] + [num_items_per_worker] * (world_size - 1) + [num_items_remainder])\n    worker_end_indices = np.cumsum([num_items_per_worker] * (world_size - 1) + [num_items_remainder])\n\n    # Create a list of worker indices\n    worker_indices = list(range(world_size))\n\n    # Shuffle the worker indices\n    np.random.shuffle(worker_indices)\n\n    # Assign the items to the workers based on the start and end indices\n    worker_items = [\n        user_items[worker_start_indices[i] : worker_end_indices[i]] for i in worker_indices\n    ]\n\n    # Ensure the output list has a length equal to the number of workers\n    if len(worker_items) != world_size:\n        raise RuntimeError(\n            f\"The output list has a length of {len(worker_items)} but should have a length of {world_size}.\"\n        )\n\n    return worker_items"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    worker_items = [[] for _ in range(world_size)]\n    worker_items_cumsum = [0]\n    for i, item in enumerate(user_items):\n        worker_items_cumsum.append(worker_items_cumsum[-1] + 1)\n        worker_items[i % world_size].append(item)\n\n    for i in range(world_size - 1, 0, -1):\n        if worker_items_cumsum[i] < worker_items_cumsum[i - 1]:\n            worker_items[i].append(user_items[worker_items_cumsum[i - 1] - 1])\n            worker_items_cumsum[i] += 1\n\n    if len(worker_items) != num_workers:\n        raise RuntimeError(f\"The number of workers {num_workers} doesn't match the number of items {len(worker_items)}.\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    if len(user_items) < world_size:\n        raise RuntimeError(\n            f\"The number of items ({len(user_items)}) is lower than the number of workers ({world_size}).\"\n        )\n\n    # 4. Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // world_size\n    num_items_per_worker_remainder = len(user_items) % world_size\n\n    # 5. Adjust for any remainder\n    if num_items_per_worker_remainder > 0:\n        num_items_per_worker += 1\n        num_items_per_worker_remainder -= 1\n        for i in range(num_items_per_worker_remainder):\n            user_items[i + world_size * num_items_per_worker - num_items_per_worker_remainder] = None\n\n    # 6. Distribute the items to the workers\n    worker_items = [[] for _ in range(world_size)]\n    for i, item in enumerate(user_items):\n        worker_items[i % world_size].append(item)\n\n    # 7. Check that the output list has the same length as the number of workers\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\n            f\"The number of workers ({num_workers}) is not equal to the number of worker items ({len(worker_items)}).\"\n        )\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    if num_workers < 1:\n        raise ValueError(\"The num_workers should be greater than 0.\")\n\n    if not user_items:\n        raise ValueError(\"The user_items should not be empty.\")\n\n    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // world_size\n    num_remainder = len(user_items) % world_size\n\n    # Distribute the remainder among the last workers\n    num_items_per_worker += num_remainder // (world_size - num_remainder)\n\n    # Adjust the number of items per worker for the last workers\n    num_items_per_worker += num_remainder % (world_size - num_remainder)\n\n    # Calculate the start and end indices for each worker's items\n    worker_start_indices = np.cumsum([0] + [num_items_per_worker] * (world_size - 1))\n    worker_end_indices = np.cumsum([num_items_per_worker] * world_size)\n\n    # Ensure the output list has a length equal to the number of workers\n    if len(worker_start_indices) != num_workers:\n        raise RuntimeError(\n            f\"The output list has a length {len(worker_start_indices)}\"\n            f\" which is not equal to the number of workers {num_workers}.\"\n        )\n\n    # Assign the items to the workers based on their indices\n    worker_items = [\n        user_items[worker_start_indices[worker_id] : worker_end_indices[worker_id]]\n        for worker_id in range(num_workers)\n    ]\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    if len(user_items) == 0:\n        raise ValueError(\"The provided user_items should not be empty.\")\n\n    if len(user_items) < world_size:\n        raise ValueError(\n            f\"The provided user_items should have at least {world_size} items. \"\n            f\"The provided user_items has {len(user_items)} items.\"\n        )\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // world_size\n    num_items_per_worker_remainder = len(user_items) % world_size\n\n    # Distribute the remainder among the last workers\n    num_items_per_worker_remainder_per_worker = num_items_per_worker_remainder // num_workers\n    num_items_per_worker_remainder_per_worker_remainder = num_items_per_worker_remainder % num_workers\n\n    # Initialize a list to hold the worker items\n    worker_items = [[] for _ in range(world_size)]\n\n    # Distribute the items evenly among the workers\n    for worker_id in range(world_size):\n        start_index = worker_id * num_items_per_worker\n        end_index = start_index + num_items_per_worker\n\n        # Handle the remainder\n        if worker_id < num_items_per_worker_remainder_per_worker_remainder:\n            end_index += 1\n\n        worker_items[worker_id] = user_items[start_index:end_index]\n\n    # Distribute the remainder among the last workers\n    for worker_id in range(num_items_per_worker_remainder_per_worker_remainder, num_workers):\n        worker_items[worker_id].append(user_items[-1])\n\n    # Remove any"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    if not num_workers:\n        raise ValueError(\"The number of workers should be greater than 0.\")\n\n    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = (len(user_items) + world_size - 1) // world_size\n    num_items_per_worker_remainder = (len(user_items) + world_size - 1) % world_size\n\n    # Adjust for any remainder by adding extra items to the workers starting from the end of the list\n    adjusted_num_items_per_worker = [num_items_per_worker] * world_size\n    for i in range(num_items_per_worker_remainder):\n        adjusted_num_items_per_worker[i] += 1\n\n    # Use cumulative sum to efficiently calculate the start and end indices for each worker's items\n    cumulative_sum = np.cumsum(adjusted_num_items_per_worker)\n    start_indices = [0] + list(cumulative_sum[:-1])\n    end_indices = list(cumulative_sum)\n\n    # Ensure the output list has a length equal to the number of workers\n    if len(start_indices) != num_workers:\n        raise RuntimeError(\"The output list has a length different from the number of workers.\")\n\n    # Assign the items to the workers based on the start and end indices\n    worker_items = [\n        user_items[start_indices[worker_index] : end_indices[worker_index]] for worker_index in range(num_workers)\n    ]\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    if num_workers <= 1:\n        return [user_items]\n\n    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    # The number of items each worker should process\n    num_items_per_worker = len(user_items) // world_size\n\n    # The remainder of the items that should be distributed to the last workers\n    remainder = len(user_items) % world_size\n\n    # The start and end indices for each worker's items\n    start_indices = np.cumsum([num_items_per_worker] * world_size)\n    end_indices = np.cumsum([num_items_per_worker] * world_size) + np.arange(world_size)\n    end_indices[-remainder:] += remainder\n\n    # The start and end indices for the current worker's items\n    start_index = start_indices[node_rank * num_workers]\n    end_index = end_indices[node_rank * num_workers]\n\n    # Check that the output list has the correct length\n    assert end_index - start_index == len(user_items) // world_size\n\n    # Distribute the items to the workers\n    return [user_items[start_index:end_index]]\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n    items_per_worker = len(user_items) // world_size\n    remainder = len(user_items) % world_size\n    start_indices = np.cumsum([items_per_worker] * world_size)\n    start_indices = np.insert(start_indices, 0, 0)\n    end_indices = np.cumsum([items_per_worker + 1] * world_size)\n    end_indices[-1] = len(user_items)\n    end_indices = np.insert(end_indices, 0, 0)\n    worker_items = [user_items[start_indices[i] : end_indices[i]] for i in range(world_size)]\n\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\n            f\"The number of workers ({num_workers}) doesn't match the number of workers per node ({world_size}).\"\n        )\n\n    for i, worker_item in enumerate(worker_items):\n        if len(worker_item) == 0:\n            raise RuntimeError(\n                f\"The worker {i} didn't get any items. It is likely that the number of items ({len(user_items)}) is \"\n                f\"not divisible by the number of workers per node ({world_size}).\"\n            )\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    if num_workers <= 0:\n        raise ValueError(\"The num_workers should be a positive integer.\")\n\n    if num_workers > _get_num_nodes() * _get_node_rank():\n        raise ValueError(\n            f\"The num_workers should be less than the number of nodes * node rank. \"\n            f\"Found num_workers={num_workers} and num_nodes={_get_num_nodes()} * node_rank={_get_node_rank()}.\"\n        )\n\n    # 1. Calculate the number of items each worker should process\n    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    num_items_per_worker = len(user_items) // (num_nodes * num_workers)\n    num_items_per_worker_remainder = len(user_items) % (num_nodes * num_workers)\n\n    # 2. Distribute the remainder to the first workers\n    num_items_per_worker_remainder_distributed = num_items_per_worker_remainder // num_nodes\n    num_items_per_worker_remainder_remainder = num_items_per_worker_remainder % num_nodes\n\n    # 3. Create a list of lists to store the items assigned to each worker\n    worker_items = [[] for _ in range(num_nodes * num_workers)]\n\n    # 4. Distribute the items to the workers\n    start = 0\n    for i in range(num_nodes * num_workers):\n        # 4.1. Calculate the start and end indices for the current worker's items\n        end = start + num_items_per_worker\n        if i < num_items_per_worker_remainder_distributed:\n            end += 1\n        elif i < num_items_per_worker_remainder_distributed + num_items_per_worker_remainder_remainder:\n            end += 1\n\n        # 4.2. Add the items to the"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        cache_dir = _get_cache_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        cache_dir = _get_cache_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        cache_dir = _get_cache_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        cache_dir = _get_cache_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        cache_dir = _get_cache_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        cache_dir = _get_cache_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        cache_dir = _get_cache_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        cache_dir = _get_cache_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        def _cleanup_cache(self) -> None:\n            cache_dir = _get_cache_dir()\n\n            # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n            if os.path.exists(cache_dir):\n                shutil.rmtree(cache_dir, ignore_errors=True)\n\n            os.makedirs(cache_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        def cleanup_cache(self):\n            \"\"\"\n            The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n            Input-Output Arguments\n            :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n            :return: No return values. This method performs operations on the filesystem but does not return any value.\n            \"\"\"\n            cache_dir = _get_cache_dir()\n\n            # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n            if os.path.exists(cache_dir):\n                shutil.rmtree(cache_dir, ignore_errors=True)\n\n            os.makedirs(cache_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        cache_dir = _get_cache_dir()\n        cache_data_dir = _get_cache_data_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        if os.path.exists(cache_data_dir):\n            shutil.rmtree(cache_data_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)\n        os.makedirs(cache_data_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        def _cleanup_cache(self):\n            \"\"\"\n            The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n            Input-Output Arguments\n            :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n            :return: No return values. This method performs operations on the filesystem but does not return any value.\n            \"\"\"\n\n            cache_dir = _get_cache_dir()\n\n            # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n            if os.path.exists(cache_dir):\n                shutil.rmtree(cache_dir, ignore_errors=True)\n\n            os.makedirs(cache_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "def _to_path(element: Any) -> str:\n    if isinstance(element, str):\n        return element\n    if isinstance(element, Path):\n        return str(element.absolute())\n    raise ValueError(f\"The provided element {element} is not a string or a path.\")\n\n\ndef _is_path(input_dir: Optional[str], element: Any) -> bool:\n    if isinstance(element, str):\n        return True\n    if isinstance(element, Path):\n        if input_dir is None:\n            return True\n        element = str(element.absolute())"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "def _to_path(element: Any) -> str:\n    if isinstance(element, str):\n        return element\n\n    if isinstance(element, Path):\n        return str(element)\n\n    raise ValueError(f\"The provided element {element} is not a string or a path.\")\n\n\ndef _is_path(input_dir: Optional[str], element: Any) -> bool:\n    if isinstance(element, Path):\n        element = str(element)\n\n    if isinstance(element, str):\n        if not element.startswith(\"/\"):\n            return False\n\n        if not element.startswith(\"/\"):\n            return False\n\n        if not element.startswith(\"/\"):\n            return False\n\n        if not element.startswith(\"/\"):\n            return False\n\n        if not element.startswith(\"/\"):\n            return False\n\n        if not element.startswith(\"/\"):\n            return False\n\n        if not element.startswith(\"/\"):\n            return False\n\n        if not element.startswith(\"/\"):\n            return False\n\n        if not element.startswith(\"/\"):\n            return False\n\n        if not element.startswith(\"/\"):\n            return False\n\n        if not element.startswith(\"/\"):\n            return False\n\n        if not element.startswith(\"/\"):\n            return False\n\n        if not element.startswith(\"/\"):\n            return False\n\n        if not element.startswith(\"/\"):\n            return False\n\n        if not element.startswith(\"/\"):\n            return False\n\n        if not element.startswith(\"/\"):\n            return False\n\n        if not element.startswith(\"/\"):\n            return False\n\n        if not element.startswith(\"/\"):\n            return False\n\n        if not element.startswith(\"/\"):\n            return False\n\n        if not element.startswith(\"/\"):\n            return False\n\n        if not element.startswith(\"/\"):\n            return False\n\n        if not element.startswith(\"/\"):\n            return False"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "def _to_path(element: Any) -> str:\n    if isinstance(element, str):\n        return element\n    if isinstance(element, Path):\n        return str(element)\n    raise ValueError(f\"The provided element {element} is not a string or a path.\")\n\n\ndef _is_path(input_dir: str, element: Any) -> bool:\n    if isinstance(element, str):\n        if element.startswith(\"s3://\"):\n            return True\n        if element.startswith(\"gs://\"):\n            return True\n        if element.startswith(\"hdfs://\"):\n            return True\n        if element.startswith(\"file://\"):\n            return True\n        if element.startswith(\"http://\"):\n            return True\n        if element.startswith(\"https://\"):\n            return True\n        return False\n\n    if isinstance(element, Path):"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "def _to_path(element: Any) -> str:\n    if isinstance(element, str):\n        return element\n    if isinstance(element, Path):\n        return str(element)\n    raise ValueError(f\"The provided element {element} is not a string or a path.\")\n\n\ndef _is_path(input_dir: str, element: Any) -> bool:\n    if isinstance(element, str):\n        return True\n    if isinstance(element, Path):\n        return True\n\n    if input_dir is None:\n        return False\n\n    if not isinstance(element, str):\n        return False\n\n    if not element.startswith(\"/\") and not element.startswith(\"./\"):"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [f.result() for f in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "class BaseWorker:\n    def __init__(\n        self,\n        worker_index: int,\n        num_workers: int,\n        node_rank: int,\n        "}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "class BaseWorker:\n    def __init__(\n        self,\n        worker_index: int,\n        num_workers: int,\n        node_rank: int,\n        "}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "class BaseWorker:\n    def __init__(\n        self,\n        worker_index: int,\n        num_workers: int,\n        node_rank: int,\n        "}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "class BaseWorker:\n    def __init__(\n        self,\n        worker_index: int,\n        num_workers: int,\n        node_rank: int,\n        "}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "class BaseWorker:\n    def __init__(\n        self,\n        worker_index: int,\n        num_workers: int,\n        node_rank: int,\n        "}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "class BaseWorker:\n    def __init__(\n        self,\n        worker_index: int,\n        num_workers: int,\n        node_rank: int,\n        "}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "class BaseWorker:\n    def __init__(\n        self,\n        worker_index: int,\n        num_workers: int,\n        node_rank: int,\n        "}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "class BaseWorker:\n    def __init__(\n        self,\n        worker_index: int,\n        num_workers: int,\n        node_rank: int,\n        "}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "class BaseWorker:\n    def __init__(\n        self,\n        worker_index: int,\n        num_workers: int,\n        node_rank: int,\n        "}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "class BaseWorker:\n    def __init__(\n        self,\n        worker_index: int,\n        num_workers: int,\n        node_rank: int,\n        "}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "class BaseWorker:\n    def __init__(\n        self,\n        worker_index: int,\n        num_workers: int,\n        node_rank: int,\n        "}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "class BaseWorker:\n    def __init__(\n        self,\n        worker_index: int,\n        num_workers: int,\n        node_rank: int,"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "class BaseWorker:\n    def __init__(\n        self,\n        worker_index: int,\n        num_workers: int,\n        node_rank: int,"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "class BaseWorker:\n    def __init__(\n        self,\n        worker_index: int,\n        num_workers: int,\n        node_rank: int,"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "class BaseWorker:\n    def __init__(\n        self,\n        worker_index: int,\n        num_workers: int,\n        node_rank: int,"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    return element.startswith(input_dir) if input_dir else os.path.exists(element)\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if element.startswith(input_dir):\n        return True\n\n    if os.path.exists(element):\n        return True\n\n    return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    return os.path.exists(element) or (input_dir is not None and element.startswith(input_dir))\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    return os.path.exists(element)\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if not element:\n        return False\n\n    if input_dir:\n        if element.startswith(input_dir):\n            return True\n        if os.path.exists(element):\n            return True\n        return False\n\n    return os.path.exists(element)\n\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"n_layers must be greater than 0\"\n        assert n_neurons > 0, \"n_neurons must be greater than 0\"\n\n        if self.tcnn:\n            if n_neurons > 1000:\n                n_neurons = 1000\n            return tcnn.Network(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                n_neurons=n_neurons,\n                n_hidden_layers=n_layers,\n                activation=activation,\n                output_activation=output_activation,\n                seed=self._get_seed(),\n            )\n        else:\n            model_list = []\n            in_features = n_input_dims\n            for i in range(n_layers):\n                model_list.extend(self._get_torch_layer(\n                    in_features=in_features,\n                    out_features=n_neurons,\n                    activation_name=activation,\n                ))\n                in_features = n_neurons\n            model_list.extend(self._get_torch_layer(\n                in_features=in_features,\n                out_features=n_output_dims,\n                activation_name=output_activation,\n            ))\n            return nn.Sequential(*model_list)"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons <= 128:\n                return tcnn.NetworkWithInputEncoding(\n                    n_input_dims=n_input_dims,\n                    encoding_config={\n                        \"otype\": \"Conv1d\",\n                        \"activation\": \"ReLU\",\n                        \"output_dim\": n_neurons,\n                    },\n                    n_output_dims=n_output_dims,\n                    network_config={\n                        \"otype\": \"FullyFusedMLP\",\n                        \"activation\": activation,\n                        \"output_activation\": output_activation,\n                        \"n_neurons\": n_neurons,\n                        \"n_hidden_layers\": n_layers - 1,\n                    },\n                    seed=self._get_seed(),\n                )\n            else:\n                return tcnn.NetworkWithInputEncoding(\n                    n_input_dims=n_input_dims,\n                    encoding_config={\n                        \"otype\": \"Conv1d\",\n                        \"activation\": \"ReLU\",\n                        \"output_dim\": 128,\n                    },\n                    n_output_dims=128,\n                    network_config={\n                        \"otype\": \"FullyFusedMLP\",\n                        \"activation\": activation,\n                        \"output_activation\": activation,\n                        \"n_neurons\": n_neurons,\n                        \"n_hidden_layers\": n_layers - 2,\n                    },\n                    seed=self._get_seed(),\n                )\n        else:\n            model_list = []\n            model_list.extend(self._get_torch_layer(\n                in_features=n_input_dims,\n                out_features=n_neurons,\n                activation_name=activation,\n            ))\n            for _ in range(n_layers - 2):\n                model_list.extend(self._get_torch_layer(\n                    in_features=n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons > 1024:\n                raise ValueError(\"too many neurons\")\n            model = tinycudann.Network(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                n_neurons=n_neurons,\n                n_layers=n_layers,\n                activation=activation,\n                output_activation=output_activation,\n                seed=self._get_seed(),\n            )\n        else:\n            model_list = []\n            in_features = n_input_dims\n            for _ in range(n_layers):\n                model_list.extend(self._get_torch_layer(\n                    in_features=in_features,\n                    out_features=n_neurons,\n                    activation_name=activation,\n                ))\n                in_features = n_neurons\n\n            model_list.extend(self._get_torch_layer(\n                in_features=in_features,\n                out_features=n_output_dims,\n                activation_name=output_activation,\n            ))\n            model = nn.Sequential(*model_list)\n\n        return model"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons < 256:\n                model = tcnn.NetworkWithInputEncoding(\n                    n_input_dims=n_input_dims,\n                    encoding_config={\n                        \"otype\": \"Grid\",\n                        \"n_levels\": 1,\n                        \"n_features_per_level\": 2,\n                    },\n                    n_output_dims=n_output_dims,\n                    network_config={\n                        \"otype\": \"FullyFusedMLP\",\n                        \"activation\": activation,\n                        \"output_activation\": output_activation,\n                        \"n_neurons\": n_neurons,\n                        \"n_hidden_layers\": n_layers - 1,\n                    },\n                    seed=self._get_seed(),\n                )\n            else:\n                model = tcnn.NetworkWithInputEncoding(\n                    n_input_dims=n_input_dims,\n                    encoding_config={\n                        \"otype\": \"Hash\",\n                        \"n_levels\": 1,\n                        \"n_features_per_level\": 2,\n                    },\n                    n_output_dims=n_output_dims,\n                    network_config={\n                        \"otype\": \"FullyFusedMLP\",\n                        \"activation\": activation,\n                        \"output_activation\": output_activation,\n                        \"n_neurons\": n_neurons,\n                        \"n_hidden_layers\": n_layers - 1,\n                    },\n                    seed=self._get_seed(),\n                )\n        else:\n            model = nn.Sequential()\n            model.append(nn.Linear(n_input_dims, n_neurons))\n            model.append(self._get_torch_activation(activation))\n            for i in range(n_layers - 2):\n                model.append(nn.Linear(n_neurons, n_neurons))\n                model.append("}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            # build network with tinycudann\n            model = tc.models.MLP(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                n_neurons=n_neurons,\n                n_hidden_layers=n_layers,\n                activation=activation,\n                output_activation=output_activation,\n                seed=self._get_seed(),\n            )\n        else:\n            # build network with pytorch\n            model = nn.Sequential()\n            in_features = n_input_dims\n            for i in range(n_layers):\n                out_features = n_neurons\n                if i == n_layers - 1:\n                    out_features = n_output_dims\n                model.extend(self._get_torch_layer(\n                    in_features, out_features, activation))\n                in_features = out_features\n\n        return model"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        if self.tcnn:\n            assert n_layers > 0\n            assert n_neurons > 0\n\n            if n_neurons < 128:\n                return tcnn.NetworkWithInputEncoding(\n                    n_input_dims=n_input_dims,\n                    encoding_config={\n                        \"otype\": \"Conv1d\",\n                        \"activation\": \"ReLU\",\n                        \"output_dim\": n_neurons,\n                    },\n                    n_output_dims=n_output_dims,\n                    n_neurons=n_neurons,\n                    n_layers=n_layers,\n                    activation=activation,\n                    output_activation=output_activation,\n                    seed=self._get_seed(),\n                )\n            else:\n                return tcnn.Network(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    n_neurons=n_neurons,\n                    n_layers=n_layers,\n                    activation=activation,\n                    output_activation=output_activation,\n                    seed=self._get_seed(),\n                )\n        else:\n            assert n_layers > 0\n            assert n_neurons > 0\n\n            model_list = []\n            model_list.extend(self._get_torch_layer(n_input_dims, n_neurons, activation))\n            for _ in range(n_layers - 1):\n                model_list.extend(self._get_torch_layer(n_neurons, n_neurons, activation))\n            model_list.extend(self._get_torch_layer(n_neurons, n_output_dims, output_activation))\n\n            return nn.Sequential(*model_list)"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        if n_layers < 1:\n            raise ValueError(\"n_layers must be greater than 0\")\n        if n_neurons < 1:\n            raise ValueError(\"n_neurons must be greater than 0\")\n\n        if self.tcnn:\n            if n_neurons > 1024:\n                raise ValueError(\"n_neurons must be less than 1024 in tinycudann\")\n\n            model = tinycudann.Network()\n            model.add(tinycudann.Dense(n_input_dims, n_neurons,\n                                       activation=\"relu\", seed=self._get_seed()))\n            for _ in range(n_layers - 1):\n                model.add(tinycudann.Dense(n_neurons, n_neurons,\n                                           activation=\"relu\", seed=self._get_seed()))\n            model.add(tinycudann.Dense(n_neurons, n_output_dims,\n                                       activation=self._get_torch_activation(output_activation), seed=self._get_seed()))\n            return model\n\n        model = nn.Sequential()\n        model.append(nn.Linear(n_input_dims, n_neurons))\n        model.append(self._get_torch_activation(activation))\n        for _ in range(n_layers - 1):\n            model.append(nn.Linear(n_neurons, n_neurons))\n            model.append(self._get_torch_activation(activation))\n        model.append(nn.Linear(n_neurons, n_output_dims))\n        model.append(self._get_torch_activation(output_activation))\n        return model"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            # tinycudann\n            if n_neurons <= 128:\n                activation = \"ReLU\"\n            else:\n                activation = \"None\"\n            return tinycudann.NeuralNetwork(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                n_layers=n_layers,\n                n_neurons=n_neurons,\n                activation=activation,\n                output_activation=output_activation,\n                seed=self._get_seed(),\n            )\n        else:\n            # pytorch\n            model_list = []\n            in_features = n_input_dims\n            for i in range(n_layers):\n                out_features = n_neurons\n                if i == n_layers - 1:\n                    out_features = n_output_dims\n                model_list.extend(self._get_torch_layer(\n                    in_features=in_features,\n                    out_features=out_features,\n                    activation_name=activation,\n                ))\n                in_features = out_features\n            return nn.Sequential(*model_list)"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons > 1024:\n                raise ValueError(\"tcnn only supports up to 1024 neurons\")\n\n            model = tinycudann.Network()\n            model.add_layer(tinycudann.Dense(\n                n_input_dims, n_neurons, activation=activation, seed=self._get_seed()))\n            for _ in range(n_layers - 2):\n                model.add_layer(tinycudann.Dense(\n                    n_neurons, n_neurons, activation=activation, seed=self._get_seed()))\n            model.add_layer(tinycudann.Dense(\n                n_neurons, n_output_dims, activation=output_activation, seed=self._get_seed()))\n            return model\n        else:\n            model = nn.Sequential()\n            model.append(nn.Linear(n_input_dims, n_neurons))\n            model.append(self._get_torch_activation(activation))\n            for _ in range(n_layers - 2):\n                model.append(nn.Linear(n_neurons, n_neurons))\n                model.append(self._get_torch_activation(activation))\n            model.append(nn.Linear(n_neurons, n_output_dims))\n            model.append(self._get_torch_activation(output_activation))\n            return model\n\n    "}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            return self._get_tcnn_network(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                n_layers=n_layers,\n                n_neurons=n_neurons,\n                activation=activation,\n                output_activation=output_activation,\n            )\n\n        return self._get_torch_network(\n            n_input_dims=n_input_dims,\n            n_output_dims=n_output_dims,\n            n_layers=n_layers,\n            n_neurons=n_neurons,\n            activation=activation,\n            output_activation=output_activation,\n        )\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        if self.tcnn:\n            return self._get_tcnn_network(n_input_dims, n_output_dims, n_layers, n_neurons, activation, output_activation)\n        else:\n            return self._get_torch_network(n_input_dims, n_output_dims, n_layers, n_neurons, activation, output_activation)\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            # tinycudann\n            if n_neurons > 10000:\n                return self._get_torch_layer(n_input_dims, n_output_dims, output_activation)\n            else:\n                return nn.Sequential(\n                    nn.Input(n_input_dims),\n                    nn.Dense(n_neurons, activation=activation),\n                    *self._get_torch_layer(n_neurons, n_output_dims, output_activation),\n                )\n        else:\n            # pytorch\n            model_list = []\n            model_list.extend(self._get_torch_layer(n_input_dims, n_neurons, activation))\n            for _ in range(n_layers - 2):\n                model_list.extend(self._get_torch_layer(n_neurons, n_neurons, activation))\n            model_list.extend(self._get_torch_layer(n_neurons, n_output_dims, output_activation))\n\n            return nn.Sequential(*model_list)\n\n    "}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons <= 1024:\n                return tc.Sequential(\n                    tc.Input(n_input_dims),\n                    *self._get_tcnn_layer(n_neurons, activation),\n                    *self._get_tcnn_layer(n_output_dims, output_activation),\n                    tc.BatchNorm(),\n                    tc.Dropout(0.1),\n                    tc.Linear(n_neurons),\n                    tc.BatchNorm(),\n                    tc.Dropout(0.1),\n                    tc.Linear(n_output_dims),\n                    tc.BatchNorm(),\n                    tc.Dropout(0.1),\n                    tc.Activation(output_activation),\n                    tc.BatchNorm(),\n                    tc.Dropout(0.1),\n                )\n            else:\n                return tc.Sequential(\n                    tc.Input(n_input_dims),\n                    *self._get_tcnn_layer(n_neurons, activation),\n                    *self._get_tcnn_layer(n_neurons, activation),\n                    *self._get_tcnn_layer(n_neurons, activation),\n                    *self._get_tcnn_layer(n_output_dims, output_activation),\n                    tc.BatchNorm(),\n                    tc.Dropout(0.1),\n                    tc.Linear(n_neurons),\n                    tc.BatchNorm(),\n                    tc.Dropout(0.1),\n                    tc.Linear(n_output_dims),\n                    tc.BatchNorm(),\n                    tc.Dropout(0.1),\n                    tc.Activation(output_activation),\n                    tc.BatchNorm(),\n                    tc.Dropout(0.1),\n                )\n        else:\n            model = nn.Sequential(\n                *self._get_torch_layer"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        if n_layers <= 0:\n            raise ValueError(\"n_layers must be greater than 0\")\n        if n_neurons <= 0:\n            raise ValueError(\"n_neurons must be greater than 0\")\n\n        if self.tcnn:\n            if n_neurons > 1024:\n                raise ValueError(\"n_neurons must be less than 1024 for tcnn\")\n            if n_layers > 10:\n                raise ValueError(\"n_layers must be less than 10 for tcnn\")\n\n            # create model\n            model = tinycudann.Network()\n            model.add_input_layer(n_input_dims)\n            for i in range(n_layers):\n                model.add_fully_connected_layer(n_neurons, activation)\n            model.add_output_layer(n_output_dims, output_activation)\n\n            # set seed\n            model.set_seed(self._get_seed())\n\n            return model\n        else:\n            # create model\n            model = nn.Sequential()\n            model.append(nn.Linear(n_input_dims, n_neurons))\n            model.append(self._get_torch_activation(activation))\n            for i in range(n_layers - 1):\n                model.append(nn.Linear(n_neurons, n_neurons))\n                model.append(self._get_torch_activation(activation))\n            model.append(nn.Linear(n_neurons, n_output_dims))\n            model.append(self._get_torch_activation(output_activation))\n\n            return model"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons > 1024:\n                raise ValueError(\n                    \"tcnn does not support more than 1024 neurons per layer\")\n            if n_layers > 1:\n                raise ValueError(\n                    \"tcnn does not support more than 1 hidden layer\")\n            import tinycudann as tcnn\n            model = tcnn.NetworkWithInputEncoding(\n                n_input_dims=n_input_dims,\n                encoding_config={\n                    \"otype\": \"Grid\",\n                    \"n_steps\": n_neurons,\n                    \"n_input_dims\": n_input_dims,\n                },\n                n_output_dims=n_output_dims,\n                output_activation=output_activation,\n                seed=self._get_seed(),\n            )\n            return model\n\n        model = nn.Sequential(\n            *self._get_torch_layer(n_input_dims, n_neurons, activation),\n            *self._get_torch_layer(n_neurons, n_output_dims, output_activation),\n        )\n\n        return model"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        if n_layers < 1:\n            raise ValueError(\"n_layers must be greater than 0\")\n        if n_neurons < 1:\n            raise ValueError(\"n_neurons must be greater than 0\")\n\n        if self.tcnn:\n            if n_neurons > 1024:\n                raise ValueError(\"n_neurons must be less than 1024 for tinycudann\")\n\n            model = tinycudann.Network()\n            model.add(\n                tinycudann.Dense(\n                    n_input_dims,\n                    n_neurons,\n                    activation=tinycudann.Activation.ReLU,\n                    seed=self._get_seed(),\n                )\n            )\n            for _ in range(n_layers - 1):\n                model.add(\n                    tinycudann.Dense(\n                        n_neurons,\n                        n_neurons,\n                        activation=tinycudann.Activation.ReLU,\n                        seed=self._get_seed(),\n                    )\n                )\n            model.add(\n                tinycudann.Dense(\n                    n_neurons,\n                    n_output_dims,\n                    activation=self._get_torch_activation(output_activation),\n                    seed=self._get_seed(),\n                )\n            )\n            return model\n\n        model_list = []\n        model_list += self._get_torch_layer(\n            n_input_dims, n_neurons, activation)\n        for _ in range(n_layers - 2):\n            model_list += self._get_torch_layer(\n                n_neurons, n_neurons, activation)\n        model_list += self._get_torch_layer(\n            n_neurons, n_output_dims, output_activation)\n        return nn.Sequential(*model_list)\n\n    "}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons > 2048:\n                raise ValueError(\"tcnn does not support more than 2048 neurons\")\n            return self.get_network_tcnn(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                n_layers=n_layers,\n                n_neurons=n_neurons,\n                activation=activation,\n                output_activation=output_activation,\n            )\n\n        model = nn.Sequential()\n\n        # add input layer\n        model.append(nn.Linear(n_input_dims, n_neurons))\n\n        # add hidden layers\n        for _ in range(n_layers - 2):\n            model.append(nn.Linear(n_neurons, n_neurons))\n            model.append(self._get_torch_activation(activation))\n\n        # add output layer\n        model.append(nn.Linear(n_neurons, n_output_dims))\n        model.append(self._get_torch_activation(output_activation))\n\n        return model\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"n_layers must be greater than 0\"\n        assert n_neurons > 0, \"n_neurons must be greater than 0\"\n\n        if self.tcnn:\n            activation_type = \"relu\" if activation == \"ReLU\" else \"none\"\n            output_activation_type = \"relu\" if output_activation == \"ReLU\" else \"sigmoid\" if output_activation == \"Sigmoid\" else \"none\"\n            return tcnn.Network(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                n_neurons=n_neurons,\n                n_hidden_layers=n_layers,\n                activation=activation_type,\n                output_activation=output_activation_type,\n                seed=self._get_seed(),\n            )\n        else:\n            activation_fn = self._get_torch_activation(activation)\n            output_activation_fn = self._get_torch_activation(output_activation)\n\n            model = nn.Sequential(\n                *self._get_torch_layer(n_input_dims, n_neurons, activation)\n            )\n            for i in range(n_layers - 2):\n                model.extend(\n                    self._get_torch_layer(n_neurons, n_neurons, activation)\n                )\n            model.extend(\n                self._get_torch_layer(n_neurons, n_output_dims, output_activation)\n            )\n\n            return model"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        if n_layers < 1:\n            raise ValueError(\"n_layers must be greater than 0\")\n        if n_neurons < 1:\n            raise ValueError(\"n_neurons must be greater than 0\")\n\n        if self.tcnn:\n            if n_neurons > 1024:\n                raise ValueError(\"n_neurons must be less than 1024\")\n\n            model = nn.Sequential()\n            model.add_module(\"input\", nn.Input(n_input_dims))\n            for i in range(n_layers - 1):\n                model.add_module(f\"hidden{i}\", nn.Dense(n_neurons))\n                model.add_module(f\"activation{i}\", nn.ReLU())\n            model.add_module(\"output\", nn.Dense(\n                n_output_dims, activation=self._get_tcnn_activation(output_activation)))\n\n            return model\n\n        # PyTorch\n        model_list = []\n        for i in range(n_layers):\n            if i == 0:\n                model_list.extend(self._get_torch_layer(\n                    in_features=n_input_dims, out_features=n_neurons, activation_name=activation))\n            elif i == n_layers - 1:\n                model_list.extend(self._get_torch_layer(\n                    in_features=n_neurons, out_features=n_output_dims, activation_name=output_activation))\n            else:\n                model_list.extend(self._get_torch_layer(\n                    in_features=n_neurons, out_features=n_neurons, activation_name=activation))\n\n        return nn.Sequential(*model_list)\n\n    "}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        if n_layers <= 0:\n            raise ValueError(\"n_layers must be greater than 0\")\n        if n_neurons <= 0:\n            raise ValueError(\"n_neurons must be greater than 0\")\n\n        if self.tcnn:\n            import tinycudann as tcnn\n\n            # select the network type based on the number of neurons\n            if n_neurons <= 128:\n                network_type = \"SimpleNetwork\"\n            elif n_neurons <= 256:\n                network_type = \"SimpleNetworkWithDense\"\n            else:\n                network_type = \"SimpleNetworkWithDense\"\n\n            # create the network\n            return tcnn.EncodingNetworkWithInputEncoding(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                n_neurons=n_neurons,\n                n_hidden_layers=n_layers - 1,\n                network_type=network_type,\n                activation=activation,\n                output_activation=output_activation,\n                seed=self._get_seed(),\n            )\n        else:\n            model = nn.Sequential()\n            model.append(nn.Linear(n_input_dims, n_neurons))\n            model.append(self._get_torch_activation(activation))\n\n            for _ in range(n_layers - 2):\n                model.append(nn.Linear(n_neurons, n_neurons))\n                model.append(self._get_torch_activation(activation))\n\n            model.append(nn.Linear(n_neurons, n_output_dims))\n            model.append(self._get_torch_activation(output_activation))\n\n            return model\n\n    "}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": ""}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": ""}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": ""}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": ""}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": ""}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        \"\"\"\n        This function calculates the rolling median of a 1D signal array. It shifts the signal by a range defined by the kernel offset, computes the median of these shifted signals, and then trims the resulting median array to account for edge effects introduced by the shifting process.\n\n        Input-Output Arguments\n        :param signal: np.ndarray, The input signal array for which the rolling median is to be computed. It is used to generate shifted versions of itself, which are then used to calculate the rolling median.\n        :param kernel_offset: int, The offset for the kernel, determining how far the signal is shifted in both directions to compute the rolling median. It defines the range of shifts applied to the signal for median calculation.\n        :return: np.ndarray, The rolling median of the input signal. This array will be shorter than the input signal by 2 * kernel_offset elements due to the trimming process to remove edge effects.\n        \"\"\"\n\n        \"\"\"\n        This function calculates the rolling median of a 1D signal array. It shifts the signal by a range defined by the kernel offset, computes the median of these shifted signals, and then trims the resulting median array to account for edge effects introduced by the shifting process.\n\n        Input-Output Arguments\n        :param signal: np.ndarray, The input signal array for which the rolling median is to be computed. It is used to generate shifted versions of itself, which are then used to calculate the rolling median.\n        :param kernel_offset: int, The offset for the kernel, determining how far the signal is shifted in both directions to compute the rolling median. It defines the range of shifts applied to the signal for median calculation.\n        :return: np.ndarray, The rolling median of the input signal. This array will be shorter than the input signal by 2 * kernel_offset elements due to the trimming process to remove edge effects.\n        \"\"\"\n\n        \"\"\"\n        This function calculates the rolling median of a 1D signal array. It shifts the signal by a range defined by the kernel offset, computes the median of these shifted signals, and then trims the resulting median array to account for edge effects introduced by the shifting process.\n\n        Input-Output Arguments\n        :param signal: np.ndarray"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        kernel_size = 2 * kernel_offset + 1\n        signal_length = len(signal)\n        signal_padded = np.pad(signal, kernel_offset, mode=\"edge\")\n        signal_rolled = np.lib.stride_tricks.sliding_window_view(signal_padded, kernel_size)\n        signal_median = np.median(signal_rolled, axis=1)\n        signal_median_trimmed = signal_median[kernel_offset:-kernel_offset]\n        return signal_median_trimmed"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        kernel_size = 2 * kernel_offset + 1\n\n        signal_size = len(signal)\n        signal_padded = np.pad(signal, (kernel_offset, kernel_offset), mode=\"reflect\")\n\n        rolling_median = np.zeros(signal_size)\n\n        for i in range(signal_size):\n            rolling_median[i] = np.median(signal_padded[i : i + kernel_size])\n\n        return rolling_median[kernel_offset:-kernel_offset]"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the length of the signal and the total number of shifts\n        signal_length = len(signal)\n        num_shifts = 2 * kernel_offset + 1\n\n        # Create an empty array to store the shifted signals\n        shifted_signals = np.zeros((num_shifts, signal_length))\n\n        # Shift the signal by different amounts and store the shifted versions in the shifted_signals array\n        for i in range(num_shifts):\n            shifted_signals[i] = np.roll(signal, i - kernel_offset)\n\n        # Compute the rolling median of the shifted signals\n        rolling_median = np.median(shifted_signals, axis=0)\n\n        # Trim the rolling median to account for edge effects\n        trimmed_rolling_median = rolling_median[kernel_offset:-kernel_offset]\n\n        return trimmed_rolling_median"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the number of elements to shift the signal by\n        shift_size = 2 * kernel_offset + 1\n\n        # Initialize an empty array to store the shifted signals\n        shifted_signals = np.zeros((len(signal), shift_size))\n\n        # Shift the signal by the specified kernel offset and store the shifted signals in the shifted_signals array\n        for i in range(shift_size):\n            shifted_signals[:, i] = np.roll(signal, i - kernel_offset)\n\n        # Compute the rolling median of the shifted signals along the second axis (axis=1)\n        rolling_median = np.median(shifted_signals, axis=1)\n\n        # Trim the rolling median to remove edge effects\n        trimmed_rolling_median = rolling_median[kernel_offset:-kernel_offset]\n\n        return trimmed_rolling_median"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the length of the signal and the kernel size\n        signal_length = len(signal)\n        kernel_size = 2 * kernel_offset + 1\n\n        # Initialize an array to store the shifted signals\n        shifted_signals = np.zeros((signal_length, kernel_size))\n\n        # Shift the signal by the kernel offset and store the shifted signals in the shifted_signals array\n        for i in range(kernel_size):\n            shifted_signals[:, i] = np.roll(signal, i - kernel_offset)\n\n        # Calculate the rolling median of the shifted signals along the axis of the kernel\n        rolling_median = np.median(shifted_signals, axis=1)\n\n        # Trim the rolling median to remove edge effects introduced by the shifting process\n        trimmed_rolling_median = rolling_median[kernel_offset:-kernel_offset]\n\n        return trimmed_rolling_median"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        kernel_size = 2 * kernel_offset + 1\n        trim_size = kernel_size // 2\n\n        # Shift the signal by kernel_offset to the left and right\n        shifted_signals = np.lib.stride_tricks.sliding_window_view(signal, kernel_size)\n\n        # Compute the median of the shifted signals\n        medians = np.median(shifted_signals, axis=1)\n\n        # Trim the medians to remove edge effects\n        medians = medians[trim_size:-trim_size]\n\n        return medians"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the length of the signal and the kernel size\n        signal_length = len(signal)\n        kernel_size = 2 * kernel_offset + 1\n\n        # Initialize an empty array to store the rolling median\n        rolling_median = np.zeros(signal_length)\n\n        # Iterate over the signal, shifting it by the kernel offset and calculating the rolling median at each iteration\n        for i in range(signal_length):\n            # Calculate the start and end indices of the shifted signal\n            start_index = max(0, i - kernel_offset)\n            end_index = min(signal_length, i + kernel_offset + 1)\n\n            # Extract the shifted signal from the original signal\n            shifted_signal = signal[start_index:end_index]\n\n            # Calculate the rolling median of the shifted signal\n            rolling_median[i] = np.median(shifted_signal)\n\n        # Trim the rolling median array to remove edge effects due to the shifting process\n        rolling_median = rolling_median[kernel_offset:-kernel_offset]\n\n        return rolling_median"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the length of the signal and the total number of shifts\n        signal_length = len(signal)\n        num_shifts = 2 * kernel_offset + 1\n\n        # Initialize an empty array to store the shifted signals\n        shifted_signals = np.empty((num_shifts, signal_length))\n\n        # Generate shifted versions of the signal and store them in the shifted_signals array\n        for i in range(num_shifts):\n            shift_idx = i - kernel_offset\n            shifted_signals[i] = np.roll(signal, shift_idx)\n\n        # Compute the median of each shifted signal along the axis of the signal\n        median_signals = np.median(shifted_signals, axis=0)\n\n        # Trim the median array to remove edge effects due to shifting\n        median_trimmed = median_signals[kernel_offset:-kernel_offset]\n\n        return median_trimmed"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        \"\"\"Rolling median function.\n\n        Args:\n            signal (np.ndarray): Input signal.\n            kernel_offset (int): Kernel offset.\n\n        Returns:\n            np.ndarray: Rolling median.\n        \"\"\"\n        signal_length = len(signal)\n        kernel_size = 2 * kernel_offset + 1\n\n        # Create a matrix of shifted signals\n        shifted_signals = np.zeros((signal_length, kernel_size))\n        for i in range(kernel_size):\n            shifted_signals[:, i] = np.roll(signal, i - kernel_offset)\n\n        # Compute the median of each row of the shifted signals\n        median = np.median(shifted_signals, axis=1)\n\n        # Trim the median array to remove edge effects\n        trimmed_median = median[kernel_offset:-kernel_offset]\n\n        return trimmed_median"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Compute the length of the input signal\n        signal_length = len(signal)\n\n        # Compute the length of the shifted signals\n        shifted_signal_length = signal_length + 2 * kernel_offset\n\n        # Initialize an empty array to store the shifted signals\n        shifted_signals = np.zeros((shifted_signal_length, signal_length))\n\n        # Compute the shifted signals by shifting the input signal by a range of offsets\n        for i in range(shifted_signal_length):\n            shifted_signals[i] = np.roll(signal, i - kernel_offset)\n\n        # Compute the rolling median by taking the median of the shifted signals along the first axis\n        rolling_median = np.median(shifted_signals, axis=0)\n\n        # Trim the rolling median to remove edge effects introduced by the shifting process\n        trimmed_rolling_median = rolling_median[kernel_offset:-kernel_offset]\n\n        # Return the trimmed rolling median\n        return trimmed_rolling_median"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Initialize an empty list to store the shifted signal arrays\n        shifted_signals = []\n\n        # Shift the signal by the kernel offset in both directions\n        for shift in range(-kernel_offset, kernel_offset + 1):\n            shifted_signal = np.roll(signal, shift)\n            shifted_signals.append(shifted_signal)\n\n        # Stack the shifted signals into a single array\n        stacked_signals = np.stack(shifted_signals, axis=-1)\n\n        # Compute the median along the last axis to get the rolling median\n        rolling_median = np.median(stacked_signals, axis=-1)\n\n        # Trim the rolling median to account for edge effects\n        trimmed_median = rolling_median[kernel_offset:-kernel_offset]\n\n        return trimmed_median"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Create an array of indices to shift the signal by.\n        shift_indices = np.arange(-kernel_offset, kernel_offset + 1)\n\n        # Create a list to store the shifted signals.\n        shifted_signals = []\n\n        # Iterate over the shift indices and shift the signal by each index.\n        for shift_index in shift_indices:\n            shifted_signal = np.roll(signal, shift_index)\n            shifted_signals.append(shifted_signal)\n\n        # Convert the shifted signals to a 2D array.\n        shifted_signals = np.array(shifted_signals)\n\n        # Calculate the rolling median of the shifted signals.\n        rolling_median = np.median(shifted_signals, axis=0)\n\n        # Trim the rolling median to remove edge effects.\n        trimmed_median = rolling_median[kernel_offset:-kernel_offset]\n\n        return trimmed_median"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Compute the length of the input signal and the total number of shifts\n        signal_length = len(signal)\n        num_shifts = 2 * kernel_offset + 1\n\n        # Initialize the output array for the rolling median\n        rolling_median = np.zeros(signal_length)\n\n        # Iterate over each element in the input signal\n        for i in range(signal_length):\n            # Compute the start and end indices for the current shift window\n            start = max(0, i - kernel_offset)\n            end = min(signal_length, i + kernel_offset + 1)\n\n            # Extract the shifted signal within the current window\n            shifted_signal = signal[start:end]\n\n            # Compute the median of the shifted signal\n            rolling_median[i] = np.median(shifted_signal)\n\n        # Trim the output array to remove edge effects introduced by the shifting process\n        rolling_median = rolling_median[kernel_offset:-kernel_offset]\n\n        return rolling_median"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the number of elements in the signal array\n        n = signal.size\n\n        # Calculate the number of elements in the shifted signal array\n        n_shifted = n + 2 * kernel_offset\n\n        # Create a new array to store the shifted signal\n        signal_shifted = np.zeros(n_shifted)\n\n        # Fill the shifted signal array with the original signal values\n        signal_shifted[kernel_offset : kernel_offset + n] = signal\n\n        # Fill the first and last elements of the shifted signal array with the corresponding elements from the original signal\n        signal_shifted[:kernel_offset] = signal[0]\n        signal_shifted[kernel_offset + n :] = signal[-1]\n\n        # Calculate the rolling median by applying a median filter to the shifted signal array\n        signal_median = np.zeros(n)\n        for i in range(n):\n            signal_median[i] = np.median(signal_shifted[i : i + 2 * kernel_offset + 1])\n\n        # Trim the median array to remove edge effects introduced by the shifting process\n        signal_median = signal_median[kernel_offset:-kernel_offset]\n\n        return signal_median"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    "}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    "}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    \"\"\"\n    This function calculates the Hamming distance between two iris templates, considering an allowed rotation shift. It supports optional parameters for normalized and weighted Hamming distance calculations. The function returns the minimum Hamming distance and the corresponding rotation shift that achieves this minimum distance.\n\n    Input-Output Arguments\n    :param template_probe: IrisTemplate. The iris template from the probe, used as one of the inputs for the Hamming distance calculation.\n    :param template_gallery: IrisTemplate. The iris template from the gallery, used as the other input for the Hamming distance calculation.\n    :param rotation_shift: int. The amount of rotation allowed in the matching process, which is converted to columns for the calculation.\n    :param nm_dist: Optional[float] = None. The nonmatch distance, an optional parameter for calculating a normalized Hamming distance. Defaults to None.\n    :param weights: Optional[List[np.ndarray]] = None. A list of weights tables, an optional parameter for calculating a weighted Hamming distance. Defaults to None.\n    :return: Tuple[float, int]. The function returns a tuple containing the minimum Hamming distance and the corresponding rotation shift that achieves this minimum distance.\n    \"\"\"\n\n    \"\"\"\n    This function calculates the Hamming distance between two iris templates, considering an allowed rotation shift. It supports optional parameters for normalized and weighted Hamming distance calculations. The function returns the minimum Hamming distance and the corresponding rotation shift that achieves this minimum distance.\n\n    Input-Output Arguments\n    :param template_probe: IrisTemplate. The iris template from the probe, used as one of the inputs for the Hamming distance calculation.\n    :param template_gallery: IrisTemplate. The iris template from the gallery, used as the other input for the Hamming distance calculation.\n    :param rotation_shift: int. The amount of rotation allowed in the matching process, which is converted to columns for the calculation.\n    :param nm_dist: Optional[float] = None. The nonmatch distance, an optional parameter for calculating a normalized Hamming distance. Defaults to None.\n    :param weights: Optional[List[np.ndarray]] = None. A list of weights tables, an optional"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Check if the input templates have the same shape\n    if template_probe.irisbits.shape != template_gallery.irisbits.shape:\n        raise MatcherError(\"Iris templates have different shapes.\")\n\n    # Check if the input templates have the same number of codes\n    if template_probe.irisbits.shape[0] != template_gallery.irisbits.shape[0]:\n        raise MatcherError(\"Iris templates have different numbers of codes.\")\n\n    # Check if the input templates have the same number of codes\n    if template_probe.irisbits.shape[1] != template_gallery.irisbits.shape[1]:\n        raise MatcherError(\"Iris templates have different numbers of codes.\")\n\n    # Check if the input templates have the same number of codes\n    if template_probe.irisbits.shape[2] != template_gallery.irisbits.shape[2]:\n        raise MatcherError(\"Iris templates have different numbers of codes.\")\n\n    # Check if the input templates have the same number of codes\n    if template_probe.irisbits.shape[3] != template_gallery.irisbits.shape[3]:\n        raise MatcherError(\"Iris templates have different numbers of codes.\")\n\n    # Check if the input templates have the same number of codes\n    if template_probe.irisbits.shape[4] != template_gallery.irisbits.shape[4]:\n        raise MatcherError(\"Iris templates have different numbers of codes.\")\n\n    # Check if the input templates have the same number of codes\n    if template_probe.irisbits.shape[5] != template_gallery.irisbits.shape[5]:\n        raise MatcherError(\"Iris templates have different numbers of codes.\")\n\n    # Check if the input templates have the same number of codes\n    if template_probe.irisbits.shape[6] != template_gallery.irisbits.shape[6]:\n        raise MatcherError(\"Iris templates have different numbers of codes.\")\n\n    # Check if the input templates have the same number of codes\n    if template_probe."}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Check if the input templates have the same number of codes\n    if template_probe.iriscode.shape[0] != template_gallery.iriscode.shape[0]:\n        raise MatcherError(\"The number of iriscodes are different.\")\n\n    # Check if the input templates have the same number of codes\n    if template_probe.iriscode.shape[1] != template_gallery.iriscode.shape[1]:\n        raise MatcherError(\"The number of iriscodes are different.\")\n\n    # Check if the input templates have the same number of codes\n    if template_probe.iriscode.shape[2] != template_gallery.iriscode.shape[2]:\n        raise MatcherError(\"The number of iriscodes are different.\")\n\n    # Check if the input templates have the same number of codes\n    if template_probe.iriscode.shape[3] != template_gallery.iriscode.shape[3]:\n        raise MatcherError(\"The number of iriscodes are different.\")\n\n    # Check if the input templates have the same number of codes\n    if template_probe.iriscode.shape[4] != template_gallery.iriscode.shape[4]:\n        raise MatcherError(\"The number of iriscodes are different.\")\n\n    # Check if the input templates have the same number of codes\n    if template_probe.maskcode.shape[0] != template_gallery.maskcode.shape[0]:\n        raise MatcherError(\"The number of iriscodes are different.\")\n\n    # Check if the input templates have the same number of codes\n    if template_probe.maskcode.shape[1] != template_gallery.maskcode.shape[1]:\n        raise MatcherError(\"The number of iriscodes are different.\")\n\n    # Check if the input templates have the same number of codes\n    if template_probe.maskcode.shape[2] != template_gallery.maskcode.shape[2]:\n        raise MatcherError(\"The number of iriscodes are different.\")\n\n    # Check if the input templates have the same number of codes\n    if"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Check if the templates have the same size\n    if template_probe.irisbits.shape != template_gallery.irisbits.shape:\n        raise MatcherError(\"Templates must have the same size.\")\n\n    # Calculate the total number of bits in the iris templates\n    total_bits = template_probe.irisbits.size\n\n    # Calculate the total number of bits in the iris templates\n    total_bits = template_probe.irisbits.size\n\n    # Calculate the total number of bits in the iris templates\n    total_bits = template_probe.irisbits.size\n\n    # Calculate the total number of bits in the iris templates\n    total_bits = template_probe.irisbits.size\n\n    # Calculate the total number of bits in the iris templates\n    total_bits = template_probe.irisbits.size\n\n    # Calculate the total number of bits in the iris templates\n    total_bits = template_probe.irisbits.size\n\n    # Calculate the total number of bits in the iris templates\n    total_bits = template_probe.irisbits.size\n\n    # Calculate the total number of bits in the iris templates\n    total_bits = template_probe.irisbits.size\n\n    # Calculate the total number of bits in the iris templates\n    total_bits = template_probe.irisbits.size\n\n    # Calculate the total number of bits in the iris templates\n    total_bits = template_probe.irisbits.size\n\n    # Calculate the total number of bits in the iris templates\n    total_bits = template_probe.irisbits.size\n\n    # Calculate the total number of bits in the iris templates\n    total_bits = template_probe.irisbits.size\n\n    # Calculate the total number of bits in the iris templates\n    total_bits = template_probe.irisbits.size\n\n    # Calculate the total number of bits in the iris templates\n    total_bits = template_probe.irisbits.size\n\n    # Calculate the total number of bits in the ir"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if template_probe.iris_code.shape != template_gallery.iris_code.shape:\n        raise MatcherError(\"iris codes must be same shape\")\n\n    if template_probe.mask_code.shape != template_gallery.mask_code.shape:\n        raise MatcherError(\"mask codes must be same shape\")\n\n    if template_probe.iris_code.shape[0] != template_gallery.iris_code.shape[0]:\n        raise MatcherError(\"iris codes must have same number of rows\")\n\n    if template_probe.iris_code.shape[1] != template_gallery.iris_code.shape[1]:\n        raise MatcherError(\"iris codes must have same number of columns\")\n\n    if template_probe.mask_code.shape[0] != template_gallery.mask_code.shape[0]:\n        raise MatcherError(\"mask codes must have same number of rows\")\n\n    if template_probe.mask_code.shape[1] != template_gallery.mask_code.shape[1]:\n        raise MatcherError(\"mask codes must have same number of columns\")\n\n    if template_probe.mask_code.shape[2] != template_gallery.mask_code.shape[2]:\n        raise MatcherError(\"mask codes must have same number of channels\")\n\n    if template_probe.mask_code.shape[3] != template_gallery.mask_code.shape[3]:\n        raise MatcherError(\"mask codes must have same number of channels\")\n\n    if template_probe.iris_code.shape[2] != template_gallery.iris_code.shape[2]:\n        raise MatcherError(\"iris codes must have same number of channels\")\n\n    if template_probe.iris_code.shape[3] != template_gallery.iris_code.shape[3]:\n        raise MatcherError(\"iris codes must have same number of channels\")\n\n    if template_probe.mask_code.shape[0] != template_gallery.mask_code.shape["}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Check if the input templates have the same size\n    if template_probe.iriscode.shape != template_gallery.iriscode.shape:\n        raise MatcherError(\"Iris templates have different sizes\")\n\n    # Calculate the total size of the iris codes\n    toal_codesize = template_probe.iriscode.shape[0]\n\n    # Calculate the half width of the iris codes\n    half_width = [int(template_probe.iriscode.shape[1] / 2), int(template_gallery.iriscode.shape[1] / 2)]\n\n    # Calculate the square root of the total bit count\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        toal_codesize, half_width, weights\n    )\n\n    # Calculate the nonmatch bits for Hamming distance\n    irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n        template_probe.iriscode, template_gallery.iriscode, half_width, weights\n    )\n\n    # Calculate the Hamming distance for top iris\n    hamming_dist_top = normalized_HD(\n        irisbitcount_top, maskbitcount_top, sqrt_totalbitcount_top, nm_dist\n    ) if nm_dist else irisbitcount_top / maskbitcount_top\n\n    # Calculate the Hamming distance for bottom iris\n    hamming_dist_bot = normalized_HD(\n        irisbitcount_bot, maskbitcount_bot, sqrt_totalbitcount_bot, nm_dist\n    ) if nm_dist else irisbitcount_bot / maskbitcount_bot\n\n    # Calculate the Hamming distance for whole iris\n    hamming_dist = normalized_HD(\n        irisbitcount_top + irisbitcount_bot, maskbitcount_top + maskbitcount_bot, sqrt_totalbitcount, nm_dist\n    ) if"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Check if the templates are valid\n    if not (template_probe.is_valid() and template_gallery.is_valid()):\n        raise MatcherError(\"Invalid template.\")\n\n    # Check if the templates have the same size\n    if template_probe.size != template_gallery.size:\n        raise MatcherError(\"Templates have different sizes.\")\n\n    # Check if the rotation shift is valid\n    if rotation_shift < 0 or rotation_shift >= template_probe.size:\n        raise MatcherError(\"Invalid rotation shift.\")\n\n    # Check if the templates have the same number of codes\n    if template_probe.num_codes != template_gallery.num_codes:\n        raise MatcherError(\"Templates have different number of codes.\")\n\n    # Check if the templates have the same number of iris codes\n    if template_probe.num_iris_codes != template_gallery.num_iris_codes:\n        raise MatcherError(\"Templates have different number of iris codes.\")\n\n    # Check if the templates have the same number of mask codes\n    if template_probe.num_mask_codes != template_gallery.num_mask_codes:\n        raise MatcherError(\"Templates have different number of mask codes.\")\n\n    # Check if the templates have the same number of mask bits\n    if template_probe.num_mask_bits != template_gallery.num_mask_bits:\n        raise MatcherError(\"Templates have different number of mask bits.\")\n\n    # Check if the templates have the same number of iris bits\n    if template_probe.num_iris_bits != template_gallery.num_iris_bits:\n        raise MatcherError(\"Templates have different number of iris bits.\")\n\n    # Check if the templates have the same number of iris codes\n    if template_probe.num_iris_codes != template_gallery.num_iris_codes:\n        raise MatcherError(\"Templates have different number of iris codes.\")\n\n    # Check if the templates have the same number of mask codes\n    if template_probe.num"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Calculate the total number of iris codes\n    total_codesize = template_probe.iriscodes.shape[0] * template_probe.iriscodes.shape[1]\n\n    # Calculate the half width of the iris codes\n    half_width = [int(template_probe.iriscodes.shape[1] / 2)] * template_probe.iriscodes.shape[0]\n\n    # Calculate the square root of the total number of iris codes\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        total_codesize, half_width, weights\n    )\n\n    # Calculate the nonmatch bits for the Hamming distance\n    irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n        template_probe.iriscodes, template_gallery.iriscodes, half_width, weights\n    )\n\n    # Calculate the Hamming distance for the top iris\n    hamming_distance_top = normalized_HD(\n        irisbitcount_top, maskbitcount_top, sqrt_totalbitcount_top, nm_dist\n    ) if nm_dist else irisbitcount_top / maskbitcount_top\n\n    # Calculate the Hamming distance for the bottom iris\n    hamming_distance_bot = normalized_HD(\n        irisbitcount_bot, maskbitcount_bot, sqrt_totalbitcount_bot, nm_dist\n    ) if nm_dist else irisbitcount_bot / maskbitcount_bot\n\n    # Calculate the minimum Hamming distance and the corresponding rotation shift\n    hamming_distance = min(hamming_distance_top, hamming_distance_bot)\n    rotation_shift = rotation_shift if hamming_distance_top < hamming_distance_bot else -rotation_shift\n\n    # Return the minimum Hamming distance and the corresponding rotation shift\n    return hamming_distance, rotation_shift"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Check if the input templates are valid\n    if not isinstance(template_probe, IrisTemplate) or not isinstance(template_gallery, IrisTemplate):\n        raise MatcherError(\"Invalid input template type.\")\n\n    # Check if the rotation shift is valid\n    if rotation_shift < 0 or rotation_shift > template_probe.code_width:\n        raise MatcherError(\"Invalid rotation shift.\")\n\n    # Check if the number of iriscodes is equal\n    if template_probe.code_width != template_gallery.code_width:\n        raise MatcherError(\"The number of iriscodes is not equal.\")\n\n    # Check if the number of iriscodes is equal\n    if template_probe.code_height != template_gallery.code_height:\n        raise MatcherError(\"The number of iriscodes is not equal.\")\n\n    # Check if the number of iriscodes is equal\n    if template_probe.code_size != template_gallery.code_size:\n        raise MatcherError(\"The number of iriscodes is not equal.\")\n\n    # Check if the number of iriscodes is equal\n    if template_probe.code_num != template_gallery.code_num:\n        raise MatcherError(\"The number of iriscodes is not equal.\")\n\n    # Check if the number of iriscodes is equal\n    if template_probe.code_type != template_gallery.code_type:\n        raise MatcherError(\"The number of iriscodes is not equal.\")\n\n    # Check if the number of iriscodes is equal\n    if template_probe.code_width != template_gallery.code_width:\n        raise MatcherError(\"The number of iriscodes is not equal.\")\n\n    # Check if the number of iriscodes is equal\n    if template_probe.code_height != template_gallery.code_height:\n        raise MatcherError(\"The number of iriscodes is not equal.\")\n\n    # Check if the number of iriscodes is equal\n    if template_probe.code_size != template_gallery.code_size:\n        raise MatcherError"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Check if the input templates are of the same size\n    if template_probe.iriscodes.shape != template_gallery.iriscodes.shape:\n        raise MatcherError(\"Input templates must be of the same size.\")\n\n    # Extract the iris codes from the input templates\n    iriscodes_probe = template_probe.iriscodes\n    iriscodes_gallery = template_gallery.iriscodes\n\n    # Calculate the total amount of sqrt bits\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        template_probe.iriscodes.shape[0], template_probe.half_width, weights\n    )\n\n    # Calculate the nonmatch bits and the common mask bits for the Hamming distance calculation\n    irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n        iriscodes_probe, iriscodes_gallery, template_probe.half_width, weights\n    )\n\n    # Calculate the Hamming distance for the top iris\n    hd_top = normalized_HD(irisbitcount_top, maskbitcount_top, sqrt_totalbitcount_top, nm_dist)\n\n    # Calculate the Hamming distance for the bottom iris\n    hd_bot = normalized_HD(irisbitcount_bot, maskbitcount_bot, sqrt_totalbitcount_bot, nm_dist)\n\n    # Calculate the Hamming distance for the whole iris\n    hd_whole = normalized_HD(irisbitcount_top + irisbitcount_bot, maskbitcount_top + maskbitcount_bot, sqrt_totalbitcount, nm_dist)\n\n    # Return the minimum Hamming distance and the corresponding rotation shift\n    return min(hd_top, hd_bot, hd_whole), rotation_shift"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Check if the input templates have the same iriscode size\n    if template_probe.iriscode.shape[0] != template_gallery.iriscode.shape[0]:\n        raise MatcherError(\"The iriscode size of the two templates is not the same.\")\n\n    # Get the iriscode size\n    iriscode_size = template_probe.iriscode.shape[0]\n\n    # Calculate the half width of the iriscode\n    half_width = iriscode_size // 2\n\n    # Get the irisbits from the templates\n    irisbits_probe = template_probe.irisbits\n    irisbits_gallery = template_gallery.irisbits\n\n    # Get the maskbits from the templates\n    maskbits_probe = template_probe.maskbits\n    maskbits_gallery = template_gallery.maskbits\n\n    # Calculate the nonmatch bits and the common maskbits\n    irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n        irisbits_probe, maskbits_probe, half_width, weights\n    )\n    irisbitcount_top_gallery, maskbitcount_top_gallery, irisbitcount_bot_gallery, maskbitcount_bot_gallery = count_nonmatchbits(\n        irisbits_gallery, maskbits_gallery, half_width, weights\n    )\n\n    # Calculate the square root of the bit counts\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        iriscode_size, half_width, weights\n    )\n\n    # Calculate the Hamming distance\n    hd_top = normalized_HD(irisbitcount_top, maskbitcount_top, sqrt_totalbitcount_top, nm_dist)\n    hd_bot = normalized_HD(irisbitcount_bot, maskbitcount_bot, sqrt_totalbitcount_bot, nm_dist)\n\n    # Calculate"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Convert the rotation shift to columns\n    columns = rotation_shift\n\n    # Check if the number of columns is valid\n    if columns < 0 or columns >= template_probe.iriscodes.shape[1]:\n        raise MatcherError(\n            f\"The number of columns ({columns}) is not valid. It must be between 0 and {template_probe.iriscodes.shape[1] - 1}.\"\n        )\n\n    # Extract the iris codes from the templates\n    iriscodes_probe = template_probe.iriscodes\n    iriscodes_gallery = template_gallery.iriscodes\n\n    # Calculate the Hamming distance between the iris codes\n    irisbits = np.logical_xor(iriscodes_probe, np.roll(iriscodes_gallery, columns, axis=1))\n    maskbits = np.logical_and(iriscodes_probe, iriscodes_gallery)\n\n    # Calculate the total amount of sqrt bits\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        iriscodes_probe.size, template_probe.half_width, weights\n    )\n\n    # Count nonmatch bits\n    irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n        irisbits, maskbits, template_probe.half_width, weights\n    )\n\n    # Calculate the Hamming distance\n    hamming_distance_top = normalized_HD(\n        irisbitcount_top, maskbitcount_top, sqrt_totalbitcount_top, nm_dist\n    ) if nm_dist else irisbitcount_top / maskbitcount_top\n    hamming_distance_bot = normalized_HD(\n        irisbitcount_bot, maskbitcount_bot, sqrt_totalbitcount_bot, nm_dist\n    ) if nm_dist else irisbitcount_bot / maskbitcount_bot\n\n    hamming_distance = min(hamming_distance_top"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Check if the input templates have the same shape\n    if template_probe.irisbits.shape != template_gallery.irisbits.shape:\n        raise MatcherError(\n            \"Input templates must have the same shape\",\n            \"Check the input templates and try again.\",\n        )\n\n    # Check if the input templates have the same number of codes\n    if template_probe.irisbits.shape[0] != template_gallery.irisbits.shape[0]:\n        raise MatcherError(\n            \"Input templates must have the same number of codes\",\n            \"Check the input templates and try again.\",\n        )\n\n    # Calculate the half width of the iris codes\n    half_width = [int(hw) for hw in template_probe.half_width]\n\n    # Calculate the square root of the total number of bits in the iris codes\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        template_probe.irisbits.size, half_width, weights\n    )\n\n    # Calculate the nonmatch bits for the Hamming distance calculation\n    irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n        template_probe.irisbits, template_gallery.irisbits, half_width, weights\n    )\n\n    # Calculate the Hamming distance for the top iris\n    hamming_distance_top = normalized_HD(\n        irisbitcount_top, maskbitcount_top, sqrt_totalbitcount_top, nm_dist\n    ) if nm_dist else irisbitcount_top / maskbitcount_top\n\n    # Calculate the Hamming distance for the bottom iris\n    hamming_distance_bot = normalized_HD(\n        irisbitcount_bot, maskbitcount_bot, sqrt_totalbitcount_bot, nm_dist\n    ) if nm_dist else irisbitcount_bot / maskbitcount_bot\n\n    # Return the minimum Hamming distance and the corresponding rotation"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Check if the templates have the same size and code width\n    if template_probe.code_width != template_gallery.code_width:\n        raise MatcherError(\"Iris templates have different code widths.\")\n\n    # Calculate the total code size\n    total_codesize = template_probe.code_width * template_probe.code_height\n\n    # Calculate the half width of the iris templates\n    half_width = template_probe.code_width // 2\n\n    # Check if the rotation shift is within the allowed range\n    if rotation_shift < 0 or rotation_shift > half_width:\n        raise MatcherError(\"Invalid rotation shift.\")\n\n    # Check if the templates have the same code width\n    if template_probe.code_width != template_gallery.code_width:\n        raise MatcherError(\"Iris templates have different code widths.\")\n\n    # Check if the templates have the same code height\n    if template_probe.code_height != template_gallery.code_height:\n        raise MatcherError(\"Iris templates have different code heights.\")\n\n    # Check if the templates have the same number of iris codes\n    if template_probe.iris_codes.shape[0] != template_gallery.iris_codes.shape[0]:\n        raise MatcherError(\"Iris templates have different number of iris codes.\")\n\n    # Check if the templates have the same number of mask codes\n    if template_probe.mask_codes.shape[0] != template_gallery.mask_codes.shape[0]:\n        raise MatcherError(\"Iris templates have different number of mask codes.\")\n\n    # Check if the templates have the same number of mask codes\n    if template_probe.mask_codes.shape[0] != template_gallery.mask_codes.shape[0]:\n        raise MatcherError(\"Iris templates have different number of mask codes.\")\n\n    # Check if the templates have the same number of mask codes\n    if template_probe.mask_codes.shape[0] != template_gallery.mask_codes.shape[0]:"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Check if the input templates have the same size\n    if template_probe.iris_template.shape != template_gallery.iris_template.shape:\n        raise MatcherError(\n            f\"Iris templates have different sizes: {template_probe.iris_template.shape} and {template_gallery.iris_template.shape}\"\n        )\n\n    # Get the iris and mask bits from the templates\n    irisbits_probe = template_probe.iris_bits\n    irisbits_gallery = template_gallery.iris_bits\n    maskbits_probe = template_probe.mask_bits\n    maskbits_gallery = template_gallery.mask_bits\n\n    # Get the total code size from the templates\n    total_codesize = template_probe.total_codesize\n\n    # Get the half widths from the templates\n    half_width_probe = template_probe.half_width\n    half_width_gallery = template_gallery.half_width\n\n    # Count the square root of bit counts\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        total_codesize, half_width_probe, weights\n    )\n\n    # Count the nonmatch bits\n    irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n        irisbits_probe, maskbits_probe, half_width_probe, weights\n    )\n    irisbitcount_top_gallery, maskbitcount_top_gallery, irisbitcount_bot_gallery, maskbitcount_bot_gallery = count_nonmatchbits(\n        irisbits_gallery, maskbits_gallery, half_width_gallery, weights\n    )\n\n    # Calculate the Hamming distance for the top iris\n    hamming_dist_top = (\n        irisbitcount_top / maskbitcount_top\n        if nm_dist is None\n        else normalized_HD(irisbitcount_top, mask"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Check if the input templates have the same size\n    if template_probe.iris_size != template_gallery.iris_size:\n        raise MatcherError(\"Templates have different sizes\")\n\n    # Get the iriscode size from the template\n    iris_size = template_probe.iris_size\n\n    # Calculate the half width of the iriscodes\n    half_width = [int(iris_size / 4), int(iris_size / 2), int(3 * iris_size / 4)]\n\n    # Get the irisbits from the templates\n    irisbits_probe = template_probe.irisbits\n    irisbits_gallery = template_gallery.irisbits\n\n    # Get the maskbits from the templates\n    maskbits_probe = template_probe.maskbits\n    maskbits_gallery = template_gallery.maskbits\n\n    # Calculate the Hamming distance between the irisbits\n    irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n        irisbits_probe, maskbits_probe, half_width, weights\n    )\n\n    irisbitcount_top_gallery, maskbitcount_top_gallery, irisbitcount_bot_gallery, maskbitcount_bot_gallery = count_nonmatchbits(\n        irisbits_gallery, maskbits_gallery, half_width, weights\n    )\n\n    # Calculate the square root of the bit counts\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        iris_size, half_width, weights\n    )\n\n    # Calculate the Hamming distance between the irisbits\n    hamming_dist = np.sqrt(\n        irisbitcount_top + irisbitcount_top_gallery + irisbitcount_bot + irisbitcount_bot_gallery\n    ) / sqrt_totalbitcount\n\n    hamming_dist_top = np.sqrt("}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Check if the number of templates in both probe and gallery is equal to 1\n    if len(template_probe.irisbits) != 1 or len(template_gallery.irisbits) != 1:\n        raise MatcherError(\"Number of templates in probe and gallery must be equal to 1.\")\n\n    # Get the irisbits and maskbits from both probe and gallery templates\n    irisbits_probe = template_probe.irisbits[0]\n    maskbits_probe = template_probe.maskbits[0]\n    irisbits_gallery = template_gallery.irisbits[0]\n    maskbits_gallery = template_gallery.maskbits[0]\n\n    # Check if the irisbits and maskbits from both probe and gallery have the same shape\n    if irisbits_probe.shape != irisbits_gallery.shape or maskbits_probe.shape != maskbits_gallery.shape:\n        raise MatcherError(\"Irisbits and maskbits from probe and gallery must have the same shape.\")\n\n    # Get the total size of the irisbits and maskbits\n    total_codesize = irisbits_probe.size\n\n    # Check if the total size of the irisbits and maskbits from both probe and gallery are equal\n    if total_codesize != irisbits_gallery.size or total_codesize != maskbits_gallery.size:\n        raise MatcherError(\"Total size of irisbits and maskbits from probe and gallery must be equal.\")\n\n    # Get the half width of the irisbits and maskbits\n    half_width = [int(hw) for hw in total_codesize / 4]\n\n    # Count the square root of the total bit counts\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        total_codesize, half_width, weights\n    )\n\n    # Count the nonmatch bits and common mask bits for the Hamming distance calculation\n    irisbitcount_top, maskbitcount_top, irisbitcount_"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Check if the input templates are valid\n    if not (\n        isinstance(template_probe, IrisTemplate)\n        and isinstance(template_gallery, IrisTemplate)\n        and len(template_probe.iriscodes) == len(template_gallery.iriscodes)\n    ):\n        raise MatcherError(\"Invalid input templates\")\n\n    # Calculate the total code size and half width\n    total_codesize = template_probe.iriscodes[0].shape[0]\n    half_width = [int(hw) for hw in template_probe.half_width]\n\n    # Convert the rotation shift to columns\n    columns = rotation_shift\n\n    # Create a mask for the rotation\n    mask = np.eye(total_codesize, dtype=np.bool_)\n\n    # Create a list of irisbits and maskbits\n    irisbits = [\n        np.rot90(np.logical_xor(template_probe.iriscodes[i], template_gallery.iriscodes[i]), k=columns)\n        for i in range(len(template_probe.iriscodes))\n    ]\n    maskbits = [\n        np.rot90(np.logical_and(template_probe.maskcodes[i], template_gallery.maskcodes[i]), k=columns)\n        for i in range(len(template_probe.iriscodes))\n    ]\n\n    # Calculate the nonmatch bits and common maskbits\n    irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n        irisbits, maskbits, half_width, weights\n    )\n\n    # Calculate the square root of total bits\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        total_codesize, half_width, weights\n    )\n\n    # Calculate the Hamming distance\n    HD_top = normalized_HD(\n        irisbitcount_top, maskbitcount"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        first_bisectors_point = np.zeros((num_bisectors, 2))\n        second_bisectors_point = np.zeros((num_bisectors, 2))\n\n        for i in range(num_bisectors):\n            for j in range(max_iterations):\n                fst_point_idx, sec_point_idx = np.random.choice(len(polygon), 2, replace=False)\n\n                fst_point = polygon[fst_point_idx]\n                sec_point = polygon[sec_point_idx]\n\n                if np.linalg.norm(fst_point - sec_point) >= min_distance_between_sector_points_in_px:\n                    first_bisectors_point[i] = fst_point\n                    second_bisectors_point[i] = sec_point\n                    break\n\n            if np.linalg.norm(first_bisectors_point[i] - second_bisectors_point[i]) < min_distance_between_sector_points_in_px:\n                raise EyeCentersEstimationError(\n                    \"Could not find a sufficient number of point pairs that meet the distance criterion within the maximum number of iterations allowed.\"\n                )\n\n        return first_bisectors_point, second_bisectors_point"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        first_bisectors_point = np.zeros((num_bisectors, 2))\n        second_bisectors_point = np.zeros((num_bisectors, 2))\n\n        for i in range(num_bisectors):\n            first_point, second_point = self._get_random_points_from_polygon(polygon, min_distance_between_sector_points_in_px)\n\n            first_bisectors_point[i] = first_point\n            second_bisectors_point[i] = second_point\n\n        return first_bisectors_point, second_bisectors_point\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        first_bisectors_point = np.zeros((self.params.num_bisectors, 2))\n        second_bisectors_point = np.zeros((self.params.num_bisectors, 2))\n\n        for i in range(self.params.num_bisectors):\n            for j in range(self.params.max_iterations):\n                first_bisectors_point[i, :], second_bisectors_point[i, :] = self._get_bisector_points(\n                    polygon, min_distance_between_sector_points_in_px\n                )\n                if np.linalg.norm(first_bisectors_point[i, :] - second_bisectors_point[i, :]) > min_distance_between_sector_points_in_px:\n                    break\n            else:\n                raise EyeCentersEstimationError(\n                    f\"Failed to find a sufficient number of point pairs that meet the distance criterion within {self.params.max_iterations} iterations.\"\n                )\n\n        return first_bisectors_point, second_bisectors_point\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        first_bisectors_point = np.zeros((num_bisectors, 2))\n        second_bisectors_point = np.zeros((num_bisectors, 2))\n\n        for i in range(num_bisectors):\n            for j in range(max_iterations):\n                random_points = np.random.choice(polygon.shape[0], size=2, replace=False)\n                fst_point, sec_point = polygon[random_points]\n                distance = np.linalg.norm(fst_point - sec_point)\n\n                if distance > min_distance_between_sector_points_in_px:\n                    first_bisectors_point[i] = fst_point\n                    second_bisectors_point[i] = sec_point\n                    break\n\n        if np.all(first_bisectors_point == 0) or np.all(second_bisectors_point == 0):\n            raise EyeCentersEstimationError(\n                \"Could not find sufficient point pairs with distance greater than the minimum distance\"\n            )\n\n        return first_bisectors_point, second_bisectors_point"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        first_bisectors_point = np.zeros((num_bisectors, 2))\n        second_bisectors_point = np.zeros((num_bisectors, 2))\n\n        for i in range(num_bisectors):\n            for j in range(max_iterations):\n                # Choose two random points from the polygon\n                first_point = polygon[np.random.randint(0, polygon.shape[0])]\n                second_point = polygon[np.random.randint(0, polygon.shape[0])]\n\n                # Calculate the perpendicular bisector\n                bisector_slope = (second_point[1] - first_point[1]) / (second_point[0] - first_point[0])\n                bisector_intercept = first_point[1] - bisector_slope * first_point[0]\n\n                # Calculate the intersection point of the bisector with the circle\n                center_x = (bisector_intercept - bisector_intercept) / (2 * bisector_slope)\n                center_y = bisector_slope * center_x + bisector_intercept\n\n                # Check if the distance between the two points is greater than the minimum distance\n                distance = np.linalg.norm(first_point - second_point)\n                if distance > min_distance_between_sector_points_in_px:\n                    first_bisectors_point[i] = first_point\n                    second_bisectors_point[i] = second_point\n                    break\n\n            if j == max_iterations - 1:\n                raise EyeCentersEstimationError(\n                    f\"Failed to find {num_bisectors} point pairs with a distance greater than {min_distance_between_sector_points_in_px} in {max_iterations} iterations.\"\n                )\n\n        return first_bisectors_point, second_bisectors_point"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        polygon_vertices_num = polygon.shape[0]\n\n        fst_points = np.zeros((num_bisectors, 2))\n        sec_points = np.zeros((num_bisectors, 2))\n\n        for i in range(num_bisectors):\n            for j in range(max_iterations):\n                # Choose two random points from the polygon\n                random_indices = np.random.choice(polygon_vertices_num, size=2, replace=False)\n                fst_point = polygon[random_indices[0]]\n                sec_point = polygon[random_indices[1]]\n\n                # Calculate the distance between the two points\n                distance = np.linalg.norm(fst_point - sec_point)\n\n                if distance > min_distance_between_sector_points_in_px:\n                    # If the distance is greater than the minimum distance, store the points and break the loop\n                    fst_points[i] = fst_point\n                    sec_points[i] = sec_point\n                    break\n\n            else:\n                # If the loop was not broken, raise an error\n                raise EyeCentersEstimationError(\n                    f\"Could not find {num_bisectors} pairs of points with a distance greater than {min_distance_between_sector_points_in_px}.\"\n                )\n\n        return fst_points, sec_points"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        first_bisectors_point = np.zeros((num_bisectors, 2))\n        second_bisectors_point = np.zeros((num_bisectors, 2))\n\n        for i in range(num_bisectors):\n            for j in range(max_iterations):\n                # Select two random points from the polygon\n                first_point, second_point = self._select_random_points(polygon)\n\n                # Calculate the perpendicular bisector\n                bisector_point = self._calculate_perpendicular_bisector(first_point, second_point)\n\n                # Check if the distance between the two points is greater than the minimum distance\n                if self._check_distance_between_points(\n                    first_point, second_point, min_distance_between_sector_points_in_px\n                ):\n                    first_bisectors_point[i] = first_point\n                    second_bisectors_point[i] = bisector_point\n                    break\n\n        if np.count_nonzero(first_bisectors_point) != num_bisectors:\n            raise EyeCentersEstimationError(\"Could not find enough perpendicular bisectors.\")\n\n        return first_bisectors_point, second_bisectors_point\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        # Randomly choose num_bisectors pairs of points from the polygon\n        random_indices = np.random.choice(len(polygon), size=(num_bisectors, 2), replace=False)\n        random_points = polygon[random_indices]\n\n        # Calculate the perpendicular bisectors\n        bisectors = self._calculate_perpendicular_bisectors_from_points(random_points)\n\n        # Check if the distance between each pair of points is greater than min_distance_between_sector_points_in_px\n        for _ in range(max_iterations):\n            distances = np.linalg.norm(bisectors[:, 0] - bisectors[:, 1], axis=1)\n            if np.all(distances > min_distance_between_sector_points_in_px):\n                break\n\n            # If not, randomly choose another pair of points\n            random_indices = np.random.choice(len(polygon), size=(num_bisectors, 2), replace=False)\n            random_points = polygon[random_indices]\n\n            # Calculate the perpendicular bisectors\n            bisectors = self._calculate_perpendicular_bisectors_from_points(random_points)\n\n        else:\n            raise EyeCentersEstimationError(\n                f\"Failed to find {num_bisectors} point pairs with distance greater than \"\n                f\"{min_distance_between_sector_points_in_px} after {max_iterations} iterations.\"\n            )\n\n        return bisectors[:, 0], bisectors[:, 1]\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        bisectors_points = []\n\n        for _ in range(self.params.max_iterations):\n            first_bisectors_point, second_bisectors_point = self._get_random_bisectors_points(\n                polygon, min_distance_between_sector_points_in_px\n            )\n            bisectors_points.append((first_bisectors_point, second_bisectors_point))\n\n        if not bisectors_points:\n            raise EyeCentersEstimationError(\n                \"Could not find a sufficient number of point pairs that meet the distance criterion.\"\n            )\n\n        return bisectors_points[0]\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        # Randomly select points from the polygon\n        polygon_points = polygon\n        num_points = polygon_points.shape[0]\n        points_idx = np.random.choice(num_points, size=(num_bisectors, 2), replace=True)\n\n        # Ensure that the selected points are not too close to each other\n        for _ in range(max_iterations):\n            # Calculate the distances between the selected points\n            distances = np.linalg.norm(polygon_points[points_idx[:, 0]] - polygon_points[points_idx[:, 1]], axis=1)\n\n            # Check if all distances are greater than the minimum distance\n            if np.all(distances >= min_distance_between_sector_points_in_px):\n                break\n\n            # If not, select new points\n            points_idx = np.random.choice(num_points, size=(num_bisectors, 2), replace=True)\n\n        else:\n            raise EyeCentersEstimationError(\n                f\"Failed to find a sufficient number of point pairs with a distance greater than {min_distance_between_sector_points_in_px} pixels.\"\n            )\n\n        # Calculate the perpendicular bisectors\n        bisectors_start_points = polygon_points[points_idx[:, 0]]\n        bisectors_end_points = polygon_points[points_idx[:, 1]]\n\n        return bisectors_start_points, bisectors_end_points"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        for _ in range(max_iterations):\n            # Randomly choose a pair of points from the polygon\n            points = self._get_random_points(polygon, num_bisectors)\n\n            # Calculate the perpendicular bisectors\n            bisectors = self._calculate_bisectors(points)\n\n            # Check if the distance between any two points in a pair is greater than the minimum distance\n            distances = np.linalg.norm(points[:, 0] - points[:, 1], axis=1)\n            if np.all(distances > min_distance_between_sector_points_in_px):\n                return bisectors[:, 0], bisectors[:, 1]\n\n        raise EyeCentersEstimationError(\"Failed to find a sufficient number of point pairs\")\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        # Choose random points from the polygon\n        points = polygon[np.random.choice(polygon.shape[0], size=num_bisectors, replace=False)]\n\n        # Initialize the arrays for storing the bisectors\n        first_bisectors_point = np.zeros((num_bisectors, 2))\n        second_bisectors_point = np.zeros((num_bisectors, 2))\n\n        # Iterate over the chosen points\n        for i in range(num_bisectors):\n            # Choose a random point from the polygon\n            point = points[i]\n\n            # Find the closest point on the polygon to the chosen point\n            closest_point = self._find_closest_point(point, polygon)\n\n            # Calculate the perpendicular bisector of the chosen point and the closest point\n            bisector = self._calculate_perpendicular_bisector(point, closest_point)\n\n            # Find the intersection of the perpendicular bisector with the polygon\n            intersection_point = self._find_intersection_point(bisector, polygon)\n\n            # Calculate the perpendicular bisector of the intersection point and the chosen point\n            perpendicular_bisector = self._calculate_perpendicular_bisector(intersection_point, point)\n\n            # Find the intersection of the perpendicular bisector with the polygon\n            intersection_point = self._find_intersection_point(perpendicular_bisector, polygon)\n\n            # Store the intersection point as the second bisector point\n            second_bisectors_point[i] = intersection_point\n\n            # Calculate the perpendicular bisector of the intersection point and the chosen point\n            perpendicular_bisector = self._calculate_perpendicular_bisector(intersection_point, point)\n\n            # Find the intersection of the perpendicular bisector with the polygon\n            intersection_point = self._find_intersection_point(perpendicular_bisector, polygon)\n\n            # Store the intersection point as the first bisector point\n            first_bisectors_point"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        first_bisectors_point, second_bisectors_point = self._select_random_points_from_polygon(\n            polygon, min_distance_between_sector_points_in_px\n        )\n\n        for _ in range(self.params.max_iterations):\n            first_bisectors_point, second_bisectors_point = self._select_random_points_from_polygon(\n                polygon, min_distance_between_sector_points_in_px\n            )\n\n            if (\n                np.linalg.norm(first_bisectors_point - second_bisectors_point)\n                > min_distance_between_sector_points_in_px\n            ):\n                break\n\n        else:\n            raise EyeCentersEstimationError(\n                f\"Failed to find {self.params.num_bisectors} points that are at least \"\n                f\"{min_distance_between_sector_points_in_px} apart.\"\n            )\n\n        return first_bisectors_point, second_bisectors_point\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Calculate the number of points in the polygon\n        num_points = polygon.shape[0]\n\n        # Initialize the arrays to store the selected points and the number of iterations\n        selected_points = np.zeros((self.params.num_bisectors, 2), dtype=int)\n        iterations = 0\n\n        # Iterate until the required number of points have been selected or the maximum number of iterations has been reached\n        while iterations < self.params.max_iterations:\n            # Select two random points from the polygon\n            point_1 = polygon[np.random.randint(num_points)]\n            point_2 = polygon[np.random.randint(num_points)]\n\n            # Calculate the distance between the two points\n            distance = np.linalg.norm(point_1 - point_2)\n\n            # If the distance is greater than the minimum distance, add the points to the selected points array\n            if distance > min_distance_between_sector_points_in_px:\n                selected_points[iterations] = [point_1, point_2]\n                iterations += 1\n\n        # If the required number of points have not been selected after the maximum number of iterations, raise an error\n        if iterations < self.params.num_bisectors:\n            raise EyeCentersEstimationError(\n                f\"Failed to find {self.params.num_bisectors} pairs of points with distance greater than \"\n                f\"{min_distance_between_sector_points_in_px} after {self.params.max_iterations} iterations.\"\n            )\n\n        # Calculate the perpendicular bisectors of the selected points\n        first_bisectors_point = (selected_points[:, 0] + selected_points[:, 1]) / 2\n        second_bisectors_point = first_bisectors_point + np.array([0, 1])\n\n        return first_bisectors_point, second_bisectors_point"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        # Generate random indices for the polygon\n        polygon_indices = np.arange(polygon.shape[0])\n        np.random.shuffle(polygon_indices)\n\n        # Initialize arrays to store the bisectors\n        first_bisectors_point = np.zeros((num_bisectors, 2))\n        second_bisectors_point = np.zeros((num_bisectors, 2))\n\n        # Iterate until we have enough bisectors\n        for i in range(num_bisectors):\n            # Select two random points from the polygon\n            fst_point_idx = polygon_indices[i]\n            sec_point_idx = polygon_indices[i + 1]\n\n            # Calculate the distance between the two points\n            distance = np.linalg.norm(polygon[fst_point_idx] - polygon[sec_point_idx])\n\n            # If the distance is less than the minimum distance, regenerate the points\n            if distance < min_distance_between_sector_points_in_px:\n                if i == 0:\n                    raise EyeCentersEstimationError(\n                        f\"Failed to find a sufficient number of point pairs that meet the distance criterion within {max_iterations} iterations.\"\n                    )\n\n                i -= 1\n                continue\n\n            # Calculate the midpoint between the two points\n            midpoint = (polygon[fst_point_idx] + polygon[sec_point_idx]) / 2\n\n            # Calculate the normal vector to the line between the two points\n            normal_vector = np.array([polygon[sec_point_idx][1] - polygon[fst_point_idx][1], polygon[fst_point_idx][0] - polygon[sec_point_idx][0]])\n            normal_vector = normal_vector / np.linalg.norm(normal_vector)\n\n            # Calculate the perpendicular bisector of the line between the two points\n            bis"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        # Generate a random number of bisectors\n        for _ in range(max_iterations):\n            # Choose a random pair of points from the polygon\n            fst_points, sec_points = self._get_random_pair_of_points(polygon, num_bisectors)\n\n            # Check if the distance between the chosen points is greater than the minimum distance\n            distance_between_points = np.linalg.norm(sec_points - fst_points, axis=1)\n            if np.all(distance_between_points > min_distance_between_sector_points_in_px):\n                return fst_points, sec_points\n\n        raise EyeCentersEstimationError(\"Failed to find enough points that meet the distance criterion.\")\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        # Generate random indices for the polygon vertices\n        polygon_size = polygon.shape[0]\n        random_indices = np.random.choice(polygon_size, size=(num_bisectors, 2), replace=False)\n\n        # Extract the corresponding vertices from the polygon\n        first_points = polygon[random_indices[:, 0]]\n        second_points = polygon[random_indices[:, 1]]\n\n        # Calculate the perpendicular bisectors\n        perpendicular_bisectors = self._calculate_perpendicular_bisectors_from_points(first_points, second_points)\n\n        # Check if the perpendicular bisectors are valid\n        if not self._are_perpendicular_bisectors_valid(perpendicular_bisectors, min_distance_between_sector_points_in_px):\n            raise EyeCentersEstimationError(\"Could not find perpendicular bisectors that meet the distance criterion.\")\n\n        return first_points, second_points\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize variables\n        bisectors_points = []\n        max_iterations = self.params.max_iterations\n        num_bisectors = self.params.num_bisectors\n\n        # Generate random points\n        random_points = self._generate_random_points(polygon, num_bisectors)\n\n        # Iterate until the required number of bisectors is found or the maximum number of iterations is exceeded\n        for _ in range(max_iterations):\n            # Select two random points\n            first_point, second_point = random_points[np.random.choice(random_points.shape[0], size=2, replace=False)]\n\n            # Calculate the distance between the points\n            distance = np.linalg.norm(first_point - second_point)\n\n            # Check if the distance is greater than the minimum distance\n            if distance > min_distance_between_sector_points_in_px:\n                # Calculate the perpendicular bisector\n                bisector_point = self._calculate_perpendicular_bisector(first_point, second_point)\n\n                # Add the bisector point to the list\n                bisectors_points.append(bisector_point)\n\n            # Check if the required number of bisectors is found\n            if len(bisectors_points) == num_bisectors:\n                break\n\n        # Check if the required number of bisectors was found\n        if len(bisectors_points) != num_bisectors:\n            raise EyeCentersEstimationError(\n                f\"Failed to find {num_bisectors} bisectors in {max_iterations} iterations.\"\n            )\n\n        # Convert the list of bisectors points to a numpy array\n        bisectors_points = np.array(bisectors_points)\n\n        # Split the bisectors points into two arrays\n        first_bisectors_point = bisectors_points[:, :2]\n        second_bisectors_point = bisectors_points[:, 2:]\n\n        return first_bisectors_point, second_bisectors"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        if polygon.shape[0] < 2:\n            raise EyeCentersEstimationError(\"Polygon must have at least 2 points.\")\n\n        # Generate the random indices for the points\n        random_indices = np.random.choice(polygon.shape[0], size=(self.params.num_bisectors, 2), replace=False)\n\n        # Iterate over the random indices and calculate the perpendicular bisectors\n        for _ in range(self.params.max_iterations):\n            # Extract the points from the polygon using the random indices\n            points = polygon[random_indices]\n\n            # Calculate the perpendicular bisectors\n            bisectors = self._calculate_perpendicular_bisectors_from_points(points)\n\n            # Check if the distance between the points in each pair is greater than the minimum distance\n            distances = np.linalg.norm(points[:, 0] - points[:, 1], axis=1)\n            if np.all(distances > min_distance_between_sector_points_in_px):\n                return bisectors[:, 0], bisectors[:, 1]\n\n            # If not, regenerate the random indices\n            random_indices = np.random.choice(polygon.shape[0], size=(self.params.num_bisectors, 2), replace=False)\n\n        raise EyeCentersEstimationError(\"Failed to find sufficient point pairs with the required distance.\")\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        num_vertices = len(polygon)\n\n        # Initialize arrays to store the first and second bisectors points\n        first_bisectors_point = np.zeros((num_bisectors, 2))\n        second_bisectors_point = np.zeros((num_bisectors, 2))\n\n        # Initialize a counter to keep track of the number of iterations\n        iteration_count = 0\n\n        # Generate random pairs of points until we find enough points that satisfy the distance criterion\n        while iteration_count < max_iterations:\n            # Generate random pairs of points\n            random_indices = np.random.choice(num_vertices, size=(num_bisectors, 2), replace=False)\n            random_points = polygon[random_indices]\n\n            # Calculate the distance between each pair of points\n            distances = np.linalg.norm(random_points[:, 0] - random_points[:, 1], axis=1)\n\n            # Check if the distance between each pair of points is greater than the minimum distance\n            if np.all(distances >= min_distance_between_sector_points_in_px):\n                # If the distance criterion is satisfied, store the points in the corresponding arrays\n                first_bisectors_point = random_points[:, 0]\n                second_bisectors_point = random_points[:, 1]\n                break\n\n            # If the distance criterion is not satisfied, increment the iteration count and try again\n            iteration_count += 1\n\n        # If we have reached the maximum number of iterations without finding enough points, raise an exception\n        if iteration_count >= max_iterations:\n            raise EyeCentersEstimationError(\"Failed to find sufficient point pairs that satisfy the distance criterion.\")\n\n        return first_bisectors_point, second_bisectors_point"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(self, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(self, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(self, result, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(self, result, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(self, result, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(self, result, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(self, result, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(self, result, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_execute(self, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_execute(self, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(self, result, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_execution(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_execution(self, result, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_execution(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_execution(self, result, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execution(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execution(self, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_execution(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_execution(self, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_execution(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_execution(self, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after(self, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after(self, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        self.pre_execute(*args, **kwargs)\n        result = self.run(*args, **kwargs)\n        self.post_execute(*args, **kwargs)\n\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        self.pre_execute(*args, **kwargs)\n        result = self.run(*args, **kwargs)\n        self.post_execute(*args, **kwargs)\n\n        return result\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except json.JSONDecodeError:\n            return False\n\n    "}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except json.JSONDecodeError:\n            return False\n\n    "}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except json.JSONDecodeError:\n            return False\n\n    "}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except json.JSONDecodeError:\n            return False\n\n    "}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except json.JSONDecodeError:\n            return False\n\n    "}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except json.JSONDecodeError:\n            return False\n\n    "}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except json.JSONDecodeError:\n            return False"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except json.JSONDecodeError:\n            return False\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n        return self.check_type(deserialized_output, type_definition)\n\n    "}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized = json.loads(output)\n            return self.check_type(deserialized, type_definition)\n        except json.JSONDecodeError:\n            return False"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError as e:\n            print(f\"Failed to deserialize output: {e}\")\n            return False\n\n        return self.check_type(deserialized_output, type_definition)"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized = json.loads(output)\n            return self.check_type(deserialized, type_definition)\n        except Exception as e:\n            print(e)\n            return False"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        if type_definition is Any:\n            return True\n        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except json.JSONDecodeError:\n            return False"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except (json.JSONDecodeError, TypeError):\n            return False\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except json.JSONDecodeError:\n            return False\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            output_json = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n        return self.check_type(output_json, type_definition)\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except Exception as e:\n            print(e)\n            return False\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        if type_definition is Any:\n            return True\n        try:\n            deserialized = json.loads(output)\n            return self.check_type(deserialized, type_definition)\n        except json.JSONDecodeError:\n            return False\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized = json.loads(output)\n        except json.JSONDecodeError as e:\n            print(f\"Error deserializing output: {e}\")\n            return False\n\n        return self.check_type(deserialized, type_definition)\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError as e:\n            print(f\"Failed to deserialize output: {e}\")\n            return False\n\n        return self.check_type(deserialized_output, type_definition)\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": ""}
{"namespace": "tanuki.register.Register.load_function_description", "completion": ""}
{"namespace": "tanuki.register.Register.load_function_description", "completion": ""}
{"namespace": "tanuki.register.Register.load_function_description", "completion": ""}
{"namespace": "tanuki.register.Register.load_function_description", "completion": ""}
{"namespace": "tanuki.register.Register.load_function_description", "completion": ""}
{"namespace": "tanuki.register.Register.load_function_description", "completion": ""}
{"namespace": "tanuki.register.Register.load_function_description", "completion": ""}
{"namespace": "tanuki.register.Register.load_function_description", "completion": ""}
{"namespace": "tanuki.register.Register.load_function_description", "completion": ""}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        sig = inspect.signature(func_object)\n        hints = get_type_hints(func_object)\n\n        input_hints = {}\n        output_hints = {}\n\n        for param_name, param in sig.parameters.items():\n            if param.kind == param.VAR_POSITIONAL or param.kind == param.VAR_KEYWORD:\n                continue\n            if param_name in hints:\n                input_hints[param_name] = hints[param_name]\n\n        if len(sig.return_annotation) > 0:\n            output_hints = hints[sig.return_annotation]\n\n        input_class_definitions = {}\n        for param_name, param_type in input_hints.items():\n            input_class_definitions[param_name] = get_class_definition(param_type)\n\n        output_class_definition = get_class_definition(output_hints)\n\n        if issubclass(output_hints, Union):\n            if issubclass(output_hints.__args__[0], Embedding):\n                function_type = FunctionType.EMBEDDABLE\n            else:\n                function_type = FunctionType.SYMBOLIC\n        else:\n            if issubclass(output_hints, Embedding):\n                function_type = FunctionType.EMBEDDABLE\n            else:\n                function_type = FunctionType.SYMBOLIC\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=func_object.__doc__,\n            input_hints=input_hints,\n            output_hints=output_hints,\n            input_class_definitions=input_class_definitions,\n            output_class_definition=output_class_definition,\n            function_type=function_type,\n            source=get_source(func_object)\n        )\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n        input_class_definitions = {}\n        output_class_definitions = {}\n\n        for param_name, param in signature.parameters.items():\n            if param.annotation in type_hints:\n                input_type_hints[param_name] = type_hints[param.annotation]\n                input_class_definitions[param_name] = get_class_definition(input_type_hints[param_name])\n\n        if signature.return_annotation in type_hints:\n            output_type_hints[signature.return_annotation] = type_hints[signature.return_annotation]\n            output_class_definitions[signature.return_annotation] = get_class_definition(\n                output_type_hints[signature.return_annotation])\n\n        if issubclass(output_type_hints[signature.return_annotation], Union):\n            for type_hint in output_type_hints[signature.return_annotation].__args__:\n                if issubclass(type_hint, Embedding):\n                    function_type = FunctionType.EMBEDDABLE\n                    output_class_definitions[signature.return_annotation] = get_class_definition(type_hint)\n                    break\n            else:\n                function_type = FunctionType.SYMBOLIC\n        else:\n            if issubclass(output_type_hints[signature.return_annotation], Embedding):\n                function_type = FunctionType.EMBEDDABLE\n                output_class_definitions[signature.return_annotation] = get_class_definition(\n                    output_type_hints[signature.return_annotation])\n            else:\n                function_type = FunctionType.SYMBOLIC\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=func_object.__doc__,\n            input_type_hints=input_type_hints,\n            input"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        def get_class_definition(type_hint):\n            if isinstance(type_hint, type):\n                if issubclass(type_hint, Embedding):\n                    return type_hint\n            elif isinstance(type_hint, Union):\n                for item in type_hint.__args__:\n                    if isinstance(item, type) and issubclass(item, Embedding):\n                        return item\n            elif isinstance(type_hint, type):\n                if issubclass(type_hint, Embedding):\n                    return type_hint\n            elif isinstance(type_hint, type):\n                if issubclass(type_hint, Embedding):\n                    return type_hint\n            elif isinstance(type_hint, type):\n                if issubclass(type_hint, Embedding):\n                    return type_hint\n            elif isinstance(type_hint, type):\n                if issubclass(type_hint, Embedding):\n                    return type_hint\n            elif isinstance(type_hint, type):\n                if issubclass(type_hint, Embedding):\n                    return type_hint\n            elif isinstance(type_hint, type):\n                if issubclass(type_hint, Embedding):\n                    return type_hint\n            elif isinstance(type_hint, type):\n                if issubclass(type_hint, Embedding):\n                    return type_hint\n            elif isinstance(type_hint, type):\n                if issubclass(type_hint, Embedding):\n                    return type_hint\n            elif isinstance(type_hint, type):\n                if issubclass(type_hint, Embedding):\n                    return type_hint\n            elif isinstance(type_hint, type):\n                if issubclass(type_hint, Embedding):\n                    return type_hint\n            elif isinstance(type_hint, type):\n                if issubclass(type_hint, Embedding):\n                    return type_hint\n            elif isinstance(type_hint, type):\n               "}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        hints = get_type_hints(func_object)\n\n        input_hints = {}\n        output_hint = None\n        output_class_definition = None\n\n        for param_name, param in signature.parameters.items():\n            if param.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD:\n                if param_name in hints:\n                    input_hints[param_name] = hints[param_name]\n\n        if signature.return_annotation in hints:\n            output_hint = hints[signature.return_annotation]\n\n        if output_hint:\n            output_class_definition = get_class_definition(output_hint)\n            if issubclass(output_hint, Union):\n                if Embedding in output_hint.__args__:\n                    output_class_definition = output_hint.__args__[0]\n                    function_type = FunctionType.EMBEDDABLE\n                else:\n                    function_type = FunctionType.SYMBOLIC\n            else:\n                function_type = FunctionType.SYMBOLIC\n\n        return FunctionDescription(func_object.__name__, func_object.__doc__, input_hints, output_hint,\n                                   output_class_definition, function_type)\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        hints = get_type_hints(func_object)\n\n        input_hints = {}\n        output_hints = {}\n        for param in signature.parameters.values():\n            if param.name in hints:\n                hint = hints[param.name]\n                if param.kind == param.POSITIONAL_OR_KEYWORD:\n                    input_hints[param.name] = hint\n                elif param.kind == param.VAR_POSITIONAL:\n                    input_hints[param.name] = hint\n                elif param.kind == param.VAR_KEYWORD:\n                    input_hints[param.name] = hint\n\n        output_hints = hints.get('return', None)\n\n        if output_hints is None:\n            raise ValueError(\"Function must have a return type hint\")\n\n        output_class_definition = get_class_definition(output_hints)\n        function_type = FunctionType.SYMBOLIC\n\n        if issubclass(output_hints, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n\n        if get_origin(output_hints) == Union:\n            if issubclass(output_hints.__args__[0], Embedding):\n                function_type = FunctionType.EMBEDDABLE\n                output_class_definition = get_class_definition(output_hints.__args__[0])\n\n        return FunctionDescription(func_object.__name__, func_object.__doc__, input_hints, output_hints,\n                                   output_class_definition, function_type)\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n        for param_name, param in signature.parameters.items():\n            if param.annotation in type_hints:\n                input_type_hints[param_name] = type_hints[param.annotation]\n\n        if signature.return_annotation in type_hints:\n            output_type_hints = type_hints[signature.return_annotation]\n\n        input_class_definitions = {}\n        output_class_definitions = {}\n\n        for param_name, param_type in input_type_hints.items():\n            input_class_definitions[param_name] = get_class_definition(param_type)\n\n        if isinstance(output_type_hints, Union):\n            function_type = FunctionType.SYMBOLIC\n            for subtype in output_type_hints.__args__:\n                if issubclass(subtype, Embedding):\n                    function_type = FunctionType.EMBEDDABLE\n                    output_class_definitions = get_class_definition(subtype)\n                    break\n        else:\n            function_type = FunctionType.SYMBOLIC\n            if issubclass(output_type_hints, Embedding):\n                function_type = FunctionType.EMBEDDABLE\n                output_class_definitions = get_class_definition(output_type_hints)\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=inspect.getdoc(func_object),\n            input_type_hints=input_type_hints,\n            output_type_hints=output_type_hints,\n            input_class_definitions=input_class_definitions,\n            output_class_definitions=output_class_definitions,\n            function_type=function_type,\n            source=get_source(func_object)\n        )\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        def get_class_definition(type_hint):\n            if isinstance(type_hint, type):\n                return type_hint\n            elif isinstance(type_hint, str):\n                return type_hint\n            elif isinstance(type_hint, Tuple):\n                return type_hint[0]\n            elif isinstance(type_hint, Union):\n                return type_hint[0]\n            elif isinstance(type_hint, Literal):\n                return type_hint\n            else:\n                return type(type_hint)\n\n        input_type_hints = {}\n        output_type_hints = {}\n\n        for param_name, param in signature.parameters.items():\n            if param.kind == param.VAR_KEYWORD:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.KEYWORD_ONLY:\n                continue\n            if param.kind == param.POSITIONAL_ONLY:\n                continue\n\n            if param_name in type_hints:\n                input_type_hints[param_name] = get_class_definition(type_hints[param_name])\n            else:\n                input_type_hints[param_name] = get_class_definition(param.annotation)\n\n        if signature.return_annotation in type_hints:\n            output_type_hints = get_class_definition(type_hints[signature.return_annotation])\n        else:\n            output_type_hints = get_class_definition(signature.return_annotation)\n\n        if issubclass(output_type_hints, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        elif issubclass(output_type_hints, Union):\n            if any(issubclass(output_type_hints, Embedding) for output_type_hint in output_type_hints.__args__):\n               "}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        func_signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        # Get the input and output type hints\n        input_type_hints = {}\n        output_type_hints = {}\n        for param in func_signature.parameters.values():\n            if param.name in type_hints:\n                if param.kind == inspect.Parameter.POSITIONAL_ONLY:\n                    input_type_hints[param.name] = type_hints[param.name]\n                else:\n                    input_type_hints[param.name] = type_hints[param.name]\n\n        output_type_hints = type_hints[list(func_signature.return_annotation.__dict__.keys())[0]]\n\n        # Get the class definitions for the input and output types\n        input_class_definitions = {}\n        for key, value in input_type_hints.items():\n            input_class_definitions[key] = get_class_definition(value)\n\n        output_class_definition = get_class_definition(output_type_hints)\n\n        # Determine the function type based on the output type hint\n        function_type = FunctionType.SYMBOLIC\n        if issubclass(output_type_hints, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        elif isinstance(output_type_hints, type) and issubclass(output_type_hints, Union):\n            for subtype in output_type_hints.__args__:\n                if issubclass(subtype, Embedding):\n                    function_type = FunctionType.EMBEDDABLE\n                    break\n\n        # Create the FunctionDescription instance\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=func_object.__doc__,\n            input_type_hints=input_type_hints,\n            output_type_hints=output_type_hints,\n            input_class_definitions=input_class_definitions,\n            output_class_definition=output"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        input_type_hints = {}\n        output_type_hints = {}\n        output_class_definitions = {}\n        for param_name, param in signature.parameters.items():\n            if param.annotation == inspect.Parameter.empty:\n                continue\n            if param_name in type_hints:\n                input_type_hints[param_name] = type_hints[param_name]\n        if signature.return_annotation == inspect.Signature.empty:\n            output_type_hints['return'] = None\n        else:\n            output_type_hints['return'] = signature.return_annotation\n\n        if output_type_hints['return'] is None:\n            output_class_definitions['return'] = None\n        elif issubclass(output_type_hints['return'], Embedding):\n            output_class_definitions['return'] = output_type_hints['return'].__name__\n        elif isinstance(output_type_hints['return'], Union):\n            for type in output_type_hints['return'].__args__:\n                if issubclass(type, Embedding):\n                    output_class_definitions['return'] = type.__name__\n                    break\n        else:\n            output_class_definitions['return'] = output_type_hints['return'].__name__\n\n        if len(input_type_hints) == 0:\n            input_class_definitions = None\n        else:\n            input_class_definitions = {}\n            for param_name, param_type in input_type_hints.items():\n                if param_type is None:\n                    input_class_definitions[param_name] = None\n                elif issubclass(param_type, Embedding):\n                    input_class_definitions[param_name] = param_type.__name__\n                else:\n                    input_class_definitions[param_name] = param_type.__name__\n\n        if output_type_hints['return'] is None:"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        hints = get_type_hints(func_object)\n\n        input_types = []\n        output_types = []\n        input_class_definitions = []\n        output_class_definitions = []\n\n        for parameter in signature.parameters.values():\n            if parameter.name in hints:\n                input_types.append(hints[parameter.name])\n                input_class_definitions.append(get_class_definition(hints[parameter.name]))\n\n        if len(signature.return_annotation) > 0:\n            output_types.append(signature.return_annotation)\n            output_class_definitions.append(get_class_definition(signature.return_annotation))\n\n        if len(output_types) == 1:\n            output_type = output_types[0]\n            output_class_definition = output_class_definitions[0]\n\n            if issubclass(output_type, Union):\n                if output_class_definition.type == \"Union\":\n                    for type in output_class_definition.types:\n                        if issubclass(type, Embedding):\n                            function_type = FunctionType.EMBEDDABLE\n                            output_class_definition = type\n                            break\n                        else:\n                            function_type = FunctionType.SYMBOLIC\n            else:\n                if issubclass(output_type, Embedding):\n                    function_type = FunctionType.EMBEDDABLE\n                else:\n                    function_type = FunctionType.SYMBOLIC\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=func_object.__doc__,\n            input_types=input_types,\n            output_types=output_types,\n            input_class_definitions=input_class_definitions,\n            output_class_definitions=output_class_definitions,\n            type=function_type\n        )\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        if len(loaded_bit_array) != self.size:\n            logging.warning(f\"Loaded bit array length {len(loaded_bit_array)} does not match expected length {self.size}\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        if loaded_bit_array.length() != self.size:\n            logging.warning(\"Loaded bit array length does not match expected length. Reinitializing bit array.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        if len(bit_array) != self.size:\n            logging.warning(f\"Loaded bit array length ({len(bit_array)}) does not match expected length ({self.size}). Reinitializing bit array.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        if len(bit_array) != self.size:\n            logging.warning(f\"BloomFilter: Corruption detected, reinitializing bit array and indices\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        if bit_array and len(bit_array) != self.size:\n            logging.warning(\"Bit array length mismatch. Reinitializing bit array.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        if loaded_bit_array.length() != self.size:\n            logging.warning(\"Corruption detected, reinitializing bit array\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        if len(bit_array) != self.size:\n            logging.warning(f\"BloomFilter: BitArray length mismatch. Expected {self.size}, got {len(bit_array)}\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        if len(bit_array) != self.size:\n            logging.warning(\"Loaded bit array length does not match expected length. Reinitializing.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        if loaded_bit_array.length() != self.size:\n            logging.warning(\"Loaded bit array has a different size than expected. Reinitializing bit array.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        if bit_array is None:\n            logging.warning(\"No bit array found in persistence, reinitializing\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        elif len(bit_array) != self.size:\n            logging.warning(\"Bit array length mismatch, reinitializing\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        if bit_array is None:\n            logging.warning(\"BloomFilter: No bit array found in persistence. Reinitializing.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        elif len(bit_array) != self.size:\n            logging.warning(\"BloomFilter: Bit array length does not match expected size. Reinitializing.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        if loaded_bit_array.length() != self.size:\n            logging.warning(f\"Bit array length mismatch. Expected length: {self.size}, Actual length: {loaded_bit_array.length()}. Reinitializing bit array.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        if len(bit_array) != self.size:\n            logging.warning(f\"BloomFilter: Bit array length mismatch, expected {self.size}, got {len(bit_array)}. Reinitializing.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n\n        if len(loaded_bit_array) != self.size:\n            logging.warning(f\"Bit array length mismatch, expected {self.size}, got {len(loaded_bit_array)}\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        if bit_array and len(bit_array) != self.size:\n            logging.warning(\"Bit array size mismatch, reinitializing bit array\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = bit_array\n\n    "}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        if bit_array is None:\n            logging.warning(\"BloomFilter: Unable to load bit array from persistence\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n            return\n\n        if len(bit_array) != self.size:\n            logging.warning(f\"BloomFilter: Loaded bit array has length {len(bit_array)}, expected {self.size}\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n            return\n\n        self.bit_array = bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        if loaded_bit_array is None:\n            logging.warning(\"BloomFilter: No bit array found in persistence. Reinitializing.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n            return\n\n        if len(loaded_bit_array) != self.size:\n            logging.warning(\"BloomFilter: Loaded bit array has unexpected length. Reinitializing.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n            return\n\n        self.bit_array = loaded_bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        if bit_array is None:\n            logging.warning(\"Bloom filter bit array is None, reinitializing\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            if len(bit_array) != self.size:\n                logging.warning(\"Bloom filter bit array length mismatch, reinitializing\")\n                self.bit_array, self.indices = self.init_bit_array(self.size)\n                self.save()\n            else:\n                self.bit_array = bit_array\n        "}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n\n        if loaded_bit_array.length() != self.size:\n            logging.warning(f\"BloomFilter: Corruption detected. Expected length {self.size}, got {loaded_bit_array.length()}\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        if bit_array is not None:\n            if len(bit_array) != self.size:\n                logging.warning(\"BloomFilter: Bit array loaded from persistence has a different size than expected. \"\n                                \"Reinitializing bit array and indices.\")\n                self.bit_array, self.indices = self.init_bit_array(self.size)\n                self.save()\n            else:\n                self.bit_array = bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n\n    "}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        for key, value in json_dict.items():\n            if key == \"distilled_model\":\n                self.distilled_model = config_factory.create_model_config(value)\n            elif key == \"current_model_stats\":\n                self.current_model_stats = value\n            elif key == \"last_training_run\":\n                self.last_training_run = value\n            elif key == \"current_training_run\":\n                self.current_training_run = value\n            elif key == \"nr_of_training_runs\":\n                self.nr_of_training_runs = value\n            elif key == \"teacher_models\":\n                self.teacher_models = [config_factory.create_model_config(teacher_model) for teacher_model in value]"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        for key, value in json_dict.items():\n            if key == DISTILLED_MODEL:\n                self.distilled_model = config_factory.get_model_config(value)\n            elif key == TEACHER_MODEL:\n                self.teacher_models = [config_factory.get_model_config(teacher_model) for teacher_model in value]\n            else:\n                setattr(self, key, value)"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        for key, value in json_dict.items():\n            if key == DISTILLED_MODEL:\n                self.distilled_model = config_factory.get_model_config(value)\n            elif key == TEACHER_MODEL:\n                self.teacher_models = [config_factory.get_model_config(teacher_model) for teacher_model in value]\n            else:\n                setattr(self, key, value)"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        for key, value in json_dict.items():\n            if key == \"distilled_model\":\n                self.distilled_model = config_factory.create_model_config(value)\n            elif key == \"current_model_stats\":\n                self.current_model_stats = value\n            elif key == \"last_training_run\":\n                self.last_training_run = value\n            elif key == \"current_training_run\":\n                self.current_training_run = value\n            elif key == \"nr_of_training_runs\":\n                self.nr_of_training_runs = value\n            elif key == \"teacher_models\":\n                self.teacher_models = [config_factory.create_model_config(teacher_model) for teacher_model in value]\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        for key, value in json_dict.items():\n            if key == \"distilled_model\":\n                self.distilled_model = config_factory.create_model_config(value, DISTILLED_MODEL)\n            elif key == \"current_model_stats\":\n                self.current_model_stats = value\n            elif key == \"last_training_run\":\n                self.last_training_run = value\n            elif key == \"current_training_run\":\n                self.current_training_run = value\n            elif key == \"teacher_models\":\n                self.teacher_models = [config_factory.create_model_config(teacher_model, TEACHER_MODEL) for teacher_model in value]\n            elif key == \"nr_of_training_runs\":\n                self.nr_of_training_runs = value"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.create_model_config(json_dict[\"distilled_model\"])\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        self.teacher_models = [config_factory.create_model_config(teacher_model) for teacher_model in json_dict[\"teacher_models\"]]"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        for key, value in json_dict.items():\n            if key == \"distilled_model\":\n                self.distilled_model = config_factory.get_model_config(value)\n            elif key == \"current_model_stats\":\n                self.current_model_stats = value\n            elif key == \"last_training_run\":\n                self.last_training_run = value\n            elif key == \"current_training_run\":\n                self.current_training_run = value\n            elif key == \"nr_of_training_runs\":\n                self.nr_of_training_runs = value\n            elif key == \"teacher_models\":\n                self.teacher_models = [config_factory.get_model_config(teacher_model) for teacher_model in value]\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.create_model_config(json_dict[\"distilled_model\"])\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.teacher_models = [config_factory.create_model_config(teacher_model) for teacher_model in json_dict[\"teacher_models\"]]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        "}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        for key, value in json_dict.items():\n            if key in self.__dict__:\n                if key == \"distilled_model\":\n                    self.distilled_model = config_factory.create_model_config(value)\n                elif key == \"teacher_models\":\n                    self.teacher_models = [config_factory.create_model_config(teacher_model) for teacher_model in value]\n                else:\n                    self.__dict__[key] = value"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.get_model_config(json_dict[\"distilled_model\"])\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        self.teacher_models = [config_factory.get_model_config(teacher_model) for teacher_model in json_dict[\"teacher_models\"]]\n        "}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        for key, value in json_dict.items():\n            if key == \"distilled_model\":\n                self.distilled_model = config_factory.create_model_config(value)\n            elif key == \"current_model_stats\":\n                self.current_model_stats = value\n            elif key == \"last_training_run\":\n                self.last_training_run = value\n            elif key == \"current_training_run\":\n                self.current_training_run = value\n            elif key == \"nr_of_training_runs\":\n                self.nr_of_training_runs = value\n            elif key == \"teacher_models\":\n                self.teacher_models = [config_factory.create_model_config(teacher_model) for teacher_model in value]"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.get_model_config(json_dict[\"distilled_model\"])\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        self.teacher_models = [config_factory.get_model_config(teacher_model) for teacher_model in json_dict[\"teacher_models\"]]\n        return self\n    "}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        for key, value in json_dict.items():\n            if key == \"distilled_model\":\n                self.distilled_model = config_factory.create_model_config(value, DISTILLED_MODEL)\n            elif key == \"current_model_stats\":\n                self.current_model_stats = value\n            elif key == \"last_training_run\":\n                self.last_training_run = value\n            elif key == \"current_training_run\":\n                self.current_training_run = value\n            elif key == \"teacher_models\":\n                self.teacher_models = [config_factory.create_model_config(teacher_model, TEACHER_MODEL) for teacher_model in value]\n            elif key == \"nr_of_training_runs\":\n                self.nr_of_training_runs = value\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        for key, value in json_dict.items():\n            if key == \"distilled_model\":\n                self.distilled_model = config_factory.load_model_config(value)\n            elif key == \"current_model_stats\":\n                self.current_model_stats = value\n            elif key == \"last_training_run\":\n                self.last_training_run = value\n            elif key == \"current_training_run\":\n                self.current_training_run = value\n            elif key == \"nr_of_training_runs\":\n                self.nr_of_training_runs = value\n            elif key == \"teacher_models\":\n                self.teacher_models = [config_factory.load_model_config(teacher_model) for teacher_model in value]"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.get_model_config(json_dict[\"distilled_model\"])\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.teacher_models = [config_factory.get_model_config(teacher_model) for teacher_model in json_dict[\"teacher_models\"]]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        for key, value in json_dict.items():\n            if key == DISTILLED_MODEL:\n                self.distilled_model = config_factory.get_model_config(value)\n            elif key == TEACHER_MODEL:\n                self.teacher_models = [config_factory.get_model_config(teacher_model) for teacher_model in value]\n            elif key == \"current_model_stats\":\n                self.current_model_stats = value\n            elif key == \"last_training_run\":\n                self.last_training_run = value\n            elif key == \"current_training_run\":\n                self.current_training_run = value\n            elif key == \"nr_of_training_runs\":\n                self.nr_of_training_runs = value"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.get_model_config(json_dict[\"distilled_model\"])\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if \"teacher_models\" in json_dict:\n            self.teacher_models = [config_factory.get_model_config(teacher_model) for teacher_model in json_dict[\"teacher_models\"]]\n        else:\n            self.teacher_models = [DEFAULT_TEACHER_MODELS[teacher_model_name] for teacher_model_name in DEFAULT_TEACHER_MODEL_NAMES]\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.get_model_config(\n            json_dict[\"distilled_model\"])\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.teacher_models = [config_factory.get_model_config(teacher_model) for teacher_model in json_dict[\"teacher_models\"]]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n    "}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.load_from_dict(json_dict[\"distilled_model\"])\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.teacher_models = [config_factory.load_from_dict(teacher_model_config) for teacher_model_config in json_dict[\"teacher_models\"]]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        if \"distilled_model\" in json_dict:\n            self.distilled_model = config_factory.create_model_config(\n                json_dict[\"distilled_model\"], DISTILLED_MODEL)\n        if \"current_model_stats\" in json_dict:\n            self.current_model_stats = json_dict[\"current_model_stats\"]\n        if \"last_training_run\" in json_dict:\n            self.last_training_run = json_dict[\"last_training_run\"]\n        if \"current_training_run\" in json_dict:\n            self.current_training_run = json_dict[\"current_training_run\"]\n        if \"nr_of_training_runs\" in json_dict:\n            self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if \"teacher_models\" in json_dict:\n            self.teacher_models = [\n                config_factory.create_model_config(teacher_model, TEACHER_MODEL)\n                for teacher_model in json_dict[\"teacher_models\"]]"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # validate the parameters\n        for parameter in LLM_GENERATION_PARAMETERS:\n            if parameter not in kwargs:\n                raise ValueError(f\"Missing required parameter: {parameter}\")\n\n        # create the system message\n        system_message = {\n            \"role\": \"system\",\n            \"content\": system_message\n        }\n\n        # create the prompt\n        prompt = [\n            system_message,\n            {\n                \"role\": \"user\",\n                \"content\": prompt\n            }\n        ]\n\n        # set the model name\n        model_name = model.model_name\n\n        # set the parsing helper tokens\n        parsing_helper_tokens = model.parsing_helper_tokens\n\n        # set the parsing helper tokens\n        parsing_helper_tokens = model.parsing_helper_tokens\n\n        # set the parsing helper tokens\n        parsing_helper_tokens = model.parsing_helper_tokens\n\n        # set the parsing helper tokens\n        parsing_helper_tokens = model.parsing_helper_tokens\n\n        # set the parsing helper tokens\n        parsing_helper_tokens = model.parsing_helper_tokens\n\n        # set the parsing helper tokens\n        parsing_helper_tokens = model.parsing_helper_tokens\n\n        # set the parsing helper tokens\n        parsing_helper_tokens = model.parsing_helper_tokens\n\n        # set the parsing helper tokens\n        parsing_helper_tokens = model.parsing_helper_tokens\n\n        # set the parsing helper tokens\n        parsing_helper_tokens = model.parsing_helper_tokens\n\n        # set the parsing helper tokens\n        parsing_helper_tokens = model.parsing_helper_tokens\n\n        # set the parsing helper tokens\n        parsing_helper_tokens = model.parsing_helper_tokens\n\n        # set the parsing helper tokens\n        parsing_helper_tokens = model.parsing_helper_tok"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # check if api key is not none\n        self.check_api_key()\n\n        # check if model is not none\n        if not model:\n            raise ValueError(\"Model is not set\")\n\n        # check if system_message is not none\n        if not system_message:\n            raise ValueError(\"System message is not set\")\n\n        # check if prompt is not none\n        if not prompt:\n            raise ValueError(\"Prompt is not set\")\n\n        # check if kwargs are valid\n        for key, value in kwargs.items():\n            if key not in LLM_GENERATION_PARAMETERS:\n                raise ValueError(f\"Invalid parameter: {key}\")\n\n        # check if model is valid\n        if model.model_name not in self.client.models.list().data:\n            raise ValueError(\"Model not found\")\n\n        # check if system_message is valid\n        if not system_message.startswith(\"system\"):\n            raise ValueError(\"System message must start with 'system'\")\n\n        # check if prompt is valid\n        if not prompt.startswith(\"user\"):\n            raise ValueError(\"Prompt must start with 'user'\")\n\n        # check if kwargs are valid\n        for key, value in kwargs.items():\n            if key not in LLM_GENERATION_PARAMETERS:\n                raise ValueError(f\"Invalid parameter: {key}\")\n\n        # check if model is valid\n        if model.model_name not in self.client.models.list().data:\n            raise ValueError(\"Model not found\")\n\n        # check if system_message is valid\n        if not system_message.startswith(\"system\"):\n            raise ValueError(\"System message must start with 'system'\")\n\n        # check if prompt is valid\n        if not prompt.startswith(\"user\"):\n            raise ValueError(\"Prompt must start with 'user'\")\n\n        # check if kwargs are valid\n        for key, value in kwargs.items():\n            if key not in LLM_GENERATION_PARAMETERS:\n                raise ValueError("}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n        # check if api key is not none\n        if not self.api_key:\n            raise ValueError(\"OpenAI API key is not set\")\n\n        if not self.client:\n            self.client = OpenAI(api_key=self.api_key)\n\n        # validate the parameters\n        for param in LLM_GENERATION_PARAMETERS:\n            if param in kwargs:\n                if not isinstance(kwargs[param], (int, float)):\n                    raise ValueError(f\"{param} must be a number\")\n                if param == \"temperature\" and (kwargs[param] < 0 or kwargs[param] > 2):\n                    raise ValueError(f\"{param} must be between 0 and 2\")\n                if param == \"top_p\" and (kwargs[param] < 0 or kwargs[param] > 1):\n                    raise ValueError(f\"{param} must be between 0 and 1\")\n                if param == \"frequency_penalty\" and (kwargs[param] < -2 or kwargs[param] > 2):\n                    raise ValueError(f\"{param} must be between -2 and 2\")\n                if param == \"presence_penalty\" and (kwargs[param] < -2 or kwargs[param] > 2):\n                    raise ValueError(f\"{param} must be between -2 and 2\")\n                if param == \"max_new_tokens\" and kwargs[param] < 1:\n                    raise ValueError(f\"{param} must be greater than 0\")\n\n        # check if the model is valid\n        if not model.model_name:\n            raise ValueError(\"Model name is not set\")\n\n        # check if the system message is valid\n        if not system_message:\n            raise ValueError(\"System message is not set\")\n\n        # check if the prompt is valid\n        if not prompt:\n            raise ValueError(\"Prompt is not set\")\n\n        # check if the model is valid\n        if not model.model_name:\n            raise ValueError(\"Model name is not set\")"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # Check if the provided model is an OpenAIConfig instance\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"Model must be an OpenAIConfig instance\")\n\n        # Check if the provided system message is a string\n        if not isinstance(system_message, str):\n            raise ValueError(\"System message must be a string\")\n\n        # Check if the provided prompt is a string\n        if not isinstance(prompt, str):\n            raise ValueError(\"Prompt must be a string\")\n\n        # Check if the provided parameters are valid\n        for param in kwargs:\n            if param not in LLM_GENERATION_PARAMETERS:\n                raise ValueError(f\"Invalid parameter: {param}\")\n\n        # Check if the provided model name is valid\n        if model.model_name not in self.get_models():\n            raise ValueError(f\"Invalid model name: {model.model_name}\")\n\n        # Check if the provided system message is valid\n        if system_message not in self.get_system_messages():\n            raise ValueError(f\"Invalid system message: {system_message}\")\n\n        # Check if the provided prompt is valid\n        if prompt not in self.get_prompts():\n            raise ValueError(f\"Invalid prompt: {prompt}\")\n\n        # Check if the provided parameters are valid\n        for param in kwargs:\n            if param not in LLM_GENERATION_PARAMETERS:\n                raise ValueError(f\"Invalid parameter: {param}\")\n\n        # Check if the provided model name is valid\n        if model.model_name not in self.get_models():\n            raise ValueError(f\"Invalid model name: {model.model_name}\")\n\n        # Check if the provided system message is valid\n        if system_message not in self.get_system_messages():\n            raise ValueError(f\"Invalid system message: {system_message}\")\n\n        # Check if the provided prompt is valid\n        if prompt not in self.get_prompts():\n            raise ValueError(f\"Invalid prompt: {prom"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # check if api key is not none\n        self.check_api_key()\n\n        # check if the model is valid\n        if not model:\n            raise ValueError(\"Model is not set\")\n\n        # check if the system message is valid\n        if not system_message:\n            raise ValueError(\"System message is not set\")\n\n        # check if the prompt is valid\n        if not prompt:\n            raise ValueError(\"Prompt is not set\")\n\n        # check if the kwargs are valid\n        for key, value in kwargs.items():\n            if key not in LLM_GENERATION_PARAMETERS:\n                raise ValueError(f\"Invalid parameter: {key}\")\n\n        # check if the model is valid\n        if not model.model_name:\n            raise ValueError(\"Model is not set\")\n\n        # check if the model has parsing helper tokens\n        if not model.parsing_helper_tokens:\n            raise ValueError(\"Model does not have parsing helper tokens\")\n\n        # check if the model has parsing helper tokens\n        if not model.parsing_helper_tokens:\n            raise ValueError(\"Model does not have parsing helper tokens\")\n\n        # check if the system message is valid\n        if not system_message:\n            raise ValueError(\"System message is not set\")\n\n        # check if the prompt is valid\n        if not prompt:\n            raise ValueError(\"Prompt is not set\")\n\n        # check if the kwargs are valid\n        for key, value in kwargs.items():\n            if key not in LLM_GENERATION_PARAMETERS:\n                raise ValueError(f\"Invalid parameter: {key}\")\n\n        # check if the model is valid\n        if not model.model_name:\n            raise ValueError(\"Model is not set\")\n\n        # check if the model has parsing helper tokens\n        if not model.parsing_helper_tokens:\n            raise ValueError(\"Model does not have parsing helper tokens\")\n\n        # check if the model has parsing helper tokens\n        if not model.parsing_helper_tokens:\n            raise ValueError(\""}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n        # check if api key is not none\n        if not self.api_key:\n            # try to get the api key from the environment, maybe it has been set later\n            self.api_key = os.getenv(\"OPENAI_API_KEY\")\n            if not self.api_key:\n                raise ValueError(\"OpenAI API key is not set\")\n\n        # check if api key is not none\n        if not self.client:\n            self.client = OpenAI(api_key=self.api_key)\n\n        # check if the model is supported\n        if model.model_name not in self.supported_models:\n            raise ValueError(f\"Model {model.model_name} is not supported\")\n\n        # check if the model has the required parsing helper tokens\n        if not model.has_parsing_helper_tokens():\n            raise ValueError(f\"Model {model.model_name} does not have the required parsing helper tokens\")\n\n        # check if the system message is not empty\n        if not system_message:\n            raise ValueError(\"System message is empty\")\n\n        # check if the prompt is not empty\n        if not prompt:\n            raise ValueError(\"Prompt is empty\")\n\n        # check if the parameters are valid\n        for param in kwargs:\n            if param not in LLM_GENERATION_PARAMETERS:\n                raise ValueError(f\"Invalid parameter: {param}\")\n\n        # check if the model has the required parsing helper tokens\n        if not model.has_parsing_helper_tokens():\n            raise ValueError(f\"Model {model.model_name} does not have the required parsing helper tokens\")\n\n        # check if the system message is not empty\n        if not system_message:\n            raise ValueError(\"System message is empty\")\n\n        # check if the prompt is not empty\n        if not prompt:\n            raise ValueError(\"Prompt is empty\")\n\n        # check if the parameters are valid\n        for param in kwargs:\n            if param not in LLM_GENERATION_PARAMETERS:\n                raise"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # Check if all required parameters are provided\n        required_parameters = [\"temperature\", \"top_p\", \"frequency_penalty\", \"presence_penalty\", \"max_new_tokens\"]\n        for parameter in required_parameters:\n            if parameter not in kwargs:\n                raise ValueError(f\"Missing required parameter: {parameter}\")\n\n        # Check if the model name is provided\n        if not model.model_name:\n            raise ValueError(\"Model name is not provided\")\n\n        # Check if the system message is provided\n        if not system_message:\n            raise ValueError(\"System message is not provided\")\n\n        # Check if the prompt is provided\n        if not prompt:\n            raise ValueError(\"Prompt is not provided\")\n\n        # Check if the prompt is a string\n        if not isinstance(prompt, str):\n            raise ValueError(\"Prompt is not a string\")\n\n        # Check if the prompt is not empty\n        if not prompt.strip():\n            raise ValueError(\"Prompt is empty\")\n\n        # Check if the prompt is not too long\n        if len(prompt) > 1024:\n            raise ValueError(\"Prompt is too long\")\n\n        # Check if the prompt is not too short\n        if len(prompt) < 10:\n            raise ValueError(\"Prompt is too short\")\n\n        # Check if the prompt contains any parsing helper tokens\n        if any(token in prompt for token in model.parsing_helper_tokens):\n            raise ValueError(\"Prompt contains parsing helper tokens\")\n\n        # Check if the prompt contains any parsing helper tokens\n        if any(token in system_message for token in model.parsing_helper_tokens):\n            raise ValueError(\"System message contains parsing helper tokens\")\n\n        # Check if the prompt contains any parsing helper tokens\n        if any(token in system_message for token in model.parsing_helper_tokens):\n            raise ValueError(\"System message contains parsing helper tokens\")\n\n        # Check if the prompt contains any parsing helper tokens\n        if any(token in system_message"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n        # check if the model is a valid OpenAIConfig object\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"model must be an instance of OpenAIConfig\")\n\n        # check if the system message is a string\n        if not isinstance(system_message, str):\n            raise ValueError(\"system_message must be a string\")\n\n        # check if the prompt is a string\n        if not isinstance(prompt, str):\n            raise ValueError(\"prompt must be a string\")\n\n        # check if the model name is not empty\n        if not model.model_name:\n            raise ValueError(\"model_name must not be empty\")\n\n        # check if the parsing helper tokens are valid\n        if not isinstance(model.parsing_helper_tokens, list):\n            raise ValueError(\"parsing_helper_tokens must be a list\")\n\n        # check if the parsing helper tokens are not empty\n        if not model.parsing_helper_tokens:\n            raise ValueError(\"parsing_helper_tokens must not be empty\")\n\n        # check if the parsing helper tokens are unique\n        if len(model.parsing_helper_tokens) != len(set(model.parsing_helper_tokens)):\n            raise ValueError(\"parsing_helper_tokens must be unique\")\n\n        # check if the parsing helper tokens are not empty strings\n        if any(not isinstance(token, str) or not token for token in model.parsing_helper_tokens):\n            raise ValueError(\"parsing_helper_tokens must not be empty strings\")\n\n        # check if the parsing helper tokens are not longer than 10 characters\n        if any(len(token) > 10 for token in model.parsing_helper_tokens):\n            raise ValueError(\"parsing_helper_tokens must not be longer than 10 characters\")\n\n        # check if the parsing helper tokens are not longer than 10 characters\n        if any(len(token) > 10 for token in model.parsing_helper_tokens):\n            raise Value"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # validate the parameters\n        for param in LLM_GENERATION_PARAMETERS:\n            if param in kwargs:\n                if not isinstance(kwargs[param], (int, float)):\n                    raise ValueError(f\"{param} must be a number\")\n                if param == \"temperature\" and (kwargs[param] < 0 or kwargs[param] > 2):\n                    raise ValueError(f\"{param} must be between 0 and 2\")\n                if param == \"top_p\" and (kwargs[param] < 0 or kwargs[param] > 1):\n                    raise ValueError(f\"{param} must be between 0 and 1\")\n                if param == \"max_new_tokens\" and kwargs[param] < 0:\n                    raise ValueError(f\"{param} must be greater than 0\")\n                if param == \"frequency_penalty\" and (kwargs[param] < 0 or kwargs[param] > 2):\n                    raise ValueError(f\"{param} must be between 0 and 2\")\n                if param == \"presence_penalty\" and (kwargs[param] < 0 or kwargs[param] > 2):\n                    raise ValueError(f\"{param} must be between 0 and 2\")\n\n        # create the request body\n        request_body = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            \"max_tokens\": 1000,\n            \"temperature\": 0.7,\n            \"frequency_penalty\": 0,\n            \"presence_penalty\": 0,\n            \"top_p\": 1,\n            \"stream\": True,\n            \"n\": 1\n        }\n\n        # update the request body with the provided parameters\n        for param in LLM_GENERATION_PARAMETERS:\n            if param in kwargs:\n                request_body[param] = kwargs["}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # validate input parameters\n        for param in LLM_GENERATION_PARAMETERS:\n            if param in kwargs:\n                if not isinstance(kwargs[param], (int, float)):\n                    raise ValueError(f\"{param} must be an integer or float\")\n                if param == \"temperature\" and (kwargs[param] < 0 or kwargs[param] > 1):\n                    raise ValueError(\"temperature must be between 0 and 1\")\n                if param == \"top_p\" and (kwargs[param] < 0 or kwargs[param] > 1):\n                    raise ValueError(\"top_p must be between 0 and 1\")\n                if param == \"max_new_tokens\" and kwargs[param] < 1:\n                    raise ValueError(\"max_new_tokens must be greater than 0\")\n                if param == \"frequency_penalty\" and (kwargs[param] < -2 or kwargs[param] > 2):\n                    raise ValueError(\"frequency_penalty must be between -2 and 2\")\n                if param == \"presence_penalty\" and (kwargs[param] < -2 or kwargs[param] > 2):\n                    raise ValueError(\"presence_penalty must be between -2 and 2\")\n\n        # construct the request body\n        request_body = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            **kwargs\n        }\n\n        # retry up to 5 times with exponential backoff\n        for attempt in range(5):\n            try:\n                # send the request to the OpenAI API\n                response = requests.post(OPENAI_URL, json=request_body, headers={\"Authorization\": f\"Bearer {self.api_key}\"})\n\n                # check if the request was successful\n                if response.status_code == 200:\n                    # parse the response and remove any parsing helper tokens\n                    response_json ="}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # validate the input parameters\n        if not isinstance(model, OpenAIConfig):\n            raise TypeError(\"model must be an instance of OpenAIConfig\")\n        if not isinstance(system_message, str):\n            raise TypeError(\"system_message must be a string\")\n        if not isinstance(prompt, str):\n            raise TypeError(\"prompt must be a string\")\n        for param in kwargs:\n            if param not in LLM_GENERATION_PARAMETERS:\n                raise ValueError(f\"Invalid parameter: {param}\")\n\n        # set the default values for the parameters\n        default_params = {\n            \"temperature\": 0.7,\n            \"top_p\": 1,\n            \"frequency_penalty\": 0,\n            \"presence_penalty\": 0,\n            \"max_new_tokens\": 100,\n        }\n        default_params.update(kwargs)\n\n        # create the request payload\n        payload = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            **default_params\n        }\n\n        # retry up to 5 times with exponential backoff\n        for i in range(5):\n            try:\n                response = requests.post(OPENAI_URL, json=payload, headers={\"Authorization\": f\"Bearer {self.api_key}\"})\n                response.raise_for_status()\n                response_json = response.json()\n                # extract the generated text from the response\n                generated_text = response_json[\"choices\"][0][\"message\"][\"content\"]\n                # remove any parsing helper tokens from the generated text\n                generated_text = self.remove_parsing_helper_tokens(generated_text, model)\n                return generated_text\n            except requests.exceptions.RequestException as e:\n                logging.error(f\"Request failed: {e}\")\n                # exponential backoff\n                wait_time"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n        # Validate the input parameters\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"model must be an instance of OpenAIConfig\")\n\n        if not isinstance(system_message, str):\n            raise ValueError(\"system_message must be a string\")\n\n        if not isinstance(prompt, str):\n            raise ValueError(\"prompt must be a string\")\n\n        for param in LLM_GENERATION_PARAMETERS:\n            if param in kwargs:\n                if not isinstance(kwargs[param], (int, float)):\n                    raise ValueError(f\"{param} must be an integer or float\")\n\n        # Set the API key\n        self.client.api_key = self.api_key\n\n        # Set the model name\n        model_name = model.model_name\n\n        # Set the system message\n        system_message = system_message\n\n        # Set the prompt\n        prompt = prompt\n\n        # Set the other parameters\n        other_parameters = kwargs\n\n        # Set the number of retries\n        num_retries = 5\n\n        # Set the initial backoff time\n        backoff_time = 1\n\n        # Set the initial response\n        response = None\n\n        # Set the initial error message\n        error_message = None\n\n        # Set the initial error code\n        error_code = None\n\n        # Set the initial error type\n        error_type = None\n\n        # Set the initial error source\n        error_source = None\n\n        # Set the initial error param\n        error_param = None\n\n        # Set the initial error code\n        error_code = None\n\n        # Set the initial error type\n        error_type = None\n\n        # Set the initial error source\n        error_source = None\n\n        # Set the initial error param\n        error_param = None\n\n        # Set the initial error code\n        error_code = None\n\n        # Set the initial error type\n        error_type = None\n\n        # Set the initial error source\n        error_source = None\n\n        # Set the initial error param"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n        # check if all required parameters are present\n        if not all(param in kwargs for param in LLM_GENERATION_PARAMETERS):\n            raise ValueError(\"Missing required parameters for generation\")\n\n        # check if all parameters are valid\n        for param in LLM_GENERATION_PARAMETERS:\n            if param in kwargs and not isinstance(kwargs[param], float):\n                raise ValueError(f\"Invalid value for parameter {param}\")\n\n        # check if system_message and prompt are strings\n        if not isinstance(system_message, str) or not isinstance(prompt, str):\n            raise ValueError(\"system_message and prompt must be strings\")\n\n        # check if model is an instance of OpenAIConfig\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"model must be an instance of OpenAIConfig\")\n\n        # check if model_name is not empty\n        if not model.model_name:\n            raise ValueError(\"model_name must not be empty\")\n\n        # check if parsing_helper_tokens is a list of strings\n        if model.parsing_helper_tokens and not isinstance(model.parsing_helper_tokens, list) or not all(\n                isinstance(token, str) for token in model.parsing_helper_tokens):\n            raise ValueError(\"parsing_helper_tokens must be a list of strings\")\n\n        # check if parsing_helper_tokens is not empty\n        if not model.parsing_helper_tokens:\n            raise ValueError(\"parsing_helper_tokens must not be empty\")\n\n        # check if parsing_helper_tokens is not a subset of model_name\n        if set(model.parsing_helper_tokens).issubset(model.model_name.split(\":\")):\n            raise ValueError(\"parsing_helper_tokens must not be a subset of model_name\")\n\n        # check if parsing_helper_tokens is not a subset of the model name\n        if set(model.parsing_helper_tokens).iss"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # Validate input parameters\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"model must be an instance of OpenAIConfig\")\n        if not isinstance(system_message, str):\n            raise ValueError(\"system_message must be a string\")\n        if not isinstance(prompt, str):\n            raise ValueError(\"prompt must be a string\")\n        if not all(key in LLM_GENERATION_PARAMETERS for key in kwargs.keys()):\n            raise ValueError(f\"Invalid generation parameter(s): {', '.join(kwargs.keys())}\")\n\n        # Set default values for optional parameters\n        kwargs.setdefault(\"temperature\", 0.5)\n        kwargs.setdefault(\"top_p\", 1.0)\n        kwargs.setdefault(\"frequency_penalty\", 0.0)\n        kwargs.setdefault(\"presence_penalty\", 0.0)\n        kwargs.setdefault(\"max_new_tokens\", 100)\n\n        # Set the model name and parsing helper tokens\n        kwargs[\"model\"] = model.model_name\n        kwargs[\"stop\"] = model.stop_sequences\n\n        # Set the system message and prompt\n        kwargs[\"messages\"] = [\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n\n        # Set the API endpoint\n        api_endpoint = OPENAI_URL\n\n        # Set the headers\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.api_key}\"\n        }\n\n        # Set the request data\n        request_data = {\n            \"model\": kwargs[\"model\"],\n            \"messages\": kwargs[\"messages\"],\n            \"temperature\": kwargs[\"temperature\"],\n            \"top_p\": kwargs[\"top_p\"],\n            \"frequency_penalty\": kwargs[\"frequency_penalty\"],\n            \""}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # validate input parameters\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"Model must be an instance of OpenAIConfig\")\n        if not isinstance(system_message, str):\n            raise ValueError(\"System message must be a string\")\n        if not isinstance(prompt, str):\n            raise ValueError(\"Prompt must be a string\")\n        for key in kwargs:\n            if key not in LLM_GENERATION_PARAMETERS:\n                raise ValueError(f\"Invalid parameter: {key}\")\n\n        # check if the model has parsing helper tokens\n        if model.has_parsing_helper_tokens:\n            # add the parsing helper tokens to the prompt\n            prompt = f\"{model.parsing_helper_tokens}\\n{prompt}\"\n\n        # set up the API request\n        data = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            **kwargs\n        }\n\n        # set up the API request\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.api_key}\"\n        }\n\n        # make the API request\n        for i in range(5):\n            try:\n                response = requests.post(OPENAI_URL, headers=headers, json=data)\n                response.raise_for_status()\n                break\n            except requests.exceptions.RequestException as e:\n                logging.error(f\"Error making API request: {e}\")\n                time.sleep(2 ** i)\n        else:\n            raise Exception(\"Failed to make API request after 5 retries\")\n\n        # parse the response\n        response_json = response.json()\n        if \"error\" in response_json:\n            raise Exception(f\"API error: {response_json['error']}\")\n        choices = response_json[\"choices\"]\n        if not choices:\n            raise Exception(\""}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # validate the parameters\n        for parameter in LLM_GENERATION_PARAMETERS:\n            if parameter in kwargs:\n                if not isinstance(kwargs[parameter], (int, float)):\n                    raise ValueError(f\"{parameter} must be an integer or float\")\n\n        # create the request body\n        request_body = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            **kwargs\n        }\n\n        # send the request to the OpenAI API\n        for attempt in range(5):\n            try:\n                response = requests.post(\n                    OPENAI_URL,\n                    headers={\"Authorization\": f\"Bearer {self.api_key}\"},\n                    json=request_body\n                )\n                response.raise_for_status()\n                response_json = response.json()\n                break\n            except requests.exceptions.RequestException as e:\n                # handle any request exceptions\n                logging.error(f\"Request failed: {e}\")\n                # exponential backoff with jitter\n                wait_time = 2 ** attempt + random.uniform(0, 1)\n                logging.info(f\"Retrying in {wait_time} seconds\")\n                time.sleep(wait_time)\n        else:\n            # if all attempts failed, raise an exception\n            raise Exception(\"Failed to generate response after 5 attempts\")\n\n        # process the response to remove any parsing helper tokens\n        response_text = response_json[\"choices\"][0][\"message\"][\"content\"]\n        # remove the parsing helper tokens from the response text\n        response_text = self.remove_parsing_helper_tokens(response_text, model.parsing_helper_tokens)\n        return response_text\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # validate the input parameters\n        for key, value in kwargs.items():\n            if key not in LLM_GENERATION_PARAMETERS:\n                raise ValueError(f\"Invalid parameter: {key}\")\n\n        # create the system message\n        system_message = {\"role\": \"system\", \"content\": system_message}\n\n        # create the prompt\n        prompt = [system_message, {\"role\": \"user\", \"content\": prompt}]\n\n        # create the request\n        request = {\n            \"model\": model.model_name,\n            \"messages\": prompt,\n            \"max_tokens\": 1000,\n            \"temperature\": 0.7,\n            \"frequency_penalty\": 0,\n            \"presence_penalty\": 0,\n            **kwargs\n        }\n\n        # try to generate the response 5 times with exponential backoff\n        for i in range(5):\n            try:\n                response = requests.post(OPENAI_URL, json=request, headers={\"Authorization\": f\"Bearer {self.api_key}\"})\n                response.raise_for_status()\n                break\n            except requests.exceptions.RequestException as e:\n                logging.error(f\"Error generating response: {e}\")\n                backoff_time = 2 ** i\n                time.sleep(backoff_time)\n        else:\n            raise RuntimeError(\"Failed to generate response after 5 attempts\")\n\n        # parse the response\n        response_json = response.json()\n        response_text = response_json[\"choices\"][0][\"message\"][\"content\"]\n\n        # remove any parsing helper tokens\n        response_text = response_text.replace(model.parsing_helper_token, \"\")\n\n        return response_text"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # Check if the input parameters are valid\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"model must be an instance of OpenAIConfig\")\n        if not isinstance(system_message, str):\n            raise ValueError(\"system_message must be a string\")\n        if not isinstance(prompt, str):\n            raise ValueError(\"prompt must be a string\")\n        if not isinstance(kwargs, dict):\n            raise ValueError(\"kwargs must be a dictionary\")\n        for key, value in kwargs.items():\n            if key not in LLM_GENERATION_PARAMETERS:\n                raise ValueError(f\"Invalid parameter: {key}\")\n\n        # Check if the model has parsing helper tokens\n        if not model.parsing_helper_tokens:\n            raise ValueError(\"Model does not have parsing helper tokens\")\n\n        # Set the API key and create an OpenAI client\n        self.client = OpenAI(api_key=self.api_key)\n\n        # Set the model and prompt\n        model_name = model.model_name\n        prompt = f\"{system_message} {prompt}\"\n\n        # Set the default parameters\n        default_parameters = {\n            \"temperature\": 0.7,\n            \"top_p\": 1,\n            \"frequency_penalty\": 0,\n            \"presence_penalty\": 0,\n            \"max_new_tokens\": 150\n        }\n\n        # Update the default parameters with the provided ones\n        for key, value in kwargs.items():\n            default_parameters[key] = value\n\n        # Set the parameters for the API request\n        parameters = {\n            \"model\": model_name,\n            \"prompt\": prompt,\n            \"temperature\": default_parameters[\"temperature\"],\n            \"top_p\": default_parameters[\"top_p\"],\n            \"frequency_penalty\": default_parameters[\"frequency_penalty\"],\n            \"presence_penalty\": default_parameters[\"presence_penalty\"],\n            \"max_new_tokens"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n        # validate the parameters\n        for param in LLM_GENERATION_PARAMETERS:\n            if param in kwargs:\n                if not isinstance(kwargs[param], float) and not isinstance(kwargs[param], int):\n                    raise ValueError(f\"{param} must be a float or int\")\n\n        # create the system message\n        system_message = {\"role\": \"system\", \"content\": system_message}\n        # create the messages\n        messages = [system_message, {\"role\": \"user\", \"content\": prompt}]\n\n        # set the model\n        model_name = model.model_name\n\n        # set the parsing helper tokens\n        parsing_helper_tokens = model.parsing_helper_tokens\n\n        # set the parameters\n        parameters = {\n            \"model\": model_name,\n            \"messages\": messages,\n            \"max_new_tokens\": 1000,\n            \"temperature\": 0.7,\n            \"top_p\": 1,\n            \"frequency_penalty\": 0,\n            \"presence_penalty\": 0,\n            \"stream\": True,\n        }\n\n        # update the parameters with the kwargs\n        parameters.update(kwargs)\n\n        # set the headers\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.api_key}\"\n        }\n\n        # set the data\n        data = {\n            \"model\": model_name,\n            \"messages\": messages,\n            \"stream\": True,\n            \"temperature\": parameters[\"temperature\"],\n            \"top_p\": parameters[\"top_p\"],\n            \"frequency_penalty\": parameters[\"frequency_penalty\"],\n            \"presence_penalty\": parameters[\"presence_penalty\"],\n            \"max_new_tokens\": parameters[\"max_new_tokens\"],\n            \"stop\": parsing_helper_tokens\n        }\n\n        # set the retries\n        retries = 5\n        retry_delay = 1\n\n        # set"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # check if all required parameters are present\n        if not all(param in kwargs for param in LLM_GENERATION_PARAMETERS):\n            raise ValueError(\"Missing required parameters for generation\")\n\n        # check if all required parameters are valid\n        for param in LLM_GENERATION_PARAMETERS:\n            if param in kwargs:\n                if not isinstance(kwargs[param], (int, float)):\n                    raise ValueError(f\"Invalid value for parameter {param}\")\n\n        # create a copy of the kwargs dictionary to avoid modifying the original\n        generation_kwargs = copy.deepcopy(kwargs)\n\n        # remove any parsing helper tokens from the kwargs\n        for token in model.parsing_helper_tokens:\n            if token in generation_kwargs:\n                del generation_kwargs[token]\n\n        # set the model name in the kwargs\n        generation_kwargs[\"model\"] = model.model_name\n\n        # set the system message\n        generation_kwargs[\"messages\"] = [\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n\n        # set the retries to 5\n        retries = 5\n\n        # set the exponential backoff factor to 2\n        backoff_factor = 2\n\n        # set the initial backoff time to 1 second\n        backoff_time = 1\n\n        # set the max backoff time to 32 seconds\n        max_backoff_time = 32\n\n        # set the initial response to None\n        response = None\n\n        # loop through the retries\n        for i in range(retries):\n            # try to generate the response\n            try:\n                response = self.client.chat.create(**generation_kwargs)\n                break\n            except Exception as e:\n                # if the response is not successful, sleep for the current backoff time and double the backoff time\n                time.sleep(backoff_time)\n                backoff_time = min(backoff_time * backoff_factor, max_backoff"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T, atol=1e-5):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T, atol=1e-5):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.ndim != 2:\n        raise ValueError(\"The matrix must be a 2D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix must be square\")\n    if not np.allclose(x, x.T, atol=1e-5):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal elements must be close to zero\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal elements must be close to zero\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if x.ndim != 2 or x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix must be square\")\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal must be zero\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if x.ndim != 2 or x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix must be square\")\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal elements must be close to zero\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if x.ndim != 2 or x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix must be square\")\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal elements must be close to zero\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if x.ndim != 2 or x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix must be square\")\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal elements must be close to zero\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal of the matrix must be zero\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal elements must be zero\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal elements of the matrix must be close to zero\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal of the matrix must be zero\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal of the matrix must be zero\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if x.ndim != 2 or x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix must be square\")\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal elements must be close to zero\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal of the matrix must be close to zero\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if x.ndim != 2:\n        raise ValueError(\"The matrix must be a 2D array\")\n    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal must be zero\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal must be zero\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must have a zero diagonal\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_symmetric(x)\n    assert_is_square(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must have zeros on the diagonal\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if x.ndim != 2:\n        raise ValueError(\"`x` must be a 2D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")\n    if not np.allclose(x, x.T):\n        raise ValueError(\"`x` must be a symmetric matrix\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"`x` must be a diagonal matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    assert_is_symmetric(x)\n\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal of the matrix must be zeros\")\n\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if not isinstance(x, np.ndarray):\n        raise TypeError(\"Input must be a numpy array\")\n    if not (x.ndim == 2 and x.shape[0] == x.shape[1]):\n        raise ValueError(\"Input must be a square matrix\")\n    if not np.allclose(x, x.T):\n        raise ValueError(\"Input must be a symmetric matrix\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"Input must have zeros on the diagonal\")"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the teacher models\n        teacher_models = self.function_modeler.get_models(function_description)[1]\n        # get the distilled models\n        distilled_models = self.function_modeler.get_models(function_description)[0]\n        # get the examples\n        aligns = self.function_modeler.get_symbolic_alignments(func_hash, max=5)\n        # get the token counts\n        examples = [f\"Inputs:\\nArgs: {align['args']}\\nKwargs: {align['kwargs']}\\nOutput: {align['output']}\" for align in\n                 aligns]\n        examples_token_count = sum([approximate_token_count(example) for example in examples])\n        input_prompt_token_count = approximate_token_count(f\"Function: {function_description}\\n---\\nInputs:\\nArgs: {args}\\nKwargs: {kwargs}\\nOutput:\")\n        generation_tokens = llm_parameters.get(\"max_new_tokens\", self.default_generation_length)\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize it\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # check if the examples are the same\n        if self.initialized_functions[func_hash][\"examples\"] != aligns:\n            # if not, update the examples\n            self.initialized_functions[func_hash][\"examples\"] = aligns\n\n        # check if the model is the same\n        if self.initialized_functions[func_hash][\"model\"] == \"\":\n            # if not, update the model\n            model = self.choose_model_from_tokens(teacher_models, \n                                                  examples_token_count+input_prompt_token_count+generation_tokens)\n            if model:\n                self.initialized_functions[func_hash][\"model\"] = model.model_name\n\n        # check if the model is the same\n        if self"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the models\n        distilled_models, teacher_models = self.function_modeler.get_models(function_description)\n        # get the token counts\n        examples = self.function_modeler.get_symbolic_alignments(function_description.__hash__(), max=5)\n        # get the function description\n        f = str(function_description.__dict__.__repr__() + \"\\n\")\n        # get the prompt\n        prompt = self.construct_prompt(f, args, kwargs, examples, teacher_models[0])\n        # get the token count of the prompt\n        prompt_token_count = approximate_token_count(prompt)\n        # check if the prompt is too long\n        if prompt_token_count > teacher_models[0].context_length:\n            # if it is, then we need to use the distilled model\n            model = distilled_models[0]\n            # check if the prompt is too long for the distilled model\n            if prompt_token_count > model.context_length:\n                # if it is, then we need to use the distilled model\n                model = distilled_models[0]\n                # check if the prompt is too long for the distilled model\n                if prompt_token_count > model.context_length:\n                    # if it is, then we need to use the distilled model\n                    model = distilled_models[0]\n                    # check if the prompt is too long for the distilled model\n                    if prompt_token_count > model.context_length:\n                        # if it is, then we need to use the distilled model\n                        model = distilled_models[0]\n                        # check if the prompt is too long for the distilled model\n                        if prompt_token_count > model.context_length:\n                            # if it is, then we need to use the distilled model\n                            model = distilled_models[0]\n                            # check if the prompt is too long for the distilled model\n                            if prompt_token_count > model.context_length:\n                                # if it is, then we need to use the distilled model\n                                model = distilled_models[0]\n                               "}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {}\n            self.initialized_functions[func_hash][\"model\"] = \"\"\n            self.initialized_functions[func_hash][\"examples\"] = []\n            self.initialized_functions[func_hash][\"distilled_model\"] = None\n\n        # get the examples for the function\n        examples = self.function_modeler.get_symbolic_alignments(func_hash, max=5)\n        # get the models for the function\n        models, distilled_models = self.function_modeler.get_models(function_description)\n        # get the token counts\n        examples_token_count = sum([approximate_token_count(example) for example in examples])\n        input_prompt_token_count = approximate_token_count(f\"Function: {function_description}\\n---\\nInputs:\\nArgs: {args}\\nKwargs: {kwargs}\\nOutput:\")\n        generation_tokens = llm_parameters.get(\"max_new_tokens\", self.default_generation_length)\n        # check if the function is already initialized\n        if self.initialized_functions[func_hash][\"model\"] == \"\":\n            # if not, check if the function is suitable for distillation\n            suitable_for_distillation, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs, function_description, distilled_models[0])\n            if suitable_for_distillation:\n                # if it is, choose the distilled model\n                model = distilled_models[0]\n                save_to_finetune = False\n                is_distilled_model = True\n            else:\n                # if not, choose the teacher model\n                model = models[0]\n                save_to_finetune = True\n                is_distilled_model = False\n        else:\n            # if the function is already initialized, check if the examples have changed\n            if examples"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the distilled model\n        distilled_model = self.function_modeler.get_models(function_description)[0]\n        # get the teacher models\n        teacher_models = self.function_modeler.get_models(function_description)[1]\n        # get the alignments\n        aligns = self.function_modeler.get_symbolic_alignments(func_hash, max=5)\n        # get the examples\n        examples = [f\"Inputs:\\nArgs: {align['args']}\\nKwargs: {align['kwargs']}\\nOutput: {align['output']}\" for align in\n                 aligns]\n        # get the token counts\n        examples_token_count = sum([approximate_token_count(example) for example in examples])\n        input_prompt_token_count = approximate_token_count(f\"Function: {function_description}\\n---\\nInputs:\\nArgs: {args}\\nKwargs: {kwargs}\\nOutput:\")\n        generation_tokens = llm_parameters.get(\"max_new_tokens\", self.default_generation_length)\n        # check if the function is initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize it\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n            # check if the function is suitable for distillation\n            suitable_for_distillation, input_prompt_token_count = self.suitable_for_distillation_token_check(args, kwargs, function_description, distilled_model)\n            if suitable_for_distillation:\n                # if so, use the distilled model\n                model = distilled_model\n                save_to_finetune = False\n            else:\n                # if not, use the teacher model\n                model = self.choose_model_from_tokens(teacher_models, \n                                                      examples_token_count+input_prompt_token_count+generation_tokens)\n                save_to_finetune = True\n        else:\n            # if the"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # check if function is initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize it\n            self.initialize_function(func_hash, function_description)\n        # get the model and examples\n        model, examples = self.get_model_and_examples(func_hash)\n        # check if the model is suitable for distillation\n        suitable_for_distillation, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs, f=function_description, distilled_model=model)\n        # check if the model is suitable for finetuning\n        suitable_for_finetune = self.suitable_for_finetuning_token_check(args, kwargs, f=function_description, distilled_model=model)[0]\n        # construct the prompt\n        prompt = self.construct_prompt(function_description, args, kwargs, examples, model)\n        # check if the model is suitable for distillation\n        if suitable_for_distillation:\n            return prompt, model, False, True\n        elif suitable_for_finetune:\n            return prompt, model, True, False\n        else:\n            return prompt, model, False, False\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the model and the examples\n        distilled_model, teacher_models, examples = self.function_modeler.get_models(function_description)\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize it\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": examples}\n            # check if the model is suitable for distillation\n            if distilled_model:\n                # if it is, use the distilled model\n                model = distilled_model\n                save_to_finetune = False\n                is_distilled_model = True\n            else:\n                # if not, use the teacher model\n                model = teacher_models[0]\n                save_to_finetune = True\n                is_distilled_model = False\n        else:\n            # if it is, check if the examples have changed\n            if self.initialized_functions[func_hash][\"examples\"] != examples:\n                # if they have, update the examples\n                self.initialized_functions[func_hash][\"examples\"] = examples\n                save_to_finetune = True\n            else:\n                save_to_finetune = False\n            # check if the model is suitable for distillation\n            if distilled_model:\n                # if it is, use the distilled model\n                model = distilled_model\n                is_distilled_model = True\n            else:\n                # if not, use the teacher model\n                model = teacher_models[0]\n                is_distilled_model = False\n\n        # check if the model is suitable for finetuning\n        suitable_for_finetune, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs,\n                                                                                                   function_description.name,\n                                                                                                   model)\n        # construct the prompt\n        prompt = self.construct_prompt(function_description.name, args, kwargs, examples, model)\n        return prompt, model, save_to_finetune, is_distilled_model"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the model\n        models = self.function_modeler.get_models(function_description)\n        distilled_model = models[0]\n        teacher_models = models[1]\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize the function\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # check if the function is suitable for distillation\n        suitable_for_distillation, input_prompt_token_count = self.suitable_for_distillation_token_check(args, kwargs, function_description, distilled_model)\n        if suitable_for_distillation:\n            model = distilled_model\n            save_to_finetune = False\n            is_distilled_model = True\n        else:\n            # check if the function is suitable for finetuning\n            suitable_for_finetune, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs, function_description, teacher_models[0])\n            if suitable_for_finetune:\n                model = teacher_models[0]\n                save_to_finetune = True\n                is_distilled_model = False\n            else:\n                # if not, choose the model with the least examples\n                model = self.choose_model_from_tokens(teacher_models, input_prompt_token_count, len(self.initialized_functions[func_hash][\"examples\"]))\n                save_to_finetune = True\n                is_distilled_model = False\n\n        # construct the prompt\n        prompt = self.construct_prompt(str(function_description.__dict__.__repr__() + \"\\n\"), args, kwargs, self.initialized_functions[func_hash][\"examples\"], model)\n\n        # if the function is suitable for finetuning, update the examples\n        if save_to_finetune:\n            self.initialized_functions[func_hash][\""}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the models\n        distilled_model, teacher_models = self.function_modeler.get_models(function_description)\n        # get the examples\n        examples = self.function_modeler.get_symbolic_alignments(func_hash, max=5)\n        # get the token counts\n        examples_token_count = sum([approximate_token_count(example) for example in examples])\n        input_prompt_token_count = approximate_token_count(f\"Function: {function_description}\\n---\\nInputs:\\nArgs: {args}\\nKwargs: {kwargs}\\nOutput:\")\n        generation_tokens = llm_parameters.get(\"max_new_tokens\", self.default_generation_length)\n        # get the model\n        model = self.choose_model_from_tokens(teacher_models, \n                                              examples_token_count+input_prompt_token_count+generation_tokens)\n        if not model:\n            model = distilled_model\n        # check if finetunable\n        suitable_for_finetune, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs, function_description, model)\n        # construct the prompt\n        prompt = self.construct_prompt(function_description, args, kwargs, examples, model)\n        # if not initialized, initialize it\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": model.model_name,\n                                                     \"examples\": examples,\n                                                     \"input_prompt_token_count\": input_prompt_token_count,\n                                                     \"suitable_for_finetuning\": suitable_for_finetune}\n        # if initialized, update the examples if necessary\n        else:\n            if self.initialized_functions[func_hash][\"input_prompt_token_count\"] != input_prompt_token_count:\n                self.initialized_functions[func_hash][\"input_prompt_token_count"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the models\n        distilled_model, teacher_models = self.function_modeler.get_models(function_description)\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize it\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # get the token counts\n        examples = self.function_modeler.get_symbolic_alignments(func_hash, max=5)\n        examples_token_count = sum([approximate_token_count(example[\"output\"]) for example in examples])\n        input_prompt_token_count = approximate_token_count(f\"Function: {function_description}\\n---\\nInputs:\\nArgs: {args}\\nKwargs: {kwargs}\\nOutput:\")\n        generation_tokens = llm_parameters.get(\"max_new_tokens\", self.default_generation_length)\n        # check if the function is suitable for distillation\n        if distilled_model:\n            # if so, choose the distilled model\n            model = distilled_model\n            save_to_finetune = False\n            is_distilled_model = True\n        else:\n            # if not, check if the input is suitable for finetuning\n            suitable_for_finetune, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs, function_description, distilled_model)\n            if suitable_for_finetune:\n                # if so, choose the teacher model\n                model = self.choose_model_from_tokens(teacher_models, input_prompt_token_count+generation_tokens+examples_token_count)\n                save_to_finetune = False\n                is_distilled_model = False\n            else:\n                # if not, choose the distilled model\n                model = distilled_model\n                save_to_finetune = True\n                is_distilled_model = False\n\n        # construct the prompt\n        prompt ="}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the model\n        model, distilled_model = self.function_modeler.get_models(function_description)\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # check if the model is suitable for distillation\n        suitable_for_finetune, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs, function_description, distilled_model)\n        # if the model is suitable for distillation, use the distilled model\n        if suitable_for_finetune:\n            prompt = self.construct_prompt(function_description, args, kwargs, [], distilled_model)\n            return prompt, distilled_model, False, True\n        else:\n            # get the examples\n            examples = self.function_modeler.get_symbolic_alignments(function_description.__hash__(), max=5)\n            # update the examples\n            if examples:\n                self.function_modeler.update_examples(function_description.__hash__(), examples)\n            prompt = self.construct_prompt(function_description, args, kwargs, examples, model)\n            return prompt, model, True, False\n\n    "}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the model\n        model, distilled_model = self.function_modeler.get_models(function_description)\n        # get the alignments\n        aligns = self.function_modeler.get_symbolic_alignments(func_hash, max=5)\n        # get the examples\n        examples = [f\"Inputs:\\nArgs: {align['args']}\\nKwargs: {align['kwargs']}\\nOutput: {align['output']}\" for align in\n                    aligns]\n        # get the token count\n        token_count = self.token_counts.get(func_hash, 0)\n        # get the prompt\n        prompt = self.construct_prompt(str(function_description.__dict__.__repr__() + \"\\n\"), args, kwargs, examples, model)\n        # check if the function is initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize it\n            self.initialized_functions[func_hash] = {\"model\": model.model_name, \"examples\": examples}\n            # update the token count\n            self.token_counts[func_hash] = approximate_token_count(prompt)\n            # check if the function is suitable for finetuning\n            suitable_for_finetune, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs,\n                                                                                                       str(function_description.__dict__.__repr__() + \"\\n\"),\n                                                                                                       distilled_model)\n            # if not, add the examples to the finetuning dataset\n            if not suitable_for_finetune:\n                self.function_modeler.postprocess_symbolic_datapoint(func_hash, function_description,\n                                                                     FunctionExample(args, kwargs, prompt),\n                                                                     repaired=False)\n            # if the function is suitable for finetuning, add the examples to the finetuning dataset\n            else:\n                self.function_modeler.postprocess_symbolic_datapoint(func_hash, function_description,\n                                                                     FunctionExample(args"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # if the function is not initialized, initialize it\n        if func_hash not in self.initialized_functions:\n            self.initialize_function(func_hash, function_description)\n\n        # get the model\n        model = self.choose_model_from_tokens(self.function_modeler.get_models(function_description)[0],\n                                              self.token_counts.get(func_hash, 0))\n\n        # get the examples\n        examples = self.function_modeler.get_symbolic_alignments(func_hash, max=5)\n\n        # construct the prompt\n        prompt = self.construct_prompt(function_description, args, kwargs, examples, model)\n\n        # check if the model is suitable for distillation\n        distilled_model = False\n        if model.system_message_token_count < 0:\n            model.system_message_token_count = approximate_token_count(model.system_message)\n        if model.instruction_token_count < 0:\n            model.instruction_token_count = approximate_token_count(model.instructions)\n        suitable_for_distillation, input_prompt_token_count = self.suitable_for_finetuning_token_check(args,\n                                                                                                       kwargs,\n                                                                                                       function_description,\n                                                                                                       model)\n        save_to_finetune = False\n        if suitable_for_distillation:\n            self.token_counts[func_hash] = input_prompt_token_count\n            save_to_finetune = True\n            distilled_model = True\n        return prompt, model, save_to_finetune, distilled_model\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # check if function is initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize it\n            self.initialized_functions[func_hash] = self.function_modeler.initialize_function(function_description)\n        # get the function examples\n        examples = self.initialized_functions[func_hash][\"examples\"]\n        # get the distilled models\n        distilled_models = self.function_modeler.get_models(function_description)[0]\n        # get the teacher models\n        teacher_models = self.function_modeler.get_models(function_description)[1]\n        # get the token counts\n        examples_token_count = sum([approximate_token_count(example) for example in examples])\n        input_prompt_token_count = approximate_token_count(f\"Function: {function_description}\\n---\\nInputs:\\nArgs: {args}\\nKwargs: {kwargs}\\nOutput:\")\n        generation_tokens = llm_parameters.get(\"max_new_tokens\", self.default_generation_length)\n        # get the model\n        model = self.choose_model_from_tokens(distilled_models, input_prompt_token_count+generation_tokens+examples_token_count, len(examples))\n        if model:\n            # if a model is found, return it\n            prompt = self.construct_prompt(str(function_description.__dict__.__repr__() + \"\\n\"), args, kwargs, examples, model)\n            return prompt, model, False, True\n        else:\n            # if not, get the teacher model\n            model = self.choose_model_from_tokens(teacher_models, input_prompt_token_count+generation_tokens+examples_token_count, len(examples))\n            if model:\n                # if a model is found, return it\n                prompt = self.construct_prompt(str(function_description.__dict__.__repr__() + \"\\n\"), args, kwargs, examples, model)\n                return prompt, model, True, False\n           "}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the model\n        model, distilled_model = self.function_modeler.get_models(function_description)\n        # get the examples\n        examples = self.function_modeler.get_symbolic_alignments(func_hash, max=10)\n        # get the prompt\n        prompt = self.construct_prompt(str(function_description.__dict__.__repr__() + \"\\n\"), args, kwargs, examples, model)\n        # check if the model is suitable for finetuning\n        suitable_for_finetune, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs, str(function_description.__dict__.__repr__() + \"\\n\"), distilled_model)\n        # check if the model is suitable for distillation\n        suitable_for_distillation = self.suitable_for_distillation_token_check(args, kwargs, str(function_description.__dict__.__repr__() + \"\\n\"), distilled_model)\n        # check if the model is already initialized\n        already_initialized = self.function_modeler.check_if_initialized(func_hash)\n        # if the model is not initialized, initialize it\n        if not already_initialized:\n            self.function_modeler.initialize_function(func_hash, function_description, model, distilled_model, examples)\n        else:\n            # if the model is initialized, update the examples\n            self.function_modeler.update_function_examples(func_hash, examples)\n        # if the model is suitable for distillation, return the distilled model\n        if suitable_for_distillation:\n            return prompt, distilled_model, True, True\n        # if the model is not suitable for distillation, return the model\n        else:\n            return prompt, model, suitable_for_finetune, False\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the models\n        models = self.function_modeler.get_models(function_description)\n        # get the examples\n        examples = self.function_modeler.get_symbolic_alignments(func_hash)\n        # get the token counts\n        examples_token_count = sum([approximate_token_count(example) for example in examples])\n        input_prompt_token_count = approximate_token_count(f\"Function: {function_description}\\n---\\nInputs:\\nArgs: {args}\\nKwargs: {kwargs}\\nOutput:\")\n        generation_tokens = llm_parameters.get(\"max_new_tokens\", self.default_generation_length)\n        # check if finetunable\n        finetunable, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs, function_description, models[0])\n        # check if the model is distilled\n        distilled_model = models[0].distilled_model\n        # if the model is distilled, then we can use the model with the least examples\n        if distilled_model:\n            model = self.choose_model_from_tokens(models, input_prompt_token_count+generation_tokens+examples_token_count)\n            # if the model is not distilled, then we can use the model with the least examples\n        else:\n            model = self.choose_model_from_tokens(models, input_prompt_token_count+generation_tokens+examples_token_count, len(examples))\n        # construct the prompt\n        prompt = self.construct_prompt(function_description, args, kwargs, examples, model)\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize the function\n            self.initialized_functions[func_hash] = {\"model\": model.model_name, \"examples\": examples}\n        # check if the function is suitable for finetuning\n        if finetunable:\n            # if so, save"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialize_function(function_description, llm_parameters)\n        # get the model\n        model = self.choose_model(function_description)\n        # get the examples\n        examples = self.function_modeler.get_symbolic_alignments(func_hash, max=10)\n        # check if the model is suitable for distillation\n        distilled_model = self.suitable_for_distillation(function_description, model, args, kwargs, examples)\n        # check if the model is suitable for finetuning\n        suitable_for_finetune = self.suitable_for_finetuning(function_description, model, args, kwargs, examples)\n        # construct the prompt\n        prompt = self.construct_prompt(function_description, args, kwargs, examples, model)\n        return prompt, model, suitable_for_finetune, distilled_model\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the models\n        distilled_models, teacher_models = self.function_modeler.get_models(function_description)\n        # get the token counts\n        examples = self.function_modeler.get_symbolic_alignments(func_hash, max=5)\n        examples_token_count = sum([approximate_token_count(example) for example in examples])\n        input_prompt_token_count = approximate_token_count(f\"Function: {function_description}\\n---\\nInputs:\\nArgs: {args}\\nKwargs: {kwargs}\\nOutput:\")\n        generation_tokens = llm_parameters.get(\"max_new_tokens\", self.default_generation_length)\n        # check if finetunable\n        suitable_for_finetune, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs, function_description, distilled_models[0])\n        # check if distillable\n        suitable_for_distillation = self.suitable_for_distillation_token_check(args, kwargs, function_description, distilled_models[0], examples_token_count, input_prompt_token_count, generation_tokens)\n        # check if initialized\n        initialized = self.function_modeler.function_initialized(func_hash)\n        # if not initialized, initialize it\n        if not initialized:\n            self.function_modeler.initialize_function(func_hash, function_description)\n        # if initialized, check if examples are up to date\n        if initialized:\n            self.function_modeler.check_examples_up_to_date(func_hash, function_description, examples, examples_token_count)\n        # if not initialized, add examples\n        if initialized and not suitable_for_finetune:\n            self.function_modeler.add_examples(func_hash, function_description, examples, examples_token_count)\n        # if initialized and not suitable for finetune, save examples\n        if initialized and not suitable_for_finetune:"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the model\n        models = self.function_modeler.get_models(function_description)\n        # get the examples\n        aligns = self.function_modeler.get_symbolic_alignments(func_hash, max=5)\n        # get the token count\n        token_count = self.token_counts.get(func_hash, 0)\n        # get the function description\n        f = str(function_description.__dict__.__repr__() + \"\\n\")\n        # get the prompt\n        prompt = self.construct_prompt(f, args, kwargs, aligns, models[0])\n        # get the model\n        model = models[0]\n        # get the token count\n        token_count = approximate_token_count(prompt)\n        # get the distilled model\n        distilled_model = models[1]\n        # get the suitable for finetuning\n        suitable_for_finetuning = self.suitable_for_finetuning_token_check(args, kwargs, f, distilled_model)\n        # get the save to finetune\n        save_to_finetune = False\n        if suitable_for_finetuning[0] and token_count < distilled_model.context_length:\n            save_to_finetune = True\n\n        # if the function is not initialized, initialize it\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": model.model_name, \"examples\": []}\n            self.token_counts[func_hash] = 0\n\n        # if the function is initialized, update the examples\n        if func_hash in self.initialized_functions:\n            if save_to_finetune:\n                self.initialized_functions[func_hash][\"examples\"].append(f\"Inputs:\\nArgs: {args}\\nKwargs: {kwargs}\\nOutput: {prompt}\")\n                self.token_counts[func_hash] += token_count\n\n        # if the function is initialized and not suitable for finetuning, update"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize it\n            self.initialized_functions[func_hash] = {}\n            self.initialized_functions[func_hash][\"model\"] = \"\"\n            self.initialized_functions[func_hash][\"examples\"] = []\n            self.initialized_functions[func_hash][\"model_type\"] = \"\"\n\n        # get the models\n        models, teacher_models = self.function_modeler.get_models(function_description)\n        # get the examples\n        examples = self.function_modeler.get_symbolic_alignments(func_hash, max=5)\n        # check if the function is suitable for finetuning\n        suitable_for_finetuning, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs,\n                                                                                                    function_description,\n                                                                                                    teacher_models[0])\n        # get the prompt\n        prompt = self.construct_prompt(function_description, args, kwargs, examples, teacher_models[0])\n        # check if the prompt is suitable for distillation\n        suitable_for_distillation = self.suitable_for_distillation_token_check(args, kwargs, function_description,\n                                                                               models[0])\n        # return the prompt, model, suitable for distillation and if the function is initialized\n        return prompt, models[0], suitable_for_finetuning, suitable_for_distillation\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the model and the alignment statements\n        model, aligns = self.function_modeler.get_models(function_description)\n        # check if the model is already initialized\n        if func_hash in self.initialized_functions:\n            # if it is, get the model and the aligns\n            model = self.initialized_functions[func_hash][\"model\"]\n            aligns = self.initialized_functions[func_hash][\"examples\"]\n        else:\n            # if it's not, initialize it\n            self.initialized_functions[func_hash] = {\"model\": model, \"examples\": aligns}\n\n        # check if the model is suitable for distillation\n        suitable_for_finetune, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs, str(function_description.__dict__.__repr__() + \"\\n\"), model)\n        # if it is, get the distilled model\n        if suitable_for_finetune:\n            # if it's not, get the distilled model\n            distilled_model = self.function_modeler.get_distilled_model(function_description)\n            # if it's not, get the distilled model\n            if distilled_model:\n                model = distilled_model\n\n        # construct the prompt\n        prompt = self.construct_prompt(str(function_description.__dict__.__repr__() + \"\\n\"), args, kwargs, aligns, model)\n\n        # save the token counts\n        self.token_counts[func_hash] = input_prompt_token_count\n\n        # return the prompt, model, is_distilled_model, is_initialized\n        return prompt, model, suitable_for_finetune, model.is_distilled_model\n\n    "}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        return _cov_nearest_higham(cov, higham_max_iteration)\n    else:\n        return _cov_nearest_clipping(cov)"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    assert_is_symmetric(cov)\n    if higham:\n        cov_nearest = _cov_nearest_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        cov_nearest = _cov_nearest_clipped(cov)\n    return cov_nearest\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if cov.ndim != 2:\n        raise ValueError(\"`cov` must be a 2D array\")\n    if not is_cholesky_dec(cov):\n        if higham:\n            cov = _cov_nearest_higham(cov, max_iteration=higham_max_iteration)\n        else:\n            cov = _cov_nearest_clipping(cov)\n    return cov\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Use the Higham & Nick (2002) algorithm\n        cov_nearest = higham_cov_nearest(cov, max_iteration=higham_max_iteration)\n    else:\n        # Use the clipping method\n        cov_nearest = clip_cov_nearest(cov)\n\n    return cov_nearest\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        return _cov_nearest_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        return _cov_nearest_clipping(cov)\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    if not is_cholesky_dec(cov):\n        if higham:\n            cov = _cov_nearest_higham(cov, higham_max_iteration)\n        else:\n            cov = _cov_nearest_clipped(cov)\n    return cov\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Higham & Nick (2002)\n        def _higham_algorithm(cov, max_iteration):\n            # Step 1: Compute the eigenvalues and eigenvectors of the input covariance matrix\n            eigvals, eigvecs = np.linalg.eigh(cov)\n\n            # Step 2: Set the diagonal elements of the input covariance matrix to the eigenvalues\n            cov_diag = np.diag(eigvals)\n\n            # Step 3: Compute the eigenvalues and eigenvectors of the modified covariance matrix\n            eigvals_diag, eigvecs_diag = np.linalg.eigh(cov_diag)\n\n            # Step 4: Set the diagonal elements of the modified covariance matrix to the eigenvalues\n            cov_diag_diag = np.diag(eigvals_diag)\n\n            # Step 5: Compute the eigenvalues and eigenvectors of the modified covariance matrix\n            eigvals_diag_diag, eigvecs_diag_diag = np.linalg.eigh(cov_diag_diag)\n\n            # Step 6: Compute the eigenvalues and eigenvectors of the modified covariance matrix\n            eigvals_diag_diag_diag, eigvecs_diag_diag_diag = np.linalg.eigh(\n                cov_diag_diag_diag\n            )\n\n            # Step 7: Compute the eigenvalues and eigenvectors of the modified covariance matrix\n            eigvals_diag_diag_diag_diag, eigvecs_diag_diag_diag_diag = np.linalg.eigh(\n                cov_diag_diag_diag_diag\n            )\n\n            # Step 8: Compute the eigenvalues and eigenvectors of the modified covariance matrix\n            eigvals_diag_diag_diag_diag_diag, eigvecs_diag_diag_diag_diag_diag = np.linalg.eigh(\n                cov_"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Compute the nearest positive definite matrix using the Higham & Nick (2002) algorithm\n        # The algorithm is based on the Symmetric Eigenvalue Decomposition (SEDE) algorithm\n        # The algorithm is implemented in the scipy.linalg.solve_sylvester function\n        # The algorithm is described in the following paper:\n        # Higham, Nicholas J., and Peter J. Nick. \"Computing the nearest correlation matrix:\n        # A problem from finance.\" IMA Journal of Numerical Analysis 17.1 (2002): 171-179.\n        # The algorithm is implemented in the scipy.linalg.solve_sylvester function\n        # The algorithm is described in the following paper:\n        # Higham, Nicholas J., and Peter J. Nick. \"Computing the nearest correlation matrix:\n        # A problem from finance.\" IMA Journal of Numerical Analysis 17.1 (2002): 171-179.\n        # The algorithm is implemented in the scipy.linalg.solve_sylvester function\n        # The algorithm is described in the following paper:\n        # Higham, Nicholas J., and Peter J. Nick. \"Computing the nearest correlation matrix:\n        # A problem from finance.\" IMA Journal of Numerical Analysis 17.1 (2002): 171-179.\n        # The algorithm is implemented in the scipy.linalg.solve_sylvester function\n        # The algorithm is described in the following paper:\n        # Higham, Nicholas J., and Peter J. Nick. \"Computing the nearest correlation matrix:\n        # A problem from finance.\" IMA Journal of Numerical Analysis 17.1 (2002): 171-179.\n        # The algorithm is implemented in the scipy.linalg.solve_sylvester function\n        # The algorithm is described in the following paper:\n        # Higham, Nicholas J., and Peter J. Nick. \"Computing the nearest correlation matrix:"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Compute the nearest positive definite covariance matrix using the Higham & Nick (2002) algorithm\n        cov_nearest = cov_nearest_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        # Compute the nearest positive definite covariance matrix using clipping\n        cov_nearest = cov_nearest_clipping(cov)\n\n    return cov_nearest\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        cov_nearest = higham_cov_nearest(cov, max_iteration=higham_max_iteration)\n    else:\n        cov_nearest = _clip_cov(cov)\n    return cov_nearest\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Higham & Nick (2002) algorithm\n        (n, m) = cov.shape\n        if n != m:\n            raise ValueError(\"The matrix must be square\")\n        if not np.allclose(cov, cov.T):\n            raise ValueError(\"The matrix must be symmetric\")\n        if not np.allclose(np.diag(cov), np.zeros(n), atol=1e-5):\n            raise ValueError(\"The distance matrix must have diagonal elements close to zeros\")\n        d = np.diag(cov).copy()\n        d[d < 0] = 0\n        d_sqrt = np.sqrt(d)\n        if np.any(d_sqrt == 0):\n            raise ValueError(\"The matrix must have positive eigenvalues\")\n        cov_sqrt = cov / d_sqrt / d_sqrt[:, None]\n        cov_sqrt = np.linalg.cholesky(cov_sqrt)\n        cov_sqrt = cov_sqrt * d_sqrt / np.diag(cov_sqrt)\n        cov_nearest = cov_sqrt @ cov_sqrt.T\n    else:\n        # Clipping eigenvalues\n        (n, m) = cov.shape\n        if n != m:\n            raise ValueError(\"The matrix must be square\")\n        if not np.allclose(cov, cov.T):\n            raise ValueError(\"The matrix must be symmetric\")\n        if not np.allclose(np.diag(cov), np.zeros(n), atol=1e-5):\n            raise ValueError(\"The distance matrix must have diagonal elements close to zeros\")\n        d = np.diag(cov).copy()\n        d[d < 0] = 0\n        d_sqrt = np.sqrt(d)\n        if np.any(d_sqrt == 0):\n            raise ValueError(\"The matrix must have positive eigenvalues\")\n        cov_sqrt = cov / d_sqrt / d_sqrt[:, None]\n        cov_sqrt = np.linalg.cholesky(cov_sqrt)\n        cov_sqrt = cov_sqrt * d_sqrt / np.di"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        return _cov_nearest_higham(cov, max_iteration=higham_max_iteration)\n    else:\n        return _cov_nearest_clipped(cov)\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        if not is_cholesky_dec(cov):\n            cov = _higham_algorithm(cov, max_iteration=higham_max_iteration)\n    else:\n        cov = _clipping_algorithm(cov)\n    return cov\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        return _cov_nearest_higham(cov, higham_max_iteration)\n    else:\n        return _cov_nearest_clipping(cov)\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Higham & Nick (2002)\n        # The Higham & Nick (2002) algorithm is used to compute the nearest positive definite matrix.\n        # The algorithm is based on the modified Cholesky decomposition method.\n        # It involves iteratively adjusting the covariance matrix until it becomes positive definite.\n        # The algorithm stops when the maximum number of iterations is reached or when the covariance matrix becomes positive definite.\n        # The resulting covariance matrix is then returned.\n        # The algorithm is implemented in the `higham_cov_nearest` function.\n        return higham_cov_nearest(cov, max_iteration=higham_max_iteration)\n    else:\n        # Clip eigenvalues\n        # The eigenvalues are clipped based on the specified parameters to ensure the resulting matrix is positive definite.\n        # The clipping is done by setting the eigenvalues below the clipping value to the clipping value.\n        # The resulting covariance matrix is then returned.\n        # The clipping value is set to a small positive value to ensure the resulting matrix is positive definite.\n        # The clipping value is set to a small positive value to ensure the resulting matrix is positive definite.\n        # The clipping value is set to a small positive value to ensure the resulting matrix is positive definite.\n        # The clipping value is set to a small positive value to ensure the resulting matrix is positive definite.\n        # The clipping value is set to a small positive value to ensure the resulting matrix is positive definite.\n        # The clipping value is set to a small positive value to ensure the resulting matrix is positive definite.\n        # The clipping value is set to a small positive value to ensure the resulting matrix is positive definite.\n        # The clipping value is set to a small positive value to ensure the resulting matrix is positive definite.\n        # The clipping value is set to a small positive value to ensure the resulting matrix is positive definite.\n        # The clipping value is set to a small positive value to ensure the resulting matrix is positive definite.\n        # The clipping value is set to a small positive value to ensure the resulting matrix is positive definite.\n        # The clipping value is set to a small positive value to ensure the resulting matrix is positive definite."}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    if not np.allclose(cov, cov.T):\n        raise ValueError(\"`cov` must be symmetric\")\n\n    if higham:\n        # Higham & Nick (2002) algorithm\n        # The Higham & Nick (2002) algorithm is used to compute the nearest positive definite matrix.\n        # It is based on the iterative process of adding a small multiple of the identity matrix to the input matrix until a positive definite matrix is obtained.\n        # The algorithm stops when the matrix becomes positive definite or the maximum number of iterations is reached.\n        # The multiple is chosen to be the smallest positive number that makes the matrix positive definite.\n        # The algorithm is implemented in the function _higham_nick_algorithm.\n        cov_nearest = _higham_nick_algorithm(cov, max_iteration=higham_max_iteration)\n    else:\n        # Clipping eigenvalues\n        # The eigenvalues are clipped to ensure that the resulting matrix is positive definite.\n        # The clipping is done by adding a small multiple of the identity matrix to the input matrix.\n        # The multiple is chosen to be the smallest positive number that makes the matrix positive definite.\n        cov_nearest = _clip_eigenvalues(cov)\n\n    return cov_nearest\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Higham & Nick (2002) algorithm\n        cov_nearest = np.copy(cov)\n        for _ in range(higham_max_iteration):\n            cov_nearest = _higham_nick_2002(cov_nearest)\n            if is_cholesky_dec(cov_nearest):\n                break\n    else:\n        # Clipping eigenvalues\n        cov_nearest = np.copy(cov)\n        cov_nearest = _clip_eigenvalues(cov_nearest)\n    return cov_nearest\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    assert_is_symmetric(cov)\n\n    if higham:\n        # Compute the nearest positive definite matrix using the Higham & Nick (2002) algorithm\n        # The algorithm is based on the idea of iteratively adding a small multiple of the identity matrix to the input matrix until it becomes positive definite\n        # The algorithm is designed to handle matrices with large eigenvalues and avoid numerical instability\n        # The algorithm is implemented using a combination of the scipy.linalg.eigh and scipy.linalg.cholesky functions\n        # The algorithm is adapted from the original Higham & Nick (2002) algorithm to handle large eigenvalues and avoid numerical instability\n        # The algorithm is implemented using a combination of the scipy.linalg.eigh and scipy.linalg.cholesky functions\n        # The algorithm is adapted from the original Higham & Nick (2002) algorithm to handle large eigenvalues and avoid numerical instability\n        # The algorithm is implemented using a combination of the scipy.linalg.eigh and scipy.linalg.cholesky functions\n        # The algorithm is adapted from the original Higham & Nick (2002) algorithm to handle large eigenvalues and avoid numerical instability\n        # The algorithm is implemented using a combination of the scipy.linalg.eigh and scipy.linalg.cholesky functions\n        # The algorithm is adapted from the original Higham & Nick (2002) algorithm to handle large eigenvalues and avoid numerical instability\n        # The algorithm is implemented using a combination of the scipy.linalg.eigh and scipy.linalg.cholesky functions\n        # The algorithm is adapted from the original Higham & Nick (2002) algorithm to handle large eigenvalues and avoid numerical instability\n        # The algorithm is implemented using a combination of the scipy.linalg.eigh and scipy.linalg.cholesky functions\n        # The algorithm is adapted from the original Higham & Nick (2"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    # Check if the input matrix is a valid covariance matrix\n    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    if cov.shape[0] != cov.shape[1]:\n        raise ValueError(\"`cov` must be a square matrix\")\n\n    # Compute the nearest positive definite matrix\n    if higham:\n        # Use the Higham & Nick (2002) algorithm to find the nearest positive definite matrix\n        # This algorithm is based on the idea of iteratively adding a small positive value to the diagonal elements of the matrix\n        # The algorithm stops when the resulting matrix is positive definite or when the maximum number of iterations is reached\n        cov_nearest = cov.copy()\n        for _ in range(higham_max_iteration):\n            # Compute the eigenvalues and eigenvectors of the matrix\n            eigvals, eigvecs = np.linalg.eigh(cov_nearest)\n\n            # Check if the matrix is already positive definite\n            if np.all(eigvals > 0):\n                break\n\n            # Add a small positive value to the diagonal elements of the matrix\n            eigvals[eigvals < 0] += _CLIPPING_VALUE\n            cov_nearest = eigvecs @ np.diag(eigvals) @ eigvecs.T\n\n        # Check if the matrix is still positive definite after the maximum number of iterations\n        if not np.all(np.linalg.eigvals(cov_nearest) > 0):\n            raise ValueError(\"The matrix is not positive definite after the maximum number of iterations\")\n\n    else:\n        # Clip the eigenvalues of the matrix to ensure they are positive\n        eigvals, eigvecs = np.linalg.eigh(cov)\n        eigvals[eigvals < 0] = 0\n        cov_nearest = eigvecs @ np.diag(eigvals) @ eigvecs.T\n\n    # Return the nearest positive definite matrix\n    return cov_nearest"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if cov.ndim != 2:\n        raise ValueError(\"`cov` must be a 2D array\")\n    assert_is_symmetric(cov)\n\n    if higham:\n        # Higham & Nick (2002)\n        # Compute the nearest covariance matrix that is positive definite and allows for a Cholesky decomposition, keeping the variance unchanged.\n        # This function uses the Higham & Nick (2002) algorithm to find the nearest covariance matrix.\n        # The input covariance matrix is used as the base for computing the nearest positive definite covariance matrix.\n        # The Higham & Nick (2002) algorithm is an iterative algorithm that adjusts the input covariance matrix to ensure that it is positive definite and allows for a Cholesky decomposition.\n        # The algorithm iteratively adjusts the input covariance matrix by adding a small positive value to the diagonal elements of the input matrix.\n        # The algorithm stops when the resulting covariance matrix is positive definite and allows for a Cholesky decomposition.\n        # The input covariance matrix is modified in place.\n        # The function returns the modified input covariance matrix.\n        # The Higham & Nick (2002) algorithm is a robust algorithm that can handle matrices with large eigenvalues.\n        # The algorithm is based on the idea of adjusting the input matrix to ensure that it is positive definite.\n        # The algorithm is iterative and can be applied to any covariance matrix.\n        # The algorithm is based on the idea of adjusting the input matrix to ensure that it is positive definite.\n        # The algorithm is iterative and can be applied to any covariance matrix.\n        # The algorithm is based on the idea of adjusting the input matrix to ensure that it is positive definite.\n        # The algorithm is iterative and can be applied to any covariance matrix.\n        # The algorithm is based on the idea of adjusting the input matrix to ensure that it is positive definite.\n        # The algorithm is iterative and can be applied to any covariance matrix.\n        # The algorithm is based on the idea of adjusting the input matrix to ensure that it is positive definite.\n        # The algorithm is iterative and can be applied to any covariance matrix.\n        # The algorithm is based on the idea of adjusting the input matrix to ensure"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home=data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home=data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home=data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home=data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home=data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home=data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home, ignore_errors=True)"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, torch.Tensor):\n        return obj, IdentitySchema()\n    elif isinstance(obj, str):\n        return obj, IdentitySchema()\n    elif isinstance(obj, bytes):\n        return obj, IdentitySchema()\n    elif isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    elif isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    elif isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, (Boxes, ROIMasks)):\n        return TensorWrapSchema.flatten(obj)\n    else:\n        raise ValueError(f\"Unsupported type {type(obj)}\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, bytes):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    if isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    if isinstance(obj, dict):\n        return DictSchema.flatten(obj)\n    if isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    if isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    raise ValueError(f\"Cannot flatten object of type {type(obj)}\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, str):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, bytes):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    elif isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    elif isinstance(obj, dict):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    elif isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    elif isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    else:\n        raise ValueError(f\"Cannot flatten object of type {type(obj)}\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, torch.Tensor):\n        return obj, IdentitySchema()\n    if isinstance(obj, (list, tuple)):\n        return ListSchema.flatten(obj)\n    if isinstance(obj, dict):\n        return DictSchema.flatten(obj)\n    if isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    if isinstance(obj, (Boxes, ROIMasks)):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    raise TypeError(f\"Cannot flatten object of type {type(obj)}\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, (list, tuple)):\n        ret, schema = ListSchema.flatten(obj)\n        return ret, schema\n    elif isinstance(obj, collections.Mapping):\n        ret, schema = DictSchema.flatten(obj)\n        return ret, schema\n    elif isinstance(obj, Instances):\n        ret, schema = InstancesSchema.flatten(obj)\n        return ret, schema\n    elif isinstance(obj, (Boxes, ROIMasks)):\n        ret, schema = TensorWrapSchema.flatten(obj)\n        return ret, schema\n    else:\n        raise ValueError(f\"Unsupported type {type(obj)}\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, bytes):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    if isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    if isinstance(obj, dict):\n        return DictSchema.flatten(obj)\n    if isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    if isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    raise TypeError(f\"Unsupported type {type(obj)}\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    if isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    if isinstance(obj, dict):\n        return DictSchema.flatten(obj)\n    if isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    if isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    raise TypeError(f\"Cannot flatten object of type {type(obj)}.\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    elif isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    elif isinstance(obj, dict):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    elif isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    elif isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    else:\n        raise TypeError(f\"Cannot flatten object of type {type(obj)}.\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, bytes):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    if isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    if isinstance(obj, dict):\n        return DictSchema.flatten(obj)\n    if isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    if isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    raise ValueError(f\"Cannot flatten type {type(obj)}\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, list):\n        ret, schema = ListSchema.flatten(obj)\n        return ret, schema\n    elif isinstance(obj, tuple):\n        ret, schema = TupleSchema.flatten(obj)\n        return ret, schema\n    elif isinstance(obj, dict):\n        ret, schema = DictSchema.flatten(obj)\n        return ret, schema\n    elif isinstance(obj, Instances):\n        ret, schema = InstancesSchema.flatten(obj)\n        return ret, schema\n    elif isinstance(obj, (Boxes, ROIMasks)):\n        ret, schema = TensorWrapSchema.flatten(obj)\n        return ret, schema\n    elif isinstance(obj, collections.abc.Mapping):\n        ret, schema = DictSchema.flatten(obj)\n        return ret, schema\n    else:\n        raise TypeError(f\"Cannot flatten object of type {type(obj)}\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    if isinstance(obj, collections.abc.Sequence):\n        return ListSchema.flatten(obj)\n    if isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    if isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    if isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    raise ValueError(f\"Cannot flatten {type(obj)}\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, str):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, bytes):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    if isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    if isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    if isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    if isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n\n    raise TypeError(f\"Cannot flatten {type(obj)}.\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, str):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, bytes):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    elif isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    elif isinstance(obj, dict):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    elif isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    elif isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    else:\n        raise ValueError(f\"Cannot flatten object of type {type(obj)}.\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, (list, tuple)):\n        res = [flatten_to_tuple(k) for k in obj]\n        values, schema = ListSchema.flatten(res)\n        return values, schema\n    if isinstance(obj, collections.abc.Mapping):\n        res = [flatten_to_tuple(obj[k]) for k in sorted(obj.keys())]\n        values, schema = ListSchema.flatten(res)\n        return values, DictSchema(schema.schemas, schema.sizes, list(obj.keys()))\n    if isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    if isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    raise ValueError(f\"Unsupported type {type(obj)}\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n\n    if isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n\n    if isinstance(obj, (Boxes, ROIMasks)):\n        return TensorWrapSchema.flatten(obj)\n\n    if isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n\n    if isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n\n    if isinstance(obj, (list, tuple)):\n        return ListSchema.flatten(obj)\n\n    raise ValueError(f\"Unsupported type {type(obj)}\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, list):\n        res = [flatten_to_tuple(k) for k in obj]\n        values, schema = ListSchema.flatten(res)\n        return values, schema\n    elif isinstance(obj, tuple):\n        res = [flatten_to_tuple(k) for k in obj]\n        values, schema = TupleSchema.flatten(res)\n        return values, schema\n    elif isinstance(obj, dict):\n        res = [flatten_to_tuple(k) for k in obj.values()]\n        values, schema = DictSchema.flatten(res)\n        return values, DictSchema(schema.schemas, schema.sizes, list(obj.keys()))\n    elif isinstance(obj, Instances):\n        res, schema = InstancesSchema.flatten(obj)\n        return res, schema\n    elif isinstance(obj, (Boxes, ROIMasks)):\n        res, schema = TensorWrapSchema.flatten(obj)\n        return res, schema\n    else:\n        raise ValueError(f\"Unsupported type {type(obj)}\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, torch.Tensor):\n        return obj, IdentitySchema()\n    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, list):\n        res = [flatten_to_tuple(k) for k in obj]\n        values, schema = ListSchema.flatten(obj)\n        return values, schema\n    if isinstance(obj, tuple):\n        res = [flatten_to_tuple(k) for k in obj]\n        values, schema = TupleSchema.flatten(obj)\n        return values, schema\n    if isinstance(obj, dict):\n        res = [flatten_to_tuple(obj[k]) for k in obj.keys()]\n        values, schema = DictSchema.flatten(obj)\n        return values, schema\n    if isinstance(obj, Instances):\n        values, schema = InstancesSchema.flatten(obj)\n        return values, schema\n    if isinstance(obj, (Boxes, ROIMasks)):\n        values, schema = TensorWrapSchema.flatten(obj)\n        return values, schema\n    if isinstance(obj, collections.abc.Mapping):\n        res = [flatten_to_tuple(obj[k]) for k in obj.keys()]\n        values, schema = DictSchema.flatten(obj)\n        return values, schema\n\n    raise TypeError(f\"Cannot flatten object of type {type(obj)}\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, str):\n        return (torch.tensor(ord(c)) for c in obj), TupleSchema(\n            [IdentitySchema() for _ in range(len(obj))]\n        )\n    if isinstance(obj, bytes):\n        return (torch.tensor(b) for b in obj), TupleSchema(\n            [IdentitySchema() for _ in range(len(obj))]\n        )\n    if isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    if isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    if isinstance(obj, dict):\n        return DictSchema.flatten(obj)\n    if isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    if isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n\n    if isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n\n    raise ValueError(f\"Unsupported type {type(obj)}\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, str):\n        return (torch.tensor(obj.encode()),), TensorWrapSchema(\"str\")\n    elif isinstance(obj, bytes):\n        return (torch.tensor(obj),), TensorWrapSchema(\"bytes\")\n    elif isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    elif isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    elif isinstance(obj, dict):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, Boxes):\n        return (obj.tensor,), TensorWrapSchema(\"Boxes\")\n    elif isinstance(obj, ROIMasks):\n        return ROIMasks.flatten(obj)\n    elif isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    else:\n        raise TypeError(f\"Type {type(obj)} is not supported for flattening.\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, torch.Tensor):\n        return obj, IdentitySchema()\n    if isinstance(obj, (list, tuple)):\n        ret, schema = ListSchema.flatten(obj)\n        return ret, schema\n    if isinstance(obj, dict):\n        ret, schema = DictSchema.flatten(obj)\n        return ret, schema\n    if isinstance(obj, Instances):\n        ret, schema = InstancesSchema.flatten(obj)\n        return ret, schema\n    if isinstance(obj, (Boxes, ROIMasks)):\n        ret, schema = TensorWrapSchema.flatten(obj)\n        return ret, schema\n    if isinstance(obj, str):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, bytes):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, collections.Mapping):\n        raise ValueError(\"Cannot flatten a mapping object.\")\n    if isinstance(obj, collections.Sequence):\n        raise ValueError(\"Cannot flatten a sequence object.\")\n    raise ValueError(f\"Cannot flatten an object of type {type(obj)}.\")"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.array(groups)\n    equations = np.array(equations)\n    if groups.ndim != 2:\n        raise EquationToMatrixError(\n            f\"{names[0]} should be a 2D array of shape (n_groups, n_assets)\"\n        )\n    if equations.ndim != 1:\n        raise EquationToMatrixError(\n            f\"{names[1]} should be a 1D array of shape (n_equations,)\"\n        )\n    if groups.shape[1] != equations.shape[0]:\n        raise EquationToMatrixError(\n            f\"{names[0]} and {names[1]} should have the same number of assets\"\n        )\n    n = groups.shape[1]\n    left = np.zeros((equations.shape[0], n))\n    right = np.zeros(equations.shape[0])\n    for i, equation in enumerate(equations):\n        try:\n            left[i], right[i] = _string_to_equation(\n                groups=groups, string=equation, sum_to_one=sum_to_one\n            )\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            warnings.warn(str(e))\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    n_groups, n_assets = groups.shape\n    if sum_to_one:\n        groups = groups / groups.sum(axis=1, keepdims=True)\n    n_equations = len(equations)\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n    for i, equation in enumerate(equations):\n        try:\n            left[i], right[i] = _string_to_equation(\n                groups=groups, string=equation, sum_to_one=sum_to_one\n            )\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            warnings.warn(\n                f\"{e}. The equation '{equation}' will be ignored\",\n                UserWarning,\n            )\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    if not isinstance(groups, np.ndarray):\n        groups = np.array(groups)\n    if not isinstance(equations, np.ndarray):\n        equations = np.array(equations)\n    if groups.ndim != 2:\n        raise EquationToMatrixError(\n            f\"The '{names[0]}' array should be 2D. It is {groups.ndim}D\"\n        )\n    if equations.ndim != 1:\n        raise EquationToMatrixError(\n            f\"The '{names[1]}' array should be 1D. It is {equations.ndim}D\"\n        )\n\n    n = groups.shape[1]\n    left = np.zeros((equations.size, n))\n    right = np.zeros(equations.size)\n    for i, eq in enumerate(equations):\n        try:\n            l, r = _string_to_equation(groups=groups, string=eq, sum_to_one=sum_to_one)\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e from None\n            warnings.warn(str(e))\n            continue\n        left[i] = l\n        right[i] = r\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    if groups.ndim != 2:\n        raise EquationToMatrixError(\n            f\"The '{names[0]}' array should have 2 dimensions, but it has {groups.ndim}\"\n        )\n    if equations.ndim != 1:\n        raise EquationToMatrixError(\n            f\"The '{names[1]}' array should have 1 dimension, but it has {equations.ndim}\"\n        )\n\n    n = groups.shape[1]\n    left = np.zeros((len(equations), n))\n    right = np.zeros(len(equations))\n    group_names = set(groups.flatten())\n    for i, eq in enumerate(equations):\n        try:\n            left[i], right[i] = _string_to_equation(groups, eq, sum_to_one)\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            warnings.warn(str(e))\n\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n\n    if groups.ndim != 2:\n        raise EquationToMatrixError(\n            f\"'{names[0]}' should be a 2D array, but it is {groups.ndim}D\"\n        )\n\n    if equations.ndim != 1:\n        raise EquationToMatrixError(\n            f\"'{names[1]}' should be a 1D array, but it is {equations.ndim}D\"\n        )\n\n    n_groups = groups.shape[0]\n    n_assets = groups.shape[1]\n    n_equations = equations.shape[0]\n\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n\n    for i, equation in enumerate(equations):\n        try:\n            left[i], right[i] = _string_to_equation(\n                groups=groups, string=equation, sum_to_one=sum_to_one\n            )\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            warnings.warn(e.args[0])\n\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    if len(groups.shape) != 2:\n        raise EquationToMatrixError(\n            f\"The '{names[0]}' array should be a 2D array of shape (n_groups, n_assets)\"\n        )\n    if len(equations.shape) != 1:\n        raise EquationToMatrixError(\n            f\"The '{names[1]}' array should be a 1D array of shape (n_equations,)\"\n        )\n    if groups.shape[0] != equations.shape[0]:\n        raise EquationToMatrixError(\n            f\"The '{names[0]}' and '{names[1]}' arrays should have the same number of rows\"\n        )\n\n    n = groups.shape[1]\n    left = np.zeros((len(equations), n))\n    right = np.zeros(len(equations))\n    for i, eq in enumerate(equations):\n        try:\n            l, r = _string_to_equation(groups=groups, string=eq, sum_to_one=sum_to_one)\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            warnings.warn(e.args[0])\n            continue\n        left[i] = l\n        right[i] = r\n\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    if len(groups.shape) != 2:\n        raise EquationToMatrixError(\n            f\"The '{names[0]}' array must be a 2D array of shape (n_groups, n_assets)\"\n        )\n    if len(equations.shape) != 1:\n        raise EquationToMatrixError(\n            f\"The '{names[1]}' array must be a 1D array of shape (n_equations,)\"\n        )\n\n    n = groups.shape[1]\n    m = len(equations)\n    left = np.zeros((m, n))\n    right = np.zeros(m)\n    group_names = set(groups.flatten())\n\n    for i, equation in enumerate(equations):\n        try:\n            l, r = _string_to_equation(groups, equation, sum_to_one)\n            left[i] = l\n            right[i] = r\n        except (GroupNotFoundError, EquationToMatrixError) as e:\n            if raise_if_group_missing:\n                raise e\n            else:\n                warnings.warn(str(e))\n\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n    n_groups, n_assets = groups.shape\n    n_equations = equations.shape[0]\n\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n    for i, eq in enumerate(equations):\n        try:\n            l, r = _string_to_equation(groups=groups, string=eq, sum_to_one=sum_to_one)\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            warnings.warn(e.args[0])\n            continue\n        left[i] = l\n        right[i] = r\n\n    if not np.any(left):\n        raise EquationToMatrixError(\n            f\"None of the groups in the {names[0]} array were found in the {names[1]}.\"\n        )\n\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    if groups.ndim != 2:\n        raise EquationToMatrixError(\n            f\"The '{names[0]}' array should be a 2D array. Got {groups.ndim}D array\"\n        )\n    if equations.ndim != 1:\n        raise EquationToMatrixError(\n            f\"The '{names[1]}' array should be a 1D array. Got {equations.ndim}D array\"\n        )\n\n    n_assets = groups.shape[1]\n    n_equations = len(equations)\n\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n    for i, equation in enumerate(equations):\n        try:\n            left[i], right[i] = _string_to_equation(groups, equation, sum_to_one)\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            else:\n                warnings.warn(e.args[0])\n\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    if groups is None or equations is None:\n        return None\n\n    groups = np.array(groups, dtype=object)\n    equations = np.array(equations, dtype=object)\n\n    if groups.ndim != 2:\n        raise EquationToMatrixError(\n            f\"{names[0]} must be a 2D array of shape (n_groups, n_assets)\"\n        )\n\n    if equations.ndim != 1:\n        raise EquationToMatrixError(f\"{names[1]} must be a 1D array of shape (n_equations,)\")\n\n    n_groups, n_assets = groups.shape\n    n_equations = equations.shape[0]\n\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n\n    group_names = set(groups.flatten())\n\n    for i, equation in enumerate(equations):\n        try:\n            left[i], right[i] = _string_to_equation(\n                groups=groups,\n                string=equation,\n                sum_to_one=sum_to_one,\n            )\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            warnings.warn(str(e))\n\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    if len(groups.shape) != 2:\n        raise EquationToMatrixError(\n            f\"The '{names[0]}' array should be 2D. It is {groups.shape}\"\n        )\n    if len(equations.shape) != 1:\n        raise EquationToMatrixError(\n            f\"The '{names[1]}' array should be 1D. It is {equations.shape}\"\n        )\n    if np.any(groups != groups.astype(bool)):\n        raise EquationToMatrixError(\n            f\"The '{names[0]}' array should only contain 1 or 0. It contains {np.unique(groups)}\"\n        )\n\n    n_groups, n_assets = groups.shape\n    n_equations = len(equations)\n\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n\n    for i, equation in enumerate(equations):\n        try:\n            left[i], right[i] = _string_to_equation(\n                groups=groups, string=equation, sum_to_one=sum_to_one\n            )\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            warnings.warn(str(e))\n\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    if groups.ndim != 2:\n        raise EquationToMatrixError(\n            f\"groups should be a 2D array, but got a {groups.ndim}D array\"\n        )\n    if equations.ndim != 1:\n        raise EquationToMatrixError(\n            f\"equations should be a 1D array, but got a {equations.ndim}D array\"\n        )\n    if not isinstance(sum_to_one, bool):\n        raise EquationToMatrixError(\n            f\"sum_to_one should be a bool, but got a {type(sum_to_one)}\"\n        )\n    if not isinstance(raise_if_group_missing, bool):\n        raise EquationToMatrixError(\n            f\"raise_if_group_missing should be a bool, but got a {type(raise_if_group_missing)}\"\n        )\n    if not isinstance(names, tuple):\n        raise EquationToMatrixError(\n            f\"names should be a tuple, but got a {type(names)}\"\n        )\n    if len(names) != 2:\n        raise EquationToMatrixError(\n            f\"names should be a tuple of length 2, but got a {len(names)}\"\n        )\n    if not isinstance(names[0], str):\n        raise EquationToMatrixError(\n            f\"names[0] should be a string, but got a {type(names[0])}\"\n        )\n    if not isinstance(names[1], str):\n        raise EquationToMatrixError(\n            f\"names[1] should be a string, but got a {type(names[1])}\"\n        )\n\n    groups = np.array(groups)\n    equations = np.array(equations)\n\n    n_groups = groups.shape[0]\n    n_assets = groups.shape[1]\n    n_equations = equations.shape[0]\n\n    if n_groups == 0:\n        warnings.warn(\n            f\"{names[0]} is empty, so no equation can be created. Returning None\"\n        )\n        return None, None"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    if groups is None or equations is None:\n        return None\n    if not isinstance(groups, np.ndarray):\n        groups = np.array(groups)\n    if not isinstance(equations, np.ndarray):\n        equations = np.array(equations)\n    if groups.ndim != 2:\n        raise EquationToMatrixError(\n            f\"The '{names[0]}' parameter should be a 2D array, not a {groups.ndim}D array\"\n        )\n    if equations.ndim != 1:\n        raise EquationToMatrixError(\n            f\"The '{names[1]}' parameter should be a 1D array, not a {equations.ndim}D array\"\n        )\n    if groups.shape[1] != equations.shape[0]:\n        raise EquationToMatrixError(\n            f\"The number of assets in the '{names[0]}' parameter should be equal to the number of assets in the '{names[1]}' parameter\"\n        )\n    if groups.shape[0] == 0:\n        warnings.warn(\n            f\"The '{names[0]}' parameter is empty. The '{names[1]}' parameter will be ignored.\"\n        )\n        return None\n    if equations.shape[0] == 0:\n        warnings.warn(\n            f\"The '{names[1]}' parameter is empty. The '{names[0]}' parameter will be ignored.\"\n        )\n        return None\n\n    n_groups = groups.shape[0]\n    n_assets = groups.shape[1]\n    n_equations = equations.shape[0]\n\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n\n    for i, equation in enumerate(equations):\n        try:\n            l, r = _string_to_equation(groups=groups, string=equation, sum_to_one=sum_to_one)\n        except (GroupNotFoundError, EquationToMatrixError) as e:\n            if raise_if_group_missing:\n                raise e from None\n            warnings"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    if groups is None or equations is None:\n        raise EquationToMatrixError(\"groups and equations cannot be None\")\n    groups = np.array(groups, dtype=object)\n    equations = np.array(equations, dtype=object)\n    n = groups.shape[1]\n    left = np.zeros((len(equations), n))\n    right = np.zeros(len(equations))\n    for i, e in enumerate(equations):\n        try:\n            l, r = _string_to_equation(groups=groups, string=e, sum_to_one=sum_to_one)\n            left[i] = l\n            right[i] = r\n        except GroupNotFoundError as err:\n            if raise_if_group_missing:\n                raise err\n            warnings.warn(\n                f\"{names[1]} {i} is ignored: {err}\",\n                UserWarning,\n            )\n    if np.all(left == 0):\n        return None, None\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Check the input data types\n    if not isinstance(groups, np.ndarray):\n        raise TypeError(f\"'{names[0]}' must be a numpy array\")\n    if not isinstance(equations, np.ndarray):\n        raise TypeError(f\"'{names[1]}' must be a numpy array\")\n\n    # Check the input shapes\n    if groups.ndim != 2:\n        raise ValueError(f\"'{names[0]}' must be a 2D array\")\n    if equations.ndim != 1:\n        raise ValueError(f\"'{names[1]}' must be a 1D array\")\n\n    # Check the input values\n    if not np.issubdtype(groups.dtype, np.number):\n        raise ValueError(f\"'{names[0]}' must be a numeric array\")\n    if not np.issubdtype(equations.dtype, np.str_):\n        raise ValueError(f\"'{names[1]}' must be a string array\")\n\n    # Check the input values\n    if groups.shape[1] != equations.shape[0]:\n        raise ValueError(\n            f\"The number of assets in '{names[0]}' must match the number of \"\n            f\"equations in '{names[1]}'\"\n        )\n\n    # Convert the equations into the left and right matrices\n    n_equations = equations.shape[0]\n    n_assets = groups.shape[1]\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n    for i, equation in enumerate(equations):\n        try:\n            l, r = _string_to_equation(groups, equation, sum_to_one)\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            warnings.warn(str(e), UserWarning)\n            continue\n        left[i, :] = l\n        right[i] = r\n\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    if len(groups) == 0:\n        warnings.warn(\"No groups provided\")\n        return None, None\n    if len(equations) == 0:\n        warnings.warn(\"No equations provided\")\n        return None, None\n    if groups.ndim != 2:\n        raise EquationToMatrixError(\n            f\"The '{names[0]}' argument must be a 2D array, but it has {groups.ndim} \"\n            \"dimensions\"\n        )\n    if equations.ndim != 1:\n        raise EquationToMatrixError(\n            f\"The '{names[1]}' argument must be a 1D array, but it has {equations.ndim} \"\n            \"dimensions\"\n        )\n    if groups.shape[1] != equations.shape[0]:\n        raise EquationToMatrixError(\n            f\"The '{names[0]}' argument must have the same number of assets as the \"\n            f\"'{names[1]}' argument\"\n        )\n\n    left = []\n    right = []\n    for equation in equations:\n        try:\n            l, r = _string_to_equation(groups, equation, sum_to_one)\n        except (GroupNotFoundError, EquationToMatrixError) as e:\n            if raise_if_group_missing:\n                raise e\n            warnings.warn(str(e))\n            continue\n        left.append(l)\n        right.append(r)\n\n    if len(left) == 0:\n        warnings.warn(\"No equations were valid\")\n        return None, None\n\n    left = np.array(left)\n    right = np.array(right)\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    if len(groups.shape) != 2:\n        raise EquationToMatrixError(\n            f\"groups should be a 2D array. It was a {groups.shape} array\"\n        )\n    if len(equations.shape) != 1:\n        raise EquationToMatrixError(\n            f\"equations should be a 1D array. It was a {equations.shape} array\"\n        )\n\n    groups = np.array(groups)\n    equations = np.array(equations)\n    n = groups.shape[1]\n    m = equations.shape[0]\n\n    left = np.zeros((m, n))\n    right = np.zeros(m)\n\n    for i, string in enumerate(equations):\n        try:\n            left[i], right[i] = _string_to_equation(\n                groups=groups,\n                string=string,\n                sum_to_one=sum_to_one,\n            )\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            warnings.warn(\n                f\"{e.args[0]}. The equation '{string}' will be ignored.\"\n                f\" Set 'raise_if_group_missing' to True to raise the error.\"\n            )\n\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n    if groups.ndim != 2:\n        raise EquationToMatrixError(\n            f\"groups should be a 2D array, not {groups.ndim}D array\"\n        )\n    if equations.ndim != 1:\n        raise EquationToMatrixError(\n            f\"equations should be a 1D array, not {equations.ndim}D array\"\n        )\n    n_groups = groups.shape[0]\n    n_assets = groups.shape[1]\n    n_equations = equations.shape[0]\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n    group_names = set(groups.flatten())\n\n    for i, equation in enumerate(equations):\n        left_i, right_i = _string_to_equation(\n            groups=groups, string=equation, sum_to_one=sum_to_one\n        )\n        if (left_i == 0).all() and right_i == 0:\n            if raise_if_group_missing:\n                raise EquationToMatrixError(\n                    f\"Equation '{equation}' is not valid. {names[1]} should contain\"\n                    f\" at least one group from {names[0]}\"\n                )\n            else:\n                warnings.warn(\n                    f\"Equation '{equation}' is not valid. {names[1]} should contain\"\n                    f\" at least one group from {names[0]}\"\n                )\n            continue\n        left[i, :] = left_i\n        right[i] = right_i\n\n    if (left == 0).all() and (right == 0).all():\n        if raise_if_group_missing:\n            raise EquationToMatrixError(\n                f\"None of the equations in {names[1]} are valid. {names[1]} should\"\n                f\" contain at least one group from {names[0]}\"\n            )\n        else:\n            warnings.warn(\n                f\"None of the equations in"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    if groups.ndim != 2:\n        raise ValueError(\n            f\"groups should be a 2D array, got {groups.ndim}D instead. \"\n            f\"If you want to convert a 1D array into a 2D array, \"\n            f\"you can use the function numpy.reshape(-1, 1)\"\n        )\n    if equations.ndim != 1:\n        raise ValueError(\n            f\"equations should be a 1D array, got {equations.ndim}D instead. \"\n            f\"If you want to convert a 2D array into a 1D array, \"\n            f\"you can use the function numpy.ravel()\"\n        )\n    if groups.shape[0] != equations.shape[0]:\n        raise ValueError(\n            f\"groups and equations should have the same number of rows. \"\n            f\"Got {groups.shape[0]} and {equations.shape[0]} instead.\"\n        )\n\n    n_groups = groups.shape[0]\n    n_assets = groups.shape[1]\n    n_equations = equations.shape[0]\n\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n    for i, equation in enumerate(equations):\n        try:\n            left[i], right[i] = _string_to_equation(\n                groups=groups, string=equation, sum_to_one=sum_to_one\n            )\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            warnings.warn(\n                f\"{e}. The equation '{equation}' will be ignored. \"\n                f\"If you want to keep the equation, please add the missing groups \"\n                f\"to the {names[0]} array\"\n            )\n\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    if groups.ndim != 2:\n        raise EquationToMatrixError(\n            f\"groups should be a 2D array, got {groups.shape}\"\n        )\n    n_groups, n_assets = groups.shape\n    if n_assets < 1:\n        raise EquationToMatrixError(\"groups should have at least one column\")\n    if sum_to_one:\n        if not np.allclose(np.sum(groups, axis=1), 1):\n            warnings.warn(\n                f\"{names[0]} are not summing to one. The 'sum_to_one' argument is set to\"\n                f\" True but the sum of each row of {names[0]} is not equal to one.\"\n            )\n    equations = np.asarray(equations)\n    if equations.ndim != 1:\n        raise EquationToMatrixError(\n            f\"equations should be a 1D array, got {equations.shape}\"\n        )\n    n_equations = equations.shape[0]\n    if n_equations < 1:\n        raise EquationToMatrixError(\"equations should have at least one element\")\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n    for i, equation in enumerate(equations):\n        try:\n            left[i], right[i] = _string_to_equation(\n                groups=groups, string=equation, sum_to_one=sum_to_one\n            )\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            warnings.warn(str(e))\n    return left, right"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n\n    cls_name, cls_def = _gen_instance_module(fields)\n    with tempfile.NamedTemporaryFile(\"w\", suffix=\".py\", delete=False) as f:\n        f.write(cls_def)\n        f.flush()\n        path = f.name\n\n    # clear jit cache\n    _clear_jit_cache()\n\n    # import the module\n    module = _import(path)\n    newInstances = getattr(module, cls_name)\n    _add_instances_conversion_methods(newInstances)\n\n    # patch the class\n    with mock.patch(\"detectron2.structures.Instances\", newInstances):\n        yield newInstances\n\n    # clean up\n    os.remove(path)\n    del sys.modules[module.__name__]"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n    new_cls_name, cls_def = _gen_instance_module(fields)\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n        f.write(cls_def)\n        f.flush()\n        path = f.name\n\n    # clear jit cache\n    _clear_jit_cache()\n\n    # import new class\n    newInstances = _import(path)\n    _add_instances_conversion_methods(newInstances)\n\n    # patch Instances\n    with mock.patch(\"detectron2.structures.Instances\", newInstances):\n        yield newInstances\n\n    # clean up\n    os.remove(path)\n    _clear_jit_cache()"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n    with ExitStack() as stack:\n        # clear jit cache\n        stack.enter_context(_clear_jit_cache())\n\n        # patch Instances\n        cls_name, cls_def = _gen_instance_module(fields)\n        with tempfile.NamedTemporaryFile(suffix=\".py\") as f:\n            f.write(cls_def.encode(\"utf-8\"))\n            f.flush()\n            newInstances = _import(f.name)\n            _add_instances_conversion_methods(newInstances)\n\n        # patch Instances in detectron2\n        with mock.patch(\"detectron2.structures.Instances\", newInstances):\n            yield newInstances"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    with ExitStack() as stack:\n        # create a new module for the class\n        cls_name, cls_def = _gen_instance_module(fields)\n        fd, path = tempfile.mkstemp(suffix=\".py\")\n        os.write(fd, cls_def.encode(\"utf-8\"))\n        os.close(fd)\n        # import the module\n        newInstances = _import(path)\n        # add conversion methods\n        _add_instances_conversion_methods(newInstances)\n        # replace the original Instances class\n        stack.enter_context(mock.patch(\"detectron2.structures.Instances\", newInstances))\n        # clear jit cache\n        stack.enter_context(_clear_jit_cache())\n        # yield the new class\n        yield newInstances\n        # clean up\n        os.remove(path)"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    # TODO: support nested fields\n    # TODO: support list fields\n    # TODO: support dict fields\n    # TODO: support nested dict fields\n    # TODO: support nested list fields\n    # TODO: support nested list of dict fields\n\n    global _counter\n    _counter += 1\n\n    cls_name, cls_def = _gen_instance_module(fields)\n\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n        f.write(cls_def)\n        f.flush()\n        path = f.name\n\n    with ExitStack() as stack:\n        stack.enter_context(patch_builtin_len())\n        stack.enter_context(mock.patch(\"detectron2.structures.Instances\", cls_name))\n        stack.enter_context(mock.patch(\"detectron2.structances.Instances\", cls_name))\n        stack.enter_context(mock.patch(\"detectron2.structures.instances.Instances\", cls_name))\n        stack.enter_context(mock.patch(\"detectron2.structures.instances.instances.Instances\", cls_name))\n        stack.enter_context(mock.patch(\"detectron2.structures.instances.instances.instances.Instances\", cls_name))\n        stack.enter_context(mock.patch(\"detectron2.structures.instances.instances.instances.instances.Instances\", cls_name))\n        stack.enter_context(mock.patch(\"detectron2.structures.instances.instances.instances.instances.instances.Instances\", cls_name))\n        stack.enter_context(mock.patch(\"detectron2.structures.instances.instances.instances.instances.instances.instances.Instances\", cls_name))\n        stack.enter_context(mock.patch(\"detectron2.structures.instances.instances.instances.instances.instances.instances.instances.Instances\", cls_name"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    with ExitStack() as stack:\n        # clear jit cache\n        stack.enter_context(_clear_jit_cache())\n\n        # create a new module with the new class\n        cls_name, cls_def = _gen_instance_module(fields)\n        tmp_file = tempfile.NamedTemporaryFile(suffix=\".py\", delete=False)\n        tmp_file.write(cls_def.encode(\"utf-8\"))\n        tmp_file.close()\n        stack.enter_context(mock.patch.dict(\"sys.modules\", {\"\": None}))\n        stack.enter_context(mock.patch.dict(sys.modules, {\"\": None}))\n        stack.enter_context(mock.patch.dict(sys.modules, {cls_name: None}))\n        stack.enter_context(mock.patch.dict(sys.modules, {\"__main__\": None}))\n        stack.enter_context(mock.patch.dict(sys.modules, {cls_name: None}))\n        stack.enter_context(mock.patch.dict(sys.modules, {cls_name: None}))\n        stack.enter_context(mock.patch.dict(sys.modules, {cls_name: None}))\n        stack.enter_context(mock.patch.dict(sys.modules, {cls_name: None}))\n        stack.enter_context(mock.patch.dict(sys.modules, {cls_name: None}))\n        stack.enter_context(mock.patch.dict(sys.modules, {cls_name: None}))\n        stack.enter_context(mock.patch.dict(sys.modules, {cls_name: None}))\n        stack.enter_context(mock.patch.dict(sys.modules, {cls_name: None}))\n        stack.enter_context(mock.patch.dict(sys.modules, {cls_name: None}))\n        stack.enter_context(mock.patch.dict(sys.modules, {cls_name: None}))\n        stack.enter_context(mock.patch.dict(sys.modules, {cls_name: None}))\n        stack.enter_context(mock.patch.dict(sys."}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    # clear jit cache\n    _clear_jit_cache()\n\n    # patch instances\n    cls_name, cls_def = _gen_instance_module(fields)\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\") as f:\n        f.write(cls_def)\n        f.flush()\n        mod_name = os.path.splitext(os.path.basename(f.name))[0]\n        _import(f.name)\n        newInstances = getattr(sys.modules[__name__], cls_name)\n        _add_instances_conversion_methods(newInstances)\n        yield newInstances\n\n    # cleanup\n    del sys.modules[mod_name]\n    del sys.modules[__name__]\n    _clear_jit_cache()"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n\n    with ExitStack() as stack:\n        # patch Instances class\n        cls_name, cls_def = _gen_instance_module(fields)\n        tmp_file = tempfile.NamedTemporaryFile(suffix=\".py\", delete=False)\n        tmp_file.write(cls_def.encode(\"utf-8\"))\n        tmp_file.close()\n        cls = _import(tmp_file.name)\n        stack.enter_context(mock.patch(\"detectron2.structures.Instances\", cls))\n\n        # patch Instances.from_instances()\n        _add_instances_conversion_methods(cls)\n\n        # patch torch._C._jit_script_instance_method()\n        stack.enter_context(\n            mock.patch(\n                \"torch._C._jit_script_instance_method\",\n                wraps=torch._C._jit_script_instance_method,\n            )\n        )\n\n        # patch torch.jit.script()\n        stack.enter_context(\n            mock.patch(\n                \"torch.jit.script\",\n                wraps=torch.jit.script,\n            )\n        )\n\n        # patch torch.jit.annotate()\n        stack.enter_context(\n            mock.patch(\n                \"torch.jit.annotate\",\n                wraps=torch.jit.annotate,\n            )\n        )\n\n        # patch torch.jit.isinstance()\n        stack.enter_context(\n            mock.patch(\n                \"torch.jit.isinstance\",\n                wraps=torch.jit.isinstance,\n            )\n        )\n\n        # patch torch.jit.cast()\n        stack.enter_context(\n            mock.patch(\n                \"torch.jit.cast\",\n                wraps=torch.jit.cast,\n            )\n        )\n\n        # patch torch.jit.is_scripting()\n        stack."}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n\n    with ExitStack() as stack:\n        # clean up after ourselves\n        stack.enter_context(patch_builtin_len())\n\n        # patch torch.jit.isinstance\n        stack.enter_context(mock.patch(\"torch.jit.isinstance\", wrap_isinstance))\n\n        # patch torch.jit.is_scripting()\n        stack.enter_context(mock.patch(\"torch.jit.is_scripting\", lambda: True))\n\n        # patch torch.jit.is_tracing()\n        stack.enter_context(mock.patch(\"torch.jit.is_tracing\", lambda: False))\n\n        # patch torch.jit.is_script()\n        stack.enter_context(mock.patch(\"torch.jit.is_script\", lambda: False))\n\n        # patch torch.jit.is_compiling()\n        stack.enter_context(mock.patch(\"torch.jit.is_compiling\", lambda: False))\n\n        # patch torch.jit.is_profiling()\n        stack.enter_context(mock.patch(\"torch.jit.is_profiling\", lambda: False))\n\n        # patch torch.jit.is_differentiable()\n        stack.enter_context(mock.patch(\"torch.jit.is_differentiable\", lambda: False))\n\n        # patch torch.jit.is_gradcheck()\n        stack.enter_context(mock.patch(\"torch.jit.is_gradcheck\", lambda: False))\n\n        # patch torch.jit.is_gradmode()\n        stack.enter_context(mock.patch(\"torch.jit.is_gradmode\", lambda: False))\n\n        # patch torch.jit.is_jitmode()\n        stack.enter_context(mock.patch(\"torch.jit.is_jitmode\", lambda: True))\n\n        # patch torch.jit.is_scripting()\n        stack.enter_context("}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    with ExitStack() as stack:\n        # create a temporary file to hold the new class definition\n        temp_file = tempfile.NamedTemporaryFile(suffix=\".py\", delete=False)\n        temp_file.write(_gen_instance_module(fields).encode(\"utf-8\"))\n        temp_file.close()\n\n        # import the new class\n        module = _import(temp_file.name)\n        newInstances = module.ScriptedInstances\n        _add_instances_conversion_methods(newInstances)\n\n        # replace Instances with the new class\n        stack.enter_context(mock.patch(\"detectron2.structures.Instances\", newInstances))\n\n        # clear JIT cache\n        _clear_jit_cache()\n\n        # yield the new class\n        yield newInstances\n\n        # clean up\n        os.unlink(temp_file.name)"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    # TODO: add support for nested fields\n    assert all(isinstance(x, tuple) for x in fields), \"fields must be a list of tuples\"\n    assert all(len(x) == 2 for x in fields), \"fields must be a list of tuples\"\n    assert all(isinstance(x[0], str) for x in fields), \"field name must be str\"\n    assert all(hasattr(Instances, x[0]) for x in fields), \"field name must be a valid Instances field\"\n    assert all(isinstance(x[1], type) for x in fields), \"field type must be a type\"\n\n    fields = {x[0]: x[1] for x in fields}\n    cls_name, cls_def = _gen_instance_module(fields)\n\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with open(os.path.join(tmpdir, \"scripted_instances.py\"), \"w\") as f:\n            f.write(cls_def)\n        with mock.patch.dict(\"sys.modules\", {cls_name: _import(os.path.join(tmpdir, \"scripted_instances.py\"))}):\n            newInstances = sys.modules[cls_name]\n            _add_instances_conversion_methods(newInstances)\n            yield newInstances\n\n    _clear_jit_cache()"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    with ExitStack() as stack:\n        _clear_jit_cache()\n        cls_name, cls_def = _gen_instance_module(fields)\n        with tempfile.NamedTemporaryFile(suffix=\".py\", delete=False) as f:\n            f.write(cls_def.encode(\"utf-8\"))\n            f.flush()\n            mod_name = \"patch_instances\" + str(_counter)\n            stack.enter_context(mock.patch.dict(\"sys.modules\", {mod_name: None}))\n            stack.enter_context(mock.patch.dict(\"sys.modules\", {cls_name: None}))\n            stack.enter_context(mock.patch.dict(\"sys.modules\", {\"__main__\": None}))\n            stack.enter_context(mock.patch.dict(\"sys.modules\", {\"__file__\": f.name}))\n            stack.enter_context(mock.patch.dict(\"sys.modules\", {\"__name__\": mod_name}))\n            stack.enter_context(mock.patch.dict(\"sys.modules\", {\"__package__\": mod_name}))\n            _import(f.name)\n            newInstances = sys.modules[mod_name].ScriptedInstances\n            _add_instances_conversion_methods(newInstances)\n            yield newInstances\n        _clear_jit_cache()\n        del sys.modules[mod_name]\n        del sys.modules[cls_name]\n        os.remove(f.name)\n        _counter += 1"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n\n    fields = {k: v for k, v in fields}\n    cls_name, cls_def = _gen_instance_class(fields)\n    module_name, module_def = _gen_instance_module(fields)\n\n    with tempfile.TemporaryDirectory() as tmpdir:\n        path = os.path.join(tmpdir, f\"{module_name}.py\")\n        with open(path, \"w\") as f:\n            f.write(module_def)\n\n        with ExitStack() as stack:\n            stack.enter_context(mock.patch.dict(sys.modules, {cls_name: None}))\n            stack.enter_context(mock.patch.dict(sys.modules, {module_name: None}))\n            stack.enter_context(mock.patch.dict(sys.modules, {\"__main__\": None}))\n\n            _import(path)\n            _clear_jit_cache()\n            _add_instances_conversion_methods(getattr(sys.modules[module_name], cls_name))\n\n            yield getattr(sys.modules[module_name], cls_name)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    new_cls_name, cls_def = _gen_instance_module(fields)\n    with tempfile.NamedTemporaryFile(suffix=\".py\", delete=False) as f:\n        f.write(cls_def.encode(\"utf-8\"))\n        f.flush()\n        path = f.name\n\n    # clean up the file after the context\n    @contextmanager\n    def cleanup():\n        try:\n            yield\n        finally:\n            os.remove(path)\n\n    with ExitStack() as stack:\n        stack.enter_context(cleanup())\n        stack.enter_context(patch_builtin_len())\n        stack.enter_context(mock.patch(\"detectron2.structures.Instances\", new_cls_name))\n        stack.enter_context(mock.patch(\"detectron2.structures.Boxes\", new_cls_name))\n        stack.enter_context(mock.patch(\"detectron2.structures.PolygonMasks\", new_cls_name))\n        stack.enter_context(mock.patch(\"detectron2.structures.BitMasks\", new_cls_name))\n        stack.enter_context(mock.patch(\"detectron2.structures.Keypoints\", new_cls_name))\n        stack.enter_context(mock.patch(\"detectron2.structures.RLEs\", new_cls_name))\n        stack.enter_context(mock.patch(\"detectron2.structures.RLEs\", new_cls_name))\n        stack.enter_context(mock.patch(\"detectron2.structures.PolygonMasks\", new_cls_name))\n        stack.enter_context(mock.patch(\"detectron2.structures.BitMasks\", new_cls_name))\n        stack.enter_context(mock.patch(\"detectron2.structures.Keypoints\", new_cls_name))\n        stack.enter_context(mock.patch(\"detectron2.structures.RLEs\", new_cls_name))\n        stack.enter_context(mock.patch(\"detectron"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n\n    with ExitStack() as stack:\n        # patch torch.jit.script\n        # TODO: remove this patch once torch.jit.script supports __prepare_scriptable__\n        # https://github.com/pytorch/pytorch/issues/38964\n        ctx = stack.enter_context(mock.patch(\"torch.jit.script\", wrap_script))\n        ctx.script_module = None\n\n        # patch torch.jit.script_method\n        ctx = stack.enter_context(mock.patch(\"torch.jit.script_method\", wrap_script_method))\n        ctx.script_module = None\n\n        # patch torch.jit.script_module\n        ctx = stack.enter_context(mock.patch(\"torch.jit.script_module\", wrap_script_module))\n        ctx.script_module = None\n\n        # patch torch.jit.is_scripting\n        ctx = stack.enter_context(mock.patch(\"torch.jit.is_scripting\", lambda: True))\n\n        # patch torch.jit.is_tracing\n        ctx = stack.enter_context(mock.patch(\"torch.jit.is_tracing\", lambda: False))\n\n        # patch torch.jit.is_script\n        ctx = stack.enter_context(mock.patch(\"torch.jit.is_script\", lambda: True))\n\n        # patch torch.jit.is_python\n        ctx = stack.enter_context(mock.patch(\"torch.jit.is_python\", lambda: False))\n\n        # patch torch.jit.is_compiling\n        ctx = stack.enter_context(mock.patch(\"torch.jit.is_compiling\", lambda: False))\n\n        # patch torch.jit.is_native\n        ctx = stack.enter_context(mock.patch(\"torch.jit.is_native\", lambda: True))\n\n        # patch torch.jit.is_scripting"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n\n    # clear jit cache to avoid caching previous instances\n    _clear_jit_cache()\n\n    cls_name, cls_def = _gen_instance_module(fields)\n\n    with tempfile.NamedTemporaryFile(\n        suffix=\".py\", prefix=\"instances\", dir=tempfile.gettempdir(), delete=False\n    ) as f:\n        f.write(cls_def.encode(\"utf-8\"))\n        f.flush()\n\n        # import the module\n        mod = _import(f.name)\n\n        # get the class\n        newInstances = getattr(mod, cls_name)\n\n        # add conversion methods\n        _add_instances_conversion_methods(newInstances)\n\n        # update the environment\n        sys.modules[\"detectron2.structures.instances\"] = mod\n\n        # yield the class\n        yield newInstances\n\n    # clean up\n    del sys.modules[\"detectron2.structures.instances\"]\n    os.remove(f.name)\n    _clear_jit_cache()"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n\n    cls_name, cls_def = _gen_instance_module(fields)\n    with tempfile.NamedTemporaryFile(suffix=\".py\", delete=False) as f:\n        f.write(cls_def.encode(\"utf-8\"))\n        f.flush()\n        path = f.name\n\n    try:\n        # need to clear jit cache to avoid reusing the old class\n        _clear_jit_cache()\n        # patch sys.modules\n        with ExitStack() as stack:\n            stack.enter_context(mock.patch.dict(sys.modules, {}, clear=True))\n            stack.enter_context(mock.patch.dict(sys.modules, {\"detectron2.structures\": None}))\n            # patch import\n            stack.enter_context(mock.patch(\"sys.modules\", sys.modules))\n            stack.enter_context(mock.patch(\"sys.path\", sys.path))\n            stack.enter_context(mock.patch(\"sys.path.insert\", sys.path.insert))\n            stack.enter_context(mock.patch(\"sys.path.remove\", sys.path.remove))\n            stack.enter_context(mock.patch(\"sys.path.append\", sys.path.append))\n            stack.enter_context(mock.patch(\"sys.path.extend\", sys.path.extend))\n            stack.enter_context(mock.patch(\"sys.path.insert\", sys.path.insert))\n            stack.enter_context(mock.patch(\"sys.path.pop\", sys.path.pop))\n            stack.enter_context(mock.patch(\"sys.path.remove\", sys.path.remove))\n            stack.enter_context(mock.patch(\"sys.path.append\", sys.path.append))\n            stack.enter_context(mock.patch(\"sys.path.extend\", sys.path.extend))\n            stack.enter_context(mock.patch(\"sys.path.insert\", sys.path.insert))\n            stack.enter_context(mock.patch(\"sys.path.pop\", sys.path.pop))\n            stack.enter"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    # The following code is adapted from detectron2.utils.script.py\n    # with some modifications to support scripting.\n\n    global _counter\n    _counter += 1\n\n    cls_name, cls_def = _gen_instance_module(fields)\n\n    # write the new class to a temporary file\n    with tempfile.NamedTemporaryFile(suffix=\".py\", mode=\"w\", delete=False) as f:\n        f.write(cls_def)\n        path = f.name\n    # import the new class\n    newInstances = _import(path)\n    # clean up the temporary file\n    os.remove(path)\n\n    # add from_instances method to the scripted Instances class\n    _add_instances_conversion_methods(newInstances)\n\n    # replace Instances with the new class\n    with ExitStack() as stack:\n        stack.enter_context(mock.patch(\"detectron2.structures.Instances\", new=newInstances))\n        yield newInstances\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n\n    cls_name, cls_def = _gen_instance_module(fields)\n    with tempfile.NamedTemporaryFile(\"w\", suffix=\".py\", delete=False) as f:\n        f.write(cls_def)\n        path = f.name\n\n    # We have to use a stack to clean up the environment.\n    # This is because we need to re-import detectron2\n    # after patching the Instances class.\n    with ExitStack() as stack:\n        # patch Instances class\n        stack.enter_context(patch_builtin_len())\n        stack.enter_context(patch_instances_class(cls_name))\n\n        # patch detectron2\n        stack.enter_context(patch_detectron2(path))\n\n        # patch torch.jit\n        stack.enter_context(patch_jit())\n\n        # patch detectron2.modeling.meta_arch\n        stack.enter_context(patch_meta_arch())\n\n        # patch detectron2.modeling.proposal_generator\n        stack.enter_context(patch_proposal_generator())\n\n        # patch detectron2.modeling.roi_heads\n        stack.enter_context(patch_roi_heads())\n\n        # patch detectron2.modeling.roi_heads.fast_rcnn\n        stack.enter_context(patch_fast_rcnn())\n\n        # patch detectron2.modeling.roi_heads.mask_head\n        stack.enter_context(patch_mask_head())\n\n        # patch detectron2.modeling.roi_heads.keypoint_head\n        stack.enter_context(patch_keypoint_head())\n\n        # patch detectron2.modeling.postprocessing\n        stack.enter_context(patch_postprocessing())\n\n        # patch detectron2.modeling.sem_seg\n        stack.enter_context(patch_sem_seg())\n\n        # patch detectron2.modeling.meta_arch.semantic_fpn"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    cls_name, cls_def = _gen_instance_module(fields)\n\n    # We use a temporary directory for the scripted module,\n    # because we do not want to pollute the user's global namespace.\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with open(os.path.join(tmpdir, \"scripted_instances.py\"), \"w\") as f:\n            f.write(cls_def)\n\n        # We need to clear the jit cache to avoid caching the original class.\n        # This is because the original class is not scriptable.\n        _clear_jit_cache()\n\n        # We need to import the scripted module from the temporary directory.\n        # This is because the original class is not scriptable.\n        scripted_instances = _import(os.path.join(tmpdir, \"scripted_instances\"))\n\n        # We need to set the environment variable to the temporary directory.\n        # This is because the original class is not scriptable.\n        with mock.patch.dict(os.environ, {\"PYTHONPATH\": tmpdir}):\n            # We need to clear the jit cache to avoid caching the original class.\n            # This is because the original class is not scriptable.\n            _clear_jit_cache()\n\n            # We need to import the scripted module from the temporary directory.\n            # This is because the original class is not scriptable.\n            scripted_instances = _import(os.path.join(tmpdir, \"scripted_instances\"))\n\n            # We need to set the environment variable to the temporary directory.\n            # This is because the original class is not scriptable.\n            with mock.patch.dict(os.environ, {\"PYTHONPATH\": tmpdir}):\n                # We need to clear the jit cache to avoid caching the original class.\n                # This is because the original class is not scriptable.\n                _clear_jit_cache()\n\n                # We need to import the scripted module from the temporary directory.\n                # This is because the original class is not scriptable.\n                script"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n    return image"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n    return image"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n    return image"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n    return image"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n    return image"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n    return image"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n    return image"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n        return image"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image_bytes = f.read()\n    image = Image.open(BytesIO(image_bytes))\n    image = _apply_exif_orientation(image)\n    image = convert_PIL_to_numpy(image, format)\n    return image"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n        return image"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        return convert_PIL_to_numpy(image, format)"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n        return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n        return image\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image_bytes = f.read()\n    image = Image.open(io.BytesIO(image_bytes))\n    image = _apply_exif_orientation(image)\n    image = convert_PIL_to_numpy(image, format)\n    return image"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n\n    return image"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image_bytes = f.read()\n    image = Image.open(BytesIO(image_bytes))\n    image = _apply_exif_orientation(image)\n    return convert_image_to_rgb(image, format)"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image_bytes = f.read()\n\n    # https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html#gif\n    if format is None and file_name.lower().endswith(\".gif\"):\n        format = \"GIF\"\n\n    # https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html#jpeg-2000\n    if format is None and file_name.lower().endswith(\".jp2\"):\n        format = \"JPEG2000\"\n\n    # https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html#jpeg\n    if format is None and file_name.lower().endswith(\".jpg\"):\n        format = \"JPEG\"\n\n    # https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html#png\n    if format is None and file_name.lower().endswith(\".png\"):\n        format = \"PNG\"\n\n    # https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html#webp\n    if format is None and file_name.lower().endswith(\".webp\"):\n        format = \"WEBP\"\n\n    # https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html#tiff\n    if format is None and file_name.lower().endswith(\".tif\"):\n        format = \"TIFF\"\n\n    # https://pillow.readthedocs.io/en/stable/handbook/image-file-formats.html#microsoftoffice\n    if format is None and file_name.lower().endswith(\".bmp\"):\n        format = \"BMP\"\n\n    # https://pillow.readthedocs.io/en/stable/"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # This function is a wrapper of PIL.Image.open, which supports\n    # reading image from local files and URLs.\n    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n    return image"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    # This function is ported from\n    # https://github.com/facebookresearch/detectron2/blob/master/detectron2/data/common.py\n    # but changed to only support HWC image and remove the following APIs:\n    # - .convert('RGB')\n    # - .transpose(0, 1, 2)\n    # - .tobytes()\n    # So that we can use it in torchscript.\n\n    # fmt: off\n    # The following lines are modified from the original implementation\n    # to add support for loading images from local files.\n    with PathManager.open(file_name, \"rb\") as f:\n        image_bytes = f.read()\n    # fmt: on\n    image = Image.open(Image.open(Image.open(Image.open(io.BytesIO(image_bytes)))))\n    image = _apply_exif_orientation(image)\n    image = convert_PIL_to_numpy(image, format)\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # Transform bounding boxes\n    if \"bbox\" in annotation:\n        bbox = BoxMode.convert(\n            annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS\n        )\n        bbox = transforms.apply_box([bbox])[0]\n        bbox = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n        annotation[\"bbox\"] = bbox\n\n    # Transform segmentation\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            segmentation = transforms.apply_polygons([np.asarray(annotation[\"segmentation\"])])[0]\n            segmentation = segmentation.tolist()\n        elif isinstance(annotation[\"segmentation\"], dict):\n            # COCO RLE\n            segmentation = transforms.apply_segmentation([annotation[\"segmentation\"]])[0]\n        else:\n            # Assume it is a binary mask\n            segmentation = transforms.apply_binary_mask([annotation[\"segmentation\"]], image_size)[0]\n        annotation[\"segmentation\"] = segmentation\n\n    # Transform keypoints\n    if \"keypoints\" in annotation:\n        keypoints = transform_keypoint_annotations(\n            annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints\n\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # Convert to XYXY_ABS\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform bbox\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox = transforms.apply_box([bbox])[0]\n    annotation[\"bbox\"] = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n\n    # Transform segmentation\n    if \"segmentation\" in annotation:\n        segm = annotation[\"segmentation\"]\n        if isinstance(segm, list):\n            # polygon\n            segm = transforms.apply_polygons([segm])[0]\n        elif isinstance(segm, dict):\n            # COCO RLE\n            segm = transforms.apply_polygons([segm])[0]\n        elif isinstance(segm, np.ndarray):\n            # mask array\n            segm = transforms.apply_coords(segm.reshape(-1, 2)).reshape(segm.shape)\n        else:\n            raise ValueError(\n                \"Cannot convert segmentation of type '{}' to BitMasks!\"\n                \"Supported types are: polygons as list[list[float] or ndarray],\"\n                \" COCO-style RLE as a dict, or a binary segmentation mask \"\n                \" in a 2D numpy array of shape HxW.\".format(type(segm))\n            )\n        annotation[\"segmentation\"] = segm\n\n    # Transform keypoints\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        keypoints = transform_keypoint_annotations(\n            keypoints, transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # TODO: support keypoints\n    if \"bbox\" in annotation:\n        annotation[\"bbox\"] = transforms.apply_box(\n            BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n        )\n        annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n        annotation[\"bbox\"] = BoxMode.convert(\n            annotation[\"bbox\"], BoxMode.XYXY_ABS, BoxMode.convert(annotation[\"bbox\"], BoxMode.XYXY_ABS, BoxMode.XYXY_ABS).mode\n        )\n        annotation[\"bbox\"] = BoxMode.convert(\n            annotation[\"bbox\"], BoxMode.XYXY_ABS, BoxMode.convert(annotation[\"bbox\"], BoxMode.XYXY_ABS, BoxMode.XYXY_ABS).mode\n        )\n        annotation[\"bbox\"] = BoxMode.convert(\n            annotation[\"bbox\"], BoxMode.XYXY_ABS, BoxMode.convert(annotation[\"bbox\"], BoxMode.XYXY_ABS, BoxMode.XYXY_ABS).mode\n        )\n        annotation[\"bbox\"] = BoxMode.convert(\n            annotation[\"bbox\"], BoxMode.XYXY_ABS, BoxMode.convert(annotation[\"bbox\"], BoxMode.XYXY_ABS, BoxMode.XYXY_ABS).mode\n        )\n        annotation[\"bbox\"] = BoxMode.convert(\n            annotation[\"bbox\"], BoxMode.XYXY_ABS, BoxMode.convert(annotation[\"bbox\"], BoxMode.XYXY_ABS, BoxMode.XYXY_ABS).mode\n        )\n        annotation[\"bbox\"] = BoxMode.convert(\n            annotation[\"bbox\"], BoxMode.XYXY_ABS, BoxMode.convert(annotation[\"bbox\"], BoxMode.XYXY_ABS, BoxMode.XYXY_ABS).mode\n        )\n        annotation[\"bbox\"] = BoxMode.convert(\n            annotation[\"bbox\"], BoxMode.XYXY_ABS, BoxMode.convert(annotation[\"bbox\"], BoxMode.XYXY_ABS, BoxMode"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # Transform bbox\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox = transforms.apply_box([bbox])[0]\n    bbox = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n    annotation[\"bbox\"] = bbox\n\n    # Transform segmentation\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            rles = mask_util.frPyObjects(annotation[\"segmentation\"], *image_size)\n            rle = mask_util.merge(rles)\n        elif isinstance(annotation[\"segmentation\"], dict):\n            # rle\n            rle = annotation[\"segmentation\"]\n        else:\n            # Assume it is a binary mask\n            rle = mask_util.encode(\n                np.array(annotation[\"segmentation\"], dtype=\"uint8\", order=\"F\")\n            )\n        rle = transforms.apply_segmentation(rle)\n        annotation[\"segmentation\"] = rle\n\n    # Transform keypoints\n    if \"keypoints\" in annotation:\n        keypoints = transform_keypoint_annotations(\n            annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints\n\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # Transform bounding box\n    if \"bbox\" in annotation:\n        bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n        bbox = transforms.apply_box([bbox])[0]\n        annotation[\"bbox\"] = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n\n    # Transform segmentation\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            for i in range(len(annotation[\"segmentation\"])):\n                annotation[\"segmentation\"][i] = transforms.apply_coords(\n                    np.asarray(annotation[\"segmentation\"][i]).reshape(-1, 2)\n                ).reshape(-1).tolist()\n        elif isinstance(annotation[\"segmentation\"], dict):\n            # COCO RLE\n            annotation[\"segmentation\"] = mask_util.frPyObjects(\n                annotation[\"segmentation\"], *image_size\n            )\n        elif isinstance(annotation[\"segmentation\"], np.ndarray):\n            # mask array\n            annotation[\"segmentation\"] = transforms.apply_segmentation(\n                annotation[\"segmentation\"]\n            )\n        else:\n            raise ValueError(\n                \"Cannot convert segmentation of type '{}' to BitMasks!\"\n                \"Supported types are: polygons as list[list[float] or ndarray],\"\n                \" COCO-style RLE as a dict, or a binary segmentation mask \"\n                \" in a 2D numpy array of shape HxW.\".format(type(annotation[\"segmentation\"]))\n            )\n\n    # Transform keypoints\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        keypoints = transform_keypoint_annotations(\n            keypoints, transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoint"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    bbox = annotation[\"bbox\"]\n    if \"bbox_mode\" in annotation:\n        bbox_mode = annotation[\"bbox_mode\"]\n    else:\n        bbox_mode = BoxMode.XYXY_ABS\n\n    # Transform bounding box\n    bbox = transforms.apply_box([bbox], bbox_mode)[0]\n    bbox_mode = BoxMode.XYXY_ABS\n\n    # Transform segmentation\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            segmentation = transforms.apply_polygons([np.asarray(annotation[\"segmentation\"])])[0]\n        elif isinstance(annotation[\"segmentation\"], dict):\n            # COCO RLE\n            segmentation = transforms.apply_segmentation([annotation[\"segmentation\"]])[0]\n        elif isinstance(annotation[\"segmentation\"], np.ndarray):\n            # mask array\n            segmentation = transforms.apply_segmentation([annotation[\"segmentation\"]])[0]\n        else:\n            raise ValueError(\n                \"Cannot convert segmentation of type '{}' to BitMasks!\"\n                \"Supported types are: polygons as list[list[float] or ndarray],\"\n                \" COCO-style RLE as a dict, or a binary segmentation mask \"\n                \" in a 2D numpy array of shape HxW.\".format(type(annotation[\"segmentation\"]))\n            )\n    else:\n        segmentation = None\n\n    # Transform keypoints\n    if \"keypoints\" in annotation:\n        keypoints = transform_keypoint_annotations(\n            annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n        )\n    else:\n        keypoints = None\n\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = bbox_mode\n    annotation[\"segmentation\"] = segmentation\n    annotation[\"keypoints\"] = keypoints\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # Transform bounding boxes\n    if \"bbox\" in annotation:\n        bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n        bbox = transforms.apply_box([bbox])[0]\n        annotation[\"bbox\"] = bbox\n        annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform segmentation\n    if \"segmentation\" in annotation:\n        segm = annotation[\"segmentation\"]\n        if isinstance(segm, list):\n            # polygon\n            segm = PolygonMasks(segm)\n        elif isinstance(segm, dict):\n            # COCO RLE\n            segm = mask_util.frPyObjects([segm], *image_size)\n            segm = segm[0]\n        else:\n            raise ValueError(\n                \"Cannot convert segmentation of type '{}' to BitMasks!\"\n                \"Supported types are: polygons as list[list[float] or ndarray],\"\n                \" COCO-style RLE as a dict, or a binary segmentation mask \"\n                \" in a 2D numpy array of shape HxW.\".format(type(segm))\n            )\n        segm = transforms.apply_segmentation(segm)\n        annotation[\"segmentation\"] = segm\n\n    # Transform keypoints\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        keypoints = transform_keypoint_annotations(keypoints, transforms, image_size, keypoint_hflip_indices)\n        annotation[\"keypoints\"] = keypoints\n\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, T.TransformList):\n        transforms = transforms.transforms\n\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox = transforms.apply_box([bbox])[0]\n    bbox = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            for i, segm in enumerate(annotation[\"segmentation\"]):\n                annotation[\"segmentation\"][i] = transforms.apply_coords(\n                    np.asarray(segm).reshape(-1, 2)\n                ).reshape(-1).tolist()\n        elif isinstance(annotation[\"segmentation\"], dict):\n            # COCO RLE\n            annotation[\"segmentation\"] = mask_util.frPyObjects(\n                annotation[\"segmentation\"], *image_size\n            )\n        elif isinstance(annotation[\"segmentation\"], np.ndarray):\n            # mask array\n            annotation[\"segmentation\"] = transforms.apply_segmentation(\n                annotation[\"segmentation\"]\n            )\n        else:\n            raise ValueError(\n                \"Cannot convert segmentation of type '{}' to BitMasks!\"\n                \"Supported types are: polygons as list[list[float] or ndarray],\"\n                \" COCO-style RLE as a dict, or a binary segmentation mask \"\n                \" in a 2D numpy array of shape HxW.\".format(type(annotation[\"segmentation\"]))\n            )\n\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        keypoints = np.asarray(keypoints, dtype=\"float64\").reshape(-1, 3)\n        keypoints"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # Transform bbox\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox = transforms.apply_box([bbox])[0]\n    annotation[\"bbox\"] = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n\n    # Transform segmentation\n    if \"segmentation\" in annotation:\n        segm = annotation[\"segmentation\"]\n        if isinstance(segm, list):\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            if isinstance(segm[0], list):\n                # polygon -- a single mask\n                segm = mask_util.frPyObjects(segm, *image_size)\n                segm = mask_util.merge(segm)\n            else:\n                # mask\n                segm = mask_util.frPyObjects(segm, *image_size)\n        elif isinstance(segm, dict):\n            # mask\n            segm = mask_util.merge([segm])\n        elif isinstance(segm, np.ndarray):\n            # mask\n            segm = segm.astype(np.uint8)\n            segm = mask_util.encode(segm)\n        else:\n            raise ValueError(\"Unknown segmentation format: {}\".format(segm))\n        annotation[\"segmentation\"] = segm\n\n    # Transform keypoints\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        keypoints = np.asarray(keypoints, dtype=\"float32\").reshape(-1, 3)\n        keypoints = keypoints[keypoints[:, 2] > 0, :]\n        keypoints[:, :2] = transforms.apply_coords(keypoints[:, :2])\n        annotation[\"keypoints\"] = keypoints.flatten().tolist()\n\n    annotation[\""}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    bbox = annotation[\"bbox\"]\n    bbox_mode = annotation.get(\"bbox_mode\", BoxMode.XYXY_ABS)\n    bbox = transforms.apply_box([bbox], bbox_mode)[0]\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    if \"segmentation\" in annotation:\n        segm = annotation[\"segmentation\"]\n        if isinstance(segm, list):\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            if isinstance(segm[0], list):\n                # polygon\n                mask = PolygonMasks(segm)\n            else:\n                # mask\n                mask = mask_util.decode(segm)\n            mask = transforms.apply_segmentation(mask)\n            annotation[\"segmentation\"] = mask_util.encode(\n                np.asarray(\n                    mask_util.remove_small_polygons(mask, min_size=1e-3, max_size=None),\n                    dtype=\"uint8\",\n                )\n            )\n        else:\n            # rle\n            segm = transforms.apply_segmentation(segm)\n            annotation[\"segmentation\"] = segm\n\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        keypoints = transform_keypoint_annotations(\n            keypoints, transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if \"bbox\" in annotation:\n        annotation[\"bbox\"] = transforms.apply_box(\n            BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n        )\n        annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            annotation[\"segmentation\"] = transforms.apply_segmentation(\n                annotation[\"segmentation\"]\n            )\n        elif isinstance(annotation[\"segmentation\"], dict):\n            # COCO RLE\n            annotation[\"segmentation\"] = transforms.apply_segmentation_proposal(\n                annotation[\"segmentation\"]\n            )\n        elif isinstance(annotation[\"segmentation\"], np.ndarray):\n            # mask array\n            annotation[\"segmentation\"] = transforms.apply_segmentation_mask(\n                annotation[\"segmentation\"]\n            )\n        else:\n            raise ValueError(\n                \"Cannot convert segmentation of type '{}' to BitMasks!\"\n                \"Supported types are: polygons as list[list[float] or ndarray],\"\n                \" COCO-style RLE as a dict, or a binary segmentation mask \"\n                \" in a 2D numpy array of shape HxW.\".format(type(annotation[\"segmentation\"]))\n            )\n        annotation[\"segmentation\"] = transforms.apply_box(\n            BoxMode.convert(\n                annotation[\"segmentation\"],\n                BoxMode.XYXY_ABS,\n                BoxMode.XYXY_ABS,\n            )\n        )\n        annotation[\"segmentation\"] = transforms.apply_image(annotation[\"segmentation\"])\n        annotation[\"segmentation\"] = transforms.apply_box(\n            BoxMode.convert(\n                annotation[\"segmentation\"],\n                BoxMode.XYXY_ABS,\n                BoxMode.XYXY_ABS,\n            )\n        )\n\n    if \"keypoints\" in annotation:\n        annotation[\"keyp"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # TODO: add support for \"keypoints\"\n    # TODO: add support for \"bbox_mode\"\n    # TODO: add support for \"segmentation_mode\"\n    # TODO: add support for \"iscrowd\"\n    # TODO: add support for \"area\"\n    # TODO: add support for \"category_id\"\n    # TODO: add support for \"ignore\"\n    # TODO: add support for \"segmentation_id\"\n    # TODO: add support for \"bbox_id\"\n    # TODO: add support for \"keypoint_id\"\n    # TODO: add support for \"image_id\"\n    # TODO: add support for \"id\"\n    # TODO: add support for \"iscrowd\"\n    # TODO: add support for \"category_id\"\n    # TODO: add support for \"ignore\"\n    # TODO: add support for \"segmentation_id\"\n    # TODO: add support for \"bbox_id\"\n    # TODO: add support for \"keypoint_id\"\n    # TODO: add support for \"image_id\"\n    # TODO: add support for \"id\"\n    # TODO: add support for \"iscrowd\"\n    # TODO: add support for \"category_id\"\n    # TODO: add support for \"ignore\"\n    # TODO: add support for \"segmentation_id\"\n    # TODO: add support for \"bbox_id\"\n    # TODO: add support for \"keypoint_id\"\n    # TODO: add support for \"image_id\"\n    # TODO: add support for \"id\"\n    # TODO: add support for \"iscrowd\"\n    # TODO: add support for \"category_id\"\n    # TODO: add support for \"ignore\"\n    # TODO: add support for \"segmentation_id\"\n    # TODO: add support for \"bbox_id\"\n    # TODO: add support for \"keypoint_id\"\n    # TODO: add support for \"image_id\"\n    # TODO: add support for \"id\"\n    # TODO: add support for \"iscrowd\"\n    # TODO: add support for \"category_id\""}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # Transform bounding boxes\n    if \"bbox\" in annotation:\n        bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n        bbox = transforms.apply_box([bbox])[0]\n        annotation[\"bbox\"] = bbox\n        annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform segmentation\n    if \"segmentation\" in annotation:\n        segm = annotation[\"segmentation\"]\n        if isinstance(segm, list):\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            if isinstance(segm[0], list):\n                # polygon -- a single mask\n                mask = mask_util.frPyObjects(segm, *image_size)\n            else:\n                # mask\n                mask = mask_util.decode(segm)\n        elif isinstance(segm, dict):\n            # RLE\n            mask = mask_util.decode(segm)\n        else:\n            # Assume it is a binary mask\n            mask = segm\n\n        # TODO: support instance masks\n        mask = transforms.apply_segmentation(mask)\n        annotation[\"segmentation\"] = mask_util.encode(\n            np.asarray(mask, order=\"F\", dtype=\"uint8\")\n        )\n\n    # Transform keypoints\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        keypoints = transform_keypoint_annotations(\n            keypoints, transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # Transform bounding boxes\n    if \"bbox\" in annotation:\n        bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n        bbox = transforms.apply_box([bbox])[0]\n        annotation[\"bbox\"] = bbox\n        annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform segmentation\n    if \"segmentation\" in annotation:\n        segm = annotation[\"segmentation\"]\n\n        # Transform segmentation polygons\n        if isinstance(segm, list):\n            transformed_segm = transforms.apply_segmentation(segm)\n            annotation[\"segmentation\"] = transformed_segm\n\n        # Transform segmentation RLE\n        elif isinstance(segm, dict) and \"counts\" in segm:\n            transformed_segm = transforms.apply_segmentation_masks([segm])[0]\n            annotation[\"segmentation\"] = transformed_segm\n\n        # Transform segmentation bitmask\n        elif isinstance(segm, np.ndarray):\n            transformed_segm = transforms.apply_segmentation_masks([segm])[0]\n            annotation[\"segmentation\"] = transformed_segm\n\n        # Transform segmentation mask\n        else:\n            raise ValueError(\n                \"Cannot convert segmentation of type '{}' to BitMasks!\"\n                \"Supported types are: polygons as list[list[float] or ndarray],\"\n                \" COCO-style RLE as a dict, or a binary segmentation mask \"\n                \" in a 2D numpy array of shape HxW.\".format(type(segm))\n            )\n\n    # Transform keypoints\n    if \"keypoints\" in annotation:\n        keypoints = transform_keypoint_annotations(\n            annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints\n\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if \"bbox\" in annotation:\n        bbox = annotation[\"bbox\"]\n        if isinstance(bbox, list):\n            bbox = BoxMode.convert(bbox, BoxMode.XYXY_ABS, BoxMode.XYXY_ABS)\n        else:\n            bbox = BoxMode.convert(bbox, annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n        bbox = transforms.apply_box([bbox])[0]\n        bbox = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n        annotation[\"bbox\"] = bbox\n        annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    if \"segmentation\" in annotation:\n        segm = annotation[\"segmentation\"]\n        if isinstance(segm, list):\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            if isinstance(segm[0], list):\n                # polygon\n                mask = PolygonMasks(segm)\n            else:\n                # mask\n                mask = mask_util.frPyObjects(segm, *image_size)\n                mask = torch.as_tensor(mask.astype(\"uint8\"))\n                mask = mask[None].nonzero()\n                mask = mask.reshape(-1)\n                mask = mask.split(4)\n                mask = mask_util.merge(mask)\n        else:\n            # RLE\n            mask = mask_util.frPyObjects(segm, *image_size)\n            mask = torch.as_tensor(mask.astype(\"uint8\"))\n        annotation[\"segmentation\"] = transforms.apply_segmentation(mask)[0]\n\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        keypoints = np.asarray(keypoints, dtype=\"float64\").reshape(-1, 3)\n        keypoints = transforms.apply_coords(keypoints"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # Transform bounding box\n    if \"bbox\" in annotation:\n        bbox = BoxMode.convert(\n            annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS\n        )\n        bbox = transforms.apply_box([bbox])[0]\n        annotation[\"bbox\"] = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n\n    # Transform segmentation\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            segmentation = transforms.apply_segmentation(annotation[\"segmentation\"])\n            annotation[\"segmentation\"] = segmentation\n        elif isinstance(annotation[\"segmentation\"], dict):\n            # COCO RLE\n            segmentation = transforms.apply_segmentation([annotation[\"segmentation\"]])[0]\n            annotation[\"segmentation\"] = segmentation\n        elif isinstance(annotation[\"segmentation\"], np.ndarray):\n            # mask array\n            segmentation = transforms.apply_segmentation([annotation[\"segmentation\"]])[0]\n            annotation[\"segmentation\"] = segmentation\n        else:\n            raise ValueError(\n                \"Cannot convert segmentation of type '{}' to BitMasks!\"\n                \"Supported types are: polygons as list[list[float] or ndarray],\"\n                \" COCO-style RLE as a dict, or a binary segmentation mask \"\n                \" in a 2D numpy array of shape HxW.\".format(type(annotation[\"segmentation\"]))\n            )\n\n        # Transform mask coordinates\n        if \"bbox\" in annotation:\n            segmentation = transforms.apply_coords(segmentation)\n            annotation[\"segmentation\"] = segmentation\n\n    # Transform keypoints\n    if \"keypoints\" in annotation:\n        annotation[\"keypoints\"] = transform_keypoint_annotations(\n            annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n        )\n\n    # Transform b"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if not isinstance(transforms, TransformList):\n        transforms = TransformList(transforms)\n\n    if \"bbox\" in annotation:\n        annotation[\"bbox\"] = transforms.apply_box(\n            BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n        )\n        annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            if len(annotation[\"segmentation\"]) == 0:\n                annotation[\"segmentation\"] = mask_util.frPyObjects(\n                    [], *image_size\n                )  # pycocotools's format\n            else:\n                # in COCO, all parts of an object are merged into one segmentation mask\n                # we merge all parts into one mask rle code\n                segmentation = [\n                    mask_util.frPyObjects(part, *image_size) for part in annotation[\"segmentation\"]\n                ]\n                segmentation = mask_util.merge(segmentation)\n        else:\n            # mask\n            segmentation = annotation[\"segmentation\"]\n            if isinstance(segmentation, dict):\n                # COCO RLE\n                segmentation = mask_util.frPyObjects(segmentation, *image_size)\n            else:\n                # mask\n                segmentation = mask_util.decode(segmentation)\n\n        annotation[\"segmentation\"] = transforms.apply_segmentation(segmentation)\n\n    if \"keypoints\" in annotation:\n        annotation[\"keypoints\"] = transform_keypoint_annotations(\n            annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n        )"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    bbox = annotation[\"bbox\"]\n    bbox = BoxMode.convert(bbox, annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox = transforms.apply_box([bbox])[0]\n    annotation[\"bbox\"] = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    if \"segmentation\" in annotation:\n        segm = annotation[\"segmentation\"]\n        if isinstance(segm, list):\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            if len(segm) > 0:\n                # merge all parts\n                rles = mask_util.frPyObjects(segm, *image_size)\n                rle = mask_util.merge(rles)\n            else:\n                rle = mask_util.frPyObjects(segm, *image_size)\n            ann_segm = mask_util.decode(rle)\n            ann_segm = transforms.apply_segmentation(ann_segm)\n            ann_segm = mask_util.encode(\n                np.array(\n                    ann_segm[:, :, np.newaxis], order=\"F\", dtype=\"uint8\"\n                )  # TODO: remove the dimension\n            )\n            if len(ann_segm[\"counts\"]) == 0:\n                ann_segm = mask_util.encode(\n                    np.zeros((image_size[0], image_size[1], 1), dtype=\"uint8\", order=\"F\")\n                )\n            segm = mask_util.decode(ann_segm)\n            if len(segm.shape) < 3:\n                # mask_util.decode returns a mask with only one (1) channel\n                # for COCO and it seems, in this case, to have to add a channel\n                segm = segm[..., np.newaxis]\n            segm = segm.astype"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if not isinstance(transforms, (list, T.TransformList)):\n        transforms = [transforms]\n    transforms = T.TransformList(transforms)\n\n    if \"bbox\" in annotation:\n        annotation[\"bbox\"] = transforms.apply_box(\n            BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n        )\n        annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            if all(isinstance(obj, list) for obj in annotation[\"segmentation\"]):\n                # merge instance-level RLEs\n                segmentation = mask_util.merge(annotation[\"segmentation\"])\n            else:\n                # merge whole-image masks\n                segmentation = annotation[\"segmentation\"]\n        else:\n            # Assume it is a COCO-style RLE\n            segmentation = annotation[\"segmentation\"]\n\n        if isinstance(segmentation, dict):\n            # COCO-style RLE\n            segmentation = mask_util.frPyObjects(segmentation, *image_size)\n        elif isinstance(segmentation, list):\n            # polygon\n            segmentation = mask_util.frPyObjects(segmentation, *image_size)\n            segmentation = [\n                mask_util.decode(segm).astype(\"uint8\") for segm in segmentation\n            ]\n        else:\n            # Assume it is a binary mask\n            segmentation = mask_util.decode(segmentation).astype(\"uint8\")\n\n        if \"bbox\" in annotation:\n            # Crop the mask, accounting for image size here\n            x0, y0, x1, y1 = annotation[\"bbox\"]\n            x0 = max(0, x0)\n            y0 = max(0, y0)\n            x1 = min(x1, image_size[1])\n            y1 = min("}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # The reason we do not use `transforms.apply_box` here is that\n    # it will apply denormalization first, which is not what we want.\n    # We want to apply the transforms first and then apply the\n    # normalization.\n    if \"bbox\" in annotation:\n        bbox = np.array(annotation[\"bbox\"])\n        if bbox.size == 0:\n            bbox = np.zeros((0, 4), dtype=bbox.dtype)\n        else:\n            bbox = BoxMode.convert(bbox, annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n        bbox = transforms.apply_box([bbox])[0]\n        bbox = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n        annotation[\"bbox\"] = bbox\n\n    if \"segmentation\" in annotation:\n        segm = annotation[\"segmentation\"]\n        if isinstance(segm, list):\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            if len(segm) == 0:\n                segm = np.zeros((0, 4), dtype=np.float64)\n            else:\n                segm = merge_polygons(segm, image_size)\n                segm = transforms.apply_coords([segm])[0]\n                segm = segm.reshape(-1).tolist()\n        elif isinstance(segm, dict):\n            # Assume it is a COCO-style RLE to avoid additional dependencies\n            segm = transforms.apply_coords([segm])[0]\n        else:\n            raise ValueError(\n                \"Unknown segmentation format: {}\".format(\n                    type(segm)\n                )  # polygons as list, COCO-style RLE as a dict\n            )\n        annotation[\"segmentation\"] = segm\n\n    if \"keypoints\" in annotation:\n        kpts = annotation[\"keypoints\"]\n        if len("}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        return cv2.transform(coords, self.rm_coords)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        return cv2.transform(coords, self.rm_coords)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        return cv2.transform(coords, self.rm_coords)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        return cv2.transform(coords, self.rm_coords)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        return cv2.transform(coords[None, :, :], self.rm_coords)[0, :, :]"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        return cv2.transform(coords, self.rm_coords)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        coords = np.array(coords)\n        coords = np.hstack((coords, np.ones((coords.shape[0], 1))))\n        coords = np.dot(coords, self.rm_coords.T)\n        return coords[:, :2]"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        # Apply the rotation matrix to the coordinates\n        rotated_coords = cv2.transform(coords[None, :, :], self.rm_coords)[0, :, :]\n        return rotated_coords"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        coords = np.asarray(coords)\n        return cv2.transform(coords[None, :, :], self.rm_coords)[0, :, :]"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        coords = np.array(coords)\n        coords = np.concatenate([coords, np.ones((len(coords), 1))], axis=1)\n        coords = np.matmul(coords, self.rm_coords.T)\n        return coords[:, :2]"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        coords = np.array(coords, dtype=np.float32)\n        coords = cv2.transform(coords[None, None, :], self.rm_coords)[0, 0, :]\n        return coords"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        coords = np.array(coords, dtype=np.float32)\n        coords -= self.center\n        coords = np.dot(coords, self.rm_coords[:2, :2])\n        coords += self.center\n        return coords"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        return cv2.transform(coords[:, None, :] + self.offset, self.rm_coords)[:, 0, :] - self.offset"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        coords = np.hstack([coords, np.ones((coords.shape[0], 1))])\n        coords = np.dot(coords, self.rm_coords.T)\n        return coords[:, :2]"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        coords = np.asarray(coords, dtype=np.float32)\n        coords -= self.center\n        coords = cv2.transform(coords[:, None, :], self.rm_coords)[:, 0, :]\n        coords += self.center\n        return coords"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        coords = np.array(coords, dtype=np.float64)\n        return cv2.transform(coords[None, :, :], self.rm_coords)[0, :, :]"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        coords = np.asarray(coords)\n        coords = cv2.transform(coords[:, None, :], self.rm_coords)[:, 0, :]\n        return coords"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        coords = np.asarray(coords)\n        coords = cv2.transform(coords[None, :, :], self.rm_coords)[0, :, :]\n        return coords"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        coords = np.asarray(coords)\n        coords = coords - self.center\n        coords = np.dot(coords, self.rm_coords.T)\n        return coords"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        # Rotate each point\n        coords = np.concatenate([coords, np.ones(coords.shape[0])[:, None]], axis=1)\n        coords = np.matmul(coords, self.rm_coords.T)\n        coords = coords[:, :2]\n        return coords"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if len(img.shape) > 2 and img.shape[2] == 1:\n            img = img[:, :, 0]\n        if len(img.shape) > 2:\n            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        if self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if len(img.shape) > 2 and img.shape[2] == 1:\n            img = img[:, :, 0]\n        if len(img.shape) > 2:\n            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        if self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if len(img.shape) > 2 and img.shape[2] == 1:\n            img = img[:, :, 0]\n        if len(img.shape) > 2:\n            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        if self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if len(img.shape) > 2 and img.shape[2] == 1:\n            img = img[:, :, 0]\n        if len(img.shape) > 2:\n            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        if self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if len(img.shape) > 2 and img.shape[2] == 1:\n            img = img[:, :, 0]\n        if img.shape[:2] == (self.h, self.w) and self.angle % 360 == 0:\n            return img\n        return cv2.warpAffine(\n            img, self.rm_image, (self.bound_w, self.bound_h), flags=cv2.INTER_LINEAR\n        )"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if len(img.shape) > 2 and img.shape[2] == 1:\n            img = img[:, :, 0]\n        if len(img.shape) > 2 and img.shape[2] == 3:\n            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        if img.shape[:2] == (self.h, self.w) and self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if len(img.shape) > 2 and img.shape[2] == 1:\n            img = img[:, :, 0]\n        if len(img.shape) > 2:\n            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        if self.angle % 360 == 0:\n            return img\n        interp_method = interp if interp is not None else self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp_method)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if len(img.shape) > 2 and img.shape[2] == 1:\n            img = img[:, :, 0]\n        if len(img.shape) == 2:\n            img = img[..., None]\n        if img.shape[:2] == (self.h, self.w) and self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if len(img.shape) > 2 and img.shape[2] == 1:\n            img = img[:, :, 0]\n\n        if len(img.shape) > 2:\n            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n\n        if self.angle % 360 == 0:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if len(img.shape) > 2 and img.shape[2] == 1:\n            pil_image = Image.fromarray(img[:, :, 0], mode=\"L\")\n        else:\n            pil_image = Image.fromarray(img)\n        if self.angle % 360 == 0:\n            return np.asarray(pil_image)\n        interp_method = interp if interp is not None else self.interp\n        pil_image = pil_image.rotate(self.angle, expand=self.expand, center=self.center, resample=interp_method)\n        ret = np.asarray(pil_image)\n        if len(img.shape) > 2 and img.shape[2] == 1:\n            ret = np.expand_dims(ret, -1)\n        return ret\n\n    "}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if len(img.shape) > 2 and img.shape[2] == 1:\n            pil_image = Image.fromarray(img[:, :, 0], mode=\"L\")\n        else:\n            pil_image = Image.fromarray(img)\n        if self.angle % 360 == 0:\n            return img\n        return np.asarray(\n            pil_image.transform(\n                size=(self.bound_w, self.bound_h),\n                method=Image.AFFINE,\n                data=self.rm_image[:2, :].flatten(),\n                resample=interp if interp else self.interp,\n            )\n        )"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if len(img.shape) > 2 and img.shape[2] == 1:\n            img = img[:, :, 0]\n        if len(img.shape) == 2:\n            img = np.expand_dims(img, -1)\n        if len(img.shape) != 3:\n            raise ValueError(\"Image must be Height * Width * Nchannels\")\n        if self.angle % 360 == 0:\n            return img\n        return cv2.warpAffine(\n            img, self.rm_image, (self.bound_w, self.bound_h), flags=cv2.INTER_LINEAR\n        )"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if len(img.shape) > 2 and img.shape[2] == 1:\n            img = img[:, :, 0]\n        if len(img.shape) == 2:\n            img = np.expand_dims(img, -1)\n        if len(img.shape) > 2 and img.shape[2] == 1:\n            img = img[:, :, 0]\n        if img.shape[:2] == (self.h, self.w) and self.angle % 360 == 0:\n            return img\n        if img.shape[:2] != (self.h, self.w):\n            raise ValueError(\"Image shape should be {}\".format((self.h, self.w)))\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if len(img.shape) > 2 and img.shape[2] == 1:\n            img = img[:, :, 0]\n        if len(img.shape) > 2:\n            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        if self.angle % 360 == 0:\n            return img\n        if img.shape[:2] != (self.h, self.w):\n            raise ValueError(\"Image shape must match the shape of the original image\")\n        return cv2.warpAffine(\n            img, self.rm_image, (self.bound_w, self.bound_h), flags=cv2.INTER_NEAREST\n        )"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if len(img.shape) > 2 and img.shape[2] == 1:\n            img = img[:, :, 0]\n        if len(img.shape) == 2:\n            img = img.reshape(img.shape + (1,))\n        if img.shape[2] == 3:\n            img = img[:, :, ::-1]\n        if len(img.shape) == 3:\n            img = img.transpose(2, 0, 1)\n        if len(img.shape) > 3:\n            raise ValueError(\"Image shape should be Height * Width * Nchannels\")\n        if len(img) == 0 or self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if len(img.shape) > 2 and img.shape[2] == 1:\n            img = img[:, :, 0]\n        if img.shape[:2] == (self.h, self.w) and self.angle % 360 == 0:\n            return img\n        if len(img.shape) == 2:\n            img = img[np.newaxis, :, :]\n        elif len(img.shape) == 3:\n            img = img.transpose(2, 0, 1)\n        else:\n            raise ValueError(\"Input image should be either a 2D or 3D array\")\n        if img.dtype == np.uint8:\n            img = img.astype(np.float32) / 255.0\n        img = cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n        if img.dtype == np.float32:\n            img = (img * 255).astype(np.uint8)\n        if len(img.shape) == 3:\n            img = img.transpose(1, 2, 0)\n        elif len(img.shape) == 2:\n            img = img[0, :, :]\n        else:\n            raise ValueError(\"Input image should be either a 2D or 3D array\")\n        return img"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if len(img.shape) > 2 and img.shape[2] == 1:\n            img = img[:, :, 0]\n        if len(img.shape) == 2:\n            img = img[None, :, :]\n        if len(img.shape) == 3 and img.shape[0] == 1:\n            img = img[0]\n        if len(img.shape) > 2 and img.shape[2] == 1:\n            img = img[:, :, 0]\n\n        if len(img.shape) == 3:\n            img = np.transpose(img, (2, 0, 1))\n\n        if len(img.shape) == 3 and img.shape[0] == 1:\n            img = img[0]\n        if len(img.shape) == 2:\n            img = img[None, :, :]\n\n        if len(img.shape) == 3 and img.shape[0] == 1:\n            img = img[0]\n\n        if len(img.shape) == 2:\n            img = img[None, :, :]\n\n        if len(img.shape) == 3 and img.shape[0] == 1:\n            img = img[0]\n\n        if len(img.shape) == 2:\n            img = img[None, :, :]\n\n        if len(img.shape) == 3 and img.shape[0] == 1:\n            img = img[0]\n\n        if len(img.shape) == 2:\n            img = img[None, :, :]\n\n        if len(img.shape) == 3 and img.shape[0] == 1:\n            img = img[0]\n\n        if len(img.shape) == 2:\n            img = img[None, :, :]\n\n        if len(img.shape) == 3 and img.shape[0] == 1:\n            img = img[0]\n\n        if len(img.shape) == 2:\n            img = img[None, :, :]\n\n        if len(img.shape) == 3 and img.shape[0]"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if len(img.shape) > 2 and img.shape[2] == 1:\n            img = img[:, :, 0]\n        if len(img.shape) == 2:\n            img = np.expand_dims(img, -1)\n        if len(img.shape) == 3 and img.shape[2] == 1:\n            img = img[:, :, 0]\n        if len(img.shape) == 3 and img.shape[2] == 3:\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        if len(img.shape) == 3 and img.shape[2] == 4:\n            img = cv2.cvtColor(img, cv2.COLOR_BGRA2RGBA)\n        if img.shape[0] != self.h or img.shape[1] != self.w:\n            raise ValueError(\"Image shape is not correct\")\n\n        if self.angle % 360 == 0:\n            return img\n\n        if interp is None:\n            interp = self.interp\n        ret = cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n        if len(img.shape) == 2:\n            ret = np.expand_dims(ret, -1)\n        return ret"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if len(img.shape) > 2 and img.shape[2] == 1:\n            img = img[:, :, 0]\n        if len(img.shape) == 2:\n            img = np.expand_dims(img, -1)\n        if len(img.shape) != 3:\n            raise ValueError(\"Input image should be a Height * Width * Nchannels array\")\n        if self.angle % 360 == 0:\n            return img\n        h, w = img.shape[:2]\n        if self.expand:\n            output_shape = (self.bound_h, self.bound_w)\n        else:\n            output_shape = (h, w)\n        if interp is None:\n            interp = self.interp\n        ret = cv2.warpAffine(img, self.rm_image, output_shape, flags=interp)\n        if self.expand:\n            # shift the rotation center to the new coordinates\n            rm_shift = np.eye(3)\n            rm_shift[:2, 2] = self.image_center - self.center\n            rm = np.dot(self.rm_image, rm_shift)\n            ret = cv2.warpAffine(ret, rm, output_shape, flags=interp)\n        if len(img.shape) > 2 and img.shape[2] == 1:\n            ret = np.expand_dims(ret, -1)\n        return ret"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if len(img.shape) == 3:\n            img = img[:, :, ::-1]\n        if len(img.shape) == 2:\n            img = img[..., np.newaxis]\n        if len(img.shape) > 4 or img.shape[2] > 4:\n            raise ValueError(\"Image shape should be at most HWC\")\n        if len(img.shape) == 4:\n            img = img.transpose(0, 3, 1, 2)\n        if img.shape[:2] == (self.h, self.w) and self.angle % 360 == 0:\n            return img\n        if img.shape[:2] != (self.h, self.w):\n            raise ValueError(\"Shape of img should be (h, w)\")\n        if self.angle % 360 == 0:\n            raise ValueError(\"angle should not be a multiple of 360\")\n        if img.dtype == np.uint8:\n            img = cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n        else:\n            img = torch.from_numpy(img)\n            shape = list(img.shape)\n            shape_4d = shape[:2] + [1] * (4 - len(shape)) + shape[2:]\n            img = img.view(shape_4d).permute(2, 3, 0, 1)  # hw(c) -> nchw\n            img = F.affine_grid(\n                torch.from_numpy(self.rm_image.astype(np.float32)),\n                img.shape,\n                align_corners=False,\n            )\n            img = F.grid_sample(img, img.new_tensor(img.shape[2:]).view(1, 2, 1, 1), align_corners=False)\n            shape[:2] = (self.bound_h, self.bound_w)\n            img = img.per"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if not predictions:\n            return self.output\n\n        if self._instance_mode == ColorMode.IMAGE_BW:\n            self.output.reset_image(self._create_grayscale_image(predictions.pred_masks))\n\n        if self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\n            colors = [\n                self._jitter([x / 255 for x in self.metadata.thing_colors[c]])\n                for c in predictions.pred_classes\n            ]\n        else:\n            colors = None\n        labels = _create_text_labels(\n            predictions.pred_classes,\n            predictions.scores,\n            self.metadata.thing_classes,\n            predictions.get(\"pred_crowd\", None),\n        )\n        self.overlay_instances(\n            boxes=predictions.pred_boxes,\n            labels=labels,\n            masks=predictions.pred_masks,\n            assigned_colors=colors,\n        )\n\n        if predictions.has(\"pred_keypoints\"):\n            self.draw_and_connect_keypoints(predictions.pred_keypoints)\n\n        return self.output\n\n    "}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if isinstance(predictions, torch.Tensor):\n            predictions = predictions.to(self.cpu_device)\n        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes if predictions.has(\"pred_classes\") else None\n        masks = predictions.pred_masks if predictions.has(\"pred_masks\") else None\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if self._instance_mode == ColorMode.IMAGE_BW:\n            self.output.reset_image(self._create_grayscale_image(predictions.pred_masks.any(dim=0)))\n\n        labels = _create_text_labels(classes, scores, self.metadata.thing_classes)\n        self.overlay_instances(\n            boxes=boxes, labels=labels, masks=masks, keypoints=keypoints, alpha=0.5\n        )\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if self._instance_mode == ColorMode.IMAGE_BW:\n            self.output.reset_image(self._create_grayscale_image())\n\n        if \"pred_boxes\" in predictions:\n            boxes = predictions[\"pred_boxes\"]\n            if isinstance(boxes, RotatedBoxes):\n                boxes = boxes.tensor.numpy()\n            else:\n                boxes = boxes.tensor.cpu().numpy()\n            boxes = BoxMode.convert(boxes, BoxMode.XYXY_ABS, BoxMode.XYXY_ABS)\n            labels = _create_text_labels(\n                predictions[\"pred_classes\"],\n                predictions[\"scores\"],\n                self.metadata.get(\"thing_classes\", None),\n            )\n            self.overlay_instances(boxes=boxes, labels=labels)\n\n        if \"pred_masks\" in predictions:\n            masks = predictions[\"pred_masks\"]\n            if isinstance(masks, torch.Tensor):\n                masks = masks.numpy()\n            if isinstance(masks, list):\n                masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n            else:\n                masks = [GenericMask(masks, self.output.height, self.output.width)]\n            labels = _create_text_labels(\n                predictions[\"pred_classes\"],\n                predictions[\"scores\"],\n                self.metadata.get(\"thing_classes\", None),\n            )\n            self.overlay_instances(masks=masks, labels=labels)\n\n        if \"pred_masks_rle\" in predictions:\n            masks = predictions[\"pred_masks_rle\"]\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n            labels = _create_text_labels(\n                predictions[\"pred_classes\"],\n                predictions[\"scores\"],\n                self.metadata.get(\"thing_classes\", None),\n            )\n            self.overlay_instances(masks=masks"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if not isinstance(predictions, torch.Tensor):\n            predictions = predictions.to(self.cpu_device)\n        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.thing_classes)\n        masks = predictions.pred_masks if predictions.has(\"pred_masks\") else None\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        self.overlay_instances(boxes=boxes, labels=labels, masks=masks, keypoints=keypoints)\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if predictions.has(\"pred_masks\"):\n            pred_masks = predictions.pred_masks\n            if isinstance(pred_masks, torch.Tensor):\n                pred_masks = [x for x in pred_masks]\n            elif isinstance(pred_masks, GenericMask):\n                pred_masks = [pred_masks]\n            elif isinstance(pred_masks, list) and isinstance(pred_masks[0], torch.Tensor):\n                pred_masks = [GenericMask(x, self.output.height, self.output.width) for x in pred_masks]\n            else:\n                assert isinstance(pred_masks, list) and isinstance(pred_masks[0], GenericMask)\n        else:\n            pred_masks = None\n\n        if predictions.has(\"pred_masks_rle\"):\n            pred_masks_rle = predictions.pred_masks_rle\n            if isinstance(pred_masks_rle, torch.Tensor):\n                pred_masks_rle = [x for x in pred_masks_rle]\n            elif isinstance(pred_masks_rle, list) and isinstance(pred_masks_rle[0], torch.Tensor):\n                pred_masks_rle = [GenericMask(x, self.output.height, self.output.width) for x in pred_masks_rle]\n            else:\n                assert isinstance(pred_masks_rle, list) and isinstance(pred_masks_rle[0], GenericMask)\n        else:\n            pred_masks_rle = None\n\n        if predictions.has(\"pred_keypoints\"):\n            pred_keypoints = predictions.pred_keypoints\n            if isinstance(pred_keypoints, torch.Tensor):\n                pred_keypoints = [x for x in pred_keypoints]\n            elif isinstance(pred_keypoints, list) and isinstance(pred_keypoints"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if not isinstance(predictions, (list, tuple)):\n            predictions = [predictions]\n        for pred in predictions:\n            if \"pred_masks\" in pred:\n                self.draw_instance_predictions(pred[\"pred_boxes\"], pred[\"pred_classes\"], pred[\"scores\"], pred[\"pred_masks\"])\n            elif \"pred_masks_rle\" in pred:\n                self.draw_instance_predictions(pred[\"pred_boxes\"], pred[\"pred_classes\"], pred[\"scores\"], pred[\"pred_masks_rle\"])\n            else:\n                self.draw_instance_predictions(pred[\"pred_boxes\"], pred[\"pred_classes\"], pred[\"scores\"])\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if isinstance(predictions, torch.Tensor):\n            predictions = {\"instances\": predictions}\n        elif isinstance(predictions, list):\n            predictions = {\"instances\": predictions[0]}\n\n        if \"instances\" in predictions:\n            instances = predictions[\"instances\"].to(self.cpu_device)\n            boxes = instances.pred_boxes\n            scores = instances.scores\n            classes = instances.pred_classes\n            if instances.has(\"pred_masks\"):\n                masks = instances.pred_masks\n            elif instances.has(\"pred_masks_rle\"):\n                masks = instances.pred_masks_rle\n            else:\n                masks = None\n            keypoints = instances.pred_keypoints\n\n            labels = _create_text_labels(\n                classes, scores, self.metadata.thing_classes, instances.pred_classes\n            )\n            self.overlay_instances(\n                boxes=boxes,\n                masks=masks,\n                labels=labels,\n                keypoints=keypoints,\n            )\n        return self.output\n\n    "}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if isinstance(predictions, torch.Tensor):\n            predictions = {\"instances\": predictions}\n        if \"instances\" in predictions:\n            instances = predictions[\"instances\"].to(self.cpu_device)\n            if instances.has(\"pred_masks\"):\n                self.draw_instance_predictions(instances)\n            elif instances.has(\"pred_boxes\"):\n                boxes = instances.pred_boxes if instances.pred_boxes.tensor.shape[1] == 4 else instances.pred_masks\n                boxes = boxes.tensor if isinstance(boxes, RotatedBoxes) else boxes.get_boxes()\n                scores = instances.scores if instances.has(\"scores\") else None\n                labels = instances.pred_classes\n                self.overlay_instances(\n                    boxes=boxes,\n                    labels=labels,\n                    scores=scores,\n                )\n        if \"proposals\" in predictions:\n            self.draw_instance_predictions(predictions[\"proposals\"])\n        return self.output\n\n    "}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if isinstance(predictions, torch.Tensor):\n            predictions = predictions.to(self.cpu_device)\n        if isinstance(predictions, dict):\n            predictions = [predictions]\n\n        if self._instance_mode == ColorMode.IMAGE_BW:\n            self.output.reset_image(self._create_grayscale_image(predictions[0][\"pred_masks\"]))\n\n        for pred in predictions:\n            if \"pred_boxes\" in pred:\n                boxes = pred[\"pred_boxes\"].tensor.numpy()\n                boxes = BoxMode.convert(boxes, BoxMode.XYXY_ABS, BoxMode.XYXY_ABS)\n            else:\n                boxes = None\n\n            if \"pred_classes\" in pred:\n                classes = pred[\"pred_classes\"].tolist()\n            else:\n                classes = None\n\n            if \"scores\" in pred:\n                scores = pred[\"scores\"].tolist()\n            else:\n                scores = None\n\n            if \"pred_masks\" in pred:\n                masks = pred[\"pred_masks\"]\n            elif \"pred_masks_rle\" in pred:\n                masks = pred[\"pred_masks_rle\"]\n            else:\n                masks = None\n\n            if \"pred_keypoints\" in pred:\n                keypoints = pred[\"pred_keypoints\"]\n            else:\n                keypoints = None\n\n            if \"pred_keypoint_heatmaps\" in pred:\n                keypoint_heatmaps = pred[\"pred_keypoint_heatmaps\"]\n            else:\n                keypoint_heatmaps = None\n\n            if \"pred_keypoint_scores\" in pred:\n                keypoint_scores = pred[\"pred_keypoint_scores\"]\n            else:\n                keypoint_scores = None\n\n            labels = _create_text_labels(classes, scores, self.metadata.thing_classes)\n\n            colors = None\n            if self._instance_mode == ColorMode.SEGMENTATION and"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if self._instance_mode == ColorMode.IMAGE_BW:\n            self.output.reset_image(self._create_grayscale_image())\n\n        if predictions.has(\"pred_masks\"):\n            if self._instance_mode == ColorMode.IMAGE_BW:\n                masks = predictions.pred_masks\n                if isinstance(masks, torch.Tensor):\n                    masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n            else:\n                masks = predictions.pred_masks\n        elif predictions.has(\"pred_masks_rle\"):\n            masks = predictions.pred_masks_rle\n        else:\n            masks = None\n\n        if predictions.has(\"pred_keypoints\"):\n            keypoints = predictions.pred_keypoints\n        else:\n            keypoints = None\n\n        if predictions.has(\"pred_boxes\"):\n            boxes = predictions.pred_boxes\n        elif predictions.has(\"pred_masks\"):\n            boxes = masks\n        else:\n            boxes = None\n\n        if predictions.has(\"scores\"):\n            scores = predictions.scores\n        else:\n            scores = None\n\n        if predictions.has(\"pred_classes\"):\n            classes = predictions.pred_classes\n        else:\n            classes = None\n\n        if predictions.has(\"pred_keypoints\"):\n            keypoints = predictions.pred_keypoints\n        else:\n            keypoints = None\n\n        if predictions.has(\"pred_boxes\"):\n            boxes = predictions.pred_boxes\n        elif predictions.has(\"pred_masks\"):\n            boxes = masks\n        else:\n            boxes = None\n\n        if predictions.has(\"pred_classes\"):\n            classes = predictions.pred_classes\n        else:\n            classes = None\n\n        if predictions.has(\"pred_classes\"):\n            classes = predictions.pred_classes\n        else:\n            classes = None\n\n        if predictions.has(\""}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if predictions.has(\"pred_masks\"):\n            if self._instance_mode == ColorMode.IMAGE_BW:\n                self.output.reset_image(self._create_grayscale_image(predictions.pred_masks))\n            self.draw_instance_predictions(predictions)\n\n        if predictions.has(\"pred_boxes\"):\n            self.draw_instance_predictions(predictions)\n\n        if predictions.has(\"pred_keypoints\"):\n            self.draw_instance_predictions(predictions)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if isinstance(predictions, torch.Tensor):\n            predictions = predictions.to(self.cpu_device)\n\n        if \"pred_masks\" in predictions:\n            masks = predictions[\"pred_masks\"]\n            masks = masks[:, None] if masks.dim() == 2 else masks\n            masks = [x for x in masks]\n        else:\n            masks = None\n\n        if \"pred_masks_rle\" in predictions:\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in predictions[\"pred_masks_rle\"]]\n\n        if \"pred_boxes\" in predictions:\n            boxes = predictions[\"pred_boxes\"].tensor\n            boxes = boxes[:, None] if boxes.dim() == 2 else boxes\n            boxes = [x for x in boxes]\n        else:\n            boxes = None\n\n        if \"pred_classes\" in predictions:\n            classes = predictions[\"pred_classes\"]\n            classes = classes[:, None] if classes.dim() == 1 else classes\n            classes = [x for x in classes]\n        else:\n            classes = None\n\n        if \"scores\" in predictions:\n            scores = predictions[\"scores\"]\n            scores = scores[:, None] if scores.dim() == 1 else scores\n            scores = [x for x in scores]\n        else:\n            scores = None\n\n        if \"pred_keypoints\" in predictions:\n            keypoints = predictions[\"pred_keypoints\"]\n            keypoints = keypoints[:, None] if keypoints.dim() == 2 else keypoints\n            keypoints = [x for x in keypoints]\n        else:\n            keypoints = None\n\n        labels = _create_text_labels(classes, scores, self.metadata.thing_classes)\n\n        self.overlay_instances(\n            boxes=boxes,\n            masks=masks,\n            labels=labels,\n            assigned_colors=None,\n            keypoints=keypoints,"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if isinstance(predictions, torch.Tensor):\n            predictions = predictions.to(self.cpu_device)\n        pred_boxes = predictions.pred_boxes\n        pred_classes = predictions.pred_classes\n        scores = predictions.scores\n        pred_masks = predictions.pred_masks\n        pred_keypoints = predictions.pred_keypoints\n\n        if self._instance_mode == ColorMode.IMAGE_BW:\n            self.output.reset_image(self._create_grayscale_image(predictions.pred_masks.any(dim=0)))\n\n        if self._instance_mode == ColorMode.IMAGE:\n            instances_found = True\n            colors = None\n        elif self._instance_mode == ColorMode.SEM_SEG:\n            instances_found = False\n            colors = None\n        else:\n            instances_found = True\n            if self.metadata.get(\"thing_colors\"):\n                colors = [\n                    self._jitter([x / 255 for x in self.metadata.thing_colors[c]])\n                    for c in pred_classes.tolist()\n                ]\n            else:\n                colors = None\n\n        if instances_found:\n            self.overlay_instances(\n                boxes=pred_boxes,\n                labels=pred_classes,\n                masks=pred_masks,\n                keypoints=pred_keypoints,\n                assigned_colors=colors,\n                scores=scores,\n            )\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if predictions.has(\"pred_masks\"):\n            if self._instance_mode == ColorMode.IMAGE_BW:\n                self.output.reset_image(self._create_grayscale_image(predictions.pred_masks))\n\n        if predictions.has(\"pred_masks_rle\"):\n            # use RLE to draw masks\n            for mask, sinfo in zip(predictions.pred_masks_rle, predictions.pred_masks_metadata):\n                self.draw_binary_mask(\n                    mask,\n                    color=sinfo[\"color\"],\n                    edge_color=_OFF_WHITE,\n                    text=sinfo[\"label\"],\n                    alpha=0.5,\n                    area_threshold=0,\n                )\n        else:\n            # use bitmasks to draw masks\n            for mask, sinfo in zip(predictions.pred_masks, predictions.pred_masks_metadata):\n                self.draw_binary_mask(\n                    mask,\n                    color=sinfo[\"color\"],\n                    edge_color=_OFF_WHITE,\n                    text=sinfo[\"label\"],\n                    alpha=0.5,\n                    area_threshold=0,\n                )\n\n        if predictions.has(\"pred_boxes\"):\n            self.overlay_instances(\n                boxes=predictions.pred_boxes,\n                labels=predictions.pred_classes,\n                assigned_colors=predictions.pred_colors,\n            )\n\n        if predictions.has(\"pred_keypoints\"):\n            self.overlay_instances(keypoints=predictions.pred_keypoints)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if self._instance_mode == ColorMode.IMAGE_BW:\n            self.output.reset_image(self._create_grayscale_image())\n\n        if \"pred_boxes\" in predictions:\n            boxes = predictions[\"pred_boxes\"].tensor.numpy()\n            scores = predictions[\"scores\"].tolist()\n            classes = predictions[\"pred_classes\"].tolist()\n            labels = _create_text_labels(classes, scores, self.metadata.thing_classes)\n            self.overlay_instances(boxes=boxes, labels=labels)\n\n        if \"pred_masks\" in predictions:\n            masks = predictions[\"pred_masks\"].numpy()\n            if \"pred_boxes\" in predictions:\n                boxes = predictions[\"pred_boxes\"].tensor.numpy()\n                masks = [masks[i, ..., None] for i, b in enumerate(boxes) if masks[i, b[0], b[1]].any()]\n            self.overlay_instances(masks=masks)\n\n        if \"pred_masks_rle\" in predictions:\n            masks = [\n                GenericMask(x, self.output.height, self.output.width) for x in predictions[\"pred_masks_rle\"]\n            ]\n            self.overlay_instances(masks=masks)\n\n        if \"pred_keypoints\" in predictions:\n            keypoints = predictions[\"pred_keypoints\"].tensor.cpu()\n            keypoints = keypoints.reshape(-1, len(keypoints), 3)\n            self.overlay_instances(keypoints=keypoints)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if isinstance(predictions, dict):\n            predictions = [predictions]\n\n        for prediction in predictions:\n            if \"instances\" in prediction:\n                instances = prediction[\"instances\"]\n                boxes = instances.pred_boxes if instances.has(\"pred_boxes\") else None\n                scores = instances.scores if instances.has(\"scores\") else None\n                classes = instances.pred_classes if instances.has(\"pred_classes\") else None\n                keypoints = instances.pred_keypoints if instances.has(\"pred_keypoints\") else None\n                labels = _create_text_labels(\n                    classes, scores, self.metadata.thing_classes, instances.get(\"is_crowd\")\n                )\n                masks = instances.pred_masks if instances.has(\"pred_masks\") else None\n                self.overlay_instances(\n                    boxes=boxes,\n                    labels=labels,\n                    masks=masks,\n                    keypoints=keypoints,\n                    assigned_colors=None,\n                )\n            if \"proposals\" in prediction:\n                proposals = prediction[\"proposals\"]\n                boxes = proposals.proposal_boxes if proposals.has(\"proposal_boxes\") else None\n                scores = proposals.objectness_logits if proposals.has(\"objectness_logits\") else None\n                classes = proposals.pred_classes if proposals.has(\"pred_classes\") else None\n                labels = _create_text_labels(\n                    classes, scores, self.metadata.thing_classes, proposals.get(\"is_crowd\")\n                )\n                self.overlay_instances(\n                    boxes=boxes, labels=labels, assigned_colors=None, alpha=0.5\n                )\n        return self.output\n\n    "}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if self._instance_mode == ColorMode.IMAGE_BW:\n            self.output.reset_image(self._create_grayscale_image())\n\n        if isinstance(predictions, dict):\n            predictions = [predictions]\n        if not isinstance(predictions, list):\n            raise ValueError(\"predictions must be a list of dicts or a single dict\")\n\n        for pred in predictions:\n            if \"instances\" in pred:\n                boxes = pred[\"instances\"].pred_boxes\n                scores = pred[\"instances\"].scores\n                classes = pred[\"instances\"].pred_classes\n                masks = pred[\"instances\"].pred_masks\n                if \"keypoints\" in pred[\"instances\"]:\n                    keypts = pred[\"instances\"].keypoints\n                else:\n                    keypts = None\n                labels = _create_text_labels(classes, scores, self.metadata.thing_classes)\n                self.overlay_instances(\n                    boxes=boxes,\n                    labels=labels,\n                    masks=masks,\n                    keypoints=keypts,\n                    assigned_colors=pred[\"instances\"].get(\"colors\", None),\n                )\n            if \"proposals\" in pred:\n                self.overlay_instances(\n                    boxes=pred[\"proposals\"].proposal_boxes,\n                    assigned_colors=pred[\"proposals\"].get(\"colors\", None),\n                )\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # convert boxes from XYXY_ABS / XYWH_ABS / XYXY_REL / XYWH_REL to XYXY_ABS / XYXY_REL\n        if isinstance(predictions, dict):\n            boxes = predictions[\"pred_boxes\"]\n            boxes = self._convert_boxes(boxes)\n            scores = predictions[\"scores\"]\n            labels = predictions[\"pred_classes\"]\n            if \"pred_masks\" in predictions:\n                masks = predictions[\"pred_masks\"]\n            elif \"pred_masks_rle\" in predictions:\n                masks = predictions[\"pred_masks_rle\"]\n            else:\n                masks = None\n            if \"pred_keypoints\" in predictions:\n                keypts = predictions[\"pred_keypoints\"]\n            else:\n                keypts = None\n        elif isinstance(predictions, torch.Tensor):\n            boxes = predictions[\"pred_boxes\"]\n            boxes = self._convert_boxes(boxes)\n            scores = predictions[\"scores\"]\n            labels = predictions[\"pred_classes\"]\n            if \"pred_masks\" in predictions:\n                masks = predictions[\"pred_masks\"]\n            elif \"pred_masks_rle\" in predictions:\n                masks = predictions[\"pred_masks_rle\"]\n            else:\n                masks = None\n            if \"pred_keypoints\" in predictions:\n                keypts = predictions[\"pred_keypoints\"]\n            else:\n                keypts = None\n        else:\n            raise TypeError(\n                \"predictions in draw_instance_predictions() should be either a \"\n                \"dict or a tensor; got {}\".format(type(predictions))\n            )\n\n        if self._instance_mode == ColorMode.IMAGE_BW:\n            self.output.reset_image(self._create_grayscale_image(masks))\n\n        if self._instance_mode == ColorMode.IMAGE:\n            self.overlay_instances(boxes=boxes, labels=labels, masks=mas"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if predictions.has(\"pred_masks\"):\n            if self._instance_mode == ColorMode.IMAGE_BW:\n                self.output.reset_image(self._create_grayscale_image(predictions.pred_masks))\n            self.draw_instance_predictions(predictions)\n        elif predictions.has(\"pred_boxes\"):\n            self.draw_instance_predictions(predictions)\n        elif predictions.has(\"pred_keypoints\"):\n            self.draw_keypoints(predictions.pred_keypoints, connections=self.metadata.keypoint_connections)\n        else:\n            raise NotImplementedError(\"No supported prediction type found\")\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if predictions.has(\"pred_masks\"):\n            masks = predictions.pred_masks\n            if isinstance(masks, BitMasks):\n                masks = masks.tensor.numpy()\n            elif isinstance(masks, torch.Tensor):\n                masks = masks.numpy()\n            elif isinstance(masks, list):\n                masks = np.asarray(masks)\n            else:\n                raise ValueError(\n                    \"predictions.pred_masks should be of type BitMasks, torch.Tensor, \"\n                    \"or list of ndarray, but got {}\".format(type(masks))\n                )\n\n            if self._instance_mode == ColorMode.IMAGE_BW:\n                self.output.reset_image(self._create_grayscale_image(masks))\n\n            masks = masks.transpose(0, 2, 3, 1)\n            for mask in masks:\n                self.draw_binary_mask(mask, color=_RED, alpha=0.5)\n\n        if predictions.has(\"pred_keypoints\"):\n            keypoints = predictions.pred_keypoints\n            if isinstance(keypoints, torch.Tensor):\n                keypoints = keypoints.cpu().numpy()\n            keypoints = keypoints.reshape(-1, len(keypoints), 3)\n            self.overlay_instances(keypoints=keypoints)\n\n        if predictions.has(\"pred_boxes\"):\n            boxes = predictions.pred_boxes\n            if isinstance(boxes, Boxes):\n                boxes = boxes.tensor.cpu().numpy()\n            elif isinstance(boxes, RotatedBoxes):\n                boxes = boxes.tensor.cpu().numpy()\n            elif isinstance(boxes, torch.Tensor):\n                boxes = boxes.cpu().numpy()\n            else:\n                boxes = np.asarray(boxes)\n            self.overlay_instances"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        self.fig.canvas.draw()\n        data = np.frombuffer(self.fig.canvas.tostring_rgb(), dtype=np.uint8)\n        data = data.reshape(self.height, self.width, 3)\n        return data"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert from RGBA to RGB\n        img = self.canvas.buffer_rgba()\n        img = np.array(img)\n        img = img[:, :, :3]\n        return img"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert the image from RGBA to RGB format and return it as a numpy ndarray of uint8 type.\n        self.fig.canvas.draw()\n        image = np.frombuffer(self.fig.canvas.tostring_rgb(), dtype=\"uint8\")\n        image = image.reshape(self.height, self.width, 3)\n        return image"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        self.fig.canvas.draw()\n        buf = self.fig.canvas.buffer_rgba()\n        img = np.array(buf)\n        return img"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        self.fig.canvas.draw()\n        image = np.array(self.fig.canvas.renderer._renderer)\n        return image"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert the figure to a numpy array\n        self.fig.canvas.draw()\n        img = np.frombuffer(self.fig.canvas.tostring_rgb(), dtype=np.uint8)\n        img = img.reshape(self.height, self.width, 3)\n        return img"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert the image from RGBA to RGB format\n        img_rgba = self.canvas.buffer_rgba()\n        img_rgb = np.frombuffer(img_rgba, dtype=np.uint8).reshape(self.height, self.width, 4)\n        img_rgb = img_rgb[:, :, :3]\n        return img_rgb"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        self.fig.canvas.draw()\n        image = np.array(self.fig.canvas.renderer._renderer)\n        return image"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert from RGBA to RGB format.\n        img = self.canvas.buffer_rgba()\n        img = np.array(img)\n        img = img[:, :, :3]\n        return img"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # self.fig.canvas.draw()\n        # buf = self.fig.canvas.buffer_rgba()\n        # arr = np.array(buf)\n        # return arr\n        self.fig.canvas.draw()\n        buf = self.fig.canvas.buffer_rgba()\n        arr = np.array(buf)\n        return arr"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert the image from RGBA to RGB format\n        self.fig.canvas.draw()\n        image = np.frombuffer(self.fig.canvas.tostring_rgb(), dtype='uint8')\n        image = image.reshape(self.height, self.width, 3)\n        return image"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert from RGBA to RGB format\n        self.fig.canvas.draw()\n        buf = self.fig.canvas.buffer_rgba()\n        image = np.array(buf).astype(np.uint8)\n        image = image[..., :3]\n        return image"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        self.fig.canvas.draw()\n        canvas = self.fig.canvas\n        width, height = canvas.get_width_height()\n        img = np.frombuffer(canvas.tostring_rgb(), dtype=\"uint8\")\n        img = img.reshape((height, width, 3))\n        return img"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert the image to RGBA format\n        img_rgba = self.canvas.buffer_rgba()\n        # Convert the image to a numpy array\n        img_array = np.asarray(img_rgba)\n        # Convert the image from RGBA to RGB format\n        img_rgb = img_array[:, :, :3]\n        return img_rgb"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert the canvas to a numpy ndarray\n        self.fig.canvas.draw()\n        image = np.frombuffer(self.fig.canvas.tostring_rgb(), dtype=\"uint8\")\n        image = image.reshape(self.fig.canvas.get_width_height()[::-1] + (3,))\n        return image"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # self.fig.canvas.draw()\n        # buf = self.fig.canvas.tostring_rgb()\n        # ncolors = int(buf.size/self.height/self.width/3)\n        # X = np.frombuffer(buf, dtype='uint8')\n        # X = X.reshape(self.height, self.width, 3)\n        # return X\n\n        self.fig.canvas.draw()\n        buf = self.fig.canvas.tostring_rgb()\n        ncolors = int(buf.size / self.height / self.width / 3)\n        X = np.frombuffer(buf, dtype=\"uint8\")\n        X = X.reshape(self.height, self.width, 3)\n        return X"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert the image from RGBA to RGB format, and convert it to a numpy array of uint8 type.\n        # The image is then returned.\n        self.fig.canvas.draw()\n        image = np.frombuffer(self.fig.canvas.tostring_rgb(), dtype=\"uint8\")\n        image = image.reshape(self.height, self.width, 3)\n        return image"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        fig = self.fig\n        fig.canvas.draw()\n        # Get the RGBA buffer from the figure\n        w, h = fig.canvas.get_width_height()\n        buf = np.array(fig.canvas.renderer._renderer)\n        # buf is (h, w, 4)\n        buf = np.transpose(buf, axes=(1, 0, 2))\n        buf = buf[:, :, :3]\n        return buf"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert the image to RGBA format and retrieve the data as a NumPy array.\n        self.fig.canvas.draw()\n        rgba = np.array(self.fig.canvas.renderer._renderer)\n\n        # Convert the RGBA image to RGB format and return it.\n        return rgba[:, :, :3].astype(np.uint8)"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert the image from RGBA to RGB format, and convert it to a numpy ndarray of uint8 type.\n        # The image is expected to be in RGB format, so we convert it to RGBA to ensure that the\n        # alpha channel is preserved.\n        img_rgba = self.ax.get_images()[0]\n        img_rgba = img_rgba.get_array()\n        img_rgba = img_rgba[:, :, :3]\n        img_rgba = np.array(img_rgba, dtype=np.uint8)\n        return img_rgba"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        annotations = dic.get(\"annotations\", None)\n        if annotations is not None:\n            self.draw_instance_predictions(annotations)\n\n        panoptic_seg = dic.get(\"panoptic_seg\", None)\n        if panoptic_seg is not None:\n            segments_info = dic.get(\"segments_info\", None)\n            self.draw_panoptic_seg(panoptic_seg, segments_info)\n\n        sem_seg = dic.get(\"sem_seg\", None)\n        if sem_seg is not None:\n            self.draw_sem_seg(sem_seg)\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            self.draw_instance_predictions(dic[\"annotations\"])\n        if \"sem_seg_file_name\" in dic:\n            with PathManager.open(dic[\"sem_seg_file_name\"], \"rb\") as f:\n                sem_seg = Image.open(f)\n                sem_seg = np.asarray(sem_seg, dtype=np.uint8)\n            self.draw_sem_seg(sem_seg)\n        if \"panoptic_seg_file_name\" in dic:\n            with PathManager.open(dic[\"panoptic_seg_file_name\"], \"rb\") as f:\n                panoptic_seg = Image.open(f)\n                panoptic_seg = np.asarray(panoptic_seg, dtype=np.uint8)\n            self.draw_panoptic_seg(panoptic_seg, dic.get(\"segments_info\"))\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            annotations = dic[\"annotations\"]\n            if \"segmentation\" in annotations:\n                self.draw_sem_seg(annotations[\"segmentation\"])\n            if \"keypoints\" in annotations:\n                self.draw_instance_predictions(annotations[\"keypoints\"])\n            if \"bbox\" in annotations:\n                self.draw_instance_predictions(annotations[\"bbox\"])\n\n        if \"sem_seg_file_name\" in dic:\n            with PathManager.open(dic[\"sem_seg_file_name\"], \"rb\") as f:\n                sem_seg = Image.open(f)\n                sem_seg.load()\n                sem_seg = np.asarray(sem_seg, dtype=\"uint8\")\n            self.draw_sem_seg(sem_seg)\n\n        if \"panoptic_seg_file_name\" in dic:\n            with PathManager.open(dic[\"panoptic_seg_file_name\"], \"rb\") as f:\n                panoptic_seg = Image.open(f)\n                panoptic_seg.load()\n                panoptic_seg = np.asarray(panoptic_seg, dtype=\"uint32\")\n            segments_info = dic[\"segments_info\"]\n            self.draw_panoptic_seg(panoptic_seg, segments_info)\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # Draw annotations\n        if \"annotations\" in dic:\n            self.draw_instance_predictions(dic[\"annotations\"])\n\n        # Draw semantic segmentation\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"])\n\n        # Draw panoptic segmentation\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(dic[\"panoptic_seg\"], dic[\"segments_info\"])\n\n        # Draw bounding boxes\n        if \"instances\" in dic:\n            self.draw_instance_predictions(dic[\"instances\"])\n\n        # Draw keypoints\n        if \"instances\" in dic:\n            self.draw_instance_predictions(dic[\"instances\"])\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            annotations = dic[\"annotations\"]\n            if annotations:\n                if \"segmentation\" in annotations[0]:\n                    self.draw_panoptic_seg(\n                        dic[\"pan_seg\"],\n                        dic[\"seg_info\"],\n                        area_threshold=_LARGE_MASK_AREA_THRESH,\n                        alpha=0.5,\n                    )\n                else:\n                    self.draw_instance_predictions(annotations)\n\n        if \"sem_seg_file_name\" in dic:\n            with PathManager.open(dic[\"sem_seg_file_name\"], \"rb\") as f:\n                sem_seg = Image.open(f)\n                sem_seg = np.asarray(sem_seg, dtype=np.uint8)\n            self.draw_sem_seg(sem_seg, area_threshold=_LARGE_MASK_AREA_THRESH, alpha=0.5)\n\n        if \"stuff_seg_file_name\" in dic:\n            with PathManager.open(dic[\"stuff_seg_file_name\"], \"rb\") as f:\n                stuff_seg = Image.open(f)\n                stuff_seg = np.asarray(stuff_seg, dtype=np.uint8)\n            self.draw_sem_seg(stuff_seg, area_threshold=_LARGE_MASK_AREA_THRESH, alpha=0.5)\n\n        if \"pan_seg_file_name\" in dic:\n            with PathManager.open(dic[\"pan_seg_file_name\"], \"rb\") as f:\n                pan_seg = Image.open(f)\n                pan_seg = np.asarray(pan_seg, dtype=np.uint8)\n            self.draw_panoptic_seg(pan_seg, dic[\"seg_info\"], area_threshold=_LARGE_MASK_AREA_THRESH)\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            annos = dic[\"annotations\"]\n            if \"segmentation\" in annos:\n                self.draw_sem_seg(annos[\"segmentation\"], alpha=0.5)\n            if \"keypoints\" in annos:\n                self.draw_and_connect_keypoints(annos[\"keypoints\"])\n            if \"bbox\" in annos:\n                self.draw_box(annos[\"bbox\"])\n\n        if \"sem_seg_file_name\" in dic:\n            with PathManager.open(dic[\"sem_seg_file_name\"], \"rb\") as f:\n                sem_seg = Image.open(f)\n                sem_seg = np.asarray(sem_seg, dtype=\"uint8\")\n            self.draw_sem_seg(sem_seg, alpha=0.5)\n\n        if \"panoptic_seg_file_name\" in dic:\n            with PathManager.open(dic[\"panoptic_seg_file_name\"], \"rb\") as f:\n                panoptic_seg = Image.open(f)\n                panoptic_seg = np.asarray(panoptic_seg, dtype=\"uint32\")\n            segments_info = dic[\"segments_info\"]\n            self.draw_panoptic_seg(panoptic_seg, segments_info, alpha=0.5)\n\n        if \"instances_file_name\" in dic:\n            with PathManager.open(dic[\"instances_file_name\"], \"rb\") as f:\n                instances = torch.load(f, map_location=self.cpu_device)\n            self.draw_instance_predictions(instances)\n\n        if \"pred_boxes\" in dic:\n            self.draw_instance_predictions(dic)\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            self.draw_instance_predictions(dic[\"annotations\"])\n        if \"sem_seg_file_name\" in dic:\n            with PathManager.open(dic[\"sem_seg_file_name\"], \"rb\") as f:\n                sem_seg = Image.open(f)\n                sem_seg = np.asarray(sem_seg, dtype=\"uint8\")\n                self.draw_sem_seg(sem_seg)\n        if \"panoptic_seg_file_name\" in dic:\n            with PathManager.open(dic[\"panoptic_seg_file_name\"], \"rb\") as f:\n                panoptic_seg = Image.open(f)\n                panoptic_seg = np.asarray(panoptic_seg, dtype=\"uint8\")\n                segments_info = dic.get(\"segments_info\", None)\n                self.draw_panoptic_seg(panoptic_seg, segments_info)\n        if \"pred_masks\" in dic:\n            self.draw_instance_predictions(dic)\n        if \"pred_keypoints\" in dic:\n            self.draw_instance_predictions(dic)\n        if \"pred_boxes\" in dic:\n            self.draw_instance_predictions(dic)\n        if \"pred_classes\" in dic:\n            self.draw_instance_predictions(dic)\n        if \"scores\" in dic:\n            self.draw_instance_predictions(dic)\n        if \"pred_masks_rle\" in dic:\n            self.draw_instance_predictions(dic)\n        if \"pred_masks_bitmask\" in dic:\n            self.draw_instance_predictions(dic)\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # TODO: support more annotation types.\n        # TODO: support drawing segmentation masks\n        # TODO: support drawing keypoints\n        # TODO: support drawing bounding boxes\n        # TODO: support drawing instance segmentation\n        # TODO: support drawing semantic segmentation\n        # TODO: support drawing panoptic segmentation\n        # TODO: support drawing keypoints\n        # TODO: support drawing bounding boxes\n        # TODO: support drawing instance segmentation\n        # TODO: support drawing semantic segmentation\n        # TODO: support drawing panoptic segmentation\n        # TODO: support drawing keypoints\n        # TODO: support drawing bounding boxes\n        # TODO: support drawing instance segmentation\n        # TODO: support drawing semantic segmentation\n        # TODO: support drawing panoptic segmentation\n        # TODO: support drawing keypoints\n        # TODO: support drawing bounding boxes\n        # TODO: support drawing instance segmentation\n        # TODO: support drawing semantic segmentation\n        # TODO: support drawing panoptic segmentation\n        # TODO: support drawing keypoints\n        # TODO: support drawing bounding boxes\n        # TODO: support drawing instance segmentation\n        # TODO: support drawing semantic segmentation\n        # TODO: support drawing panoptic segmentation\n        # TODO: support drawing keypoints\n        # TODO: support drawing bounding boxes\n        # TODO: support drawing instance segmentation\n        # TODO: support drawing semantic segmentation\n        # TODO: support drawing panoptic segmentation\n        # TODO: support drawing keypoints\n        # TODO: support drawing bounding boxes\n        # TODO: support drawing instance segmentation\n        # TODO: support drawing semantic segmentation\n        # TODO: support drawing panoptic segmentation\n        # TODO: support drawing keypoints\n        # TODO: support drawing bounding boxes\n        # TODO: support drawing instance segmentation\n        # TODO: support drawing semantic segmentation\n        # TODO: support drawing panoptic segmentation\n        # TODO: support drawing keypoints\n        # TODO: support drawing bounding boxes\n        # TODO: support drawing instance segmentation\n        # TODO: support drawing semantic segmentation\n        # TODO: support drawing panoptic segmentation\n        # TODO: support drawing keypoints\n        # TODO: support drawing bounding boxes\n        # TODO: support drawing instance segmentation"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # Draw segmentation masks\n        if \"sem_masks\" in dic:\n            sem_masks = dic[\"sem_masks\"]\n            if isinstance(sem_masks, torch.Tensor):\n                sem_masks = sem_masks.numpy()\n            if isinstance(sem_masks, list):\n                for sem_mask in sem_masks:\n                    self.draw_binary_mask(sem_mask)\n            else:\n                self.draw_binary_mask(sem_masks)\n\n        # Draw instance masks\n        if \"instances\" in dic:\n            instances = dic[\"instances\"]\n            if instances.has(\"pred_masks\"):\n                masks = instances.pred_masks.cpu()\n                if isinstance(masks, torch.Tensor):\n                    masks = masks.numpy()\n                if isinstance(masks, list):\n                    for mask in masks:\n                        self.draw_binary_mask(mask)\n                else:\n                    self.draw_binary_mask(masks)\n\n        # Draw keypoints\n        if \"instances\" in dic:\n            instances = dic[\"instances\"]\n            if instances.has(\"pred_keypoints\"):\n                keypoints = instances.pred_keypoints.cpu()\n                if isinstance(keypoints, torch.Tensor):\n                    keypoints = keypoints.numpy()\n                self.draw_and_connect_keypoints(keypoints)\n\n        # Draw bounding boxes\n        if \"instances\" in dic:\n            instances = dic[\"instances\"]\n            if instances.has(\"pred_boxes\"):\n                boxes = instances.pred_boxes.tensor.cpu()\n                if isinstance(boxes, torch.Tensor):\n                    boxes = boxes.numpy()\n                self.overlay_instances(boxes=boxes)\n\n        # Draw rotated bounding boxes\n        if \"instances\" in dic:\n            instances = dic[\"instances\"]\n            if instances.has(\"pred"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # Drawing semantic segmentation\n        if dic.get(\"sem_seg\") is not None:\n            self.draw_sem_seg(dic[\"sem_seg\"])\n\n        # Drawing instance segmentation\n        if dic.get(\"instances\") is not None:\n            self.draw_instance_predictions(dic[\"instances\"])\n\n        # Drawing panoptic segmentation\n        if dic.get(\"panoptic_seg\") is not None:\n            self.draw_panoptic_seg(\n                dic[\"panoptic_seg\"],\n                dic[\"panoptic_seg_info\"],\n            )\n\n        # Drawing bounding boxes\n        if dic.get(\"pred_boxes\") is not None:\n            self.draw_box(dic[\"pred_boxes\"])\n\n        # Drawing keypoints\n        if dic.get(\"pred_keypoints\") is not None:\n            self.draw_and_connect_keypoints(dic[\"pred_keypoints\"])\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            annotations = dic[\"annotations\"]\n            if \"segmentation\" in annotations:\n                self.draw_sem_seg(annotations[\"segmentation\"])\n            if \"keypoints\" in annotations:\n                self.draw_and_connect_keypoints(annotations[\"keypoints\"])\n            if \"bbox\" in annotations:\n                self.draw_box(annotations[\"bbox\"])\n            if \"category_id\" in annotations:\n                self.draw_text(\n                    self.metadata.thing_classes[annotations[\"category_id\"]],\n                    annotations[\"bbox\"][:2],\n                )\n\n        if \"sem_seg_file_name\" in dic:\n            with PathManager.open(dic[\"sem_seg_file_name\"], \"rb\") as f:\n                sem_seg = np.asarray(Image.open(f), dtype=np.uint8)\n            self.draw_sem_seg(sem_seg)\n\n        if \"panoptic_seg_file_name\" in dic:\n            with PathManager.open(dic[\"panoptic_seg_file_name\"], \"rb\") as f:\n                panoptic_seg = np.asarray(Image.open(f), dtype=np.uint8)\n            segments_info = dic.get(\"segments_info\", None)\n            self.draw_panoptic_seg(panoptic_seg, segments_info)\n\n        if \"file_name\" in dic:\n            file_name = dic[\"file_name\"]\n            if not PathManager.exists(file_name):\n                file_name = PathManager.get_local_path(file_name)\n            with PathManager.open(file_name, \"rb\") as f:\n                img = np.asarray(Image.open(f), dtype=np.uint8)\n            self.output.reset_image(img)\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # Draw bounding boxes\n        if \"annotations\" in dic:\n            for ann in dic[\"annotations\"]:\n                if ann.get(\"iscrowd\", 0) or ann[\"category_id\"] not in self.metadata.thing_dataset_id_to_contiguous_id:\n                    continue\n                self.draw_box(ann[\"bbox\"], edge_color=_RED)\n\n        # Draw segmentation masks\n        if \"sem_seg_file_name\" in dic:\n            with PathManager.open(dic[\"sem_seg_file_name\"], \"rb\") as f:\n                sem_seg = Image.open(f)\n                sem_seg = np.asarray(sem_seg, dtype=\"uint8\")\n            self.draw_sem_seg(sem_seg)\n\n        # Draw instance masks\n        if \"pred_masks\" in dic:\n            masks = dic[\"pred_masks\"]\n            if isinstance(masks, torch.Tensor):\n                masks = masks.numpy()\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n            self.overlay_instances(masks=masks)\n\n        # Draw keypoints\n        if \"keypoints\" in dic:\n            keypoints = dic[\"keypoints\"]\n            if isinstance(keypoints, torch.Tensor):\n                keypoints = keypoints.numpy()\n            self.draw_and_connect_keypoints(keypoints)\n\n        # Draw panoptic segmentation\n        if \"panoptic_seg_file_name\" in dic:\n            with PathManager.open(dic[\"panoptic_seg_file_name\"], \"rb\") as f:\n                panoptic_seg = Image.open(f)\n                panoptic_seg = np.asarray(panoptic_seg, dtype=\"uint8\")\n            if \"segments_info\" in dic:\n                segments_info = dic[\"segments_info\"]\n            else:"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            annos = dic[\"annotations\"]\n            if \"segmentation\" in annos:\n                self.draw_sem_seg(dic[\"sem_seg\"].squeeze(0).numpy())\n            if \"keypoints\" in annos:\n                self.draw_instance_predictions(dic[\"instances\"])\n        elif \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"].squeeze(0).numpy())\n        elif \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(dic[\"panoptic_seg\"].squeeze(0).numpy(), dic[\"segments_info\"])\n        elif \"instances\" in dic:\n            self.draw_instance_predictions(dic[\"instances\"])\n        else:\n            raise ValueError(\"The input dictionary does not contain any annotations.\")\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if dic[\"file_name\"] is not None:\n            self.output.reset_image(dic[\"file_name\"])\n\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"])\n\n        if \"pred_masks\" in dic:\n            self.overlay_instances(\n                masks=dic[\"pred_masks\"],\n                labels=dic[\"pred_classes\"],\n                assigned_colors=[\n                    self._jitter([x / 255 for x in self.metadata.thing_colors[c]])\n                    for c in dic[\"pred_classes\"]\n                ],\n            )\n\n        if \"pred_boxes\" in dic:\n            self.overlay_instances(\n                boxes=dic[\"pred_boxes\"],\n                labels=dic[\"pred_classes\"],\n                assigned_colors=[\n                    self._jitter([x / 255 for x in self.metadata.thing_colors[c]])\n                    for c in dic[\"pred_classes\"]\n                ],\n            )\n\n        if \"pred_keypoints\" in dic:\n            self.overlay_instances(keypoints=dic[\"pred_keypoints\"])\n\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(dic[\"panoptic_seg\"], dic[\"segments_info\"])\n\n        if \"gt_masks\" in dic:\n            self.overlay_instances(\n                masks=dic[\"gt_masks\"],\n                labels=dic[\"gt_classes\"],\n                assigned_colors=[\n                    self._jitter([x / 255 for x in self.metadata.thing_colors[c]])\n                    for c in dic[\"gt_classes\"]\n                ],\n            )\n\n        if \"gt_boxes\" in dic:\n            self.overlay_instances(\n                boxes=dic[\"gt_boxes\"],\n                labels=dic[\"gt_classes\"],\n                assigned_colors=[\n                    self._jitter([x / 255 for"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            annotations = dic[\"annotations\"]\n            if annotations:\n                if \"segmentation\" in annotations[0]:\n                    self.draw_panoptic_seg(\n                        dic[\"sem_seg\"], segments_info=dic[\"seg_info\"], area_threshold=100\n                    )\n                else:\n                    self.overlay_instances(\n                        boxes=annotations[\"bbox\"],\n                        labels=annotations[\"category_id\"],\n                        keypoints=annotations[\"keypoints\"],\n                    )\n        elif \"sem_seg_file_name\" in dic:\n            with PathManager.open(dic[\"sem_seg_file_name\"], \"rb\") as f:\n                sem_seg = Image.open(f)\n                sem_seg = np.asarray(sem_seg, dtype=\"uint8\")\n            self.draw_sem_seg(sem_seg, area_threshold=100)\n        elif \"panoptic_seg_file_name\" in dic:\n            with PathManager.open(dic[\"panoptic_seg_file_name\"], \"rb\") as f:\n                panoptic_seg = Image.open(f)\n                panoptic_seg = np.asarray(panoptic_seg, dtype=\"uint8\")\n            self.draw_panoptic_seg(panoptic_seg, segments_info=dic[\"seg_info\"], area_threshold=100)\n        else:\n            logger.warning(\"No annotations found in the dataset dict.\")\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if dic[\"file_name\"] is not None:\n            self.output.reset_image(dic[\"file_name\"])\n\n        if dic[\"annotations\"] is not None:\n            boxes = dic[\"annotations\"].get(\"bbox\", None)\n            labels = dic[\"annotations\"].get(\"category_id\", None)\n            labels = [self.metadata.thing_classes[label] for label in labels]\n            masks = dic[\"annotations\"].get(\"segmentation\", None)\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n            keypoints = dic[\"annotations\"].get(\"keypoints\", None)\n\n            if self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\n                colors = [\n                    self._jitter([x / 255 for x in self.metadata.thing_colors[c]])\n                    for c in labels\n                ]\n                alpha = 0.8\n            else:\n                colors = None\n                alpha = 0.5\n\n            if self._instance_mode == ColorMode.IMAGE_BW:\n                self.output.reset_image(\n                    self._create_grayscale_image(\n                        (dic[\"annotations\"][\"segmentation\"].any(dim=0) > 0).numpy()\n                    )\n                )\n                alpha = 0.3\n\n            self.overlay_instances(\n                masks=masks,\n                boxes=boxes,\n                labels=labels,\n                keypoints=keypoints,\n                assigned_colors=colors,\n                alpha=alpha,\n            )\n\n        if dic[\"sem_seg_file_name\"] is not None:\n            sem_seg = dic[\"sem_seg_file_name\"]\n            if isinstance(sem_seg, str):\n                with PathManager.open(sem_seg, \"rb\") as f:\n                    sem_seg = Image.open(f)\n                    sem_seg = np.asarray(sem_seg.convert(\"P\"))\n            self"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            annotations = dic[\"annotations\"]\n            if \"segmentation\" in annotations:\n                self.draw_sem_seg(annotations[\"segmentation\"])\n            if \"keypoints\" in annotations:\n                self.draw_keypoints(annotations[\"keypoints\"])\n            if \"bbox\" in annotations:\n                self.draw_box(annotations[\"bbox\"])\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"])\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(dic[\"panoptic_seg\"])\n        if \"instances\" in dic:\n            self.draw_instance_predictions(dic[\"instances\"])\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            annotations = dic[\"annotations\"]\n            if \"segmentation\" in annotations:\n                self.draw_sem_seg(dic[\"sem_seg\"], alpha=0.5)\n            if \"keypoints\" in annotations:\n                self.draw_keypoints(annotations[\"keypoints\"], connections=self.metadata.skeleton)\n            if \"bbox\" in annotations:\n                self.draw_box(annotations[\"bbox\"])\n        if \"sem_seg_file_name\" in dic:\n            with PathManager.open(dic[\"sem_seg_file_name\"], \"rb\") as f:\n                sem_seg = Image.open(f)\n                sem_seg.load()\n                sem_seg = np.asarray(sem_seg, dtype=\"uint8\")\n            self.draw_sem_seg(sem_seg, alpha=0.5)\n        if \"panoptic_seg_file_name\" in dic:\n            with PathManager.open(dic[\"panoptic_seg_file_name\"], \"rb\") as f:\n                panoptic_seg = Image.open(f)\n                panoptic_seg.load()\n                panoptic_seg = np.asarray(panoptic_seg, dtype=\"uint32\")\n            self.draw_panoptic_seg(panoptic_seg, area_threshold=0)\n        if \"panoptic_seg_annotations\" in dic:\n            segments_info = dic[\"panoptic_seg_annotations\"]\n            self.draw_panoptic_seg(dic[\"panoptic_seg\"], segments_info, area_threshold=0)\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            annotations = dic[\"annotations\"]\n            for ann in annotations:\n                if ann[\"iscrowd\"] == 0:\n                    bbox = ann[\"bbox\"]\n                    x1, y1, x2, y2 = bbox\n                    self.draw_box((x1, y1, x2, y2))\n                else:\n                    keypoints = ann[\"keypoints\"]\n                    self.draw_and_connect_keypoints(keypoints)\n\n        if \"sem_seg_file_name\" in dic:\n            sem_seg_file_name = dic[\"sem_seg_file_name\"]\n            with PathManager.open(sem_seg_file_name, \"rb\") as f:\n                sem_seg = Image.open(f)\n                sem_seg = np.array(sem_seg)\n            self.draw_sem_seg(sem_seg)\n\n        if \"panoptic_seg_file_name\" in dic:\n            panoptic_seg_file_name = dic[\"panoptic_seg_file_name\"]\n            with PathManager.open(panoptic_seg_file_name, \"rb\") as f:\n                panoptic_seg = Image.open(f)\n                panoptic_seg = np.array(panoptic_seg)\n            segments_info = dic[\"segments_info\"]\n            self.draw_panoptic_seg(panoptic_seg, segments_info)\n\n        if \"pred_masks\" in dic:\n            pred_masks = dic[\"pred_masks\"]\n            for mask in pred_masks:\n                self.draw_binary_mask(mask)\n\n        if \"pred_keypoints\" in dic:\n            pred_keypoints = dic[\"pred_keypoints\"]\n            self.draw_and_connect_keypoints(pred_keypoints)\n\n        if \"pred_boxes\" in dic:\n            pred_boxes = dic[\"pred_boxes\"]\n            for box in pred_boxes:\n                self."}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if dic[\"file_name\"].startswith(\"http\"):\n            file_name = dic[\"file_name\"]\n            with PathManager.open(file_name, \"rb\") as f:\n                img = Image.open(f)\n                img = np.array(img)\n        else:\n            img = self.img\n\n        self.output = VisImage(img, scale=self.output.scale)\n\n        # draw segmentation masks\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"], area_threshold=10)\n\n        # draw panoptic segmentation\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(dic[\"panoptic_seg\"], dic[\"panoptic_seg_info\"])\n\n        # draw instance predictions\n        if \"instances\" in dic:\n            self.draw_instance_predictions(dic[\"instances\"])\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        shape2d = (binary_mask.shape[0], binary_mask.shape[1])\n        rgba = np.zeros(shape2d + (4,), dtype=\"float32\")\n        rgba[:, :, :3] = color\n        rgba[:, :, 3] = binary_mask * alpha\n        self.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\n\n        if text is not None:\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            self._draw_text_in_mask(binary_mask, text, lighter_color)\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        shape2d = (binary_mask.shape[0], binary_mask.shape[1])\n        rgba = np.zeros(shape2d + (4,), dtype=\"float32\")\n        rgba[:, :, :3] = color\n        rgba[:, :, 3] = binary_mask * alpha\n        self.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\n\n        if text is not None:\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            self._draw_text_in_mask(binary_mask, text, lighter_color)\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        shape2d = (binary_mask.shape[0], binary_mask.shape[1])\n        rgba = np.zeros(shape2d + (4,), dtype=\"float32\")\n        rgba[:, :, :3] = color\n        rgba[:, :, 3] = binary_mask * alpha\n        self.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\n\n        if text is not None:\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            self._draw_text_in_mask(binary_mask, text, lighter_color)\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        shape2d = (binary_mask.shape[0], binary_mask.shape[1])\n        rgba = np.zeros(shape2d + (4,), dtype=\"float32\")\n        rgba[:, :, :3] = color\n        rgba[:, :, 3] = binary_mask * alpha\n        self.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\n\n        if text is not None:\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            self._draw_text_in_mask(binary_mask, text, lighter_color)\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        shape2d = (binary_mask.shape[0], binary_mask.shape[1])\n        rgba = np.zeros(shape2d + (4,), dtype=\"float32\")\n        rgba[:, :, :3] = color\n        rgba[:, :, 3] = binary_mask * alpha\n        self.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\n\n        if text is not None:\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            self._draw_text_in_mask(binary_mask, text, lighter_color)\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n        shape2d = (binary_mask.shape[0], binary_mask.shape[1])\n        rgba = np.zeros(shape2d + (4,), dtype=\"float32\")\n        rgba[:, :, :3] = color\n        rgba[:, :, 3] = binary_mask * alpha\n        self.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\n\n        if text is not None:\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            self._draw_text_in_mask(binary_mask, text, lighter_color)\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        if edge_color is None:\n            # make edge color darker than the polygon color\n            if alpha > 0.8:\n                edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n            else:\n                edge_color = color\n        edge_color = mplc.to_rgb(edge_color) + (1,)\n\n        shape2d = (binary_mask.shape[0], binary_mask.shape[1])\n        rgba = np.zeros(shape2d + (4,), dtype=\"float32\")\n        rgba[:, :, :3] = color\n        rgba[:, :, 3] = binary_mask * alpha\n        self.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\n\n        if text is not None:\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            self._draw_text_in_mask(binary_mask, text, lighter_color)\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        if edge_color is None:\n            edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n        edge_color = mplc.to_rgb(edge_color)\n\n        shape2d = (binary_mask.shape[0], binary_mask.shape[1])\n        rgba = np.zeros(shape2d + (4,), dtype=\"float32\")\n        rgba[:, :, :3] = color\n        rgba[:, :, 3] = binary_mask * alpha\n        self.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\n\n        if text is not None:\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            self._draw_text_in_mask(binary_mask, text, lighter_color)\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        if edge_color is None:\n            # make edge color darker than the polygon color\n            edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n        edge_color = mplc.to_rgb(edge_color) + (1,)\n\n        shape2d = (binary_mask.shape[0], binary_mask.shape[1])\n        rgba = np.zeros(shape2d + (4,), dtype=\"float32\")\n        rgba[:, :, :3] = color\n        rgba[:, :, 3] = binary_mask * alpha\n        self.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\n\n        if text is not None:\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            self._draw_text_in_mask(binary_mask, text, lighter_color)\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        if edge_color is None:\n            edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n        edge_color = mplc.to_rgb(edge_color)\n\n        shape2d = (binary_mask.shape[0], binary_mask.shape[1])\n        rgba = np.zeros(shape2d + (4,), dtype=\"float32\")\n        rgba[:, :, :3] = color\n        rgba[:, :, 3] = binary_mask * alpha\n        self.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\n\n        if text is not None:\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            self._draw_text_in_mask(binary_mask, text, lighter_color)\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        if edge_color is None:\n            edge_color = color\n        edge_color = mplc.to_rgb(edge_color)\n\n        shape2d = (binary_mask.shape[0], binary_mask.shape[1])\n        rgba = np.zeros(shape2d + (4,), dtype=\"float32\")\n        rgba[:, :, :3] = color\n        rgba[:, :, 3] = binary_mask * alpha\n        self.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\n\n        if text is not None:\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            self._draw_text_in_mask(binary_mask, text, lighter_color)\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        if not isinstance(binary_mask, np.ndarray):\n            binary_mask = binary_mask.cpu().numpy()\n        binary_mask = binary_mask.astype(\"uint8\")\n\n        shape2d = (binary_mask.shape[0], binary_mask.shape[1])\n        rgba = np.zeros(shape2d + (4,), dtype=\"float32\")\n        rgba[:, :, :3] = color\n        rgba[:, :, 3] = binary_mask * alpha\n        self.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\n\n        if text is not None:\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            self._draw_text_in_mask(binary_mask, text, lighter_color)\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        if edge_color is None:\n            # make edge color darker than the polygon color\n            if alpha > 0.8:\n                edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n            else:\n                edge_color = color\n        edge_color = mplc.to_rgb(edge_color) + (1,)\n\n        if isinstance(binary_mask, torch.Tensor):\n            binary_mask = binary_mask.numpy()\n\n        # find connected components\n        num_cc, cc_labels, stats, centroids = cv2.connectedComponentsWithStats(\n            binary_mask.astype(\"uint8\"), 8\n        )\n        if stats[1:, -1].size == 0:\n            return self.output\n\n        # draw all components with large enough area\n        for cid in range(1, num_cc):\n            if stats[cid, -1] > area_threshold:\n                # median is more stable than centroid\n                # center = centroids[largest_component_id]\n                center = np.median((cc_labels == cid).nonzero(), axis=1)[::-1]\n                self.draw_circle(center, color=color, radius=3)\n\n        # draw text on the largest component, as well as other very large components.\n        largest_component_id = np.argmax(stats[1:, -1]) + 1\n        if text is not None:\n            self._draw_text_in_mask(cc_labels == largest_component_id, text, color)\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        # convert to mask api\n        if isinstance(binary_mask, torch.Tensor):\n            binary_mask = binary_mask.numpy()\n        if isinstance(binary_mask, np.ndarray):\n            binary_mask = GenericMask(binary_mask, self.output.height, self.output.width)\n\n        # filter small components\n        if area_threshold > 0:\n            binary_mask = binary_mask.mask\n            _num_cc, cc_labels, stats, centroids = cv2.connectedComponentsWithStats(\n                binary_mask, 8\n            )\n            keep = stats[1:, -1] >= area_threshold\n            binary_mask = (cc_labels > 0) & keep[cc_labels[..., None] - 1]\n\n        # draw polygons\n        for segment in binary_mask.polygons:\n            self.draw_polygon(segment.reshape(-1, 2), color, edge_color=edge_color, alpha=alpha)\n\n        # draw text\n        if text is not None:\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            self._draw_text_in_mask(binary_mask.mask, text, lighter_color)\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        if edge_color is None:\n            edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n        edge_color = mplc.to_rgb(edge_color)\n\n        # convert mask to polygons\n        if isinstance(binary_mask, torch.Tensor):\n            binary_mask = binary_mask.numpy()\n        if isinstance(binary_mask, np.ndarray):\n            binary_mask = binary_mask.astype(\"uint8\")\n        binary_mask = binary_mask.copy()\n        binary_mask[binary_mask > 0] = 1\n        polygons = mask_util.encode(np.asfortranarray(binary_mask))[0]\n        polygons = [p.reshape(-1, 2) for p in polygons[\"counts\"]]\n\n        for segment in polygons:\n            self.draw_polygon(segment, color, edge_color, alpha)\n\n        if text is not None:\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            self._draw_text_in_mask(binary_mask, text, lighter_color)\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        if isinstance(binary_mask, torch.Tensor):\n            binary_mask = binary_mask.numpy()\n        if isinstance(binary_mask, np.ndarray):\n            binary_mask = binary_mask.astype(\"uint8\")\n\n        if edge_color is None:\n            edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n        edge_color = mplc.to_rgb(edge_color) + (1,)\n\n        # find contours\n        contours, hierarchy = cv2.findContours(\n            binary_mask.copy(), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE\n        )\n        # filter out small contours\n        contours = [c for c in contours if cv2.contourArea(c) > area_threshold]\n        # draw contours\n        for c in contours:\n            cv2.drawContours(self.img, [c], -1, color, -1)\n\n        # draw text\n        if text is not None:\n            self._draw_text_in_mask(binary_mask, text, edge_color)\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        if edge_color is None:\n            edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n        edge_color = mplc.to_rgb(edge_color) + (1,)\n\n        # use a different approach for masks with holes\n        if isinstance(binary_mask, list):\n            binary_mask = np.asarray(binary_mask)\n            if binary_mask.ndim == 3:\n                binary_mask = binary_mask[0]\n            binary_mask = binary_mask.astype(np.uint8)\n            # use a different approach for masks with holes\n            if binary_mask.ndim == 3:\n                binary_mask = binary_mask[0]\n            binary_mask = binary_mask.astype(np.uint8)\n            contours, hierarchy = cv2.findContours(\n                binary_mask, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE\n            )\n            for contour, hier in zip(contours, hierarchy[0]):\n                if hier[3] == -1 and cv2.contourArea(contour) > area_threshold:\n                    polygon = mpl.patches.Polygon(\n                        contour.reshape(-1, 2),\n                        fill=True,\n                        facecolor=color + (alpha,),\n                        edgecolor=edge_color,\n                        linewidth=max(self._default_font_size // 15 * self.output.scale, 1),\n                    )\n                    self.output.ax.add_patch(polygon)\n        else:\n            # use a different approach for masks with holes\n            if binary_mask.ndim == 3:\n                binary_mask = binary_mask[0]\n            binary_mask = binary_mask.astype(np.uint8)\n            contours, hierarchy = cv2.findContours("}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        # cv2.RETR_CCOMP flag retrieves all the contours and arranges them to a 2-level\n        # hierarchy. External contours (boundary) of the object are placed in hierarchy-1.\n        # Internal contours (holes) are placed in hierarchy-2.\n        # cv2.CHAIN_APPROX_NONE flag gets vertices of polygons from contours.\n        res = cv2.findContours(binary_mask.astype(\"uint8\"), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\n        hierarchy = res[-1]\n        if hierarchy is None:  # empty mask\n            return self.output\n\n        # only draw the largest connected component\n        largest_component_id = np.argmax(hierarchy[:, -1])\n        largest_component = hierarchy[largest_component_id, :]\n        if largest_component[3] == -1 and largest_component[2] != -1 and (\n            hierarchy[largest_component[2], 3] != -1\n        ):  # skip contours inside other contours\n            return self.output\n\n        # draw polygons\n        res = res[-2]\n        for contour in res:\n            if contour.shape[0] < 3:\n                continue\n            contour = np.squeeze(contour)\n            if contour.shape[0] < 3:\n                continue\n            contour = np.flip(contour, axis=1)\n            contour = np.append(contour, [contour[0]], axis=0)\n            self.draw_polygon(contour, color, edge_color=edge_color, alpha=alpha)\n\n        # draw text on the mask\n        if text is not None:\n            # draw text on the largest component\n            mask = (cc_labels == largest_component_id + 1).astype(\"uint8\")"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        if edge_color is None:\n            edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n        edge_color = mplc.to_rgb(edge_color)\n\n        if isinstance(binary_mask, torch.Tensor):\n            binary_mask = binary_mask.numpy()\n        if isinstance(binary_mask, np.ndarray):\n            binary_mask = binary_mask.astype(np.uint8)\n        if isinstance(binary_mask, list):\n            binary_mask = np.array(binary_mask)\n\n        if len(binary_mask.shape) != 2:\n            raise ValueError(\"binary_mask should be of shape (H, W)\")\n\n        if np.any(binary_mask > 1):\n            raise ValueError(\"binary_mask should be in {0, 1}\")\n\n        if np.any(binary_mask < 0):\n            raise ValueError(\"binary_mask should be in {0, 1}\")\n\n        # TODO: this is a bit slow.\n        # TODO: this will not work for mask rle format.\n        # TODO: this will not work for mask in COCO format.\n        # TODO: this will not work for mask in polygons format.\n        # TODO: this will not work for mask in polygons format.\n        # TODO: this will not work for mask in polygons format.\n        # TODO: this will not work for mask in polygons format.\n        # TODO: this will not work for mask in polygons format.\n        # TODO: this will not work for mask in polygons format.\n        # TODO: this will not work for mask in polygons format.\n        # TODO: this will not work for mask in polygons format.\n        # TODO: this will not work for mask in polygons format.\n        # TODO: this will not work for mask in polygons format.\n        # TODO: this will not work for mask in"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        if edge_color is None:\n            edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n        edge_color = mplc.to_rgb(edge_color)\n\n        # use binary mask to find connected components\n        # and draw them as polygons\n        _num_cc, cc_labels, stats, centroids = cv2.connectedComponentsWithStats(\n            binary_mask, connectivity=8\n        )\n        # cc_labels: HxW, each pixel value is the connected component id (0: background)\n        # stats: (N, 5) array, where N is the number of connected components, stats[0] is\n        # background so we skip it. Columns: left, top, width, height, area\n        # centroids: (N, 2) array, each row is the (x, y) position of the centroid\n        # of the connected component\n\n        for cid in range(1, _num_cc):\n            if stats[cid, -1] > area_threshold:\n                # median is more stable than centroid\n                # center = centroids[largest_component_id]\n                center = np.median((cc_labels == cid).nonzero(), axis=1)[::-1]\n                self.draw_text(text, center, color=edge_color)\n\n        # draw polygons\n        for cid in range(1, _num_cc):\n            if stats[cid, -1] > area_threshold:\n                # find all pixels in this connected component\n                ys, xs = (cc_labels == cid).nonzero()\n                # convert to x0, y0, x1, y1 format\n                x0, y0 = np.min(xs), np.min(ys)\n                x1, y1 = np.max(xs) + 1, np.max(ys) + 1\n                # draw polygon\n               "}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if isinstance(input, ScriptInstances):\n        input = convert_scripted_instances(input)\n    if isinstance(other, ScriptInstances):\n        other = convert_scripted_instances(other)\n\n    # check image_size\n    if size_as_tensor:\n        assert torch.equal(input.image_size, other.image_size), msg\n    else:\n        assert input.image_size == other.image_size, msg\n\n    # check fields\n    for name in input._field_names:\n        input_val = getattr(input, name)\n        other_val = getattr(other, name)\n        if isinstance(input_val, (Boxes, ROIMasks)):\n            assert torch.equal(input_val.tensor, other_val.tensor), msg\n        elif isinstance(input_val, torch.Tensor):\n            assert torch.allclose(input_val, other_val, rtol=rtol), msg\n        else:\n            assert input_val == other_val, msg"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    # Check image size\n    if size_as_tensor:\n        assert torch.equal(\n            torch.tensor(input.image_size), torch.tensor(other.image_size)\n        ), f\"{msg}image_size mismatch: {input.image_size} vs {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg}image_size mismatch: {input.image_size} vs {other.image_size}\"\n\n    # Check each field\n    for name in input._field_names:\n        input_val = getattr(input, name)\n        other_val = getattr(other, name)\n        if isinstance(input_val, (Boxes, ROIMasks)):\n            assert input_val == other_val, f\"{msg}{name} mismatch: {input_val} vs {other_val}\"\n        elif isinstance(input_val, torch.Tensor):\n            assert torch.allclose(\n                input_val, other_val, rtol=rtol\n            ), f\"{msg}{name} mismatch: {input_val} vs {other_val}\"\n        else:\n            assert input_val == other_val, f\"{msg}{name} mismatch: {input_val} vs {other_val}\""}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    # Check if the image sizes are equal\n    if size_as_tensor:\n        assert torch.equal(input.image_size, other.image_size), msg\n    else:\n        assert input.image_size == other.image_size, msg\n\n    # Check if the fields in the instances are equal\n    for field_name in input._field_names:\n        input_field = getattr(input, field_name)\n        other_field = getattr(other, field_name)\n\n        if isinstance(input_field, Boxes):\n            assert input_field.tensor.equal(other_field.tensor), msg\n        elif isinstance(input_field, ROIMasks):\n            assert input_field.tensor.equal(other_field.tensor), msg\n            assert input_field.size == other_field.size, msg\n        elif isinstance(input_field, torch.Tensor):\n            assert torch.allclose(input_field, other_field, rtol=rtol), msg\n        else:\n            assert input_field == other_field, msg"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), f\"Expect an Instances object, but got {type(input)}!\"\n    assert isinstance(other, Instances), f\"Expect an Instances object, but got {type(other)}!\"\n    assert input.image_size == other.image_size, f\"{msg}Expect image_size to be the same, but got {input.image_size} and {other.image_size}!\"\n    if size_as_tensor:\n        assert torch.equal(\n            torch.as_tensor(input.image_size), torch.as_tensor(other.image_size)\n        ), f\"{msg}Expect image_size to be the same, but got {input.image_size} and {other.image_size}!\"\n\n    for name in input._field_names:\n        val1 = getattr(input, \"_\" + name, None)\n        val2 = getattr(other, \"_\" + name, None)\n        if val1 is None and val2 is None:\n            continue\n        if val1 is None or val2 is None:\n            raise AssertionError(f\"{msg}Expect {name} to be the same, but got {val1} and {val2}\")\n        if isinstance(val1, Boxes):\n            assert val1.tensor.allclose(val2.tensor, rtol=rtol), f\"{msg}Expect {name} to be the same, but got {val1} and {val2}\"\n        elif isinstance(val1, ROIMasks):\n            assert val1.tensor.allclose(val2.tensor, rtol=rtol), f\"{msg}Expect {name} to be the same, but got {val1} and {val2}\"\n        elif isinstance(val1, torch.Tensor):\n            assert val1.allclose(val2, rtol=rtol), f\"{msg}Expect {name} to be the same, but got {val1} and {val2}\"\n        elif isinstance(val1, (list, tuple)):\n            assert val1 == val2"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances) and isinstance(other, Instances), (\n        f\"{msg}Expect two instances, but got {type(input)} and {type(other)}!\"\n    )\n    assert input.image_size == other.image_size, (\n        f\"{msg}Expect same image_size, but got {input.image_size} and {other.image_size}!\"\n    )\n    if size_as_tensor:\n        assert torch.allclose(\n            torch.tensor(input.image_size), torch.tensor(other.image_size)\n        ), (\n            f\"{msg}Expect same image_size as tensor, but got {input.image_size} and {other.image_size}!\"\n        )\n\n    for name in input._field_names:\n        val1 = getattr(input, \"_\" + name, None)\n        val2 = getattr(other, \"_\" + name, None)\n        if val1 is None or val2 is None:\n            assert val1 == val2, (\n                f\"{msg}Expect {name} to be None, but got {val1} and {val2}!\"\n            )\n            continue\n        if isinstance(val1, Boxes) or isinstance(val1, ROIMasks):\n            assert val1 == val2, (\n                f\"{msg}Expect {name} to be equal, but got {val1} and {val2}!\"\n            )\n        elif isinstance(val1, torch.Tensor):\n            assert torch.allclose(val1, val2, rtol=rtol), (\n                f\"{msg}Expect {name} to be close, but got {val1} and {val2}!\"\n            )\n        else:\n            assert val1 == val2, (\n                f\"{msg}Expect {name} to be equal, but got {val1} and {val2}!\"\n            )"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances)\n    assert isinstance(other, Instances)\n    if size_as_tensor:\n        assert torch.equal(\n            torch.tensor(input.image_size), torch.tensor(other.image_size)\n        ), f\"{msg}image_size mismatch: {input.image_size} vs {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg}image_size mismatch: {input.image_size} vs {other.image_size}\"\n    for name in input._field_names:\n        val1 = getattr(input, name, None)\n        val2 = getattr(other, name, None)\n        if val1 is None and val2 is None:\n            continue\n        if val1 is None or val2 is None:\n            raise AssertionError(f\"{msg}field {name} is missing in one of the instances\")\n        if isinstance(val1, Boxes):\n            assert val1.tensor.shape == val2.tensor.shape, f\"{msg}field {name} shape mismatch: {val1.tensor.shape} vs {val2.tensor.shape}\"\n            assert torch.allclose(\n                val1.tensor, val2.tensor, rtol=rtol\n            ), f\"{msg}field {name} values mismatch: {val1.tensor} vs {val2.tensor}\"\n        elif isinstance(val1, ROIMasks):\n            assert val1.tensor.shape == val2.tensor.shape, f\"{msg}field {name} shape mismatch: {val1.tensor.shape} vs {val2.tensor.shape}\"\n            assert torch.allclose(\n                val1.tensor, val2.tensor, rtol=rtol\n            ), f\"{msg}field {name} values mismatch: {val1.tensor} vs {val2.tensor}\"\n        elif isinstance(val1, torch.Tensor):\n            assert val1.shape == val2.shape, f\"{msg}field {name} shape mismatch"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if not isinstance(input, Instances):\n        raise ValueError(f\"{msg}Expect an Instances object, but got {type(input)}\")\n    if not isinstance(other, Instances):\n        raise ValueError(f\"{msg}Expect an Instances object, but got {type(other)}\")\n\n    if size_as_tensor:\n        assert torch.equal(\n            input.image_size, other.image_size\n        ), f\"{msg}Expect the same image_size, but got {input.image_size} and {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg}Expect the same image_size, but got {input.image_size} and {other.image_size}\"\n\n    for name in input._field_names:\n        val = getattr(input, \"_\" + name, None)\n        other_val = getattr(other, \"_\" + name, None)\n        if val is None and other_val is None:\n            continue\n        if val is None or other_val is None:\n            raise ValueError(f\"{msg}Expect the same fields, but got {name}\")\n        if isinstance(val, Boxes):\n            assert val.tensor.equal(other_val.tensor), f\"{msg}Expect the same {name}\"\n        elif isinstance(val, ROIMasks):\n            assert val.tensor.equal(other_val.tensor), f\"{msg}Expect the same {name}\"\n        elif isinstance(val, torch.Tensor):\n            assert torch.allclose(\n                val, other_val, rtol=rtol\n            ), f\"{msg}Expect the same {name}\"\n        else:\n            raise ValueError(f\"{msg}Unknown field type {type(val)}\")"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), f\"Expect an Instances object, but got {type(input)}!\"\n    assert isinstance(other, Instances), f\"Expect an Instances object, but got {type(other)}!\"\n    assert input.image_size == other.image_size, f\"{msg}Image sizes are not equal: {input.image_size} vs {other.image_size}\"\n    if size_as_tensor:\n        assert torch.allclose(\n            torch.tensor(input.image_size), torch.tensor(other.image_size)\n        ), f\"{msg}Image sizes are not equal: {input.image_size} vs {other.image_size}\"\n    for name in input._field_names:\n        val1 = getattr(input, \"_\" + name, None)\n        val2 = getattr(other, \"_\" + name, None)\n        if val1 is None and val2 is None:\n            continue\n        if val1 is None or val2 is None:\n            raise AssertionError(f\"{msg}Field {name} is None in one of the instances\")\n        if isinstance(val1, Boxes):\n            assert val1.tensor.shape == val2.tensor.shape, f\"{msg}Shape of {name} is not equal: {val1.tensor.shape} vs {val2.tensor.shape}\"\n            assert torch.allclose(val1.tensor, val2.tensor), f\"{msg}Values of {name} are not equal: {val1.tensor} vs {val2.tensor}\"\n        elif isinstance(val1, ROIMasks):\n            assert val1.tensor.shape == val2.tensor.shape, f\"{msg}Shape of {name} is not equal: {val1.tensor.shape} vs {val2.tensor.shape}\"\n            assert torch.allclose(val1.tensor, val2.tensor), f\"{msg}Values of {name} are not equal: {val1.tensor} vs {val2.tensor}\"\n        elif isinstance(val1, torch.Tensor):\n            assert val1."}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    # Check if the image sizes are the same\n    if size_as_tensor:\n        assert torch.equal(\n            torch.tensor(input.image_size), torch.tensor(other.image_size)\n        ), f\"{msg}image_size mismatch: {input.image_size} vs {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg}image_size mismatch: {input.image_size} vs {other.image_size}\"\n\n    # Check if the fields are the same or close\n    for name in input._field_names:\n        val1 = getattr(input, \"_\" + name, None)\n        val2 = getattr(other, \"_\" + name, None)\n        if val1 is None and val2 is None:\n            continue\n        if val1 is None or val2 is None:\n            raise ValueError(f\"{msg}field {name} exists in only one of the instances\")\n        if isinstance(val1, Boxes):\n            assert val1.tensor.shape == val2.tensor.shape, f\"{msg}field {name} shape mismatch: {val1.tensor.shape} vs {val2.tensor.shape}\"\n            assert torch.allclose(val1.tensor, val2.tensor, rtol=rtol), f\"{msg}field {name} mismatch: {val1.tensor} vs {val2.tensor}\"\n        elif isinstance(val1, ROIMasks):\n            assert val1.tensor.shape == val2.tensor.shape, f\"{msg}field {name} shape mismatch: {val1.tensor.shape} vs {val2.tensor.shape}\"\n            assert torch.allclose(val1.tensor, val2.tensor, rtol=rtol), f\"{msg}field {name} mismatch: {val1.tensor} vs {val2.tensor}\"\n        elif isinstance(val1, torch.Tensor):\n            assert val1.shape == val2.shape, f\"{msg}field {name} shape mismatch: {val"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), f\"Expect an Instances object, but got {type(input)}!\"\n    assert isinstance(other, Instances), f\"Expect an Instances object, but got {type(other)}!\"\n\n    if size_as_tensor:\n        assert torch.equal(\n            input.image_size, other.image_size\n        ), f\"{msg}Expect the same image_size, but got {input.image_size} and {other.image_size}!\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg}Expect the same image_size, but got {input.image_size} and {other.image_size}!\"\n\n    for name in input._field_names:\n        val1 = getattr(input, \"_\" + name, None)\n        val2 = getattr(other, \"_\" + name, None)\n        if val1 is None and val2 is None:\n            continue\n        if val1 is None or val2 is None:\n            raise AssertionError(f\"{msg}Expect {name} to be the same, but got {val1} and {val2}\")\n        if isinstance(val1, Boxes):\n            assert torch.allclose(val1.tensor, val2.tensor), f\"{msg}Expect {name} to be the same, but got {val1} and {val2}\"\n        elif isinstance(val1, ROIMasks):\n            assert torch.equal(val1.tensor, val2.tensor), f\"{msg}Expect {name} to be the same, but got {val1} and {val2}\"\n        elif isinstance(val1, torch.Tensor):\n            assert torch.allclose(val1, val2, rtol=rtol), f\"{msg}Expect {name} to be the same, but got {val1} and {val2}\"\n        else:\n            assert val1 == val2, f\"{msg}Expect {name} to be the same, but got {val1} and {val2}\""}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), f\"{msg}Expect input to be an Instances object, but got {type(input)}!\"\n    assert isinstance(other, Instances), f\"{msg}Expect other to be an Instances object, but got {type(other)}!\"\n\n    # check image_size\n    if size_as_tensor:\n        assert torch.equal(input.image_size, other.image_size), f\"{msg}Expect input.image_size to be {other.image_size}, but got {input.image_size}!\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg}Expect input.image_size to be {other.image_size}, but got {input.image_size}!\"\n\n    # check fields\n    for name in input._field_names:\n        val = getattr(input, \"_\" + name, None)\n        other_val = getattr(other, \"_\" + name, None)\n        if val is None:\n            assert other_val is None, f\"{msg}Expect other.{name} to be None, but got {other_val}!\"\n            continue\n\n        if isinstance(val, Boxes):\n            assert isinstance(other_val, Boxes), f\"{msg}Expect other.{name} to be a Boxes object, but got {type(other_val)}!\"\n            assert torch.equal(val.tensor, other_val.tensor), f\"{msg}Expect other.{name}.tensor to be {val.tensor}, but got {other_val.tensor}!\"\n        elif isinstance(val, ROIMasks):\n            assert isinstance(other_val, ROIMasks), f\"{msg}Expect other.{name} to be a ROIMasks object, but got {type(other_val)}!\"\n            assert torch.equal(val.tensor, other_val.tensor), f\"{msg}Expect other.{name}.tensor to be {val.tensor}, but got {other_val.tensor}!\"\n        elif isinstance(val, torch.Tensor):"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), \"Expect an Instances object!\"\n    assert isinstance(other, Instances), \"Expect an Instances object!\"\n\n    assert input.image_size == other.image_size, f\"{msg}Expect image_size to be equal!\"\n    if size_as_tensor:\n        assert torch.allclose(\n            torch.tensor(input.image_size), torch.tensor(other.image_size)\n        ), f\"{msg}Expect image_size to be equal!\"\n\n    for name in input._field_names:\n        val1 = getattr(input, \"_\" + name, None)\n        val2 = getattr(other, \"_\" + name, None)\n        if val1 is None and val2 is None:\n            continue\n        if val1 is None or val2 is None:\n            raise AssertionError(f\"{msg}Expect {name} to be equal or both None.\")\n        if isinstance(val1, Boxes):\n            assert val1.tensor.allclose(val2.tensor), f\"{msg}Expect {name} to be equal!\"\n        elif isinstance(val1, ROIMasks):\n            assert val1.tensor.allclose(val2.tensor), f\"{msg}Expect {name} to be equal!\"\n        elif isinstance(val1, torch.Tensor):\n            assert torch.allclose(val1, val2, rtol=rtol), f\"{msg}Expect {name} to be equal!\"\n        else:\n            raise ValueError(f\"Unknown field type {type(val1)} for {name}.\")"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), f\"{msg}Expect input to be Instances!\"\n    assert isinstance(other, Instances), f\"{msg}Expect other to be Instances!\"\n\n    if size_as_tensor:\n        assert torch.equal(\n            torch.tensor(input.image_size), torch.tensor(other.image_size)\n        ), f\"{msg}Expect image_size to be equal!\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg}Expect image_size to be equal!\"\n\n    for name in input._field_names:\n        val1 = getattr(input, name, None)\n        val2 = getattr(other, name, None)\n        if val1 is None or val2 is None:\n            assert val1 is val2, f\"{msg}Expect {name} to be equal!\"\n        elif isinstance(val1, Boxes):\n            assert val1.tensor.shape == val2.tensor.shape, f\"{msg}Expect {name} to be equal!\"\n            assert torch.allclose(val1.tensor, val2.tensor), f\"{msg}Expect {name} to be close!\"\n        elif isinstance(val1, ROIMasks):\n            assert val1.tensor.shape == val2.tensor.shape, f\"{msg}Expect {name} to be equal!\"\n            assert torch.allclose(val1.tensor, val2.tensor), f\"{msg}Expect {name} to be close!\"\n        elif isinstance(val1, torch.Tensor):\n            assert val1.shape == val2.shape, f\"{msg}Expect {name} to be equal!\"\n            assert torch.allclose(val1, val2, rtol=rtol), f\"{msg}Expect {name} to be close!\"\n        else:\n            raise ValueError(f\"{msg}Don't know how to compare {name} of type {type(val1)}\")"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), f\"{msg}Expect input to be an Instances object, but got {type(input)}!\"\n    assert isinstance(other, Instances), f\"{msg}Expect other to be an Instances object, but got {type(other)}!\"\n    assert isinstance(size_as_tensor, bool), f\"{msg}Expect size_as_tensor to be a bool, but got {type(size_as_tensor)}!\"\n\n    if size_as_tensor:\n        assert torch.equal(\n            torch.tensor(input.image_size), torch.tensor(other.image_size)\n        ), f\"{msg}Expect image_size to be equal, but got {input.image_size} and {other.image_size}!\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg}Expect image_size to be equal, but got {input.image_size} and {other.image_size}!\"\n\n    for name in input._field_names:\n        input_val = getattr(input, \"_\" + name, None)\n        other_val = getattr(other, \"_\" + name, None)\n        if input_val is not None and other_val is not None:\n            if isinstance(input_val, Boxes):\n                assert input_val.tensor.allclose(other_val.tensor), f\"{msg}Expect {name} to be equal, but got {input_val} and {other_val}!\"\n            elif isinstance(input_val, ROIMasks):\n                assert input_val.tensor.allclose(other_val.tensor), f\"{msg}Expect {name} to be equal, but got {input_val} and {other_val}!\"\n            elif isinstance(input_val, torch.Tensor):\n                assert input_val.allclose(other_val), f\"{msg}Expect {name} to be equal, but got {input_val} and {other_val}!\"\n            elif isinstance(input_val, tuple):\n                assert input_val =="}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), f\"{msg}Expect an Instances object, but got {type(input)}!\"\n    assert isinstance(other, Instances), f\"{msg}Expect an Instances object, but got {type(other)}!\"\n\n    assert input.image_size == other.image_size, f\"{msg}image_size mismatch!\"\n\n    for name in input._field_names:\n        val = getattr(input, name, None)\n        other_val = getattr(other, name, None)\n        if val is None:\n            if other_val is not None:\n                raise AssertionError(f\"{msg}field {name} is missing in input\")\n            continue\n        if other_val is None:\n            raise AssertionError(f\"{msg}field {name} is missing in other\")\n\n        if isinstance(val, Boxes):\n            assert val.tensor.shape == other_val.tensor.shape, f\"{msg}tensor shape mismatch for field {name}\"\n            assert torch.allclose(val.tensor, other_val.tensor, rtol=rtol), f\"{msg}tensor mismatch for field {name}\"\n        elif isinstance(val, ROIMasks):\n            assert val.tensor.shape == other_val.tensor.shape, f\"{msg}tensor shape mismatch for field {name}\"\n            assert torch.allclose(val.tensor, other_val.tensor, rtol=rtol), f\"{msg}tensor mismatch for field {name}\"\n        elif isinstance(val, torch.Tensor):\n            assert val.shape == other_val.shape, f\"{msg}tensor shape mismatch for field {name}\"\n            assert torch.allclose(val, other_val, rtol=rtol), f\"{msg}tensor mismatch for field {name}\"\n        else:\n            if isinstance(val, tuple) and isinstance(other_val, tuple):\n                assert len(val) == len(other_val), f\"{msg}tuple length mismatch for field {name}\"\n                for i in range("}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    # Check if the image sizes are equal\n    if size_as_tensor:\n        assert torch.allclose(\n            torch.tensor(input.image_size), torch.tensor(other.image_size)\n        ), f\"{msg}image_size is different\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg}image_size is different\"\n\n    # Iterate through the fields of the instances\n    for name in input._field_names:\n        val = getattr(input, \"_\" + name, None)\n        other_val = getattr(other, \"_\" + name, None)\n        if val is None:\n            assert other_val is None, f\"{msg}{name} is missing in the first instance\"\n        elif other_val is None:\n            assert False, f\"{msg}{name} is missing in the second instance\"\n        else:\n            if isinstance(val, Boxes):\n                assert isinstance(other_val, Boxes), f\"{msg}{name} has different types\"\n                assert torch.allclose(val.tensor, other_val.tensor), f\"{msg}{name} is different\"\n            elif isinstance(val, ROIMasks):\n                assert isinstance(other_val, ROIMasks), f\"{msg}{name} has different types\"\n                assert torch.allclose(val.tensor, other_val.tensor), f\"{msg}{name} is different\"\n            elif isinstance(val, torch.Tensor):\n                assert isinstance(other_val, torch.Tensor), f\"{msg}{name} has different types\"\n                assert torch.allclose(val, other_val, rtol=rtol), f\"{msg}{name} is different\"\n            else:\n                assert val == other_val, f\"{msg}{name} is different\""}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    # Check image_size\n    if size_as_tensor:\n        assert torch.equal(\n            input.image_size, other.image_size\n        ), f\"{msg}image_size mismatch: {input.image_size} vs {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg}image_size mismatch: {input.image_size} vs {other.image_size}\"\n\n    # Check fields\n    for name in input._field_names:\n        val1 = getattr(input, name)\n        val2 = getattr(other, name)\n        if name == \"pred_masks\":\n            # val1 and val2 are both ROIMasks\n            assert val1.tensor.shape == val2.tensor.shape, f\"{msg}pred_masks.tensor.shape mismatch: {val1.tensor.shape} vs {val2.tensor.shape}\"\n            assert torch.equal(val1.tensor, val2.tensor), f\"{msg}pred_masks.tensor mismatch\"\n            assert torch.equal(val1.size, val2.size), f\"{msg}pred_masks.size mismatch: {val1.size} vs {val2.size}\"\n            assert torch.equal(val1.mode, val2.mode), f\"{msg}pred_masks.mode mismatch: {val1.mode} vs {val2.mode}\"\n            assert torch.equal(val1.extra_fields, val2.extra_fields), f\"{msg}pred_masks.extra_fields mismatch: {val1.extra_fields} vs {val2.extra_fields}\"\n        elif name == \"pred_boxes\":\n            # val1 and val2 are both Boxes\n            assert val1.tensor.shape == val2.tensor.shape, f\"{msg}pred_boxes.tensor.shape mismatch: {val1.tensor.shape} vs {val2.tensor.shape}\"\n            assert torch.allclose(val1.tensor, val2.tensor, rtol="}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if isinstance(input, ScriptInstances):\n        input = convert_scripted_instances(input)\n    if isinstance(other, ScriptInstances):\n        other = convert_scripted_instances(other)\n\n    assert isinstance(input, Instances)\n    assert isinstance(other, Instances)\n    assert input.image_size == other.image_size, msg + \"image_size\"\n    for name in input._field_names:\n        val1 = getattr(input, name, None)\n        val2 = getattr(other, name, None)\n        if val1 is None and val2 is None:\n            continue\n        if val1 is None or val2 is None:\n            raise ValueError(f\"{name} field is missing in one of the instances\")\n        if isinstance(val1, Boxes):\n            assert val1.tensor.shape == val2.tensor.shape, msg + f\"{name}.tensor.shape\"\n            assert torch.allclose(val1.tensor, val2.tensor), msg + f\"{name}.tensor\"\n        elif isinstance(val1, ROIMasks):\n            assert val1.tensor.shape == val2.tensor.shape, msg + f\"{name}.tensor.shape\"\n            assert torch.allclose(val1.tensor, val2.tensor), msg + f\"{name}.tensor\"\n        elif isinstance(val1, torch.Tensor):\n            assert val1.shape == val2.shape, msg + f\"{name}.shape\"\n            assert torch.allclose(val1, val2, rtol=rtol), msg + f\"{name}\"\n        elif isinstance(val1, tuple) and size_as_tensor and isinstance(val2, tuple):\n            assert torch.allclose(\n                torch.tensor(val1), torch.tensor(val2)\n            ), msg + f\"{name} (tuple)\"\n        else:\n            assert val1 == val2, msg + f\"{name}\""}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    # Check if the image sizes are equal\n    if size_as_tensor:\n        assert torch.equal(\n            torch.tensor(input.image_size), torch.tensor(other.image_size)\n        ), msg + f\"image_size is not equal: {input.image_size} vs {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, msg + f\"image_size is not equal: {input.image_size} vs {other.image_size}\"\n\n    # Check if all fields are equal or close to each other\n    for field_name in input._field_names:\n        input_val = getattr(input, field_name)\n        other_val = getattr(other, field_name)\n\n        if isinstance(input_val, Boxes):\n            assert input_val.tensor.allclose(other_val.tensor, rtol=rtol), msg + f\"field {field_name} is not close: {input_val} vs {other_val}\"\n        elif isinstance(input_val, ROIMasks):\n            assert input_val.tensor.allclose(other_val.tensor, rtol=rtol), msg + f\"field {field_name} is not close: {input_val} vs {other_val}\"\n        elif isinstance(input_val, torch.Tensor):\n            assert input_val.allclose(other_val, rtol=rtol), msg + f\"field {field_name} is not close: {input_val} vs {other_val}\"\n        else:\n            assert input_val == other_val, msg + f\"field {field_name} is not equal: {input_val} vs {other_val}\"\n\n    # If all checks passed, return\n    "}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if not isinstance(input, Instances):\n        raise ValueError(\"Expect input to be an Instances object\")\n    if not isinstance(other, Instances):\n        raise ValueError(\"Expect other to be an Instances object\")\n\n    assert input.image_size == other.image_size, msg + \"image_size\"\n    assert input.image_size_h == other.image_size_h, msg + \"image_size_h\"\n    assert input.image_size_w == other.image_size_w, msg + \"image_size_w\"\n\n    for name in input._field_names:\n        if name == \"image_size\":\n            continue\n        val1 = getattr(input, name, None)\n        val2 = getattr(other, name, None)\n        if val1 is None and val2 is None:\n            continue\n        if val1 is None or val2 is None:\n            raise AssertionError(f\"{msg}Expect {name} to be equal\")\n        if isinstance(val1, Boxes):\n            assert val1.tensor.allclose(\n                val2.tensor, rtol=rtol\n            ), f\"{msg}Expect {name} to be equal\"\n        elif isinstance(val1, ROIMasks):\n            assert val1.tensor.equal(val2.tensor), f\"{msg}Expect {name} to be equal\"\n        elif isinstance(val1, torch.Tensor):\n            if size_as_tensor:\n                assert val1.shape == val2.shape, f\"{msg}Expect {name} to be equal\"\n            assert val1.allclose(\n                val2, rtol=rtol\n            ), f\"{msg}Expect {name} to be equal\"\n        else:\n            assert val1 == val2, f\"{msg}Expect {name} to be equal\"\n\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        widths = self.tensor[:, 2]\n        heights = self.tensor[:, 3]\n        return widths * heights"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        widths = self.tensor[:, 2]\n        heights = self.tensor[:, 3]\n        return widths * heights"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        widths = self.tensor[:, 2]\n        heights = self.tensor[:, 3]\n        return widths * heights"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        widths = self.tensor[:, 2]\n        heights = self.tensor[:, 3]\n        return widths * heights"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        widths = self.tensor[:, 2]\n        heights = self.tensor[:, 3]\n        return widths * heights"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        widths = self.tensor[:, 2]\n        heights = self.tensor[:, 3]\n        return widths * heights"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        widths = self.tensor[:, 2]\n        heights = self.tensor[:, 3]\n        return widths * heights"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        widths = self.tensor[:, 2]\n        heights = self.tensor[:, 3]\n        return widths * heights"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        widths = self.tensor[:, 2]\n        heights = self.tensor[:, 3]\n        return widths * heights"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        widths = self.tensor[:, 2]\n        heights = self.tensor[:, 3]\n        return widths * heights"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        widths = self.tensor[:, 2]\n        heights = self.tensor[:, 3]\n        return widths * heights"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        widths = self.tensor[:, 2]\n        heights = self.tensor[:, 3]\n        return widths * heights"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        widths = self.tensor[:, 2]\n        heights = self.tensor[:, 3]\n        return widths * heights"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        widths = self.tensor[:, 2]\n        heights = self.tensor[:, 3]\n        return widths * heights"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        widths = self.tensor[:, 2]\n        heights = self.tensor[:, 3]\n        return widths * heights\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        widths = self.tensor[:, 2]\n        heights = self.tensor[:, 3]\n        areas = widths * heights\n        return areas"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        widths = self.tensor[:, 2]\n        heights = self.tensor[:, 3]\n        areas = widths * heights\n        return areas"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        widths = self.tensor[:, 2]\n        heights = self.tensor[:, 3]\n        areas = widths * heights\n        return areas"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        widths = self.tensor[:, 2]\n        heights = self.tensor[:, 3]\n        areas = widths * heights\n        return areas\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        box = self.tensor\n        widths = box[:, 2]\n        heights = box[:, 3]\n        area = widths * heights\n        return area"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    else:\n        return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    else:\n        return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    else:\n        return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    else:\n        return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    else:\n        return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    else:\n        return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    else:\n        return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    else:\n        return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    else:\n        return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    "}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    "}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    else:\n        return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    else:\n        return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    else:\n        return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        gt_classes = cat([proposal.gt_classes for proposal in proposals], dim=0)\n        loss_box_reg = self.box_reg_loss(\n            cat([proposal.proposal_boxes.tensor for proposal in proposals], dim=0),\n            cat([proposal.gt_boxes.tensor for proposal in proposals], dim=0),\n            proposal_deltas,\n            gt_classes,\n        )\n        loss_cls = cross_entropy(\n            scores,\n            gt_classes,\n            self.loss_weight.get(\"loss_cls\", 1.0),\n        )\n        _log_classification_stats(scores, gt_classes)\n        return {\n            \"loss_cls\": loss_cls,\n            \"loss_box_reg\": loss_box_reg,\n        }\n\n    "}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        gt_classes = cat([proposal.gt_classes for proposal in proposals], dim=0)\n        gt_boxes = cat([proposal.gt_boxes.tensor for proposal in proposals], dim=0)\n        loss_box_reg = self.box_reg_loss(\n            cat([proposal.proposal_boxes.tensor for proposal in proposals], dim=0),\n            gt_boxes,\n            proposal_deltas,\n            gt_classes,\n        )\n        loss_cls = cross_entropy(\n            scores,\n            gt_classes,\n            self.loss_weight[\"loss_cls\"],\n        )\n        _log_classification_stats(scores, gt_classes)\n        return {\"loss_cls\": loss_cls, \"loss_box_reg\": loss_box_reg}"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n        gt_boxes = cat([p.gt_boxes.tensor for p in proposals], dim=0)\n        loss_cls = cross_entropy(scores, gt_classes, self.loss_weight[\"loss_cls\"])\n        loss_box_reg = self.box_reg_loss(\n            cat([p.proposal_boxes.tensor for p in proposals], dim=0),\n            gt_boxes,\n            proposal_deltas,\n            gt_classes,\n        )\n        _log_classification_stats(scores, gt_classes)\n        return {\n            \"loss_cls\": loss_cls,\n            \"loss_box_reg\": loss_box_reg,\n        }"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        gt_boxes = cat([proposal.gt_boxes.tensor for proposal in proposals], dim=0)\n        gt_classes = cat([proposal.gt_classes for proposal in proposals], dim=0)\n        loss_box_reg = self.box_reg_loss(\n            cat([proposal.proposal_boxes.tensor for proposal in proposals], dim=0),\n            gt_boxes,\n            proposal_deltas,\n            gt_classes,\n        )\n        loss_cls = cross_entropy(\n            scores,\n            gt_classes,\n            self.loss_weight.get(\"loss_cls\", 1.0),\n        )\n        _log_classification_stats(scores, gt_classes)\n        return {\n            \"loss_cls\": loss_cls,\n            \"loss_box_reg\": loss_box_reg,\n        }"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        proposals = [p for p in proposals if len(p)]\n        if len(proposals) == 0:\n            return {}\n\n        losses = {}\n        if self.loss_weight[\"loss_cls\"] > 0:\n            gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n            loss_cls = cross_entropy(\n                scores,\n                gt_classes,\n                weight=self.loss_weight[\"loss_cls\"],\n                ignore_index=self.num_classes,\n            )\n            losses[\"loss_cls\"] = loss_cls\n            _log_classification_stats(scores, gt_classes)\n\n        if self.loss_weight[\"loss_box_reg\"] > 0:\n            proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)\n            gt_boxes = cat([p.gt_boxes.tensor for p in proposals], dim=0)\n            loss_box_reg = self.box_reg_loss(proposal_boxes, gt_boxes, proposal_deltas, gt_classes)\n            losses[\"loss_box_reg\"] = loss_box_reg\n\n        return losses"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        gt_classes = cat([proposal.gt_classes for proposal in proposals], dim=0)\n        gt_boxes = cat([proposal.gt_boxes.tensor for proposal in proposals], dim=0)\n\n        loss_box_reg = self.box_reg_loss(\n            cat([proposal.proposal_boxes.tensor for proposal in proposals], dim=0),\n            gt_boxes,\n            proposal_deltas,\n            gt_classes,\n        )\n        loss_cls = cross_entropy(\n            scores.view(-1, self.num_classes + 1),\n            gt_classes,\n            self.loss_weight.get(\"loss_cls\", 1.0),\n        )\n        _log_classification_stats(scores, gt_classes)\n\n        losses = {\n            \"loss_cls\": loss_cls,\n            \"loss_box_reg\": loss_box_reg,\n        }\n        return losses\n\n    "}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        proposals = [p for p in proposals if len(p)]\n        if not proposals:\n            return {}\n\n        gt_boxes = cat([p.gt_boxes.tensor for p in proposals], dim=0)\n        gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n\n        loss_box_reg = self.box_reg_loss(\n            cat([p.proposal_boxes.tensor for p in proposals], dim=0),\n            gt_boxes,\n            proposal_deltas,\n            gt_classes,\n        )\n        loss_cls = cross_entropy(\n            scores,\n            gt_classes,\n            self.loss_weight.get(\"loss_cls\", 1.0),\n        )\n        _log_classification_stats(scores, gt_classes)\n        return {\n            \"loss_box_reg\": loss_box_reg,\n            \"loss_cls\": loss_cls,\n        }\n\n    "}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        gt_classes = cat([proposal.gt_classes for proposal in proposals], dim=0)\n        gt_boxes = cat([proposal.gt_boxes.tensor for proposal in proposals], dim=0)\n        deltas = self.predict_boxes(predictions, proposals)\n        loss_cls = cross_entropy(scores, gt_classes, self.loss_weight[\"loss_cls\"])\n        loss_box_reg = self.box_reg_loss(\n            cat([proposal.proposal_boxes.tensor for proposal in proposals], dim=0),\n            gt_boxes,\n            cat(deltas, dim=0),\n            gt_classes,\n        )\n        losses = {\n            \"loss_cls\": loss_cls,\n            \"loss_box_reg\": loss_box_reg,\n        }\n        return losses"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        gt_classes = cat([proposal.gt_classes for proposal in proposals], dim=0)\n        loss_cls = cross_entropy(scores, gt_classes, self.loss_weight[\"loss_cls\"])\n        if self.box_reg_loss_type == \"giou\":\n            from detectron2.layers import generalized_box_iou_loss\n\n            loss_box_reg = generalized_box_iou_loss(\n                self.box2box_transform.get_deltas(proposals),\n                proposal_deltas,\n                gt_classes,\n                self.loss_weight[\"loss_box_reg\"],\n            )\n        else:\n            loss_box_reg = self.box_reg_loss(\n                cat([proposal.proposal_boxes.tensor for proposal in proposals], dim=0),\n                cat([proposal.gt_boxes.tensor for proposal in proposals], dim=0),\n                proposal_deltas,\n                gt_classes,\n            )\n        _log_classification_stats(scores, gt_classes)\n        return {\"loss_cls\": loss_cls, \"loss_box_reg\": loss_box_reg}"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        losses = {}\n        if self.loss_weight.get(\"loss_cls\", 1.0) != 0:\n            loss_cls = cross_entropy(\n                scores,\n                proposals.gt_classes,\n                self.loss_weight.get(\"loss_cls\", 1.0),\n            )\n            _log_classification_stats(scores, proposals.gt_classes)\n            losses[\"loss_cls\"] = loss_cls\n\n        if self.loss_weight.get(\"loss_box_reg\", 1.0) != 0:\n            loss_box_reg = self.box_reg_loss(\n                proposals.proposal_boxes,\n                proposals.gt_boxes,\n                proposal_deltas,\n                proposals.gt_classes,\n            )\n            losses[\"loss_box_reg\"] = loss_box_reg\n        return losses\n\n    "}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        proposals = [x for x in proposals if x.has_field(\"gt_boxes\")]\n        gt_boxes = cat([x.gt_boxes.tensor for x in proposals], dim=0)\n        gt_classes = cat([x.gt_classes for x in proposals], dim=0)\n        loss_box_reg = self.box_reg_loss(\n            cat([x.proposal_boxes.tensor for x in proposals]),\n            gt_boxes,\n            proposal_deltas,\n            gt_classes,\n        )\n        loss_cls = cross_entropy(\n            scores,\n            gt_classes,\n            self.loss_weight.get(\"loss_cls\", 1.0),\n        )\n        _log_classification_stats(scores, gt_classes)\n        return {\n            \"loss_cls\": loss_cls,\n            \"loss_box_reg\": loss_box_reg,\n        }\n\n    "}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        gt_boxes = [x.gt_boxes for x in proposals]\n        gt_classes = [x.gt_classes for x in proposals]\n        gt_boxes = cat(gt_boxes, dim=0)\n        gt_classes = cat(gt_classes, dim=0)\n\n        loss_box_reg = self.box_reg_loss(\n            cat([p.proposal_boxes.tensor for p in proposals], dim=0),\n            gt_boxes,\n            proposal_deltas,\n            gt_classes,\n        )\n        loss_cls = cross_entropy(scores, gt_classes, self.loss_weight[\"loss_cls\"])\n\n        _log_classification_stats(scores, gt_classes)\n\n        return {\"loss_cls\": loss_cls, \"loss_box_reg\": loss_box_reg}"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        gt_classes = cat([proposal.gt_classes for proposal in proposals], dim=0)\n        gt_boxes = cat([proposal.gt_boxes.tensor for proposal in proposals], dim=0)\n\n        loss_box_reg = self.box_reg_loss(\n            cat([proposal.proposal_boxes.tensor for proposal in proposals], dim=0),\n            gt_boxes,\n            proposal_deltas,\n            gt_classes,\n        )\n\n        loss_cls = cross_entropy(\n            scores,\n            gt_classes,\n            self.loss_weight.get(\"loss_cls\", 1.0),\n            ignore_index=self.num_classes,\n        )\n        losses = {\"loss_cls\": loss_cls, \"loss_box_reg\": loss_box_reg}\n        if self.loss_weight.get(\"loss_cls\", 1.0) != 1.0:\n            _log_classification_stats(scores, gt_classes)\n        return losses\n\n    "}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # Unpack\n        scores, proposal_deltas = predictions\n        gt_classes = cat([proposal.gt_classes for proposal in proposals], dim=0)\n        gt_boxes = cat([proposal.gt_boxes.tensor for proposal in proposals], dim=0)\n        deltas_pred = self.box2box_transform.apply_deltas(proposal_deltas)\n        # Losses\n        loss_cls = cross_entropy(scores, gt_classes, self.loss_weight[\"loss_cls\"])\n        loss_box_reg = self.box_reg_loss(\n            proposals, gt_boxes, deltas_pred, gt_classes\n        )\n        losses = {\"loss_cls\": loss_cls, \"loss_box_reg\": loss_box_reg}\n        return losses"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        proposals = [p for p in proposals if p.has(\"gt_boxes\")]\n        gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n        if self.box_reg_loss_type == \"giou\" or self.box_reg_loss_type == \"diou\":\n            gt_boxes = cat([p.gt_boxes.tensor for p in proposals], dim=0)\n            proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)\n        else:\n            gt_boxes = cat([p.gt_boxes.tensor[:, :4] for p in proposals], dim=0)\n            proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)\n\n        loss_cls = cross_entropy(\n            scores,\n            gt_classes,\n            self.loss_weight.get(\"loss_cls\", 1.0),\n        )\n        loss_box_reg = self.box_reg_loss(proposal_boxes, gt_boxes, proposal_deltas, gt_classes)\n        losses = {\"loss_cls\": loss_cls, \"loss_box_reg\": loss_box_reg}\n        return losses"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        proposals = [p for p in proposals if p.has(\"gt_boxes\")]\n        if len(proposals) == 0:\n            return {}\n\n        # Compute the classification loss\n        gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n        loss_cls = cross_entropy(\n            scores,\n            gt_classes,\n            self.loss_weight.get(\"loss_cls\", 1.0) * scores.size()[0],\n            ignore_index=self.num_classes,\n        )\n        _log_classification_stats(scores, gt_classes)\n\n        # Compute the box regression loss\n        gt_boxes = cat([p.gt_boxes.tensor for p in proposals], dim=0)\n        loss_box_reg = self.box_reg_loss(\n            cat([p.proposal_boxes.tensor for p in proposals], dim=0),\n            gt_boxes,\n            proposal_deltas,\n            gt_classes,\n        )\n\n        return {\n            \"loss_cls\": loss_cls,\n            \"loss_box_reg\": loss_box_reg,\n        }\n\n    "}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        proposals = [x.detach() for x in proposals]  # Protect ground truths\n        losses = {}\n\n        # Classification loss\n        loss_cls = cross_entropy(\n            scores.view(-1, self.num_classes + 1),\n            cat([x.gt_classes for x in proposals], dim=0),\n        )\n        losses[\"loss_cls\"] = loss_cls * self.loss_weight[\"loss_cls\"]\n        _log_classification_stats(scores, cat([x.gt_classes for x in proposals], dim=0))\n\n        # Box regression loss\n        if self.box_reg_loss_type == \"smooth_l1\":\n            loss_box_reg = self.box_reg_loss(\n                cat([x.proposal_boxes.tensor for x in proposals], dim=0),\n                cat([x.gt_boxes.tensor for x in proposals], dim=0),\n                proposal_deltas,\n                cat([x.gt_classes for x in proposals], dim=0),\n            )\n        else:\n            raise NotImplementedError(f\"Unknown box regression loss type: {self.box_reg_loss_type}\")\n        losses[\"loss_box_reg\"] = loss_box_reg * self.loss_weight[\"loss_box_reg\"]\n\n        return losses\n\n    "}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        proposals = [x.detach() for x in proposals]  # proposals are not differentiable\n        num_classes = scores.shape[1] - 1\n        assert num_classes == self.num_classes\n\n        # Classification loss\n        loss_cls = cross_entropy(\n            scores,\n            cat([p.gt_classes for p in proposals], dim=0),\n            self.loss_weight.get(\"loss_cls\", 1.0),\n        )\n\n        # Box regression loss\n        if self.box_reg_loss_type == \"smooth_l1\":\n            loss_box_reg = self.smooth_l1_loss(\n                cat([p.proposal_boxes.tensor for p in proposals], dim=0),\n                cat([p.gt_boxes.tensor for p in proposals], dim=0),\n                proposal_deltas,\n                cat([p.gt_classes for p in proposals], dim=0),\n            )\n        elif self.box_reg_loss_type == \"giou\":\n            loss_box_reg = self.box_reg_loss(\n                cat([p.proposal_boxes.tensor for p in proposals], dim=0),\n                cat([p.gt_boxes.tensor for p in proposals], dim=0),\n                proposal_deltas,\n                cat([p.gt_classes for p in proposals], dim=0),\n            )\n        else:\n            raise ValueError(f\"Unsupported box_reg_loss_type: {self.box_reg_loss_type}\")\n\n        return {\n            \"loss_cls\": loss_cls,\n            \"loss_box_reg\": loss_box_reg,\n        }\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        proposals = [x.detach() for x in proposals]  # proposals are not differentiable\n        gt_classes = cat([x.gt_classes for x in proposals], dim=0)\n        gt_boxes = cat([x.gt_boxes.tensor for x in proposals], dim=0)\n        # print(gt_classes, gt_boxes)\n        # print(gt_classes.shape, gt_boxes.shape)\n\n        # print(scores.shape, proposal_deltas.shape)\n        # print(gt_classes.shape, gt_boxes.shape)\n\n        loss_box_reg = self.box_reg_loss(\n            cat([x.proposal_boxes.tensor for x in proposals], dim=0),\n            gt_boxes,\n            proposal_deltas,\n            gt_classes,\n        )\n\n        loss_cls = cross_entropy(\n            scores,\n            gt_classes,\n            self.loss_weight[\"loss_cls\"],\n        )\n        _log_classification_stats(scores, gt_classes)\n\n        return {\"loss_cls\": loss_cls, \"loss_box_reg\": loss_box_reg}"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # In training, the proposals are the ground-truth boxes of the image\n        # If they're not available, use the bounding-box regression deltas instead\n        if self.training:\n            gt_boxes = [x.gt_boxes for x in proposals]\n        else:\n            gt_boxes = self.predict_boxes_for_gt_classes(predictions, proposals)\n        gt_classes = cat([x.gt_classes for x in proposals], dim=0)\n\n        scores, proposal_deltas = predictions\n        loss_cls = cross_entropy(\n            scores.view(-1, self.num_classes + 1),\n            gt_classes,\n            self.loss_weight.get(\"loss_cls\", 1.0),\n        )\n\n        if self.training:\n            loss_box_reg = self.box_reg_loss(\n                cat([x.proposal_boxes.tensor for x in proposals], dim=0),\n                cat(gt_boxes, dim=0),\n                proposal_deltas,\n                gt_classes,\n            )\n            losses = {\"loss_cls\": loss_cls, \"loss_box_reg\": loss_box_reg}\n        else:\n            losses = {\"loss_cls\": loss_cls}\n\n        _log_classification_stats(scores, gt_classes, prefix=\"fast_rcnn\")\n\n        return losses\n\n    "}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker_head = TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)\n    return tracker_head"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker = TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker = TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker_head = TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)\n    return tracker_head"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker = TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.MODEL.TRACKER.NAME\n    tracker_head = TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)\n    return tracker_head"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.MODEL.TRACKER.NAME\n    tracker_head = TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)\n    return tracker_head"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.TRACKER.NAME\n    tracker = TRACKER_HEADS_REGISTRY.get(name)(cfg)\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker_class = TRACKER_HEADS_REGISTRY.get(tracker_name)\n    tracker = tracker_class(cfg)\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.TRACKER.NAME\n    return TRACKER_HEADS_REGISTRY.get(name)(cfg)"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.TRACKER.NAME\n    return TRACKER_HEADS_REGISTRY.get(name)(cfg)"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.MODEL.TRACKER_HEAD.NAME\n    tracker_head = TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)\n    return tracker_head"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.MODEL.TRACKER.NAME\n    tracker = TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)\n\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.MODEL.TRACKER.NAME\n    tracker_class = TRACKER_HEADS_REGISTRY.get(tracker_name)\n    tracker = tracker_class(cfg)\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.MODEL.TRACKER_HEAD.NAME\n    return TRACKER_HEADS_REGISTRY.get(name)(cfg)"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.MODEL.TRACKER.NAME\n    tracker = TRACKER_HEADS_REGISTRY.get(name)\n    return tracker.from_config(cfg)"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.MODEL.TRACKER.NAME\n    tracker = TRACKER_HEADS_REGISTRY.get(name)\n    return tracker.from_config(cfg)"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.TRACKER.NAME\n    tracker = TRACKER_HEADS_REGISTRY.get(name)\n    return tracker(cfg=cfg)"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.TRACKER.NAME\n    tracker = TRACKER_HEADS_REGISTRY.get(name)()\n    tracker.from_config(cfg)\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.TRACKER.NAME\n    tracker_head = TRACKER_HEADS_REGISTRY.get(name)\n    return tracker_head.from_config(cfg)"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[1] % 4 == 0 and boxes.shape[1] == 4\n\n        boxes = boxes.to(deltas.dtype).unsqueeze(2)\n\n        ctr_x = boxes[:, 0]\n        ctr_y = boxes[:, 1]\n        widths = boxes[:, 2]\n        heights = boxes[:, 3]\n\n        wx, wy, ww, wh = self.weights\n\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = dx * widths + ctr_x  # x_ctr\n        pred_boxes[:, 1::4] = dy * heights + ctr_y  # y_ctr\n        pred_boxes[:, 2::4] = torch.exp(dw) * widths  # width\n        pred_boxes[:, 3::4] = torch.exp(dh) * heights  # height\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[1] % 4 == 0 and boxes.shape[1] == 4\n\n        boxes = boxes.to(deltas.dtype).unsqueeze(2)\n\n        ctr_x = boxes[:, 0]\n        ctr_y = boxes[:, 1]\n        widths = boxes[:, 2]\n        heights = boxes[:, 3]\n\n        wx, wy, ww, wh = self.weights\n\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = dx * widths + ctr_x  # x_ctr\n        pred_boxes[:, 1::4] = dy * heights + ctr_y  # y_ctr\n        pred_boxes[:, 2::4] = torch.exp(dw) * widths  # width\n        pred_boxes[:, 3::4] = torch.exp(dh) * heights  # height\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[1] % 4 == 0 and boxes.shape[1] == 4\n\n        boxes = boxes.to(deltas.dtype).unsqueeze(2)\n\n        ctr_x = boxes[:, 0]\n        ctr_y = boxes[:, 1]\n        widths = boxes[:, 2]\n        heights = boxes[:, 3]\n\n        wx, wy, ww, wh = self.weights\n\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = dx * widths + ctr_x  # x_ctr\n        pred_boxes[:, 1::4] = dy * heights + ctr_y  # y_ctr\n        pred_boxes[:, 2::4] = torch.exp(dw) * widths  # width\n        pred_boxes[:, 3::4] = torch.exp(dh) * heights  # height\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[1] % 4 == 0 and boxes.shape[1] == 4\n\n        boxes = boxes.to(deltas.dtype).unsqueeze(2)\n\n        ctr_x = boxes[:, 0]\n        ctr_y = boxes[:, 1]\n        widths = boxes[:, 2]\n        heights = boxes[:, 3]\n\n        wx, wy, ww, wh = self.weights\n\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = dx * widths + ctr_x  # x_ctr\n        pred_boxes[:, 1::4] = dy * heights + ctr_y  # y_ctr\n        pred_boxes[:, 2::4] = torch.exp(dw) * widths  # width\n        pred_boxes[:, 3::4] = torch.exp(dh) * heights  # height\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[1] % 4 == 0 and boxes.shape[1] == 4\n\n        boxes = boxes.to(deltas.dtype).unsqueeze(2)\n\n        ctr_x = boxes[:, 0]\n        ctr_y = boxes[:, 1]\n        widths = boxes[:, 2]\n        heights = boxes[:, 3]\n\n        wx, wy, ww, wh = self.weights\n\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = dx * widths + ctr_x  # x_ctr\n        pred_boxes[:, 1::4] = dy * heights + ctr_y  # y_ctr\n        pred_boxes[:, 2::4] = torch.exp(dw) * widths  # width\n        pred_boxes[:, 3::4] = torch.exp(dh) * heights  # height\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[1] % 4 == 0 and boxes.shape[1] == 4\n\n        boxes = boxes.to(deltas.dtype).unsqueeze(2)\n\n        ctr_x = boxes[:, 0]\n        ctr_y = boxes[:, 1]\n        widths = boxes[:, 2]\n        heights = boxes[:, 3]\n\n        wx, wy, ww, wh = self.weights\n\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = dx * widths + ctr_x  # x_ctr\n        pred_boxes[:, 1::4] = dy * heights + ctr_y  # y_ctr\n        pred_boxes[:, 2::4] = torch.exp(dw) * widths  # width\n        pred_boxes[:, 3::4] = torch.exp(dh) * heights  # height\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[1] % 4 == 0 and boxes.shape[1] == 4\n\n        boxes = boxes.to(deltas.dtype).unsqueeze(2)\n\n        ctr_x = boxes[:, 0]\n        ctr_y = boxes[:, 1]\n        widths = boxes[:, 2]\n        heights = boxes[:, 3]\n\n        wx, wy, ww, wh = self.weights\n\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = dx * widths + ctr_x  # x_ctr\n        pred_boxes[:, 1::4] = dy * heights + ctr_y  # y_ctr\n        pred_boxes[:, 2::4] = torch.exp(dw) * widths  # width\n        pred_boxes[:, 3::4] = torch.exp(dh) * heights  # height\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[1] % 4 == 0 and boxes.shape[1] == 4\n\n        boxes = boxes.to(deltas.dtype).unsqueeze(2)\n\n        ctr_x = boxes[:, 0]\n        ctr_y = boxes[:, 1]\n        widths = boxes[:, 2]\n        heights = boxes[:, 3]\n\n        wx, wy, ww, wh = self.weights\n\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = dx * widths + ctr_x  # x_ctr\n        pred_boxes[:, 1::4] = dy * heights + ctr_y  # y_ctr\n        pred_boxes[:, 2::4] = torch.exp(dw) * widths  # width\n        pred_boxes[:, 3::4] = torch.exp(dh) * heights  # height\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[-1] % 4 == 0 and boxes.shape[-1] == 4\n\n        boxes = boxes.to(deltas.dtype).unsqueeze(2)\n\n        ctr_x = boxes[:, 0]\n        ctr_y = boxes[:, 1]\n        widths = boxes[:, 2]\n        heights = boxes[:, 3]\n\n        wx, wy, ww, wh = self.weights\n\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = dx * widths + ctr_x  # x_ctr\n        pred_boxes[:, 1::4] = dy * heights + ctr_y  # y_ctr\n        pred_boxes[:, 2::4] = torch.exp(dw) * widths  # width\n        pred_boxes[:, 3::4] = torch.exp(dh) * heights  # height\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[1] % 4 == 0 and boxes.shape[1] == 4\n\n        boxes = boxes.to(deltas.dtype).unsqueeze(2)\n\n        ctr_x = boxes[:, 0]\n        ctr_y = boxes[:, 1]\n        widths = boxes[:, 2]\n        heights = boxes[:, 3]\n\n        wx, wy, ww, wh = self.weights\n\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = dx * widths + ctr_x  # x_ctr\n        pred_boxes[:, 1::4] = dy * heights + ctr_y  # y_ctr\n        pred_boxes[:, 2::4] = torch.exp(dw) * widths  # width\n        pred_boxes[:, 3::4] = torch.exp(dh) * heights  # height\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[-1] == 4\n        assert boxes.shape[-1] == 4\n\n        boxes = boxes.to(deltas.dtype).unsqueeze(2)\n\n        ctr_x = boxes[:, 0]\n        ctr_y = boxes[:, 1]\n        widths = boxes[:, 2]\n        heights = boxes[:, 3]\n\n        wx, wy, ww, wh = self.weights\n\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = dx * widths + ctr_x  # x_ctr\n        pred_boxes[:, 1::4] = dy * heights + ctr_y  # y_ctr\n        pred_boxes[:, 2::4] = torch.exp(dw) * widths  # width\n        pred_boxes[:, 3::4] = torch.exp(dh) * heights  # height\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[1] % 4 == 0 and boxes.shape[1] == 4\n\n        boxes = boxes.to(deltas.dtype).unsqueeze(2)\n\n        ctr_x = boxes[:, 0]\n        ctr_y = boxes[:, 1]\n        widths = boxes[:, 2]\n        heights = boxes[:, 3]\n\n        wx, wy, ww, wh = self.weights\n\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = dx * widths + ctr_x  # x_ctr\n        pred_boxes[:, 1::4] = dy * heights + ctr_y  # y_ctr\n        pred_boxes[:, 2::4] = torch.exp(dw) * widths  # width\n        pred_boxes[:, 3::4] = torch.exp(dh) * heights  # height\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[-1] == 4\n        assert boxes.shape[-1] == 4\n\n        boxes = boxes.to(deltas.dtype).unsqueeze(2)\n\n        ctr_x = boxes[:, 0]\n        ctr_y = boxes[:, 1]\n        widths = boxes[:, 2]\n        heights = boxes[:, 3]\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = dx * widths + ctr_x  # x_ctr\n        pred_boxes[:, 1::4] = dy * heights + ctr_y  # y_ctr\n        pred_boxes[:, 2::4] = torch.exp(dw) * widths  # width\n        pred_boxes[:, 3::4] = torch.exp(dh) * heights  # height\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[-1] % 4 == 0 and boxes.shape[-1] == 4\n\n        boxes = boxes.to(deltas.dtype).unsqueeze(2)\n\n        ctr_x = boxes[:, 0]\n        ctr_y = boxes[:, 1]\n        widths = boxes[:, 2]\n        heights = boxes[:, 3]\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = dx * widths + ctr_x  # x_ctr\n        pred_boxes[:, 1::4] = dy * heights + ctr_y  # y_ctr\n        pred_boxes[:, 2::4] = torch.exp(dw) * widths  # width\n        pred_boxes[:, 3::4] = torch.exp(dh) * heights  # height\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[-1] == 4\n        assert boxes.shape[-1] == 4\n\n        boxes = boxes.to(deltas.dtype)\n\n        ctr_x = boxes[..., 0] + 0.5 * boxes[..., 2]\n        ctr_y = boxes[..., 1] + 0.5 * boxes[..., 3]\n        w = boxes[..., 2]\n        h = boxes[..., 3]\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[..., 0] / wx\n        dy = deltas[..., 1] / wy\n        dw = deltas[..., 2] / ww\n        dh = deltas[..., 3] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * w + ctr_x\n        pred_ctr_y = dy * h + ctr_y\n        pred_w = torch.exp(dw) * w\n        pred_h = torch.exp(dh) * h\n\n        pred_boxes = torch.zeros_like(deltas)\n        # x1\n        pred_boxes[..., 0] = pred_ctr_x - 0.5 * pred_w\n        # y1\n        pred_boxes[..., 1] = pred_ctr_y - 0.5 * pred_h\n        # x2\n        pred_boxes[..., 2] = pred_ctr_x + 0.5 * pred_w\n        # y2\n        pred_boxes[..., 3] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[-1] == 4, deltas.shape\n        boxes = boxes.to(deltas.dtype)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = dx * widths + ctr_x - 0.5 * torch.exp(dw) * widths\n        pred_boxes[:, 1::4] = dy * heights + ctr_y - 0.5 * torch.exp(dh) * heights\n        pred_boxes[:, 2::4] = dx * widths + ctr_x + 0.5 * torch.exp(dw) * widths\n        pred_boxes[:, 3::4] = dy * heights + ctr_y + 0.5 * torch.exp(dh) * heights\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[-1] == 4 and boxes.shape[-1] == 4\n\n        boxes = boxes.to(deltas.dtype).unsqueeze(2)\n\n        ctr_x = boxes[..., 0]\n        ctr_y = boxes[..., 1]\n        widths = boxes[..., 2]\n        heights = boxes[..., 3]\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[..., 0] / wx\n        dy = deltas[..., 1] / wy\n        dw = deltas[..., 2] / ww\n        dh = deltas[..., 3] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[..., 0] = dx * widths + ctr_x  # x_ctr\n        pred_boxes[..., 1] = dy * heights + ctr_y  # y_ctr\n        pred_boxes[..., 2] = torch.exp(dw) * widths  # width\n        pred_boxes[..., 3] = torch.exp(dh) * heights  # height\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[-1] % 4 == 0, \"deltas must have multiple of 4 channels\"\n        assert deltas.shape[-1] // 4 == boxes.shape[-1]\n\n        boxes = boxes.to(deltas.dtype).unsqueeze(2)\n\n        ctr_x = boxes[..., 0]\n        ctr_y = boxes[..., 1]\n        widths = boxes[..., 2]\n        heights = boxes[..., 3]\n\n        wx, wy, ww, wh = self.weights\n\n        dx = deltas[..., 0::4] / wx\n        dy = deltas[..., 1::4] / wy\n        dw = deltas[..., 2::4] / ww\n        dh = deltas[..., 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[..., 0::4] = dx * widths + ctr_x  # x_ctr\n        pred_boxes[..., 1::4] = dy * heights + ctr_y  # y_ctr\n        pred_boxes[..., 2::4] = torch.exp(dw) * widths  # width\n        pred_boxes[..., 3::4] = torch.exp(dh) * heights  # height\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[-1] == 4, deltas.shape\n        boxes = boxes.to(deltas.dtype)\n\n        ctr_x = boxes[..., 0] + 0.5 * boxes[..., 2]\n        ctr_y = boxes[..., 1] + 0.5 * boxes[..., 3]\n        w = boxes[..., 2]\n        h = boxes[..., 3]\n\n        wx, wy, ww, wh = self.weights\n\n        dx = deltas[..., 0] / wx\n        dy = deltas[..., 1] / wy\n        dw = deltas[..., 2] / ww\n        dh = deltas[..., 3] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * w + ctr_x\n        pred_ctr_y = dy * h + ctr_y\n        pred_w = torch.exp(dw) * w\n        pred_h = torch.exp(dh) * h\n\n        pred_boxes = torch.zeros_like(deltas)\n        # x1\n        pred_boxes[..., 0] = pred_ctr_x - 0.5 * pred_w\n        # y1\n        pred_boxes[..., 1] = pred_ctr_y - 0.5 * pred_h\n        # x2\n        pred_boxes[..., 2] = pred_ctr_x + 0.5 * pred_w\n        # y2\n        pred_boxes[..., 3] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[-1] == 4\n        assert boxes.shape[-1] == 4\n\n        boxes = boxes.to(deltas.dtype)\n\n        # denormalize deltas with weights\n        widths = boxes[..., 2] - boxes[..., 0]\n        heights = boxes[..., 3] - boxes[..., 1]\n        ctr_x = boxes[..., 0] + 0.5 * widths\n        ctr_y = boxes[..., 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[..., 0::4] / wx\n        dy = deltas[..., 1::4] / wy\n        dw = deltas[..., 2::4] / ww\n        dh = deltas[..., 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths + ctr_x\n        pred_ctr_y = dy * heights + ctr_y\n        pred_w = torch.exp(dw) * widths\n        pred_h = torch.exp(dh) * heights\n\n        # Convert center-xy/width/height to top-left/bottom-right\n        pred_boxes1 = torch.zeros_like(deltas)\n        pred_boxes1[..., 0::4] = pred_ctr_x - 0.5 * pred_w\n        pred_boxes1[..., 1::4] = pred_ctr_y - 0.5 * pred_h\n        pred_boxes1[..., 2::4] = pred_ctr_x + 0.5 * pred_w\n        pred_boxes1[..., 3::4] = pred_ctr_y + 0.5 * pred"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": ""}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": ""}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": ""}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": ""}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": ""}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": ""}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        output = self.general_ins(image)\n        if anno_type is None:\n            return output\n        else:\n            return {tp: output[tp] for tp in anno_type}"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            return self.general_ins.run(image)\n        else:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            elif isinstance(anno_type, (list, tuple)):\n                pass\n            else:\n                raise Exception(f'Error anno_type: {anno_type}')\n\n        output = self.general_ins.run(image)\n        if all(tp in output.keys() for tp in anno_type):\n            return {tp: output[tp] for tp in anno_type}\n        else:\n            return output"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            return self.general_ins(image)\n        else:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            elif isinstance(anno_type, (list, tuple)):\n                pass\n            else:\n                raise Exception(f'Error anno_type: {anno_type}')\n            output = self.general_ins(image)\n            output = {k: v for k, v in output.items() if k in anno_type}\n            if len(output) == 1:\n                output = list(output.values())[0]\n            return output"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            output = self.general_ins(image)\n        else:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            elif isinstance(anno_type, (list, tuple)):\n                pass\n            else:\n                raise Exception(f'Error anno_type: {anno_type}')\n            output = self.general_ins(image)\n            output = {tp: output[tp] for tp in anno_type}\n\n        return output"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            return self.general_ins.run(image)\n        else:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            elif isinstance(anno_type, (list, tuple)):\n                assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n            else:\n                raise Exception(f'Error anno_type: {anno_type}')\n\n            output = self.general_ins.run(image)\n            anno_dict = {}\n            for tp in anno_type:\n                if tp in output.keys():\n                    anno_dict[tp] = output[tp]\n            if len(anno_dict) == 0:\n                return output\n            elif len(anno_dict) == 1:\n                return anno_dict[list(anno_dict.keys())[0]]\n            else:\n                return anno_dict"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            return self.general_ins(image)\n        elif isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            pass\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        output = self.general_ins(image)\n        anno_dict = {}\n        for tp in anno_type:\n            if tp in output.keys():\n                anno_dict[tp] = output[tp]\n        if len(anno_dict) == 1:\n            return anno_dict[tp]\n        else:\n            return anno_dict"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            anno_type = self.anno_type_map.keys()\n        elif isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            pass\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        output = self.general_ins(image)\n        output = {k: v for k, v in output.items() if k in anno_type}\n        return output"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            pass\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        anno_dict = self.general_ins(image)\n        if anno_type is None:\n            return anno_dict\n        else:\n            return {tp: anno_dict[tp] for tp in anno_type if tp in anno_dict}"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            return self.general_ins(image)\n        else:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            elif isinstance(anno_type, (list, tuple)):\n                pass\n            else:\n                raise Exception(f'Error anno_type: {anno_type}')\n\n            output = self.general_ins(image)\n            output_keys = output.keys()\n            output_dict = {}\n            for tp in anno_type:\n                if tp in output_keys:\n                    output_dict[tp] = output[tp]\n            if len(output_dict) == 0:\n                return None\n            elif len(output_dict) == 1:\n                return list(output_dict.values())[0]\n            else:\n                return output_dict"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            anno_type = self.anno_type_map.keys()\n        if isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        outputs = self.general_ins(image)\n        if len(anno_type) == 1:\n            if anno_type[0] in outputs.keys():\n                return outputs[anno_type[0]]\n            else:\n                return None\n        else:\n            return {tp: outputs[tp] for tp in anno_type if tp in outputs.keys()}"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            anno_type = self.anno_type_map.keys()\n        if isinstance(anno_type, str):\n            anno_type = [anno_type]\n\n        out = self.general_ins(image)\n        if len(anno_type) == 1:\n            anno_type = anno_type[0]\n            if anno_type in out.keys():\n                return out[anno_type]\n            else:\n                return None\n        else:\n            anno_dict = {}\n            for anno in anno_type:\n                if anno in out.keys():\n                    anno_dict[anno] = out[anno]\n            return anno_dict"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            return self.general_ins(image)\n        else:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            elif isinstance(anno_type, (list, tuple)):\n                assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n            else:\n                raise Exception(f'Error anno_type: {anno_type}')\n\n        output = self.general_ins(image)\n        if isinstance(anno_type, str):\n            if anno_type in output:\n                return output[anno_type]\n            else:\n                return None\n        else:\n            anno_dict = {}\n            for tp in anno_type:\n                if tp in output:\n                    anno_dict[tp] = output[tp]\n            return anno_dict"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        # Run the general annotator\n        outputs = self.general_ins(image)\n\n        # If no specific annotation type is requested, return the entire processed output\n        if anno_type is None:\n            return outputs\n\n        # If a single annotation type is requested, return that specific annotation if it is found in the output\n        if isinstance(anno_type, str):\n            if anno_type in outputs:\n                return outputs[anno_type]\n            else:\n                return None\n\n        # If multiple annotation types are requested, return a dictionary of those types found in the output\n        if isinstance(anno_type, (list, tuple)):\n            anno_dict = {}\n            for anno in anno_type:\n                if anno in outputs:\n                    anno_dict[anno] = outputs[anno]\n            return anno_dict\n\n        raise Exception(f'Error anno_type: {anno_type}')"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        image = image.astype(np.float32) / 255.\n        image = np.transpose(image, (2, 0, 1))\n        image = torch.from_numpy(image).unsqueeze(0).to(we.device_id)\n        output = self.general_ins(image)\n        output = output[0]\n        output = {\n            key: value.cpu().numpy()\n            for key, value in output.items()\n        }\n        if anno_type is not None:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            elif isinstance(anno_type, (list, tuple)):\n                pass\n            else:\n                raise Exception(f'Error anno_type: {anno_type}')\n            output = {\n                key: value\n                for key, value in output.items()\n                if key in anno_type\n            }\n        return output"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        result = {}\n        for kw in normalize_string(query).split(\" \"):\n            result = update_url_scores(result, self.bm25(kw))\n        return result"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        result = {}\n        for kw in normalize_string(query).split(\" \"):\n            result = update_url_scores(result, self.bm25(kw))\n        return result"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        result = {}\n        for kw in normalize_string(query).split(\" \"):\n            result = update_url_scores(result, self.bm25(kw))\n        return result"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        result = {}\n        for kw in normalize_string(query).split(\" \"):\n            result = update_url_scores(result, self.bm25(kw))\n        return result"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        result = {}\n        for kw in normalize_string(query).split(\" \"):\n            result = update_url_scores(result, self.bm25(kw))\n        return result"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        result = {}\n        for kw in normalize_string(query).split(\" \"):\n            result = update_url_scores(result, self.bm25(kw))\n        return result"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        result = {}\n        for kw in normalize_string(query).split(\" \"):\n            result = update_url_scores(result, self.bm25(kw))\n        return result"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        result = {}\n        for kw in query.split(\" \"):\n            result = update_url_scores(result, self.bm25(kw))\n        return result"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = normalize_string(query)\n        keywords = query.split(\" \")\n        scores = {}\n        for kw in keywords:\n            scores = update_url_scores(scores, self.bm25(kw))\n        return scores"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = normalize_string(query)\n        keywords = query.split(\" \")\n        result = {}\n        for kw in keywords:\n            result = update_url_scores(result, self.bm25(kw))\n        return result"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        result = {}\n        for kw in query.split(\" \"):\n            result = update_url_scores(result, self.bm25(kw))\n        return result"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query_keywords = normalize_string(query).split(\" \")\n        result = {}\n        for kw in query_keywords:\n            result = update_url_scores(result, self.bm25(kw))\n        return result"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = normalize_string(query)\n        keywords = query.split(\" \")\n        url_scores = {}\n        for kw in keywords:\n            url_scores = update_url_scores(url_scores, self.bm25(kw))\n        return url_scores"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query_words = normalize_string(query).split(\" \")\n        result = {}\n        for kw in query_words:\n            result = update_url_scores(result, self.bm25(kw))\n        return result"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = normalize_string(query)\n        keywords = query.split(\" \")\n        scores = {}\n        for keyword in keywords:\n            bm25_scores = self.bm25(keyword)\n            scores = update_url_scores(scores, bm25_scores)\n        return scores"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        result = {}\n        for kw in query.split(\" \"):\n            bm25_score = self.bm25(kw)\n            result = update_url_scores(result, bm25_score)\n        return result"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query_words = normalize_string(query).split(\" \")\n        scores = {}\n        for kw in query_words:\n            scores = update_url_scores(scores, self.bm25(kw))\n        return scores"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query_words = normalize_string(query).split(\" \")\n        scores = {}\n        for word in query_words:\n            scores = update_url_scores(scores, self.bm25(word))\n        return scores"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = normalize_string(query)\n        scores = defaultdict(float)\n        for keyword in query.split(\" \"):\n            scores = update_url_scores(scores, self.bm25(keyword))\n        return scores"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        result = {}\n        for keyword in query.split(\" \"):\n            result = update_url_scores(result, self.bm25(keyword))\n        return result"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        box = self.tensor\n        inds = torch.abs(box[:, 4]) < clip_angle_threshold\n        box_xy = box[:, :2]\n        box_wh = box[:, 2:4]\n        box_angle = box[:, 4]\n        box_xy = torch.min(torch.max(box_xy, torch.zeros_like(box_xy)), torch.tensor(box_size).to(box_xy))\n        box_wh = torch.min(torch.max(box_wh, torch.zeros_like(box_wh)), torch.tensor(box_size).to(box_wh))\n        box[:, :2] = box_xy\n        box[:, 2:4] = box_wh\n        box[:, 4] = box_angle"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        box = self.tensor\n        # Identify the indices of the boxes that are nearly horizontal\n        # (i.e., within the specified angle threshold)\n        near_horizontal_inds = torch.abs(box[:, 4]) <= clip_angle_threshold\n        # Convert the representation from (center x, center y, width, height, angle)\n        # to (x1, y1, x2, y2)\n        x1 = box[:, 0] - box[:, 2] / 2.0\n        y1 = box[:, 1] - box[:, 3] / 2.0\n        x2 = box[:, 0] + box[:, 2] / 2.0\n        y2 = box[:, 1] + box[:, 3] / 2.0\n        # Clamp the x and y coordinates to ensure they do not exceed the specified box_size limits\n        x1 = torch.clamp(x1, min=0, max=box_size[1])\n        y1 = torch.clamp(y1, min=0, max=box_size[0])\n        x2 = torch.clamp(x2, min=0, max=box_size[1])\n        y2 = torch.clamp(y2, min=0, max=box_size[0])\n        # Convert the boxes back to the original representation\n        # (center x, center y, width, height, angle)\n        box[:, 0] = (x1 + x2) / 2.0\n        box[:, 1] = (y1 + y2) / 2.0\n        box[:, 2] = x2 - x1\n        box[:, 3] = y2 - y1\n        # Set the angle of the boxes that are nearly horizontal to 0\n        box[near_horizontal_inds, 4] = 0.0"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n\n        box = self.tensor\n        height, width = box_size\n\n        # Identify the indices of the boxes that are nearly horizontal\n        # based on the clip_angle_threshold\n        angle_inds = torch.abs(box[:, 4]) < clip_angle_threshold\n\n        # Convert the representation from (center x, center y, width, height, angle)\n        # to (x1, y1, x2, y2)\n        x1 = box[:, 0] - box[:, 2] / 2\n        y1 = box[:, 1] - box[:, 3] / 2\n        x2 = box[:, 0] + box[:, 2] / 2\n        y2 = box[:, 1] + box[:, 3] / 2\n\n        # Clamp the x and y coordinates to ensure they do not exceed the specified\n        # box_size limits\n        x1 = torch.clamp(x1, min=0, max=width - 1)\n        y1 = torch.clamp(y1, min=0, max=height - 1)\n        x2 = torch.clamp(x2, min=0, max=width - 1)\n        y2 = torch.clamp(y2, min=0, max=height - 1)\n\n        # Convert back to the original representation\n        # (center x, center y, width, height, angle)\n        box[:, 0] = (x1 + x2) / 2\n        box[:, 1] = (y1 + y2) / 2\n        box[:, 2] = x2 - x1\n        box[:, 3] = y2 - y1"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        box = self.tensor\n        box_size_height, box_size_width = box_size\n        x1 = torch.min(box[:, 0] - box[:, 2] / 2, box[:, 0] + box[:, 2] / 2)\n        y1 = torch.min(box[:, 1] - box[:, 3] / 2, box[:, 1] + box[:, 3] / 2)\n        x2 = torch.max(box[:, 0] - box[:, 2] / 2, box[:, 0] + box[:, 2] / 2)\n        y2 = torch.max(box[:, 1] - box[:, 3] / 2, box[:, 1] + box[:, 3] / 2)\n        x1 = torch.clamp(x1, min=0, max=box_size_width - 1)\n        y1 = torch.clamp(y1, min=0, max=box_size_height - 1)\n        x2 = torch.clamp(x2, min=0, max=box_size_width - 1)\n        y2 = torch.clamp(y2, min=0, max=box_size_height - 1)\n        width = x2 - x1\n        height = y2 - y1\n        angle = box[:, 4]\n        inds = torch.abs(angle) < clip_angle_threshold\n        box[inds, 0] = (x1[inds] + x2[inds]) / 2\n        box[inds, 1] = (y1[inds] + y2[inds]) / 2\n        box[inds, 2] = width[inds]\n        box[inds, 3] = height[inds]\n        box[inds, 4] = 0.0"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        box = self.tensor\n        angle = box[:, 4]\n        inds = torch.abs(angle) <= clip_angle_threshold\n        box[inds, 2] = torch.clamp(box[inds, 2], max=box_size[1])\n        box[inds, 3] = torch.clamp(box[inds, 3], max=box_size[0])\n        x1 = box[:, 0] - box[:, 2] / 2\n        y1 = box[:, 1] - box[:, 3] / 2\n        x2 = box[:, 0] + box[:, 2] / 2\n        y2 = box[:, 1] + box[:, 3] / 2\n        box[:, 0] = torch.clamp(x1, max=box_size[1])\n        box[:, 1] = torch.clamp(y1, max=box_size[0])\n        box[:, 2] = torch.clamp(x2, min=0) - box[:, 0]\n        box[:, 3] = torch.clamp(y2, min=0) - box[:, 1]"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        box = self.tensor\n        widths = box[:, 2]\n        heights = box[:, 3]\n        x1 = box[:, 0] - widths / 2\n        y1 = box[:, 1] - heights / 2\n        x2 = box[:, 0] + widths / 2\n        y2 = box[:, 1] + heights / 2\n\n        # Identify the indices of the boxes that are nearly horizontal\n        # based on the clip_angle_threshold\n        angle_indices = torch.abs(box[:, 4]) < clip_angle_threshold\n\n        # Convert the representation from (center x, center y, width, height, angle)\n        # to (x1, y1, x2, y2)\n        x1[angle_indices] = torch.min(\n            torch.min(x1[angle_indices], y1[angle_indices]), torch.zeros_like(x1[angle_indices])\n        )\n        y1[angle_indices] = torch.min(\n            torch.min(y1[angle_indices], x2[angle_indices]), torch.zeros_like(y1[angle_indices])\n        )\n        x2[angle_indices] = torch.max(\n            torch.max(x2[angle_indices], y2[angle_indices]), torch.zeros_like(x2[angle_indices])\n        )\n        y2[angle_indices] = torch.max(\n            torch.max(y2[angle_indices], x1[angle_indices]), torch.zeros_like(y2[angle_indices])\n        )\n\n        # Clamp the x and y coordinates to ensure they do not exceed the specified box_size limits\n        box[:, 0] = torch.clamp(box[:, 0], min=0, max=box_size[1])\n        box[:, 1] = torch.clamp("}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        box = self.tensor\n\n        # Convert to (x1, y1, x2, y2) representation\n        x1 = box[:, 0] - box[:, 2] / 2\n        y1 = box[:, 1] - box[:, 3] / 2\n        x2 = box[:, 0] + box[:, 2] / 2\n        y2 = box[:, 1] + box[:, 3] / 2\n\n        # Clip the coordinates\n        x1 = torch.clamp(x1, min=0, max=box_size[1])\n        y1 = torch.clamp(y1, min=0, max=box_size[0])\n        x2 = torch.clamp(x2, min=0, max=box_size[1])\n        y2 = torch.clamp(y2, min=0, max=box_size[0])\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        box[:, 0] = (x1 + x2) / 2\n        box[:, 1] = (y1 + y2) / 2\n        box[:, 2] = x2 - x1\n        box[:, 3] = y2 - y1\n\n        # Clip the angles\n        angle = box[:, 4]\n        angle = torch.abs(angle)\n        angle = torch.where(angle < clip_angle_threshold, angle, torch.zeros_like(angle))\n        box[:, 4] = angle"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        box = self.tensor\n        x1 = box[:, 0] - box[:, 2] / 2\n        y1 = box[:, 1] - box[:, 3] / 2\n        x2 = box[:, 0] + box[:, 2] / 2\n        y2 = box[:, 1] + box[:, 3] / 2\n\n        # Clip\n        x1 = x1.clamp(min=0, max=box_size[1])\n        y1 = y1.clamp(min=0, max=box_size[0])\n        x2 = x2.clamp(min=0, max=box_size[1])\n        y2 = y2.clamp(min=0, max=box_size[0])\n\n        # Convert back to (x1, y1, x2, y2)\n        box[:, 0] = (x1 + x2) / 2\n        box[:, 1] = (y1 + y2) / 2\n        box[:, 2] = x2 - x1\n        box[:, 3] = y2 - y1\n\n        # Clip angles\n        angle_diff = torch.abs(box[:, 4])\n        inds = angle_diff < clip_angle_threshold\n        box[inds, 4] = 0.0"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        box = self.tensor\n        height, width = box_size\n        cnt_x = box[:, 0]\n        cnt_y = box[:, 1]\n        half_w = box[:, 2] / 2.0\n        half_h = box[:, 3] / 2.0\n        a = box[:, 4]\n        c = torch.cos(a * math.pi / 180.0)\n        s = torch.sin(a * math.pi / 180.0)\n        # This basically computes the horizontal bounding rectangle of the rotated box\n        max_rect_dx = c * half_w + s * half_h\n        max_rect_dy = c * half_h + s * half_w\n\n        # Identify the indices of boxes that are nearly horizontal\n        near_horiz_inds = torch.abs(a) < clip_angle_threshold\n\n        # Convert the representation from (center x, center y, width, height, angle)\n        # to (x1, y1, x2, y2)\n        x1 = cnt_x - max_rect_dx\n        y1 = cnt_y - max_rect_dy\n        x2 = cnt_x + max_rect_dx\n        y2 = cnt_y + max_rect_dy\n\n        # Clamp the x and y coordinates to ensure they do not exceed the specified box_size limits\n        x1 = torch.clamp(x1, min=0, max=width)\n        y1 = torch.clamp(y1, min=0, max=height)\n        x2 = torch.clamp(x2, min=0, max=width)\n        y2 = torch.clamp(y2, min=0, max=height)\n\n        # Convert the boxes back to their original representation\n        # (center x, center y, width, height, angle)\n        box[:, 0] = (x1 + x2) / 2.0\n        box[:, 1] = ("}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        box = self.tensor\n        x1 = box[:, 0] - box[:, 2] / 2.0\n        y1 = box[:, 1] - box[:, 3] / 2.0\n        x2 = box[:, 0] + box[:, 2] / 2.0\n        y2 = box[:, 1] + box[:, 3] / 2.0\n\n        # clip\n        x1 = x1.clamp(min=0, max=box_size[1])\n        y1 = y1.clamp(min=0, max=box_size[0])\n        x2 = x2.clamp(min=0, max=box_size[1])\n        y2 = y2.clamp(min=0, max=box_size[0])\n\n        # convert back to (x1, y1, x2, y2)\n        box[:, 0] = (x1 + x2) / 2\n        box[:, 1] = (y1 + y2) / 2\n        box[:, 2] = (x2 - x1)\n        box[:, 3] = (y2 - y1)\n\n        # clip again to avoid numerical errors\n        box[:, 2] = box[:, 2].clamp(min=0)\n        box[:, 3] = box[:, 3].clamp(min=0)\n\n        # convert back to (x_center, y_center, width, height, angle)\n        box[:, 0] = (box[:, 0] - box[:, 2] / 2).clamp(min=0, max=box_size[1])\n        box[:, 1] = (box[:, 1] - box[:, 3] / 2).clamp(min=0, max=box_size[0])\n        box[:, 2] = (box[:, 2]).clamp(min=0)\n        box[:, 3] = (box[:, 3]).clamp(min"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n\n        # Find the indices of nearly horizontal boxes\n        angle_threshold = clip_angle_threshold * math.pi / 180.0\n        angle_near_horizontal = torch.abs(self.tensor[:, 4] * math.pi / 180.0) < angle_threshold\n\n        # Convert the boxes to (x1, y1, x2, y2) format\n        x1 = self.tensor[:, 0] - self.tensor[:, 2] / 2.0\n        y1 = self.tensor[:, 1] - self.tensor[:, 3] / 2.0\n        x2 = self.tensor[:, 0] + self.tensor[:, 2] / 2.0\n        y2 = self.tensor[:, 1] + self.tensor[:, 3] / 2.0\n        boxes = torch.stack([x1, y1, x2, y2], dim=1)\n\n        # Clip the boxes to fit within the specified box size\n        boxes[:, 0::2].clamp_(min=0, max=box_size[1])\n        boxes[:, 1::2].clamp_(min=0, max=box_size[0])\n\n        # Convert the boxes back to (x_center, y_center, width, height, angle) format\n        x_center = (boxes[:, 0] + boxes[:, 2]) / 2.0\n        y_center = (boxes[:, 1] + boxes[:, 3]) / 2.0\n        width = boxes[:, 2] - boxes[:, 0]\n        height = boxes[:, 3] - boxes[:, 1]\n        angle = self.tensor[:, 4]\n\n        # Update the tensor attribute of the RotatedBoxes instance\n        self.tensor = torch.stack([x_center, y_center, width, height, angle], dim=1)"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n\n        # Find indices of nearly horizontal boxes\n        angle_threshold = clip_angle_threshold * math.pi / 180.0\n        angle = self.tensor[:, 4] * math.pi / 180.0\n        near_horizontal_indices = torch.abs(angle) < angle_threshold\n\n        # Convert to (x1, y1, x2, y2) format\n        x, y, w, h = self.tensor[:, 0], self.tensor[:, 1], self.tensor[:, 2], self.tensor[:, 3]\n        x1 = x - w / 2\n        y1 = y - h / 2\n        x2 = x + w / 2\n        y2 = y + h / 2\n\n        # Clip coordinates\n        height, width = box_size\n        x1 = torch.clamp(x1, min=0, max=width - 1)\n        y1 = torch.clamp(y1, min=0, max=height - 1)\n        x2 = torch.clamp(x2, min=0, max=width - 1)\n        y2 = torch.clamp(y2, min=0, max=height - 1)\n\n        # Convert back to (x, y, w, h, angle) format\n        x = (x1 + x2) / 2\n        y = (y1 + y2) / 2\n        w = x2 - x1\n        h = y2 - y1\n        angle = self.tensor[:, 4]\n\n        # Update tensor\n        self.tensor[:, 0] = x\n        self.tensor[:, 1] = y\n        self.tensor[:, 2] = w\n        self.tensor[:, 3] = h\n        self.tensor[:, 4] = angle\n\n        # Convert to (x1, y1, x2, y2) format for nearly horizontal boxes\n        x1 = x - w / 2\n        y1 = y - h / 2\n       "}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        box = self.tensor\n        x1 = box[:, 0] - box[:, 2] / 2\n        y1 = box[:, 1] - box[:, 3] / 2\n        x2 = box[:, 0] + box[:, 2] / 2\n        y2 = box[:, 1] + box[:, 3] / 2\n        x1, y1, x2, y2 = (\n            x1.clamp(min=0, max=box_size[1]),\n            y1.clamp(min=0, max=box_size[0]),\n            x2.clamp(min=0, max=box_size[1]),\n            y2.clamp(min=0, max=box_size[0]),\n        )\n        width = x2 - x1\n        height = y2 - y1\n        angle = box[:, 4]\n        angle_abs = torch.abs(angle)\n        angle_abs_lt_threshold = angle_abs < clip_angle_threshold\n        angle_abs_gt_threshold = angle_abs > clip_angle_threshold\n        angle_lt_threshold = angle.sign() * angle_abs_lt_threshold\n        angle_gt_threshold = angle.sign() * angle_abs_gt_threshold\n        angle_cos = torch.cos(angle_lt_threshold * math.pi / 180.0)\n        angle_sin = torch.sin(angle_lt_threshold * math.pi / 180.0)\n        width = width * angle_cos + height * angle_sin\n        height = height * angle_cos + width * angle_sin\n        angle = angle_gt_threshold\n        box[:, 0] = (x1 + x2) / 2\n        box[:, 1] = (y1 + y2) / 2\n        box[:, 2] = width\n        box[:, 3] = height\n        box[:, 4] = angle"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize angles to [-180, 180)\n        self.normalize_angles()\n\n        # Identify boxes that are nearly horizontal based on the clip_angle_threshold\n        angle_threshold = clip_angle_threshold * math.pi / 180.0\n        near_horizontal_indices = torch.abs(self.tensor[:, 4] * math.pi / 180.0) < angle_threshold\n\n        # Convert boxes to (x1, y1, x2, y2) representation\n        box = self.tensor\n        x1 = box[:, 0] - box[:, 2] / 2.0\n        y1 = box[:, 1] - box[:, 3] / 2.0\n        x2 = box[:, 0] + box[:, 2] / 2.0\n        y2 = box[:, 1] + box[:, 3] / 2.0\n        box = torch.stack([x1, y1, x2, y2], dim=1)\n\n        # Clip the boxes\n        box[:, 0] = torch.clamp(box[:, 0], min=0)\n        box[:, 1] = torch.clamp(box[:, 1], min=0)\n        box[:, 2] = torch.clamp(box[:, 2], max=box_size[1] - 1)\n        box[:, 3] = torch.clamp(box[:, 3], max=box_size[0] - 1)\n\n        # Convert boxes back to (center_x, center_y, width, height, angle) representation\n        box[:, 0] = (box[:, 0] + box[:, 2]) / 2.0\n        box[:, 1] = (box[:, 1] + box[:, 3]) / 2.0\n        box[:, 2] = box[:, 2] - box[:, 0]\n        box[:, 3] = box[:, 3] - box[:, "}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n\n        # Identify boxes that are nearly horizontal based on the angle threshold\n        angle_threshold = clip_angle_threshold * math.pi / 180.0\n        near_horizontal_indices = torch.abs(self.tensor[:, 4] * math.pi / 180.0) < angle_threshold\n\n        # Convert the boxes to (x1, y1, x2, y2) format\n        box = self.tensor\n        x1 = box[:, 0] - box[:, 2] / 2.0\n        y1 = box[:, 1] - box[:, 3] / 2.0\n        x2 = box[:, 0] + box[:, 2] / 2.0\n        y2 = box[:, 1] + box[:, 3] / 2.0\n        box = torch.stack((x1, y1, x2, y2), dim=-1)\n\n        # Clamp the coordinates to fit within the specified box size\n        box[:, [0, 2]] = box[:, [0, 2]].clamp(min=0, max=box_size[1])\n        box[:, [1, 3]] = box[:, [1, 3]].clamp(min=0, max=box_size[0])\n\n        # Convert the boxes back to (center x, center y, width, height, angle) format\n        box[:, 0] = (box[:, 0] + box[:, 2]) / 2.0\n        box[:, 1] = (box[:, 1] + box[:, 3]) / 2.0\n        box[:, 2] = box[:, 2] - box[:, 0]\n        box[:, 3] = box[:, 3] - box[:, 1]\n\n        # Convert the angles back to the original range\n        self.tensor[:, 4] = (self.tensor[:, 4] + 180.0) % 360.0 - 180.0"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        box = self.tensor\n        x1 = box[:, 0] - box[:, 2] / 2.0\n        y1 = box[:, 1] - box[:, 3] / 2.0\n        x2 = box[:, 0] + box[:, 2] / 2.0\n        y2 = box[:, 1] + box[:, 3] / 2.0\n        x1, y1, x2, y2 = _clip_boxes(x1, y1, x2, y2, box_size)\n        box[:, 0] = (x1 + x2) / 2.0\n        box[:, 1] = (y1 + y2) / 2.0\n        box[:, 2] = x2 - x1\n        box[:, 3] = y2 - y1\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        box = self.tensor\n        widths = box[:, 2]\n        heights = box[:, 3]\n        x1 = box[:, 0] - 0.5 * widths\n        y1 = box[:, 1] - 0.5 * heights\n        x2 = box[:, 0] + 0.5 * widths\n        y2 = box[:, 1] + 0.5 * heights\n        x1_t, y1_t, x2_t, y2_t = x1, y1, x2, y2\n        angle = box[:, 4]\n        angle_mask = torch.abs(angle) < clip_angle_threshold\n        angle_mask.squeeze_()\n        x1, y1, x2, y2 = (\n            torch.where(angle_mask, x1, x1_t),\n            torch.where(angle_mask, y1, y1_t),\n            torch.where(angle_mask, x2, x2_t),\n            torch.where(angle_mask, y2, y2_t),\n        )\n        max_h, max_w = box_size\n        x1 = x1.clamp_(min=0, max=max_w)\n        y1 = y1.clamp_(min=0, max=max_h)\n        x2 = x2.clamp_(min=0, max=max_w)\n        y2 = y2.clamp_(min=0, max=max_h)\n        widths = x2 - x1\n        heights = y2 - y1\n        box[:, 0] = (x1 + x2) / 2\n        box[:, 1] = (y1 + y2) / 2\n        box[:, 2] = widths\n        box[:, 3] = heights\n        box[:, 4] = torch.where(angle_mask, angle, box[:, 4])"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n\n        box = self.tensor\n        widths = box[:, 2]\n        heights = box[:, 3]\n        x_ctr = box[:, 0]\n        y_ctr = box[:, 1]\n        angles = box[:, 4]\n\n        # Identify the indices of boxes that are nearly horizontal\n        # (i.e., with angles close to 0 or 180 degrees)\n        near_horizontal_indices = torch.abs(angles) < clip_angle_threshold\n\n        # Convert the representation from (center x, center y, width, height, angle)\n        # to (x1, y1, x2, y2)\n        x1 = x_ctr - widths / 2.0\n        y1 = y_ctr - heights / 2.0\n        x2 = x_ctr + widths / 2.0\n        y2 = y_ctr + heights / 2.0\n\n        # Clamp the x and y coordinates to ensure they do not exceed the specified box_size limits\n        x1 = torch.clamp(x1, min=0, max=box_size[1])\n        y1 = torch.clamp(y1, min=0, max=box_size[0])\n        x2 = torch.clamp(x2, min=0, max=box_size[1])\n        y2 = torch.clamp(y2, min=0, max=box_size[0])\n\n        # Convert the boxes back to their original representation\n        # (x1, y1, x2, y2) -> (center x, center y, width, height, angle)\n        x_ctr = (x1 + x2) / 2.0\n        y_ctr = (y1 + y2) / 2.0\n        widths = x2 - x1\n        heights = y2 - y1\n        angles = angles.clone()\n\n        # Set the angles of nearly horizontal boxes to 0\n        angles[near_horizontal_indices] = 0.0\n\n        # Update the tensor attribute"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        box = self.tensor\n        widths = box[:, 2]\n        heights = box[:, 3]\n        cnt_x = box[:, 0]\n        cnt_y = box[:, 1]\n        theta = box[:, 4] * math.pi / 180.0\n        c = torch.cos(theta)\n        s = torch.sin(theta)\n        # This basically computes the horizontal bounding rectangle of the rotated box\n        max_rect_dx = c * widths / 2.0 + s * heights / 2.0\n        max_rect_dy = c * heights / 2.0 - s * widths / 2.0\n        # This basically computes the vertical bounding rectangle of the rotated box\n        max_rect_dx_v = c * heights / 2.0 + s * widths / 2.0\n        max_rect_dy_v = c * widths / 2.0 - s * heights / 2.0\n        # This basically computes the diagonal bounding rectangle of the rotated box\n        max_rect_dx_d = torch.max(c * widths / 2.0 + s * heights / 2.0, c * heights / 2.0 + s * widths / 2.0)\n        max_rect_dy_d = torch.max(c * heights / 2.0 - s * widths / 2.0, c * widths / 2.0 - s * heights / 2.0)\n        # This basically computes the diagonal bounding rectangle of the rotated box\n        # when the angle is a multiple of 90 degrees\n        max_rect_dx_d_90 = torch.max(c * widths, s * heights)\n        max_rect_dy_d_90 = torch.max(c * heights, -s * widths)\n\n        # Find the indices of the boxes that are nearly horizontal\n        # (i.e., the angle is within the specified clip_angle_threshold)\n        inds_horiz"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        box = self.tensor\n        box_size = torch.tensor(\n            box_size, dtype=box.dtype, device=box.device\n        )  # type: torch.Tensor\n        box_size = box_size.view(1, 2).expand_as(box[:, :2])\n\n        # Identify the indices of the boxes that are nearly horizontal\n        near_horizontal_inds = torch.abs(box[:, 4]) < clip_angle_threshold\n\n        # Convert the representation from (center x, center y, width, height, angle)\n        # to (x1, y1, x2, y2)\n        x1y1 = box[:, :2] - box[:, 2:] / 2.0\n        x2y2 = box[:, :2] + box[:, 2:] / 2.0\n        x1y1 = torch.min(torch.max(x1y1, torch.zeros_like(x1y1)), box_size - 1)\n        x2y2 = torch.min(torch.max(x2y2, torch.zeros_like(x2y2)), box_size - 1)\n\n        # Clamp the x and y coordinates to ensure they do not exceed the specified box_size limits\n        box[:, :2] = torch.min(torch.max(box[:, :2], x1y1), x2y2)\n        box[:, 2:] = torch.clamp(box[:, 2:], min=0)\n\n        # Convert the boxes back to their original representation\n        # (x1, y1, x2, y2) -> (center x, center y, width, height, angle)\n        box[:, :2] = (box[:, :2] + box[:, 2:]) / 2.0\n        box[:, 2:] = box[:, 2:] - box[:, :2]\n\n        # Adjust the angles of the boxes that were nearly horizontal\n        near_horizontal_inds"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        stats = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n        for item in self.data:\n            stats[item['type']] += 1\n        return stats"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        stats = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n        for item in self.data:\n            stats[item['type']] += 1\n        return stats"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        stats = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n        for item in self.data:\n            stats[item['type']] += 1\n        return stats"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        stats = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n        for item in self.data:\n            stats[item['type']] += 1\n        return stats"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n        for item in self.data:\n            statistics[item['type']] += 1\n        return statistics"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        stats = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0,\n        }\n\n        for item in self.data:\n            stats[item['type']] += 1\n\n        return stats"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n\n        for item in self.data:\n            statistics[item['type']] += 1\n\n        return statistics"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0,\n        }\n\n        for item in self.data:\n            statistics[item['type']] += 1\n\n        return statistics"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        stats = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0,\n        }\n\n        for item in self.data:\n            stats[item['type']] += 1\n\n        return stats"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n        for item in self.data:\n            statistics[item['type']] += 1\n        return statistics"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        stats = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n\n        for d in self.data:\n            stats[d['type']] += 1\n\n        return stats"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        types = ['doc', 'gen', 'kno', 'num']\n        stats = {t: 0 for t in types}\n        for d in self.data:\n            stats[d['type']] += 1\n        return stats"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        stats = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0,\n        }\n        for d in self.data:\n            stats[d['type']] += 1\n        return stats"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        stats = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0,\n        }\n        for d in self.data:\n            stats[d['type']] += 1\n        return stats"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        stats = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n        for item in self.data:\n            stats[item['type']] += 1\n        return stats"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n        for data in self.data:\n            statistics[data['type']] += 1\n        return statistics"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n        for item in self.data:\n            statistics[item['type']] += 1\n        return statistics"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        stat = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n        for d in self.data:\n            stat[d['type']] += 1\n        return stat"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        stats = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n        for obj in self.data:\n            stats[obj['type']] += 1\n        return stats"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        stats = {}\n        for sample in self.data:\n            stats[sample['type']] = stats.get(sample['type'], 0) + 1\n        return stats"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMSEG_LOSSES.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMSEG_LOSSES.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        if cfg['type'] in MMDET_LOSSES._module_dict.keys():\n            return MMDET_LOSSES.build(cfg)\n        elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n            return MMSEG_LOSSES.build(cfg)\n        else:\n            raise NotImplementedError"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        if cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n            return MMSEG_LOSSES.build(cfg)\n        else:\n            return MMDET_LOSSES.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMSEG_SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        raise NotImplementedError"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMSEG_SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMSEG_SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        raise NotImplementedError"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMSEG_MODELS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    "}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    "}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    "}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    "}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    "}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    "}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    "}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    "}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    "}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": ""}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": ""}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": ""}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    "}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    \"\"\"\n    This function evaluates the detection results by comparing the detection annotations with the ground truth annotations. It calculates the mean Average Precision (mAP) and mean Average Recall (mAR) for different IoU thresholds and organizes the evaluation results into a dictionary and a formatted table for logging.\n\n    Input-Output Arguments\n    :param gt_annos: list[dict]. Ground truth annotations containing information about the true bounding boxes and their labels.\n    :param dt_annos: list[dict]. Detection annotations containing detected bounding boxes, their labels, and scores.\n    :param metric: list[float]. A list of IoU thresholds used for computing average precisions and recalls.\n    :param label2cat: dict. A mapping from numerical labels to category names.\n    :param logger: logging.Logger or str, optional. Specifies how the mAP summary is printed. If None, the default logging method is used.\n    :param box_type_3d: class, optional. The type of 3D bounding box used, which determines how the boxes are converted and processed.\n    :param box_mode_3d: str, optional. The mode or format in which the 3D bounding boxes are represented.\n    :return: dict[str, float]. A dictionary containing the evaluation results, including class-wise AP and AR for each IoU threshold, as well as overall mAP and mAR.\n\n    \"\"\"\n\n    \"\"\"\n    This function evaluates the detection results by comparing the detection annotations with the ground truth annotations. It calculates the mean Average Precision (mAP) and mean Average Recall (mAR) for different IoU thresholds and organizes the evaluation results into a dictionary and a formatted table for logging.\n\n    Input-Output Arguments\n    :param gt_annos: list[dict]. Ground truth annotations containing information about the true bounding boxes and their labels.\n    :param dt_annos: list[dict]. Detection annotations containing detected bounding boxes, their labels, and scores.\n    :param metric: list[float]. A list of IoU thresholds used for computing average precisions and recalls."}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    # "}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    from .indoor_eval_utils import indoor_eval_utils\n    return indoor_eval_utils.indoor_eval(gt_annos, dt_annos, metric, label2cat, logger, box_type_3d, box_mode_3d)"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    # get gt and dt\n    gt_annos = [\n        gt_anno.copy() for gt_anno in gt_annos\n    ]\n    dt_annos = [\n        dt_anno.copy() for dt_anno in dt_annos\n    ]\n\n    # filter gt and dt\n    if len(gt_annos) > 0 and isinstance(gt_annos[0], dict):\n        gt_annos = [gt_anno for gt_anno in gt_annos if gt_anno['name'] in label2cat]\n    if len(dt_annos) > 0 and isinstance(dt_annos[0], dict):\n        dt_annos = [\n            dt_anno for dt_anno in dt_annos if dt_anno['name'] in label2cat\n        ]\n\n    # get gt and dt\n    gt_annos = [\n        gt_anno.copy() for gt_anno in gt_annos\n    ]\n    dt_annos = [\n        dt_anno.copy() for dt_anno in dt_annos\n    ]\n\n    # filter gt and dt\n    if len(gt_annos) > 0 and isinstance(gt_annos[0], dict):\n        gt_annos = [gt_anno for gt_anno in gt_annos if gt_anno['name'] in label2cat]\n    if len(dt_annos) > 0 and isinstance(dt_annos[0], dict):\n        dt_annos = [\n            dt_anno for dt_anno in dt_annos if dt_anno['name'] in label2cat\n        ]\n\n    # get gt and dt\n    gt_annos = [\n        gt_anno.copy() for gt_anno in gt_annos\n    ]\n    dt_annos = [\n        dt_anno.copy() for dt_anno in dt_annos\n    ]\n\n    # filter gt"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    # gt_annos = [ann for ann in gt_annos if ann['name'] in label2cat]\n    # dt_annos = [ann for ann in dt_annos if ann['name'] in label2cat]\n\n    # if len(dt_annos) == 0:\n    #     return None\n\n    # gt_annos = [ann for ann in gt_annos if ann['name'] in label2cat]\n    # dt_annos = [ann for ann in dt_annos if ann['name'] in label2cat]\n\n    # if len(dt_annos) == 0:\n    #     return None\n\n    # gt_annos = [ann for ann in gt_annos if ann['name'] in label2cat]\n    # dt_annos = [ann for ann in dt_annos if ann['name'] in label2cat]\n\n    # if len(dt_annos) == 0:\n    #     return None\n\n    # gt_annos = [ann for ann in gt_annos if ann['name'] in label2cat]\n    # dt_annos = [ann for ann in dt_annos if ann['name'] in label2cat]\n\n    # if len(dt_annos) == 0:\n    #     return None\n\n    # gt_annos = [ann for ann in gt_annos if ann['name'] in label2cat]\n    # dt_annos = [ann for ann in dt_annos if ann['name'] in label2cat]\n\n    # if len(dt_annos) == 0:\n    #     return None\n\n    # gt_annos = [ann for ann in gt_annos if ann['name'] in label2cat]\n    # dt_annos = [ann for ann in dt_annos if ann['name'] in label2cat]\n\n    # if len(dt_annos) == 0:\n    #     return None\n\n    # gt_annos = [ann for ann in gt_annos if ann['name'] in label2cat]\n    #"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    # convert ground truth and detection annotations to 3D boxes\n    gt_bboxes_3d = [anno['gt_bboxes_3d'] for anno in gt_annos]\n    dt_bboxes_3d = [anno['dt_bboxes_3d'] for anno in dt_annos]\n    gt_labels_3d = [anno['gt_labels_3d'] for anno in gt_annos]\n    dt_labels_3d = [anno['dt_labels_3d'] for anno in dt_annos]\n    dt_scores_3d = [anno['dt_scores_3d'] for anno in dt_annos]\n\n    # convert 3D boxes to 2D boxes\n    gt_bboxes = [\n        box_type_3d(\n            box=box.tensor, box_mode=box_mode_3d).convert_to(BoxMode.XYXY_ABS)\n        for box in gt_bboxes_3d\n    ]\n    dt_bboxes = [\n        box_type_3d(\n            box=box.tensor, box_mode=box_mode_3d).convert_to(BoxMode.XYXY_ABS)\n        for box in dt_bboxes_3d\n    ]\n\n    # evaluate the detection results using the provided metric\n    eval_results = eval_map_recall(\n        dt_bboxes,\n        gt_bboxes,\n        ovthresh=metric,\n        logger=logger,\n        label2cat=label2cat)\n\n    # extract the evaluation results from the dictionary\n    recall = eval_results[0]\n    precision = eval_results[1]\n    ap = eval_results[2]\n\n    # calculate the mean average precision (mAP) and mean average recall (mAR)\n    mAP = sum(ap.values()) / len(ap)\n    mAR = sum(recall.values()) / len(recall)\n\n    # format the evaluation results as a"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    # evaluate 3d detection\n    # TODO: support 3d detection\n    if box_type_3d is not None:\n        raise NotImplementedError\n\n    if box_mode_3d is not None:\n        raise NotImplementedError\n\n    # evaluate 2d detection\n    else:\n        eval_results = eval_map_recall(dt_annos, gt_annos, metric)\n\n    # summarize the results\n    summary = {}\n    summary['mAP'] = np.mean([x[0] for x in eval_results[0][0].values()])\n    summary['mAR'] = np.mean([x[0] for x in eval_results[0][1].values()])\n    for i, x in enumerate(eval_results[0][0].values()):\n        summary['AP/{}'.format(metric[i])] = x[2]\n    for i, x in enumerate(eval_results[0][1].values()):\n        summary['AR/{}'.format(metric[i])] = x[2]\n\n    # format results\n    results = OrderedDict()\n    results['mAP'] = summary['mAP']\n    results['mAR'] = summary['mAR']\n    for i, x in enumerate(eval_results[0][0].values()):\n        results['AP/{}'.format(metric[i])] = x[2]\n    for i, x in enumerate(eval_results[0][1].values()):\n        results['AR/{}'.format(metric[i])] = x[2]\n\n    # format table\n    table_header = ['mAP', 'mAR'] + ['AP/{}'.format(x) for x in metric] + [\n        'AR/{}'.format(x) for x in metric\n    ]\n    table_data = [[summary[x] for x in table_header]]\n    table = AsciiTable(table_data, table_header)\n\n    # print the results\n    print_log('\\n' + table.table, logger=logger)"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        box_cls = LiDARInstance3DBoxes\n        mode = 'lidar'\n    elif box_type == 'Camera':\n        box_cls = CameraInstance3DBoxes\n        mode = 'camera'\n    elif box_type == 'Depth':\n        box_cls = DepthInstance3DBoxes\n        mode = 'depth'\n    else:\n        raise ValueError(f'box_type: {box_type} is not supported.')\n\n    return box_cls, mode"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        box_cls = LiDARInstance3DBoxes\n        mode = 'lidar'\n    elif box_type == 'Camera':\n        box_cls = CameraInstance3DBoxes\n        mode = 'camera'\n    elif box_type == 'Depth':\n        box_cls = DepthInstance3DBoxes\n        mode = 'depth'\n    else:\n        raise ValueError(f'box type {box_type} is not supported')\n    return box_cls, mode"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        box_cls = LiDARInstance3DBoxes\n        mode = 'lidar'\n    elif box_type == 'Camera':\n        box_cls = CameraInstance3DBoxes\n        mode = 'camera'\n    elif box_type == 'Depth':\n        box_cls = DepthInstance3DBoxes\n        mode = 'depth'\n    else:\n        raise ValueError(f'box_type: {box_type} is not supported')\n    return box_cls, mode"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        box_cls = LiDARInstance3DBoxes\n        mode = 'lidar'\n    elif box_type == 'Camera':\n        box_cls = CameraInstance3DBoxes\n        mode = 'camera'\n    elif box_type == 'Depth':\n        box_cls = DepthInstance3DBoxes\n        mode = 'depth'\n    else:\n        raise ValueError(f'box_type {box_type} is not supported.')\n\n    return box_cls, mode"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        box_cls = LiDARInstance3DBoxes\n        mode = 'lidar'\n    elif box_type == 'Camera':\n        box_cls = CameraInstance3DBoxes\n        mode = 'camera'\n    elif box_type == 'Depth':\n        box_cls = DepthInstance3DBoxes\n        mode = 'depth'\n    else:\n        raise ValueError(f'box_type must be LiDAR, Camera or Depth, '\n                         f'but got {box_type}')\n\n    return box_cls, mode"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        box_cls = LiDARInstance3DBoxes\n        box_mode = 'lidar'\n    elif box_type == 'Camera':\n        box_cls = CameraInstance3DBoxes\n        box_mode = 'camera'\n    elif box_type == 'Depth':\n        box_cls = DepthInstance3DBoxes\n        box_mode = 'depth'\n    else:\n        raise ValueError(f'box_type: {box_type} is not supported')\n\n    return box_cls, box_mode"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        box_cls = LiDARInstance3DBoxes\n        mode = 'lidar'\n    elif box_type == 'Camera':\n        box_cls = CameraInstance3DBoxes\n        mode = 'camera'\n    elif box_type == 'Depth':\n        box_cls = DepthInstance3DBoxes\n        mode = 'depth'\n    else:\n        raise ValueError(f'Invalid box type: {box_type}')\n    return box_cls, mode"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        box_cls = LiDARInstance3DBoxes\n        mode = 'lidar'\n    elif box_type == 'Camera':\n        box_cls = CameraInstance3DBoxes\n        mode = 'camera'\n    elif box_type == 'Depth':\n        box_cls = DepthInstance3DBoxes\n        mode = 'depth'\n    else:\n        raise ValueError(f'box_type: {box_type} is not defined')\n    return box_cls, mode"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    box_type = box_type.lower()\n    if box_type == 'lidar':\n        return LiDARInstance3DBoxes, 'lidar'\n    elif box_type == 'camera':\n        return CameraInstance3DBoxes, 'camera'\n    elif box_type == 'depth':\n        return DepthInstance3DBoxes, 'depth'\n    else:\n        raise ValueError(f'box_type: {box_type} is not supported.')"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    box_type = box_type.lower()\n    if box_type == 'lidar':\n        box_cls = LiDARInstance3DBoxes\n        box_mode = 'lidar'\n    elif box_type == 'camera':\n        box_cls = CameraInstance3DBoxes\n        box_mode = 'camera'\n    elif box_type == 'depth':\n        box_cls = DepthInstance3DBoxes\n        box_mode = 'depth'\n    else:\n        raise ValueError(f'box_type must be \"lidar\", \"camera\" or \"depth\", '\n                         f'but got {box_type}')\n    return box_cls, box_mode"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        from . import LiDARInstance3DBoxes\n        return LiDARInstance3DBoxes, 'lidar'\n    elif box_type == 'Camera':\n        from . import CameraInstance3DBoxes\n        return CameraInstance3DBoxes, 'camera'\n    elif box_type == 'Depth':\n        from . import DepthInstance3DBoxes\n        return DepthInstance3DBoxes, 'depth'\n    else:\n        raise NotImplementedError(f'box type {box_type} is not supported')"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        box_class = LiDARInstance3DBoxes\n        mode = 'lidar'\n    elif box_type == 'Camera':\n        box_class = CameraInstance3DBoxes\n        mode = 'camera'\n    elif box_type == 'Depth':\n        box_class = DepthInstance3DBoxes\n        mode = 'depth'\n    else:\n        raise ValueError(f'box_type: {box_type} not supported')\n\n    return box_class, mode"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        return LiDARInstance3DBoxes, 'lidar'\n    elif box_type == 'Camera':\n        return CameraInstance3DBoxes, 'cam'\n    elif box_type == 'Depth':\n        return DepthInstance3DBoxes, 'depth'\n    else:\n        raise ValueError(f'Unsupported box type: {box_type}')"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    box_type = box_type.lower()\n    if box_type == 'lidar':\n        return LiDARInstance3DBoxes, 'LIDAR'\n    elif box_type == 'camera':\n        return CameraInstance3DBoxes, 'CAM'\n    elif box_type == 'depth':\n        return DepthInstance3DBoxes, 'DEPTH'\n    else:\n        raise ValueError('box_type {} is not defined'.format(box_type))"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        box_cls = LiDARInstance3DBoxes\n        mode = 'lidar'\n    elif box_type == 'Camera':\n        box_cls = CameraInstance3DBoxes\n        mode = 'camera'\n    elif box_type == 'Depth':\n        box_cls = DepthInstance3DBoxes\n        mode = 'depth'\n    else:\n        raise ValueError('box_type must be one of \"LiDAR\", \"Camera\", \"Depth\"')\n    return box_cls, mode"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        from .lidar_box3d import LiDARInstance3DBoxes\n        return LiDARInstance3DBoxes, 'lidar'\n    elif box_type == 'Camera':\n        from .cam_box3d import CameraInstance3DBoxes\n        return CameraInstance3DBoxes, 'camera'\n    elif box_type == 'Depth':\n        from .depth_box3d import DepthInstance3DBoxes\n        return DepthInstance3DBoxes, 'depth'\n    else:\n        raise ValueError(f'box_type must be LiDAR, Camera or Depth, '\n                         f'but got {box_type}')"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        box_cls = LiDARInstance3DBoxes\n        mode = 'lidar'\n    elif box_type == 'Camera':\n        box_cls = CameraInstance3DBoxes\n        mode = 'camera'\n    elif box_type == 'Depth':\n        box_cls = DepthInstance3DBoxes\n        mode = 'depth'\n    else:\n        raise ValueError(f'Invalid box_type: {box_type}.')\n    return box_cls, mode"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        from . import LiDARInstance3DBoxes\n        return LiDARInstance3DBoxes, 'lidar'\n    elif box_type == 'Camera':\n        from . import CameraInstance3DBoxes\n        return CameraInstance3DBoxes, 'camera'\n    elif box_type == 'Depth':\n        from . import DepthInstance3DBoxes\n        return DepthInstance3DBoxes, 'depth'\n    else:\n        raise NotImplementedError(\n            'box_type {} is not defined'.format(box_type))"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        box_cls = LiDARInstance3DBoxes\n        box_mode = 'lidar'\n    elif box_type == 'Camera':\n        box_cls = CameraInstance3DBoxes\n        box_mode = 'img'\n    elif box_type == 'Depth':\n        box_cls = DepthInstance3DBoxes\n        box_mode = 'depth'\n    else:\n        raise NotImplementedError(\n            f'box type {box_type} is not defined')\n    return box_cls, box_mode"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    from . import (CameraInstance3DBoxes, DepthInstance3DBoxes,\n                   LiDARInstance3DBoxes)\n    box_type = box_type.upper()\n    if box_type == 'LIDAR':\n        box_class = LiDARInstance3DBoxes\n        mode = 'lidar'\n    elif box_type == 'CAMERA':\n        box_class = CameraInstance3DBoxes\n        mode = 'camera'\n    elif box_type == 'DEPTH':\n        box_class = DepthInstance3DBoxes\n        mode = 'depth'\n    else:\n        raise ValueError(f'box type {box_type} is not defined.')\n    return box_class, mode"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if messages:\n      for message in messages:\n        if not isinstance(message, dict):\n          raise TypeError('messages must be a list of strings')\n        if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n          raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n        if not message.get('content'):\n          raise RequestError('messages must contain content')\n        if images := message.get('images'):\n          message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list of strings')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if not messages:\n      messages = []\n\n    for message in messages:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if not messages:\n      messages = []\n\n    for message in messages:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if not messages:\n      messages = []\n    elif not isinstance(messages, list):\n      raise TypeError('messages must be a list of strings')\n    for message in messages:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if not isinstance(messages, Sequence):\n      raise TypeError('messages must be a list of strings')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n            break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest\n\n  "}
{"namespace": "ollama._client.Client._create_blob", "completion": "    # Calculate the SHA-256 checksum of the file\n    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n            break\n        sha256sum.update(chunk)\n\n    # Calculate the digest of the file\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    # Check if the blob already exists on the server\n    try:\n        self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n        if e.status_code != 404:\n            raise\n\n        # Upload the file as a new blob\n        def upload_bytes():\n            with open(path, 'rb') as r:\n                while True:\n                    chunk = r.read(32 * 1024)\n                    if not chunk:\n                        break\n                    yield chunk\n\n        self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    # Calculate the SHA-256 checksum of the file\n    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n            break\n        sha256sum.update(chunk)\n\n    # Get the SHA-256 digest of the file\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    # Check if the blob already exists\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      # Upload the file as a new blob\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n            break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n            break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n            break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n        await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n        if e.status_code != 404:\n            raise\n\n        with open(path, 'rb') as r:\n            await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n            break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n            break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n        await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n        if e.status_code != 404:\n            raise\n\n        with open(path, 'rb') as r:\n            await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n            break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n            break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n        await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n        if e.status_code != 404:\n            raise\n\n        with open(path, 'rb') as r:\n            await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n            break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n        await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n        if e.status_code != 404:\n            raise\n\n        with open(path, 'rb') as r:\n            await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n            break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n            break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n            break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n        response = await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n        if e.status_code != 404:\n            raise\n\n        with open(path, 'rb') as r:\n            await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n            break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n        if e.status_code != 404:\n            raise\n\n        with open(path, 'rb') as r:\n            await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n            break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n        response = await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n        if e.status_code != 404:\n            raise\n\n        with open(path, 'rb') as r:\n            response = await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    async with aiofiles.open(path, 'rb') as r:\n      while True:\n        chunk = await r.read(32 * 1024)\n        if not chunk:\n            break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      async with aiofiles.open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=await r.read())\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n            break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n        response = await self._request('HEAD', f'/api/blobs/{digest}')\n        if response.status_code == 200:\n            return digest\n    except ResponseError as e:\n        if e.status_code != 404:\n            raise\n\n    with open(path, 'rb') as r:\n        response = await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(user_code)\n            f.write(test_code)\n            file_path = f.name\n\n        # Run Pyright to perform type checking\n        try:\n            output = subprocess.check_output(\n                [\"pyright\", \"--noEmit\", file_path],\n                stderr=subprocess.STDOUT,\n                universal_newlines=True,\n            )\n        except subprocess.CalledProcessError as e:\n            output = e.output\n\n        # Parse the Pyright output to extract the expected type errors\n        expected_errors = []\n        for line in output.splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_no = int(match.group(1))\n                message = match.group(2)\n                if cls.EXPECT_ERROR_COMMENT in message:\n                    expected_errors.append(line_no)\n\n        # Remove the temporary file\n        Path(file_path).unlink()\n\n        # Return the result of the type check\n        return TypeCheckResult(\n            message=output,\n            passed=not expected_errors,\n            debug_info={\"expected_errors\": expected_errors},\n        )"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", encoding=\"utf-8\") as f:\n            f.write(user_code)\n            f.write(cls.CODE_SPLITTER)\n            f.write(test_code)\n            f.flush()\n\n            # Run Pyright\n            result = subprocess.run(\n                [\"pyright\", \"--outputjson\", f.name],\n                capture_output=True,\n                text=True,\n            )\n\n        # Parse Pyright output\n        messages = []\n        for line in result.stdout.splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                messages.append(match.group(2))\n\n        # Check if there are any expected errors\n        expected_errors = [\n            message for message in messages if cls.EXPECT_ERROR_COMMENT in message\n        ]\n        passed = not expected_errors\n\n        return TypeCheckResult(\n            message=\"\\n\".join(messages),\n            passed=passed,\n            debug_info={\"user_code\": user_code, \"test_code\": test_code},\n        )"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine user code and test code\n        code = user_code + cls.CODE_SPLITTER + test_code\n\n        # Create a temporary file to store the code\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(code)\n            code_file = f.name\n\n        # Run Pyright to perform the type check\n        result = subprocess.run(\n            [\"pyright\", \"--outputjson\", code_file],\n            capture_output=True,\n            text=True,\n        )\n\n        # Parse the Pyright output to extract error messages and line numbers\n        error_lines = []\n        for line in result.stdout.splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_no = int(match.group(1))\n                message = match.group(2)\n                error_lines.append((line_no, message))\n\n        # Find the line numbers of expected type errors\n        expected_error_lines = [\n            line_no\n            for line_no, message in error_lines\n            if cls.EXPECT_ERROR_COMMENT in message\n        ]\n\n        # Determine if the type check passed or failed\n        passed = not expected_error_lines\n\n        # Create a result object\n        return TypeCheckResult(\n            message=\"Passed\" if passed else \"Failed\",\n            passed=passed,\n            debug_info={\"expected_error_lines\": expected_error_lines},\n        )"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\") as f:\n            f.write(user_code)\n            f.write(test_code)\n            f.flush()\n\n            try:\n                output = subprocess.check_output(\n                    [\"pyright\", f.name],\n                    stderr=subprocess.STDOUT,\n                    text=True,\n                )\n            except subprocess.CalledProcessError as e:\n                return TypeCheckResult(\n                    message=f\"Pyright type check failed with error: {e.output}\",\n                    passed=False,\n                )\n\n        errors = []\n        for line in output.splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if not match:\n                continue\n\n            line_no, message = match.groups()\n            if cls.EXPECT_ERROR_COMMENT in message:\n                errors.append(line_no)\n\n        if errors:\n            return TypeCheckResult(\n                message=f\"Expected type errors on lines {', '.join(errors)}\",\n                passed=False,\n            )\n\n        return TypeCheckResult(\n            message=\"Type check passed\",\n            passed=True,\n        )"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary file for the user code\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as user_file:\n            user_file.write(user_code)\n            user_file_path = Path(user_file.name)\n\n        # Create a temporary file for the test code\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as test_file:\n            test_file.write(test_code)\n            test_file_path = Path(test_file.name)\n\n        # Run Pyright with the user code and test code\n        try:\n            output = subprocess.check_output(\n                [\"pyright\", \"--outputjson\", user_file_path, test_file_path],\n                stderr=subprocess.STDOUT,\n            )\n        except subprocess.CalledProcessError as e:\n            output = e.output\n\n        # Parse the Pyright output to extract the expected type error lines\n        expected_error_lines = set()\n        for line in output.decode(\"utf-8\").splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_number = int(match.group(1))\n                message = match.group(2)\n                if cls.EXPECT_ERROR_COMMENT in message:\n                    expected_error_lines.add(line_number)\n\n        # Remove the temporary files\n        user_file_path.unlink()\n        test_file_path.unlink()\n\n        # Return the result of the type check\n        passed = not expected_error_lines\n        message = \"Type check passed.\" if passed else \"Type check failed.\"\n        return TypeCheckResult(message=message, passed=passed)"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(user_code)\n            f.write(test_code)\n            f.close()\n            command = [\"pyright\", f.name]\n            result = subprocess.run(\n                command,\n                capture_output=True,\n                encoding=\"utf-8\",\n            )\n\n        # Extract the expected errors from the pyright output\n        expected_errors = []\n        for line in result.stdout.splitlines():\n            match = re.search(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_no, message = match.groups()\n                if cls.EXPECT_ERROR_COMMENT in message:\n                    expected_errors.append(int(line_no))\n\n        # Check if the type check passed\n        passed = result.returncode == 0 and not expected_errors\n\n        # Return the result\n        return TypeCheckResult(\n            message=result.stdout, passed=passed, debug_info={\"expected_errors\": expected_errors}\n        )"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\") as f:\n            f.write(user_code)\n            f.write(test_code)\n            f.flush()\n\n            # Run pyright\n            try:\n                output = subprocess.check_output(\n                    [\"pyright\", \"--outputjson\", f.name],\n                    stderr=subprocess.STDOUT,\n                    text=True,\n                )\n            except subprocess.CalledProcessError as e:\n                return TypeCheckResult(\n                    message=f\"Pyright returned non-zero exit code: {e.returncode}\",\n                    passed=False,\n                )\n\n        # Parse the output\n        errors = []\n        for line in output.splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_no = int(match.group(1))\n                message = match.group(2)\n                errors.append((line_no, message))\n\n        # Find the expected errors\n        expected_errors = []\n        for line_no, message in errors:\n            if cls.EXPECT_ERROR_COMMENT in message:\n                expected_errors.append((line_no, message))\n\n        # Check if the expected errors are present\n        passed = len(expected_errors) == len(errors)\n\n        # Return the result\n        return TypeCheckResult(\n            message=\"\\n\".join(f\"{line_no}: {message}\" for line_no, message in errors),\n            passed=passed,\n            debug_info={\"expected_errors\": expected_errors},\n        )"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(user_code)\n            f.write(test_code)\n            f.flush()\n\n            # Run Pyright to perform type checking\n            result = subprocess.run(\n                [\"pyright\", f.name],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.STDOUT,\n                encoding=\"utf-8\",\n            )\n\n        # Parse the Pyright output to extract error messages and line numbers\n        errors = []\n        for line in result.stdout.splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_no = int(match.group(1))\n                message = match.group(2)\n                if cls.EXPECT_ERROR_COMMENT in message:\n                    errors.append((line_no, message))\n\n        # Check if any expected errors were found\n        if errors:\n            return TypeCheckResult(\n                message=f\"{len(errors)} error(s) found\",\n                passed=False,\n                debug_info={\"errors\": errors},\n            )\n\n        # If no errors were found, the type check passed\n        return TypeCheckResult(message=\"No errors found\", passed=True)"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", encoding=\"utf-8\") as f:\n            f.write(user_code)\n            f.flush()\n            # Run Pyright on the file\n            result = subprocess.run(\n                [\"pyright\", f.name],\n                capture_output=True,\n                text=True,\n            )\n\n        # Parse the output of Pyright to get the line numbers of expected type errors\n        expected_error_lines = set()\n        for line in result.stdout.splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match and cls.EXPECT_ERROR_COMMENT in match.group(2):\n                expected_error_lines.add(int(match.group(1)))\n\n        # Combine the user code and test code and check if the expected type errors are present\n        combined_code = user_code + test_code\n        combined_lines = combined_code.splitlines()\n        passed = all(\n            line_no not in expected_error_lines\n            for line_no, line in enumerate(combined_lines, start=1)\n            if cls.EXPECT_ERROR_COMMENT not in line\n        )\n\n        # Return the result of the type check\n        return TypeCheckResult(\n            message=result.stdout,\n            passed=passed,\n            debug_info={\n                \"user_code\": user_code,\n                \"test_code\": test_code,\n                \"expected_error_lines\": expected_error_lines,\n                \"combined_code\": combined_code,\n            },\n        )"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine user code and test code\n        code = user_code + cls.CODE_SPLITTER + test_code\n\n        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(code)\n            f.flush()\n            file_name = f.name\n\n        # Run Pyright and capture the output\n        output = subprocess.check_output(\n            [\"pyright\", file_name],\n            stderr=subprocess.STDOUT,\n            universal_newlines=True,\n        )\n\n        # Remove the temporary file\n        Path(file_name).unlink()\n\n        # Parse the Pyright output to identify lines with expected type errors\n        expected_error_lines = set()\n        for line in output.splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_no = int(match.group(1))\n                message = match.group(2)\n                if cls.EXPECT_ERROR_COMMENT in message:\n                    expected_error_lines.add(line_no)\n\n        # Determine if the type check passed or failed\n        passed = len(expected_error_lines) == 0\n\n        # Construct the result message\n        if passed:\n            message = \"All tests passed!\"\n        else:\n            message = f\"Test failed on lines: {', '.join(map(str, expected_error_lines))}\"\n\n        # Return the result\n        return TypeCheckResult(message, passed)"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(user_code)\n            f.write(cls.CODE_SPLITTER)\n            f.write(test_code)\n            f.flush()\n\n            process = subprocess.Popen(\n                [\"pyright\", f.name],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.STDOUT,\n                encoding=\"utf-8\",\n            )\n            stdout, _ = process.communicate()\n\n        # Parse Pyright output\n        expected_error_lines = set()\n        error_lines = set()\n        for line in stdout.splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_no, message = match.groups()\n                if cls.EXPECT_ERROR_COMMENT in message:\n                    expected_error_lines.add(int(line_no))\n                else:\n                    error_lines.add(int(line_no))\n\n        passed = len(error_lines) == 0\n        if passed:\n            message = \"Congratulations, your code passed the type check!\"\n        else:\n            message = (\n                f\"Your code has {len(error_lines)} type errors, \"\n                f\"and {len(expected_error_lines)} expected type errors. \"\n                f\"Please fix them to pass the type check.\"\n            )\n\n        return TypeCheckResult(message=message, passed=passed)"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(user_code)\n            f.write(test_code)\n            f.flush()\n\n            # Run Pyright with the temporary file as input\n            output = subprocess.check_output(\n                [\"pyright\", f.name], stderr=subprocess.STDOUT, text=True\n            )\n\n        # Parse the Pyright output to extract the expected type errors\n        expected_errors = []\n        for line in output.splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_no = int(match.group(1))\n                message = match.group(2)\n                if cls.EXPECT_ERROR_COMMENT in message:\n                    expected_errors.append(line_no)\n\n        # Check if the type check passed or failed\n        passed = not expected_errors\n\n        # Create a TypeCheckResult object with the appropriate message and passed flag\n        if passed:\n            message = \"Congratulations, you have passed all the tests!\"\n        else:\n            message = (\n                \"You have failed the following tests: \"\n                + \", \".join(str(line_no) for line_no in expected_errors)\n            )\n\n        # Return the TypeCheckResult object\n        return TypeCheckResult(message=message, passed=passed)"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_dir_path = Path(temp_dir)\n\n            # Write user code to a file\n            user_code_file = temp_dir_path / \"user_code.py\"\n            user_code_file.write_text(user_code, encoding=\"utf-8\")\n\n            # Write test code to a file\n            test_code_file = temp_dir_path / \"test_code.py\"\n            test_code_file.write_text(test_code, encoding=\"utf-8\")\n\n            # Run Pyright type check on the combined code\n            try:\n                output = subprocess.check_output(\n                    [\"pyright\", \"--outputjson\", user_code_file, test_code_file],\n                    stderr=subprocess.STDOUT,\n                )\n            except subprocess.CalledProcessError as e:\n                # If Pyright fails, return a failed result with the error message\n                return TypeCheckResult(\n                    message=f\"Pyright failed with error: {e.output.decode()}\",\n                    passed=False,\n                )\n\n        # Parse the Pyright output to extract the expected type errors\n        expected_errors = []\n        for line in output.decode().splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_no = int(match.group(1))\n                message = match.group(2)\n                if cls.EXPECT_ERROR_COMMENT in message:\n                    expected_errors.append(line_no)\n\n        # If there are no expected type errors, return a passed result\n        if not expected_errors:\n            return TypeCheckResult(\n                message=\"All type errors are expected.\",\n                passed=True,\n            )\n\n        # If there are expected type errors, return a failed result with the expected errors\n        return TypeCheckResult(\n            message=f\"Expected type errors on lines {expected_errors}.\",\n            passed=False,\n            debug_info={\n                \"expected_errors\": expected_"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_dir_path = Path(temp_dir)\n            user_code_file = temp_dir_path / \"user_code.py\"\n            test_code_file = temp_dir_path / \"test_code.py\"\n            user_code_file.write_text(user_code, encoding=\"utf-8\")\n            test_code_file.write_text(test_code, encoding=\"utf-8\")\n\n            # Run Pyright and capture its output\n            try:\n                pyright_output = subprocess.check_output(\n                    [\"pyright\", str(user_code_file), str(test_code_file)],\n                    stderr=subprocess.STDOUT,\n                )\n            except subprocess.CalledProcessError as e:\n                pyright_output = e.output\n\n            # Extract error messages and line numbers from Pyright output\n            messages = []\n            for line in pyright_output.decode(\"utf-8\").splitlines():\n                match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n                if match:\n                    line_no = int(match.group(1))\n                    message = match.group(2).strip()\n                    messages.append((line_no, message))\n\n            # Check if there are any expected type errors in the user code\n            expected_error_lines = set()\n            for line_no, message in messages:\n                if cls.EXPECT_ERROR_COMMENT in message:\n                    expected_error_lines.add(line_no)\n\n            # Check if there are any expected type errors in the test code\n            for line in test_code.splitlines():\n                if cls.EXPECT_ERROR_COMMENT in line:\n                    expected_error_lines.add(test_code.count(\"\\n\", 0, line.find(line)))\n\n            # Check if there are any unexpected type errors in the user code\n            unexpected_error_lines = set()\n            for line_no, message in messages:\n                if cls.EXPECT_ERROR_COMMENT"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(user_code)\n            f.write(test_code)\n            f.flush()\n            temp_file_path = f.name\n\n        # Run pyright with the temporary file\n        result = subprocess.run(\n            [\n                \"pyright\",\n                \"--outputjson\",\n                \"--output\",\n                \"/dev/null\",\n                temp_file_path,\n            ],\n            capture_output=True,\n            text=True,\n        )\n\n        # Parse the output to get the line numbers of expected type errors\n        expected_error_lines = set()\n        for line in result.stderr.splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_number = int(match.group(1))\n                message = match.group(2)\n                if cls.EXPECT_ERROR_COMMENT in message:\n                    expected_error_lines.add(line_number)\n\n        # Remove the temporary file\n        Path(temp_file_path).unlink()\n\n        # If there are no expected errors, the type check passed\n        if not expected_error_lines:\n            return TypeCheckResult(\n                message=\"Type check passed\", passed=True, debug_info={\"pyright_output\": result.stdout}\n            )\n\n        # Otherwise, the type check failed\n        return TypeCheckResult(\n            message=\"Type check failed\",\n            passed=False,\n            debug_info={\"pyright_output\": result.stdout, \"expected_error_lines\": expected_error_lines},\n        )"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine user code and test code\n        code = user_code + cls.CODE_SPLITTER + test_code\n\n        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(code)\n            file_path = Path(f.name)\n\n        # Run Pyright to perform the type check\n        result = subprocess.run(\n            [\n                \"pyright\",\n                \"--noEmit\",\n                \"--skipInvalidCheck\",\n                \"--skipNonExistentFiles\",\n                \"--skipMissingImports\",\n                \"--strict\",\n                \"--outputJson\",\n                str(file_path),\n            ],\n            capture_output=True,\n            text=True,\n        )\n\n        # Parse the Pyright output to extract the error messages and line numbers\n        errors = []\n        for line in result.stdout.splitlines():\n            match = re.search(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_no = int(match.group(1))\n                message = match.group(2)\n                errors.append((line_no, message))\n\n        # Remove the temporary file\n        file_path.unlink()\n\n        # Check if there are any expected type errors in the code\n        expected_error_lines = [\n            token.start[0]\n            for token in tokenize.generate_tokens(io.StringIO(code).readline)\n            if token.string == cls.EXPECT_ERROR_COMMENT\n        ]\n\n        # If there are no expected type errors, the type check should fail\n        if not expected_error_lines:\n            return TypeCheckResult(\n                message=\"No expected type errors found in the code.\",\n                passed=False,\n                debug_info={\"errors\": errors},\n            )\n\n        # Check if the expected type errors are present in the errors list\n        passed = all(line in [line_no for line_no, _ in errors] for line in expected"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(user_code + \"\\n\" + test_code)\n            f.flush()\n            file_path = f.name\n\n        # Run Pyright to perform the type check\n        result = subprocess.run(\n            [\"pyright\", file_path], capture_output=True, text=True\n        )\n\n        # Parse the Pyright output to extract the expected error messages and line numbers\n        expected_errors = defaultdict(list)\n        errors = []\n        for line in result.stdout.splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_no = int(match.group(1))\n                message = match.group(2).strip()\n                if cls.EXPECT_ERROR_COMMENT in message:\n                    expected_errors[line_no].append(message)\n                else:\n                    errors.append(message)\n\n        # Check if the type check passed or failed based on the expected errors\n        passed = not errors and not expected_errors\n\n        # Construct the result message\n        if passed:\n            message = \"Passed\"\n        else:\n            message = \"Failed\"\n            if errors:\n                message += f\"\\n{errors}\"\n            if expected_errors:\n                message += f\"\\nExpected errors on lines {list(expected_errors.keys())}\"\n\n        # Remove the temporary file\n        Path(file_path).unlink()\n\n        # Return the result\n        return TypeCheckResult(message=message, passed=passed, debug_info=result)"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_dir = Path(temp_dir)\n\n            # Write user code to a file\n            user_file = temp_dir / \"user_code.py\"\n            user_file.write_text(user_code, encoding=\"utf-8\")\n\n            # Write test code to a file\n            test_file = temp_dir / \"test_code.py\"\n            test_file.write_text(test_code, encoding=\"utf-8\")\n\n            # Run Pyright on the combined file\n            pyright_output = subprocess.run(\n                [\"pyright\", \"--outputjson\", str(temp_dir / \"pyright.json\"), str(user_file)],\n                capture_output=True,\n                text=True,\n            )\n\n            # Parse Pyright output to extract expected error lines\n            expected_error_lines = set()\n            for line in pyright_output.stdout.splitlines():\n                match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n                if match:\n                    line_no = int(match.group(1))\n                    if cls.EXPECT_ERROR_COMMENT in line:\n                        expected_error_lines.add(line_no)\n\n            # Read Pyright output JSON file and extract error lines\n            pyright_json_file = temp_dir / \"pyright.json\"\n            with open(pyright_json_file, \"r\", encoding=\"utf-8\") as f:\n                pyright_json = json.load(f)\n\n            error_lines = set()\n            for diagnostic in pyright_json[\"diagnostics\"]:\n                if diagnostic[\"severity\"] == \"error\":\n                    error_lines.add(diagnostic[\"line\"])\n\n            # Determine if type check passed\n            passed = error_lines == expected_error_lines\n\n            # Construct message\n            if passed:\n                message = \"Congratulations, type check passed!\"\n            else:\n                message = f\"Type check failed, expected error lines: {expected_error_lines},"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\") as f:\n            f.write(user_code)\n            f.write(test_code)\n            f.flush()\n\n            # Run pyright\n            output = subprocess.check_output(\n                [\"pyright\", f.name],\n                stderr=subprocess.STDOUT,\n                encoding=\"utf-8\",\n            )\n\n        # Parse pyright output\n        lines = output.splitlines()\n        expected_error_lines = []\n        for line in lines:\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match and cls.EXPECT_ERROR_COMMENT in line:\n                expected_error_lines.append(int(match.group(1)))\n\n        if expected_error_lines:\n            # We expect the error on the last line\n            expected_error_line = expected_error_lines[-1]\n            if expected_error_line == len(lines):\n                # The error is on the last line, so the type check failed\n                return TypeCheckResult(\n                    message=lines[-1],\n                    passed=False,\n                )\n            # The error is on the next line, so the type check passed\n            return TypeCheckResult(\n                message=lines[expected_error_line],\n                passed=True,\n            )\n        # No expected error found, so the type check passed\n        return TypeCheckResult(\n            message=\"No type error found.\",\n            passed=True,\n        )"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", encoding=\"utf-8\") as f:\n            f.write(user_code)\n            f.write(test_code)\n            f.flush()\n\n            try:\n                output = subprocess.check_output(\n                    [\"pyright\", f.name], stderr=subprocess.STDOUT, text=True\n                )\n            except subprocess.CalledProcessError as e:\n                return TypeCheckResult(\n                    message=e.output,\n                    passed=False,\n                )\n\n            lines_with_errors = []\n            for line in output.splitlines():\n                match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n                if match:\n                    line_no, error_message = match.groups()\n                    lines_with_errors.append(int(line_no))\n\n        # If there are no errors, the type check passed\n        if not lines_with_errors:\n            return TypeCheckResult(message=\"Passed\", passed=True)\n\n        # If there are errors, the type check failed\n        # We need to find the line number of the error\n        # The error is usually at the end of the user code\n        # So we need to find the line number of the last line of the user code\n        # We can do this by counting the number of newlines in the user code\n        # The line number of the error is the line number of the last line of the user code\n        # plus the line number of the error\n        # We also need to find the line number of the first line of the test code\n        # We can do this by counting the number of newlines in the test code\n        # The line number of the first line of the test code is the line number of the first line of the test code\n        # plus the line number of the error\n        # We need to find the line number of the error in the test code\n        # We can do this by counting the number of newlines in the test code\n        # The line number of the error is the line number of the error\n        # plus the line number of the first line of the"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if not (path or modelfile):\n      raise RequestError('must provide either path or modelfile')\n\n    if path:\n      modelfile = await self._parse_modelfile(await _as_path(path).read_text(), base=await _as_path(path).parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if not (path or modelfile):\n      raise RequestError('must provide either path or modelfile')\n\n    if path:\n      modelfile = await self._parse_modelfile(await _as_path(path).read_text(), base=await _as_path(path).parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if not (path or modelfile):\n      raise RequestError('must provide either path or modelfile')\n\n    if path:\n      modelfile = await self._parse_modelfile(await _as_path(path).read_text(), base=await _as_path(path).parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if not (path or modelfile):\n      raise RequestError('must provide either path or modelfile')\n\n    if path:\n      modelfile = await self._parse_modelfile(await _as_path(path).read_text(), base=await _as_path(path).parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if not (path or modelfile):\n      raise RequestError('must provide either path or modelfile')\n\n    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if not (path or modelfile):\n      raise RequestError('must provide either path or modelfile')\n\n    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if not (path or modelfile):\n      raise RequestError('must provide either path or modelfile')\n\n    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if not (path or modelfile):\n      raise RequestError('must provide either path or modelfile')\n\n    if not path and not modelfile:\n      raise RequestError('must provide either path or modelfile')\n\n    if path:\n      modelfile = await self._parse_modelfile(await _as_path(path).read_text(), base=await _as_path(path).parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if not (path or modelfile):\n      raise RequestError('must provide either path or modelfile')\n\n    if path:\n      if (realpath := _as_path(path)) and realpath.exists():\n        modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n      else:\n        raise RequestError('path does not exist')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if not (realpath := _as_path(path)) and not modelfile:\n      raise RequestError('must provide either path or modelfile')\n\n    if realpath and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if not (path or modelfile):\n      raise RequestError('must provide either path or modelfile')\n\n    if path:\n      if (realpath := _as_path(path)) and realpath.exists():\n        modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n      else:\n        raise RequestError('path must exist')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if not (path or modelfile):\n      raise RequestError('must provide either path or modelfile')\n\n    if not path:\n      return await self._request_stream(\n        'POST',\n        '/api/create',\n        json={\n          'name': model,\n          'modelfile': await self._parse_modelfile(modelfile),\n          'stream': stream,\n        },\n        stream=stream,\n      )\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': await self._parse_modelfile(path),\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if not (path or modelfile):\n      raise RequestError('must provide either path or modelfile')\n\n    if not path and modelfile:\n      return await self._request_stream(\n        'POST',\n        '/api/create',\n        json={\n          'name': model,\n          'modelfile': await self._parse_modelfile(modelfile),\n          'stream': stream,\n        },\n        stream=stream,\n      )\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': await self._parse_modelfile(path.read_text(), base=path.parent),\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if not (path or modelfile):\n      raise RequestError('must provide either path or modelfile')\n\n    if not path:\n      return await self._request_stream(\n        'POST',\n        '/api/create',\n        json={\n          'name': model,\n          'modelfile': modelfile,\n          'stream': stream,\n        },\n        stream=stream,\n      )\n\n    if not modelfile:\n      modelfile = await self._parse_modelfile(await _as_path(path).read_text(), base=await _as_path(path).parent)\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if not path and not modelfile:\n      raise RequestError('must provide either path or modelfile')\n\n    if not path and modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    elif path:\n      if (realpath := _as_path(path)) and realpath.exists():\n        modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n      else:\n        raise RequestError('path must be a valid path')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_fn=get_compiler_fn(title=\"Module\"))\n    else:\n        return aot_function(fn, compiler_fn=get_compiler_fn(title=\"Function\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_fn=get_compiler_fn(title=\"Module\"))\n    else:\n        return aot_function(fn, compiler_fn=get_compiler_fn(title=\"Function\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_fn=get_compiler_fn(title=\"Module\"))\n    else:\n        return aot_function(fn, compiler_fn=get_compiler_fn(title=\"Function\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_fn=get_compiler_fn(title=\"Module\"))\n    else:\n        return aot_function(fn, compiler_fn=get_compiler_fn(title=\"Function\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_fn=get_compiler_fn(title=\"Module\"))\n    else:\n        return aot_function(fn, compiler_fn=get_compiler_fn(title=\"Function\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_fn=get_compiler_fn(\"Module\"))\n    else:\n        return aot_function(fn, compiler_fn=get_compiler_fn(\"Function\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_fn=get_compiler_fn(\"Module\"))\n    else:\n        return aot_function(fn, compiler_fn=get_compiler_fn(\"Function\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_fn=get_compiler_fn(title=\"Module\"))\n    else:\n        return aot_function(fn, compiler_fn=get_compiler_fn(title=\"Function\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_fn=get_compiler_fn(title=\"Module\"))\n    else:\n        return aot_function(fn, compiler_fn=get_compiler_fn(title=\"Function\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, forward_compiler=get_compiler_fn(\"Forward Compiler\"), backward_compiler=get_compiler_fn(\"Backward Compiler\"))\n    else:\n        return aot_function(fn, forward_compiler=get_compiler_fn(\"Forward Compiler\"), backward_compiler=get_compiler_fn(\"Backward Compiler\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, forward_compiler=get_compiler_fn(\"Forward Compiler\"), backward_compiler=get_compiler_fn(\"Backward Compiler\"))\n    else:\n        return aot_function(fn, forward_compiler=get_compiler_fn(\"Forward Compiler\"), backward_compiler=get_compiler_fn(\"Backward Compiler\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_fn=get_compiler_fn(\"Module\"))\n    else:\n        return aot_function(fn, compiler_fn=get_compiler_fn(\"Function\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_creator=get_compiler_fn(\"Module\"))\n    else:\n        return aot_function(fn, compiler_creator=get_compiler_fn(\"Function\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, forward_compiler=get_compiler_fn(\"Forward\"),\n                          backward_compiler=get_compiler_fn(\"Backward\"))\n    else:\n        return aot_function(fn, forward_compiler=get_compiler_fn(\"Forward\"),\n                            backward_compiler=get_compiler_fn(\"Backward\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, forward_compiler=get_compiler_fn(), backward_compiler=get_compiler_fn())\n    else:\n        return aot_function(fn, forward_compiler=get_compiler_fn(), backward_compiler=get_compiler_fn())"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_fn=get_compiler_fn(\"AOT module\"))\n    else:\n        return aot_function(fn, compiler_fn=get_compiler_fn(\"AOT function\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, example_inputs=(torch.randn(1, 3, 224, 224),),\n                          compiler_fn=get_compiler_fn(title=\"Module Compiler\"))\n    else:\n        return aot_function(fn, example_inputs=(torch.randn(1, 3, 224, 224),),\n                            compiler_fn=get_compiler_fn(title=\"Function Compiler\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        aot_module(fn, compiler_fn=get_compiler_fn(title=\"Module Compilation\"))\n    else:\n        aot_function(fn, compiler_fn=get_compiler_fn(title=\"Function Compilation\"))\n    return fn"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, example_inputs=(torch.randn(1, 3, 224, 224),))(\n            torch.randn(1, 3, 224, 224))\n    else:\n        return aot_function(fn, example_inputs=(torch.randn(1, 3, 224, 224),))(\n            torch.randn(1, 3, 224, 224))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_fn=get_compiler_fn(title=\"Module\"))\n    elif callable(fn):\n        return aot_function(fn, compiler_fn=get_compiler_fn(title=\"Function\"))\n    else:\n        raise ValueError(\"Input must be a torch.nn.Module or a function.\")"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        try:\n            config_dict = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            logger.error(exc)\n            raise exc\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n    if output_path is not None:\n        with open(output_path, 'w') as f:\n            yaml.dump(best_config, f)\n    return best_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        try:\n            config_dict = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            logger.error(exc)\n            raise exc\n    best_config_dict = summary_df_to_yaml(summary_df, config_dict)\n    if output_path is not None:\n        with open(output_path, 'w') as f:\n            yaml.safe_dump(best_config_dict, f)\n    return best_config_dict"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(os.path.join(trial_path, 'summary.csv'))\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        try:\n            config_dict = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            logger.error(exc)\n            raise exc\n    best_config_dict = summary_df_to_yaml(summary_df, config_dict)\n    if output_path is not None:\n        with open(output_path, 'w') as f:\n            yaml.safe_dump(best_config_dict, f)\n    return best_config_dict"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        try:\n            config_dict = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            logger.error(exc)\n            raise exc\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n    if output_path is not None:\n        with open(output_path, 'w') as f:\n            yaml.safe_dump(best_config, f)\n    return best_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        try:\n            config_dict = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            logger.error(exc)\n            raise exc\n\n    best_config_dict = summary_df_to_yaml(summary_df, config_dict)\n\n    if output_path is not None:\n        with open(output_path, 'w') as f:\n            yaml.safe_dump(best_config_dict, f)\n\n    return best_config_dict"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(os.path.join(trial_path, 'summary.csv'))\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        try:\n            config_dict = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            logger.error(exc)\n            raise exc\n\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n\n    if output_path is not None:\n        with open(output_path, 'w') as f:\n            yaml.safe_dump(best_config, f)\n\n    return best_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(os.path.join(trial_path, 'summary.csv'))\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        try:\n            config_dict = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            logger.error(exc)\n            raise exc\n\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n\n    if output_path is not None:\n        with open(output_path, 'w') as f:\n            yaml.dump(best_config, f)\n\n    return best_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(os.path.join(trial_path, 'summary.csv'))\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        try:\n            config_dict = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            logger.error(exc)\n            raise exc\n\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n\n    if output_path is not None:\n        with open(output_path, 'w') as f:\n            yaml.safe_dump(best_config, f)\n\n    return best_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(os.path.join(trial_path, 'summary.csv'))\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        try:\n            config_dict = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            logger.error(exc)\n            raise exc\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n    if output_path is not None:\n        with open(output_path, 'w') as f:\n            yaml.safe_dump(best_config, f)\n    return best_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        try:\n            config_dict = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            logger.error(exc)\n            raise exc\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n\n    if output_path is not None:\n        with open(output_path, 'w') as f:\n            yaml.dump(best_config, f)\n\n    return best_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    config_dict = load_config_file(trial_path)\n    best_config_dict = summary_df_to_yaml(summary_df, config_dict)\n\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError('Output path must end with .yaml or .yml')\n        with open(output_path, 'w') as f:\n            yaml.dump(best_config_dict, f)\n\n    return best_config_dict"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(os.path.join(trial_path, 'summary.csv'))\n    config_dict = load_yaml(os.path.join(trial_path, 'config.yaml'))\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n    if output_path is not None:\n        save_yaml(best_config, output_path)\n    return best_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(os.path.join(trial_path, 'summary.csv'))\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        try:\n            config_dict = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            logger.error(exc)\n            raise exc\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n\n    if output_path is not None:\n        with open(output_path, 'w') as f:\n            yaml.dump(best_config, f)\n\n    return best_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        try:\n            config_dict = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            logger.error(exc)\n            raise exc\n\n    config_dict = summary_df_to_yaml(summary_df, config_dict)\n\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            output_path += '.yaml'\n        with open(output_path, 'w') as f:\n            yaml.safe_dump(config_dict, f)\n\n    return config_dict"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    config_dict = yaml.safe_load(open(os.path.join(trial_path, 'config.yaml'), 'r'))\n    best_config_dict = summary_df_to_yaml(summary_df, config_dict)\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            output_path += '.yaml'\n        with open(output_path, 'w') as f:\n            yaml.safe_dump(best_config_dict, f)\n    return best_config_dict"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        config_dict = yaml.safe_load(f)\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n\n    if output_path is not None:\n        output_path = os.path.abspath(output_path)\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError('Output path must end with .yaml or .yml')\n        with open(output_path, 'w') as f:\n            yaml.safe_dump(best_config, f)\n\n    return best_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    config_file_path = os.path.join(trial_path, 'config.yaml')\n    with open(config_file_path, 'r') as f:\n        try:\n            config_dict = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            logger.error(exc)\n            raise exc\n\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError(\"The output path must end with .yaml or .yml if provided.\")\n        with open(output_path, 'w') as f:\n            yaml.dump(best_config, f)\n\n    return best_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        try:\n            config_dict = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            logger.error(exc)\n            raise exc\n\n    config_dict = summary_df_to_yaml(summary_df, config_dict)\n\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError(\"output_path must end with .yaml or .yml if provided.\")\n        with open(output_path, 'w') as f:\n            yaml.safe_dump(config_dict, f)\n\n    return config_dict"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    best_trial = summary_df.iloc[0]\n    config_path = os.path.join(trial_path, 'config.yaml')\n    with open(config_path, 'r') as f:\n        config_dict = yaml.safe_load(f)\n    best_config_dict = summary_df_to_yaml(best_trial, config_dict)\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError(\"output_path must end with .yaml or .yml\")\n        with open(output_path, 'w') as f:\n            yaml.safe_dump(best_config_dict, f)\n    return best_config_dict"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(os.path.join(trial_path, 'summary.csv'))\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        try:\n            config_dict = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            logger.error(exc)\n            raise exc\n    best_config_dict = summary_df_to_yaml(summary_df, config_dict)\n\n    if output_path is not None:\n        with open(output_path, 'w') as f:\n            yaml.dump(best_config_dict, f)\n        logger.info(f'Extracted best pipeline configuration saved to {output_path}')\n    return best_config_dict"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    "}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    "}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    cache = threading.local()\n    lock = threading.Lock()\n\n    @functools.wraps(func)\n    def wrapped(*args, **kwargs):\n        key = (args, kwargs)\n        with lock:\n            if key not in cache:\n                cache.value = {}\n            if key not in cache.value:\n                cache.value[key] = AutoTraceCompiler(\n                    ts_compiler=ts_compiler, **kwargs_).compile(func, args,\n                                                                kwargs)\n            return cache.value[key]\n\n    return wrapped"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    cache = {}\n    lock = threading.Lock()\n\n    @functools.wraps(func)\n    def traced_func(*args, **kwargs):\n        key = (args, kwargs)\n        with lock:\n            if key not in cache:\n                cache[key] = trace_with_kwargs(func,\n                                               example_inputs=args,\n                                               example_kwarg_inputs=kwargs,\n                                               **kwargs_)\n            return cache[key]\n\n    return traced_func"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    cache = {}\n    lock = threading.Lock()\n\n    def wrapper(*args, **kwargs):\n        key = (func, args, kwargs)\n        with lock:\n            if key not in cache:\n                cache[key] = trace_with_kwargs(func,\n                                               ts_compiler=ts_compiler,\n                                               **kwargs_)\n            traced_module, call_helper = cache[key]\n        return call_helper(traced_module)(*args, **kwargs)\n\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    cache_key = (func, tuple(sorted(kwargs_.items())))\n    cache = threading.local()\n    cache.value = {}\n\n    def trace_and_cache(func, *, ts_compiler=None, **kwargs):\n        if cache_key not in cache.value:\n            cache.value[cache_key] = trace_with_kwargs(func,\n                                                       ts_compiler=ts_compiler,\n                                                       **kwargs)\n        return cache.value[cache_key]\n\n    @functools.wraps(func)\n    def wrapped(*args, **kwargs):\n        traced_module, call_helper = trace_and_cache(func,\n                                                     ts_compiler=ts_compiler,\n                                                     **kwargs_)\n        return call_helper(traced_module)(*args, **kwargs)\n\n    return wrapped"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    def wrapper(*args, **kwargs):\n        key = (func, args, kwargs)\n        if key not in cache:\n            cache[key] = lazy_trace.compiler(func, args, kwargs, **kwargs_)\n        return cache[key]\n\n    wrapper.__name__ = func.__name__\n    wrapper.__qualname__ = func.__qualname__\n    wrapper.__doc__ = func.__doc__\n    wrapper.__annotations__ = func.__annotations__\n    wrapper.__module__ = func.__module__\n    wrapper.__dict__ = func.__dict__\n    wrapper.__wrapped__ = func\n    wrapper.__signature__ = inspect.signature(func)\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper.__wrapped__ = func\n    wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    # Check if the function is already traced and cached\n    cache_key = (func, tuple(kwargs_.items()))\n    if cache_key in _TRACED_FUNCS:\n        return _TRACED_FUNCS[cache_key]\n\n    # Define a wrapper function that uses the cache\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        nonlocal ts_compiler\n        if ts_compiler is None:\n            ts_compiler = _TRACED_FUNCS.get(\n                (func, tuple(kwargs_.items())), None)\n        if ts_compiler is None:\n            ts_compiler = AutoTraceCompiler(**kwargs_)\n        return ts_compiler.compile(func, args, kwargs)\n\n    # Cache the traced function\n    _TRACED_FUNCS[cache_key] = wrapper\n\n    return wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    # Create a lock to ensure thread safety\n    lock = threading.Lock()\n\n    # Create a cache to store traced modules\n    cache = {}\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # Acquire the lock before accessing or updating the cache\n        with lock:\n            # Generate a key based on the function and its arguments\n            key = (func, args, tuple(sorted(kwargs.items())))\n\n            # Check if the key exists in the cache\n            if key in cache:\n                # If the key exists, return the cached module\n                return cache[key]\n\n            # If the key does not exist, trace the function or module's forward method\n            traced_module, call_helper = trace_with_kwargs(func,\n                                                           args,\n                                                           kwargs,\n                                                           **kwargs_)\n\n            # If a compiler function is provided, compile the traced module\n            if ts_compiler is not None:\n                if 'call_helper' in inspect.signature(\n                        ts_compiler).parameters:\n                    traced_module = ts_compiler(traced_module, call_helper,\n                                                args, kwargs)\n                else:\n                    converted_args = call_helper(traced_module).convert_inputs(\n                        args, kwargs)\n                    traced_module = ts_compiler(traced_module, converted_args)\n\n            # Cache the traced module with the key\n            cache[key] = traced_module\n\n        # Return the traced module\n        return traced_module\n\n    # Return the wrapper function"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    def wrapper(*args, **kwargs):\n        nonlocal ts_compiler\n        nonlocal kwargs_\n        cache_key = (func, args, kwargs)\n        if cache_key not in cache:\n            cache[cache_key] = trace_with_kwargs(func, *args, **kwargs_)\n        traced_module, call_helper = cache[cache_key]\n        return call_helper(traced_module)(*args, **kwargs)\n\n    cache = {}\n    return wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    # Initialize the cache and lock\n    cache = {}\n    lock = threading.Lock()\n\n    # Define the wrapper function\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # Compute the key for the inputs\n        key = (args, tuple(sorted(kwargs.items())))\n\n        # Acquire the lock\n        with lock:\n            # Check if the key is in the cache\n            if key in cache:\n                # Return the cached result\n                return cache[key]\n\n            # Trace the function with the provided arguments\n            traced_func = trace_with_kwargs(func, args, kwargs, **kwargs_)\n\n            # Cache the traced function\n            cache[key] = traced_func\n\n            # Return the traced function\n            return traced_func\n\n    # Return the wrapper function\n    return wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    if ts_compiler is not None:\n        assert callable(ts_compiler)\n\n    if not callable(func):\n        raise TypeError(f'Expected callable, got {type(func)}')\n\n    cache = {}\n    lock = threading.Lock()\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        with lock:\n            key = (args, kwargs)\n            if key not in cache:\n                cache[key] = AutoTraceCompiler(\n                    ts_compiler=ts_compiler,\n                    **kwargs_).compile(func, args, kwargs)\n            return cache[key]\n\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    if not isinstance(func, (torch.nn.Module, functools.partial)):\n        raise ValueError(\n            f'func must be a torch.nn.Module or a functools.partial, but got {type(func)}'\n        )\n    if not isinstance(ts_compiler, (type(None), functools.partial)):\n        raise ValueError(\n            f'ts_compiler must be a functools.partial or None, but got {type(ts_compiler)}'\n        )\n    if not isinstance(kwargs_, dict):\n        raise ValueError(\n            f'kwargs_ must be a dict, but got {type(kwargs_)}')\n\n    cache = {}\n    lock = threading.Lock()\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = (args, tuple(kwargs.items()))\n        with lock:\n            if key in cache:\n                return cache[key]\n\n        compiled_func = AutoTraceCompiler(\n            ts_compiler=ts_compiler, **kwargs_).compile(func, args, kwargs)\n        cache[key] = compiled_func\n        return compiled_func(*args, **kwargs)\n\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    def _wrapper(*args, **kwargs):\n        key = (func, args, kwargs)\n        with lazy_trace.lock:\n            if key in lazy_trace.cache:\n                return lazy_trace.cache[key]\n            traced_func = trace_with_kwargs(func, ts_compiler=ts_compiler,\n                                            **kwargs_)[0]\n            lazy_trace.cache[key] = traced_func\n            return traced_func(*args, **kwargs)\n\n    _wrapper.__name__ = func.__name__\n    return _wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    cache = threading.local()\n    lock = threading.Lock()\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = (args, kwargs)\n        with lock:\n            if key not in cache.value:\n                cache.value[key] = AutoTraceCompiler(\n                    ts_compiler=ts_compiler, **kwargs_).compile(func, args,\n                                                                kwargs)\n            return cache.value[key]\n\n    wrapper.cache = cache\n    wrapper.lock = lock\n    return wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    # Create a lock to ensure thread safety\n    lock = threading.Lock()\n\n    # Initialize a dictionary to store the cached traces\n    cached_traces = {}\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # Acquire the lock to ensure exclusive access to the cache\n        with lock:\n            # Generate a key for the current call arguments\n            key = (args, kwargs)\n\n            # Check if the trace for this key is already cached\n            if key in cached_traces:\n                # If so, return the cached trace\n                return cached_traces[key]\n\n            # If not, trace the function or module's forward method\n            traced_module, call_helper = trace_with_kwargs(func,\n                                                           *args,\n                                                           **kwargs,\n                                                           **kwargs_)\n\n            # Compile the traced module if a compiler function is provided\n            if ts_compiler is not None:\n                if 'call_helper' in inspect.signature(\n                        ts_compiler).parameters:\n                    traced_module = ts_compiler(traced_module, call_helper,\n                                                *args, **kwargs)\n                else:\n                    converted_args = call_helper(traced_module).convert_inputs(\n                        *args, **kwargs)\n                    traced_module = ts_compiler(traced_module, converted_args)\n\n            # Cache the traced module for future calls\n            cached_traces[key] = call_helper(traced_module)\n\n            # Return the traced module\n            return cached_traces[key]\n\n    # Return the wrapper function\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    # Define the cache for traced modules\n    cache = {}\n    lock = threading.Lock()\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = (args, kwargs)\n\n        # Check if the function has been traced already\n        with lock:\n            if key in cache:\n                return cache[key]\n\n        # Trace the function and cache the result\n        traced_func = trace_with_kwargs(func, *args, **kwargs, **kwargs_)[0]\n        if ts_compiler is not None:\n            traced_func = ts_compiler(traced_func)\n        with lock:\n            cache[key] = traced_func\n        return traced_func\n\n    return wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    # Create a lock to synchronize access to the cache\n    cache_lock = threading.Lock()\n\n    # Create a cache to store the traced modules\n    cache = {}\n\n    # Define a wrapper function that uses the cache to trace and cache the original function\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # Acquire the lock to synchronize access to the cache\n        with cache_lock:\n            # Generate a key for the current function call\n            key = (func, args, kwargs)\n\n            # Check if the function call is already in the cache\n            if key in cache:\n                # If it is, return the cached result\n                return cache[key]\n\n            # If the function call is not in the cache, trace it and cache the result\n            traced_module, call_helper = trace_with_kwargs(func,\n                                                           args,\n                                                           kwargs,\n                                                           **kwargs_)\n            if ts_compiler is not None:\n                if 'call_helper' in inspect.signature(\n                        ts_compiler).parameters:\n                    traced_module = ts_compiler(traced_module, call_helper,\n                                                args, kwargs)\n                else:\n                    converted_args = call_helper(traced_module).convert_inputs(\n                        args, kwargs)\n                    traced_module = ts_compiler(traced_module, converted_args)\n            traced_module = call_helper(traced_module)\n\n            # Cache the traced module for future calls\n            cache[key] = traced_module\n\n            # Return the traced module\n            return traced_module\n\n    # Return the wrapper function\n    return wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    # Create a lock to ensure thread safety when accessing or updating the cache of traced modules\n    lock = threading.Lock()\n\n    # Define a cache of traced modules\n    cache = {}\n\n    # Define a function that traces a given function or PyTorch module with optional compiler enhancements\n    def trace_func(func, *, ts_compiler=None, **kwargs):\n        # Acquire the lock to ensure thread safety\n        with lock:\n            # Check if the function or module has already been traced and cached\n            key = (func, tuple(kwargs.items()))\n            if key in cache:\n                # If it has, return the cached version\n                return cache[key]\n\n            # If not, trace the function or module and cache the result\n            traced_module, call_helper = trace_with_kwargs(func, **kwargs)\n            if ts_compiler is not None:\n                # If a compiler function is provided, use it to further process the traced module\n                traced_module = ts_compiler(traced_module, call_helper)\n\n            # Cache the traced module for future use\n            cache[key] = traced_module\n\n            # Return the traced module\n            return traced_module\n\n    # Return the trace_func function as the main output of the function\n    return trace_func"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    # Define a lock to ensure thread safety when accessing or updating the cache of traced modules\n    lock = threading.Lock()\n\n    # Define a cache to store the traced modules for future calls\n    cache = {}\n\n    # Define a function that can be used to trace the function or module's forward method\n    def trace_func(func, *args, **kwargs):\n        # Compute the key for the traced module based on the inputs and keyword arguments\n        key = (func, args, kwargs)\n\n        # Acquire the lock to ensure exclusive access to the cache\n        with lock:\n            # Check if the traced module is already in the cache\n            if key in cache:\n                # If the traced module is in the cache, return it\n                return cache[key]\n\n            # If the traced module is not in the cache, trace it\n            traced_module = trace_with_kwargs(func, *args, **kwargs)\n\n            # Add the traced module to the cache with the computed key\n            cache[key] = traced_module\n\n        # Return the traced module\n        return traced_module\n\n    # Wrap the original function or module's forward method with the trace_func function\n    if isinstance(func, torch.nn.Module):\n        # If the input is a PyTorch module, wrap the forward method\n        func.forward = functools.wraps(func.forward)(trace_func)\n    else:\n        # If the input is a function, wrap the function\n        func = functools.wraps(func)(trace_func)\n\n    # Return the wrapped function or module\n    return func"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        return cls(config, project_dir=os.path.dirname(trial_path))\n\n    "}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        return cls(config, project_dir=os.path.dirname(trial_path))\n\n    "}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        yaml_dict = extract_best_config(trial_path)\n        return cls(yaml_dict)\n\n    "}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        yaml_dict = extract_best_config(trial_path)\n        return cls(yaml_dict)\n\n    "}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        yaml_dict = extract_best_config(trial_path)\n        return cls(yaml_dict)\n\n    "}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config_yaml_path = os.path.join(trial_path, 'config.yaml')\n        with open(config_yaml_path, 'r') as f:\n            try:\n                config = yaml.safe_load(f)\n            except yaml.YAMLError as exc:\n                logger.error(exc)\n                raise exc\n        return cls(config, project_dir=os.path.dirname(trial_path))\n\n    "}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config_yaml_path = os.path.join(trial_path, 'config.yaml')\n        with open(config_yaml_path, 'r') as f:\n            try:\n                config = yaml.safe_load(f)\n            except yaml.YAMLError as exc:\n                logger.error(exc)\n                raise exc\n        return cls(config, project_dir=os.path.dirname(trial_path))\n\n    "}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config_yaml_path = os.path.join(trial_path, 'config.yaml')\n        with open(config_yaml_path, 'r') as f:\n            try:\n                config = yaml.safe_load(f)\n            except yaml.YAMLError as exc:\n                logger.error(exc)\n                raise exc\n        return cls(config, project_dir=os.path.dirname(trial_path))\n\n    "}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config_yaml_path = os.path.join(trial_path, 'config.yaml')\n        with open(config_yaml_path, 'r') as f:\n            try:\n                config = yaml.safe_load(f)\n            except yaml.YAMLError as exc:\n                logger.error(exc)\n                raise exc\n        return cls(config, project_dir=os.path.dirname(trial_path))\n\n    "}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config_yaml_path = os.path.join(trial_path, 'config.yaml')\n        with open(config_yaml_path, 'r') as f:\n            try:\n                config = yaml.safe_load(f)\n            except yaml.YAMLError as exc:\n                logger.error(exc)\n                raise exc\n        return cls(config, project_dir=os.path.dirname(trial_path))\n\n    "}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config_yaml_path = os.path.join(trial_path, 'config.yaml')\n        with open(config_yaml_path, 'r') as f:\n            try:\n                config = yaml.safe_load(f)\n            except yaml.YAMLError as exc:\n                logger.error(exc)\n                raise exc\n        return cls(config, project_dir=os.path.dirname(trial_path))\n\n    "}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config_dict = extract_best_config(trial_path)\n        return cls(config_dict)\n\n    "}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        return cls(config, project_dir=os.path.dirname(trial_path))"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        yaml_path = os.path.join(trial_path, 'best_config.yaml')\n        if not os.path.exists(yaml_path):\n            raise ValueError(f\"best_config.yaml does not exist in {trial_path}.\")\n        with open(yaml_path, 'r') as f:\n            try:\n                config = yaml.safe_load(f)\n            except yaml.YAMLError as exc:\n                logger.error(exc)\n                raise exc\n        return cls(config, project_dir=os.path.dirname(trial_path))\n\n    "}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        yaml_path = os.path.join(trial_path, 'best_config.yaml')\n        if not os.path.exists(yaml_path):\n            raise ValueError(f\"best_config.yaml does not exist in {trial_path}.\")\n        project_dir = os.path.dirname(trial_path)\n        with open(yaml_path, 'r') as f:\n            try:\n                config = yaml.safe_load(f)\n            except yaml.YAMLError as exc:\n                logger.error(exc)\n                raise exc\n        return cls(config, project_dir=project_dir)\n\n    "}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        yaml_path = os.path.join(trial_path, 'best_config.yaml')\n        return cls.from_yaml(yaml_path)\n\n    "}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config_dict = extract_best_config(trial_path)\n        return cls(config_dict)"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config_yaml_path = os.path.join(trial_path, 'config.yaml')\n        if not os.path.exists(config_yaml_path):\n            raise ValueError(f\"config.yaml does not exist in {trial_path}.\")\n        with open(config_yaml_path, 'r') as f:\n            try:\n                config = yaml.safe_load(f)\n            except yaml.YAMLError as exc:\n                logger.error(exc)\n                raise exc\n        return cls(config, project_dir=os.path.dirname(trial_path))"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config_dict = extract_best_config(trial_path)\n        project_dir = os.path.abspath(os.path.join(trial_path, os.pardir))\n        return cls(config_dict, project_dir=project_dir)"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        best_config_dict = extract_best_config(trial_path)\n        project_dir = os.path.dirname(trial_path)\n        return cls(best_config_dict, project_dir=project_dir)"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    logger.info(\"Running retrieval node...\")\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Evaluate and select the best module among retrieval node results\n    result_df = evaluate_and_select_best_module(modules, module_params, previous_result, node_line_dir, strategies)\n\n    logger.info(\"Retrieval node finished.\")\n    return result_df"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    result_dfs = []\n    for module, module_param in zip(modules, module_params):\n        result_df = module(**module_param)\n        result_df = result_df.rename(columns={'retrieved_contents': 'retrieved_contents_' + module.__name__,\n                                              'retrieved_ids': 'retrieved_ids_' + module.__name__,\n                                              'retrieve_scores': 'retrieve_scores_' + module.__name__})\n        result_dfs.append(result_df)\n\n    result_df = pd.concat(result_dfs, axis=1)\n    result_df = pd.concat([previous_result, result_df], axis=1)\n    result_df = result_df.drop(columns=['retrieved_contents', 'retrieved_ids', 'retrieve_scores'])\n\n    if strategies['speed']:\n        result_df = measure_speed(result_df, strategies['speed'])\n\n    if strategies['evaluate']:\n        result_df = evaluate_retrieval_node(result_df, strategies['evaluate']['retrieval_gt'], strategies['evaluate']['metrics'])\n\n    if strategies['filter_by_threshold']:\n        result_df = filter_by_threshold(result_df, strategies['filter_by_threshold'])\n\n    if strategies['select_best_average']:\n        result_df = select_best_average(result_df, strategies['select_best_average'])\n\n    result_df.to_parquet(os.path.join(node_line_dir, \"result.parquet\"))\n    result_df.to_csv(os.path.join(node_line_dir, \"result.csv\"))\n\n    summary_df = load_summary_file(os.path.join(node_line_dir, \"summary.csv\"))\n    summary_df = summary_df.append(result_df, ignore_index"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    logger.info(f\"Running retrieval node at {node_line_dir}\")\n\n    # Create the directory if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Evaluate and select the best module\n    best_result, summary_df = evaluate_and_select_best_module(modules, module_params, previous_result, node_line_dir, strategies)\n\n    # Save the summary to disk\n    summary_df.to_csv(os.path.join(node_line_dir, \"summary.csv\"), index=False)\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the node directory if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Run each module with given parameters\n    results = list(map(lambda module, param: module(**param), modules, module_params))\n\n    # Measure execution times\n    execution_times = list(map(measure_speed, results))\n\n    # Evaluate results\n    evaluation_results = list(map(lambda result, execution_time: evaluate_retrieval_node(result, previous_result, strategies['metrics']), results, execution_times))\n\n    # Filter by threshold\n    filtered_results = list(map(lambda result: filter_by_threshold(result, strategies['speed_threshold']), evaluation_results))\n\n    # Select best result\n    best_result = select_best_average(filtered_results)\n\n    # Save results\n    for result, execution_time, filename in zip(results, execution_times, strategies['filenames']):\n        result.to_parquet(os.path.join(node_line_dir, filename))\n\n    # Save summary\n    summary_df = pd.DataFrame({\n        'filename': strategies['filenames'],\n        'module_name': strategies['module_names'],\n        'module_params': module_params,\n        'execution_time': execution_times,\n    })\n    for col in strategies['metrics']:\n        summary_df[col] = list(map(lambda df: df[col].tolist(), evaluation_results))\n    summary_df.to_csv(os.path.join(node_line_dir, \"summary.csv\"), index=False)\n\n    # Return best result\n    return best_result"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    logger.info(f\"Running retrieval node with {len(modules)} modules.\")\n    if len(modules) != len(module_params):\n        raise ValueError(\"The number of modules and module_params must be the same.\")\n\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Evaluate and select the best module\n    result_df = evaluate_retrieval_node(previous_result, strategies['retrieval_gt'], strategies['metrics'])\n    result_df = select_best_average(result_df, strategies['metrics'])\n    result_df.to_parquet(os.path.join(node_line_dir, \"result.parquet\"))\n\n    # Save summary of execution times and evaluation metrics\n    summary_df = load_summary_file(os.path.join(node_line_dir, \"summary.csv\"))\n    summary_df = measure_speed(summary_df, node_line_dir, strategies['speed_thresholds'])\n    summary_df = filter_by_threshold(summary_df, strategies['speed_thresholds'])\n    summary_df.to_csv(os.path.join(node_line_dir, \"summary.csv\"), index=False)\n\n    return result_df\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Initialize result dataframe\n    result_df = pd.DataFrame(columns=previous_result.columns)\n\n    # Run each retrieval module with given parameters\n    for module, module_param in zip(modules, module_params):\n        logger.info(f\"Running {module.__name__} with params {module_param}\")\n        module_result = module(**module_param)\n        module_result.to_parquet(os.path.join(node_line_dir, f\"{module.__name__}.parquet\"))\n        result_df = pd.concat([result_df, module_result])\n\n    # Evaluate and select the best result\n    result_df = evaluate_retrieval_node(result_df, previous_result, strategies['metrics'])\n    result_df = select_best_average(result_df, strategies['metrics'])\n\n    # Save the result\n    result_df.to_parquet(os.path.join(node_line_dir, \"result.parquet\"))\n\n    # Save the summary of the execution times and evaluation metrics\n    summary_df = load_summary_file(os.path.join(node_line_dir, \"summary.csv\"))\n    summary_df = pd.concat([summary_df, result_df], ignore_index=True)\n    summary_df.to_csv(os.path.join(node_line_dir, \"summary.csv\"), index=False)\n\n    return result_df\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    if not strategies:\n        strategies = {\n            'metrics': ['recall', 'precision', 'f1'],\n            'speed_threshold': 0.5,\n        }\n\n    if 'metrics' not in strategies:\n        strategies['metrics'] = ['recall', 'precision', 'f1']\n\n    if 'speed_threshold' not in strategies:\n        strategies['speed_threshold'] = 0.5\n\n    result_dfs = []\n    for module, module_param in zip(modules, module_params):\n        result_df = module(**module_param)\n        result_df['module_name'] = module.__name__\n        result_df['module_params'] = module_param\n        result_df['filename'] = module.__name__ + '.parquet'\n        result_dfs.append(result_df)\n\n    result_df = pd.concat(result_dfs)\n    result_df = result_df.reset_index(drop=True)\n\n    if previous_result is not None:\n        result_df = pd.concat([previous_result, result_df], axis=1)\n\n    result_df = evaluate_retrieval_node(result_df, strategies['metrics'])\n    result_df = measure_speed(result_df, strategies['speed_threshold'])\n    result_df = filter_by_threshold(result_df)\n    result_df = select_best_average(result_df)\n\n    result_df.to_parquet(os.path.join(node_line_dir, \"result.parquet\"))\n    result_df.to_csv(os.path.join(node_line_dir, \"result.csv\"))\n\n    summary_df = result_df.drop(columns=['retrieved_contents', 'retrieved_ids', 'retrieve_scores'])\n    summary_df.to_csv(os.path.join(node_line_dir, \"summary.csv"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    os.makedirs(node_line_dir, exist_ok=True)\n\n    # 1. Evaluate and select best module\n    result_dfs = list(map(lambda module, params: module(**params), modules, module_params))\n    result_dfs = list(map(lambda df: pd.concat([previous_result, df], axis=1), result_dfs))\n    result_dfs = list(map(lambda df: evaluate_retrieval_node(df, strategies['retrieval_gt'], strategies['metrics']), result_dfs))\n    result_dfs = list(map(lambda df: select_best_average(df, strategies['metrics']), result_dfs))\n    result_df = pd.concat(result_dfs)\n\n    # 2. Measure execution time\n    result_df = measure_speed(result_df, strategies['speed_thresholds'])\n\n    # 3. Save results\n    result_df.to_parquet(os.path.join(node_line_dir, \"result.parquet\"), index=False)\n\n    # 4. Save summary\n    summary_df = pd.DataFrame(columns=['filename', 'module_name', 'module_params', 'execution_time'])\n    for i, df in enumerate(result_dfs):\n        summary_df = pd.concat([summary_df, pd.DataFrame({\n            'filename': [f\"result_{i}.parquet\"],\n            'module_name': [modules[i].__name__],\n            'module_params': [module_params[i]],\n            'execution_time': [df['execution_time'].sum()],\n        })])\n    summary_df = pd.concat([summary_df, pd.DataFrame({\n        'filename': [\"result.parquet\"],\n        'module_name': [result_df.columns[0]],\n        'module_params': [{}],\n        'execution_time': [result_df['execution_time'].sum()],\n    })])\n    summary_df.to_csv("}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    os.makedirs(node_line_dir, exist_ok=True)\n\n    result_dfs = list(map(lambda module, params: module(**params), modules, module_params))\n\n    result_dfs = list(map(lambda df: df.assign(module_name=module.__name__, module_params=params), result_dfs, module_params))\n\n    result_dfs = list(map(lambda df: df.assign(execution_time=measure_speed(df)), result_dfs))\n\n    if strategies['speed_threshold']:\n        result_dfs = list(filter(lambda df: df['execution_time'] < strategies['speed_threshold'], result_dfs))\n\n    if strategies['metrics']:\n        result_dfs = list(map(lambda df: evaluate_retrieval_node(df, strategies['retrieval_gt'], strategies['metrics']), result_dfs))\n\n    result_dfs = list(map(lambda df: df.assign(filename=os.path.basename(df['filename'])), result_dfs))\n\n    result_df = pd.concat(result_dfs)\n\n    result_df = select_best_average(result_df, strategies['metrics'])\n\n    result_df = result_df.assign(execution_time=measure_speed(result_df))\n\n    result_df = result_df.assign(filename=os.path.basename(result_df['filename']))\n\n    result_df = pd.concat([previous_result, result_df], axis=1)\n\n    result_df.to_parquet(os.path.join(node_line_dir, \"result.parquet\"))\n\n    result_df.to_csv(os.path.join(node_line_dir, \"result.csv\"))\n\n    summary_df = load_summary_file(os.path.join(node_line_dir, \"summary.csv\"))\n\n    summary_df = pd.concat([summary_df, result_df], axis=0)\n\n    summary_df.to"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create directory for this node line\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Evaluate each module with given parameters\n    result_dfs = []\n    for module, module_param in zip(modules, module_params):\n        result_df = module(**module_param)\n        result_df['module_name'] = module.__name__\n        result_df['module_params'] = module_param\n        result_dfs.append(result_df)\n\n    # Combine previous result and retrieval node result\n    previous_result['retrieved_contents'] = previous_result['retrieved_contents'].apply(list)\n    previous_result['retrieved_ids'] = previous_result['retrieved_ids'].apply(list)\n    previous_result['retrieve_scores'] = previous_result['retrieve_scores'].apply(list)\n    previous_result['retrieved_contents'] = previous_result['retrieved_contents'].apply(lambda x: x + [None])\n    previous_result['retrieved_ids'] = previous_result['retrieved_ids'].apply(lambda x: x + [None])\n    previous_result['retrieve_scores'] = previous_result['retrieve_scores'].apply(lambda x: x + [None])\n    previous_result['module_name'] = 'previous_result'\n    previous_result['module_params'] = None\n    result_dfs.append(previous_result)\n\n    # Evaluate each result dataframe\n    for result_df in result_dfs:\n        result_df = evaluate_retrieval_node(result_df, strategies['retrieval_gt'], strategies['metrics'])\n\n    # Measure execution times for each result dataframe\n    for result_df in result_dfs:\n        result_df = measure_speed(result_df)\n\n    # Filter by speed threshold\n    if 'speed_threshold' in strategies:\n        for result_df in result_dfs:\n            result_df = filter_by_threshold(result_df, strategies['speed_"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    if len(modules) != len(module_params):\n        raise ValueError(f\"The number of modules and module_params must be the same. \"\n                         f\"Got {len(modules)} modules and {len(module_params)} module_params.\")\n\n    results_dfs = []\n    for module, params in zip(modules, module_params):\n        result_df = module(**params)\n        results_dfs.append(result_df)\n\n    results_df = pd.concat(results_dfs)\n    results_df = results_df.reset_index(drop=True)\n\n    if previous_result is not None:\n        results_df = pd.concat([previous_result, results_df], axis=1)\n\n    results_df = measure_speed(results_df)\n    results_df = filter_by_threshold(results_df, strategies['speed_threshold'])\n\n    if strategies['metrics']:\n        results_df = evaluate_retrieval_node(results_df, strategies['retrieval_gt'], strategies['metrics'])\n\n    if strategies['select_strategy']:\n        results_df = select_best_average(results_df, strategies['select_strategy'])\n\n    results_df.to_parquet(os.path.join(node_line_dir, \"result.parquet\"))\n\n    summary_df = load_summary_file(os.path.join(node_line_dir, \"summary.csv\"))\n    summary_df = pd.concat([summary_df, results_df], ignore_index=True)\n    summary_df.to_csv(os.path.join(node_line_dir, \"summary.csv\"), index=False)\n\n    return results_df"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    logger.info(f\"Running {len(modules)} retrieval modules.\")\n\n    # Create the directory for this node line if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Run each module with its parameters\n    results = []\n    for module, params in zip(modules, module_params):\n        result_df = module(previous_result, **params)\n        results.append(result_df)\n\n    # Evaluate each result\n    evaluation_results = []\n    for result_df, module in zip(results, modules):\n        evaluation_result = evaluate_retrieval_node(result_df, strategies['retrieval_gt'], strategies['metrics'])\n        evaluation_results.append(evaluation_result)\n\n    # Measure the execution time of each result\n    execution_times = []\n    for result_df in results:\n        execution_time = measure_speed(result_df)\n        execution_times.append(execution_time)\n\n    # Save results to disk\n    for result_df, filename in zip(results, strategies['filenames']):\n        result_df.to_parquet(os.path.join(node_line_dir, filename))\n\n    # Save evaluation results to disk\n    evaluation_results_df = pd.concat(evaluation_results, axis=1)\n    evaluation_results_df.to_parquet(os.path.join(node_line_dir, \"evaluation_results.parquet\"))\n\n    # Save execution times to disk\n    execution_times_df = pd.DataFrame({'execution_time': execution_times})\n    execution_times_df.to_parquet(os.path.join(node_line_dir, \"execution_times.parquet\"))\n\n    # Save summary of execution times and evaluation metrics to disk\n    summary_df = pd.concat([execution_times_df, evaluation_results_df], axis=1)\n    summary_df['module_name'] = strategies['module_names']\n    summary_df['module_params'] = strategies['"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create node line directory\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Run each module and measure execution time\n    results = []\n    for module, params in zip(modules, module_params):\n        result_df = module(**params)\n        result_df['module_name'] = module.__name__\n        result_df['module_params'] = params\n        result_df = measure_speed(result_df)\n        results.append(result_df)\n\n    # Evaluate each result\n    for result in results:\n        result = evaluate_retrieval_node(result, previous_result, strategies['metrics'])\n\n    # Select the best result\n    result_df = select_best_average(results, strategies['metrics'])\n\n    # Save results to disk\n    for result in results:\n        result.to_parquet(os.path.join(node_line_dir, f\"{result['module_name']}.parquet\"))\n\n    # Save summary of execution times and evaluation metrics to disk\n    summary_df = pd.concat(results)\n    summary_df.to_csv(os.path.join(node_line_dir, \"summary.csv\"), index=False)\n\n    # Apply strategies to filter and select the best result\n    if strategies['speed_threshold']:\n        result_df = filter_by_threshold(result_df, strategies['speed_threshold'])\n\n    # Combine the previous result columns with the selected retrieval node's result columns\n    result_df = pd.concat([previous_result, result_df], axis=1)\n\n    return result_df"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    logger.info(f\"Running retrieval node from {node_line_dir}\")\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # evaluate each module\n    result_dfs = list(map(lambda module, module_param: module(**module_param), modules, module_params))\n\n    # evaluate each module\n    result_dfs = list(map(lambda df: evaluate_retrieval_node(df, previous_result, strategies['metrics']), result_dfs))\n\n    # measure execution time of each module\n    result_dfs = list(map(measure_speed, result_dfs))\n\n    # filter by speed threshold\n    result_dfs = list(filter_by_threshold(result_dfs, strategies['speed_threshold']))\n\n    # select the best module\n    best_result_df = select_best_average(result_dfs, strategies['metrics'])\n\n    # save the result\n    best_result_df.to_parquet(os.path.join(node_line_dir, f\"{best_result_df.iloc[0]['module_name']}.parquet\"))\n\n    # save the summary\n    summary_df = pd.DataFrame(result_dfs)\n    summary_df.to_csv(os.path.join(node_line_dir, \"summary.csv\"), index=False)\n\n    # combine the previous result columns with the selected retrieval node's result columns\n    best_result_df = pd.concat([previous_result, best_result_df], axis=1)\n\n    return best_result_df"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    summary_df = pd.DataFrame(columns=['filename', 'module_name', 'module_params', 'execution_time'])\n\n    for module, module_param in zip(modules, module_params):\n        module_name = module.__name__\n        logger.info(f\"Running {module_name} with parameters {module_param}\")\n        result_df = module(previous_result, **module_param)\n\n        result_df = result_df.reset_index(drop=True)\n        result_df.insert(0, 'module_name', module_name)\n        result_df.insert(1, 'module_params', module_param)\n\n        result_df.to_parquet(os.path.join(node_line_dir, f\"{module_name}.parquet\"))\n\n        execution_time = measure_speed(module)\n        summary_df = summary_df.append(\n            {\n                'filename': f\"{module_name}.parquet\",\n                'module_name': module_name,\n                'module_params': module_param,\n                'execution_time': execution_time,\n            },\n            ignore_index=True\n        )\n\n    if 'metrics' in strategies.keys():\n        logger.info(f\"Evaluating results with metrics: {strategies['metrics']}\")\n        summary_df = evaluate_retrieval_node(summary_df, strategies['retrieval_gt'], strategies['metrics'])\n\n    if 'speed_threshold' in strategies.keys():\n        logger.info(f\"Filtering results by speed threshold: {strategies['speed_threshold']}\")\n        summary_df = filter_by_threshold(summary_df, strategies['speed_threshold'])\n\n    if 'select_best' in strategies.keys():\n        logger.info(f\"Selecting best result by {strategies['select_best']}\")\n        best_result = select_best_average(summary_df"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the directory for the node line\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create a list of module names\n    module_names = [module.__name__ for module in modules]\n\n    # Create a list of result dataframes\n    result_dfs = []\n\n    # Loop through the modules and their parameters\n    for module, module_params in zip(modules, module_params):\n        # Create a directory for the module\n        module_dir = os.path.join(node_line_dir, module.__name__)\n        os.makedirs(module_dir, exist_ok=True)\n\n        # Run the module with the given parameters\n        result_df = module(**module_params)\n\n        # Add the previous result columns to the result dataframe\n        result_df = pd.concat([previous_result, result_df], axis=1)\n\n        # Save the result dataframe to a parquet file\n        result_filename = os.path.join(module_dir, \"result.parquet\")\n        result_df.to_parquet(result_filename)\n\n        # Add the result dataframe to the list\n        result_dfs.append(result_df)\n\n    # Evaluate the results and select the best one\n    best_result_df = select_best_result(result_dfs, strategies, module_names, node_line_dir)\n\n    # Return the best result dataframe\n    return best_result_df\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Run each module with given parameters and measure execution times.\n    results = []\n    for module, params in zip(modules, module_params):\n        logger.info(f\"Running {module.__name__} with parameters {params}\")\n        result = measure_speed(module, **params)\n        results.append(result)\n\n    # Evaluate each result using given metrics.\n    results_df = pd.concat(results, axis=0)\n    results_df = evaluate_retrieval_node(results_df, previous_result, strategies['metrics'])\n    results_df = filter_by_threshold(results_df, strategies['speed_threshold'])\n    results_df = select_best_average(results_df, strategies['metrics'])\n\n    # Save results and summary of execution times and evaluation metrics to disk.\n    results_df.to_parquet(os.path.join(node_line_dir, \"results.parquet\"))\n    results_df.to_csv(os.path.join(node_line_dir, \"results.csv\"), index=False)\n    results_df.to_json(os.path.join(node_line_dir, \"results.json\"), orient=\"records\")\n    summary_df = load_summary_file(os.path.join(node_line_dir, \"summary.csv\"))\n    summary_df = summary_df.append(results_df, ignore_index=True)\n    summary_df.to_csv(os.path.join(node_line_dir, \"summary.csv\"), index=False)\n\n    return results_df"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create node directory if not exists\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Evaluate and select the best result among all modules\n    results = []\n    for module, params in zip(modules, module_params):\n        result_df = module(**params)\n        result_df = evaluate_retrieval_node(result_df, previous_result, strategies['metrics'])\n        results.append(result_df)\n    best_result = select_best_average(results, strategies['metrics'])\n\n    # Save the best result dataframe to a parquet file\n    filename = \"result.parquet\"\n    best_result.to_parquet(os.path.join(node_line_dir, filename), index=False)\n\n    # Save the summary of execution times and evaluation metrics to a CSV file\n    summary_df = pd.DataFrame(columns=['filename', 'module_name', 'module_params', 'execution_time'] + strategies['metrics'])\n    for i, (module, params) in enumerate(zip(modules, module_params)):\n        execution_time = measure_speed(module, params)\n        summary_df.loc[i] = [filename, module.__name__, params, execution_time] + list(best_result.loc[i, strategies['metrics']])\n    summary_df.to_csv(os.path.join(node_line_dir, \"summary.csv\"), index=False)\n\n    # Filter out results that are slower than the specified speed threshold\n    if 'speed_threshold' in strategies:\n        summary_df = filter_by_threshold(summary_df, strategies['speed_threshold'])\n        best_result = best_result[summary_df['filename'].isin(summary_df['filename'])]\n\n    # Return the best result dataframe\n    return best_result"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    logger.info(\"Running retrieval node\")\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Evaluate and select the best module among retrieval node results\n    result_dfs = []\n    for module, module_param in zip(modules, module_params):\n        result_df = module(**module_param)\n        result_df['module_name'] = module.__name__\n        result_df['module_params'] = module_param\n        result_dfs.append(result_df)\n\n    result_dfs = pd.concat(result_dfs, ignore_index=True)\n    result_dfs = pd.concat([previous_result, result_dfs], axis=1)\n\n    # Evaluate retrieval node\n    result_dfs = evaluate_retrieval_node(result_dfs,\n                                         strategies['retrieval_gt'],\n                                         strategies['retrieval_metrics'])\n\n    # Filter by threshold\n    result_dfs = filter_by_threshold(result_dfs, strategies['retrieval_threshold'])\n\n    # Select the best module among retrieval node results\n    result_dfs = select_best_average(result_dfs)\n\n    # Measure execution time\n    result_dfs = measure_speed(result_dfs, strategies['retrieval_speed_threshold'])\n\n    # Save results and summary\n    result_dfs.to_parquet(os.path.join(node_line_dir, \"result.parquet\"), index=False)\n    result_dfs.to_csv(os.path.join(node_line_dir, \"result.csv\"), index=False)\n    result_dfs.to_json(os.path.join(node_line_dir, \"result.json\"), orient='records', force_ascii=False)\n    result_dfs.to_feather(os.path.join(node_line_dir, \"result.feather\"))\n    result_dfs.to_pickle(os.path.join(node_line_"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Load previous result if exists.\n    if previous_result is not None:\n        previous_result = previous_result.copy()\n        previous_result['previous_result'] = True\n\n    # Run each module with given parameters and save the result to a parquet file.\n    result_df_list = []\n    for module, params in zip(modules, module_params):\n        result_df = module(**params)\n        result_df['module_name'] = module.__name__\n        result_df['module_params'] = params\n        result_df['filename'] = module.__name__ + '.parquet'\n        result_df.to_parquet(os.path.join(node_line_dir, result_df['filename'][0]))\n        result_df_list.append(result_df)\n\n    # Combine the results from each module into one dataframe.\n    result_df = pd.concat(result_df_list)\n\n    # Evaluate the results.\n    if 'evaluate' in strategies:\n        result_df = evaluate_retrieval_node(result_df, strategies['evaluate']['retrieval_gt'],\n                                            strategies['evaluate']['metrics'])\n\n    # Select the best result.\n    if 'select' in strategies:\n        if strategies['select']['strategy'] == 'speed':\n            if 'speed_threshold' in strategies['select']:\n                result_df = measure_speed(result_df, strategies['select']['speed_threshold'])\n            else:\n                result_df = measure_speed(result_df)\n        elif strategies['select']['strategy'] == 'threshold':\n            if 'threshold' in strategies['select']:\n                result_df = filter_by_threshold(result_df, strategies['select']['threshold'])\n            else:\n                raise ValueError(\"You must specify 'threshold' in the 'select' strategy.\")\n        elif strategies['select']['strategy'] == 'average':\n            if 'metrics' in"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    logger.info(\"Starting query expansion node\")\n    os.makedirs(node_line_dir, exist_ok=True)\n    results = []\n    for module, params in zip(modules, module_params):\n        logger.info(f\"Running query expansion module {module.__name__} with params {params}\")\n        result = evaluate_one_query_expansion_node(modules, params, previous_result, node_line_dir, strategies)\n        results.append(result)\n    logger.info(\"Finished query expansion node\")\n    return select_best_result(results, strategies)"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n    logger.info(f\"Running query expansion modules: {modules}\")\n    logger.info(f\"Running query expansion module parameters: {module_params}\")\n    logger.info(f\"Running query expansion previous result: {previous_result}\")\n    logger.info(f\"Running query expansion node line directory: {node_line_dir}\")\n    logger.info(f\"Running query expansion strategies: {strategies}\")\n    logger.info(f\"Running query expansion node line directory: {node_line_dir}\")\n    logger.info(f\"Running query expansion node line directory: {node_line_dir}\")\n    logger.info(f\"Running query expansion node line directory: {node_line_dir}\")\n    logger.info(f\"Running query expansion node line directory: {node_line_dir}\")\n    logger.info(f\"Running query expansion node line directory: {node_line_dir}\")\n    logger.info(f\"Running query expansion node line directory: {node_line_dir}\")\n    logger.info(f\"Running query expansion node line directory: {node_line_dir}\")\n    logger.info(f\"Running query expansion node line directory: {node_line_dir}\")\n    logger.info(f\"Running query expansion node line directory: {node_line_dir}\")\n    logger.info(f\"Running query expansion node line directory: {node_line_dir}\")\n    logger.info(f\"Running query expansion node line directory: {node_line_dir}\")\n    logger.info(f\"Running query expansion node line directory: {node_line_dir}\")\n    logger.info(f\"Running query expansion node line directory: {node_line_dir}\")\n    logger.info(f\"Running query expansion node line directory: {node_line_dir}\")\n    logger.info(f\"Running query expansion node line directory: {node_line_dir}\")\n    logger.info(f\"Running query expansion node line directory: {node_line_dir}\")\n    logger"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    os.makedirs(node_line_dir, exist_ok=True)\n    expanded_queries = previous_result['queries'].tolist()\n    retrieval_gt = previous_result['retrieval_gt'].tolist()\n    retrieval_funcs, retrieval_params = make_retrieval_callable_params(strategies)\n    evaluation_results = list(map(lambda x: evaluate_one_query_expansion_node(retrieval_funcs, retrieval_params,\n                                                                              expanded_queries, retrieval_gt,\n                                                                              strategies['metrics'],\n                                                                              project_dir=node_line_dir,\n                                                                              previous_result=previous_result),\n                                  zip(modules, module_params)))\n    best_result, _ = select_best_average(evaluation_results, strategies['metrics'])\n    best_result.to_csv(pathlib.Path(node_line_dir, \"best_result.csv\"))\n    return best_result"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    logger.info(f\"Starting query expansion node with {len(modules)} modules\")\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n    node_line_dir = pathlib.Path(node_line_dir)\n    node_line_dir.mkdir(parents=True, exist_ok=True)\n    node_line_dir.mkdir(parents=True, exist_ok=True)\n    node_line_dir.mkdir(parents=True, exist_ok=True)\n    node_line_dir.mkdir(parents=True, exist_ok=True)\n    node_line_dir.mkdir(parents=True, exist_ok=True)\n    node_line_dir.mkdir(parents=True, exist_ok=True)\n    node_line_dir.mkdir(parents=True, exist_ok=True)\n    node_line_dir.mkdir(parents=True, exist_ok=True)\n    node_line_dir.mkdir(parents=True, exist_ok=True)\n    node_line_dir.mkdir(parents=True, exist_ok=True)\n    node_line_dir.mkdir(parents=True, exist_ok=True)\n    node_line_dir.mkdir(parents=True, exist_ok=True)\n    node_line_dir.mkdir(parents=True, exist_ok=True)\n    node_line_dir.mkdir(parents=True, exist_ok=True)\n    node_line_dir.mkdir(parents=True, exist_ok=True)\n    node_line_dir.mkdir(parents=True, exist_ok=True)\n    node_line_dir.mkdir(parents=True, exist_ok=True)\n    node_line_dir.mkdir(parents=True, exist_ok=True)\n    node_line_dir.mkdir(parents=True, exist_ok=True)\n    node_line_dir.mkdir(parents=True, exist_ok=True)\n    node_line_dir.mkdir"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    logger.info(f\"Running query expansion node with {len(modules)} modules.\")\n    node_line_dir = os.path.join(node_line_dir, \"query_expansion\")\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n    query_expansion_results = list(map(lambda x: x[0](project_dir=node_line_dir, previous_result=previous_result, **x[1]),\n                                       zip(modules, module_params)))\n    query_expansion_results_df = pd.concat(query_expansion_results, axis=0)\n    query_expansion_results_df.to_csv(os.path.join(node_line_dir, \"query_expansion_results.csv\"), index=False)\n    query_expansion_results_df.to_pickle(os.path.join(node_line_dir, \"query_expansion_results.pkl\"))\n    query_expansion_results_df.to_json(os.path.join(node_line_dir, \"query_expansion_results.json\"), orient=\"records\")\n    query_expansion_results_df.to_parquet(os.path.join(node_line_dir, \"query_expansion_results.parquet\"), index=False)\n    query_expansion_results_df.to_feather(os.path.join(node_line_dir, \"query_expansion_results.feather\"), index=False)\n    query_expansion_results_df.to_hdf(os.path.join(node_line_dir, \"query_expansion_results.hdf\"), key=\"query_expansion_results\", index=False)\n    query_expansion_results_df.to_excel(os.path.join(node_line_dir, \"query_expansion_results.xlsx\"), index=False)\n    query_expansion_results_df.to_html(os.path.join(node_line_dir, \"query_expansion_results.html\"), index=False"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n    logger.info(f\"Starting to run query expansion node with {len(modules)} modules\")\n    results = []\n    for module, params in zip(modules, module_params):\n        logger.info(f\"Running module {module.__name__} with params {params}\")\n        result = evaluate_one_query_expansion_node(module, params, previous_result, node_line_dir)\n        results.append(result)\n    logger.info(f\"Finished running {len(modules)} modules\")\n    logger.info(f\"Starting to evaluate results\")\n    evaluation_results = list(map(lambda x: measure_speed(x, strategies['speed_threshold']), results))\n    evaluation_results = list(map(lambda x: filter_by_threshold(x, strategies['speed_threshold']), evaluation_results))\n    evaluation_results = list(map(lambda x: evaluate_retrieval_node(x, strategies['metrics']), evaluation_results))\n    best_result, best_result_index = select_best_average(evaluation_results, strategies['metrics'])\n    logger.info(f\"Best result is {best_result}\")\n    logger.info(f\"Best result index is {best_result_index}\")\n    logger.info(f\"Best result is {best_result}\")\n    logger.info(f\"Best result index is {best_result_index}\")\n    logger.info(f\"Best result is {best_result}\")\n    logger.info(f\"Best result index is {best_result_index}\")\n    logger.info(f\"Best result is {best_result}\")\n    logger.info(f\"Best result index is {best_result_index}\")\n    logger.info(f\"Best result is {best_result}\")\n    logger.info(f\"Best result index is {best_result_index}\")\n    logger.info(f\"Best result is {best_result}\")\n    logger.info(f\"Best"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    logger.info(\"Running Query Expansion Node\")\n    logger.info(\"Running all query expansion modules\")\n    expanded_queries = list(map(lambda x: x[0](project_dir=node_line_dir, previous_result=previous_result, **x[1]),\n                                zip(modules, module_params)))\n    logger.info(\"Running all query expansion modules\")\n    retrieval_funcs, retrieval_params = make_retrieval_callable_params(strategies)\n    retrieval_gt = list(map(lambda x: x['query_expansion_gt'], strategies['retrieval_modules']))\n    metrics = strategies['metrics']\n    logger.info(\"Evaluating all query expansion modules\")\n    evaluation_results = list(map(lambda x: evaluate_one_query_expansion_node(retrieval_funcs, retrieval_params,\n                                                                              x, retrieval_gt, metrics,\n                                                                              node_line_dir, previous_result),\n                                  expanded_queries))\n    logger.info(\"Evaluating all query expansion modules\")\n    best_result, _ = select_best_average(evaluation_results, metrics)\n    logger.info(\"Saving the best result\")\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'), index=False)\n    logger.info(\"Saving the best result\")\n    return best_result"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    logger.info(f\"Running query expansion node with {len(modules)} modules\")\n    query_expansion_results = []\n    for module, params in zip(modules, module_params):\n        result = evaluate_one_query_expansion_node(module, params, previous_result, node_line_dir)\n        query_expansion_results.append(result)\n    best_result = select_best_query_expansion_result(query_expansion_results, strategies)\n    return best_result\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    retrieval_funcs, retrieval_params = make_retrieval_callable_params(strategies)\n    expanded_queries = previous_result['queries'].to_list()\n    retrieval_gt = previous_result['ground_truth'].to_list()\n    metrics = strategies['metrics']\n    results = list(map(lambda x: evaluate_one_query_expansion_node(retrieval_funcs, retrieval_params, expanded_queries,\n                                                                   retrieval_gt, metrics, project_dir=node_line_dir,\n                                                                   previous_result=previous_result),\n                       zip(modules, module_params)))\n\n    results_df = pd.concat(results, axis=0)\n    results_df.to_csv(os.path.join(node_line_dir, 'results.csv'), index=False)\n\n    strategies_df = pd.DataFrame(strategies)\n    strategies_df.to_csv(os.path.join(node_line_dir, 'strategies.csv'), index=False)\n\n    results_df = results_df.groupby(results_df.columns.tolist()).mean().reset_index()\n    results_df.to_csv(os.path.join(node_line_dir, 'summary.csv'), index=False)\n\n    best_result, _ = select_best_average(results_df, metrics)\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'), index=False)\n    return best_result"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    logger.info(\"Starting query expansion node\")\n    os.makedirs(node_line_dir, exist_ok=True)\n    expanded_queries = previous_result['queries'].values\n    retrieval_gt = previous_result['retrieval_gt'].values\n    retrieval_funcs, retrieval_params = make_retrieval_callable_params(strategies)\n    results = list(map(lambda x: evaluate_one_query_expansion_node(retrieval_funcs, retrieval_params, expanded_queries,\n                                                                   retrieval_gt, strategies['metrics'],\n                                                                   node_line_dir, previous_result),\n                       zip(modules, module_params)))\n    results_df = pd.concat(results)\n    results_df.to_csv(os.path.join(node_line_dir, 'results.csv'), index=False)\n    results_df.to_csv(os.path.join(node_line_dir, 'results.tsv'), index=False, sep='\\t')\n    summary_df = results_df.groupby('module_name').agg({'execution_time': 'mean', 'metrics': lambda x: x.iloc[0]})\n    summary_df.to_csv(os.path.join(node_line_dir, 'summary.csv'))\n    summary_df.to_csv(os.path.join(node_line_dir, 'summary.tsv'), sep='\\t')\n    best_result, _ = select_best_average(results_df['metrics'].values, strategies['metrics'])\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'))\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.tsv'), sep='\\t')\n    return best_result"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    node_line_dir = os.path.join(node_line_dir, \"query_expansion\")\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n    logger.info(\"Evaluating query expansion modules...\")\n    expanded_queries = list(map(lambda x: x[0](project_dir=node_line_dir, previous_result=previous_result, **x[1]),\n                                zip(modules, module_params)))\n    logger.info(\"Evaluating query expansion modules...done\")\n\n    logger.info(\"Evaluating query expansion modules...\")\n    retrieval_funcs, retrieval_params = make_retrieval_callable_params(strategies)\n    retrieval_gt = list(map(lambda x: x['ground_truth'], strategies['retrieval_modules']))\n    evaluation_results = list(map(lambda x: evaluate_one_query_expansion_node(retrieval_funcs, retrieval_params,\n                                                                              x, retrieval_gt, strategies['metrics'],\n                                                                              node_line_dir, previous_result),\n                                  expanded_queries))\n    logger.info(\"Evaluating query expansion modules...done\")\n\n    logger.info(\"Selecting best query expansion module...\")\n    best_result, _ = select_best_average(evaluation_results, strategies['metrics'])\n    logger.info(\"Selecting best query expansion module...done\")\n\n    best_result.to_csv(os.path.join(node_line_dir, \"best_result.csv\"), index=False)\n    pd.DataFrame(evaluation_results).to_csv(os.path.join(node_line_dir, \"evaluation_results.csv\"), index=False)\n    return best_result"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    os.makedirs(node_line_dir, exist_ok=True)\n    result_list = []\n    for module, module_param in zip(modules, module_params):\n        result = module(**module_param)\n        result_list.append(result)\n    result_df = pd.concat(result_list, axis=0)\n    result_df.to_csv(os.path.join(node_line_dir, \"results.csv\"), index=False)\n    result_df.to_csv(os.path.join(node_line_dir, \"results_with_previous_result.csv\"), index=False)\n    result_df = evaluate_one_query_expansion_node(previous_result=previous_result,\n                                                  expanded_queries=result_df['queries'].to_list(),\n                                                  retrieval_gt=result_df['retrieval_gt'].to_list(),\n                                                  retrieval_funcs=strategies['retrieval_modules'],\n                                                  retrieval_params=make_retrieval_callable_params(strategies),\n                                                  metrics=strategies['metrics'],\n                                                  project_dir=node_line_dir)\n    result_df.to_csv(os.path.join(node_line_dir, \"results.csv\"), index=False)\n    result_df.to_csv(os.path.join(node_line_dir, \"results_with_previous_result.csv\"), index=False)\n    result_df.to_csv(os.path.join(node_line_dir, \"summary.csv\"), index=False)\n    best_result = select_best_average(result_df, strategies['metrics'])\n    best_result.to_csv(os.path.join(node_line_dir, \"best_result.csv\"), index=False)\n    return best_result"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    logger.info(f\"Running query expansion node with {len(modules)} modules\")\n    module_results = []\n    for module, params in zip(modules, module_params):\n        module_results.append(module(**params))\n\n    logger.info(f\"Evaluating query expansion node with {len(modules)} modules\")\n    evaluation_results = list(map(lambda x: evaluate_one_query_expansion_node(*x),\n                                  zip(modules, module_params, module_results, [previous_result] * len(modules),\n                                      strategies['metrics'])))\n\n    logger.info(f\"Selecting best query expansion module based on evaluation results\")\n    best_result, best_result_index = select_best_average(evaluation_results, strategies['metrics'])\n\n    logger.info(f\"Saving query expansion node results and summary\")\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n    best_result.to_csv(os.path.join(node_line_dir, \"query_expansion_result.csv\"), index=False)\n    pd.DataFrame(evaluation_results).to_csv(os.path.join(node_line_dir, \"query_expansion_summary.csv\"), index=False)\n\n    logger.info(f\"Best query expansion module: {modules[best_result_index]}\")\n    return best_result"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    logger.info(\"Running query expansion node.\")\n    os.makedirs(node_line_dir, exist_ok=True)\n    retrieval_funcs, retrieval_params = make_retrieval_callable_params(strategies)\n    expanded_queries = previous_result['queries'].tolist()\n    retrieval_gt = previous_result['ground_truth'].tolist()\n    metrics = strategies['metrics']\n    project_dir = node_line_dir\n    # get all possible combinations of modules and their parameters\n    results = list(map(lambda x: evaluate_one_query_expansion_node(retrieval_funcs,\n                                                                   retrieval_params,\n                                                                   expanded_queries,\n                                                                   retrieval_gt,\n                                                                   metrics,\n                                                                   project_dir,\n                                                                   previous_result),\n                       zip(modules, module_params)))\n    # save results\n    results_df = pd.concat(results)\n    results_df.to_csv(pathlib.Path(node_line_dir) / 'results.csv', index=False)\n    # save summaries\n    summaries = list(map(lambda x: x.summarize(), results))\n    summaries_df = pd.DataFrame(summaries)\n    summaries_df.to_csv(pathlib.Path(node_line_dir) / 'summaries.csv', index=False)\n    # select best result\n    best_result = select_best_average(summaries, metrics)\n    best_result.to_csv(pathlib.Path(node_line_dir) / 'best_result.csv', index=False)\n    return best_result"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    logger.info(\"Running query expansion node\")\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # TODO: add support for multiple modules\n    module_params = [module_params[0]]\n    modules = [modules[0]]\n    # TODO: add support for multiple modules\n    module_params = [module_params[0]]\n    modules = [modules[0]]\n    expanded_queries = [previous_result['queries'].tolist()]\n    retrieval_gt = [previous_result['ground_truth'].tolist()]\n    retrieval_callable_params = make_retrieval_callable_params(strategies)\n    retrieval_funcs, retrieval_params = list(zip(*retrieval_callable_params))\n    evaluation_results = []\n    for module, params in zip(modules, module_params):\n        logger.info(f\"Running {module.__name__} with params {params}\")\n        module_result = evaluate_one_query_expansion_node(retrieval_funcs, retrieval_params, expanded_queries,\n                                                          retrieval_gt, strategies['metrics'], node_line_dir,\n                                                          previous_result)\n        evaluation_results.append(module_result)\n    best_result, _ = select_best_average(evaluation_results, strategies['metrics'])\n    best_result.to_csv(pathlib.Path(node_line_dir) / \"best_result.csv\", index=False)\n    return best_result"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    logger.info(\"Running Query Expansion\")\n    os.makedirs(node_line_dir, exist_ok=True)\n    results = []\n    for module, params in zip(modules, module_params):\n        logger.info(f\"Running {module.__name__} with params {params}\")\n        result = module(**params)\n        result['module'] = module.__name__\n        results.append(result)\n    results_df = pd.concat(results)\n    results_df.to_csv(os.path.join(node_line_dir, 'results.csv'), index=False)\n    evaluation_results = evaluate_query_expansion_node(results_df, strategies)\n    evaluation_results.to_csv(os.path.join(node_line_dir, 'summary.csv'), index=False)\n    best_result, best_result_idx = select_best_average(evaluation_results, strategies['metrics'])\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'), index=False)\n    logger.info(f\"Best result is {best_result_idx}\")\n    return best_result\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Make the directory if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Evaluate and select the best module\n    results = evaluate_query_expansion_node(modules, module_params, previous_result, node_line_dir, strategies)\n    best_result, best_result_metrics = select_best_average(results, strategies['metrics'])\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'), index=False)\n\n    # Save the results and summaries\n    save_results(results, node_line_dir)\n    save_summaries(results, best_result_metrics, node_line_dir)\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    os.makedirs(node_line_dir, exist_ok=True)\n\n    # make a callable list of query expansion modules with their parameters\n    query_expansion_callable_params = make_retrieval_callable_params(strategies)\n    # evaluate each module\n    evaluation_results = list(map(lambda x: evaluate_one_query_expansion_node(modules,\n                                                                              x[0],\n                                                                              x[1],\n                                                                              x[2],\n                                                                              strategies['metrics'],\n                                                                              node_line_dir,\n                                                                              previous_result),\n                                  query_expansion_callable_params))\n\n    # select the best result based on the evaluation metrics\n    best_result, best_result_index = select_best_average(evaluation_results, strategies['metrics'])\n\n    # save the results and a summary\n    result_df = pd.DataFrame(evaluation_results)\n    result_df.to_csv(os.path.join(node_line_dir, 'result.csv'), index=False)\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'), index=False)\n    summary_df = result_df.describe().transpose()\n    summary_df.to_csv(os.path.join(node_line_dir, 'summary.csv'))\n\n    # save the best result\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'), index=False)\n\n    return best_result"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    logger.info(\"Running Query Expansion Nodes\")\n    node_line_dir = pathlib.Path(node_line_dir)\n    node_line_dir.mkdir(parents=True, exist_ok=True)\n    os.chdir(node_line_dir)\n\n    retrieval_callable_params = make_retrieval_callable_params(strategies)\n    retrieval_funcs, retrieval_params = zip(*retrieval_callable_params)\n    expanded_queries = previous_result['queries'].to_list()\n    retrieval_gt = previous_result['retrieval_gt'].to_list()\n    metrics = strategies['metrics']\n\n    evaluation_results = list(map(lambda x: evaluate_one_query_expansion_node(retrieval_funcs, retrieval_params,\n                                                                              expanded_queries, retrieval_gt,\n                                                                              metrics, node_line_dir, x),\n                                  zip(modules, module_params)))\n\n    evaluation_results = list(map(lambda x: pd.concat([x[0], x[1]], axis=1),\n                                  zip(evaluation_results, evaluation_results)))\n    evaluation_results = pd.concat(evaluation_results, axis=0)\n    evaluation_results.to_csv(node_line_dir / 'evaluation_results.csv', index=False)\n\n    best_result, best_result_metrics = select_best_average(evaluation_results, metrics)\n    best_result.to_csv(node_line_dir / 'best_result.csv', index=False)\n    best_result_metrics.to_csv(node_line_dir / 'best_result_metrics.csv', index=False)\n    return best_result"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create the directory if it doesn't exist\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create a list of callable query expansion modules\n    query_expansion_modules = list(map(lambda module: get_support_modules(module), modules))\n\n    # Create a list of callable query expansion modules with their parameters\n    query_expansion_params = list(map(lambda module_params: make_combinations(module_params), module_params))\n\n    # Create a list of expanded queries\n    expanded_queries = list(map(lambda module: module(previous_result['queries']), query_expansion_modules))\n\n    # Create a list of retrieval modules\n    retrieval_modules = list(map(lambda module: get_support_modules(module), strategies['retrieval_modules']))\n\n    # Create a list of retrieval modules with their parameters\n    retrieval_params = list(map(lambda module_params: make_combinations(module_params), strategies['retrieval_params']))\n\n    # Create a list of ground truth queries\n    retrieval_gt = list(map(lambda module: module(previous_result['queries']), strategies['retrieval_gt']))\n\n    # Create a list of metrics\n    metrics = strategies['metrics']\n\n    # Evaluate each query expansion module with its parameters and ground truth queries\n    evaluation_results = list(map(lambda x: evaluate_one_query_expansion_node(retrieval_modules, retrieval_params,\n                                                                              x[0], x[1], metrics, node_line_dir,\n                                                                              previous_result),\n                                  zip(expanded_queries, retrieval_gt)))\n\n    # Filter the evaluation results based on the speed threshold\n    filtered_evaluation_results = filter_by_threshold(evaluation_results, strategies['speed_thresholds'])\n\n    # Select the best result based on the evaluation metrics\n    best_result, best_index = select_best_average(filtered_evaluation_results, metrics)\n\n    # Save the best result to a"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    validate_qa_dataset(previous_result)\n    os.makedirs(node_line_dir, exist_ok=True)\n    generator_funcs, generator_params = make_generator_callable_params(strategies)\n    best_result = evaluate_one_prompt_maker_node(generator_funcs,\n                                                 generator_params,\n                                                 previous_result['prompts'].tolist(),\n                                                 previous_result['generation_gt'].tolist(),\n                                                 strategies['metrics'],\n                                                 node_line_dir)\n    best_result = pd.concat([previous_result, best_result], axis=0)\n    best_result.to_csv(pathlib.Path(node_line_dir, 'result.csv'), index=False)\n    return best_result\n\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n    generator_module_list: Optional[List[Dict]] = strategies.pop('generator_modules', None)\n    if generator_module_list is None:\n        generator_module_list = [{\n            'module_type': 'llama_index_llm',\n            'llm': 'openai',\n            'model': 'gpt-3.5-turbo',\n        }]\n    node_params = strategies\n    modules = list(map(lambda module_dict: get_support_modules(module_dict.pop('module_type')),\n                       generator_module_list))\n    param_combinations = list(map(lambda module_dict: make_combinations({**module_dict, **node_params}),\n                                  generator_module_list))\n    generator_funcs = list(map(lambda module: module[0], modules))\n    generator_params = make_generator_callable_params(node_params)\n    generator_params = list(map(lambda x: x[0](project_dir=node_line_dir, previous_result=previous_result, **x[1]),\n                                zip(generator_funcs, generator_params)))\n    prompts = previous_result['prompts'].tolist()\n    generation_gt = previous_result['generation_gt'].tolist()\n    metrics = cast_metrics(node_params.pop('metrics', None))\n    evaluation_results = list(map(lambda x: evaluate_generator_result(x[0], generation_gt, metrics),\n                                  zip(generator_params, generator_funcs)))\n    metric_names = list(map(lambda x: x['metric_name'], metrics)) if isinstance(metrics[0], dict) else metrics\n    best_result, best_result_index = select_best_average(evaluation_results, metric_names)\n    best_result = pd.concat([previous_result, best_result], axis=1)\n    best_result['module_type'] = modules[best"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    generator_funcs, generator_params = make_generator_callable_params(strategies)\n    prompts = previous_result['prompts'].tolist()\n    generation_gt = previous_result['generation_gt'].tolist()\n    metrics = strategies['metrics']\n    project_dir = os.path.join(node_line_dir, 'prompt_maker')\n    os.makedirs(project_dir, exist_ok=True)\n    result_df = pd.DataFrame()\n    for module, module_param in zip(modules, module_params):\n        result_df = pd.concat([result_df, evaluate_one_prompt_maker_node(generator_funcs,\n                                                                         generator_params,\n                                                                         prompts,\n                                                                         generation_gt,\n                                                                         metrics,\n                                                                         project_dir)], ignore_index=True)\n    result_df = pd.concat([previous_result, result_df], ignore_index=True)\n    result_df = cast_metrics(result_df, metrics)\n    result_df = filter_by_threshold(result_df, strategies['thresholds'])\n    result_df = select_best_average(result_df, strategies['metrics'])\n    result_df.to_csv(os.path.join(node_line_dir, 'prompt_maker', 'summary.csv'), index=False)\n    return result_df"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    generator_module_list: Optional[List[Dict]] = strategies.pop('generator_modules', None)\n    if generator_module_list is None:\n        generator_module_list = [{\n            'module_type': 'llama_index_llm',\n            'llm': 'openai',\n            'model': 'gpt-3.5-turbo',\n        }]\n    node_params = strategies\n    modules = list(map(lambda module_dict: get_support_modules(module_dict.pop('module_type')),\n                       generator_module_list))\n    param_combinations = list(map(lambda module_dict: make_combinations({**module_dict, **node_params}),\n                                  generator_module_list))\n    generator_callable_params = explode(modules, param_combinations)\n    generator_funcs, generator_params = zip(*generator_callable_params)\n    generator_funcs = list(generator_funcs)\n    generator_params = list(generator_params)\n    generator_params = list(map(lambda x: cast_metrics(x), generator_params))\n\n    prompt_maker_results = list(map(lambda x: x[0](project_dir=node_line_dir, previous_result=previous_result, **x[1]),\n                                    zip(modules, module_params)))\n    evaluation_results = list(map(lambda x: evaluate_one_prompt_maker_node(generator_funcs, generator_params,\n                                                                           x['prompts'], x['generation_gt'],\n                                                                           strategies['metrics'], node_line_dir),\n                                  prompt_maker_results))\n    best_result, _ = select_best_average(evaluation_results, strategies['metric_names'])\n    best_result = pd.concat([previous_result, best_result], axis=1)\n    return best_result"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n    else:\n        raise ValueError(f'{node_line_dir} already exists.')\n\n    generator_modules = strategies.get('generator_modules', [])\n    generator_module_params = make_generator_callable_params(strategies)\n    generator_module_names = list(map(lambda x: x['module_type'], generator_modules))\n    generator_module_params = list(map(lambda x: x['params'], generator_module_params))\n    generator_module_params = list(map(lambda x: {**x, 'project_dir': node_line_dir}, generator_module_params))\n    generator_module_params = list(map(lambda x: {**x, 'generator_module_name': x['module_type']},\n                                       generator_module_params))\n    generator_module_params = list(map(lambda x: {**x, 'generator_module_params': x['params']},\n                                       generator_module_params))\n    generator_module_params = list(map(lambda x: {**x, 'generator_module_params': x['params']},\n                                       generator_module_params))\n    generator_module_params = list(map(lambda x: {**x, 'generator_module_params': x['params']},\n                                       generator_module_params))\n    generator_module_params = list(map(lambda x: {**x, 'generator_module_params': x['params']},\n                                       generator_module_params))\n    generator_module_params = list(map(lambda x: {**x, 'generator_module_params': x['params']},\n                                       generator_module_params))\n    generator_module_params = list(map(lambda x: {**x, 'generator_module_params': x['params']},\n                                       generator_module_params))\n    generator_module_params = list(map(lambda x: {**x, 'generator_module_params': x['params']},\n                                       generator_module_params))\n    generator_module_params = list"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # check if previous result is empty\n    if previous_result.empty:\n        previous_result = pd.DataFrame()\n\n    # validate dataset\n    validate_qa_dataset(previous_result)\n\n    # create node line directory if not exists\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # make generator callable\n    generator_funcs, generator_params = make_generator_callable_params(strategies)\n\n    # make prompt maker callable\n    prompt_maker_funcs, prompt_maker_params = make_prompt_maker_callable_params(modules, module_params)\n\n    # make prompt maker callable\n    prompt_maker_funcs, prompt_maker_params = make_prompt_maker_callable_params(modules, module_params)\n\n    # make prompt maker callable\n    prompt_maker_funcs, prompt_maker_params = make_prompt_maker_callable_params(modules, module_params)\n\n    # make prompt maker callable\n    prompt_maker_funcs, prompt_maker_params = make_prompt_maker_callable_params(modules, module_params)\n\n    # make prompt maker callable\n    prompt_maker_funcs, prompt_maker_params = make_prompt_maker_callable_params(modules, module_params)\n\n    # make prompt maker callable\n    prompt_maker_funcs, prompt_maker_params = make_prompt_maker_callable_params(modules, module_params)\n\n    # make prompt maker callable\n    prompt_maker_funcs, prompt_maker_params = make_prompt_maker_callable_params(modules, module_params)\n\n    # make prompt maker callable\n    prompt_maker_funcs, prompt_maker_params = make_prompt_maker_callable_params(modules, module_params)\n\n    # make prompt maker callable\n    prompt_maker_funcs, prompt_maker_params = make_prompt_maker_callable_params(modules, module_params)\n\n    # make prompt maker callable"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n    node_line_dir = pathlib.Path(node_line_dir)\n    generator_module_list = strategies.get('generator_modules', [{\n        'module_type': 'llama_index_llm',\n        'llm': 'openai',\n        'model': 'gpt-3.5-turbo',\n    }])\n    generator_module_params = make_generator_callable_params(strategies)\n    generator_funcs, generator_params = zip(*generator_module_params)\n    prompts = previous_result['prompts'].tolist()\n    generation_gt = previous_result['generation_gt'].tolist()\n    metrics = strategies.get('metrics', ['rouge', 'bleu', 'bertscore'])\n    metrics = cast_metrics(metrics)\n    best_result = evaluate_one_prompt_maker_node(generator_funcs, generator_params, prompts, generation_gt, metrics,\n                                                 node_line_dir)\n    best_result = pd.concat([previous_result, best_result], axis=1)\n    return best_result"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    validate_qa_dataset(previous_result)\n    os.makedirs(node_line_dir, exist_ok=True)\n    generator_callable_params = make_generator_callable_params(strategies)\n    result_df = pd.DataFrame()\n    for i, (generator_func, generator_param) in enumerate(generator_callable_params):\n        result_df = pd.concat([result_df, run_one_prompt_maker_node(modules,\n                                                                    module_params,\n                                                                    previous_result,\n                                                                    node_line_dir,\n                                                                    generator_func,\n                                                                    generator_param,\n                                                                    strategies)])\n    result_df = pd.concat([result_df, previous_result])\n    result_df = result_df.drop_duplicates(subset=['prompts'])\n    result_df.to_csv(os.path.join(node_line_dir, 'result.csv'), index=False)\n    return result_df\n\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    node_dir = os.path.join(node_line_dir, 'prompt_maker')\n    if not os.path.exists(node_dir):\n        os.makedirs(node_dir)\n    prompts = previous_result['prompts'].tolist()\n    generation_gt = previous_result['generation_gt'].tolist()\n    metrics = strategies['metrics']\n    metrics = cast_metrics(metrics)\n    generator_funcs, generator_params = make_generator_callable_params(strategies['generator_modules'])\n    prompt_maker_results = list(map(lambda x: evaluate_one_prompt_maker_node(generator_funcs, generator_params,\n                                                                             prompts, generation_gt, metrics,\n                                                                             node_dir),\n                                    zip(modules, module_params)))\n    best_result, _ = select_best_average(prompt_maker_results, metrics)\n    best_result = pd.concat([previous_result, best_result], axis=1)\n    return best_result\n\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    validate_qa_dataset(previous_result)\n    generator_callable_params = make_generator_callable_params(strategies)\n    generator_modules = list(map(lambda x: x[0], generator_callable_params))\n    generator_params = list(map(lambda x: x[1], generator_callable_params))\n    prompts = previous_result['prompts'].tolist()\n    generation_gt = previous_result['generation_gt'].tolist()\n    metrics = strategies['metrics']\n    project_dir = strategies.get('project_dir', None)\n    if project_dir is None:\n        project_dir = os.getcwd()\n    node_line_dir = os.path.join(project_dir, node_line_dir)\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n    result_list = []\n    for module, module_param in zip(modules, module_params):\n        result = module(project_dir=project_dir, previous_result=previous_result, **module_param)\n        result_list.append(result)\n    result_df = pd.concat(result_list, axis=0)\n    result_df = cast_metrics(result_df, metrics)\n    result_df.to_csv(os.path.join(node_line_dir, 'result.csv'), index=False)\n    best_result = evaluate_one_prompt_maker_node(generator_modules,\n                                                 generator_params,\n                                                 prompts,\n                                                 generation_gt,\n                                                 metrics,\n                                                 project_dir)\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'), index=False)\n    summary_df = pd.concat([previous_result, best_result], axis=0)\n    summary_df.to_csv(os.path.join(node_line_dir, 'summary.csv'), index=False)\n    return summary_df"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    validate_qa_dataset(previous_result)\n    os.makedirs(node_line_dir, exist_ok=True)\n    generator_modules = strategies.pop('generator_modules', None)\n    generator_params = make_generator_callable_params(strategies)\n    module_params = make_combinations(module_params)\n    module_params = list(map(lambda x: {**x, **strategies}, module_params))\n    module_params = explode(modules, module_params)\n    module_params = list(map(lambda x: {**x[1], **{'generator_modules': generator_modules}},\n                             zip(modules, module_params)))\n    module_params = list(map(lambda x: {**x[1], **{'generator_params': generator_params}},\n                             zip(modules, module_params)))\n    module_params = list(map(lambda x: {**x[1], **{'project_dir': node_line_dir}},\n                             zip(modules, module_params)))\n    module_params = list(map(lambda x: {**x[1], **{'previous_result': previous_result}},\n                             zip(modules, module_params)))\n    result_df = pd.concat(list(map(lambda x: x[0](**x[1]), module_params)))\n    result_df = cast_metrics(result_df, strategies)\n    result_df = filter_by_threshold(result_df, strategies)\n    result_df = select_best_average(result_df, strategies)\n    result_df = pd.concat([previous_result, result_df], axis=0)\n    result_df.to_csv(os.path.join(node_line_dir, 'summary.csv'), index=False)\n    return result_df\n\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    validate_qa_dataset(previous_result)\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n    if not os.path.exists(os.path.join(node_line_dir, 'results')):\n        os.makedirs(os.path.join(node_line_dir, 'results'))\n    if not os.path.exists(os.path.join(node_line_dir, 'summary')):\n        os.makedirs(os.path.join(node_line_dir, 'summary'))\n\n    generator_callable_params = make_generator_callable_params(strategies)\n    generator_funcs, generator_params = list(zip(*generator_callable_params))\n    prompts = previous_result['prompts'].tolist()\n    generation_gt = previous_result['generation_gt'].tolist()\n    metrics = cast_metrics(strategies['metrics'])\n    result_dfs = []\n    for module, module_param in zip(modules, module_params):\n        result_df = evaluate_one_prompt_maker_node(generator_funcs, generator_params, prompts, generation_gt, metrics,\n                                                   node_line_dir)\n        result_df['module_type'] = module.__name__\n        result_df['module_params'] = str(module_param)\n        result_dfs.append(result_df)\n    result_df = pd.concat(result_dfs)\n    result_df['node_line_dir'] = node_line_dir\n    result_df['node_line_name'] = pathlib.Path(node_line_dir).name\n    result_df['node_line_type'] = 'prompt_maker'\n    result_df['node_line_id'] = len(os.listdir(node_line_dir))\n    result_df.to_csv(os.path.join(node_line_dir, 'results', f'{len(os.listdir(node_line_dir))}.csv'),"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    validate_qa_dataset(previous_result, 'prompts')\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n    if not os.path.exists(os.path.join(node_line_dir, 'result')):\n        os.makedirs(os.path.join(node_line_dir, 'result'))\n    if not os.path.exists(os.path.join(node_line_dir, 'summary')):\n        os.makedirs(os.path.join(node_line_dir, 'summary'))\n\n    generator_module_list: Optional[List[Dict]] = strategies.pop('generator_modules', None)\n    if generator_module_list is None:\n        generator_module_list = [{\n            'module_type': 'llama_index_llm',\n            'llm': 'openai',\n            'model': 'gpt-3.5-turbo',\n        }]\n    node_params = strategies\n    generator_funcs = list(map(lambda module_dict: get_support_modules(module_dict.pop('module_type')),\n                               generator_module_list))\n    generator_params = make_generator_callable_params(node_params)\n    prompts = previous_result['prompts'].tolist()\n    generation_gt = previous_result['generation_gt'].tolist()\n    metrics = cast_metrics(node_params.pop('metrics', None))\n\n    result_df = pd.DataFrame()\n    for module_func, module_params in zip(modules, module_params):\n        result_df = pd.concat([result_df,\n                               module_func(project_dir=node_line_dir, previous_result=previous_result,\n                                           **module_params)])\n    result_df.reset_index(drop=True, inplace=True)\n\n    evaluation_results = list(map(lambda x: evaluate_one_prompt_maker_node(generator_funcs,\n                                                                           generator_params"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # validate input\n    validate_qa_dataset(previous_result, 'prompts', 'generated_texts')\n\n    # create output directory\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # make generator callable params\n    generator_params = make_generator_callable_params(strategies)\n\n    # run prompt maker modules\n    prompt_maker_results = list(map(lambda x: x[0](project_dir=node_line_dir, previous_result=previous_result, **x[1]),\n                                    zip(modules, module_params)))\n\n    # evaluate prompt maker modules\n    evaluation_results = list(map(lambda x: evaluate_one_prompt_maker_node(x[0], x[1], x[2], x[3], x[4], node_line_dir),\n                                  zip(generator_params, generator_params, prompt_maker_results,\n                                      strategies['generation_gt'], strategies['metrics'])))\n\n    # select best prompt maker module\n    best_result, best_result_index = select_best_average(evaluation_results, strategies['metrics'])\n\n    # save best prompt maker module result\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'), index=False)\n\n    # save summary\n    summary = pd.DataFrame({'prompt_maker_module': list(map(lambda x: x['module_type'], module_params)),\n                            'prompt_maker_module_params': list(map(lambda x: x, module_params)),\n                            'evaluation_results': evaluation_results,\n                            'best_result_index': best_result_index})\n    summary.to_csv(os.path.join(node_line_dir, 'summary.csv'), index=False)\n\n    # combine results\n    combined_result = pd.concat([previous_result, best_result], axis=0)\n    return combined_result\n\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # validate input\n    validate_qa_dataset(previous_result, 'prompts', 'generated_texts')\n\n    # create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'prompts'), exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'results'), exist_ok=True)\n\n    # make generator callable params\n    generator_callable_params = make_generator_callable_params(strategies)\n\n    # run prompt maker modules\n    prompt_maker_results = list(map(lambda x: x[0](project_dir=node_line_dir, previous_result=previous_result,\n                                                   **x[1]), zip(modules, module_params)))\n\n    # evaluate prompt maker modules\n    evaluation_results = list(map(lambda x: evaluate_one_prompt_maker_node(x[0], x[1], x[2], x[3], x[4], node_line_dir),\n                                  zip(generator_callable_params, prompt_maker_results,\n                                      strategies['prompts'], strategies['generation_gt'], strategies['metrics'])))\n\n    # select the best prompt maker module\n    best_result, best_index = select_best_average(evaluation_results, strategies['metrics'])\n\n    # save the best result\n    best_result.to_csv(os.path.join(node_line_dir, 'results', 'best_result.csv'), index=False)\n\n    # save the summary\n    summary = pd.DataFrame({\n        'prompt_maker_module': [str(modules[best_index])],\n        'prompt_maker_params': [str(module_params[best_index])],\n        'execution_time': [str(best_result['execution_time'].values[0])],\n        'evaluation_metrics': [str(cast_metrics(evaluation_results[best_index]))]\n    })"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    node_line_dir = os.path.join(node_line_dir, 'prompt_maker')\n    os.makedirs(node_line_dir, exist_ok=True)\n    generator_modules = strategies.get('generator_modules', [{\n        'module_type': 'llama_index_llm',\n        'llm': 'openai',\n        'model': 'gpt-3.5-turbo',\n    }])\n    generator_funcs, generator_params = make_generator_callable_params(generator_modules)\n    generator_params = list(map(lambda x: x['params'], generator_params))\n    prompts = previous_result['prompts'].tolist()\n    generation_gt = previous_result['generation_gt'].tolist()\n    metrics = strategies.get('metrics', ['bleu', 'rouge', 'meteor', 'bert_score', 'bleurt'])\n    metrics = cast_metrics(metrics)\n    result_dfs = list(map(lambda x: evaluate_one_prompt_maker_node(generator_funcs, generator_params, prompts,\n                                                                   generation_gt, metrics, node_line_dir),\n                          zip(modules, module_params)))\n    best_result, best_result_index = select_best_average(result_dfs, ['bleu', 'rouge', 'meteor', 'bert_score', 'bleurt'])\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'), index=False)\n    pd.concat([previous_result, best_result], axis=0).to_csv(os.path.join(node_line_dir, 'result.csv'), index=False)\n    pd.concat(result_dfs, axis=0).to_csv(os.path.join(node_line_dir, 'result_all.csv'), index=False)\n    strategies['speed_threshold'] = strategies.get('speed_threshold', 100)\n    strategies['speed_threshold'] = strategies['speed_th"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    validate_qa_dataset(previous_result)\n    os.makedirs(node_line_dir, exist_ok=True)\n    results = []\n    for module, module_param in zip(modules, module_params):\n        module_dir = os.path.join(node_line_dir, module.__name__)\n        os.makedirs(module_dir, exist_ok=True)\n        module_result = module(project_dir=node_line_dir, previous_result=previous_result, **module_param)\n        module_result = cast_metrics(module_result)\n        results.append(module_result)\n    results = pd.concat(results, axis=0)\n    results = select_best_prompt_maker_module(results, strategies)\n    results = pd.concat([previous_result, results], axis=0)\n    results.to_csv(os.path.join(node_line_dir, 'summary.csv'), index=False)\n    return results\n\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    validate_qa_dataset(previous_result, ['prompts'])\n\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    if 'generator_modules' not in strategies:\n        strategies['generator_modules'] = [{\n            'module_type': 'llama_index_llm',\n            'llm': 'openai',\n            'model': 'gpt-3.5-turbo',\n        }]\n    generator_module_list = strategies.pop('generator_modules')\n    generator_module_params = make_generator_callable_params(strategies)\n    generator_funcs = list(map(lambda module_dict: get_support_modules(module_dict.pop('module_type')),\n                               generator_module_list))\n    param_combinations = list(map(lambda module_dict: make_combinations({**module_dict, **strategies}),\n                                  generator_module_list))\n    generator_funcs_params = list(map(lambda x: (x[0], x[1]), zip(generator_funcs, param_combinations)))\n\n    prompts = previous_result['prompts'].tolist()\n    generation_gt = previous_result['generation_gt'].tolist()\n    metrics = strategies.pop('metrics')\n    metrics = cast_metrics(metrics)\n    if 'speed_thresholds' in strategies:\n        speed_thresholds = strategies.pop('speed_thresholds')\n        speed_thresholds = cast_speed_thresholds(speed_thresholds)\n        speed_thresholds = list(map(lambda x: (x[0], x[1]), zip(generator_funcs, speed_thresholds)))\n    else:\n        speed_thresholds = None\n\n    # evaluate_one_prompt_maker_node(generator_funcs, generator_params, prompts, generation_gt, metrics, project_dir)\n    results = list(map(lambda x: evaluate_one_prompt_maker_"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # make sure the node line directory exists\n    os.makedirs(node_line_dir, exist_ok=True)\n    # get generator module and its parameters\n    generator_module_list: Optional[List[Dict]] = strategies.pop('generator_modules', None)\n    generator_module_params = make_generator_callable_params(strategies)\n    # get the metrics for evaluation\n    metrics = strategies.pop('metrics', None)\n    if metrics is None:\n        metrics = ['bleu', 'rouge', 'meteor']\n    metrics = cast_metrics(metrics)\n    # get the speed thresholds\n    speed_thresholds = strategies.pop('speed_thresholds', None)\n    if speed_thresholds is None:\n        speed_thresholds = [0.05, 0.1, 0.2, 0.5, 1.0]\n    # get the evaluation parameters\n    evaluation_params = strategies.pop('evaluation_params', None)\n    if evaluation_params is None:\n        evaluation_params = {}\n    # get the project directory\n    project_dir = strategies.pop('project_dir', None)\n    if project_dir is None:\n        project_dir = os.getcwd()\n    # get the prompts\n    prompts = strategies.pop('prompts', None)\n    if prompts is None:\n        raise ValueError('prompts must be provided')\n    # get the generation ground truth\n    generation_gt = strategies.pop('generation_gt', None)\n    if generation_gt is None:\n        raise ValueError('generation_gt must be provided')\n    # get the number of generations\n    num_generations = strategies.pop('num_generations', None)\n    if num_generations is None:\n        num_generations = 1\n    # get the number of workers\n    num_workers = strategies.pop('num_workers', None)\n    if num_workers is None:\n        num_workers = 1\n    # get the node name\n    node_name = strategies.pop('node_name"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Validate the input parameters\n    validate_qa_dataset(previous_result)\n    # Get the generator module callable and its parameters\n    generator_funcs, generator_params = make_generator_callable_params(strategies)\n    # Get the prompts and ground truth from the previous result\n    prompts = previous_result['prompts'].tolist()\n    generation_gt = previous_result['generation_gt'].tolist()\n    # Get the metrics and cast them to a list of dictionaries\n    metrics = cast_metrics(strategies.get('metrics', []))\n    # Get the speed thresholds\n    speed_thresholds = strategies.get('speed_thresholds', [])\n    # Create the directory for the node's output\n    os.makedirs(node_line_dir, exist_ok=True)\n    # Create a list to store the results of each prompt maker module\n    results = []\n    # Iterate over the prompt maker modules and their parameters\n    for module, module_param in zip(modules, module_params):\n        # Get the module's name\n        module_name = module.__name__\n        # Get the module's output directory\n        module_line_dir = os.path.join(node_line_dir, module_name)\n        # Create the directory for the module's output\n        os.makedirs(module_line_dir, exist_ok=True)\n        # Create a dataframe to store the module's output\n        module_output = pd.DataFrame()\n        # Run the prompt maker module with the given parameters\n        module_output = module(project_dir=node_line_dir, previous_result=previous_result, **module_param)\n        # Evaluate the module's output\n        evaluation_results = evaluate_one_prompt_maker_node(generator_funcs, generator_params, prompts, generation_gt,\n                                                            metrics, module_line_dir)\n        # Add the module's output to the list of results\n        results.append(evaluation_results)\n        # Save the module's output to a CSV"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    def extract_module_values(module: Module):\n        if key not in module.module_param:\n            return []\n        value = module.module_param[key]\n        if isinstance(value, str):\n            return [value]\n        elif isinstance(value, list):\n            return value\n        else:\n            raise ValueError(f\"{key} must be str or list, but got {type(value)}\")\n\n    values = list(map(extract_module_values, nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    def extract_module_values(module: Module):\n        if key not in module.module_param:\n            return []\n        value = module.module_param[key]\n        if isinstance(value, str):\n            return [value]\n        elif isinstance(value, list):\n            return value\n        else:\n            raise ValueError(f\"{key} must be str or list, but got {type(value)}\")\n\n    values = list(map(extract_module_values, nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    def extract_module_values(module: Module):\n        if key not in module.module_param:\n            return []\n        value = module.module_param[key]\n        if isinstance(value, str):\n            return [value]\n        elif isinstance(value, list):\n            return value\n        else:\n            raise ValueError(f\"{key} must be str or list, but got {type(value)}\")\n\n    values = list(map(extract_module_values, nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    def extract_module_values(module: Module):\n        if key not in module.module_param:\n            return []\n        value = module.module_param[key]\n        if isinstance(value, str):\n            return [value]\n        elif isinstance(value, list):\n            return value\n        else:\n            raise ValueError(f\"{key} must be str or list, but got {type(value)}\")\n\n    values = list(map(extract_module_values, nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    def extract_module_values(module: Module):\n        if key not in module.module_param:\n            return []\n        value = module.module_param[key]\n        if isinstance(value, str):\n            return [value]\n        elif isinstance(value, list):\n            return value\n        else:\n            raise ValueError(f\"{key} must be str or list, but got {type(value)}\")\n\n    values = list(map(extract_module_values, nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    def extract_values_from_node(node: Node) -> List[str]:\n        return extract_values(node, key)\n\n    values = list(map(extract_values_from_node, nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model()\n    pred_embedding = embedding_model.get_embedding(pred)\n    gt_embeddings = [embedding_model.get_embedding(gt) for gt in generation_gt]\n    return max(list(map(lambda x: calculate_cosine_similarity(pred_embedding, x), gt_embeddings)))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model()\n    pred_embedding = embedding_model.get_embedding(pred)\n    gt_embeddings = [embedding_model.get_embedding(gt) for gt in generation_gt]\n    return max(list(map(lambda x: calculate_cosine_similarity(pred_embedding, x), gt_embeddings)))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model('all-mpnet-base-v2')\n\n    pred_embedding = embedding_model.get_embedding(pred)\n    gt_embeddings = [embedding_model.get_embedding(gt) for gt in generation_gt]\n    return max(list(map(lambda x: calculate_cosine_similarity(pred_embedding, x), gt_embeddings)))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model('all-mpnet-base-v2')\n\n    pred_embedding = embedding_model.get_embedding(pred)\n    gt_embeddings = [embedding_model.get_embedding(gt) for gt in generation_gt]\n    scores = [calculate_cosine_similarity(pred_embedding, gt_embedding) for gt_embedding in gt_embeddings]\n    return max(scores)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model()\n\n    pred_embedding = embedding_model.get_embedding(pred)\n    gt_embeddings = list(map(lambda x: embedding_model.get_embedding(x), generation_gt))\n    return max(list(map(lambda x: calculate_cosine_similarity(pred_embedding, x), gt_embeddings)))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_default_embedding_model()\n\n    pred_embedding = embedding_model.get_embedding(pred)\n    gt_embeddings = [embedding_model.get_embedding(gt) for gt in generation_gt]\n    return max(list(map(lambda x: calculate_cosine_similarity(pred_embedding, x), gt_embeddings)))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model('all-mpnet-base-v2')\n\n    # Convert ground truth and prediction to embeddings\n    gt_embeddings = embedding_model.embed_documents(generation_gt)\n    pred_embedding = embedding_model.embed_query(pred)\n\n    # Calculate cosine similarity between prediction and each ground truth\n    cosine_similarities = calculate_cosine_similarity(pred_embedding, gt_embeddings)\n\n    # Return the maximum cosine similarity as the semantic similarity score\n    return max(cosine_similarities)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model()\n    embedding_model.set_embedding_model_name('all-mpnet-base-v2')\n\n    pred_embedding = embedding_model.get_embedding(pred)\n    gt_embeddings = list(map(lambda x: embedding_model.get_embedding(x), generation_gt))\n    cosine_similarities = list(map(lambda x: calculate_cosine_similarity(pred_embedding, x), gt_embeddings))\n    return max(cosine_similarities)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model()\n\n    def compute_score(gt: str, pred: str) -> float:\n        gt_embedding = embedding_model.get_embedding(gt)\n        pred_embedding = embedding_model.get_embedding(pred)\n        return calculate_cosine_similarity(gt_embedding, pred_embedding)\n\n    result = list(map(lambda x: compute_score(x[0], x[1]), zip(generation_gt, generations)))\n    return max(result)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_default_embedding_model()\n\n    cosine_similarities = calculate_cosine_similarity(embedding_model, pred, generation_gt)\n    return max(cosine_similarities)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model()\n\n    cos_sim_scores = calculate_cosine_similarity(embedding_model, pred, generation_gt)\n    return max(cos_sim_scores)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model()\n\n    # Convert ground truth strings to embeddings\n    gt_embeddings = embedding_model.embed_documents(generation_gt)\n\n    # Convert prediction to embedding\n    pred_embedding = embedding_model.embed_query(pred)\n\n    # Calculate cosine similarity between prediction and ground truth embeddings\n    cosine_similarities = calculate_cosine_similarity(pred_embedding, gt_embeddings)\n\n    # Return the maximum cosine similarity\n    return max(cosine_similarities)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.load_embedding_model('all-mpnet-base-v2')\n\n    # Convert ground truth strings into embeddings\n    gt_embeddings = embedding_model.embed_documents(generation_gt)\n\n    # Convert predicted string into an embedding\n    pred_embedding = embedding_model.embed_query(pred)\n\n    # Calculate cosine similarity between predicted embedding and ground truth embeddings\n    cosine_similarities = calculate_cosine_similarity(pred_embedding, gt_embeddings)\n\n    # Return the maximum cosine similarity score\n    return max(cosine_similarities)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model('all-mpnet-base-v2')\n\n    # Convert ground truth strings to embeddings\n    gt_embeddings = embedding_model.get_embedding(generation_gt)\n\n    # Convert predicted string to embedding\n    pred_embedding = embedding_model.get_embedding([pred])[0]\n\n    # Calculate cosine similarity between predicted embedding and ground truth embeddings\n    cosine_similarities = calculate_cosine_similarity(pred_embedding, gt_embeddings)\n\n    # Return the maximum cosine similarity score\n    return max(cosine_similarities)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model()\n\n    # convert gt to embedding\n    gt_embedding = embedding_model.embed_documents(generation_gt)\n    # convert pred to embedding\n    pred_embedding = embedding_model.embed_query(pred)\n\n    # calculate cosine similarity\n    return calculate_cosine_similarity(pred_embedding, gt_embedding)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.load_embedding_model()\n    embedding_model.set_embedding_model(\"all-mpnet-base-v2\")\n\n    pred_embedding = embedding_model.get_embedding(pred)\n    gt_embeddings = [embedding_model.get_embedding(gt) for gt in generation_gt]\n    cosine_similarities = [calculate_cosine_similarity(pred_embedding, gt_embedding) for gt_embedding in gt_embeddings]\n    return max(cosine_similarities)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_default_embedding_model()\n    embedding_model.set_max_length(128)\n\n    # Convert ground truth strings to embeddings\n    gt_embeddings = embedding_model.get_embeddings(generation_gt)\n\n    # Convert predicted string to embedding\n    pred_embedding = embedding_model.get_embeddings([pred])[0]\n\n    # Compute cosine similarity between predicted embedding and ground truth embeddings\n    cosine_similarities = calculate_cosine_similarity(pred_embedding, gt_embeddings)\n\n    # Return the maximum cosine similarity score\n    return max(cosine_similarities)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model()\n    embedding_model.set_embedding_model_name('all-mpnet-base-v2')\n    embedding_model.set_embedding_model_type('sentence')\n\n    pred_embedding = embedding_model.embedding_model.encode(pred)\n    gt_embeddings = list(map(lambda x: embedding_model.embedding_model.encode(x), generation_gt))\n    return max(list(map(lambda x: calculate_cosine_similarity(pred_embedding, x), gt_embeddings)))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model('all-mpnet-base-v2')\n\n    embedding_gt = embedding_model.get_embedding(generation_gt)\n    embedding_pred = embedding_model.get_embedding([pred])\n\n    return calculate_cosine_similarity(embedding_pred, embedding_gt)\n\n"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_default_embedding_model()\n    # convert ground truth to embeddings\n    gt_embeddings = embedding_model.get_embeddings(generation_gt)\n    # convert prediction to embedding\n    pred_embedding = embedding_model.get_embeddings([pred])[0]\n    # calculate cosine similarity\n    cosine_similarities = calculate_cosine_similarity(pred_embedding, gt_embeddings)\n    # return max cosine similarity\n    return max(cosine_similarities)\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer not set up\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer not set up\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer not set up\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer not set up\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer not set up\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer not set up\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer not set up\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer not set up\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer is not set up\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer is not set up\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer is not set up\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer not set up\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer not set up\")\n        return np_image\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer is not set up. Skipping face restoration.\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN not set up, skipping face restoration\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer is not set up, skipping face restoration\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer is not set up, skipping face restoration\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer is not set up, skipping face restoration\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer not set up\")\n        return np_image\n    else:\n        return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer is not set up, not restoring faces\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except Exception as e:\n        logger.warning(f\"Error setting up CodeFormer: {e}\")"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except Exception as e:\n        logger.error(f\"Error setting up CodeFormer: {e}\")"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except Exception as e:\n        logger.error(f\"Error setting up CodeFormer: {e}\")"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except Exception as e:\n        logger.error(f\"Error setting up CodeFormer face restoration: {e}\")"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except errors.ModelNotFoundError as e:\n        logger.error(e)"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except Exception as e:\n        logger.error(f\"Error setting up CodeFormer face restoration: {e}\")"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except errors.ModelNotFoundError as e:\n        logger.warning(f\"Error setting up CodeFormer: {e}\")"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restoration.face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except Exception as e:\n        logger.error(f\"Error setting up CodeFormer: {e}\")"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restoration.face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except errors.ModelNotFoundError as e:\n        logger.error(f\"Error setting up CodeFormer: {e}\")"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except Exception as e:\n        logger.error(f\"Error setting up codeformer model: {e}\")"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restoration.face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except errors.Error as e:\n        logger.warning(f\"Error setting up CodeFormer: {e}\")"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restoration.face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except errors.ModelNotFoundError as e:\n        logger.error(e)\n        return"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restoration.face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except errors.ModelError as e:\n        logger.error(f\"Error loading CodeFormer: {e}\")"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except Exception as e:\n        logger.error(f\"Error setting up CodeFormer face restoration model: {e}\")"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except Exception as e:\n        logger.warning(f\"Failed to setup face restorer: {e}\")"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restorer = FaceRestorerCodeFormer(dirname)\n        face_restoration.face_restorers.append(face_restorer)\n    except errors.Error as e:\n        logger.error(f\"Error setting up CodeFormer: {e}\")"}
{"namespace": "codeformer_model.setup_model", "completion": "    global codeformer\n    codeformer = FaceRestorerCodeFormer(dirname)\n    face_restoration.face_restorers.append(codeformer)"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restorer = FaceRestorerCodeFormer(dirname)\n        face_restorers.append(face_restorer)\n    except errors.ModelNotFoundError as e:\n        logger.warning(f\"Face restorer setup failed: {e}\")"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restorer = FaceRestorerCodeFormer(dirname)\n        face_restorers.append(face_restorer)\n        logger.info(f\"{face_restorer.name()} face restorer initialized\")\n    except errors.ModelNotFoundError as e:\n        logger.warning(f\"{e}\")\n    except Exception as e:\n        logger.warning(f\"{e}\")"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restoration.add_face_restorer(FaceRestorerCodeFormer(dirname))\n    except errors.ModelNotFoundError as e:\n        logger.warning(f\"{e}\")\n        logger.warning(f\"{e.model_name} model not found, skipping setup\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        global gfpgan_face_restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except Exception as e:\n        logger.warning(f\"GFPGAN face restorer setup failed: {e}\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        global gfpgan_face_restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except Exception as e:\n        logger.warning(f\"GFPGAN face restorer setup failed: {e}\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        face_restoration.patch_facexlib(dirname)\n        global gfpgan_face_restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except Exception as e:\n        logger.warning(f\"GFPGAN face restorer setup failed: {e}\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        face_restoration.patch_facexlib(dirname)\n        global gfpgan_face_restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except Exception as e:\n        logger.warning(f\"GFPGAN face restorer setup failed: {e}\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        face_restoration_utils.patch_facexlib(dirname)\n        global gfpgan_face_restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except Exception as e:\n        logger.warning(f\"GFPGAN face restorer setup failed: {e}\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        face_restoration_utils.patch_facexlib(dirname)\n        global gfpgan_face_restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except Exception as e:\n        logger.warning(f\"GFPGAN face restorer setup failed: {e}\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    try:\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except errors.ModelError as e:\n        logger.warning(f\"GFPGAN face restorer setup failed: {e}\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        global gfpgan_face_restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except errors.ModelNotFoundError:\n        logger.warning(\"GFPGAN model not found\")\n    "}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    try:\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except errors.ModelNotFoundError as e:\n        logger.error(e)\n        gfpgan_face_restorer = None"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    try:\n        face_restoration.patch_facexlib(dirname)\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except errors.ModelNotFoundError as e:\n        logger.error(f\"GFPGAN model not found: {e}\")\n    except Exception as e:\n        logger.error(f\"Error setting up GFPGAN model: {e}\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        facexlib_path = os.path.join(dirname, \"facexlib\")\n        facexlib_path = os.path.normpath(facexlib_path)\n        shared.patch_facexlib(facexlib_path)\n        global gfpgan_face_restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except Exception as e:\n        logger.warning(f\"GFPGAN face restorer setup failed: {e}\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    # Patch facexlib\n    if not shared.args.no_half:\n        facexlib_path = os.path.join(dirname, \"facexlib\")\n        facexlib_path = os.path.abspath(facexlib_path)\n        facexlib_path = os.path.normpath(facexlib_path)\n        facexlib_path = os.path.realpath(facexlib_path)\n        facexlib_path = os.path.expanduser(facexlib_path)\n        facexlib_path = os.path.expandvars(facexlib_path)\n        facexlib_path = os.path.normcase(facexlib_path)\n        facexlib_path = os.path.join(facexlib_path, \"facexlib\")\n        facexlib_path = os.path.abspath(facexlib_path)\n        facexlib_path = os.path.normpath(facexlib_path)\n        facexlib_path = os.path.realpath(facexlib_path)\n        facexlib_path = os.path.expanduser(facexlib_path)\n        facexlib_path = os.path.expandvars(facexlib_path)\n        facexlib_path = os.path.normcase(facexlib_path)\n        facexlib_path = os.path.join(facexlib_path, \"facexlib\")\n        facexlib_path = os.path.abspath(facexlib_path)\n        facexlib_path = os.path.normpath(facexlib_path)\n        facexlib_path = os.path.realpath(facexlib_path)\n        facexlib_path = os.path.expanduser(facexlib_path)\n        facexlib_path = os.path.expandvars(facexlib_path)\n        facexlib_path = os.path.normcase(facexlib_path)\n        facexlib_path = os.path.join(facexlib_path, \"facexlib\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        facexlib_dir = os.path.join(dirname, \"facexlib\")\n        facexlib_path = os.path.join(facexlib_dir, \"facexlib\")\n        facexlib_path_backup = os.path.join(facexlib_dir, \"facexlib_backup\")\n        facexlib_path_backup_bak = os.path.join(facexlib_dir, \"facexlib_backup_bak\")\n        facexlib_path_backup_bak_bak = os.path.join(facexlib_dir, \"facexlib_backup_bak_bak\")\n        facexlib_path_backup_bak_bak_bak = os.path.join(facexlib_dir, \"facexlib_backup_bak_bak_bak\")\n        facexlib_path_backup_bak_bak_bak_bak = os.path.join(facexlib_dir, \"facexlib_backup_bak_bak_bak_bak\")\n        facexlib_path_backup_bak_bak_bak_bak_bak = os.path.join(facexlib_dir, \"facexlib_backup_bak_bak_bak_bak_bak\")\n        facexlib_path_backup_bak_bak_bak_bak_bak_bak = os.path.join(facexlib_dir, \"facexlib_backup_bak_bak_bak_bak_bak_bak\")\n        facexlib_path_backup_bak_bak_bak_bak_bak_bak_bak = os.path.join(facexlib_dir, \"facexlib_backup_bak_bak_bak_bak_bak_bak_bak\")\n        facexlib_path_backup_bak_bak_bak_bak_bak_bak_bak_bak = os.path.join(facexlib_dir, \"face"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        face_restoration.patch_facexlib(dirname)\n        global gfpgan_face_restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(model_path=dirname)\n        logger.info(\"GFPGAN face restorer setup complete\")\n    except errors.FaceRestorationError as e:\n        logger.warning(\"GFPGAN face restorer setup failed: %s\", e)\n    except Exception as e:\n        logger.warning(\"GFPGAN face restorer setup failed: %s\", e)"}
{"namespace": "gfpgan_model.setup_model", "completion": "    # Patch facexlib to use the correct model path\n    facexlib_path = os.path.join(dirname, \"facexlib\")\n    facexlib_path = os.path.abspath(facexlib_path)\n    if not os.path.exists(facexlib_path):\n        os.makedirs(facexlib_path)\n    facexlib_path = os.path.join(facexlib_path, \"models\")\n    if not os.path.exists(facexlib_path):\n        os.makedirs(facexlib_path)\n    facexlib_path = os.path.join(facexlib_path, \"gfpgan\")\n    if not os.path.exists(facexlib_path):\n        os.makedirs(facexlib_path)\n    facexlib_path = os.path.join(facexlib_path, \"GFPGANv1.4.pth\")\n    facexlib_path = os.path.abspath(facexlib_path)\n    facexlib_path = os.path.dirname(facexlib_path)\n    facexlib_path = os.path.dirname(facexlib_path)\n    facexlib_path = os.path.dirname(facexlib_path)\n    facexlib_path = os.path.dirname(facexlib_path)\n    facexlib_path = os.path.dirname(facexlib_path)\n    facexlib_path = os.path.dirname(facexlib_path)\n    facexlib_path = os.path.dirname(facexlib_path)\n    facexlib_path = os.path.dirname(facexlib_path)\n    facexlib_path = os.path.dirname(facexlib_path)\n    facexlib_path = os.path.dirname(facexlib_path)\n    facexlib_path = os.path.dirname(facexlib_path)\n    facexlib_path = os.path.dirname(facexlib"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        facexlib_dir = os.path.join(dirname, \"facexlib\")\n        facexlib_path = os.path.join(facexlib_dir, \"facexlib\")\n        facexlib_init_path = os.path.join(facexlib_dir, \"__init__.py\")\n        facexlib_init_content = f\"from .facexlib import Facexlib\\n\"\n\n        if os.path.exists(facexlib_path):\n            os.remove(facexlib_path)\n\n        if not os.path.exists(facexlib_dir):\n            os.makedirs(facexlib_dir)\n\n        with open(facexlib_init_path, \"w\") as f:\n            f.write(facexlib_init_content)\n\n        global gfpgan_face_restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n\n    except Exception as e:\n        errors.report(e)"}
{"namespace": "gfpgan_model.setup_model", "completion": "    # Patch facexlib\n    facexlib_dir = os.path.join(dirname, \"facexlib\")\n    facexlib_patch_path = os.path.join(facexlib_dir, \"facexlib_patch.py\")\n    facexlib_patch_path_bak = os.path.join(facexlib_dir, \"facexlib_patch.py.bak\")\n    if not os.path.exists(facexlib_patch_path_bak):\n        os.rename(facexlib_patch_path, facexlib_patch_path_bak)\n\n    # Initialize GFPGAN face restorer\n    try:\n        global gfpgan_face_restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n        gfpgan_face_restorer.load_model()\n        logger.info(\"GFPGAN face restorer initialized\")\n    except Exception as e:\n        logger.warning(f\"Error initializing GFPGAN face restorer: {e}\")\n        gfpgan_face_restorer = None\n\n    # Restore facexlib patch\n    if os.path.exists(facexlib_patch_path_bak):\n        os.rename(facexlib_patch_path_bak, facexlib_patch_path)"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    try:\n        facexlib_path = os.path.join(dirname, \"facexlib\")\n        if not os.path.exists(facexlib_path):\n            os.makedirs(facexlib_path)\n        facexlib_path = os.path.join(facexlib_path, \"models\")\n        if not os.path.exists(facexlib_path):\n            os.makedirs(facexlib_path)\n        facexlib_path = os.path.join(facexlib_path, \"face_parsing_with_mobilenet.pth\")\n        if not os.path.exists(facexlib_path):\n            raise errors.MissingModelError(\n                \"GFPGAN face restoration requires facexlib. Please install it with `pip install facexlib`.\"\n            )\n        facexlib_path = os.path.join(dirname, \"facexlib\")\n        face_restoration.patch_facexlib(facexlib_path)\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n        logger.info(\"GFPGAN face restoration setup complete\")\n    except Exception as e:\n        logger.error(f\"Failed to setup GFPGAN face restoration: {e}\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        facexlib_dir = os.path.join(dirname, 'facexlib')\n        if not os.path.exists(facexlib_dir):\n            os.makedirs(facexlib_dir)\n        facexlib_dir = os.path.abspath(facexlib_dir)\n\n        facexlib_path = os.path.join(facexlib_dir, 'facexlib')\n        if not os.path.exists(facexlib_path):\n            os.symlink(os.path.abspath(os.path.join(dirname, 'facexlib')), facexlib_path)\n\n        face_restorer = FaceRestorerGFPGAN(model_path=dirname)\n        face_restorer.load_model()\n        global gfpgan_face_restorer\n        gfpgan_face_restorer = face_restorer\n\n    except Exception as e:\n        logger.warning(f\"GFPGAN setup failed: {e}\")\n        errors.report(e)\n        gfpgan_face_restorer = None\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    try:\n        facexlib_dir = os.path.join(dirname, \"facexlib\")\n        facexlib_patch_dir = os.path.join(facexlib_dir, \"patches\")\n        facexlib_patch_file = os.path.join(facexlib_patch_dir, \"facexlib_gfpgan.patch\")\n        facexlib_utils_file = os.path.join(facexlib_dir, \"utils.py\")\n        facexlib_utils_patch_file = os.path.join(facexlib_patch_dir, \"facexlib_utils.patch\")\n\n        facexlib_patch_file_exists = os.path.isfile(facexlib_patch_file)\n        facexlib_utils_patch_file_exists = os.path.isfile(facexlib_utils_patch_file)\n        facexlib_utils_file_exists = os.path.isfile(facexlib_utils_file)\n\n        if facexlib_patch_file_exists and facexlib_utils_patch_file_exists and facexlib_utils_file_exists:\n            logger.info(f\"Patching facexlib with {facexlib_patch_file}\")\n            patch_file_cmd = f\"patch -p0 < {facexlib_patch_file}\"\n            os.system(patch_file_cmd)\n            logger.info(f\"Patching facexlib utils with {facexlib_utils_patch_file}\")\n            patch_utils_cmd = f\"patch -p0 < {facexlib_utils_patch_file}\"\n            os.system(patch_utils_cmd)\n        else:\n            logger.warning(f\"Could not find patch files in {facexlib_patch_dir}\")\n\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n        gfpgan_face_restorer.setup()\n    except errors.FaceRestorationError as e:\n        logger.error(f\"Error setting up GFPG"}
{"namespace": "quaternion.rotate", "completion": "  q_v = jnp.concatenate([jnp.zeros_like(v[Ellipsis, :1]), v], axis=-1)\n  return multiply(multiply(q, q_v), inverse(q))[Ellipsis, 1:]"}
{"namespace": "quaternion.rotate", "completion": "  q_v = jnp.concatenate([jnp.zeros_like(v[Ellipsis, :1]), v], axis=-1)\n  return multiply(multiply(q, q_v), conjugate(q))[Ellipsis, 1:]"}
{"namespace": "quaternion.rotate", "completion": "  q_v = jnp.concatenate([jnp.zeros_like(v[Ellipsis, :1]), v], axis=-1)\n  return multiply(multiply(q, q_v), conjugate(q))[Ellipsis, 1:]"}
{"namespace": "quaternion.rotate", "completion": "  q_v = jnp.concatenate([jnp.zeros_like(v[Ellipsis, :1]), v], axis=-1)\n  return multiply(multiply(q, q_v), conjugate(q))[Ellipsis, 1:]"}
{"namespace": "quaternion.rotate", "completion": "  q_v = jnp.concatenate([jnp.zeros_like(v[Ellipsis, :1]), v], axis=-1)\n  return multiply(multiply(q, q_v), conjugate(q))[Ellipsis, 1:]"}
{"namespace": "quaternion.rotate", "completion": "  qv = jnp.concatenate([jnp.zeros_like(v[Ellipsis, :1]), v], axis=-1)\n  return multiply(multiply(q, qv), inverse(q))[Ellipsis, 1:]"}
{"namespace": "quaternion.rotate", "completion": "  q_v = jnp.concatenate([jnp.zeros_like(v[Ellipsis, :1]), v], axis=-1)\n  return multiply(multiply(q, q_v), inverse(q))[Ellipsis, 1:]"}
{"namespace": "quaternion.rotate", "completion": "  q = jnp.concatenate([jnp.zeros_like(v[Ellipsis, :1]), v], axis=-1)\n  return multiply(multiply(q, q), q)"}
{"namespace": "quaternion.rotate", "completion": "  q_v = jnp.concatenate([im(q), jnp.zeros_like(re(q))], axis=-1)\n  return multiply(multiply(q, q_v), conjugate(q))[Ellipsis, :3]"}
{"namespace": "quaternion.rotate", "completion": "  q_v = jnp.concatenate([jnp.zeros_like(v), v], axis=-1)\n  return multiply(multiply(q, q_v), inverse(q))[Ellipsis, 3:]"}
{"namespace": "quaternion.rotate", "completion": "  q_v = jnp.concatenate([jnp.zeros_like(v), v], axis=-1)\n  return multiply(multiply(q, q_v), conjugate(q))[Ellipsis, 3:]"}
{"namespace": "quaternion.rotate", "completion": "  qv = jnp.concatenate([jnp.zeros_like(v[Ellipsis, :1]), v], axis=-1)\n  return multiply(multiply(q, qv), conjugate(q))[Ellipsis, 1:]"}
{"namespace": "quaternion.rotate", "completion": "  q_inv = inverse(q)\n  return multiply(multiply(q, v), q_inv)"}
{"namespace": "quaternion.rotate", "completion": "  q_vec = jnp.concatenate([jnp.zeros_like(v[..., :1]), v], axis=-1)\n  return multiply(multiply(q, q_vec), inverse(q))[..., 1:]"}
{"namespace": "quaternion.rotate", "completion": "  q_v = jnp.concatenate([im(q), jnp.zeros_like(re(q))], axis=-1)\n  q_v = multiply(q, q_v)\n  q_v = multiply(q_v, conjugate(q))\n  return im(q_v)"}
{"namespace": "quaternion.rotate", "completion": "  q_vec = jnp.concatenate([jnp.zeros_like(v[Ellipsis, :1]), v], axis=-1)\n  return multiply(q, multiply(q_vec, conjugate(q)))[Ellipsis, 1:]"}
{"namespace": "quaternion.rotate", "completion": "  q_v = jnp.concatenate([im(v), jnp.zeros_like(im(v)[Ellipsis, :1])], axis=-1)\n  return multiply(multiply(q, q_v), inverse(q))[Ellipsis, :3]"}
{"namespace": "quaternion.rotate", "completion": "  q_v = jnp.concatenate([jnp.zeros_like(v), jnp.ones_like(v[Ellipsis, :1])],\n                        axis=-1)\n  q_v = multiply(q, q_v)\n  q_v = multiply(q_v, inverse(q))\n  return im(q_v)"}
{"namespace": "quaternion.rotate", "completion": "  q_conj = conjugate(q)\n  v_q = multiply(q, jnp.concatenate([v, jnp.zeros_like(v[..., :1])], axis=-1))\n  v_rotated = multiply(v_q, q_conj)\n  return v_rotated[..., :3]"}
{"namespace": "quaternion.rotate", "completion": "  # Convert vector to quaternion\n  q_v = jnp.concatenate([jnp.zeros_like(v), v], axis=-1)\n\n  # Multiply quaternions\n  q_v_rotated = multiply(multiply(q, q_v), conjugate(q))\n\n  # Extract rotated vector\n  v_rotated = q_v_rotated[Ellipsis, 3:]\n\n  return v_rotated"}
{"namespace": "quaternion.from_axis_angle", "completion": "  angle = jnp.linalg.norm(axis_angle, axis=-1, keepdims=True)\n  axis = axis_angle / (angle + eps)\n  return jnp.concatenate([axis * jnp.sin(angle / 2.0), jnp.cos(angle / 2.0)], axis=-1)"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis = axis_angle / (jnp.linalg.norm(axis_angle) + eps)\n  theta = jnp.linalg.norm(axis_angle)\n  q = jnp.concatenate([jnp.sin(theta / 2) * axis, jnp.cos(theta / 2)])\n  return q"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis = axis_angle / (jnp.linalg.norm(axis_angle) + eps)\n  angle = jnp.linalg.norm(axis_angle)\n  return jnp.concatenate([jnp.sin(angle / 2.0) * axis, jnp.cos(angle / 2.0)], axis=-1)"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis = axis_angle / (jnp.linalg.norm(axis_angle) + eps)\n  angle = jnp.linalg.norm(axis_angle)\n  return jnp.concatenate([axis * jnp.sin(angle / 2.0), [jnp.cos(angle / 2.0)]], axis=-1)"}
{"namespace": "quaternion.from_axis_angle", "completion": "  angle = jnp.linalg.norm(axis_angle)\n  if angle < eps:\n    return jnp.array([0.0, 0.0, 0.0, 1.0])\n\n  axis = axis_angle / angle\n  return jnp.concatenate([jnp.sin(angle / 2) * axis, jnp.cos(angle / 2)], axis=-1)"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis = axis_angle / (jnp.linalg.norm(axis_angle) + eps)\n  theta = jnp.linalg.norm(axis_angle)\n\n  return jnp.concatenate(\n      [\n          jnp.sin(theta / 2) * axis,\n          jnp.cos(theta / 2),\n      ],\n      axis=-1,\n  )"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  axis_angle = axis_angle / (jnp.linalg.norm(axis_angle) + eps)\n  q = jnp.concatenate([axis_angle * jnp.sin(0.5 * jnp.linalg.norm(axis_angle)), [jnp.cos(0.5 * jnp.linalg.norm(axis_angle))]])\n  return q"}
{"namespace": "quaternion.from_axis_angle", "completion": "  theta = jnp.linalg.norm(axis_angle, axis=-1)\n  axis = axis_angle / (theta + eps)\n  return jnp.concatenate((axis * jnp.sin(theta / 2.0),\n                          jnp.cos(theta / 2.0)), axis=-1)"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  axis = axis_angle[Ellipsis, :3]\n  angle = axis_angle[Ellipsis, 3:]\n  axis_norm = linalg.norm(axis, axis=-1, keepdims=True)\n  axis = axis / jnp.maximum(axis_norm, eps * jnp.ones_like(axis_norm))\n  angle = angle / 2.0\n  sin_angle = jnp.sin(angle)\n  cos_angle = jnp.cos(angle)\n  return jnp.concatenate([axis * sin_angle, cos_angle], axis=-1)"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  axis_angle = axis_angle / jnp.linalg.norm(axis_angle)\n  angle = jnp.linalg.norm(axis_angle)\n  axis = axis_angle / jnp.linalg.norm(axis_angle)\n  return jnp.concatenate(\n      [jnp.sin(angle / 2.0) * axis, jnp.cos(angle / 2.0)], axis=-1\n  )"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  axis = axis_angle[Ellipsis, :3]\n  angle = axis_angle[Ellipsis, 3:]\n\n  axis_norm = linalg.norm(axis, axis=-1, keepdims=True)\n  axis = axis / jnp.maximum(axis_norm, eps)\n  angle = angle / 2.0\n\n  return jnp.concatenate([jnp.sin(angle) * axis, jnp.cos(angle)], axis=-1)"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  axis = axis_angle[Ellipsis, :3]\n  angle = axis_angle[Ellipsis, 3:]\n\n  # Normalize the axis.\n  axis = axis / jnp.linalg.norm(axis, axis=-1, keepdims=True)\n\n  # Construct the quaternion.\n  q = jnp.concatenate([jnp.cos(angle / 2.0), axis * jnp.sin(angle / 2.0)], axis=-1)\n\n  # Ensure numerical stability for small angles.\n  q = q / jnp.linalg.norm(q, axis=-1, keepdims=True)\n\n  return q"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle, dtype=jnp.float32)\n  theta = jnp.linalg.norm(axis_angle)\n  axis = axis_angle / (theta + eps)\n  quaternion = jnp.concatenate(\n      [jnp.cos(theta / 2.0), axis * jnp.sin(theta / 2.0)]\n  )\n  return quaternion"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  axis = axis_angle[:3]\n  angle = axis_angle[3]\n  axis = axis / jnp.linalg.norm(axis)\n  theta = angle\n  axis = axis * theta\n  q = jnp.concatenate((jnp.sin(theta / 2) * axis, jnp.cos(theta / 2)))\n  return q"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle_norm = linalg.norm(axis_angle, axis=-1, keepdims=True)\n  axis_angle_norm = jnp.maximum(axis_angle_norm, eps * jnp.ones_like(axis_angle_norm))\n  axis_angle_norm = axis_angle / axis_angle_norm\n\n  q = jnp.concatenate(\n      [\n          jnp.cos(axis_angle_norm[Ellipsis, 3:] / 2.0),\n          axis_angle_norm[Ellipsis, :3] * jnp.sin(axis_angle_norm[Ellipsis, 3:] / 2.0),\n      ],\n      axis=-1,\n  )\n\n  return normalize(q)"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  axis_angle_norm = linalg.norm(axis_angle, axis=-1, keepdims=True)\n  axis_angle_normalized = axis_angle / jnp.maximum(axis_angle_norm, eps)\n\n  angle = axis_angle_norm * 0.5\n\n  quaternion = jnp.concatenate(\n      [jnp.cos(angle), jnp.sin(angle) * axis_angle_normalized], axis=-1\n  )\n\n  return quaternion"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  axis_angle_norm = jnp.linalg.norm(axis_angle, axis=-1, keepdims=True)\n  axis_angle_normalized = axis_angle / (axis_angle_norm + eps)\n  sin_theta_2 = jnp.linalg.norm(axis_angle_normalized, axis=-1)\n  cos_theta_2 = 1.0 - sin_theta_2**2\n  quaternion_v = axis_angle_normalized * jnp.sqrt(0.5 * cos_theta_2)\n  quaternion_s = jnp.sqrt(0.5 * sin_theta_2)\n  quaternion = jnp.concatenate([quaternion_v, quaternion_s[..., jnp.newaxis]], axis=-1)\n  return quaternion"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  axis_angle_norm = jnp.linalg.norm(axis_angle)\n\n  if axis_angle_norm < eps:\n    return jnp.array([0.0, 0.0, 0.0, 1.0])\n\n  axis_angle_normalized = axis_angle / axis_angle_norm\n  angle = axis_angle_norm\n  axis = axis_angle_normalized\n\n  half_angle = 0.5 * angle\n  sin_half_angle = jnp.sin(half_angle)\n  cos_half_angle = jnp.cos(half_angle)\n\n  q_vector = jnp.concatenate((axis * sin_half_angle, [cos_half_angle]))\n  q_vector /= jnp.linalg.norm(q_vector)\n\n  return q_vector"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis = axis_angle[Ellipsis, :3]\n  angle = axis_angle[Ellipsis, 3:]\n\n  sin_theta_2 = jnp.linalg.norm(axis, axis=-1, keepdims=True)\n  sin_theta = jnp.where(sin_theta_2 > eps, sin_theta_2, sin_theta_2)\n  cos_theta = jnp.cos(angle / 2.0)\n  return jnp.concatenate((axis * sin_theta / sin_theta_2 * cos_theta, cos_theta), axis=-1)"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.asarray(axis_angle)\n  axis = axis_angle[Ellipsis, :3]\n  angle = axis_angle[Ellipsis, 3:]\n\n  axis_norm = jnp.linalg.norm(axis, axis=-1, keepdims=True)\n  axis_norm = jnp.where(axis_norm < eps, jnp.ones_like(axis_norm), axis_norm)\n  axis = axis / axis_norm\n  theta = angle\n  half_theta = theta / 2.0\n  s = jnp.sin(half_theta)\n  c = jnp.cos(half_theta)\n  q = jnp.concatenate([axis * s, c], axis=-1)\n  return q"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # check if idx is the argmax\n    if model.argmax(prefix) == idx:\n        return 0, 1\n\n    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n    high = logit_bias[idx]\n\n    # improve estimate\n    mid = (high + low) / 2\n    while high >= low + eps:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n    return -mid, 1"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # check if idx is the argmax\n    num_calls = k\n    if model.argmax(prefix) == idx:\n        return 0, num_calls\n\n    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n        num_calls += k\n    high = logit_bias[idx]\n\n    # improve estimate\n    mid = (high + low) / 2\n    while high >= low + eps:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n        num_calls += k\n    return -mid, num_calls"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    topk_words = model.topk(prefix, logit_bias)\n    if idx in topk_words:\n        return topk_words[idx], 1\n    else:\n        while idx not in topk_words:\n            logit_bias[idx] *= 2\n            topk_words = model.topk(prefix, logit_bias)\n        return topk_words[idx], 1"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # check if idx is the argmax\n    if model.argmax(prefix) == idx:\n        return 0, 1\n\n    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n    high = logit_bias[idx]\n\n    # improve estimate\n    mid = (high + low) / 2\n    while high >= low + eps:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n    return -mid, 1"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # check if idx is the argmax\n    if model.argmax(prefix) == idx:\n        return 0, 1\n\n    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n        num_calls += 1\n    high = logit_bias[idx]\n\n    # improve estimate\n    mid = (high + low) / 2\n    while high >= low + eps:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n        num_calls += 1\n    return -mid, num_calls"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    topk_words = model.topk(prefix, logit_bias)\n    num_calls = k\n    if idx in topk_words:\n        return topk_words[idx], num_calls\n    else:\n        while idx not in topk_words:\n            logit_bias[idx] *= 2\n            topk_words = model.topk(prefix, logit_bias)\n            num_calls += k\n        return topk_words[idx], num_calls"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    vocab_size = model.vocab_size\n    logit_bias = {idx: high}\n    logit_list = []\n    num_calls = k\n    while True:\n        topk_words = model.topk(prefix, logit_bias)\n        if idx in topk_words:\n            logit_list.append(topk_words[idx])\n            break\n        logit_bias[idx] *= 2\n        num_calls += k\n    return logit_list, num_calls"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # check if idx is the argmax\n    if model.argmax(prefix) == idx:\n        return 0, 1\n\n    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n\n    # improve estimate\n    low = high / 2\n    while high >= low + 1e-8:\n        mid = (high + low) / 2\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n    return -mid, 1"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    logit_bias, num_calls = model.topk(prefix, logit_bias, k=k)\n    if logit_bias[idx] == 0:\n        return 0, num_calls\n    else:\n        logit_bias[idx] = 0\n        logit_bias, num_calls = model.topk(prefix, logit_bias, k=k)\n        return logit_bias[idx], num_calls"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    logprobs = model.topk(prefix, logit_bias)\n    logprobs = np.array([logprobs[i] for i in range(vocab_size)])\n    logprobs = logprobs - logsumexp(logprobs)\n    return logprobs[idx], 1"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    vocab_size = model.vocab_size\n    logit_bias = {idx: high}\n    logits = model.topk(prefix, logit_bias, k=vocab_size)\n    logit_bias[idx] = -logits[idx]\n    logits = model.topk(prefix, logit_bias, k=k)\n    return logits[idx], k"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    logits = model.topk(prefix, logit_bias)\n    logit = logits[idx]\n    while logits[idx] != max(logits.values()):\n        logit_bias[idx] *= 2\n        logits = model.topk(prefix, logit_bias)\n    return logit, len(logits)"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    topk_words = model.topk(prefix, logit_bias)\n    if idx not in topk_words:\n        return 0, 1\n    logprob = topk_words[idx]\n    num_calls = 1\n    while idx not in topk_words:\n        logit_bias[idx] *= 2\n        topk_words = model.topk(prefix, logit_bias)\n        num_calls += k\n    return logprob - logsumexp(np.array([topk_words[i] for i in topk_words])), num_calls"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    topk_words = model.topk(prefix, logit_bias)\n    num_calls = 1\n    if idx not in topk_words:\n        return 0, num_calls\n    while topk_words[idx] < topk_words[model.argmax(prefix, logit_bias)]:\n        logit_bias[idx] *= 2\n        topk_words = model.topk(prefix, logit_bias)\n        num_calls += 1\n    return -logit_bias[idx], num_calls"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    logprobs = model.topk(prefix, logit_bias)\n    top_logprob = logprobs[idx]\n    while logprobs[idx] < logprobs[model.argmax(prefix, logit_bias)]:\n        logit_bias[idx] *= 2\n        logprobs = model.topk(prefix, logit_bias)\n    return top_logprob, logprobs[idx]"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    topk_words = model.topk(prefix, logit_bias)\n    if idx not in topk_words:\n        raise TypeError(\n            f\"Tokens {idx} not in top-k with bias {high}. Either increase bias or provide top unbiased logprob (top_logprob)\"\n        )\n    topk_words = model.topk(prefix, logit_bias)\n    logprobs = np.array([topk_words[i] for i in range(vocab_size)])\n    log_biased_prob = logsumexp(logprobs)\n    return logprobs[idx] - np.logaddexp(\n        high + np.log1p(-np.exp(log_biased_prob)), log_biased_prob\n    ), num_calls"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    vocab_size = model.vocab_size\n    logit_bias = {idx: high}\n    topk_words = model.topk(prefix, logit_bias)\n    if idx in topk_words:\n        return topk_words[idx], 1\n    else:\n        # adjust logit_bias\n        logit_bias[idx] = high\n        while idx not in topk_words:\n            logit_bias[idx] *= 2\n            topk_words = model.topk(prefix, logit_bias)\n            k += 1\n        logit_bias[idx] = high\n        while idx not in topk_words:\n            logit_bias[idx] /= 2\n            topk_words = model.topk(prefix, logit_bias)\n            k += 1\n        return topk_words[idx], k"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    logits, _ = model.logits(prefix, logit_bias=logit_bias)\n    logits = logits[:, -1]\n    top_logprob = logits[idx]\n    top_logprobs = logits.argsort()[-k:][::-1]\n    if idx in top_logprobs:\n        return top_logprob, 1\n    else:\n        while idx not in top_logprobs:\n            logit_bias[idx] *= 2\n            logits, _ = model.logits(prefix, logit_bias=logit_bias)\n            logits = logits[:, -1]\n            top_logprob = logits[idx]\n            top_logprobs = logits.argsort()[-k:][::-1]\n        return top_logprob, 1"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    logit_bias_list = [logit_bias]\n    topk_words = model.topk(prefix, logit_bias)\n    while idx not in topk_words:\n        logit_bias[idx] *= 2\n        logit_bias_list.append(logit_bias)\n        topk_words = model.topk(prefix, logit_bias)\n\n    logit_bias = logit_bias_list[-1]\n    topk_words = model.topk(prefix, logit_bias)\n    log_prob = topk_words[idx]\n    num_calls = k\n    for logit_bias in reversed(logit_bias_list):\n        topk_words = model.topk(prefix, logit_bias)\n        log_prob = topk_words[idx] + math.log(1 - math.exp(log_prob - topk_words[idx]))\n        num_calls += k\n    return log_prob, num_calls"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    vocab_size = model.vocab_size\n    logit_bias = {idx: high}\n    logit_bias = {i: high for i in range(vocab_size)}\n    logit_bias[idx] = high\n    topk_words = model.topk(prefix, logit_bias)\n    logit_bias = {i: high for i in range(vocab_size)}\n    logit_bias[idx] = high\n    topk_words = model.topk(prefix, logit_bias)\n    if idx in topk_words:\n        return topk_words[idx], 1\n    else:\n        low = high\n        high *= 2\n        while idx not in topk_words:\n            logit_bias = {i: high for i in range(vocab_size)}\n            logit_bias[idx] = high\n            topk_words = model.topk(prefix, logit_bias)\n            high *= 2\n            if high - low < eps:\n                break\n        if idx in topk_words:\n            return topk_words[idx], 1\n        else:\n            while idx not in topk_words:\n                logit_bias = {i: high for i in range(vocab_size)}\n                logit_bias[idx] = high\n                topk_words = model.topk(prefix, logit_bias)\n                high = (high + low) / 2\n                if high - low < eps:\n                    break\n            return topk_words[idx], 1"}
{"namespace": "resample.resample_3d", "completion": "  "}
{"namespace": "resample.resample_3d", "completion": "  "}
{"namespace": "resample.resample_3d", "completion": "  "}
{"namespace": "resample.resample_3d", "completion": "  "}
{"namespace": "resample.resample_3d", "completion": "  "}
{"namespace": "resample.resample_3d", "completion": "  "}
{"namespace": "resample.resample_3d", "completion": "  "}
{"namespace": "resample.resample_3d", "completion": "  "}
{"namespace": "resample.resample_3d", "completion": "  "}
{"namespace": "resample.resample_3d", "completion": "  "}
{"namespace": "resample.resample_3d", "completion": "  "}
{"namespace": "resample.resample_3d", "completion": "  \"\"\"\n  This function resamples input 3D data at specified locations using either trilinear interpolation or nearest neighbor interpolation. It supports edge behavior handling and can adjust for half-pixel centering.\n\n  Input-Output Arguments\n  :param data: A tensor with shape [D, H, W, C], representing the volume from which to sample. It is the input volume data.\n  :param locations: A tensor with shape [D, ..., 3], containing floating point locations at which to sample the data. These locations assume voxel centers at integer coordinates.\n  :param edge_behavior: String, specifying how to handle sample points outside the input volume. Options are 'CONSTANT_OUTSIDE' (pads the volume and interpolates towards a constant value outside the tensor) and 'CLAMP' (clamps sample points to the volume). Default is 'CONSTANT_OUTSIDE'.\n  :param constant_values: Float, the constant value to use when 'edge_behavior' is set to 'CONSTANT_OUTSIDE'. Default is 0.0.\n  :param coordinate_order: String, indicating whether the sample locations are in 'xyz' or 'zyx' order. Default is 'xyz'.\n  :param method: String, specifying the interpolation method to use. Must be either 'TRILINEAR' for trilinear interpolation or 'NEAREST' for nearest neighbor interpolation. Default is 'TRILINEAR'.\n  :param half_pixel_center: Bool, determines if half-pixel centering is used, which affects the calculation of sample locations. Default is False.\n  :return: A tensor of shape [D, ..., C] containing the resampled values. This tensor contains the output volume data after resampling at the specified locations.\n\n  \"\"\"\n\n  \"\"\"\n  This function resamples input 3D data at specified locations using either trilinear interpolation or nearest neighbor interpolation. It supports edge behavior handling and can adjust for half-pixel centering.\n\n  Input-Output Arguments\n  :param data: A tensor with shape [D, H, W, C], representing the volume from which to sample. It is the"}
{"namespace": "resample.resample_3d", "completion": "  if half_pixel_center:\n    locations = locations + jnp.array([0.5, 0.5, 0.5])\n\n  if method == 'TRILINEAR':\n    return trilinear_resample_3d(\n        data, locations, edge_behavior, constant_values, coordinate_order)\n  elif method == 'NEAREST':\n    return nearest_neighbor_resample_3d(\n        data, locations, edge_behavior, constant_values, coordinate_order)\n  else:\n    raise ValueError(f'Unknown method {method}')\n\n"}
{"namespace": "resample.resample_3d", "completion": "  # If half_pixel_center is True, adjust the sample locations by half a voxel.\n  if half_pixel_center:\n    locations = locations - 0.5\n\n  # If edge_behavior is 'CONSTANT_OUTSIDE', pad the input volume with constant values.\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    data = jnp.pad(data, ((1, 1), (1, 1), (1, 1), (0, 0)), 'constant', constant_values=constant_values)\n\n  # If edge_behavior is 'CLAMP', clamp the sample locations to the input volume.\n  elif edge_behavior == 'CLAMP':\n    locations = jnp.clip(locations, 0, data.shape[0] - 1)\n\n  # If method is 'TRILINEAR', perform trilinear interpolation.\n  if method == 'TRILINEAR':\n    return trilinear_interpolation(data, locations, coordinate_order)\n\n  # If method is 'NEAREST', perform nearest neighbor interpolation.\n  elif method == 'NEAREST':\n    return nearest_neighbor_interpolation(data, locations, coordinate_order)\n\n  # If method is not 'TRILINEAR' or 'NEAREST', raise an error.\n  else:\n    raise ValueError('Invalid method specified. Must be either \"TRILINEAR\" or \"NEAREST\".')"}
{"namespace": "resample.resample_3d", "completion": "  # Check the input arguments\n  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError('Invalid edge_behavior: {}'.format(edge_behavior))\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError('Invalid method: {}'.format(method))\n\n  # Check the shape of the input data and locations\n  if len(data.shape) != 4:\n    raise ValueError('Input data must have shape [D, H, W, C]')\n  if len(locations.shape) < 3 or locations.shape[-1] != 3:\n    raise ValueError('Locations must have shape [D, ..., 3]')\n\n  # Check the coordinate_order argument\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError('Invalid coordinate_order: {}'.format(coordinate_order))\n\n  # Check the shape of the input data and locations\n  if len(data.shape) != 4:\n    raise ValueError('Input data must have shape [D, H, W, C]')\n  if len(locations.shape) < 3 or locations.shape[-1] != 3:\n    raise ValueError('Locations must have shape [D, ..., 3]')\n\n  # Check the shape of the input data and locations\n  if len(data.shape) != 4:\n    raise ValueError('Input data must have shape [D, H, W, C]')\n  if len(locations.shape) < 3 or locations.shape[-1] != 3:\n    raise ValueError('Locations must have shape [D, ..., 3]')\n\n  # Check the shape of the input data and locations\n  if len(data.shape) != 4:\n    raise ValueError('Input data must have shape [D, H, W, C]')\n  if len(locations.shape) < 3 or locations.shape[-1] != 3:\n    raise ValueError('Locations must have shape [D, ..., 3]')\n\n  # Check the shape of"}
{"namespace": "resample.resample_3d", "completion": "  # Pad the input volume with constant values if necessary.\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    data = jnp.pad(data, ((1, 1), (1, 1), (1, 1), (0, 0)), 'constant', constant_values=constant_values)\n\n  # Calculate the sample locations.\n  if half_pixel_center:\n    locations = locations + 0.5\n\n  # Gather the input volume data at the sample locations.\n  gathered_data = gather_volume(data, locations, coordinate_order)\n\n  # Perform the interpolation.\n  if method == 'TRILINEAR':\n    # Trilinear interpolation.\n    # TODO(jonas): Implement trilinear interpolation.\n    pass\n  elif method == 'NEAREST':\n    # Nearest neighbor interpolation.\n    # TODO(jonas): Implement nearest neighbor interpolation.\n    pass\n\n  # Return the resampled data.\n  return gathered_data"}
{"namespace": "resample.resample_3d", "completion": "  if half_pixel_center:\n    locations = locations + 0.5\n\n  if coordinate_order == 'xyz':\n    x_coordinate = locations[Ellipsis, 0]\n    y_coordinate = locations[Ellipsis, 1]\n    z_coordinate = locations[Ellipsis, 2]\n  elif coordinate_order == 'zyx':\n    z_coordinate = locations[Ellipsis, 0]\n    y_coordinate = locations[Ellipsis, 1]\n    x_coordinate = locations[Ellipsis, 2]\n\n  # Pad the data to handle edge cases.\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    data = jnp.pad(data, ((1, 1), (1, 1), (1, 1), (0, 0)),\n                   mode='constant',\n                   constant_values=constant_values)\n  elif edge_behavior == 'CLAMP':\n    data = jnp.pad(data, ((1, 1), (1, 1), (1, 1), (0, 0)),\n                   mode='edge')\n\n  # Calculate the fractional part of the coordinates.\n  x_fraction = x_coordinate - jnp.floor(x_coordinate)\n  y_fraction = y_coordinate - jnp.floor(y_coordinate)\n  z_fraction = z_coordinate - jnp.floor(z_coordinate)\n\n  # Calculate the indices of the four nearest neighbors.\n  x_indices = jnp.floor(x_coordinate)\n  y_indices = jnp.floor(y_coordinate)\n  z_indices = jnp.floor(z_coordinate)\n\n  # Calculate the interpolation weights.\n  if method == 'TRILINEAR':\n    x_weight = x_fraction\n    y_weight = y_fraction\n    z_weight = z_fraction\n  elif method == 'NEAREST':\n    x_weight = jnp.zeros_like"}
{"namespace": "resample.resample_3d", "completion": "  # Ensure that locations are in the correct order.\n  if coordinate_order == 'xyz':\n    z_coordinate = locations[Ellipsis, 0]\n    y_coordinate = locations[Ellipsis, 1]\n    x_coordinate = locations[Ellipsis, 2]\n  elif coordinate_order == 'zyx':\n    z_coordinate = locations[Ellipsis, 0]\n    y_coordinate = locations[Ellipsis, 1]\n    x_coordinate = locations[Ellipsis, 2]\n\n  # If half_pixel_center is True, adjust the sample locations to account for half-pixel centering.\n  if half_pixel_center:\n    z_coordinate = z_coordinate + 0.5\n    y_coordinate = y_coordinate + 0.5\n    x_coordinate = x_coordinate + 0.5\n\n  # If edge_behavior is 'CONSTANT_OUTSIDE', pad the data tensor with constant values.\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    data = jnp.pad(data, ((1, 1), (1, 1), (1, 1), (0, 0)),\n                   mode='constant',\n                   constant_values=constant_values)\n\n  # If edge_behavior is 'CLAMP', clamp the sample locations to the volume boundaries.\n  if edge_behavior == 'CLAMP':\n    z_coordinate = jnp.clip(z_coordinate, 0, data.shape[0] - 1)\n    y_coordinate = jnp.clip(y_coordinate, 0, data.shape[1] - 1)\n    x_coordinate = jnp.clip(x_coordinate, 0, data.shape[2] - 1)\n\n  # If method is 'TRILINEAR', use trilinear interpolation.\n  if method == 'TRILINEAR':\n    # Calculate the fractional coordinates.\n    fractional_z = z_coordinate - jnp.floor"}
{"namespace": "resample.resample_3d", "completion": "  # Convert locations to integers.\n  locations = jnp.round(locations).astype(jnp.int32)\n\n  # Pad the input data if needed.\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    data = jnp.pad(data, ((1, 1), (1, 1), (1, 1), (0, 0)), constant_values=constant_values)\n    locations = locations + 1\n\n  # Calculate the sample locations.\n  if half_pixel_center:\n    locations = locations + 0.5\n\n  # Gather the data at the sample locations.\n  data_at_locations = gather_volume(data, locations, coordinate_order)\n\n  # Perform trilinear interpolation.\n  if method == 'TRILINEAR':\n    # Calculate the fractional sample locations.\n    fractional_locations = locations - jnp.floor(locations)\n\n    # Calculate the trilinear weights.\n    weights = jnp.einsum('...i,...i->...', fractional_locations, fractional_locations)\n    weights = jnp.clip(1.0 - weights, 0, 1)\n    weights = jnp.expand_dims(weights, axis=-1)\n\n    # Perform trilinear interpolation.\n    data_at_locations = jnp.sum(weights * data_at_locations, axis=-2)\n\n  # Return the resampled data.\n  return data_at_locations"}
{"namespace": "resample.resample_3d", "completion": "  # Handle edge behavior.\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    # Pad the input volume with a constant value.\n    padded_data = jnp.pad(data, ((1, 1), (1, 1), (1, 1), (0, 0)),\n                          'constant',\n                          constant_values=constant_values)\n\n    # Compute the sample locations within the padded volume.\n    if half_pixel_center:\n      sample_locations = locations + 0.5\n    else:\n      sample_locations = locations\n\n    # Gather the values at the sample locations.\n    gathered_values = gather_volume(padded_data, sample_locations,\n                                    coordinate_order)\n\n    # Trilinearly interpolate the gathered values.\n    if method == 'TRILINEAR':\n      # Compute the fractional part of the sample locations.\n      fractional_locations = sample_locations - jnp.floor(sample_locations)\n\n      # Compute the trilinear weights.\n      trilinear_weights = jnp.multiply(fractional_locations,\n                                       jnp.array([1.0, 1.0, 1.0]))\n\n      # Compute the trilinear interpolation.\n      trilinear_interpolation = (\n          (1.0 - trilinear_weights[Ellipsis, 0]) *\n          (1.0 - trilinear_weights[Ellipsis, 1]) *\n          (1.0 - trilinear_weights[Ellipsis, 2]) * gathered_values[..., 0] +\n          (1.0 - trilinear_weights[Ellipsis, 0]) *\n          (1.0 - trilinear_weights[Ellipsis, 1]) *\n          trilinear_weights[Ellipsis, 2] * gathered_values[..., 1] +\n          (1.0 - trilinear_weights[Ell"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(x, +jnp.inf))"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(x, +jnp.inf))"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(x, +jnp.inf))"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(x, +jnp.inf))"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(x, +jnp.inf))"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(x, +jnp.inf))"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(x, jnp.inf))"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(x, jnp.inf))"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(x, jnp.inf))"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x), +jnp.inf)\n  )"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(x, jnp.inf))"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(x, +jnp.inf))"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x), +jnp.inf)\n  )"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x), +jnp.inf))"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(x, +jnp.inf))"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x), jnp.inf)\n  )"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(x, jnp.inf))\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x), jnp.inf))"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(x, jnp.inf))\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(x, jnp.inf))\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -np.inf)\n  )"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -np.inf)\n  )"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -np.inf)\n  )"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -np.inf)\n  )"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -np.inf)\n  )"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -np.inf)\n  )"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -np.inf)\n  )"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -np.inf)\n  )"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -np.inf)\n  )"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -np.inf)\n  )"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -np.inf)\n  )"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -np.inf)\n  )"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -np.inf)\n  )"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -np.inf)\n  )"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), jnp.inf)\n  )"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf))"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -np.inf)\n  )"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), jnp.NINF)\n  )"}
{"namespace": "math.minus_eps", "completion": "  if x < tiny_val:\n    return -tiny_val\n  else:\n    return jnp.nextafter(jnp.float32(x), jnp.float32(np.inf))"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "def safe_exp(x):\n  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, _, x_dot: jnp.exp(x) * x_dot,\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, _, x_dot: jnp.exp(x) * x_dot,\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, _, x_dot: jnp.exp(x) * x_dot,\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "def safe_exp(x):\n  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, np.nextafter(max_val, np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, _, x_dot: jnp.exp(x) * x_dot,\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, _, x_dot: jnp.exp(x) * x_dot,\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "def safe_exp(x):\n  return generate_safe_fn(\n      jnp.exp,\n      lambda x, _, x_dot: jnp.exp(x) * x_dot,\n      (min_val, np.nextafter(max_val, np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, _, x_dot: jnp.exp(x) * x_dot,\n      (np.nextafter(np.float32(-1), np.float32(0)), np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "safe_exp.defvjp(\n    lambda x: (jnp.clip(x, -1000, 1000), None),\n    lambda _, y: (y,),\n)\n\n\ndef safe_log1p(x):\n  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + x),\n      (np.nextafter(np.float32(-1), np.float32(0)), max_val),\n  )(x)\n\n\ndef safe_expm1(x):\n  return generate_safe_fn(\n      expm1,  # Note that we wrap around our more accurate expm1.\n      lambda x, _, x_dot: jnp.exp(x) * x_dot,\n      (min_val, np.nextafter(np.log1p(max_val), np.float32(0))),\n  )(x)\n\n\ndef safe_arccos(x):\n  \"\"\"jnp.arccos(x) where x is clipped to [-1, 1].\"\"\"\n  y = jnp.arccos(jnp.clip(x, plus_eps(-1), minus_eps(1)))\n  return jnp.where(x >= 1, 0, jnp.where(x <= -1, jnp.pi, y))\n\n\ndef apply_fn_to_grad(grad_fn):\n  \"\"\"Applies a scalar `grad_fn` function to the gradient of the input.\"\"\"\n\n  @jax.custom_vjp\n  def fn_out(x):\n    return x\n\n  fn_out.defvjp(lambda x: (x, None), lambda _, y: (grad_fn(y),))\n  return fn_out\n\n\ndef select(cond_pairs, default):\n  \"\"\"A helpful wrapper around jnp.select() that is easier to read.\"\"\"\n  return jnp.select(*zip(*cond_pairs), default)\n\n\ndef power_ladder_max_output(p):"}
{"namespace": "math.safe_exp", "completion": "safe_exp.defvjp(safe_exp_fwd, safe_exp_bwd)\n\n\ndef safe_exp_fwd(x):\n  \"\"\"Forward pass of the safe exponential function.\"\"\"\n  return jnp.exp(jnp.clip(x, min_val, np.nextafter(np.log(max_val), np.float32(0)))), (x,)\n\n\ndef safe_exp_bwd(res, g):\n  \"\"\"Backward pass of the safe exponential function.\"\"\"\n  x, = res\n  return jnp.clip(g * jnp.exp(x), min_val, max_val),"}
{"namespace": "math.safe_exp", "completion": "safe_exp.defvjp(safe_exp_fwd, safe_exp_bwd)\n\n\ndef safe_exp_fwd(x):\n  \"\"\"Forward pass of the safe exponential function.\"\"\"\n  return jnp.exp(jnp.clip(x, min_val, max_val)), (x,)\n\n\ndef safe_exp_bwd(res, g):\n  \"\"\"Backward pass of the safe exponential function.\"\"\"\n  (x,) = res\n  return jnp.clip(g * jnp.exp(x), min_val, max_val),\n\n\ndef safe_log1p(x):\n  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + x),\n      (np.nextafter(np.float32(-1), np.float32(0)), max_val),\n  )(x)\n\n\ndef safe_expm1(x):\n  return generate_safe_fn(\n      expm1,  # Note that we wrap around our more accurate expm1.\n      lambda x, _, x_dot: jnp.exp(x) * x_dot,\n      (min_val, np.nextafter(np.log1p(max_val), np.float32(0))),\n  )(x)\n\n\ndef safe_arccos(x):\n  \"\"\"jnp.arccos(x) where x is clipped to [-1, 1].\"\"\"\n  y = jnp.arccos(jnp.clip(x, plus_eps(-1), minus_eps(1)))\n  return jnp.where(x >= 1, 0, jnp.where(x <= -1, jnp.pi, y))\n\n\ndef apply_fn_to_grad(grad_fn):\n  \"\"\"Applies a scalar `grad_fn` function to the gradient of the input.\"\"\"\n\n  @jax.custom_vjp\n  def fn_out(x):\n    return x\n\n  fn_out.defvjp(lambda x: (x, None"}
{"namespace": "math.safe_exp", "completion": "safe_exp.defvjp(\n    lambda x: (jnp.exp(jnp.clip(x, tiny_val, max_val)), (x,)),\n    lambda res, g: (g * res[1],),\n)\n\n\ndef safe_log1p(x):\n  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + x),\n      (np.nextafter(np.float32(-1), np.float32(0)), max_val),\n  )(x)\n\n\ndef safe_sqrt(x):\n  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, _, x_dot: 0.5 * x_dot / jnp.sqrt(jnp.maximum(tiny_val, x)),\n      (0, max_val),\n  )(x)\n\n\ndef safe_expm1(x):\n  return generate_safe_fn(\n      expm1,  # Note that we wrap around our more accurate expm1.\n      lambda x, _, x_dot: jnp.exp(x) * x_dot,\n      (min_val, np.nextafter(np.log1p(max_val), np.float32(0))),\n  )(x)\n\n\ndef safe_arccos(x):\n  \"\"\"jnp.arccos(x) where x is clipped to [-1, 1].\"\"\"\n  y = jnp.arccos(jnp.clip(x, plus_eps(-1), minus_eps(1)))\n  return jnp.where(x >= 1, 0, jnp.where(x <= -1, jnp.pi, y))\n\n\ndef apply_fn_to_grad(grad_fn):\n  \"\"\"Applies a scalar `grad_fn` function to the gradient of the input.\"\"\"\n\n  @jax.custom_vjp\n  def fn_out(x):\n    return x\n\n  fn_out.defvjp(lambda x: ("}
{"namespace": "math.safe_exp", "completion": "safe_exp.defvjp(safe_exp_fwd, safe_exp_bwd)\n\n\ndef safe_exp_fwd(x):\n  return jnp.exp(jnp.clip(x, -20, 20)), (x,)\n\n\ndef safe_exp_bwd(res, g):\n  x, = res\n  return jnp.clip(g * jnp.exp(x), -20, 20),\n\n\ndef safe_log10(x):\n  return generate_safe_fn(\n      jnp.log10, lambda x, _, x_dot: x_dot / (jnp.log(10) * x), (tiny_val, max_val)\n  )(x)\n\n\ndef safe_log2(x):\n  return generate_safe_fn(\n      jnp.log2, lambda x, _, x_dot: x_dot / (jnp.log(2) * x), (tiny_val, max_val)\n  )(x)\n\n\ndef safe_log1p(x):\n  return generate_safe_fn(\n      jnp.log1p, lambda x, _, x_dot: x_dot / (1 + x), (tiny_val, max_val)\n  )(x)\n\n\ndef safe_sqrt(x):\n  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, _, x_dot: 0.5 * x_dot / jnp.sqrt(jnp.maximum(tiny_val, x)),\n      (0, max_val),\n  )(x)\n\n\ndef safe_pow(x, p):\n  return generate_safe_fn(\n      lambda x: jnp.power(x, p),\n      lambda x, _, x_dot: p * jnp.power(x, p - 1) * x_dot,\n      (tiny_val, max_val),\n  )(x)\n\n\ndef safe_rsqrt(x):\n  return generate_safe_fn(\n      jnp.rsqrt,"}
{"namespace": "math.safe_exp", "completion": "safe_exp.defvjp(safe_exp_fwd, safe_exp_bwd)\n\n\ndef safe_exp_fwd(x):\n  \"\"\"\n  The function computes the forward pass of the safe exponential function. It clips the input to a specified range and applies the exponential function to the clipped input.\n\n  Input-Output Arguments\n  :param x: The input value for which the safe exponential function is computed. It is used as the argument for the exponential function and its custom gradient.\n  :return: The result of applying the safe exponential function to the input x. This ensures that the output is within a specified range to prevent overflow errors.\n  \"\"\"\n  x = jnp.clip(x, min_val, np.nextafter(np.log(max_val), np.float32(0)))\n  return jnp.exp(x), (x,)\n\n\ndef safe_exp_bwd(res, g):\n  \"\"\"\n  The function computes the backward pass of the safe exponential function. It computes the gradient of the output with respect to the input using the gradient of the exponential function and the result of the forward pass.\n\n  Input-Output Arguments\n  :param res: The result of the forward pass of the safe exponential function. It contains the clipped input and the result of the exponential function.\n  :param g: The gradient of the output with respect to the result of the safe exponential function.\n  :return: The gradient of the output with respect to the input x. This ensures that the gradient is within a specified range to prevent overflow errors.\n  \"\"\"\n  (x,) = res\n  return g * jnp.exp(x)"}
{"namespace": "math.safe_exp", "completion": "def safe_exp_jvp(primals, tangents):\n  \"\"\"\n  The function implements the custom gradient function for the safe exponential function. It computes the gradient of the output with respect to the input x, using the chain rule.\n\n  Input-Output Arguments\n  :param primals: A tuple containing the primal values of the function. The first element is the input x, and the second element is the result of applying the safe exponential function to x.\n  :param tangents: A tuple containing the tangent values of the function. The first element is the gradient of the output with respect to the input x.\n  :return: A tuple containing the gradients of the output with respect to the input x. The first element is the gradient of the output with respect to the input x, and the second element is the gradient of the output with respect to the result of applying the safe exponential function to x.\n  \"\"\"\n  (x,) = primals\n  (x_dot,) = tangents\n  y = safe_exp(x)\n  y_dot = jnp.exp(x) * x_dot\n  return y_dot, y_dot\n\n\nsafe_exp.defvjp(safe_exp_fwd, safe_exp_jvp)\n\n\ndef safe_log1p(x):\n  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + x),\n      (np.nextafter(np.float32(-1), np.float32(0)), max_val),\n  )(x)\n\n\ndef safe_log1p_jvp(primals, tangents):\n  (x,) = primals\n  (x_dot,) = tangents\n  y = safe_log1p(x)\n  y_dot = x_dot / (1 + x)\n  return y_dot, y_dot\n\n\nsafe_log1p.defvjp(safe_log1p_fwd, safe_log1p_jvp)\n\n\ndef safe_expm1(x):\n  return generate_safe_fn("}
{"namespace": "math.safe_exp", "completion": "safe_exp.defvjp(safe_exp_fwd, safe_exp_bwd)\n\n\ndef safe_exp_fwd(x):\n  \"\"\"\n  The function calculates the forward pass of the safe exponential function. It applies a custom gradient function for backpropagation in automatic differentiation contexts.\n\n  Input-Output Arguments\n  :param x: The input value for which the safe exponential function is computed. It is used as the argument for the exponential function and its custom gradient.\n  :return: The result of applying the safe exponential function to the input x. This ensures that the output is within a specified range to prevent overflow errors.\n  \"\"\"\n  r = jnp.exp(jnp.clip(x, min_val, max_val))\n  return jnp.where(jnp.abs(x) < tiny_val, 1, r), (x, r)\n\n\ndef safe_exp_bwd(res, g):\n  \"\"\"\n  The function calculates the backward pass of the safe exponential function. It applies a custom gradient function for backpropagation in automatic differentiation contexts.\n\n  Input-Output Arguments\n  :param res: The result of the forward pass of the safe exponential function, which includes the input x and the output y.\n  :param g: The gradient of the output y with respect to the loss function.\n  :return: The gradient of the input x with respect to the loss function, which is calculated using the custom gradient function and the input and output values.\n  \"\"\"\n  x, r = res\n  x_dot = g * r\n  return x_dot, None"}
{"namespace": "math.safe_exp", "completion": "safe_exp.defvjp(safe_exp_fwd, safe_exp_bwd)\n\n\ndef safe_exp_fwd(x):\n  \"\"\"Forward pass for the safe exponential function.\n\n  This function computes the safe exponential function for a given input x. It clips the input to a specified range to prevent overflow errors.\n\n  Args:\n  :param x: The input value for which the safe exponential function is computed. It is used as the argument for the exponential function and its custom gradient.\n\n  Returns:\n  :return: The result of applying the safe exponential function to the input x. This ensures that the output is within a specified range to prevent overflow errors.\n  \"\"\"\n  return jnp.exp(jnp.clip(x, min_val, max_val)), (x,)\n\n\ndef safe_exp_bwd(res, g):\n  \"\"\"Backward pass for the safe exponential function.\n\n  This function computes the gradient of the safe exponential function with respect to the input x. It uses the custom gradient function defined in safe_exp_fwd to compute the gradient.\n\n  Args:\n  :param res: The result of applying the safe exponential function to the input x. This ensures that the output is within a specified range to prevent overflow errors.\n  :param g: The gradient of the output with respect to the input x.\n\n  Returns:\n  :return: The gradient of the safe exponential function with respect to the input x.\n  \"\"\"\n  (x,) = res\n  return safe_exp(x) * g"}
{"namespace": "math.safe_exp", "completion": "safe_exp.defvjp(safe_exp_fwd, safe_exp_bwd)\n\n\ndef safe_exp_fwd(x):\n  \"\"\"\n  The function performs a forward pass in a safe exponential function. It clips the input to a specified range and computes the exponential value.\n\n  Input-Output Arguments\n  :param x: The input value for which the safe exponential function is computed. It is clipped to a specified range and used as the argument for the exponential function.\n  :return: The result of applying the safe exponential function to the input x. This ensures that the output is within a specified range to prevent overflow errors.\n  \"\"\"\n  return jnp.exp(jnp.clip(x, min_val, max_val)), (x,)\n\n\ndef safe_exp_bwd(res, g):\n  \"\"\"\n  The function performs a backward pass in a safe exponential function. It computes the gradient of the exponential function with respect to the input.\n\n  Input-Output Arguments\n  :param res: The result of the forward pass of the safe exponential function. It contains the clipped input value.\n  :param g: The gradient of the output of the safe exponential function with respect to the output of the safe exponential function.\n  :return: The gradient of the output of the safe exponential function with respect to the input. This gradient is computed using the chain rule and the gradient of the exponential function with respect to the input.\n  \"\"\"\n  x = res[0]\n  return g * x,"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / jnp.maximum(tiny_val, x),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / jnp.maximum(tiny_val, x),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / jnp.maximum(tiny_val, x),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / jnp.maximum(tiny_val, x),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / jnp.maximum(tiny_val, x),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "def safe_log(x):\n  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / x,\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "def safe_log(x):\n  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / x,\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "def safe_log(x):\n  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / x,\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "def safe_log(x):\n  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / x,\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "def safe_log(x):\n  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / jnp.maximum(tiny_val, x),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / jnp.maximum(tiny_val, x),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / (jnp.maximum(tiny_val, x)),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, y, x_dot: x_dot / (jnp.maximum(tiny_val, x)),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / x,\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / jnp.maximum(tiny_val, x),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / jnp.maximum(tiny_val, x),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, y, x_dot: x_dot / (jnp.maximum(tiny_val, x)),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / jnp.maximum(tiny_val, x),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / (jnp.maximum(tiny_val, x)),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "def safe_log_jvp(primals, tangents):\n  \"\"\"\n  This function calculates the Jacobian-vector product (JVP) of the safe logarithm function (`safe_log`) with respect to its input. It computes the derivative of the safe logarithm function at `primals[0]` with respect to `tangents[0]`.\n\n  Input-Output Arguments\n  :param primals: A tuple containing two elements:\n    - `primals[0]`: The input value or array for which the logarithm is to be calculated. It is used as the input to the safe logarithm function created by `generate_safe_fn`.\n    - `primals[1]`: The tangent value or array with respect to which the JVP is to be computed.\n  :param tangents: A tuple containing a single element:\n    - `tangents[0]`: The tangent value or array with respect to which the JVP is to be computed.\n  :return: A tuple containing two elements:\n    - `y`: The result of applying the safe logarithm function to `primals[0]`. This could be a single value or an array of values, depending on the input `primals[0]`.\n    - `y_dot`: The derivative of the safe logarithm function at `primals[0]` with respect to `tangents[0]`. This could be a single value or an array of values, depending on the input `tangents[0]`.\n  \"\"\"\n  (x,) = primals\n  (x_dot,) = tangents\n  y = safe_log(x)\n  y_dot = jnp.where(\n      jnp.abs(x) < tiny_val,\n      0,\n      x_dot / (jnp.maximum(tiny_val, jnp.abs(x)) * y),\n  )\n  return y, y_dot\n\n\nsafe_log.defjvp(safe_log_jvp)\n\n\ndef safe_log1p(x):\n  \"\"\"jnp.log1p() but the gradient doesn't get clipped"}
{"namespace": "math.safe_sqrt", "completion": "def safe_sqrt(x):\n  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: safe_div(x_dot, 2 * y),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "def safe_sqrt(x):\n  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: safe_div(x_dot, 2 * y),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "def safe_sqrt(x):\n  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: safe_div(x_dot, 2 * y),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "def safe_sqrt(x):\n  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: safe_div(x_dot, 2 * y),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: 0.5 * x_dot / y,\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  "}
{"namespace": "math.safe_sqrt", "completion": "  "}
{"namespace": "math.safe_sqrt", "completion": "  "}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: safe_div(x_dot, 2 * y),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: safe_div(x_dot, 2 * y),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: 0.5 * x_dot / y,\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: 0.5 * x_dot / y,\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: 0.5 * x_dot / y,\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: 0.5 * x_dot / y,\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: safe_div(x_dot, 2 * y),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "def safe_sqrt(x):\n  \"\"\"\n  This function creates a safe version of the square root function that can handle edge cases more gracefully. It uses a helper function to generate this safe square root function, which includes a custom gradient for backpropagation in automatic differentiation contexts. The safe version ensures that the input is clamped between 0 and a maximum value to avoid invalid inputs like negative numbers.\n\n  Input-Output Arguments\n  :param x: The input value for which the safe square root will be computed. It is used as the argument for the square root function and its custom gradient computation.\n  :return: The result of applying the safe square root function to the input x. This includes the application of a custom gradient function for automatic differentiation purposes.\n  \"\"\"\n  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: 0.5 * x_dot / y,\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: safe_div(x_dot, 2 * y),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: x_dot / (2 * y),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: 0.5 * x_dot / jnp.clip(y, tiny_val, max_val),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  def safe_sqrt_fwd(x):\n    \"\"\"Forward pass for the safe_sqrt function.\"\"\"\n    return jnp.sqrt(jnp.clip(x, 0, max_val)), (x,)\n\n  def safe_sqrt_bwd(res, g):\n    \"\"\"Backward pass for the safe_sqrt function.\"\"\"\n    x = res[0]\n    x_dot = res[1][0]\n    return 0.5 * g / x, -0.5 * g / (2 * x) * x_dot\n\n  safe_sqrt.defvjp(safe_sqrt_fwd, safe_sqrt_bwd)\n\n  return safe_sqrt"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p > 0:\n    return jnp.inf\n  elif p < 0:\n    return -jnp.inf\n  else:\n    return 1"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 0\n  elif p == -jnp.inf:\n    return -jnp.inf\n  elif p == jnp.inf:\n    return jnp.inf\n  elif p < 0:\n    return -1\n  elif p > 0:\n    return 1\n  else:\n    return 0"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return jnp.inf\n  elif p == 0:\n    return jnp.log(jnp.inf)\n  elif p == -jnp.inf:\n    return -jnp.expm1(-jnp.inf)\n  elif p == jnp.inf:\n    return jnp.expm1(jnp.inf)\n  else:\n    return jnp.inf"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return jnp.inf\n  elif p == 0:\n    return jnp.inf\n  elif p == -jnp.inf:\n    return 1\n  elif p == jnp.inf:\n    return 0\n  else:\n    return jnp.abs(p - 1) / p * (jnp.exp(jnp.log(jnp.abs(p - 1)) * p) - 1)"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return np.inf\n  elif p == 0:\n    return 1\n  elif p == -jnp.inf:\n    return 0\n  elif p == jnp.inf:\n    return 0\n  else:\n    return jnp.abs(p - 1) / p * (jnp.e - 1)"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return np.inf\n  elif p == -jnp.inf:\n    return 0\n  elif p == jnp.inf:\n    return 1\n  elif p == 1:\n    return np.inf\n  elif p == -1:\n    return 0\n  elif p == 2:\n    return np.inf\n  elif p == -2:\n    return 0\n  else:\n    return np.inf"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return np.inf\n  elif p == 0:\n    return np.exp(np.inf)\n  elif p == -jnp.inf:\n    return -jnp.expm1(-np.inf)\n  elif p == jnp.inf:\n    return jnp.expm1(np.inf)\n  else:\n    p_safe = clip_finite_nograd(remove_zero(p))\n    return jnp.abs(p_safe - 1) * (jnp.expm1(np.inf) - 1)"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return 0\n  elif p == 0:\n    return np.inf\n  elif p == -jnp.inf:\n    return -1\n  elif p == jnp.inf:\n    return 1\n  else:\n    return jnp.abs(p - 1) / p * ((1 + 1 / jnp.abs(p - 1))**p - 1)"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 1\n  if p == -jnp.inf:\n    return 0\n  if p == jnp.inf:\n    return np.inf\n  if p < 0:\n    return -1\n  if p > 0:\n    return 1\n  return 0"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return np.inf\n  if p == 1:\n    return 1\n  if p == -jnp.inf:\n    return -1\n  if p == jnp.inf:\n    return 0\n  if p < 0:\n    return -1 / (p + 1)\n  if p > 0:\n    return 1 / (p - 1)"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return np.inf\n  if p == -jnp.inf:\n    return -np.inf\n  if p == jnp.inf:\n    return np.inf\n  if p == 1:\n    return np.inf\n  if p == -1:\n    return 0\n  return jnp.abs(p - 1) / p * ((1 + jnp.abs(p - 1))**p - 1)"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return jnp.inf\n  elif p == 0:\n    return 0\n  elif p == -jnp.inf:\n    return 1\n  elif p == jnp.inf:\n    return jnp.inf\n  elif p < 0:\n    return 1 / (p + 1)\n  else:\n    return (p + 1) / p"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return np.inf\n  if p == 0:\n    return 1\n  if p == -np.inf:\n    return 0\n  if p == np.inf:\n    return np.inf\n  if p > 0:\n    return (p - 1) / p\n  if p < 0:\n    return (p + 1) / p\n  return 1"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 0\n  elif p == 1:\n    return np.inf\n  elif p == -jnp.inf:\n    return -1\n  elif p == jnp.inf:\n    return 1\n  else:\n    return np.abs(p - 1) / p * (np.exp(np.log(np.abs(p - 1)) / p) - 1)"}
{"namespace": "math.power_ladder_max_output", "completion": "  return select(\n      [\n          (p == 1, 1),\n          (p == 0, 1),\n          (p == -jnp.inf, 1),\n          (p == jnp.inf, 1),\n      ],\n      select(\n          [\n              (p == 1, 1),\n              (p == 0, 1),\n              (p == -jnp.inf, -1),\n              (p == jnp.inf, 1),\n          ],\n          select(\n              [\n                  (p == 1, 1),\n                  (p == 0, 1),\n                  (p == -jnp.inf, 1),\n                  (p == jnp.inf, -1),\n              ],\n              select(\n                  [\n                      (p == 1, 1),\n                      (p == 0, 1),\n                      (p == -jnp.inf, 1),\n                      (p == jnp.inf, 1),\n                  ],\n                  select(\n                      [\n                          (p == 1, 1),\n                          (p == 0, 1),\n                          (p == -jnp.inf, 1),\n                          (p == jnp.inf, 1),\n                      ],\n                      jnp.nan,\n                  ),\n              ),\n          ),\n      ),\n  )"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return np.inf\n  elif p == -np.inf:\n    return 0\n  elif p == 1:\n    return np.inf\n  elif p == np.inf:\n    return 0\n  elif p > 0:\n    return (1 + (1 / p))**(1 / p)\n  else:\n    return (1 - (1 / p))**(1 / p)"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return max_val\n  if p == -jnp.inf:\n    return -max_val\n  if p == 0:\n    return 0\n  if p == jnp.inf:\n    return max_val\n  if p == -1:\n    return -max_val\n  if p == -2:\n    return -max_val\n  if p == 2:\n    return max_val\n  if p == 3:\n    return max_val\n  if p == -3:\n    return -max_val\n  if p == 4:\n    return max_val\n  if p == -4:\n    return -max_val\n  if p == 5:\n    return max_val\n  if p == -5:\n    return -max_val\n  if p == 6:\n    return max_val\n  if p == -6:\n    return -max_val\n  if p == 7:\n    return max_val\n  if p == -7:\n    return -max_val\n  if p == 8:\n    return max_val\n  if p == -8:\n    return -max_val\n  if p == 9:\n    return max_val\n  if p == -9:\n    return -max_val\n  if p == 10:\n    return max_val\n  if p == -10:\n    return -max_val\n  if p == 11:\n    return max_val\n  if p == -11:\n    return -max_val\n  if p == 12:\n    return max_val\n  if p == -12:\n    return -max_val\n  if p == 13:\n    return max_val\n  if p == -13:\n    return -max_val\n  if p == 14:\n    return max_val\n  if p == -14:\n    return -max_val\n  if p == 15:\n    return max_val\n  if p == -15:\n    return -max_val\n  if p == 16:\n    return max_val\n  if p == -16"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return np.float32(1)\n  elif p == 0:\n    return np.float32(np.log(max_val))\n  elif p == -np.inf:\n    return np.float32(-np.expm1(-np.log(max_val)))\n  elif p == np.inf:\n    return np.float32(np.expm1(np.log(max_val)))\n  else:\n    return np.float32(\n        np.abs(p - 1) / p * (np.expm1(np.log(max_val) / np.abs(p - 1)) - 1)\n    )"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return jnp.inf\n  elif p == -jnp.inf:\n    return -jnp.inf\n  elif p == jnp.inf:\n    return jnp.inf\n  elif p == 1:\n    return jnp.inf\n  elif p == -1:\n    return 0\n  else:\n    return jnp.abs(p - 1) / p * (1 - 1 / (jnp.abs(p - 1) * jnp.abs(p - 1)))"}
{"namespace": "math.power_ladder_max_output", "completion": "  # Check if p is equal to 1, 0, or -inf\n  if p == 1:\n    return np.inf\n  elif p == 0:\n    return np.exp(np.inf)\n  elif p == -np.inf:\n    return -np.exp(-np.inf)\n  else:\n    # Calculate the maximum value of the function for the given p\n    return np.exp(np.log(np.abs(p - 1)) / p)"}
{"namespace": "geopoly.generate_basis", "completion": "  "}
{"namespace": "geopoly.generate_basis", "completion": "  "}
{"namespace": "geopoly.generate_basis", "completion": "  \"\"\"\n  Generates a 3D basis by tessellating a geometric polyhedron, optionally removing symmetric basis columns to avoid redundant projections. The function supports tessellation of tetrahedron, icosahedron, or octahedron shapes and returns a matrix representing the 3D basis.\n\n  Input-Output Arguments\n  :param base_shape: string, the name of the starting polyhedron. It must be either 'tetrahedron', 'icosahedron', or 'octahedron'. This parameter determines the initial shape to be tessellated.\n  :param angular_tesselation: int, the number of times the polyhedron is tessellated. A value of 1 means no tessellation is applied. This parameter controls the complexity of the resulting basis.\n  :param remove_symmetries: bool, optional (default=True). If True, symmetric basis columns are removed to prevent redundant negative copies in projections. This parameter affects the uniqueness of the basis vectors.\n  :param eps: float, optional (default=1e-4). A small number used to determine when two vertices are considered symmetric. This parameter helps in identifying and removing symmetries with a tolerance defined by eps.\n  :return: A matrix with shape [3, n], where n is the number of vertices after tessellation (and symmetry removal, if applicable). This matrix represents the 3D basis generated from the specified polyhedron.\n  \"\"\"\n\n  \"\"\"\n  Generates a 3D basis by tessellating a geometric polyhedron, optionally removing symmetric basis columns to avoid redundant projections. The function supports tessellation of tetrahedron, icosahedron, or octahedron shapes and returns a matrix representing the 3D basis.\n\n  Input-Output Arguments\n  :param base_shape: string, the name of the starting polyhedron. It must be either 'tetrahedron', 'icosahedron', or 'octahedron'. This parameter determines the initial shape to be tessellated.\n  :param angular_tesselation: int, the number of times the polyhedron is tessellated. A value of "}
{"namespace": "geopoly.generate_basis", "completion": "  \"\"\"\n  Generates a 3D basis by tessellating a geometric polyhedron, optionally removing symmetric basis columns to avoid redundant projections. The function supports tessellation of tetrahedron, icosahedron, or octahedron shapes and returns a matrix representing the 3D basis.\n\n  Input-Output Arguments\n  :param base_shape: string, the name of the starting polyhedron. It must be either 'tetrahedron', 'icosahedron', or 'octahedron'. This parameter determines the initial shape to be tessellated.\n  :param angular_tesselation: int, the number of times the polyhedron is tessellated. A value of 1 means no tessellation is applied. This parameter controls the complexity of the resulting basis.\n  :param remove_symmetries: bool, optional (default=True). If True, symmetric basis columns are removed to prevent redundant negative copies in projections. This parameter affects the uniqueness of the basis vectors.\n  :param eps: float, optional (default=1e-4). A small number used to determine when two vertices are considered symmetric. This parameter helps in identifying and removing symmetries with a tolerance defined by eps.\n  :return: A matrix with shape [3, n], where n is the number of vertices after tessellation (and symmetry removal, if applicable). This matrix represents the 3D basis generated from the specified polyhedron.\n  \"\"\"\n  \"\"\"\n  Generates a 3D basis by tessellating a geometric polyhedron, optionally removing symmetric basis columns to avoid redundant projections. The function supports tessellation of tetrahedron, icosahedron, or octahedron shapes and returns a matrix representing the 3D basis.\n\n  Input-Output Arguments\n  :param base_shape: string, the name of the starting polyhedron. It must be either 'tetrahedron', 'icosahedron', or 'octahedron'. This parameter determines the initial shape to be tessellated.\n  :param angular_tesselation: int, the number of times the polyhedron is tessellated. A value of 1 means"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([\n        [1, 1, 1],\n        [1, -1, -1],\n        [-1, 1, -1],\n        [-1, -1, 1],\n    ])\n    base_faces = np.array([\n        [0, 1, 2],\n        [0, 1, 3],\n        [0, 2, 3],\n        [1, 2, 3],\n    ])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array([\n        [0, 0, 1],\n        [0, 1, 0],\n        [1, 0, 0],\n        [0, -1, 0],\n        [-1, 0, 0],\n        [0, 0, -1],\n        [0.5, 0.5, 0.5],\n        [0.5, -0.5, -0.5],\n        [-0.5, 0.5, -0.5],\n        [-0.5, -0.5, 0.5],\n        [0.5, 0.5, -0.5],\n        [0.5, -0.5, 0.5],\n        [-0.5, 0.5, 0.5],\n        [-0.5, -0.5, -0.5],\n    ])\n    base_faces = np.array([\n        [0, 1, 2],\n        [0, 2, 3],\n        [0, 3, 4],\n        [0, 4, 1],\n        [1, 4, 5],\n        [1, 5, 2],\n        [2, 5, 6],\n        [2, 6, 3],\n        [3, 6, 7],\n        [3, 7, 4],\n        [4, 7, 8],\n        [4, 8"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([\n        [1, 1, 1],\n        [-1, 1, -1],\n        [-1, -1, 1],\n        [1, -1, -1],\n    ])\n    base_faces = np.array([\n        [0, 1, 2],\n        [0, 1, 3],\n        [0, 2, 3],\n        [1, 2, 3],\n    ])\n  elif base_shape == 'icosahedron':\n    phi = (1 + 5**0.5) / 2\n    base_verts = np.array([\n        [0, 1, phi],\n        [0, 1, -phi],\n        [0, -1, phi],\n        [0, -1, -phi],\n        [1, phi, 0],\n        [1, -phi, 0],\n        [-1, phi, 0],\n        [-1, -phi, 0],\n        [phi, 0, 1],\n        [phi, 0, -1],\n        [-phi, 0, 1],\n        [-phi, 0, -1],\n    ])\n    base_faces = np.array([\n        [0, 1, 4],\n        [0, 4, 9],\n        [9, 4, 5],\n        [4, 8, 5],\n        [4, 1, 8],\n        [8, 1, 11],\n        [8, 11, 3],\n        [11, 2, 3],\n        [11, 7, 2],\n        [10, 7, 11],\n        [10, 6, 7],\n        [7, 6, 1],\n        [3, 2, 6],\n        [3, 6, 10],\n        [9, 5, 2],\n        [9, 2,"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape not in ['tetrahedron', 'icosahedron', 'octahedron']:\n    raise ValueError(f'base_shape {base_shape} is not supported')\n\n  if angular_tesselation < 1:\n    raise ValueError(f'angular_tesselation {angular_tesselation} must be >= 1')\n\n  if base_shape == 'tetrahedron':\n    base_verts = np.array([[1, 1, 1], [-1, -1, 1], [-1, 1, -1], [1, -1, -1]])\n    base_faces = np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array(\n        [\n            [0, 0, 1],\n            [0, 0.942809, 0.333333],\n            [0, -0.471405, -0.816497],\n            [0, -0.471405, 0.816497],\n            [0.816497, 0, 0.57735],\n            [-0.408249, 0.707107, 0.57735],\n            [-0.408249, -0.707107, 0.57735],\n            [0.408249, 0.707107, 0.57735],\n            [0.408249, -0.707107, 0.57735],\n            [0.816497, 0, -0.57735],\n            [-0.408249, 0.7071"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([\n        [1, 1, 1],\n        [-1, 1, 1],\n        [-1, -1, 1],\n        [1, -1, 1],\n        [1, 1, -1],\n        [-1, 1, -1],\n        [-1, -1, -1],\n        [1, -1, -1],\n    ])\n    base_faces = np.array([\n        [0, 1, 2],\n        [0, 1, 4],\n        [0, 2, 3],\n        [0, 3, 4],\n        [1, 2, 5],\n        [1, 4, 5],\n        [2, 3, 6],\n        [2, 5, 6],\n        [3, 4, 7],\n        [3, 6, 7],\n        [4, 5, 7],\n        [5, 6, 7],\n    ])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array([\n        [0, 0, 1],\n        [0.89442719, 0, 0.4472136],\n        [0.2763932, 0.42632564, 0.86410161],\n        [-0.72360679, 0.42632564, 0.52573111],\n        [-0.72360679, -0.42632564, 0.52573111],\n        [0.2763932, -0.42632564, 0.86410161],\n        [0.72360679, 0, -0.525"}
{"namespace": "geopoly.generate_basis", "completion": "  # Define the vertices and faces of the base shape.\n  if base_shape == 'tetrahedron':\n    base_verts = np.array([[1, 1, 1], [-1, 1, -1], [-1, -1, 1], [1, -1, -1]])\n    base_faces = np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n  elif base_shape == 'icosahedron':\n    phi = (1 + 5**0.5) / 2\n    base_verts = np.array(\n        [\n            [0, 1, phi],\n            [0, 1, -phi],\n            [0, -1, phi],\n            [0, -1, -phi],\n            [1, phi, 0],\n            [1, -phi, 0],\n            [-1, phi, 0],\n            [-1, -phi, 0],\n            [phi, 0, 1],\n            [-phi, 0, 1],\n            [phi, 0, -1],\n            [-phi, 0, -1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 1, 4],\n            [0, 4, 9],\n            [9, 4, 5],\n            [4, 8, 5],\n            [4, 1, 8],\n            [8, 1, 6],\n            [8, 6, 5],\n            [2, 11, 3],\n            [3, 11, 7],\n            [3, 7, 10],\n            [10, 7, 6],\n            [7, 11, 6],\n            [0, 2, 11],\n            [0, 5, 2],\n            [2, 5, 11],\n            [6, 11, "}
{"namespace": "geopoly.generate_basis", "completion": "  if not isinstance(angular_tesselation, int):\n    raise ValueError(f'angular_tesselation {angular_tesselation} must be an integer')\n  if angular_tesselation < 1:\n    raise ValueError(f'angular_tesselation {angular_tesselation} must be >= 1')\n\n  if base_shape == 'tetrahedron':\n    base_verts = np.array([[1, 1, 1], [-1, -1, 1], [-1, 1, -1], [1, -1, -1]])\n    base_faces = np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n  elif base_shape == 'icosahedron':\n    t = (1 + np.sqrt(5)) / 2\n    base_verts = np.array(\n        [\n            [-1, t, 0],\n            [1, t, 0],\n            [-1, -t, 0],\n            [1, -t, 0],\n            [0, -1, t],\n            [0, 1, t],\n            [0, -1, -t],\n            [0, 1, -t],\n            [t, 0, -1],\n            [t, 0, 1],\n            [-t, 0, -1],\n            [-t, 0, 1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 1, 4],\n            [0, 4, 9],\n            [9, 4, 5],\n            [4, 8, 5],\n            [4, 1, 8],\n            [8, 1, 11],\n            [8, 11, 3],\n            [11, 10, 3],\n            [8, 3, 2],\n            [11, 3, 7],\n            [7, 3, "}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([\n        [0, 0, 1],\n        [0, 1, 0],\n        [1, 0, 0],\n        [0, -1, 1],\n        [0, 1, -1],\n        [-1, 0, 1],\n        [1, 0, -1],\n        [-1, -1, 0],\n        [1, 1, 0],\n        [-1, 1, 1],\n        [1, -1, 1],\n        [-1, 1, -1],\n        [1, -1, -1],\n        [-1, -1, -1],\n    ])\n    base_faces = np.array([\n        [0, 1, 2],\n        [0, 2, 3],\n        [0, 3, 4],\n        [0, 4, 5],\n        [0, 5, 6],\n        [0, 6, 1],\n        [1, 6, 7],\n        [1, 7, 8],\n        [1, 8, 2],\n        [2, 8, 9],\n        [2, 9, 10],\n        [2, 10, 3],\n        [3, 10, 11],\n        [3, 11, 4],\n        [4, 11, 12],\n        [4, 12, 5],\n        [5, 12, 13],\n        [5, 13, 6],\n        [6, 13, 7],\n        [7, 13, 12],\n        [7, 12, 11],\n        [7, 11, 10],\n        [7, 10, 9],\n        [7, 9, 8],\n    ])\n  elif base_shape == 'icosahedron':\n    base_verts"}
{"namespace": "geopoly.generate_basis", "completion": "  # Check if base_shape is valid\n  if base_shape not in ['tetrahedron', 'icosahedron', 'octahedron']:\n    raise ValueError(f'base_shape {base_shape} is not valid')\n\n  # Generate the vertices and faces of the base shape\n  if base_shape == 'tetrahedron':\n    verts = np.array([[1, 1, 1], [-1, -1, 1], [-1, 1, -1], [1, -1, -1]])\n    faces = np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n  elif base_shape == 'icosahedron':\n    t = (1 + np.sqrt(5)) / 2\n    verts = np.array(\n        [\n            [-1, t, 0],\n            [1, t, 0],\n            [-1, -t, 0],\n            [1, -t, 0],\n            [0, -1, t],\n            [0, 1, t],\n            [0, -1, -t],\n            [0, 1, -t],\n            [t, 0, -1],\n            [t, 0, 1],\n            [-t, 0, -1],\n            [-t, 0, 1],\n        ]\n    )\n    faces = np.array(\n        [\n            [0, 1, 4],\n            [0, 4, 9],\n            [9, 4, 5],\n            [4, 8, 5],\n            [4, 1, 8],\n            [8, 1, 11],\n            [8, 11, 3],\n            [11, 10, 3],\n            [8, 3, 2],\n            [11, 3, 7],\n            [7, 10, 11],\n            [7, 6,"}
{"namespace": "geopoly.generate_basis", "completion": "  # Check if the input base_shape is valid\n  if base_shape not in ['tetrahedron', 'icosahedron', 'octahedron']:\n    raise ValueError(\n        f'base_shape {base_shape} must be either \"tetrahedron\", \"icosahedron\", or \"octahedron\"'\n    )\n\n  # Define the vertices and faces of the starting polyhedron\n  if base_shape == 'tetrahedron':\n    verts = np.array(\n        [\n            [1, 1, 1],\n            [-1, 1, -1],\n            [-1, -1, 1],\n            [1, -1, -1],\n        ]\n    )\n    faces = np.array(\n        [\n            [0, 1, 2],\n            [0, 2, 3],\n            [0, 3, 1],\n            [1, 3, 2],\n        ]\n    )\n  elif base_shape == 'icosahedron':\n    t = (1 + np.sqrt(5)) / 2\n    verts = np.array(\n        [\n            [-1, t, 0],\n            [1, t, 0],\n            [-1, -t, 0],\n            [1, -t, 0],\n            [0, -1, t],\n            [0, 1, t],\n            [0, -1, -t],\n            [0, 1, -t],\n            [t, 0, -1],\n            [t, 0, 1],\n            [-t, 0, -1],\n            [-t, 0, 1],\n        ]\n    )\n    faces = np.array(\n        [\n            [0, 1, 4],\n            [0, 4, 9],\n            [9, 4, 5],\n            [4, 8, 5],\n            [4, 1, 8],\n            [8, 1, 11],\n            [8, 11, "}
{"namespace": "geopoly.generate_basis", "completion": "  # Check if base_shape is valid\n  if base_shape not in ['tetrahedron', 'icosahedron', 'octahedron']:\n    raise ValueError(\n        f'base_shape {base_shape} must be either \"tetrahedron\", \"icosahedron\", or \"octahedron\".'\n    )\n\n  # Load the vertices and faces of the base shape\n  if base_shape == 'tetrahedron':\n    base_verts = np.array(\n        [\n            [1, 1, 1],\n            [-1, 1, -1],\n            [-1, -1, 1],\n            [1, -1, -1],\n        ]\n    )\n    base_faces = np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array(\n        [\n            [0, 0, 1],\n            [0, 0.942809, 0.333333],\n            [0, -0.471405, -0.816497],\n            [0, -0.471405, 0.816497],\n            [0.816497, 0, 0.57735],\n            [-0.408249, 0.707107, 0.57735],\n            [-0.408249, -0.707107, 0.57735],\n            [0.408249, 0.707107, 0.57735],\n            [0.408249, -0.707107, 0.57735],\n            [0.816497, 0, -0"}
{"namespace": "geopoly.generate_basis", "completion": "  # Define the vertex coordinates for each shape\n  base_verts = {\n      'tetrahedron': np.array([[1, 1, 1], [-1, 1, -1], [-1, -1, 1], [1, -1, -1]]),\n      'icosahedron': np.array(\n          [\n              [0, 0, 1],\n              [0, 0.942809042, -0.333333333],\n              [0, -0.471404521, -0.816496581],\n              [0.816496581, 0, -0.577350269],\n              [-0.471404521, 0, -0.816496581],\n              [0.471404521, 0.816496581, 0],\n              [0.471404521, -0.816496581, 0],\n              [-0.816496581, 0, 0.577350269],\n              [0.942809042, 0, -0.333333333],\n              [-0.942809042, 0, -0.333333333],\n              [0.333333333, 0.942809042, 0],\n              [0.333333333, -0.942809042, 0],\n              [-0.333333333, 0.942809042, 0],\n              [-0.333333333, -0.942809042, 0],\n             "}
{"namespace": "geopoly.generate_basis", "completion": "  # Set up the base shape\n  if base_shape == 'tetrahedron':\n    base_verts = np.array([[1, 1, 1], [-1, -1, 1], [-1, 1, -1], [1, -1, -1]])\n    base_faces = np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n  elif base_shape == 'icosahedron':\n    phi = (1 + 5**0.5) / 2\n    base_verts = np.array(\n        [\n            [0, 1, phi],\n            [0, 1, -phi],\n            [0, -1, phi],\n            [0, -1, -phi],\n            [1, phi, 0],\n            [1, -phi, 0],\n            [-1, phi, 0],\n            [-1, -phi, 0],\n            [phi, 0, 1],\n            [-phi, 0, 1],\n            [phi, 0, -1],\n            [-phi, 0, -1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 1, 4],\n            [0, 4, 9],\n            [9, 4, 5],\n            [4, 8, 5],\n            [4, 1, 8],\n            [8, 1, 6],\n            [8, 6, 5],\n            [1, 2, 6],\n            [2, 7, 6],\n            [2, 3, 7],\n            [3, 9, 7],\n            [3, 4, 9],\n            [3, 7, 11],\n            [7, 10, 11],\n            [10, 6, 11],\n            [6, 10, 2],\n            [10"}
{"namespace": "geopoly.generate_basis", "completion": "  # Check if the input is valid\n  if base_shape not in ['tetrahedron', 'icosahedron', 'octahedron']:\n    raise ValueError(f'base_shape {base_shape} must be either \"tetrahedron\", \"icosahedron\", or \"octahedron\"')\n  if not isinstance(angular_tesselation, int):\n    raise ValueError(f'angular_tesselation {angular_tesselation} must be an integer')\n  if angular_tesselation < 1:\n    raise ValueError(f'angular_tesselation {angular_tesselation} must be >= 1')\n  if not isinstance(remove_symmetries, bool):\n    raise ValueError(f'remove_symmetries {remove_symmetries} must be a boolean')\n  if not isinstance(eps, float):\n    raise ValueError(f'eps {eps} must be a float')\n\n  # Define the vertices and faces of the starting polyhedron\n  if base_shape == 'tetrahedron':\n    base_verts = np.array([[1, 1, 1], [-1, 1, -1], [-1, -1, 1], [1, -1, -1]])\n    base_faces = np.array([[0, 1, 2], [0, 2, 3], [0, 3, 1], [1, 3, 2]])\n  elif base_shape == 'icosahedron':\n    t = (1 + np.sqrt(5)) / 2\n    base_verts = np.array(\n        [\n            [-1, t, 0],\n            [1, t, 0],\n            [-1, -t, 0],\n            [1, -t, 0],\n            [0, -1, t],\n            [0, 1, t],\n            [0, -1, -t],\n            [0, 1, -t],\n            [t, 0, -1],\n            [t, 0, 1],\n            [-t,"}
{"namespace": "geopoly.generate_basis", "completion": "  # Check if the base_shape is valid\n  if base_shape not in ['tetrahedron', 'icosahedron', 'octahedron']:\n    raise ValueError(\n        f'base_shape {base_shape} must be either \"tetrahedron\", \"icosahedron\", or \"octahedron\"'\n    )\n\n  # Generate the base polyhedron\n  if base_shape == 'tetrahedron':\n    base_verts = np.array([[1, 1, 1], [-1, -1, 1], [-1, 1, -1], [1, -1, -1]])\n    base_faces = np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array(\n        [\n            [0, 0, 1],\n            [0.89442719, 0, -0.4472136],\n            [0.2763932, 0.4267767, -0.8618034],\n            [-0.72360679, 0.4267767, -0.52573111],\n            [-0.72360679, -0.4267767, -0.52573111],\n            [0.2763932, -0.4267767, -0.8618034],\n            [0.72360679, 0.4267767, 0.52573111],\n            [0.2763932, 0.4267767, 0.8618034],\n            [-0.89442719,"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([[1, 1, 1], [-1, -1, 1], [-1, 1, -1], [1, -1, -1]])\n    base_faces = np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array([\n        [0, 0, 1],\n        [0.95105652, 0, 0.30901699],\n        [0.58778525, 0.80901699, 0],\n        [-0.58778525, 0.80901699, 0],\n        [-0.95105652, 0, 0.30901699],\n        [0, 0.5, 0.8660254],\n        [0.5, 0.5, 0.70710678],\n        [0.80901699, 0.58778525, 0],\n        [0.30901699, 0.95105652, 0],\n        [-0.30901699, 0.95105652, 0],\n        [-0.80901699, 0.58778525, 0],\n        [-0.5, 0.5, 0.70710678],\n        [-0.80901699, -0.58778525, 0],\n        [-0.30901699, -0.95105652"}
{"namespace": "geopoly.generate_basis", "completion": "  # Validate the input\n  if base_shape not in ['tetrahedron', 'icosahedron', 'octahedron']:\n    raise ValueError(f'base_shape {base_shape} is not supported.')\n\n  if angular_tesselation < 1:\n    raise ValueError(f'angular_tesselation {angular_tesselation} must be >= 1')\n\n  # Define the vertices and faces of the starting polyhedron\n  if base_shape == 'tetrahedron':\n    verts = np.array([[1, 1, 1], [-1, -1, 1], [-1, 1, -1], [1, -1, -1]])\n    faces = np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n  elif base_shape == 'icosahedron':\n    t = (1 + np.sqrt(5)) / 2\n    verts = np.array(\n        [\n            [-1, t, 0],\n            [1, t, 0],\n            [-1, -t, 0],\n            [1, -t, 0],\n            [0, -1, t],\n            [0, 1, t],\n            [0, -1, -t],\n            [0, 1, -t],\n            [t, 0, -1],\n            [t, 0, 1],\n            [-t, 0, -1],\n            [-t, 0, 1],\n        ]\n    )\n    faces = np.array(\n        [\n            [0, 1, 4],\n            [0, 4, 9],\n            [9, 4, 5],\n            [4, 8, 5],\n            [4, 1, 8],\n            [8, 1, 11],\n            [8, 11, 3],\n            [11, 1, 2],\n            [11, 2,"}
{"namespace": "math.safe_log1p", "completion": "def safe_log1p(x):\n  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "def safe_log1p(x):\n  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "def safe_log1p(x):\n  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "def safe_log1p(x):\n  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "def safe_log1p(x):\n  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "def safe_log1p(x):\n  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "def safe_log1p(x):\n  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "def safe_log1p(x):\n  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": ""}
{"namespace": "math.safe_log1p", "completion": "def safe_log1p(x):\n  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "def safe_log1p(x):\n  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": ""}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  xp = jnp.abs(x)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x_max = minus_eps(power_ladder_max_output(p))\n  xp = override_gradient(jnp.clip(xp, -x_max, x_max), xp)  # Clip val, not grad.\n  y = select(\n      [\n          (p == 1, xp),\n          (p == 0, safe_log1p(xp)),\n          (p == -jnp.inf, -safe_expm1(-xp)),\n          (p == jnp.inf, safe_expm1(xp)),\n      ],\n      jnp.abs(p_safe - 1)\n      * (\n          ((safe_div(p_safe, jnp.abs(p_safe - 1)) * xp + 1)) ** (1 / p_safe) - 1\n      ),\n  )\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  if p == 1:\n    y = x\n  elif p == 0:\n    y = safe_log1p(x)\n  elif p == -jnp.inf:\n    y = -safe_log1p(-x)\n  elif p == jnp.inf:\n    y = safe_log1p(x)\n  else:\n    y = safe_div(p - 1, p) * (((safe_div(p, jnp.abs(p - 1)) * x + 1))**(1 / p) - 1)\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  xp = jnp.abs(x)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x_max = minus_eps(power_ladder_max_output(p))\n  xp = override_gradient(jnp.clip(xp, 0, x_max), xp)  # Clip val, not grad.\n  y = select(\n      [\n          (p == 1, xp),\n          (p == 0, safe_log1p(xp)),\n          (p == -jnp.inf, -safe_expm1(-xp)),\n          (p == jnp.inf, safe_expm1(xp)),\n      ],\n      jnp.abs(p_safe - 1)\n      * (\n          ((safe_div(p_safe, jnp.abs(p_safe - 1)) * xp + 1)) ** p_safe - 1\n      ),\n  )\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  xp = jnp.abs(x)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x_max = minus_eps(power_ladder_max_output(p))\n  xp = override_gradient(jnp.clip(xp, -x_max, x_max), xp)  # Clip val, not grad.\n  y = select(\n      [\n          (p == 1, xp),\n          (p == 0, safe_log1p(xp)),\n          (p == -jnp.inf, -safe_expm1(-xp)),\n          (p == jnp.inf, safe_expm1(xp)),\n      ],\n      jnp.abs(p_safe - 1)\n      * (\n          ((safe_div(p_safe, jnp.abs(p_safe - 1)) * xp + 1)) ** p_safe - 1\n      ),\n  )\n  if postmult is not None:\n    y *= postmult\n  return safe_sign(x) * y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  p_safe = clip_finite_nograd(remove_zero(p))\n  xp = jnp.abs(x)\n  x_max = minus_eps(power_ladder_max_output(p))\n  xp = override_gradient(jnp.clip(xp, -x_max, x_max), xp)  # Clip val, not grad.\n  y = select(\n      [\n          (p == 1, xp),\n          (p == 0, safe_log1p(xp)),\n          (p == -jnp.inf, -safe_expm1(-xp)),\n          (p == jnp.inf, safe_exp(xp)),\n      ],\n      jnp.abs(p_safe - 1)\n      * (\n          ((safe_div(p_safe, jnp.abs(p_safe - 1)) * xp + 1)) ** p_safe - 1\n      ),\n  )\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  y = select(\n      [\n          (p == 1, x),\n          (p == 0, safe_log1p(x)),\n          (p == -jnp.inf, -safe_expm1(-x)),\n          (p == jnp.inf, safe_expm1(x)),\n      ],\n      safe_div(p - 1, p) * (x**p - 1) + 1,\n  )\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  y = select(\n      [\n          (p == 1, x),\n          (p == 0, safe_log1p(x)),\n          (p == -jnp.inf, -safe_expm1(-x)),\n          (p == jnp.inf, safe_expm1(x)),\n      ],\n      jnp.abs(p)\n      * (\n          (safe_div(jnp.abs(p) + 1, jnp.abs(p)) * x + 1)**(1 / jnp.abs(p)) - 1\n      ),\n  )\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  if postmult is not None:\n    x *= postmult\n  xp = jnp.abs(x)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x_max = minus_eps(power_ladder_max_output(p))\n  xp = override_gradient(jnp.clip(xp, -x_max, x_max), xp)  # Clip val, not grad.\n  y = select(\n      [\n          (p == 1, xp),\n          (p == 0, safe_log1p(xp)),\n          (p == -jnp.inf, -safe_expm1(-xp)),\n          (p == jnp.inf, safe_exp(xp)),\n      ],\n      jnp.abs(p_safe - 1)\n      * (\n          ((safe_div(p_safe, jnp.abs(p_safe - 1)) * xp + 1)) ** p_safe - 1\n      ),\n  )\n  return safe_sign(x) * y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  xp = jnp.abs(x)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x_max = minus_eps(power_ladder_max_output(p))\n  xp = override_gradient(jnp.clip(xp, 0, x_max), xp)  # Clip val, not grad.\n  y = select(\n      [\n          (p == 1, xp),\n          (p == 0, safe_log1p(xp)),\n          (p == -jnp.inf, -safe_expm1(-xp)),\n          (p == jnp.inf, safe_expm1(xp)),\n      ],\n      jnp.abs(p_safe - 1)\n      * (\n          ((safe_div(p_safe, jnp.abs(p_safe - 1)) * xp + 1)) ** (1 / p_safe) - 1\n      ),\n  )\n  if postmult is not None:\n    y *= postmult\n  return jnp.sign(x) * y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  yp = jnp.abs(x)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  y_max = minus_eps(power_ladder_max_output(p))\n  yp = override_gradient(jnp.clip(yp, -y_max, y_max), yp)  # Clip val, not grad.\n  y = select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_log1p(yp)),\n          (p == -jnp.inf, -safe_expm1(-yp)),\n          (p == jnp.inf, safe_exp(yp)),\n      ],\n      jnp.abs(p_safe - 1)\n      * (\n          ((safe_div(p_safe, jnp.abs(p_safe - 1)) * yp + 1)) ** (1 / p_safe) - 1\n      ),\n  )\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  xp = jnp.abs(x)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x_max = minus_eps(power_ladder_max_output(p))\n  xp = override_gradient(jnp.clip(xp, -x_max, x_max), xp)  # Clip val, not grad.\n  y = select(\n      [\n          (p == 1, xp),\n          (p == 0, safe_log1p(xp)),\n          (p == -jnp.inf, -safe_expm1(-xp)),\n          (p == jnp.inf, safe_expm1(xp)),\n      ],\n      jnp.abs(p_safe - 1)\n      * (\n          (safe_div(p_safe, jnp.abs(p_safe - 1)) * xp + 1) ** p_safe - 1\n      ),\n  )\n  if postmult is not None:\n    y *= postmult\n  return safe_sign(x) * y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  xp = jnp.abs(x)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x_max = minus_eps(power_ladder_max_output(p))\n  xp = override_gradient(jnp.clip(xp, -x_max, x_max), xp)  # Clip val, not grad.\n  y = select(\n      [\n          (p == 1, xp),\n          (p == 0, safe_log1p(xp)),\n          (p == -jnp.inf, safe_expm1(-xp)),\n          (p == jnp.inf, safe_log1p(xp)),\n      ],\n      jnp.abs(p_safe - 1)\n      * (\n          ((safe_div(p_safe, jnp.abs(p_safe - 1)) * xp + 1)) ** (1 / p_safe) - 1\n      ),\n  )\n  if postmult is not None:\n    y *= postmult\n  return safe_sign(x) * y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  if p == -jnp.inf:\n    y = -safe_log(-x)\n  elif p == 0:\n    y = -safe_log1p(-x)\n  elif p == jnp.inf:\n    y = safe_log1p(x)\n  elif p == 1:\n    y = x\n  else:\n    y = safe_div(p - 1, p) * (safe_div(x, safe_div(p - 1, p))**p - 1)\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  if postmult is not None:\n    x *= postmult\n  p_safe = clip_finite_nograd(remove_zero(p))\n  xp = jnp.abs(x)\n  y = select(\n      [\n          (p == 1, x),\n          (p == 0, safe_log1p(xp)),\n          (p == -jnp.inf, -safe_expm1(-xp)),\n          (p == jnp.inf, safe_expm1(xp)),\n      ],\n      safe_div(p_safe - 1, p_safe) * xp**p_safe,\n  )\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  y = select(\n      [\n          (p == 1, x),\n          (p == 0, safe_log1p(x)),\n          (p == -jnp.inf, -safe_log1p(-x)),\n          (p == jnp.inf, safe_log1p(x)),\n      ],\n      jnp.sign(x)\n      * (\n          (jnp.abs(x) + 1) ** p\n          - jnp.where(p == 0, 1, jnp.where(p == -jnp.inf, 0, 1))\n      ),\n  )\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  if p == 1:\n    pass\n  elif p == 0:\n    x = safe_log1p(x)\n  elif p == -jnp.inf:\n    x = -safe_log1p(-x)\n  elif p == jnp.inf:\n    x = safe_log1p(x)\n  else:\n    p_safe = clip_finite_nograd(remove_zero(p))\n    x = select(\n        [\n            (p_safe >= 0, safe_div(p_safe - 1, p_safe) * x + 1),\n            (p_safe < 0, (safe_div(p_safe - 1, p_safe) * x + 1)**(-p_safe)),\n        ],\n        safe_div(p_safe - 1, p_safe) * x + 1,\n    )\n  if postmult is not None:\n    x *= postmult\n  return x"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  if p == 1:\n    return x\n  if p == 0:\n    return safe_log1p(x)\n  if p == -jnp.inf:\n    return -safe_log1p(-x)\n  if p == jnp.inf:\n    return safe_log1p(x)\n  y = jnp.abs(p - 1)\n  y *= jnp.sign(x)\n  y = safe_div(y, p)\n  y = jnp.abs(y)\n  y = (y + 1)**p\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  if p == -jnp.inf:\n    y = -safe_log(-x)\n  elif p == 0:\n    y = -safe_log1p(-x)\n  elif p == jnp.inf:\n    y = safe_log1p(x)\n  else:\n    p_safe = clip_finite_nograd(remove_zero(p))\n    y = jnp.where(\n        x >= 0,\n        select(\n            [\n                (p_safe == 1, x),\n                (p_safe == 0, safe_expm1(x)),\n                (p_safe == -jnp.inf, -safe_log(-x)),\n                (p_safe == jnp.inf, safe_log1p(x)),\n            ],\n            (p_safe - 1)\n            * (\n                (safe_div(p_safe, jnp.abs(p_safe - 1)) * x + 1)) ** (1 / p_safe)\n            - 1,\n        ),\n        select(\n            [\n                (p_safe == 1, -x),\n                (p_safe == 0, -safe_expm1(-x)),\n                (p_safe == -jnp.inf, safe_log(x)),\n                (p_safe == jnp.inf, -safe_log1p(-x)),\n            ],\n            (p_safe - 1)\n            * (\n                (safe_div(p_safe, jnp.abs(p_safe - 1)) * -x + 1)) ** (1 / p_safe)\n            - 1,\n        ),\n    )\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  if p == 1:\n    return x\n  if p == 0:\n    return safe_log1p(x)\n  if p == -jnp.inf:\n    return -safe_log1p(-x)\n  if p == jnp.inf:\n    return safe_log1p(x)\n  y = jnp.abs(p - 1)\n  y = safe_div(y, p)\n  y = jnp.maximum(y, tiny_val)\n  y = jnp.clip(y, 0, 1)\n  y = jnp.where(p > 1, x**y, -(-x)**y)\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  if p == 1:\n    return x\n  if p == 0:\n    return safe_log1p(x)\n  if p == -jnp.inf:\n    return -safe_log1p(-x)\n  if p == jnp.inf:\n    return safe_log1p(x)\n  y = jnp.abs(p)\n  y = jnp.where(y > 1, y - 1, y)\n  y = jnp.where(y < 1, y + 1, y)\n  x = jnp.where(x >= 0, x, -x)\n  x = jnp.where(x > 0, x, -x)\n  x = jnp.where(x == 0, 1, x)\n  y = jnp.where(y == 0, 1, y)\n  x = jnp.where(x > 0, x, 1 / x)\n  x = jnp.where(x == 0, 1, x)\n  x = jnp.where(x > 0, x**y, -(-x)**y)\n  x = jnp.where(y > 1, x**(1 / y), x**y)\n  x = jnp.where(p < 0, -x, x)\n  if postmult is not None:\n    x *= postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(y) * |p - 1|/p * ((|y|/|p-1| + 1)^p - 1)\n  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_log1p(yp)),\n          (p == -jnp.inf, -safe_expm1(-yp)),\n          (p == jnp.inf, safe_expm1(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(y) * |p - 1|/p * ((|y|/|p-1| + 1)^p - 1)\n  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_log1p(yp)),\n          (p == -jnp.inf, -safe_expm1(-yp)),\n          (p == jnp.inf, safe_expm1(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(y) * |p - 1|/p * ((|y|/|p-1| + 1)^p - 1)\n  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(-yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  "}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(y) * |p - 1|/p * ((|y|/|p-1| + 1)^p - 1)\n  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(-yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(y) * |p - 1|/p * ((|y|/|p-1| + 1)^p - 1)\n  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(-yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(y) * |p - 1|/p * ((|y|/|p-1| + 1)^p - 1)\n  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(y) * |p - 1|/p * ((|y|/|p-1| + 1)^(1/p) - 1)\n  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** (1 / p_safe) - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(y) * |p - 1|/p * ((|y|/|p-1| + 1)^p - 1)\n  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_log1p(-yp)),\n          (p == -jnp.inf, -safe_expm1(yp)),\n          (p == jnp.inf, safe_expm1(-yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(y) * |p - 1|/p * ((|y|/|p-1| + 1)^p - 1)\n  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_log1p(yp)),\n          (p == -jnp.inf, -safe_expm1(-yp)),\n          (p == jnp.inf, safe_expm1(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(y) * |p - 1|/p * ((|y|/|p-1| + 1)^(1/p) - 1)\n  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1)**(1 / p_safe) - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(y) * |p - 1|/p * ((|y|/|p-1| + 1)^(1/p) - 1)\n  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(-yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1)**(1 / p_safe) - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(y) * |p - 1|/p * ((|y|/|p-1| + 1)^p - 1)\n  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_log1p(-yp)),\n          (p == -jnp.inf, -safe_expm1(yp)),\n          (p == jnp.inf, safe_expm1(-yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y / premult\n\n  ys = jnp.abs(y)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = select(\n      [\n          (p == 1, ys),\n          (p == 0, safe_expm1(ys)),\n          (p == -jnp.inf, -safe_log1p(-ys)),\n          (p == jnp.inf, safe_log1p(ys)),\n      ],\n      clip_finite_nograd(\n          jnp.sign(y) * jnp.abs(p_safe - 1) / p_safe * (1 + ys)**(1 / p_safe)\n      ),\n  )\n\n  if postmult is not None:\n    x = x * postmult\n\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(y) * (|p - 1|/p)^(1/p) * (|y|/|p-1| + 1)^(1/p) - 1\n  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.sign(y) * (jnp.abs(p_safe - 1) / p_safe)**(1 / p_safe) *\n          (ys + 1)**(1 / p_safe) - 1\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y / premult\n  if p == 1:\n    x = y\n  elif p == 0:\n    x = jnp.expm1(y)\n  elif p == -jnp.inf:\n    x = -jnp.expm1(-y)\n  elif p == jnp.inf:\n    x = jnp.expm1(y)\n  else:\n    x = jnp.sign(y) * jnp.abs(p - 1) / p * (jnp.abs(y)**p - 1)\n  if postmult is not None:\n    x = x / postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(y) * |p - 1|/p * (|y|/|p-1| + 1)^(1/p) - 1\n  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_exp(yp) - 1),\n          (p == -jnp.inf, -safe_log(yp + 1)),\n          (p == jnp.inf, safe_log(yp + 1)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * (ys + 1)**(1 / p_safe) - 1\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y / premult\n  if p == 0:\n    x = jnp.expm1(y)\n  elif p == 1:\n    x = y\n  elif p == -jnp.inf:\n    x = -jnp.expm1(-y)\n  elif p == jnp.inf:\n    x = jnp.expm1(y)\n  else:\n    p_safe = clip_finite_nograd(remove_zero(p))\n    x = safe_sign(y) * jnp.abs(p_safe - 1) / p_safe * (\n        (jnp.abs(y) / (p_safe - 1) + 1)**p_safe - 1)\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y / premult\n\n  # Compute sign(y) * |p - 1|/p * ((|y|/|p-1| + 1)^(1/p) - 1)\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_exp(yp) - 1),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** (1 / p_safe) - 1)\n      ),\n  )\n\n  if postmult is not None:\n    x = x * postmult\n\n  return x\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(y) * |p - 1|/p * (1 - (1 - |y|/|p-1|)^p)\n  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, -safe_log1p(-yp)),\n          (p == -jnp.inf, safe_expm1(yp)),\n          (p == jnp.inf, -safe_expm1(-yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * (1 - (1 - yp / ys)**p_safe)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  # Calculate the learning rate decay based on the number of steps\n  if step < lr_delay_steps:\n    lr = lr_init * lr_delay_mult**(step / lr_delay_steps)\n  else:\n    lr = lr_init * (lr_final / lr_init)**((step - lr_delay_steps) / (max_steps - lr_delay_steps))\n\n  return lr"}
{"namespace": "math.learning_rate_decay", "completion": "  # Calculate the learning rate decay\n  lr_decay = lr_init - lr_final\n  lr_decay /= max_steps - lr_delay_steps\n\n  # Calculate the learning rate at the current step\n  if step < lr_delay_steps:\n    lr = lr_init * lr_delay_mult\n  else:\n    lr = lr_init - (step - lr_delay_steps) * lr_decay\n\n  return lr"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_init <= 0 or lr_final <= 0:\n    raise ValueError('Learning rates must be positive.')\n  if lr_delay_steps < 0:\n    raise ValueError('Delay must be non-negative.')\n  if lr_delay_mult <= 0:\n    raise ValueError('Delay multiplier must be positive.')\n  if lr_delay_steps > 0:\n    lr_init *= lr_delay_mult\n  return jnp.interp(\n      x=jnp.clip(step, 0, max_steps),\n      xp=[0, lr_delay_steps, max_steps],\n      fp=[lr_init, lr_init, lr_final],\n  )"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_init <= 0 or lr_final <= 0:\n    raise ValueError('Initial and final learning rates must be positive.')\n  if lr_delay_steps < 0:\n    raise ValueError('Delay steps must be non-negative.')\n  if lr_delay_mult <= 0:\n    raise ValueError('Delay multiplier must be positive.')\n\n  if step < lr_delay_steps:\n    lr = lr_init * lr_delay_mult\n  else:\n    lr = lr_init * lr_delay_mult * (\n        lr_final / lr_init)**(jnp.minimum(1, (step - lr_delay_steps) / max_steps))\n\n  return lr"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_init <= 0 or lr_final <= 0:\n    raise ValueError(f'Learning rates must be positive, but are {lr_init} and {lr_final}.')\n  if lr_delay_steps < 0:\n    raise ValueError(f'lr_delay_steps must be non-negative, but is {lr_delay_steps}.')\n  if lr_delay_mult <= 0:\n    raise ValueError(f'lr_delay_mult must be positive, but is {lr_delay_mult}.')\n  if step >= max_steps:\n    return lr_final\n  if step < lr_delay_steps:\n    return lr_init * lr_delay_mult\n  t = (step - lr_delay_steps) / (max_steps - lr_delay_steps)\n  return log_lerp(t, lr_init * lr_delay_mult, lr_final)"}
{"namespace": "math.learning_rate_decay", "completion": "  if step < 0:\n    raise ValueError(f'Step {step} must be non-negative.')\n  if step >= max_steps:\n    return lr_final\n  if lr_delay_steps > 0:\n    lr_init *= lr_delay_mult\n    step -= lr_delay_steps\n    max_steps -= lr_delay_steps\n  return lr_init * (lr_final / lr_init) ** (step / max_steps)\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if step < 0:\n    raise ValueError(f'Step {step} must be non-negative.')\n\n  if lr_init <= 0:\n    raise ValueError(f'Initial learning rate {lr_init} must be positive.')\n\n  if lr_final <= 0:\n    raise ValueError(f'Final learning rate {lr_final} must be positive.')\n\n  if max_steps <= 0:\n    raise ValueError(f'Max steps {max_steps} must be positive.')\n\n  if lr_delay_steps < 0:\n    raise ValueError(f'Delay steps {lr_delay_steps} must be non-negative.')\n\n  if lr_delay_mult <= 0:\n    raise ValueError(f'Delay multiplier {lr_delay_mult} must be positive.')\n\n  # Calculate the learning rate decay factor.\n  decay_factor = (lr_init / lr_final)**(1 / max_steps)\n\n  # Calculate the learning rate for the delay period.\n  if lr_delay_steps > 0:\n    lr_delay = lr_init / lr_delay_mult\n  else:\n    lr_delay = lr_init\n\n  # Calculate the learning rate for the non-delay period.\n  lr_non_delay = lr_final * decay_factor**(step - lr_delay_steps)\n\n  # Combine the learning rates for the delay and non-delay periods.\n  if lr_delay_steps > 0:\n    lr = jnp.where(step < lr_delay_steps, lr_delay, lr_non_delay)\n  else:\n    lr = lr_non_delay\n\n  return lr"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_init <= 0 or lr_final <= 0:\n    raise ValueError(f'Initial and final learning rates must be positive.')\n  if lr_delay_steps < 0:\n    raise ValueError(f'Learning rate delay steps must be non-negative.')\n  if lr_delay_mult <= 0:\n    raise ValueError(f'Learning rate delay multiplier must be positive.')\n\n  # Calculate the learning rate decay based on the step and total steps.\n  if step < lr_delay_steps:\n    lr = lr_init * lr_delay_mult\n  else:\n    lr = lr_init * lr_delay_mult * (1 - (step - lr_delay_steps) / max_steps)\n\n  # Interpolate between the initial and final learning rates.\n  lr = log_lerp(\n      (step - lr_delay_steps) / max_steps, lr_init, lr_final\n  )\n\n  return lr"}
{"namespace": "math.learning_rate_decay", "completion": "  if step < 0:\n    raise ValueError(f'Step must be >= 0, but is {step}.')\n  if lr_init <= 0:\n    raise ValueError(f'Initial learning rate must be > 0, but is {lr_init}.')\n  if lr_final <= 0:\n    raise ValueError(f'Final learning rate must be > 0, but is {lr_final}.')\n  if max_steps <= 0:\n    raise ValueError(f'Max steps must be > 0, but is {max_steps}.')\n  if lr_delay_steps < 0:\n    raise ValueError(\n        f'Learning rate delay steps must be >= 0, but is {lr_delay_steps}.'\n    )\n  if lr_delay_mult <= 0:\n    raise ValueError(\n        f'Learning rate delay multiplier must be > 0, but is {lr_delay_mult}.'\n    )\n\n  if step < lr_delay_steps:\n    lr = lr_init * lr_delay_mult\n  else:\n    lr = lr_init\n\n  lr_range = lr_init - lr_final\n  p = (step - lr_delay_steps) / (max_steps - lr_delay_steps)\n  lr -= lr_range * jnp.clip(p, 0, 1)\n  return lr"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_init <= 0 or lr_final <= 0:\n    raise ValueError(\n        f'Initial and final learning rates must be positive, but are {lr_init} and {lr_final}.'\n    )\n  if max_steps <= 0:\n    raise ValueError(\n        f'Maximum number of steps must be positive, but is {max_steps}.'\n    )\n  if lr_delay_steps < 0:\n    raise ValueError(\n        f'Delay steps must be non-negative, but is {lr_delay_steps}.'\n    )\n  if lr_delay_mult <= 0:\n    raise ValueError(\n        f'Delay multiplier must be positive, but is {lr_delay_mult}.'\n    )\n  if step < 0:\n    raise ValueError(f'Step must be non-negative, but is {step}.')\n\n  if lr_delay_steps == 0:\n    lr_delay_mult = 1\n\n  lr_delay_mult = jnp.clip(lr_delay_mult, 0, 1)\n  lr_init *= lr_delay_mult\n\n  if step < lr_delay_steps:\n    return lr_init\n  elif step >= max_steps:\n    return lr_final\n\n  step = jnp.clip(step - lr_delay_steps, 0, max_steps - lr_delay_steps)\n  lr = log_lerp(step / (max_steps - lr_delay_steps), lr_init, lr_final)\n  return lr"}
{"namespace": "math.learning_rate_decay", "completion": "  # Calculate the progress of the optimization based on the number of steps.\n  progress = jnp.clip(step / max_steps, 0, 1)\n\n  # Calculate the learning rate based on the progress.\n  if lr_delay_steps > 0:\n    # If a delay is specified, apply the delay multiplier to the initial learning rate.\n    lr_init_delayed = lr_init * lr_delay_mult\n    # Calculate the learning rate using a log-linear interpolation.\n    lr = log_lerp(progress, lr_init_delayed, lr_final)\n    # Apply the delay by scaling the learning rate down to 0.\n    lr = jnp.where(step < lr_delay_steps, lr, 0)\n  else:\n    # If no delay is specified, use a log-linear interpolation directly.\n    lr = log_lerp(progress, lr_init, lr_final)\n\n  return lr"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_init <= 0 or lr_final <= 0:\n    raise ValueError(f'Learning rate must be positive, got {lr_init} and {lr_final}.')\n  if max_steps <= 0:\n    raise ValueError(f'Number of steps must be positive, got {max_steps}.')\n  if lr_delay_steps < 0:\n    raise ValueError(f'Delay steps must be non-negative, got {lr_delay_steps}.')\n  if lr_delay_mult <= 0:\n    raise ValueError(f'Delay multiplier must be positive, got {lr_delay_mult}.')\n\n  # Calculate the learning rate decay based on the specified number of steps.\n  lr_decay = (lr_init / lr_final)**(1 / max_steps)\n\n  # If a delay is specified, apply the initial learning rate multiplier.\n  if lr_delay_steps > 0:\n    lr_init /= lr_delay_mult\n\n  # Calculate the learning rate for the current step.\n  if lr_delay_steps > 0:\n    lr = lr_init * lr_decay**max(0, step - lr_delay_steps)\n  else:\n    lr = lr_init * lr_decay**max(0, step)\n\n  return lr"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_init <= 0 or lr_final <= 0 or max_steps <= 0:\n    raise ValueError(\n        f'Learning rate decay parameters must be positive, but are {lr_init}, {lr_final}, and {max_steps}.'\n    )\n  if lr_delay_steps < 0 or lr_delay_mult <= 0:\n    raise ValueError(\n        f'Learning rate delay parameters must be non-negative, but are {lr_delay_steps} and {lr_delay_mult}.'\n    )\n  if lr_delay_steps > 0:\n    lr_init = lr_init * lr_delay_mult\n  if step < lr_delay_steps:\n    lr = lr_init * (step / lr_delay_steps)\n  else:\n    lr = lr_init * (1 - (step - lr_delay_steps) / (max_steps - lr_delay_steps))\n  lr = lr * lr_final / lr_init\n  return lr\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_init <= 0 or lr_final <= 0:\n    raise ValueError(f'Initial and final learning rates must be positive.')\n  if lr_delay_steps < 0:\n    raise ValueError(f'Learning rate delay steps must be non-negative.')\n  if lr_delay_mult <= 0:\n    raise ValueError(f'Learning rate delay multiplier must be positive.')\n\n  # Calculate the learning rate decay based on the step.\n  if lr_delay_steps > 0:\n    lr_delayed = lr_init * lr_delay_mult\n    lr = jnp.where(\n        step < lr_delay_steps,\n        lr_delayed,\n        jnp.where(\n            step >= max_steps,\n            lr_final,\n            jnp.exp(\n                jnp.clip(\n                    jnp.log(lr_init / lr_final) * (step - lr_delay_steps)\n                    / (max_steps - lr_delay_steps),\n                    -100,\n                    100,\n                )\n            )\n            * lr_delayed,\n        ),\n    )\n  else:\n    lr = jnp.where(\n        step >= max_steps,\n        lr_final,\n        jnp.exp(\n            jnp.clip(\n                jnp.log(lr_init / lr_final) * step / (max_steps - 1),\n                -100,\n                100,\n            )\n        )\n        * lr_init,\n    )\n\n  return lr"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_init <= 0 or lr_final <= 0 or max_steps <= 0 or lr_delay_steps < 0:\n    raise ValueError(\n        f'Inputs must be positive, but got: {lr_init}, {lr_final}, {max_steps}, '\n        f'{lr_delay_steps}.'\n    )\n\n  if lr_delay_steps == 0:\n    lr_delay_mult = 1\n\n  if lr_delay_mult < 1:\n    raise ValueError(\n        f'lr_delay_mult must be >= 1, but got {lr_delay_mult}.'\n    )\n\n  # Calculate the learning rate decay factor based on the current step.\n  decay_factor = jnp.minimum(\n      1, jnp.maximum(0, (max_steps - step) / (max_steps - lr_delay_steps))\n  )\n\n  # Apply the delay multiplier to the initial learning rate.\n  lr_init_delayed = lr_init * lr_delay_mult\n\n  # Calculate the learning rate based on the decay factor and the initial and final learning rates.\n  lr = jnp.where(\n      step < lr_delay_steps,\n      lr_init_delayed,\n      lr_init_delayed * decay_factor + lr_final * (1 - decay_factor),\n  )\n\n  return lr"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_init <= 0 or lr_final <= 0:\n    raise ValueError(\n        f'Learning rates must be positive, but are {lr_init} and {lr_final}.'\n    )\n  if lr_delay_steps < 0:\n    raise ValueError(\n        f'Learning rate delay steps must be non-negative, but are {lr_delay_steps}.'\n    )\n  if lr_delay_mult <= 0:\n    raise ValueError(\n        f'Learning rate delay multiplier must be positive, but is {lr_delay_mult}.'\n    )\n  if max_steps <= 0:\n    raise ValueError(\n        f'Maximum steps must be positive, but are {max_steps}.'\n    )\n  if step < 0:\n    raise ValueError(f'Step must be non-negative, but is {step}.')\n  if step > max_steps:\n    raise ValueError(\n        f'Step must be less than or equal to max steps, but are {step} and {max_steps}.'\n    )\n\n  if lr_delay_steps > 0:\n    # If a delay is specified, the initial learning rate is scaled down by a\n    # multiplier and gradually returns to the normal rate after the delay period.\n    lr_init *= lr_delay_mult\n    lr_final /= lr_delay_mult\n    lr_delay_steps = jnp.maximum(lr_delay_steps, max_steps)\n    lr_delay_mult = jnp.maximum(lr_delay_mult, 1)\n    lr_init = jnp.minimum(lr_init, lr_final)\n\n    # The learning rate is linearly interpolated between the initial and final\n    # learning rates over the delay period.\n    lr = jnp.where(\n        step < lr_delay_steps,\n        lr_init + (lr_final - lr_init) * step / lr_delay_steps,\n        lr_final,\n    )\n  else:\n    # If no delay"}
{"namespace": "math.learning_rate_decay", "completion": "  # Calculate the delay period as a fraction of the total number of steps.\n  lr_delay_period = max(lr_delay_steps, 1) / max_steps\n\n  # Calculate the learning rate decay factor based on the current step.\n  lr_decay_factor = jnp.minimum(\n      step / lr_delay_steps,\n      1 - (step - lr_delay_steps) / (max_steps - lr_delay_steps),\n  )\n\n  # Calculate the learning rate for the current step.\n  lr_current = lr_init * lr_delay_mult**lr_decay_factor\n\n  # If the learning rate is below the final learning rate, return the final\n  # learning rate. Otherwise, return the calculated learning rate.\n  return jnp.maximum(lr_current, lr_final)\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_init <= 0 or lr_final <= 0:\n    raise ValueError(f'Initial and final learning rates must be positive, but '\n                     f'were {lr_init} and {lr_final}.')\n  if max_steps <= 0:\n    raise ValueError(f'The maximum number of steps must be positive, but was '\n                     f'{max_steps}.')\n  if lr_delay_steps < 0:\n    raise ValueError(f'The number of delay steps must be non-negative, but was '\n                     f'{lr_delay_steps}.')\n  if lr_delay_mult <= 0:\n    raise ValueError(f'The learning rate delay multiplier must be positive, '\n                     f'but was {lr_delay_mult}.')\n\n  # The learning rate starts at `lr_init`, then linearly decays to `lr_final`.\n  # The delay period starts at `lr_init * lr_delay_mult`, then linearly\n  # increases to `lr_init`.\n  lr_delay_start = lr_init * lr_delay_mult\n  lr_delay_end = lr_init\n  lr_delay_steps = jnp.maximum(lr_delay_steps, 1)\n  lr_delay_mult = (lr_delay_end - lr_delay_start) / lr_delay_steps\n\n  # Calculate the learning rate based on the current step.\n  if lr_delay_steps == 0:\n    lr = jnp.where(\n        step >= max_steps, lr_final,\n        lr_init * (1 - jnp.minimum(step / max_steps, 1)))\n  else:\n    lr = jnp.where(\n        step < lr_delay_steps,\n        lr_delay_start + step * lr_delay_mult,\n        jnp.where(\n            step >= max_steps, lr_final,\n            lr_init * (1 - jnp.minimum((step - lr_delay_steps"}
{"namespace": "math.learning_rate_decay", "completion": "  # Calculate the learning rate decay factor based on the number of steps.\n  lr_decay_factor = jnp.minimum(1.0, jnp.maximum(0.0, 1 - step / max_steps))\n\n  # Apply the delay factor to the initial learning rate if specified.\n  lr_init_delayed = lr_init * jnp.maximum(1.0, lr_delay_mult)\n\n  # Apply the delay factor to the final learning rate if specified.\n  lr_final_delayed = lr_final * jnp.maximum(1.0, lr_delay_mult)\n\n  # If the delay period is specified, apply the initial learning rate decay.\n  if lr_delay_steps > 0:\n    lr_decay_factor = jnp.minimum(\n        1.0, jnp.maximum(0.0, 1 - (step - lr_delay_steps) / max_steps))\n\n  # Interpolate between the initial and final learning rates using the decay factor.\n  learning_rate = jnp.where(\n      step < lr_delay_steps, lr_init_delayed,\n      lr_init_delayed * (1 - lr_decay_factor) + lr_final_delayed * lr_decay_factor\n  )\n\n  return learning_rate"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    if lr_delay_mult <= 0:\n      raise ValueError(\n          f'`lr_delay_mult` must be positive, but is {lr_delay_mult}.'\n      )\n    if lr_delay_mult == 1:\n      raise ValueError(\n          '`lr_delay_mult` should be less than 1 if `lr_delay_steps` is '\n          f'nonzero, but is {lr_delay_mult}.'\n      )\n    lr_init *= lr_delay_mult\n    lr_delay_steps = jnp.maximum(0, lr_delay_steps)\n    lr_delay_mult = jnp.maximum(1, lr_delay_mult)\n    step = jnp.maximum(0, step - lr_delay_steps)\n\n  lr_delay_mult = jnp.maximum(1, lr_delay_mult)\n  lr_delay_steps = jnp.maximum(0, lr_delay_steps)\n  lr_init = jnp.maximum(0, lr_init)\n  lr_final = jnp.maximum(0, lr_final)\n  max_steps = jnp.maximum(1, max_steps)\n  step = jnp.maximum(0, step)\n\n  if step >= max_steps:\n    return lr_final\n\n  if lr_delay_steps == 0:\n    return jnp.interp(\n        step / max_steps,\n        [0, 1],\n        [lr_init, lr_final],\n    )\n\n  lr_init *= lr_delay_mult\n  lr_delay_mult = jnp.maximum(1, lr_delay_mult)\n  lr_delay_steps = jnp.maximum(0, lr_delay_steps)\n  lr_init = jnp.maximum(0, lr_init)\n  lr_final = jnp.maximum(0,"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1024,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=1.0,\n      far_hi=2.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  rng = random.PRNGKey(0)\n  return generate_random_rays(\n      rng,\n      1,\n      -1.0,\n      1.0,\n      0.0,\n      1.0,\n      0.0,\n      1.0,\n      0.0,\n      1.0,\n      include_exposure_idx,\n      include_exposure_values,\n      include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  rng = random.PRNGKey(0)\n  return generate_random_rays(\n      rng,\n      1,\n      -1.0,\n      1.0,\n      0.0,\n      1.0,\n      0.0,\n      1.0,\n      0.0,\n      1.0,\n      include_exposure_idx,\n      include_exposure_values,\n      include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=jax.random.PRNGKey(0),\n      n=1,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=1.0,\n      far_hi=10.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1024,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=1.0,\n      far_hi=2.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=1.0,\n      far_hi=10.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=1.0,\n      far_hi=2.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  rng = random.PRNGKey(0)\n  return generate_random_rays(\n      rng,\n      1,\n      -1.0,\n      1.0,\n      0.0,\n      1.0,\n      0.0,\n      1.0,\n      0.0,\n      1.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=jnp.zeros([], dtype=jnp.uint32),\n      n=1024,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=1.0,\n      far_hi=2.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=jax.random.PRNGKey(0),\n      n=1024,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=0.0,\n      far_hi=1.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=2.0,\n      far_hi=3.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  rng = random.PRNGKey(0)\n  return generate_random_rays(\n      rng,\n      1,\n      -1.0,\n      1.0,\n      0.0,\n      1.0,\n      0.0,\n      1.0,\n      0.0,\n      1.0,\n      include_exposure_idx,\n      include_exposure_values,\n      include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=jnp.zeros([], dtype=jnp.int32),\n      n=1,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=0.0,\n      far_hi=1.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  rng = random.PRNGKey(0)\n  return generate_random_rays(\n      rng,\n      1,\n      -1,\n      1,\n      0,\n      1,\n      0,\n      1,\n      0,\n      1,\n      include_exposure_idx,\n      include_exposure_values,\n      include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  rng = random.PRNGKey(0)\n  return generate_random_rays(\n      rng,\n      1024,\n      -1.0,\n      1.0,\n      0.0,\n      1.0,\n      0.0,\n      1.0,\n      0.0,\n      1.0,\n      include_exposure_idx,\n      include_exposure_values,\n      include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  random_rays = generate_random_rays(\n      rng=jnp.zeros([], dtype=jnp.uint32),\n      n=1,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=1.0,\n      far_hi=2.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )\n  return random_rays"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=jnp.zeros([]),\n      n=1,\n      origin_lo=0,\n      origin_hi=1,\n      radius_lo=0,\n      radius_hi=1,\n      near_lo=0,\n      near_hi=1,\n      far_lo=0,\n      far_hi=1,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=jnp.zeros([]),\n      n=1024,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=2.0,\n      far_hi=3.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=1.0,\n      far_hi=100.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=jnp.zeros([1, 3]),\n      n=100,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=1.0,\n      far_hi=2.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Must add half pixel offset to shoot rays through pixel centers.\n  def pix_to_dir(x, y):\n    return xnp.stack([x + 0.5, y + 0.5, xnp.ones_like(x)], axis=-1)\n\n  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          pix_to_dir(pix_x_int, pix_y_int),\n          pix_to_dir(pix_x_int + 1, pix_y_int),\n          pix_to_dir(pix_x_int, pix_y_int + 1),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  # Apply camera rotation matrices.\n  directions_stacked = mat_vec_mul(\n      camtoworlds[Ellipsis, :3, :3], camera_dirs_stacked\n  )\n  # Extract the offset rays.\n  directions, dx"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Must add half pixel offset to shoot rays through pixel centers.\n  def pix_to_dir(x, y):\n    return xnp.stack([x + 0.5, y + 0.5, xnp.ones_like(x)], axis=-1)\n\n  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          pix_to_dir(pix_x_int, pix_y_int),\n          pix_to_dir(pix_x_int + 1, pix_y_int),\n          pix_to_dir(pix_x_int, pix_y_int + 1),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  # Apply camera rotation matrices.\n  directions_stacked = mat_vec_mul(\n      camtoworlds[Ellipsis, :3, :3], camera_dirs_stacked\n  )\n  # Extract the offset rays.\n  directions, dx"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Must add half pixel offset to shoot rays through pixel centers.\n  def pix_to_dir(x, y):\n    return xnp.stack([x + 0.5, y + 0.5, xnp.ones_like(x)], axis=-1)\n\n  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          pix_to_dir(pix_x_int, pix_y_int),\n          pix_to_dir(pix_x_int + 1, pix_y_int),\n          pix_to_dir(pix_x_int, pix_y_int + 1),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  # Flip from OpenCV to OpenGL coordinate system.\n  camera_dirs_stacked = matmul(\n      camera_dirs_stacked, xnp.diag(xnp.array([1.0, -1.0, -1.0]))"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Convert points to homogeneous coordinates.\n  points = xnp.concatenate([points, xnp.ones_like(points[Ellipsis, :1])], axis=-1)\n\n  # Apply camera rotation matrices.\n  points = mat_vec_mul(camtoworlds[Ellipsis, :3, :3], points)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_distort(\n        points[Ellipsis, 0],\n        points[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    points = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(points[Ellipsis, :2]), axis=-1))\n    theta = xnp.minimum(xnp.pi, theta)\n\n    sin_theta_over_theta = xnp.sin(theta) / theta\n    points = xnp.stack(\n        [\n            points[Ellipsis, 0] * sin_theta_over_theta,\n            points[Ellipsis, 1] * sin_theta_over_theta,\n            xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  elif camtype == ProjectionType.PANORAMIC:\n    theta = points[Ellipsis, 0]\n    phi = points[Ellipsis, 1]\n    # Negation on y and z components accounts for expected OpenCV convention.\n    points = xnp.stack(\n        [\n            -xnp.sin(phi) * xnp.sin(theta),\n            -xnp.cos(phi),\n            -xnp.sin(phi) * xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  #"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Convert the points to camera coordinates.\n  points_cam = mat_vec_mul(camtoworlds[Ellipsis, :3, :3], points) + camtoworlds[Ellipsis, :3, -1]\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_distort(\n        points_cam[Ellipsis, 0],\n        points_cam[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    points_cam = xnp.stack([x, y, points_cam[Ellipsis, 2]], axis=-1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(points_cam[Ellipsis, :2]), axis=-1))\n    theta = xnp.minimum(xnp.pi, theta)\n\n    sin_theta_over_theta = xnp.sin(theta) / theta\n    points_cam = xnp.stack(\n        [\n            points_cam[Ellipsis, 0] * sin_theta_over_theta,\n            points_cam[Ellipsis, 1] * sin_theta_over_theta,\n            xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  elif camtype == ProjectionType.PANORAMIC:\n    theta = points_cam[Ellipsis, 0]\n    phi = points_cam[Ellipsis, 1]\n    # Negation on y and z components accounts for expected OpenCV convention.\n    points_cam = xnp.stack(\n        [\n            -xnp.sin(phi) * xnp.sin(theta),\n            -xnp.cos(phi),\n            -xnp.sin(phi) * xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  #"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Compute the 3D coordinates of the points in the camera coordinate system.\n  points_cam = mat_vec_mul(camtoworlds[Ellipsis, :3, :3], points) + camtoworlds[Ellipsis, :3, -1]\n\n  if distortion_params is not None:\n    # Apply lens distortion to the points in the camera coordinate system.\n    x, y = _radial_and_tangential_distort(\n        points_cam[Ellipsis, 0],\n        points_cam[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    points_cam = xnp.stack([x, y, points_cam[Ellipsis, 2]], axis=-1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(points_cam[Ellipsis, :2]), axis=-1))\n    theta = xnp.minimum(xnp.pi, theta)\n\n    sin_theta_over_theta = xnp.sin(theta) / theta\n    points_cam = xnp.stack(\n        [\n            points_cam[Ellipsis, 0] * sin_theta_over_theta,\n            points_cam[Ellipsis, 1] * sin_theta_over_theta,\n            xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  elif camtype == ProjectionType.PANORAMIC:\n    theta = points_cam[Ellipsis, 0]\n    phi = points_cam[Ellipsis, 1]\n    # Negation on y and z components accounts for expected OpenCV convention.\n    points_cam = xnp.stack(\n        [\n            -xnp.sin(phi) * xnp.sin(theta),\n            -xnp.cos(phi),\n            -xnp.sin(phi) * xnp.cos(theta),"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Convert points to homogenous coordinates.\n  points_hom = xnp.concatenate([points, xnp.ones_like(points[Ellipsis, :1])], axis=-1)\n\n  # Apply camera rotation matrices.\n  points_hom = mat_vec_mul(camtoworlds, points_hom)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_distort(\n        points_hom[Ellipsis, 0],\n        points_hom[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    points_hom = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(points_hom[Ellipsis, :2]), axis=-1))\n    theta = xnp.minimum(xnp.pi, theta)\n\n    sin_theta_over_theta = xnp.sin(theta) / theta\n    points_hom = xnp.stack(\n        [\n            points_hom[Ellipsis, 0] * sin_theta_over_theta,\n            points_hom[Ellipsis, 1] * sin_theta_over_theta,\n            xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  # Apply inverse intrinsic matrices.\n  points_hom = mat_vec_mul(pixtocams, points_hom)\n\n  # Flip from OpenCV to OpenGL coordinate system.\n  points_hom = matmul(\n      points_hom, xnp.diag(xnp.array([1.0, -1.0, -1.0]))\n  )\n\n  # Extract 2D image plane (x, y) coordinates.\n  coordinates = points_hom[Ellipsis, :2]\n  depth ="}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Convert points from world space to camera space.\n  points_cam = mat_vec_mul(camtoworlds[Ellipsis, :3, :3], points - camtoworlds[Ellipsis, :3, -1])\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_distort(\n        points_cam[Ellipsis, 0],\n        points_cam[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    points_cam = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(points_cam[Ellipsis, :2]), axis=-1))\n    theta = xnp.minimum(xnp.pi, theta)\n\n    sin_theta_over_theta = xnp.sin(theta) / theta\n    points_cam = xnp.stack(\n        [\n            points_cam[Ellipsis, 0] * sin_theta_over_theta,\n            points_cam[Ellipsis, 1] * sin_theta_over_theta,\n            xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  elif camtype == ProjectionType.PANORAMIC:\n    theta = points_cam[Ellipsis, 0]\n    phi = points_cam[Ellipsis, 1]\n    # Negation on y and z components accounts for expected OpenCV convention.\n    points_cam = xnp.stack(\n        [\n            -xnp.sin(phi) * xnp.sin(theta),\n            -xnp.cos(phi),\n            -xnp.sin(phi) * xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  coordinates = mat_vec_"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Convert points to homogeneous coordinates.\n  points_homogeneous = xnp.concatenate([points, xnp.ones_like(points[Ellipsis, :1])], axis=-1)\n\n  # Apply inverse intrinsic matrices.\n  camera_points = mat_vec_mul(pixtocams, points_homogeneous)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_distort(\n        camera_points[Ellipsis, 0],\n        camera_points[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_points = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_points[Ellipsis, :2]), axis=-1))\n    theta = xnp.minimum(xnp.pi, theta)\n\n    sin_theta_over_theta = xnp.sin(theta) / theta\n    camera_points = xnp.stack(\n        [\n            camera_points[Ellipsis, 0] * sin_theta_over_theta,\n            camera_points[Ellipsis, 1] * sin_theta_over_theta,\n            xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  # Apply camera rotation matrices.\n  world_points = mat_vec_mul(camtoworlds[Ellipsis, :3, :3], camera_points)\n\n  # Convert to pixel coordinates.\n  coordinates = xnp.stack(\n      [\n          world_points[Ellipsis, 0] / world_points[Ellipsis, 2],\n          world_points[Ellipsis, 1] / world_points[Ellipsis, 2],\n      ],\n      axis=-1,"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Flatten leading dimensions.\n  points = xnp.reshape(points, (-1, 3))\n  pixtocams = xnp.reshape(pixtocams, (-1, 3, 3))\n  camtoworlds = xnp.reshape(camtoworlds, (-1, 3, 4))\n\n  # Apply inverse intrinsic matrices.\n  camera_points = xnp.einsum('...ij,...j->...i', pixtocams, points)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_distort(\n        camera_points[Ellipsis, 0],\n        camera_points[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_points = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_points[Ellipsis, :2]), axis=-1))\n    theta = xnp.minimum(xnp.pi, theta)\n\n    sin_theta_over_theta = xnp.sin(theta) / theta\n    camera_points = xnp.stack(\n        [\n            camera_points[Ellipsis, 0] * sin_theta_over_theta,\n            camera_points[Ellipsis, 1] * sin_theta_over_theta,\n            xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  # Apply camera rotation matrices.\n  world_points = xnp.einsum('...ij,...j->...i', camtoworlds[Ellipsis, :3, :3], camera_points)\n\n  # Reshape to original shape.\n  return world_points.reshape(points.shape), camera_points.reshape(points.shape)"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Convert the points to homogeneous coordinates.\n  points_homogeneous = xnp.concatenate([points, xnp.ones_like(points[Ellipsis, :1])], axis=-1)\n\n  # Apply the camera rotation matrices.\n  points_camera = mat_vec_mul(camtoworlds[Ellipsis, :3, :3], points_homogeneous)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_distort(\n        points_camera[Ellipsis, 0],\n        points_camera[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    points_camera = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(points_camera[Ellipsis, :2]), axis=-1))\n    theta = xnp.minimum(xnp.pi, theta)\n\n    sin_theta_over_theta = xnp.sin(theta) / theta\n    points_camera = xnp.stack(\n        [\n            points_camera[Ellipsis, 0] * sin_theta_over_theta,\n            points_camera[Ellipsis, 1] * sin_theta_over_theta,\n            xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  # Apply the inverse intrinsic matrices.\n  points_image = mat_vec_mul(pixtocams, points_camera)\n\n  # Extract the 2D image plane (x, y) coordinates.\n  coordinates = points_image[Ellipsis, :2]\n\n  # Extract the depth values.\n  depth = points_image[Ellipsis, 2]\n\n  return coordinates, depth"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Convert points to homogenous coordinates.\n  points_hom = xnp.concatenate(\n      [points, xnp.ones_like(points[Ellipsis, :1])], axis=-1\n  )\n\n  # Apply inverse intrinsic matrices.\n  camera_points_hom = matmul(pixtocams, points_hom)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_distort(\n        camera_points_hom[Ellipsis, 0],\n        camera_points_hom[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_points_hom = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(\n        xnp.sum(xnp.square(camera_points_hom[Ellipsis, :2]), axis=-1)\n    )\n    theta = xnp.minimum(xnp.pi, theta)\n\n    sin_theta_over_theta = xnp.sin(theta) / theta\n    camera_points_hom = xnp.stack(\n        [\n            camera_points_hom[Ellipsis, 0] * sin_theta_over_theta,\n            camera_points_hom[Ellipsis, 1] * sin_theta_over_theta,\n            xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  # Apply camera rotation matrices.\n  world_points_hom = matmul(camtoworlds, camera_points_hom)\n\n  # Convert from homogenous coordinates.\n  coordinates = world_points_hom[Ellipsis, :2] / world_points_hom[Ellipsis, 2:]\n  depth = world_points_hom[Ellipsis, 2]\n\n  return coordinates, depth"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Convert the points to homogeneous coordinates.\n  points_homogeneous = xnp.concatenate(\n      [points, xnp.ones_like(points[Ellipsis, :1])], axis=-1\n  )\n\n  # Apply the camera extrinsics.\n  points_camera = matmul(camtoworlds, points_homogeneous[Ellipsis, None])[Ellipsis, 0]\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_distort(\n        points_camera[Ellipsis, 0],\n        points_camera[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    points_camera = xnp.stack([x, y, points_camera[Ellipsis, 2]], axis=-1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(points_camera[Ellipsis, :2]), axis=-1))\n    theta = xnp.minimum(xnp.pi, theta)\n\n    sin_theta_over_theta = xnp.sin(theta) / theta\n    points_camera = xnp.stack(\n        [\n            points_camera[Ellipsis, 0] * sin_theta_over_theta,\n            points_camera[Ellipsis, 1] * sin_theta_over_theta,\n            xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  # Apply the camera intrinsics.\n  points_image = matmul(pixtocams, points_camera[Ellipsis, None])[Ellipsis, 0]\n\n  # Normalize the x and y coordinates.\n  coordinates = points_image[Ellipsis, :2] / points_image[Ellipsis, 2:3]\n  depth"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Convert the input points to homogeneous coordinates.\n  points = xnp.concatenate([points, xnp.ones_like(points[Ellipsis, :1])], axis=-1)\n\n  # Apply the camera extrinsics.\n  points = mat_vec_mul(camtoworlds, points)\n\n  # Apply the camera intrinsics.\n  points = mat_vec_mul(pixtocams, points)\n\n  # If distortion parameters are provided, apply them.\n  if distortion_params is not None:\n    x, y = _radial_and_tangential_distort(\n        points[Ellipsis, 0],\n        points[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    points = xnp.stack([x, y, points[Ellipsis, 2]], axis=-1)\n\n  # Normalize the points to get the 2D pixel coordinates.\n  coordinates = points[Ellipsis, :2] / points[Ellipsis, 2:3]\n\n  # Return the 2D pixel coordinates and the depth values.\n  return coordinates, points[Ellipsis, 2:3]"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Convert points to camera coordinates.\n  points_cam = matmul(\n      camtoworlds[Ellipsis, :3, :3], points[Ellipsis, :3]\n  ) + camtoworlds[Ellipsis, :3, -1]\n\n  # Apply distortion.\n  if distortion_params is not None:\n    x, y = _radial_and_tangential_distort(\n        points_cam[Ellipsis, 0],\n        points_cam[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    points_cam = xnp.stack([x, y, points_cam[Ellipsis, 2]], axis=-1)\n\n  # Convert points to pixel coordinates.\n  coordinates = matmul(pixtocams[Ellipsis, :3, :3], points_cam[Ellipsis, :3])\n\n  # Apply camera projection model.\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(\n        xnp.sum(xnp.square(coordinates[Ellipsis, :2]), axis=-1)\n    )\n    theta = xnp.minimum(xnp.pi, theta)\n\n    sin_theta_over_theta = xnp.sin(theta) / theta\n    coordinates = xnp.stack(\n        [\n            coordinates[Ellipsis, 0] * sin_theta_over_theta,\n            coordinates[Ellipsis, 1] * sin_theta_over_theta,\n            xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  elif camtype == ProjectionType.PANORAMIC:\n    theta = coordinates[Ellipsis, 0]\n    phi = coordinates[Ellipsis, 1]\n    coordinates = xnp.stack(\n        [\n            -xnp.sin(phi) * xnp.sin(theta),\n            -xnp.cos(phi"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Convert points to homogeneous coordinates.\n  points = xnp.concatenate([points, xnp.ones_like(points[Ellipsis, :1])], axis=-1)\n\n  # Apply camera extrinsics.\n  points = mat_vec_mul(camtoworlds, points)\n\n  # Apply camera intrinsics.\n  points = mat_vec_mul(pixtocams, points)\n\n  # Normalize points to get 2D pixel coordinates.\n  points = points[Ellipsis, :2] / points[Ellipsis, 2, None]\n\n  # Apply distortion model, if specified.\n  if distortion_params is not None:\n    points = _radial_and_tangential_distort(\n        points[Ellipsis, 0],\n        points[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n\n  # Return the 2D pixel coordinates and depth values.\n  return points, points[Ellipsis, 2]"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Convert points to camera coordinates.\n  points = xnp.matmul(camtoworlds[Ellipsis, :3, :3], points[Ellipsis, :, :3].T)\n  points = points + camtoworlds[Ellipsis, :3, -1][Ellipsis, None, :]\n\n  # Apply distortion.\n  if distortion_params is not None:\n    x, y = _radial_and_tangential_distort(\n        points[Ellipsis, 0],\n        points[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    points = xnp.stack([x, y, points[Ellipsis, 2]], axis=-1)\n\n  # Apply intrinsic matrix.\n  points = xnp.matmul(pixtocams[Ellipsis, :3, :3], points[Ellipsis, :, :3].T)\n\n  # Convert to homogeneous coordinates.\n  points = points[Ellipsis, :, :3] / points[Ellipsis, :, 3:]\n\n  # Convert to pixel coordinates.\n  coordinates = points[Ellipsis, :2]\n  depth = points[Ellipsis, 2]\n\n  return coordinates, depth"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Compute the 2D pixel coordinates of the input points using the camera intrinsics.\n  points_homo = xnp.concatenate([points, xnp.ones_like(points[Ellipsis, :1])], axis=-1)\n  coordinates = mat_vec_mul(pixtocams, points_homo)\n\n  # Apply the distortion model to the pixel coordinates if provided.\n  if distortion_params is not None:\n    x, y = _radial_and_tangential_distort(\n        coordinates[Ellipsis, 0],\n        coordinates[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    coordinates = xnp.stack([x, y], axis=-1)\n\n  # Convert the pixel coordinates to 3D camera coordinates using the camera extrinsics.\n  coordinates_homo = xnp.concatenate([coordinates, xnp.ones_like(coordinates[Ellipsis, :1])], axis=-1)\n  camera_coordinates = mat_vec_mul(camtoworlds[:, :3, :3], coordinates_homo)\n\n  # Compute the depth values of the points in the camera coordinate system.\n  depth = xnp.linalg.norm(camera_coordinates, axis=-1)\n\n  return coordinates, depth"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Convert the points to homogeneous coordinates.\n  points = xnp.concatenate([points, xnp.ones_like(points[Ellipsis, :1])], axis=-1)\n\n  # Apply the camera intrinsics and extrinsics to get the points in camera space.\n  points_camera = xnp.matmul(camtoworlds, points[Ellipsis, :, :].T).T\n\n  if distortion_params is not None:\n    # Apply distortion model to correct for radial and tangential distortion.\n    x, y = _radial_and_tangential_distort(\n        points_camera[Ellipsis, 0],\n        points_camera[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    points_camera = xnp.stack([x, y, points_camera[Ellipsis, 2]], axis=-1)\n\n  # Project the points into pixel coordinates.\n  points_pixel = xnp.matmul(pixtocams, points_camera[Ellipsis, :, :].T).T\n\n  # Convert the pixel coordinates to 2D.\n  coordinates = points_pixel[Ellipsis, :2]\n\n  # Calculate the depth values.\n  depth = points_camera[Ellipsis, 2]\n\n  return coordinates, depth"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Compute the camera coordinates of the points by multiplying the inverse of the camera intrinsics matrix with the points\n  camera_coordinates = mat_vec_mul(pixtocams, points)\n\n  if distortion_params is not None:\n    # Apply distortion correction to the camera coordinates\n    x, y = _radial_and_tangential_distort(\n        camera_coordinates[Ellipsis, 0],\n        camera_coordinates[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_coordinates = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    # Apply fisheye distortion to the camera coordinates\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_coordinates[Ellipsis, :2]), axis=-1))\n    theta = xnp.minimum(xnp.pi, theta)\n\n    sin_theta_over_theta = xnp.sin(theta) / theta\n    camera_coordinates = xnp.stack(\n        [\n            camera_coordinates[Ellipsis, 0] * sin_theta_over_theta,\n            camera_coordinates[Ellipsis, 1] * sin_theta_over_theta,\n            xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  # Apply the camera extrinsics matrix to the camera coordinates to get the world coordinates\n  world_coordinates = mat_vec_mul(camtoworlds[Ellipsis, :3, :3], camera_coordinates)\n\n  # Compute the 2D pixel coordinates by multiplying the world coordinates with the inverse of the camera intrinsics matrix\n  coordinates = mat_vec_mul(pixtocams, world_coordinates)\n\n  # Compute the depth values by subtracting the camera coordinates from the world coordinates\n  depth = xnp.linalg.norm(world_"}
{"namespace": "rigid_body.exp_se3", "completion": "  w, v = screw_axis[Ellipsis, :3], screw_axis[Ellipsis, 3:]\n  theta = spin_math.safe_norm(w, min_norm=eps)\n  w = w / theta\n  W = skew(w)\n  R_taylor = jnp.eye(3) + W * jnp.sin(theta) + spin_math.matmul(W, W) * (\n      1.0 - jnp.cos(theta)\n  )\n  R = jnp.eye(3) + W * jnp.sin(theta) + spin_math.matmul(W, W) * (\n      1.0 - jnp.cos(theta)\n  )\n  X = rp_to_se3(R, v)\n  return X\n\n\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  w, v = screw_axis[Ellipsis, :3], screw_axis[Ellipsis, 3:]\n  theta = spin_math.safe_norm(w, min_norm=eps)\n  W = skew(w / theta)\n  R_taylor = jnp.eye(3) + W\n  R = (\n      jnp.eye(3)\n      + jnp.sin(theta) * W\n      + (1.0 - jnp.cos(theta)) * spin_math.matmul(W, W)\n  )\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  R_taylor = jnp.where(theta**2 > eps**2, R, R_taylor)\n  return rp_to_se3(R_taylor, v)"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[Ellipsis, :3]\n  v = screw_axis[Ellipsis, 3:]\n  theta = spin_math.safe_norm(w, min_norm=eps)\n  W = skew(w / theta)\n\n  R_taylor = jnp.eye(3) + W\n  R = jnp.eye(3) + jnp.sin(theta) * W + (1.0 - jnp.cos(theta)) * spin_math.matmul(\n      W, W\n  )\n\n  X_taylor = rp_to_se3(R_taylor, v)\n  X = rp_to_se3(R, v)\n\n  return jnp.where(theta**2 > eps**2, X, X_taylor)\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  w, v = screw_axis[Ellipsis, :3], screw_axis[Ellipsis, 3:]\n  theta = spin_math.safe_norm(w, axis=-1)\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  R_taylor = jnp.eye(3) + skew(w)\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta**2 > eps**2, w, 0.0)\n  theta_safe = jnp.where(theta**2 > eps**2, theta, 1.0)\n  axis = w_safe / theta_safe\n  W = skew(axis)\n  R = (\n      jnp.eye(3)\n      + jnp.sin(theta_safe) * W\n      + (1.0 - jnp.cos(theta_safe)) * spin_math.matmul(W, W)\n  )\n\n  X = jnp.eye(4)\n  X = X.at[Ellipsis, :3, :3].set(R)\n  X = X.at[Ellipsis, :3, 3].set(v)\n  return X"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[Ellipsis, :3]\n  v = screw_axis[Ellipsis, 3:]\n  theta = spin_math.safe_norm(w, axis=-1, min_norm=eps)\n  W = skew(w / theta)\n\n  R_taylor = jnp.eye(3) + W\n  R = (\n      jnp.eye(3)\n      + (1.0 - jnp.cos(theta)) / theta**2 * W\n      + (theta - jnp.sin(theta)) / theta**3 * spin_math.matmul(W, W)\n  )\n\n  X = jnp.eye(4)\n  X = X.at[:3, :3].set(R)\n  X = X.at[:3, 3].set(v)\n\n  return X\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[Ellipsis, :3])\n  W = skew(screw_axis[Ellipsis, :3] / theta)\n  R_taylor = jnp.eye(3) + W\n  R = (\n      jnp.eye(3)\n      + jnp.sin(theta) * W\n      + (1.0 - jnp.cos(theta)) * spin_math.matmul(W, W)\n  )\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  return jnp.where(theta**2 > eps**2, R, R_taylor)\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  w, v = screw_axis[Ellipsis, :3], screw_axis[Ellipsis, 3:]\n  theta_squared = jnp.sum(w**2, axis=-1)\n  theta = spin_math.safe_sqrt(theta_squared)\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  R_taylor = jnp.eye(3) + skew(w)\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta_squared > eps**2, w, 0.0)\n  theta_safe = jnp.where(theta_squared > eps**2, theta, 1.0)\n  W = skew(w_safe / theta_safe)\n  R = (\n      jnp.eye(3)\n      + jnp.sin(theta_safe) * W\n      + (1.0 - jnp.cos(theta_safe)) * spin_math.matmul(W, W)\n  )\n  p = spin_math.matmul(\n      spin_math.matmul(jnp.sin(theta_safe) / theta_safe, W), v[Ellipsis, jnp.newaxis]\n  ).squeeze(-1)\n  X = rp_to_se3(R, p)\n  return jnp.where(theta_squared > eps**2, X, R_taylor)\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[Ellipsis, :3]\n  v = screw_axis[Ellipsis, 3:]\n  theta_squared = jnp.sum(w**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n  W = skew(w / theta)\n\n  G_inv1 = jnp.eye(3)\n  G_inv2 = theta * -W / 2.0\n  G_inv3 = (1.0 - 0.5 * theta / jnp.tan(theta / 2.0)) * spin_math.matmul(W, W)\n  G_inv = G_inv1 + G_inv2 + G_inv3\n\n  R_taylor = jnp.eye(3) + W\n  R = jnp.eye(3) + jnp.sin(theta) * W + (1.0 - jnp.cos(theta)) * spin_math.matmul(\n      W, W\n  )\n\n  p = spin_math.matmul(G_inv, v[Ellipsis, jnp.newaxis]).squeeze(-1)\n  # If theta = 0 then the transformation is a pure translation and p = v.\n  # This avoids using the numerically unstable G matrix when theta is near zero.\n  p = jnp.where(theta_squared > eps**2, p, v)\n  X = rp_to_se3(R, p)\n  return jnp.where(theta_squared > eps**2, X, R_taylor)\n\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[Ellipsis, :3]\n  v = screw_axis[Ellipsis, 3:]\n  theta = spin_math.safe_norm(w, axis=-1)\n  W = skew(w / theta)\n\n  R_taylor = jnp.eye(3) + W * jnp.sin(theta) + spin_math.matmul(W, W) * (\n      1.0 - jnp.cos(theta)\n  )\n  R = jnp.eye(3) + W * jnp.sin(theta) + spin_math.matmul(W, W) * (\n      1.0 - jnp.cos(theta)\n  )\n  p = spin_math.matmul(W, v) * theta\n\n  X = rp_to_se3(R, p)\n  return X\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[Ellipsis, :3]\n  v = screw_axis[Ellipsis, 3:]\n  theta = spin_math.safe_norm(w, axis=-1)\n  W = skew(w / theta)\n\n  G_inv1 = jnp.eye(3)\n  G_inv2 = theta * -W / 2.0\n  G_inv3 = (1.0 - 0.5 * theta / jnp.tan(theta / 2.0)) * spin_math.matmul(W, W)\n  G_inv = G_inv1 + G_inv2 + G_inv3\n\n  R_taylor = jnp.eye(3) + W\n  R = jnp.eye(3) + jnp.sin(theta) * W + (1.0 - jnp.cos(theta)) * spin_math.matmul(\n      W, W\n  )\n  X = jnp.block([[R, v[Ellipsis, jnp.newaxis]], [jnp.array([[0.0, 0.0, 0.0, 1.0]])]])\n  X_taylor = jnp.block(\n      [[R_taylor, v[Ellipsis, jnp.newaxis]], [jnp.array([[0.0, 0.0, 0.0, 1.0]])]]\n  )\n\n  return jnp.where(theta**2 > eps**2, X, X_taylor)\n\n\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = spin_math.safe_norm(screw_axis, min_norm=eps)\n  w = screw_axis / theta\n  W = skew(w)\n\n  R_taylor = jnp.eye(3) + W * jnp.sin(theta) / theta + (\n      1.0 - jnp.cos(theta)\n  ) * W * W / theta\n\n  R = (\n      jnp.eye(3)\n      + W * (1.0 - jnp.cos(theta)) / theta\n      + W * W * (theta - jnp.sin(theta)) / theta**2\n  )\n\n  return jnp.where(theta**2 > eps**2, R, R_taylor)\n\n\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  w, v = screw_axis[Ellipsis, :3], screw_axis[Ellipsis, 3:]\n  theta_squared = jnp.sum(w**2, axis=-1)\n  theta = spin_math.safe_sqrt(theta_squared, value_at_zero=eps)\n  W = skew(w / theta)\n\n  R_taylor = jnp.eye(3) + W\n  R = jnp.eye(3) + jnp.sin(theta) * W + (1.0 - jnp.cos(theta)) * spin_math.matmul(W, W)\n\n  p = spin_math.matmul(spin_math.matmul(jnp.sin(theta) / theta, W), v[Ellipsis, jnp.newaxis])\n  p = jnp.squeeze(p, axis=-1)\n\n  X = rp_to_se3(R, p)\n  return X\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis[:3])\n  w = jnp.zeros_like(screw_axis[:3])\n  if theta > eps:\n    w = screw_axis[:3] / theta\n  theta = jnp.where(theta > eps, theta, 1.0)\n  W = skew(w)\n  R_taylor = jnp.eye(3) + W\n  R = jnp.eye(3) + jnp.sin(theta) * W + (1.0 - jnp.cos(theta)) * W @ W\n\n  p = jnp.zeros_like(screw_axis[3:])\n  if theta > eps:\n    p = screw_axis[3:] / theta\n  X_taylor = rp_to_se3(R_taylor, p)\n  X = rp_to_se3(R, p)\n  return jnp.where(theta**2 > eps**2, X, X_taylor)\n\n\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  # Extract the rotation axis and angle from the screw axis.\n  w, v = screw_axis[Ellipsis, :3], screw_axis[Ellipsis, 3:]\n  theta = spin_math.safe_norm(w, axis=-1)\n\n  # If theta is zero, the rotation is a pure translation.\n  R_taylor = jnp.eye(3)\n  W = skew(w)\n  R = jnp.eye(3) + jnp.sin(theta) / theta * W + (\n      1.0 - jnp.cos(theta)\n  ) / theta**2 * spin_math.matmul(W, W)\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta**2 > eps**2, w, 0.0)\n  theta_safe = jnp.where(theta**2 > eps**2, theta, 1.0)\n  W_safe = jnp.where(theta**2 > eps**2, W, jnp.eye(3))\n  R = jnp.where(theta**2 > eps**2, R, R_taylor)\n  p = spin_math.matmul(W_safe, v[Ellipsis, jnp.newaxis]) / theta_safe\n  p = jnp.where(theta**2 > eps**2, p.squeeze(-1), v)\n  return rp_to_se3(R, p)\n\n\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  # Extract the axis and angle of the screw axis\n  w = screw_axis[Ellipsis, :3]\n  v = screw_axis[Ellipsis, 3:]\n\n  # Compute the rotation matrix and translation vector\n  theta_squared = jnp.sum(w**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n  W = skew(w / theta)\n\n  R_taylor = jnp.eye(3) + W\n  R = (\n      jnp.eye(3)\n      + jnp.sin(theta) * W\n      + (1.0 - jnp.cos(theta)) * spin_math.matmul(W, W)\n  )\n  p = jnp.where(theta_squared > eps**2, v, v / theta)\n\n  # Construct the homogeneous transformation matrix\n  X = jnp.eye(4)\n  X = X.at[Ellipsis, :3, :3].set(R)\n  X = X.at[Ellipsis, :3, 3].set(p)\n\n  return X\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  w, v = screw_axis[Ellipsis, :3], screw_axis[Ellipsis, 3:]\n  theta = spin_math.safe_norm(w, axis=-1)\n  theta_squared = theta**2\n  theta_cubed = theta_squared * theta\n\n  w_hat = skew(w)\n  W = w_hat\n  W2 = spin_math.matmul(w_hat, w_hat)\n  R_taylor = jnp.eye(3) + theta_squared / 6.0 * W + theta_cubed / 120.0 * W2\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  w_safe = jnp.where(theta_squared > eps**2, w, 0.0)\n  theta_safe = jnp.where(theta_squared > eps**2, theta, 1.0)\n  W_safe = skew(w_safe / theta_safe)\n  R = (\n      jnp.eye(3)\n      + (1.0 - jnp.cos(theta_safe)) / theta_squared * W_safe\n      + (theta_safe - jnp.sin(theta_safe)) / theta_cubed * spin_math.matmul(W_safe, W_safe)\n  )\n\n  p = jnp.where(theta_squared > eps**2, v, v)\n  X = rp_to_se3(R, p)\n  return X\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = jnp.linalg.norm(screw_axis, axis=-1)\n  axis = screw_axis / theta\n  W = skew(axis)\n\n  R_taylor = jnp.eye(3) + W * theta\n  R = jnp.eye(3) + jnp.sin(theta) * W + (1.0 - jnp.cos(theta)) * spin_math.matmul(\n      W, W\n  )\n\n  R = jnp.where(theta**2 > eps**2, R, R_taylor)\n\n  return rp_to_se3(R, axis * theta)\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  w, v = screw_axis[Ellipsis, :3], screw_axis[Ellipsis, 3:]\n  theta = spin_math.safe_norm(w, min_norm=eps)\n  theta_squared = theta**2\n  W = skew(w)\n\n  R_taylor = jnp.eye(3) + W * jnp.sin(theta) / theta_squared + (\n      1.0 - jnp.cos(theta)\n  ) * spin_math.matmul(W, W) / theta_squared\n\n  G_inv1 = jnp.eye(3)\n  G_inv2 = (\n      -W * jnp.sin(theta) / theta_squared\n      + (1.0 - jnp.cos(theta)) * W * W / theta_squared\n  )\n  G_inv3 = (\n      (1.0 - jnp.cos(theta)) * (1.0 - jnp.cos(theta)) / theta_squared\n      + jnp.sin(theta) * W * W / theta_squared\n  )\n  G_inv = G_inv1 + G_inv2 + G_inv3\n\n  R = jnp.eye(3) + jnp.sin(theta) * W / theta + (1.0 - jnp.cos(theta)) * spin_math.matmul(\n      W, W\n  ) / theta\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  v_taylor = jnp.where(theta_squared > eps**2, v, 0.0)\n  v = spin_math.matmul(G_inv, v_taylor[Ellipsis, jnp.newaxis]).squeeze(-1)\n  p = v * jnp.sin(theta) / theta + (1.0 - jnp.cos(theta)) * spin_math.matmul(\n      W, v_taylor[Ellipsis, jnp.newaxis]\n  )."}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = spin_math.safe_norm(screw_axis, min_norm=eps, axis=-1)\n  W = skew(screw_axis / theta)\n\n  R_taylor = jnp.eye(3) + W\n\n  R = jnp.eye(3) + jnp.sin(theta) * W + (1.0 - jnp.cos(theta)) * spin_math.matmul(\n      W, W\n  )\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  # This is the same as the formula in the book, but with the constant\n  # term removed.\n  # R = jnp.eye(3) + W\n\n  # Prevent bad gradients from propagating back when theta is small.\n  W_safe = jnp.where(theta**2 > eps**2, W, 0.0)\n  theta_safe = jnp.where(theta**2 > eps**2, theta, 1.0)\n  v = spin_math.matmul(\n      jnp.eye(3) - W_safe / 2.0 + (1.0 - theta_safe / 2.0) * spin_math.matmul(\n          W_safe, W_safe\n      ),\n      screw_axis[Ellipsis, 3:],\n  )\n  X = rp_to_se3(R, v)\n\n  return jnp.where(theta**2 > eps**2, X, R_taylor)\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  w, v = screw_axis[Ellipsis, :3], screw_axis[Ellipsis, 3:]\n  theta = jnp.linalg.norm(w, axis=-1)\n  theta_squared = theta**2\n  W = skew(w / theta)\n\n  R_taylor = jnp.eye(3) + W\n  R = (\n      jnp.eye(3)\n      + (jnp.sin(theta) / theta) * W\n      + ((1.0 - jnp.cos(theta)) / theta_squared) * spin_math.matmul(W, W)\n  )\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  R = jnp.where(theta_squared > eps**2, R, R_taylor)\n\n  # We need to check for theta = 0, because otherwise we get a divide by zero.\n  # We can't use `jnp.where` to do this, because it would prevent gradients\n  # from propagating back.\n  theta = jnp.where(theta_squared > eps**2, theta, 1.0)\n  v = v / theta\n\n  return rp_to_se3(R, v)\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  axis_angle = jnp.reshape(axis_angle, (3))\n  theta = jnp.linalg.norm(axis_angle)\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  R_taylor = jnp.eye(3) + skew(axis_angle)\n\n  # Prevent bad gradients from propagating back when theta is small.\n  axis_angle_safe = jnp.where(theta**2 > eps**2, axis_angle, 0.0)\n  theta_safe = jnp.where(theta**2 > eps**2, theta, 1.0)\n  W = skew(axis_angle_safe)\n  R = (\n      jnp.eye(3)\n      + jnp.sin(theta_safe) * W\n      + (1.0 - jnp.cos(theta_safe)) * spin_math.matmul(W, W)\n  )\n\n  return jnp.where(theta**2 > eps**2, R, R_taylor)"}
{"namespace": "rigid_body.exp_so3", "completion": "  axis_angle = jnp.reshape(axis_angle, (3))\n  theta = _safe_sqrt(jnp.sum(axis_angle**2, axis=-1))\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  R_taylor = jnp.eye(3) + skew(axis_angle)\n\n  # Prevent bad gradients from propagating back when theta is small.\n  axis_angle_safe = jnp.where(theta**2 > eps**2, axis_angle, 0.0)\n  theta_safe = jnp.where(theta**2 > eps**2, theta, 1.0)\n  W = skew(axis_angle_safe)\n  R = (\n      jnp.eye(3)\n      + jnp.sin(theta_safe) * W\n      + (1.0 - jnp.cos(theta_safe)) * spin_math.matmul(W, W)\n  )\n\n  return jnp.where(theta**2 > eps**2, R, R_taylor)"}
{"namespace": "rigid_body.exp_so3", "completion": "  axis_angle = jnp.reshape(axis_angle, (3))\n  theta = jnp.linalg.norm(axis_angle)\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  R_taylor = jnp.eye(3) + skew(axis_angle)\n\n  # Prevent bad gradients from propagating back when theta is small.\n  axis_angle_safe = jnp.where(theta**2 > eps**2, axis_angle, 0.0)\n  theta_safe = jnp.where(theta**2 > eps**2, theta, 1.0)\n  W = skew(axis_angle_safe)\n  R = (\n      jnp.eye(3)\n      + jnp.sin(theta_safe) * W\n      + (1.0 - jnp.cos(theta_safe)) * spin_math.matmul(W, W)\n  )\n\n  return jnp.where(theta**2 > eps**2, R, R_taylor)"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = _safe_sqrt(jnp.sum(axis_angle**2, axis=-1))\n  w = axis_angle / theta\n  W = skew(w)\n  R = jnp.eye(3) + jnp.sin(theta) * W + (1.0 - jnp.cos(theta)) * spin_math.matmul(\n      W, W\n  )\n  return jnp.where(theta**2 > eps**2, R, jnp.eye(3))"}
{"namespace": "rigid_body.exp_so3", "completion": "  axis_angle = jnp.reshape(axis_angle, (3,))\n  theta = jnp.linalg.norm(axis_angle)\n  w = axis_angle / (theta + eps)\n  W = skew(w)\n  R = jnp.eye(3) + jnp.sin(theta) * W + (1.0 - jnp.cos(theta)) * spin_math.matmul(\n      W, W\n  )\n  return R"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = _safe_sqrt(jnp.sum(axis_angle**2, axis=-1))\n  axis_angle_safe = jnp.where(theta**2 > eps**2, axis_angle, 0.0)\n  theta_safe = jnp.where(theta**2 > eps**2, theta, 1.0)\n  W = skew(axis_angle_safe)\n  R = (\n      jnp.eye(3)\n      + jnp.sin(theta_safe) * W\n      + (1.0 - jnp.cos(theta_safe)) * spin_math.matmul(W, W)\n  )\n  return jnp.where(theta**2 > eps**2, R, jnp.eye(3))"}
{"namespace": "rigid_body.exp_so3", "completion": "  axis_angle = jnp.reshape(axis_angle, (3,))\n  theta = _safe_sqrt(jnp.sum(axis_angle**2, axis=-1))\n  w = axis_angle / theta\n  W = skew(w)\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  R_taylor = jnp.eye(3) + W\n\n  # Prevent bad gradients from propagating back when theta is small.\n  theta_safe = jnp.where(theta**2 > eps**2, theta, 1.0)\n  W_safe = jnp.where(theta**2 > eps**2, W, 0.0)\n  R = (\n      jnp.eye(3)\n      + jnp.sin(theta_safe) * W_safe\n      + (1.0 - jnp.cos(theta_safe)) * spin_math.matmul(W_safe, W_safe)\n  )\n\n  return jnp.where(theta**2 > eps**2, R, R_taylor)"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Extract the axis and angle of rotation from the input axis-angle vector.\n  axis = axis_angle[Ellipsis, :3]\n  angle = axis_angle[Ellipsis, 3]\n\n  # Compute the rotation matrix using Rodrigues' formula.\n  R = jnp.eye(3) + jnp.sin(angle) * skew(axis) + (1 - jnp.cos(angle)) * spin_math.matmul(skew(axis), skew(axis))\n\n  return R"}
{"namespace": "rigid_body.exp_so3", "completion": "  axis_angle = jnp.reshape(axis_angle, (3))\n  theta = jnp.linalg.norm(axis_angle)\n  w = axis_angle / (theta + eps)\n  theta = theta + eps\n  W = skew(w)\n  R = jnp.eye(3) + jnp.sin(theta) * W + (1.0 - jnp.cos(theta)) * W @ W\n  return R"}
{"namespace": "rigid_body.exp_so3", "completion": "  axis_angle = jnp.reshape(axis_angle, (3))\n  theta = jnp.linalg.norm(axis_angle)\n  if theta < eps:\n    return jnp.eye(3)\n  axis = axis_angle / theta\n  W = skew(axis)\n  R = jnp.eye(3) + jnp.sin(theta) * W + (1 - jnp.cos(theta)) * spin_math.matmul(\n      W, W\n  )\n  return R"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = _safe_sqrt(jnp.sum(axis_angle**2, axis=-1))\n  axis_angle_safe = jnp.where(theta**2 > eps**2, axis_angle, 0.0)\n  theta_safe = jnp.where(theta**2 > eps**2, theta, 1.0)\n  W = skew(axis_angle_safe / theta_safe)\n\n  R = (\n      jnp.eye(3)\n      + jnp.sin(theta_safe) * W\n      + (1.0 - jnp.cos(theta_safe)) * spin_math.matmul(W, W)\n  )\n  return R"}
{"namespace": "rigid_body.exp_so3", "completion": "  axis_angle = jnp.reshape(axis_angle, (3,))\n  theta = _safe_sqrt(jnp.sum(axis_angle**2, axis=-1))\n  W = skew(axis_angle / theta)\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  R_taylor = jnp.eye(3) + W\n\n  # Prevent bad gradients from propagating back when theta is small.\n  axis_angle_safe = jnp.where(theta**2 > eps**2, axis_angle, 0.0)\n  theta_safe = jnp.where(theta**2 > eps**2, theta, 1.0)\n  W_safe = skew(axis_angle_safe / theta_safe)\n  R = jnp.eye(3) + jnp.sin(theta_safe) * W_safe + (1.0 - jnp.cos(theta_safe)) * spin_math.matmul(W_safe, W_safe)\n\n  return jnp.where(theta**2 > eps**2, R, R_taylor)"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Extract the angle and axis of rotation from the input axis-angle vector.\n  theta = jnp.linalg.norm(axis_angle)\n  axis = axis_angle / (theta + eps)\n\n  # Compute the rotation matrix using Rodrigues' formula.\n  R = jnp.eye(3) + jnp.sin(theta) * skew(axis) + (1 - jnp.cos(theta)) * spin_math.matmul(skew(axis), skew(axis))\n\n  # Return the rotation matrix.\n  return R"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = jnp.linalg.norm(axis_angle, axis=-1)\n  axis = axis_angle / (theta + eps)\n  axis_x = jnp.reshape(axis, (3, 1))\n  R = jnp.eye(3) + jnp.sin(theta) * skew(axis) + (1 - jnp.cos(theta)) * spin_math.matmul(skew(axis), skew(axis))\n  return R"}
{"namespace": "rigid_body.exp_so3", "completion": "  axis_angle = jnp.reshape(axis_angle, (3,))\n  theta = jnp.linalg.norm(axis_angle, axis=-1)\n  w = axis_angle / (theta + eps)\n  W = skew(w)\n  R = jnp.eye(3) + jnp.sin(theta) * W + (1.0 - jnp.cos(theta)) * spin_math.matmul(W, W)\n  return R"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = jnp.linalg.norm(axis_angle)\n  axis_angle = axis_angle / (theta + eps)\n  axis = axis_angle\n  s = jnp.sin(theta)\n  c = jnp.cos(theta)\n  t = 1 - c\n  x, y, z = axis[0], axis[1], axis[2]\n  xs, ys, zs = x * s, y * s, z * s\n  xyt, yzt, zxt = x * y * t, y * z * t, z * x * t\n\n  R = jnp.array(\n      [\n          [t * x * x + c, xyt - zs, zxt + ys],\n          [xyt + zs, t * y * y + c, yzt - xs],\n          [zxt - ys, yzt + xs, t * z * z + c],\n      ]\n  )\n  return R"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = jnp.linalg.norm(axis_angle)\n  axis = axis_angle / (theta + eps)\n  axis = axis[jnp.newaxis, :]\n  R = jnp.eye(3) + jnp.sin(theta) * skew(axis) + (\n      1 - jnp.cos(theta)\n  ) * spin_math.matmul(skew(axis), skew(axis))\n  return R"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = jnp.linalg.norm(axis_angle, axis=-1)\n  axis_angle_normalized = axis_angle / (theta + eps)\n  w_x = skew(axis_angle_normalized)\n  R = jnp.eye(3) + jnp.sin(theta) * w_x + (1 - jnp.cos(theta)) * jnp.matmul(\n      w_x, w_x\n  )\n  return R"}
{"namespace": "rigid_body.exp_so3", "completion": "  axis = axis_angle[Ellipsis, :3]\n  theta = _safe_sqrt(jnp.sum(axis**2, axis=-1))\n  W = skew(axis / theta)\n\n  R_taylor = jnp.eye(3) + W\n  R = jnp.eye(3) + jnp.sin(theta) * W + (1.0 - jnp.cos(theta)) * W @ W\n\n  # If theta is small, use Taylor expansion.\n  return jnp.where(theta**2 > eps**2, R, R_taylor)"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Extract the axis and angle of rotation from the input axis-angle vector\n  axis = axis_angle[Ellipsis, :3]\n  angle = jnp.linalg.norm(axis, axis=-1)\n\n  # If the angle of rotation is very small, use the first-order Taylor expansion\n  # to avoid division by zero or very small values that could lead to numerical\n  # instability.\n  axis_safe = jnp.where(angle**2 > eps**2, axis, 0.0)\n  angle_safe = jnp.where(angle**2 > eps**2, angle, 1.0)\n\n  # Compute the skew-symmetric matrix from the axis of rotation\n  w_hat = skew(axis_safe)\n\n  # Compute the rotation matrix using Rodrigues' formula\n  R = jnp.eye(3) + jnp.sin(angle_safe) * w_hat + (1 - jnp.cos(angle_safe)) * spin_math.matmul(w_hat, w_hat)\n\n  return R"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, base_radius * r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, base_radius * r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, base_radius * r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, base_radius * r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, base_radius * r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, base_radius * r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, base_radius * r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, base_radius * r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, base_radius * r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, base_radius * r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, base_radius * r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, base_radius * r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, base_radius * r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, r_var * base_radius**2, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, base_radius**2 * r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Convert pixel coordinates to camera coordinates.\n  pix_x_cam = (pix_x_int - pixtocams[Ellipsis, 0, 2]) / pixtocams[Ellipsis, 0, 0]\n  pix_y_cam = (pix_y_int - pixtocams[Ellipsis, 1, 2]) / pixtocams[Ellipsis, 1, 1]\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    pix_x_cam, pix_y_cam = _radial_and_tangential_undistort(\n        pix_x_cam,\n        pix_y_cam,\n        **distortion_params,\n    )\n\n  # Convert camera coordinates to world coordinates.\n  pix_x_world = mat_vec_mul(camtoworlds[Ellipsis, :3, :3], pix_x_cam)\n  pix_y_world = mat_vec_mul(camtoworlds[Ellipsis, :3, :3], pix_y_cam)\n  pix_z_world = mat_vec_mul(camtoworlds[Ellipsis, :3, :3], xnp.ones_like(pix_x_cam))\n  pix_world = xnp.stack([pix_x_world, pix_y_world, pix_z_world], -1)\n\n  # Compute ray origins and directions.\n  origins = mat_vec_mul(camtoworlds[Ellipsis, :3, -1], xnp.ones_like(pix_x_cam))\n  directions = pix_world - origins\n\n  return origins"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Convert pixel coordinates to camera coordinates.\n  pix_x_cam = (pix_x_int - pixtocams[Ellipsis, 0, 2]) / pixtocams[Ellipsis, 0, 0]\n  pix_y_cam = (pix_y_int - pixtocams[Ellipsis, 1, 2]) / pixtocams[Ellipsis, 1, 1]\n  pix_cam = xnp.stack([pix_x_cam, pix_y_cam, xnp.ones_like(pix_x_cam)], -1)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        pix_x_cam,\n        pix_y_cam,\n        **distortion_params,\n    )\n    pix_cam = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.PANORAMIC:\n    # Convert to NDC coordinates.\n    pix_cam = mat_vec_mul(pixtocam_ndc, pix_cam)\n\n  # Convert camera coordinates to world coordinates.\n  origins = mat_vec_mul(camtoworlds[Ellipsis, :3, :3], pix_cam) + (\n      camtoworlds[Ellipsis, :3, -1]\n  )\n  directions = mat_vec_mul(camtoworlds[Ellipsis, :3, :3], pix_cam)\n  viewdirs = directions / xnp.linalg.norm(directions, axis="}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Convert pixel coordinates to camera coordinates.\n  camera_dirs = mat_vec_mul(xnp.linalg.inv(pixtocams), xnp.stack([\n      pix_x_int, pix_y_int, xnp.ones_like(pix_x_int)\n  ], -1))\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs[Ellipsis, 0],\n        camera_dirs[Ellipsis, 1],\n        **distortion_params,\n    )\n    camera_dirs = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  # OpenGL to OpenCV coordinates.\n  camera_dirs = matmul(camera_dirs, xnp.diag(xnp.array([1.0, -1.0, -1.0])))\n\n  # Transform to world coordinates.\n  origins = mat_vec_mul(camtoworlds[Ellipsis, :3, :3], camera_dirs)\n  origins = origins + mat_vec_mul(camtoworlds[Ellipsis, :3, -1],\n                                  xnp.ones_like(origins[Ellipsis, :1]))\n  directions = mat_vec_mul(camtoworlds[Ellipsis, :3, :3], camera_dirs)\n  viewdirs = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n  return origins, directions, viewdirs, camera_dirs["}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Convert pixel coordinates to camera coordinates.\n  pix_x_cam = (pix_x_int - pixtocams[Ellipsis, 0, 2]) / pixtocams[Ellipsis, 0, 0]\n  pix_y_cam = (pix_y_int - pixtocams[Ellipsis, 1, 2]) / pixtocams[Ellipsis, 1, 1]\n\n  # Apply distortion to camera coordinates.\n  if distortion_params is not None:\n    pix_x_cam, pix_y_cam = _radial_and_tangential_undistort(\n        pix_x_cam,\n        pix_y_cam,\n        **distortion_params,\n    )\n\n  # Convert camera coordinates to world coordinates.\n  camtoworld = camtoworlds[Ellipsis, :3, :]\n  camtoworld_inv = xnp.swapaxes(camtoworld, -1, -2)\n  origin = mat_vec_mul(camtoworld_inv, xnp.ones_like(pix_x_cam))\n  direction = mat_vec_mul(camtoworld_inv, xnp.stack([pix_x_cam, pix_y_cam,\n                                                     xnp.ones_like(pix_x_cam)],\n                                                    -1))\n  viewdir = direction / xnp.linalg.norm(direction, axis=-1, keepdims=True)\n  imageplane = xnp.stack([pix_x_cam, pix_y_cam], -1)\n\n  return origin, direction, viewdir, direction, imageplane"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Convert pixel coordinates to camera coordinates.\n  pixtocam = pixtocams[Ellipsis, :3, :3]\n  pixtocam_inv = xnp.linalg.inv(pixtocam)\n  pixtocam_inv = xnp.where(\n      xnp.isnan(pixtocam_inv), xnp.zeros_like(pixtocam_inv), pixtocam_inv\n  )\n  pix_cam = mat_vec_mul(pixtocam_inv, xnp.stack([pix_x_int, pix_y_int, xnp.ones_like(pix_x_int)], -1))\n\n  # Correct for lens distortion.\n  if distortion_params is not None:\n    pix_cam = _radial_and_tangential_undistort(\n        pix_cam[Ellipsis, 0],\n        pix_cam[Ellipsis, 1],\n        **distortion_params,\n    )\n\n  # Convert camera coordinates to world coordinates.\n  camtoworld = camtoworlds[Ellipsis, :3, :]\n  camtoworld_inv = xnp.linalg.inv(camtoworld)\n  camtoworld_inv = xnp.where(\n      xnp.isnan(camtoworld_inv), xnp.zeros_like(camtoworld_inv), camtoworld_inv\n  )\n  origins = mat_vec_mul(camtoworld_inv, pix_cam)\n  directions = mat_vec_mul(camtoworld_inv, pix_cam - pix_cam[Ellipsis, 2:])\n\n  return origins, directions, directions / xnp."}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if camtype != ProjectionType.PERSPECTIVE:\n    raise ValueError(f'pixels_to_rays only supports perspective projection, '\n                     f'not {camtype} mode.')\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Convert pixel coordinates to camera coordinates.\n  pix_x = pix_x_int - xnp.array([0.5, 0.5])\n  pix_y = pix_y_int - xnp.array([0.5, 0.5])\n  if pixtocam_ndc is not None:\n    pix_x, pix_y = _radial_and_tangential_undistort(\n        pix_x,\n        pix_y,\n        **distortion_params,\n        xnp=xnp,\n    )\n    pix_x, pix_y = pix_x[Ellipsis, None], pix_y[Ellipsis, None]\n    pix_x = mat_vec_mul(pixtocam_ndc, xnp.stack([pix_x, pix_y, xnp.ones_like(pix_x)], -1))\n\n  pix_cam = mat_vec_mul(pixtocams, xnp.stack([pix_x, pix_y, xnp.ones_like(pix_x)], -1))\n\n  # Convert camera coordinates to world coordinates.\n  camtoworld = camtoworlds\n  rotation = camtoworld[Ellipsis, :3, :3]\n  translation = camtoworld[Ellipsis, :3, -1]\n  world_dirs = mat_vec_mul(rotation, pix_cam)\n  origins = translation[Ellipsis, None]\n  directions = world_dirs\n "}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if camtype == ProjectionType.PERSPECTIVE:\n    # Compute ray directions in camera coordinates.\n    camera_dirs = _radial_and_tangential_undistort(\n        pix_x_int,\n        pix_y_int,\n        **distortion_params,\n    )\n  elif camtype == ProjectionType.FISHEYE:\n    # Compute ray directions in camera coordinates.\n    camera_dirs = _radial_and_tangential_undistort(\n        pix_x_int,\n        pix_y_int,\n        **distortion_params,\n    )\n    # Convert to fisheye coordinates.\n    camera_dirs = xnp.stack([\n        camera_dirs[Ellipsis, 0] / camera_dirs[Ellipsis, 2],\n        camera_dirs[Ellipsis, 1] / camera_dirs[Ellipsis, 2],\n        xnp.ones_like(camera_dirs[Ellipsis, 0]),\n    ],\n                            -1)\n  elif camtype == ProjectionType.PANORAMIC:\n    # Compute ray directions in camera coordinates.\n    camera_dirs = xnp.stack([\n        xnp.cos(pix_x_int) * xnp.sin(pix_y_int),\n        xnp.sin(pix_x_int) * xnp.sin(pix_y_int),\n        xnp.cos(pix_y_int),\n    ],\n                            -1)\n  else:\n    raise ValueError(f'Unknown camera type {camtype}')\n\n  # Apply inverse intrinsics.\n  camera_dirs = xnp.matmul(pixtocams, camera_dirs[..., None])[..., 0]\n\n  # Apply extrinsics.\n  origins = xnp.matmul(camtoworlds[..., :3, :3], camera_dirs[..., None])[\n      ..., 0\n  ]\n  directions = camera_"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Convert pixel coordinates to camera coordinates.\n  pix_x = pix_x_int.astype(xnp.float32)\n  pix_y = pix_y_int.astype(xnp.float32)\n  if pixtocam_ndc is not None:\n    pix_x, pix_y = _radial_and_tangential_undistort(\n        pix_x,\n        pix_y,\n        **distortion_params,\n        xnp=xnp,\n    )\n    pix_x, pix_y = pix_x * 2.0 - 1.0, pix_y * 2.0 - 1.0\n    pix_x, pix_y = mat_vec_mul(pixtocam_ndc, [pix_x, pix_y, xnp.ones_like(pix_x)])\n  else:\n    pix_x, pix_y = _radial_and_tangential_undistort(\n        pix_x,\n        pix_y,\n        **distortion_params,\n        xnp=xnp,\n    )\n\n  # Apply inverse intrinsics.\n  pix_x, pix_y, pix_z = mat_vec_mul(pixtocams, [pix_x, pix_y, xnp.ones_like(pix_x)])\n\n  # Apply inverse camera extrinsics.\n  ray_dir = mat_vec_mul(camtoworlds[:, :3, :3], [pix_x, pix_y, pix_z])\n  ray_orig = mat_vec_mul(camtoworlds[:, :3, 3], xnp.ones_like(pix_x))\n\n  return"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Convert pixel coordinates to camera coordinates.\n  pix_x = (pix_x_int - pixtocams[Ellipsis, 0, 2]) / pixtocams[Ellipsis, 0, 0]\n  pix_y = (pix_y_int - pixtocams[Ellipsis, 1, 2]) / pixtocams[Ellipsis, 1, 1]\n  pix_z = xnp.ones_like(pix_x)\n\n  if camtype == ProjectionType.PERSPECTIVE:\n    # Perspective projection.\n    pix_z = pix_z * -1.0\n  elif camtype == ProjectionType.FISHEYE:\n    # Fisheye projection.\n    pix_z = xnp.sqrt(pix_x**2 + pix_y**2)\n  elif camtype == ProjectionType.PANORAMIC:\n    # Panoramic projection.\n    pix_z = xnp.ones_like(pix_x)\n  else:\n    raise ValueError(f'Unknown camera type {camtype}.')\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    pix_x, pix_y = _radial_and_tangential_undistort(\n        pix_x,\n        pix_y,\n        **distortion_params,\n    )\n\n  # Convert camera coordinates to world coordinates.\n  pix_x = pix_x[Ellipsis, None]\n  pix_y = pix_y[Ellipsis, None]\n  pix_z = pix_z[Ellipsis, None]\n  return (\n      mat_vec_mul(camtoworlds,"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if camtype == ProjectionType.PERSPECTIVE:\n    # OpenCV to OpenGL coordinates.\n    pix_x_int = pix_x_int - 0.5\n    pix_y_int = pix_y_int - 0.5\n  elif camtype == ProjectionType.FISHEYE:\n    # Fisheye model.\n    pix_x_int = pix_x_int - 0.5\n    pix_y_int = pix_y_int - 0.5\n    r2 = pix_x_int * pix_x_int + pix_y_int * pix_y_int\n    pix_x_int = pix_x_int * (1.0 + r2 * (k1 + r2 * (k2 + r2 * (k3 + r2 * k4))))\n    pix_y_int = pix_y_int * (1.0 + r2 * (k1 + r2 * (k2 + r2 * (k3 + r2 * k4))))\n  elif camtype == ProjectionType.PANORAMIC:\n    # Spherical model.\n    pix_x_int = pix_x_int - 0.5\n    pix_y_int = pix_y_int - 0.5\n    pix_x_int = pix_x_int * 2.0 * np.pi\n    pix_y_int = pix_y_int * np.pi\n  else:\n    raise ValueError(f'Unsupported projection type: {camtype}')\n\n  # Apply inverse intrinsics.\n  pix_x_cam, pix_y_cam = _radial_and_tangential_undistort(\n      pix_x_int,\n      pix_y_int,\n      **(distortion_params or {}),\n  )\n\n  # Apply inverse camera matrix.\n  origins, directions = matmul(pixtocams, xnp.stack([pix_x_cam, pix_y_cam, xnp.ones_like(pix_x_cam)], -1))\n  return origins, directions"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if camtype != ProjectionType.PERSPECTIVE:\n    raise ValueError(f'pixels_to_rays only supports perspective projection, '\n                     f'not {camtype} mode.')\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Convert pixel coordinates to camera coordinates.\n  # Add half pixel offset to center the pixel.\n  x, y = (\n      pix_x_int[Ellipsis, None] + 0.5,\n      pix_y_int[Ellipsis, None] + 0.5,\n  )\n  if pixtocam_ndc is not None:\n    x, y = _radial_and_tangential_undistort(\n        x,\n        y,\n        **distortion_params,\n        xnp=xnp,\n    )\n    xy = mat_vec_mul(pixtocam_ndc, xy)\n  else:\n    xy = mat_vec_mul(pixtocams, xy)\n\n  # OpenCV to OpenGL coordinates.\n  xy = matmul(xy, xnp.diag(xnp.array([1.0, -1.0, -1.0])))\n\n  # Convert camera coordinates to world coordinates.\n  ray_dirs = mat_vec_mul(camtoworlds[Ellipsis, :3, :3], xy)\n  ray_origins = mat_vec_mul(camtoworlds[Ellipsis, :3, -1], xnp.ones_like(xy))\n\n  # Normalize directions.\n  ray_dirs = ray_dirs / xnp.linalg.norm(ray_dirs, axis=-1)[Ellipsis, None]\n\n  return ray_origins, ray"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if camtype == ProjectionType.PERSPECTIVE:\n    # Compute camera directions.\n    x, y = _radial_and_tangential_undistort(\n        pix_x_int,\n        pix_y_int,\n        **distortion_params,\n    )\n    camera_dirs = xnp.stack([x, y, xnp.ones_like(x)], -1)\n  elif camtype == ProjectionType.FISHEYE:\n    # Compute camera directions.\n    r = xnp.sqrt(pix_x_int * pix_x_int + pix_y_int * pix_y_int)\n    camera_dirs = xnp.stack([pix_x_int, pix_y_int, r], -1)\n  elif camtype == ProjectionType.PANORAMIC:\n    # Compute camera directions.\n    x = pix_x_int / width * 2.0 * np.pi\n    y = pix_y_int / height * np.pi\n    camera_dirs = xnp.stack([x, y, xnp.ones_like(x)], -1)\n  else:\n    raise ValueError(f'Unknown projection type: {camtype}')\n\n  if pixtocam_ndc is not None:\n    # Convert to NDC coordinates.\n    camera_dirs = xnp.linalg.inv(pixtocam_ndc) @ camera_dirs[..., None]\n    camera_dirs = camera_dirs[..., 0]\n\n  # OpenCV to OpenGL coordinates.\n  camera_dirs = xnp.swapaxes(camera_dirs, -1, -2)\n\n  # Apply inverse intrinsics matrix.\n  directions = xnp.linalg.inv(pixtocams) @ camera_dirs\n\n  # Compute ray origins.\n  origins = xnp.linalg.inv(camtoworlds)[Ellipsis, :3, -1]\n\n  return origins, directions, directions, directions, camera_dirs"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if camtype == ProjectionType.PERSPECTIVE:\n    # Convert to camera coordinates.\n    cam_x, cam_y = (\n        pix_x_int - pixtocams[Ellipsis, 0, 2]) / pixtocams[Ellipsis, 0, 0], (\n            pix_y_int - pixtocams[Ellipsis, 1, 2]) / pixtocams[Ellipsis, 1, 1]\n\n    # Apply distortion.\n    if distortion_params is not None:\n      cam_x, cam_y = _radial_and_tangential_undistort(\n          cam_x, cam_y, **distortion_params\n      )\n\n    # Convert to world coordinates.\n    camtoworlds = xnp.broadcast_to(camtoworlds, pixtocams.shape[:-2])\n    rotation = camtoworlds[Ellipsis, :3, :3]\n    translation = camtoworlds[Ellipsis, :3, -1]\n    camera_dirs = xnp.stack([cam_x, cam_y, xnp.ones_like(cam_x)], -1)\n    directions = matmul(rotation, camera_dirs)\n    origins = xnp.broadcast_to(translation, directions.shape)\n\n  elif camtype == ProjectionType.FISHEYE:\n    raise NotImplementedError('Fisheye camera is not implemented yet.')\n\n  elif camtype == ProjectionType.PANORAMIC:\n    # Convert to camera coordinates.\n    cam_x, cam_y = (\n        pix_x_int - pixtocams[Ellipsis, 0, 2]) / pixtocams[Ellipsis, 0, 0], (\n            pix_y_int - pixtocams[Ellipsis, 1, 2]) / pixtocams[Ellipsis, 1, 1]\n    directions = xnp.stack([cam_x, cam"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Convert pixel coordinates to camera coordinates.\n  x, y = (\n      pix_x_int - pixtocams[Ellipsis, 0, 2],\n      pix_y_int - pixtocams[Ellipsis, 1, 2],\n  )\n  if camtype == ProjectionType.PERSPECTIVE:\n    x, y = _radial_and_tangential_undistort(\n        x,\n        y,\n        **distortion_params,\n    )\n  elif camtype == ProjectionType.FISHEYE:\n    # Convert to Normalized Device Coordinates (NDC) space.\n    x, y = (\n        (x - pixtocams[Ellipsis, 0, 2]) / pixtocams[Ellipsis, 0, 0],\n        (y - pixtocams[Ellipsis, 1, 2]) / pixtocams[Ellipsis, 1, 1],\n    )\n    r = xnp.sqrt(x**2 + y**2)\n    x, y = x / r, y / r\n  elif camtype == ProjectionType.PANORAMIC:\n    # Convert to NDC space.\n    x, y = (\n        (x - pixtocams[Ellipsis, 0, 2]) / pixtocams[Ellipsis, 0, 0],\n        (y - pixtocams[Ellipsis, 1, 2]) / pixtocams[Ellipsis, 1, 1],\n    )\n  else:\n    raise ValueError(f'Unknown projection type: {camtype}')\n\n  # Transform to world space.\n  return pixels_"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if camtype == ProjectionType.PERSPECTIVE:\n    # Convert pixel coordinates to camera coordinates.\n    x, y = _radial_and_tangential_undistort(\n        pix_x_int,\n        pix_y_int,\n        **distortion_params,\n    )\n    x = x * 2.0 - 1.0\n    y = y * 2.0 - 1.0\n\n    # Apply inverse intrinsics.\n    x, y, z = matmul(pixtocams, xnp.stack([x, y, xnp.ones_like(x)], -1))\n    z = z[Ellipsis, None]\n\n    # Apply inverse NDC.\n    if pixtocam_ndc is not None:\n      x, y, z = matmul(pixtocam_ndc, xnp.stack([x, y, z], -1))\n\n    # Apply camera extrinsics.\n    origins = matmul(camtoworlds[Ellipsis, :3, :3], x) + (\n        camtoworlds[Ellipsis, :3, -1]\n    )\n    directions = matmul(camtoworlds[Ellipsis, :3, :3], x) - origins\n\n    # Normalize directions.\n    directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n    # Compute viewdirs.\n    viewdirs = -directions\n\n    # Compute ray differential radii.\n    radii = xnp.linalg.norm(origins, axis=-1)\n\n    # Compute image plane coordinates.\n    imageplane = xnp.stack([x, y], -1)\n\n  elif camtype == ProjectionType.FISHEYE:\n    raise NotImplementedError('fisheye projection not implemented.')\n\n  else:\n    raise ValueError(f'unsupported projection type: {camtype}')\n\n  return origins, directions, viewdirs,"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if camtype == ProjectionType.PERSPECTIVE:\n    # Compute ray origins and directions in camera coordinate frame.\n    x, y = xnp.meshgrid(pix_x_int, pix_y_int, indexing='xy')\n    x = x.astype(xnp.float32)\n    y = y.astype(xnp.float32)\n    z = xnp.ones_like(x)\n\n    if pixtocam_ndc is not None:\n      # Convert to Normalized Device Coordinates (NDC) space.\n      x, y, z = matmul(pixtocam_ndc, [x, y, z])\n\n    # Apply distortion correction.\n    if distortion_params is not None:\n      x, y = _radial_and_tangential_undistort(\n          x, y, **distortion_params, xnp=xnp\n      )\n\n    # Apply inverse intrinsics.\n    x, y, z = matmul(pixtocams, [x, y, z])\n\n    # Convert to world coordinate frame.\n    x, y, z = matmul(camtoworlds, [x, y, z])\n\n    # Compute ray origins and directions.\n    origins = xnp.stack([x, y, z], -1)\n    directions = normalize(origins)\n\n    # Compute view directions.\n    viewdirs = directions\n\n    # Compute ray differential radii.\n    radii = xnp.linalg.norm(origins, axis=-1)\n\n    # Compute image plane coordinates.\n    imageplane = xnp.stack([x, y], -1)\n\n  elif camtype == ProjectionType.FISHEYE:\n    raise NotImplementedError('Fisheye camera projection not supported yet.')\n\n  elif camtype == ProjectionType.PANORAMIC:\n    raise NotImplementedError('Panoramic camera projection not supported yet.')\n\n  else:\n    raise ValueError(f'Unknown camera projection"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if camtype == ProjectionType.PERSPECTIVE:\n    # For perspective, the inverse intrinsics are the same as the intrinsics.\n    pixtocam_ndc = pixtocams\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n\n  # Convert to NDC coordinates.\n  xy_ndc = matmul(pixtocam_ndc, xnp.stack([pix_x_int, pix_y_int, xnp.ones_like(pix_x_int)], -1))\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        xy_ndc[Ellipsis, 0],\n        xy_ndc[Ellipsis, 1],\n        **distortion_params,\n    )\n    xy_ndc = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  # Convert to camera coordinates.\n  xyz_cam = matmul(pixtocams, xy_ndc)\n\n  # Convert to world coordinates.\n  xyz_world = matmul(camtoworlds[:3, :3], xyz_cam) + camtoworlds[:3, 3]\n\n  # Convert to ray directions.\n  ray_dirs = xyz_world - camtoworlds[:3, 3]\n  ray_dirs = xnp.linalg.norm(ray_dirs, axis=-1, keepdims=True)\n  ray_dirs = ray_dirs * (xyz_cam / ray_dirs)\n\n  # Compute other useful quantities.\n  viewdirs = ray_dirs / xnp.linalg.norm(ray_dirs, axis=-1, keepdims=True)\n  radii = xnp.linalg.norm(xy"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if camtype == ProjectionType.PERSPECTIVE:\n    # Compute ray origins.\n    origins = xnp.stack([\n        pix_x_int,\n        pix_y_int,\n        xnp.ones_like(pix_x_int),\n    ], -1)\n    origins = xnp.matmul(origins, pixtocams)\n\n    # Compute ray directions.\n    directions = xnp.stack([\n        pix_x_int,\n        pix_y_int,\n        xnp.ones_like(pix_x_int),\n    ], -1)\n    directions = xnp.matmul(directions, pixtocams)\n\n    # Normalize ray directions.\n    directions = directions / xnp.linalg.norm(directions, axis=-1)[..., None]\n\n    # Convert ray origins and directions into camera space.\n    origins = xnp.matmul(origins, camtoworlds[:, :3, :3]) + camtoworlds[\n        :, :3, 3\n    ]\n    directions = xnp.matmul(directions, camtoworlds[:, :3, :3])\n\n    # Compute ray differential radii.\n    radii = xnp.linalg.norm(directions, axis=-1)\n\n    # Compute image plane coordinates.\n    imageplane = xnp.stack([pix_x_int, pix_y_int], -1)\n\n  elif camtype == ProjectionType.FISHEYE:\n    raise NotImplementedError(\n        'Fisheye projection not implemented yet, please use perspective.'\n    )\n\n  elif camtype == ProjectionType.PANORAMIC:\n    raise NotImplementedError(\n        'Panoramic projection not implemented yet, please use perspective.'\n    )\n\n  else:\n    raise ValueError(f'Unknown camera type: {camtype}')\n\n  return origins, directions, directions, radii, imageplane"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if camtype == ProjectionType.PERSPECTIVE:\n    # OpenCV to OpenGL coordinates.\n    pix_x_int = pix_x_int - 0.5\n    pix_y_int = pix_y_int - 0.5\n\n    # Apply inverse intrinsics matrix.\n    pix_x_cam, pix_y_cam = matmul(pixtocams, xnp.stack([pix_x_int, pix_y_int, xnp.ones_like(pix_x_int)], -1))\n\n    if distortion_params is not None:\n      # Correct for lens distortion.\n      pix_x_cam, pix_y_cam = _radial_and_tangential_undistort(\n          pix_x_cam,\n          pix_y_cam,\n          **distortion_params,\n      )\n\n    # Compute ray origins and directions.\n    origins = matmul(camtoworlds, xnp.stack([pix_x_cam, pix_y_cam, xnp.ones_like(pix_x_int)], -1))\n    directions = origins - matmul(camtoworlds, xnp.array([0.0, 0.0, 0.0, 1.0]))\n    origins = origins[..., :3] / origins[..., 3:4]\n    directions = directions[..., :3] / directions[..., 3:4]\n\n    # Compute ray view directions and image plane coordinates.\n    viewdirs = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n    imageplane = xnp.stack([pix_x_int, pix_y_int], -1)\n\n    # Compute differential radii.\n    radii = xnp.linalg.norm(directions, axis=-1)\n\n  elif camtype == ProjectionType.PANORAMIC:\n    raise NotImplementedError\n  else:\n    raise ValueError(f'Unknown projection"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n\n  # Convert to OpenGL coordinates.\n  pix_x_int = pix_x_int - width * 0.5\n  pix_y_int = height * 0.5 - pix_y_int\n\n  # Convert pixel coordinates to camera coordinates.\n  if pixtocam_ndc is None:\n    pixtocam_ndc = get_pixtocam(1.0, width, height, xnp=xnp)\n  pixtocam_ndc_inv = xnp.linalg.inv(pixtocam_ndc)\n  pix_x_cam = matmul(pixtocam_ndc_inv, xnp.array([pix_x_int, pix_y_int, 1.0]))\n  pix_y_cam = matmul(pixtocam_ndc_inv, xnp.array([pix_x_int, pix_y_int, -1.0]))\n\n  if camtype == ProjectionType.PERSPECTIVE:\n    # Convert camera coordinates to world coordinates.\n    ray_directions = matmul(\n        camtoworlds[Ellipsis, :3, :3], pix_x_cam[Ellipsis, None]\n    )\n    ray_directions = matmul(\n        camtoworlds[Ellipsis, :3, :3], pix_y_cam[Ellipsis, None]\n    ) - ray_directions\n    ray_directions = matmul(\n        camtoworlds[Ellipsis, :3, :3],\n        xnp.array([0.0, 0.0, 1.0])[Ellipsis, None],\n    ) - ray_directions\n  else:\n    raise ValueError(f'pixels_to_rays() does not support {camtype} projection.')\n\n  # Undistort ray directions.\n "}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n  weights = compute_alpha_weights_helper(density_delta, **kwargs)\n  return weights"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n  weights = compute_alpha_weights_helper(density_delta, **kwargs)\n  return weights"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density[Ellipsis, 1:] * tdist[Ellipsis, 1:]\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density[Ellipsis, 1:] - density[Ellipsis, :-1]\n  weights = compute_alpha_weights_helper(density_delta, **kwargs)\n  return weights\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density[Ellipsis, 1:] - density[Ellipsis, :-1]\n  weights = compute_alpha_weights_helper(density_delta, **kwargs)\n  return weights\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = jnp.diff(density, axis=-1)\n  density_delta = jnp.concatenate(\n      [jnp.zeros_like(density_delta[Ellipsis, :1]), density_delta], axis=-1\n  )\n\n  density_delta = density_delta * tdist\n  weights = compute_alpha_weights_helper(density_delta, **kwargs)\n\n  return weights"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = jnp.concatenate(\n      [jnp.zeros_like(density[Ellipsis, :1]), density[Ellipsis, 1:]], axis=-1\n  )\n  density_delta = density_delta * tdist[Ellipsis, 1:]\n  weights = compute_alpha_weights_helper(density_delta)\n  return weights\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density[Ellipsis, 1:] - density[Ellipsis, :-1]\n  tdist_delta = tdist[Ellipsis, 1:] - tdist[Ellipsis, :-1]\n  dirs_norm = jnp.linalg.norm(dirs, axis=-1)\n  dirs_norm_delta = dirs_norm[Ellipsis, 1:] - dirs_norm[Ellipsis, :-1]\n  norm_adjusted_tdist_delta = tdist_delta * dirs_norm_delta\n  adjusted_density_delta = density_delta * norm_adjusted_tdist_delta\n  weights = compute_alpha_weights_helper(adjusted_density_delta, **kwargs)\n  return weights"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Adjust the density values based on the norm of the direction vectors.\n  dirs_norm = jnp.linalg.norm(dirs, axis=-1, keepdims=True)\n  density_delta = density * tdist * dirs_norm\n\n  # Compute the alpha weights using the helper function.\n  weights = compute_alpha_weights_helper(density_delta, **kwargs)\n\n  return weights"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 1:\n    raise ValueError(f'num_samples must be > 1, is {num_samples}.')\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Sample from the CDF.\n  if rng is None:\n    # Use linspace for deterministic sampling.\n    u = jnp.linspace(eps, 1 - eps, num_samples, endpoint=False)\n  else:\n    # Use random sampling.\n    u = jax.random.uniform(rng, (num_samples,))\n    if single_jitter:\n      # Jitter all samples by the same amount.\n      u += jax.random.uniform(rng, (num_samples,)) / num_samples\n    else:\n      # Jitter each sample independently.\n      u += jax.random.uniform(rng, (num_samples,)) / num_samples\n\n  # Interpolate into the inverse CDF.\n  t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n\n  if deterministic_center:\n    # Center the samples in each interval.\n    t_new = (t_new[Ellipsis, 1:] + t_new[Ellipsis, :-1]) / 2\n\n  return t_new"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 1:\n    raise ValueError(f'num_samples must be > 1, is {num_samples}.')\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Sample from the CDF.\n  if rng is None:\n    # Use linspace to sample.\n    u = jnp.linspace(eps, 1 - eps, num_samples, endpoint=False)\n  else:\n    # Use random sampling.\n    if single_jitter:\n      u = jax.random.uniform(rng, (num_samples,))\n    else:\n      u = jax.random.uniform(rng, (num_samples,))\n\n  # Interpolate into the inverse CDF.\n  t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n\n  # Center the samples in the intervals.\n  if deterministic_center:\n    t_new = (t_new[Ellipsis, 1:] + t_new[Ellipsis, :-1]) / 2\n\n  return t_new"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 1:\n    raise ValueError(f'num_samples must be > 1, is {num_samples}.')\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Sample uniformly from [0, 1) for each sample.\n  u = jax.random.uniform(rng, (num_samples,), minval=eps, maxval=1 - eps)\n\n  # Interpolate into the inverse CDF.\n  if deterministic_center:\n    u = u + jnp.arange(num_samples) / num_samples\n  if single_jitter:\n    u = u + jnp.arange(num_samples) / num_samples\n  t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n\n  return t_new"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 0:\n    raise ValueError(f'num_samples must be > 0, is {num_samples}.')\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Sample from the CDF.\n  u = jax.random.uniform(rng, (num_samples,))\n  if rng is None:\n    u = jnp.linspace(eps, 1 - eps, num_samples)\n\n  # Jitter the samples.\n  if single_jitter:\n    u += jnp.arange(num_samples) / num_samples\n  else:\n    u += jax.random.uniform(rng, (num_samples,)) / num_samples\n\n  # Interpolate into the CDF to get the corresponding values.\n  t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n\n  # If we're doing deterministic sampling, we want to center the samples in\n  # each interval.\n  if deterministic_center:\n    t_new = (t_new[Ellipsis, 1:] + t_new[Ellipsis, :-1]) / 2\n\n  return t_new"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  if rng is None:\n    # If rng is None, we use linspace to generate samples.\n    u = jnp.linspace(eps, 1 - eps, num_samples, axis=-1)\n  else:\n    # Otherwise, we use random sampling.\n    if single_jitter:\n      # If single_jitter is True, we jitter all samples by the same amount.\n      u = jax.random.uniform(rng, (num_samples,))\n    else:\n      # If single_jitter is False, we jitter each sample independently.\n      u = jax.random.uniform(rng, (num_samples,))\n\n  # If deterministic_center is True, we center the samples in each interval.\n  if deterministic_center:\n    u = u + jnp.arange(num_samples) / num_samples\n\n  # Invert the CDF to get the sampled values.\n  t_new = invert_cdf(u, t, w_logits)\n  return t_new"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n  u = jax.random.uniform(rng, (num_samples,))\n  if deterministic_center:\n    u = (u + .5) / (num_samples + 1)\n  else:\n    u = u / (num_samples + 1)\n  u = jnp.minimum(1 - eps, jnp.maximum(eps, u))\n  if single_jitter:\n    u = u + jnp.arange(num_samples) / num_samples\n  else:\n    u = u + jax.random.uniform(rng, (num_samples,)) / num_samples\n  return math.sorted_interp(u, cw, t, utils.device_is_tpu())"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  if rng is None:\n    # Use linspace for deterministic sampling.\n    u = jnp.linspace(0, 1, num_samples, endpoint=False)\n    if deterministic_center:\n      u = u + jnp.arange(num_samples) / num_samples\n    u = jnp.clip(u, 0, 1 - eps)\n    t_new = invert_cdf(u, t, w_logits)\n  else:\n    # Use random sampling.\n    u = jax.random.uniform(rng, (num_samples,))\n    if single_jitter:\n      u = u + jnp.arange(num_samples) / num_samples\n    t_new = invert_cdf(u, t, w_logits)\n\n  return t_new"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Sample from the CDF.\n  if rng is None:\n    # If no random number generator is provided, use linspace.\n    u = jnp.linspace(eps, 1 - eps, num_samples, endpoint=False)\n  else:\n    # Otherwise, use a random number generator.\n    u = jax.random.uniform(rng, (num_samples,))\n    if single_jitter:\n      # If single_jitter is True, jitter each sample by the same amount.\n      u = u + jnp.arange(num_samples) / num_samples\n    else:\n      # If single_jitter is False, jitter each sample independently.\n      u = u + jax.random.uniform(rng, (num_samples,)) / num_samples\n\n  # Interpolate into the inverse CDF.\n  t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n\n  # If deterministic_center is True, shift the samples to the center of each\n  # interval.\n  if deterministic_center:\n    t_new = t_new - (jnp.diff(t) / 2)\n\n  return t_new"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Sample uniformly in [0, 1).\n  u = jax.random.uniform(rng, (num_samples,))\n\n  # If deterministic_center is True, we want to sample from the center of each\n  # interval. Otherwise, we want to sample uniformly within each interval.\n  if deterministic_center:\n    u = u + jnp.diff(cw) / 2\n  else:\n    u = u + eps\n\n  # If single_jitter is True, all samples are jittered by the same amount.\n  # Otherwise, each sample is jittered independently.\n  if single_jitter:\n    u = u + eps\n\n  # Interpolate into the inverse CDF.\n  samples = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n\n  return samples"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # If deterministic_center is True, we want to sample from the center of each\n  # interval. Otherwise, we want to sample uniformly from the entire PDF.\n  if deterministic_center:\n    # We want to sample from the center of each interval.\n    t_new = (t[Ellipsis, 1:] + t[Ellipsis, :-1]) / 2\n  else:\n    # We want to sample uniformly from the entire PDF.\n    t_new = jnp.linspace(t[..., 0], t[..., -1], num=num_samples, endpoint=False)\n\n  # If rng is None, we are using linspace sampling.\n  if rng is None:\n    # If single_jitter is True, we want to jitter each sample by the same amount.\n    if single_jitter:\n      # We want to jitter each sample by the same amount.\n      t_new = t_new + jnp.linspace(eps, 1 - eps, num=num_samples)\n    # If single_jitter is False, we want to jitter each sample independently.\n    else:\n      # We want to jitter each sample independently.\n      t_new = t_new + jax.random.uniform(rng, (num_samples,))\n\n  # If rng is not None, we are using random sampling.\n  else:\n    # If single_jitter is True, we want to jitter each sample by the same amount.\n    if single_jitter:\n      # We want to jitter each sample by the same amount.\n      t_new = t_new + jax.random.uniform(rng, (num_samples,))\n    # If single_jitter is False, we want to jitter each sample independently.\n    else:\n      #"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Sample from the CDF.\n  if rng is None:\n    # If no random number generator is provided, use linspace.\n    u = jnp.linspace(0, 1 - eps, num_samples, endpoint=False)\n  else:\n    # If a random number generator is provided, use it to sample uniformly.\n    u = jax.random.uniform(rng, (num_samples,), minval=0, maxval=1 - eps)\n\n  # If jittering is enabled, jitter each sample in the CDF.\n  if single_jitter:\n    u = u + jnp.linspace(0, 1 - eps, num_samples, endpoint=False) / num_samples\n  else:\n    u = u + jax.random.uniform(rng, (num_samples,), minval=0, maxval=1 - eps) / num_samples\n\n  # Interpolate into the inverse CDF.\n  t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n\n  # If deterministic_center is enabled, center each interval.\n  if deterministic_center:\n    t_new = t_new + (jnp.diff(t) / 2)\n\n  return t_new"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  if rng is None:\n    # If rng is None, we're doing deterministic sampling.\n    # We sample at linearly spaced points, then interpolate.\n    u = jnp.linspace(eps, 1 - eps, num_samples, endpoint=False)\n    if deterministic_center:\n      u = (u + jnp.arange(num_samples)) / num_samples\n    t_samples = jnp.vectorize(jnp.interp, signature='(n),(m),(m)->(n)')(\n        u, t, t\n    )\n  else:\n    # If rng is not None, we're doing random sampling.\n    # We sample uniformly at random, then interpolate.\n    u = jax.random.uniform(rng, (num_samples,), minval=eps, maxval=1 - eps)\n    if single_jitter:\n      # If single_jitter is True, we jitter each sample by the same amount.\n      u = u + jnp.arange(num_samples) / num_samples\n    else:\n      # If single_jitter is False, we jitter each sample independently.\n      u = u + jax.random.uniform(rng, (num_samples,), minval=0, maxval=1 / num_samples)\n    t_samples = jnp.vectorize(jnp.interp, signature='(n),(m),(m)->(n)')(\n        u, t, t\n    )\n\n  return t_samples"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 1:\n    raise ValueError(f'num_samples must be > 1, is {num_samples}.')\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # If rng is None, we'll sample from a linspace.\n  if rng is None:\n    u = jnp.linspace(eps, 1 - eps, num_samples, endpoint=False)\n    if single_jitter:\n      u = jnp.broadcast_to(u, cw.shape[:-1] + (num_samples,))\n    if deterministic_center:\n      u = (u + 0.5) / num_samples\n  else:\n    # Otherwise, we'll sample from a uniform distribution.\n    u = jax.random.uniform(rng, shape=cw.shape[:-1] + (num_samples,))\n    if single_jitter:\n      u = u + jnp.arange(num_samples) / num_samples\n    if deterministic_center:\n      u = (u + 0.5 / num_samples) % 1\n\n  # Interpolate into the inverse CDF.\n  t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n\n  return t_new"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  batch_size = t.shape[:-1]\n  num_bins = t.shape[-1] - 1\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Sample from the CDF.\n  if rng is None:\n    # Deterministic sampling.\n    if deterministic_center:\n      # Sample from the center of each interval.\n      u = jnp.linspace(eps, 1 - eps, num_samples)\n    else:\n      # Sample from the entire PDF.\n      u = jnp.linspace(eps, 1 - eps, num_samples * num_bins).reshape(\n          batch_size + (num_samples, num_bins)\n      )\n      u = u.reshape(batch_size + (num_samples * num_bins,))\n  else:\n    # Random sampling.\n    if single_jitter:\n      # Sample from the CDF and jitter each sample.\n      u = jax.random.uniform(rng, batch_size + (num_samples,))\n      u = jnp.sort(u)\n      u = (u + jax.random.uniform(rng, batch_size + (num_samples,))) / (\n          num_samples + 1\n      )\n    else:\n      # Sample from the CDF and jitter each sample independently.\n      u = jax.random.uniform(rng, batch_size + (num_samples,))\n\n  # Interpolate into the step function.\n  samples = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n  return samples"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 0:\n    raise ValueError(f'num_samples must be > 0, is {num_samples}.')\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # If we're deterministic, sample from a uniform distribution in [0, 1).\n  if rng is None:\n    u = jnp.linspace(eps, 1 - eps, num_samples, endpoint=False)\n  else:\n    u = jax.random.uniform(rng, (num_samples,))\n\n  # Jitter the samples.\n  if single_jitter:\n    u = u + jnp.arange(num_samples) / num_samples\n  else:\n    u = u + jax.random.uniform(rng, (num_samples,)) / num_samples\n\n  # Interpolate into the inverse CDF.\n  t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n\n  # If we're deterministic, we can be more precise by centering the samples\n  # around the midpoints of the intervals.\n  if deterministic_center:\n    t_new = (t_new[Ellipsis, 1:] + t_new[Ellipsis, :-1]) / 2\n\n  return t_new"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 0:\n    raise ValueError(f'num_samples must be > 0, is {num_samples}.')\n  if deterministic_center:\n    # If we're sampling deterministically, we just sample uniformly over the\n    # entire PDF.\n    return jnp.linspace(t[..., 0], t[..., -1], num=num_samples, endpoint=True)\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Sample from the CDF.\n  u = jax.random.uniform(rng, (num_samples,))\n  if single_jitter:\n    # Jitter all samples by the same amount.\n    u += jnp.linspace(0, 1, num=num_samples, endpoint=True) / num_samples\n  else:\n    # Jitter each sample independently.\n    u += jax.random.uniform(rng, (num_samples,)) / num_samples\n  u = jnp.clip(u, 0, 1 - eps)\n\n  # Interpolate into the inverse CDF.\n  t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n  return t_new"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  batch_size = t.shape[:-1]\n  num_bins = t.shape[-1] - 1\n  num_samples = num_samples\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Sample from the CDF using uniform sampling.\n  u = jax.random.uniform(rng, list(batch_size) + [num_samples])\n  if deterministic_center:\n    # Center the samples in each interval.\n    u = u + (jnp.arange(num_samples) / num_samples)\n\n  # Interpolate into the inverse CDF.\n  t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n\n  if single_jitter:\n    # Jitter the samples by the same amount.\n    t_new = t_new + jax.random.uniform(rng, list(batch_size) + [num_samples]) * eps\n  else:\n    # Jitter the samples independently.\n    t_new = t_new + jax.random.uniform(rng, list(batch_size) + [num_samples]) * eps\n\n  # Clamp the samples to the range of the step function.\n  t_new = jnp.clip(t_new, t[Ellipsis, 0], t[Ellipsis, -1])\n\n  return t_new"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # If deterministic_center is True, we want to sample from the center of each\n  # interval. Otherwise, we want to sample uniformly in the entire interval.\n  if deterministic_center:\n    # Compute the midpoints of each interval.\n    mid = (t[Ellipsis, 1:] + t[Ellipsis, :-1]) / 2\n    # Compute the width of each interval.\n    width = jnp.diff(t)\n    # Compute the left and right boundaries of the interval.\n    left = mid - width / 2\n    right = mid + width / 2\n  else:\n    left = t[Ellipsis, :-1]\n    right = t[Ellipsis, 1:]\n\n  # If rng is None, we are doing linspace sampling.\n  if rng is None:\n    # Compute the number of samples per interval.\n    n = jnp.floor(num_samples / t.shape[-1]).astype(jnp.int32)\n    # Compute the step size for linspace.\n    step = (right - left) / n\n    # Compute the linspace samples.\n    samples = jnp.linspace(left, right, n + 1)[Ellipsis, None]\n    # If single_jitter is True, jitter each sample by the same amount.\n    if single_jitter:\n      jitter = jnp.linspace(0, 1, n + 1)[Ellipsis, None] * step\n      samples = samples + jitter\n    # If single_jitter is False, jitter each sample independently.\n    else:\n      jitter = jax.random.uniform(rng, samples.shape, minval=eps, maxval=step)\n      samples = samples + jitter\n    # Resh"}
{"namespace": "stepfun.sample", "completion": "  # Convert logits to weights.\n  w = jax.nn.softmax(w_logits, axis=-1)\n\n  # Compute the CDF of the PDF.\n  cw = integrate_weights(w)\n\n  # Sample uniform numbers in [0, 1).\n  u = jax.random.uniform(rng, (num_samples,))\n\n  # Jitter the uniform numbers.\n  if single_jitter:\n    u = u + jnp.arange(num_samples) * eps\n  else:\n    u = u + jnp.arange(num_samples) * eps * jnp.random.uniform(rng, (num_samples,))\n\n  # Invert the CDF to get samples.\n  t_new = invert_cdf(u, t, w_logits)\n\n  # If deterministic_center is True, shift the samples to be centered around the\n  # bin endpoints.\n  if deterministic_center:\n    t_new = t_new + (t[Ellipsis, 1:] + t[Ellipsis, :-1]) / 2\n\n  return t_new"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 1:\n    raise ValueError(f'num_samples must be > 1, is {num_samples}.')\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # If we're deterministic, we'll sample from the uniform distribution in [0, 1).\n  if rng is None:\n    u = jnp.linspace(eps, 1 - eps, num_samples, endpoint=False)\n  else:\n    u = jax.random.uniform(rng, (num_samples,))\n\n  # If we're jittering, we'll use the inverse CDF to jitter the samples.\n  if single_jitter:\n    t_new = invert_cdf(u, t, w_logits)\n  else:\n    t_new = jax.vmap(invert_cdf, in_axes=(None, None, 0))(\n        u, t, w_logits\n    )\n\n  # If we're centering, we'll shift the samples to the center of each interval.\n  if deterministic_center:\n    t_new = (t_new[..., :-1] + t_new[..., 1:]) / 2\n\n  return t_new"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[..., :-1] + t_samples[..., 1:]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples = jnp.concatenate(\n      [\n          jnp.broadcast_to(domain[0], t_samples.shape[:-1] + (1,)),\n          t_midpoints,\n          jnp.broadcast_to(domain[1], t_samples.shape[:-1] + (1,)),\n      ],\n      axis=-1,\n  )\n\n  return t_samples"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[Ellipsis, :-1] + t_samples[Ellipsis, 1:]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples = jnp.concatenate(\n      [\n          jnp.array([domain[0]]),\n          t_midpoints,\n          jnp.array([domain[1]]),\n      ],\n      axis=-1,\n  )\n\n  return t_samples"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[..., 1:] + t_samples[..., :-1]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples = jnp.concatenate(\n      [\n          jnp.broadcast_to(domain[0], t_samples.shape[:-1] + (1,)),\n          t_midpoints,\n          jnp.broadcast_to(domain[1], t_samples.shape[:-1] + (1,)),\n      ],\n      axis=-1,\n  )\n\n  return t_samples"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate the midpoints between adjacent samples.\n  t_midpoints = (t_samples[Ellipsis, :-1] + t_samples[Ellipsis, 1:]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples = jnp.concatenate(\n      [\n          jnp.array([domain[0]]),\n          t_midpoints,\n          jnp.array([domain[1]]),\n      ],\n      axis=-1,\n  )\n\n  return t_samples"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[..., 1:] + t_samples[..., :-1]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples = jnp.concatenate(\n      [\n          jnp.full(t_samples.shape[:-1] + (1,), domain[0]),\n          t_samples,\n          jnp.full(t_samples.shape[:-1] + (1,), domain[1]),\n      ],\n      axis=-1,\n  )\n  t_midpoints = jnp.concatenate(\n      [\n          jnp.full(t_midpoints.shape[:-1] + (1,), domain[0]),\n          t_midpoints,\n          jnp.full(t_midpoints.shape[:-1] + (1,), domain[1]),\n      ],\n      axis=-1,\n  )\n\n  return t_samples, t_midpoints"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter=single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[Ellipsis, :-1] + t_samples[Ellipsis, 1:]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples = jnp.concatenate(\n      [jnp.full(t_samples.shape[:-1] + (1,), domain[0]), t_midpoints,\n       jnp.full(t_samples.shape[:-1] + (1,), domain[1])],\n      axis=-1,\n  )\n\n  return t_samples"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  t_samples_mid = (t_samples[..., :-1] + t_samples[..., 1:]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples_mid = jnp.concatenate(\n      [\n          jnp.maximum(t_samples_mid[..., :1], domain[0]),\n          t_samples_mid,\n          jnp.minimum(t_samples_mid[..., -1:], domain[1]),\n      ],\n      axis=-1,\n  )\n\n  # Return the sampled intervals.\n  return t_samples_mid"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Draw uniform samples.\n  if rng is None:\n    u = jnp.linspace(0, 1, num_samples)\n  else:\n    u = jax.random.uniform(rng, t.shape[:-1] + (num_samples,))\n\n  # Invert the CDF to get the midpoints of the intervals.\n  t_samples = invert_cdf(u, t, w_logits)\n\n  # Calculate the midpoints between adjacent samples.\n  t_samples_midpoints = (t_samples[..., :-1] + t_samples[..., 1:]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples_midpoints = jnp.concatenate(\n      [\n          jnp.full(t_samples_midpoints.shape[:-1] + (1,), domain[0]),\n          t_samples_midpoints,\n          jnp.full(t_samples_midpoints.shape[:-1] + (1,), domain[1]),\n      ],\n      axis=-1,\n  )\n\n  return t_samples_midpoints"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter=single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  t_samples_mid = (t_samples[Ellipsis, :-1] + t_samples[Ellipsis, 1:]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples_mid = jnp.concatenate(\n      [\n          jnp.full(t_samples_mid.shape[:-1] + (1,), domain[0]),\n          t_samples_mid,\n          jnp.full(t_samples_mid.shape[:-1] + (1,), domain[1]),\n      ],\n      axis=-1,\n  )\n\n  # Return the sampled intervals.\n  return t_samples_mid"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter=single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  midpoints = (t_samples[..., :-1] + t_samples[..., 1:]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples = jnp.concatenate(\n      [\n          jnp.maximum(domain[0], t_samples[..., :1]),\n          midpoints,\n          jnp.minimum(domain[1], t_samples[..., -1:]),\n      ],\n      axis=-1,\n  )\n\n  return t_samples"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function.\n  ts = sample(rng, t, w_logits, num_samples, single_jitter=single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  ts = (ts[..., :-1] + ts[..., 1:]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  ts = jnp.concatenate([jnp.full(ts.shape[:-1] + (1,), domain[0]), ts], axis=-1)\n  ts = jnp.concatenate([ts, jnp.full(ts.shape[:-1] + (1,), domain[1])], axis=-1)\n\n  return ts"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Draw uniform samples.\n  if rng is None:\n    # Match the behavior of jax.random.uniform() by spanning [0, 1-eps].\n    u = jnp.linspace(0, 1.0 - eps, num_samples)\n    u = jnp.broadcast_to(u, t.shape[:-1] + (num_samples,))\n  else:\n    # `u` is in [0, 1) --- it can be zero, but it can never be 1.\n    u_max = eps + (1 - eps) / num_samples\n    max_jitter = (1 - u_max) / (num_samples - 1) - eps\n    d = 1 if single_jitter else num_samples\n    u = jnp.linspace(0, 1.0 - u_max, num_samples) + jax.random.uniform(\n        rng, t.shape[:-1] + (d,), maxval=max_jitter\n    )\n\n  # Invert the CDF to get the samples.\n  t_samples = invert_cdf(u, t, w_logits)\n\n  # Calculate the midpoints of adjacent intervals.\n  t_midpoints = (t_samples[Ellipsis, :-1] + t_samples[Ellipsis, 1:]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples = jnp.concatenate(\n      [\n          jnp.broadcast_to(domain[0], t_samples.shape[:-1] + (1,)),\n          t_midpoints,\n          jnp.broadcast_to(domain[1], t_samples.shape[:-1] + (1,)),\n      ],\n      axis=-1,\n  )\n\n  return t_samples"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Draw uniform samples.\n  if rng is None:\n    # Match the behavior of jax.random.uniform() by spanning [0, 1-eps].\n    u = jnp.linspace(0, 1.0 - eps, num_samples)\n    u = jnp.broadcast_to(u, t.shape[:-1] + (num_samples,))\n  else:\n    # `u` is in [0, 1) --- it can be zero, but it can never be 1.\n    u_max = eps + (1 - eps) / num_samples\n    max_jitter = (1 - u_max) / (num_samples - 1) - eps\n    d = 1 if single_jitter else num_samples\n    u = jnp.linspace(0, 1.0 - u_max, num_samples) + jax.random.uniform(\n        rng, t.shape[:-1] + (d,), maxval=max_jitter\n    )\n\n  # Invert the CDF.\n  t_samples = invert_cdf(u, t, w_logits)\n\n  # Compute midpoints of adjacent samples.\n  t_samples = (t_samples[Ellipsis, :-1] + t_samples[Ellipsis, 1:]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples = jnp.concatenate(\n      [\n          jnp.full(t_samples.shape[:-1] + (1,), domain[0]),\n          t_samples,\n          jnp.full(t_samples.shape[:-1] + (1,), domain[1]),\n      ],\n      axis=-1,\n  )\n\n  return t_samples"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample points from the step function.\n  t_samples = sample(\n      rng, t, w_logits, num_samples, single_jitter=single_jitter\n  )\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[..., 1:] + t_samples[..., :-1]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the domain.\n  t_samples = jnp.concatenate(\n      [\n          jnp.expand_dims(domain[0], axis=-1),\n          t_midpoints,\n          jnp.expand_dims(domain[1], axis=-1),\n      ],\n      axis=-1,\n  )\n\n  return t_samples"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Draw uniform samples.\n  if rng is None:\n    u = jnp.linspace(0, 1 - jnp.finfo(jnp.float32).eps, num_samples)\n    u = jnp.broadcast_to(u, t.shape[:-1] + (num_samples,))\n  else:\n    u_max = jnp.finfo(jnp.float32).eps + (1 - jnp.finfo(jnp.float32).eps) / num_samples\n    max_jitter = (1 - u_max) / (num_samples - 1) - jnp.finfo(jnp.float32).eps\n    d = 1 if single_jitter else num_samples\n    u = jnp.linspace(0, 1 - u_max, num_samples) + jax.random.uniform(\n        rng, t.shape[:-1] + (d,), maxval=max_jitter\n    )\n\n  # Invert the CDF.\n  t_samples = invert_cdf(u, t, w_logits)\n\n  # Calculate midpoints between adjacent samples.\n  t_samples = (t_samples[..., :-1] + t_samples[..., 1:]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples = jnp.concatenate([t[Ellipsis, :1], t_samples, t[Ellipsis, -1:]], axis=-1)\n  t_samples = jnp.clip(t_samples, *domain)\n\n  return t_samples\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample the step function to get a set of points.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate the midpoints between adjacent samples.\n  t_midpoints = (t_samples[..., :-1] + t_samples[..., 1:]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples = jnp.concatenate(\n      [\n          jnp.full_like(t_samples[..., :1], domain[0]),\n          t_samples,\n          jnp.full_like(t_samples[..., -1:], domain[1]),\n      ],\n      axis=-1,\n  )\n  t_midpoints = jnp.concatenate(\n      [\n          jnp.full_like(t_midpoints[..., :1], domain[0]),\n          t_midpoints,\n          jnp.full_like(t_midpoints[..., -1:], domain[1]),\n      ],\n      axis=-1,\n  )\n\n  # Return the midpoints as the sampled intervals.\n  return t_midpoints"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[Ellipsis, 1:] + t_samples[Ellipsis, :-1]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples = jnp.concatenate(\n      [\n          jnp.expand_dims(domain[0], axis=-1),\n          t_midpoints,\n          jnp.expand_dims(domain[1], axis=-1),\n      ],\n      axis=-1,\n  )\n\n  return t_samples\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample points from the step function\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples\n  midpoints = (t_samples[..., :-1] + t_samples[..., 1:]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain\n  if domain[0] != -jnp.inf:\n    first_interval = jnp.where(\n        t_samples[..., 0] < domain[0],\n        domain[0] - t_samples[..., 0],\n        jnp.zeros_like(t_samples[..., 0]),\n    )\n    t_samples = t_samples + first_interval[..., None]\n    midpoints = midpoints + first_interval[..., None]\n  if domain[1] != jnp.inf:\n    last_interval = jnp.where(\n        t_samples[..., -1] > domain[1],\n        t_samples[..., -1] - domain[1],\n        jnp.zeros_like(t_samples[..., -1]),\n    )\n    t_samples = t_samples - last_interval[..., None]\n    midpoints = midpoints - last_interval[..., None]\n\n  return midpoints"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Draw uniform samples.\n  if rng is None:\n    u = jnp.linspace(0, 1.0, num_samples)\n    u = jnp.broadcast_to(u, t.shape[:-1] + (num_samples,))\n  else:\n    u_max = 1.0 / num_samples\n    max_jitter = (1 - u_max) / (num_samples - 1)\n    d = 1 if single_jitter else num_samples\n    u = jnp.linspace(0, 1.0 - u_max, num_samples) + jax.random.uniform(\n        rng, t.shape[:-1] + (d,), maxval=max_jitter\n    )\n\n  # Invert the CDF to get the endpoints of the sampled intervals.\n  t_samples = invert_cdf(u, t, w_logits)\n\n  # Calculate the midpoints of the sampled intervals.\n  t_samples_mid = (t_samples[Ellipsis, 1:] + t_samples[Ellipsis, :-1]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples_mid = jnp.concatenate(\n      [\n          jnp.repeat(domain[0], repeats=1, axis=-1),\n          t_samples_mid,\n          jnp.repeat(domain[1], repeats=1, axis=-1),\n      ],\n      axis=-1,\n  )\n\n  return t_samples_mid"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[Ellipsis, 1:] + t_samples[Ellipsis, :-1]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples = jnp.concatenate(\n      [\n          jnp.array([domain[0]]),\n          t_midpoints,\n          jnp.array([domain[1]]),\n      ],\n      axis=-1,\n  )\n\n  return t_samples\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  w_integrated = integrate_weights(w)\n  return math.sorted_interp(ps, w_integrated, t, utils.device_is_tpu())"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n\n  # Integrate the weights to get the CDF.\n  cdf = integrate_weights(w)\n\n  # Interpolate the CDF to get the percentiles.\n  percentiles = jnp.interp(ps, cdf, t)\n\n  return percentiles"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n\n  # Integrate the weights to get the CDF.\n  cw = integrate_weights(w)\n\n  # Interpolate into the CDF to get the percentiles.\n  ps_new = math.sorted_interp(ps, cw, t, utils.device_is_tpu())\n\n  return ps_new"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Integrate the weights to get the CDF.\n  cw = integrate_weights(w)\n  # Interpolate into the CDF to get the weighted percentiles.\n  ps_new = math.sorted_interp(ps / 100, cw, t, utils.device_is_tpu())\n  return ps_new"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_percentiles(ps)\n\n  # Integrate the weights to obtain the CDF.\n  cdf = integrate_weights(w)\n\n  # Interpolate the CDF to get the weighted percentiles.\n  weighted_percentiles = jnp.interp(ps / 100, cdf, t)\n\n  return weighted_percentiles"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_percentiles(ps)\n\n  # Integrate the weights to get the cumulative distribution function.\n  cdf = integrate_weights(w)\n\n  # Interpolate the cumulative distribution function to get the percentiles.\n  percentiles = math.sorted_interp(ps / 100, cdf, t, utils.device_is_tpu())\n\n  return percentiles"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n\n  # Ensure that the weights sum to 1.\n  w_sum = jnp.sum(w, axis=-1, keepdims=True)\n  w = w / w_sum\n\n  # Integrate the weights to obtain the cumulative distribution function (CDF).\n  cdf = jnp.cumsum(w, axis=-1)\n\n  # Compute the weighted percentiles by interpolating into the CDF.\n  percentiles = jnp.interp(ps, cdf, t)\n\n  return percentiles"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Ensure that the weights sum to 1.\n  w = w / jnp.sum(w, axis=-1, keepdims=True)\n\n  # Compute the integrated weights.\n  cw = integrate_weights(w)\n\n  # Interpolate into the CDF to get the percentiles.\n  ps_new = math.sorted_interp(ps / 100, cw, t, utils.device_is_tpu())\n  return ps_new"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n\n  # Ensure that the weights sum to 1.\n  w = jnp.where(jnp.sum(w, axis=-1, keepdims=True) < 1, w / jnp.sum(w, axis=-1, keepdims=True), w)\n\n  # Compute the integrated weights.\n  w_integrated = jnp.cumsum(w, axis=-1)\n\n  # Compute the weighted percentiles.\n  percentiles = jnp.interp(ps, w_integrated, t)\n\n  return percentiles"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  utils.assert_valid_percentiles(ps)\n\n  # Normalize the weights to ensure they sum to 1.\n  w = w / jnp.sum(w, axis=-1, keepdims=True)\n\n  # Integrate the normalized weights to obtain the cumulative distribution function (CDF).\n  cdf = jnp.cumsum(w, axis=-1)\n\n  # Compute the weighted percentiles by interpolating into the CDF.\n  weighted_percentiles = jnp.interp(ps / 100, cdf, t)\n\n  return weighted_percentiles"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  w_int = integrate_weights(w)\n  ps = jnp.broadcast_to(ps, w_int.shape[:-1] + (len(ps),))\n  ps = jnp.sort(ps, axis=-1)\n  ps = jnp.concatenate([jnp.zeros_like(ps[..., :1]), ps, jnp.ones_like(ps[..., -1:])], axis=-1)\n  ps = jnp.clip(ps, 0, 1)\n  return jnp.vectorize(jnp.interp, signature='(n),(n),(m)->(m)')(t, w_int, ps)"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n\n  # Ensure that the weights sum to 1.\n  w = jnp.where(jnp.abs(jnp.sum(w, axis=-1)) < 1e-6, w / jnp.sum(w, axis=-1), w)\n\n  # Integrate the weights to get the CDF.\n  cdf = integrate_weights(w)\n\n  # Interpolate the CDF to get the weighted percentiles.\n  return math.sorted_interp(ps / 100, cdf, t)"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  ps = jnp.asarray(ps)\n\n  # Ensure that the weights sum to 1.\n  w = jnp.asarray(w)\n  w = w / jnp.sum(w, axis=-1, keepdims=True)\n\n  # Compute the cumulative sum of the weights.\n  cw = jnp.cumsum(w, axis=-1)\n\n  # Interpolate the cumulative sum of the weights to find the corresponding y-values for the given percentiles.\n  y_values = jnp.interp(ps, cw, t)\n\n  return y_values"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  ps = jnp.asarray(ps)\n  if ps.ndim == 0:\n    ps = ps[None]\n\n  # Compute the integrated weights.\n  acc = jnp.cumsum(w, axis=-1)\n  acc0 = jnp.concatenate([jnp.zeros(acc.shape[:-1] + (1,)), acc], axis=-1)\n\n  # Compute the weighted percentiles.\n  ps_acc = jnp.interp(ps / 100, acc0, t)\n\n  return ps_acc"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n\n  # Integrate the weights and normalize to 1.\n  w_int = integrate_weights(w)\n  w_int /= jnp.sum(w_int, axis=-1, keepdims=True)\n\n  # Interpolate into the integrated weights to get the weighted percentiles.\n  w_int_interp = jnp.vectorize(jnp.interp, signature='(n),(n)->(m)')(\n      ps, t, w_int\n  )\n\n  return w_int_interp"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Normalize the weights to ensure they sum to 1.\n  w_norm = jnp.where(jnp.sum(w, axis=-1, keepdims=True) > 0, w / jnp.sum(w, axis=-1, keepdims=True), w)\n\n  # Integrate the normalized weights to get the cumulative distribution.\n  cdf = jnp.cumsum(w_norm, axis=-1)\n\n  # Interpolate the percentiles to get the corresponding x-values.\n  t_interp = jnp.interp(ps, cdf, t)\n\n  return t_interp"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n\n  # Ensure that the weights sum to 1.\n  w = jnp.where(jnp.isnan(w), 0, w)\n  w = jnp.where(w < 0, 0, w)\n  w = jnp.where(w > 1, 1, w)\n  w_sum = jnp.sum(w, axis=-1, keepdims=True)\n  w = w / w_sum\n\n  # Compute the integrated weights.\n  acc = jnp.cumsum(w, axis=-1)\n  acc0 = jnp.concatenate([jnp.zeros(acc.shape[:-1] + (1,)), acc], axis=-1)\n\n  # Interpolate into the integrated weights to get the weighted percentiles.\n  wp = jnp.vectorize(jnp.interp, signature='(n),(m),(m)->(n)')(t, t, acc0)\n  wp = jnp.where(wp < 0, 0, wp)\n  wp = jnp.where(wp > 1, 1, wp)\n  return wp"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  w_int = integrate_weights(w)\n  ps = jnp.asarray(ps)\n  if ps.ndim == 0:\n    ps = ps[None]\n  if ps.ndim != 1:\n    raise ValueError('ps must be a scalar or 1D array.')\n\n  # Clip percentiles to [0, 100].\n  ps = jnp.clip(ps, 0, 100)\n\n  # Interpolate the percentiles into the integrated weights.\n  ps_int = jnp.interp(ps, jnp.arange(101), w_int)\n\n  # Interpolate the percentiles into the original step function.\n  ps_out = jnp.interp(ps_int, w_int, t)\n\n  return ps_out"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  w_integrated = integrate_weights(w)\n  ps = jnp.array(ps)\n  ps = jnp.maximum(0, jnp.minimum(100, ps))\n  ps = ps / 100\n  return jnp.interp(ps, w_integrated[Ellipsis, 1:], t[Ellipsis, 1:])\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n\n  # Ensure that the weights sum to 1.\n  w = jnp.maximum(0, w)\n  w_sum = jnp.sum(w, axis=-1)\n  w_sum = jnp.maximum(1e-6, w_sum)\n  w = w / w_sum[..., None]\n\n  # Compute the cumulative sum of the weights.\n  cw = jnp.cumsum(w, axis=-1)\n\n  # Interpolate the cumulative sum of the weights to get the percentiles.\n  ps = jnp.maximum(0, ps)\n  ps = jnp.minimum(100, ps)\n  ps = ps / 100\n  ps = jnp.broadcast_to(ps, t.shape[:-1] + (len(ps),))\n  ps = math.sorted_interp(ps, cw, t, utils.device_is_tpu())\n\n  return ps"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  \"\"\"\n  Blurs a given histogram represented by time points and weights, then resamples it based on a new set of query time points. The process involves converting the histogram to a probability density function (PDF), blurring the PDF, and then resampling it to match the new time points.\n\n  Input-Output Arguments\n  :param tq: Array-like. The new time points at which the histogram is to be resampled.\n  :param t: Array-like. The original time points of the histogram.\n  :param w: Array-like. The weights or values associated with the original time points.\n  :param blur_halfwidth: Numeric. The half-width of the blur operation, determining how much the original histogram is smoothed.\n  :return: Array-like. The resampled weights corresponding to the new time points `tq`.\n  \"\"\"\n\n  \"\"\"\n  Blurs a given histogram represented by time points and weights, then resamples it based on a new set of query time points. The process involves converting the histogram to a probability density function (PDF), blurring the PDF, and then resampling it to match the new time points.\n\n  Input-Output Arguments\n  :param tq: Array-like. The new time points at which the histogram is to be resampled.\n  :param t: Array-like. The original time points of the histogram.\n  :param w: Array-like. The weights or values associated with the original time points.\n  :param blur_halfwidth: Numeric. The half-width of the blur operation, determining how much the original histogram is smoothed.\n  :return: Array-like. The resampled weights corresponding to the new time points `tq`.\n  \"\"\"\n\n  \"\"\"\n  Blurs a given histogram represented by time points and weights, then resamples it based on a new set of query time points. The process involves converting the histogram to a probability density function (PDF), blurring the PDF, and then resampling it to match the new time points.\n\n  Input-Output Arguments\n  :param tq: Array-like. The new time points at which the histogram"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  \"\"\"\n  Blurs a given histogram represented by time points and weights, then resamples it based on a new set of query time points. The process involves converting the histogram to a probability density function (PDF), blurring the PDF, and then resampling it to match the new time points.\n\n  Input-Output Arguments\n  :param tq: Array-like. The new time points at which the histogram is to be resampled.\n  :param t: Array-like. The original time points of the histogram.\n  :param w: Array-like. The weights or values associated with the original time points.\n  :param blur_halfwidth: Numeric. The half-width of the blur operation, determining how much the original histogram is smoothed.\n  :return: Array-like. The resampled weights corresponding to the new time points `tq`.\n  \"\"\"\n\n  \"\"\"\n  Blurs a given histogram represented by time points and weights, then resamples it based on a new set of query time points. The process involves converting the histogram to a probability density function (PDF), blurring the PDF, and then resampling it to match the new time points.\n\n  Input-Output Arguments\n  :param tq: Array-like. The new time points at which the histogram is to be resampled.\n  :param t: Array-like. The original time points of the histogram.\n  :param w: Array-like. The weights or values associated with the original time points.\n  :param blur_halfwidth: Numeric. The half-width of the blur operation, determining how much the original histogram is smoothed.\n  :return: Array-like. The resampled weights corresponding to the new time points `tq`.\n  \"\"\"\n\n  \"\"\"\n  Blurs a given histogram represented by time points and weights, then resamples it based on a new set of query time points. The process involves converting the histogram to a probability density function (PDF), blurring the PDF, and then resampling it to match the new time points.\n\n  Input-Output Arguments\n  :param tq: Array-like. The new time points at which the histogram"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  \"\"\"\n  Blurs a given histogram represented by time points and weights, then resamples it based on a new set of query time points. The process involves converting the histogram to a probability density function (PDF), blurring the PDF, and then resampling it to match the new time points.\n\n  Input-Output Arguments\n  :param tq: Array-like. The new time points at which the histogram is to be resampled.\n  :param t: Array-like. The original time points of the histogram.\n  :param w: Array-like. The weights or values associated with the original time points.\n  :param blur_halfwidth: Numeric. The half-width of the blur operation, determining how much the original histogram is smoothed.\n  :return: Array-like. The resampled weights corresponding to the new time points `tq`.\n  \"\"\"\n\n  \"\"\"\n  Blurs a given histogram represented by time points and weights, then resamples it based on a new set of query time points. The process involves converting the histogram to a probability density function (PDF), blurring the PDF, and then resampling it to match the new time points.\n\n  Input-Output Arguments\n  :param tq: Array-like. The new time points at which the histogram is to be resampled.\n  :param t: Array-like. The original time points of the histogram.\n  :param w: Array-like. The weights or values associated with the original time points.\n  :param blur_halfwidth: Numeric. The half-width of the blur operation, determining how much the original histogram is smoothed.\n  :return: Array-like. The resampled weights corresponding to the new time points `tq`.\n  \"\"\"\n\n  \"\"\"\n  Blurs a given histogram represented by time points and weights, then resamples it based on a new set of query time points. The process involves converting the histogram to a probability density function (PDF), blurring the PDF, and then resampling it to match the new time points.\n\n  Input-Output Arguments\n  :param tq: Array-like. The new time points at which the histogram"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  \"\"\"\n  Blurs a given histogram represented by time points and weights, then resamples it based on a new set of query time points. The process involves converting the histogram to a probability density function (PDF), blurring the PDF, and then resampling it to match the new time points.\n\n  Input-Output Arguments\n  :param tq: Array-like. The new time points at which the histogram is to be resampled.\n  :param t: Array-like. The original time points of the histogram.\n  :param w: Array-like. The weights or values associated with the original time points.\n  :param blur_halfwidth: Numeric. The half-width of the blur operation, determining how much the original histogram is smoothed.\n  :return: Array-like. The resampled weights corresponding to the new time points `tq`.\n  \"\"\"\n\n  \"\"\"\n  Blurs a given histogram represented by time points and weights, then resamples it based on a new set of query time points. The process involves converting the histogram to a probability density function (PDF), blurring the PDF, and then resampling it to match the new time points.\n\n  Input-Output Arguments\n  :param tq: Array-like. The new time points at which the histogram is to be resampled.\n  :param t: Array-like. The original time points of the histogram.\n  :param w: Array-like. The weights or values associated with the original time points.\n  :param blur_halfwidth: Numeric. The half-width of the blur operation, determining how much the original histogram is smoothed.\n  :return: Array-like. The resampled weights corresponding to the new time points `tq`.\n  \"\"\"\n\n  \"\"\"\n  Blurs a given histogram represented by time points and weights, then resamples it based on a new set of query time points. The process involves converting the histogram to a probability density function (PDF), blurring the PDF, and then resampling it to match the new time points.\n\n  Input-Output Arguments\n  :param tq: Array-like. The new time points at which the histogram"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  \"\"\"\n  Blurs a given histogram represented by time points and weights, then resamples it based on a new set of query time points. The process involves converting the histogram to a probability density function (PDF), blurring the PDF, and then resampling it to match the new time points.\n\n  Input-Output Arguments\n  :param tq: Array-like. The new time points at which the histogram is to be resampled.\n  :param t: Array-like. The original time points of the histogram.\n  :param w: Array-like. The weights or values associated with the original time points.\n  :param blur_halfwidth: Numeric. The half-width of the blur operation, determining how much the original histogram is smoothed.\n  :return: Array-like. The resampled weights corresponding to the new time points `tq`.\n  \"\"\"\n\n  \"\"\"\n  Blurs a given histogram represented by time points and weights, then resamples it based on a new set of query time points. The process involves converting the histogram to a probability density function (PDF), blurring the PDF, and then resampling it to match the new time points.\n\n  Input-Output Arguments\n  :param tq: Array-like. The new time points at which the histogram is to be resampled.\n  :param t: Array-like. The original time points of the histogram.\n  :param w: Array-like. The weights or values associated with the original time points.\n  :param blur_halfwidth: Numeric. The half-width of the blur operation, determining how much the original histogram is smoothed.\n  :return: Array-like. The resampled weights corresponding to the new time points `tq`.\n  \"\"\"\n\n  \"\"\"\n  Blurs a given histogram represented by time points and weights, then resamples it based on a new set of query time points. The process involves converting the histogram to a probability density function (PDF), blurring the PDF, and then resampling it to match the new time points.\n\n  Input-Output Arguments\n  :param tq: Array-like. The new time points at which the histogram"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the weights to a PDF.\n  p = pdf_to_weight(t, w)\n\n  # Blur the PDF using a Gaussian kernel.\n  p_blurred = linspline.blur_pdf(t, p, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points.\n  w_resampled = resample(tq, t, p_blurred)\n\n  return w_resampled"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a PDF\n  p = weight_to_pdf(t, w)\n\n  # Blur the PDF using a Gaussian kernel\n  p_blurred = linspline.gaussian_kernel_blur(p, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points\n  w_resampled = pdf_to_weight(tq, p_blurred)\n\n  return w_resampled"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  p = pdf_to_weight(t, w)\n\n  # Blur the PDF using a Gaussian kernel\n  p_blurred = linspline.blur(p, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points `tq`\n  w_blurred = resample(tq, t, p_blurred)\n\n  return w_blurred"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  p = pdf_to_weight(t, w)\n\n  # Blur the PDF using a Gaussian kernel\n  p_blurred = linspline.gaussian_filter(p, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points `tq`\n  w_resampled = resample(tq, t, p_blurred)\n\n  return w_resampled"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  p = weight_to_pdf(t, w)\n\n  # Blur the PDF using a Gaussian kernel with the specified half-width\n  p_blurred = linspline.blur_pdf(p, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points `tq`\n  w_resampled = pdf_to_weight(tq, p_blurred)\n\n  return w_resampled"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a PDF\n  p = pdf_to_weight(t, w)\n\n  # Blur the PDF using a Gaussian kernel\n  p = linspline.gaussian_kernel(p, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points `tq`\n  wq = resample(tq, t, p)\n\n  return wq"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the weights to a PDF\n  p = weight_to_pdf(t, w)\n\n  # Blur the PDF using a Gaussian kernel\n  p_blurred = linspline.gaussian_blur_1d(p, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points\n  w_resampled = pdf_to_weight(tq, p_blurred)\n\n  return w_resampled"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Compute the PDF from the weights.\n  p = weight_to_pdf(t, w)\n\n  # Blur the PDF using a Gaussian kernel.\n  p_blurred = linspline.blur_and_resample(t, p, tq, blur_halfwidth)\n\n  # Convert the blurred PDF back to weights.\n  w_blurred = pdf_to_weight(tq, p_blurred)\n\n  return w_blurred"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Convert the weights to a PDF.\n  p = weight_to_pdf(t, w)\n  # Blur the PDF using a Gaussian kernel.\n  p_blurred = linspline.blur_pdf(t, p, blur_halfwidth)\n  # Resample the blurred PDF to match the new time points.\n  w_resampled = pdf_to_weight(tq, p_blurred)\n  return w_resampled"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a PDF\n  p = weight_to_pdf(t, w)\n\n  # Blur the PDF using a Gaussian kernel\n  blurred_p = linspline.gaussian_kernel_blur(p, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points\n  resampled_w = pdf_to_weight(tq, blurred_p)\n\n  return resampled_w"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Compute the PDF and CDF for each weight vector.\n  p = jax.nn.softmax(w, axis=-1)\n  cw = integrate_weights(p)\n  # Interpolate into the inverse CDF.\n  t_new = math.sorted_interp(tq, cw, t, utils.device_is_tpu())\n  # Compute the PDF and CDF for the blurred histogram.\n  p_blurred = linspline.blur_pdf(t_new, p, blur_halfwidth)\n  cw_blurred = integrate_weights(p_blurred)\n  # Interpolate into the inverse CDF.\n  t_new_blurred = math.sorted_interp(tq, cw_blurred, t_new, utils.device_is_tpu())\n  # Compute the PDF and CDF for the blurred histogram.\n  p_blurred = linspline.blur_pdf(t_new_blurred, p_blurred, blur_halfwidth)\n  cw_blurred = integrate_weights(p_blurred)\n  # Interpolate into the inverse CDF.\n  t_new_blurred = math.sorted_interp(\n      tq, cw_blurred, t_new_blurred, utils.device_is_tpu()\n  )\n  # Compute the PDF and CDF for the blurred histogram.\n  p_blurred = linspline.blur_pdf(t_new_blurred, p_blurred, blur_halfwidth)\n  cw_blurred = integrate_weights(p_blurred)\n  # Interpolate into the inverse CDF.\n  t_new_blurred = math.sorted_interp(\n      tq, cw_blurred, t_new_blurred, utils.device_is_tpu()\n  )"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a PDF using `pdf_to_weight`\n  p = pdf_to_weight(t, w)\n\n  # Blur the PDF using `linspline.blur_pdf`\n  p_blurred = linspline.blur_pdf(t, p, blur_halfwidth)\n\n  # Resample the blurred PDF using `resample`\n  w_blurred = resample(tq, t, p_blurred)\n\n  # Return the resampled weights\n  return w_blurred"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  utils.assert_valid_stepfun(t, w)\n\n  # Compute the PDF from the weights.\n  pdf = weight_to_pdf(t, w)\n\n  # Blur the PDF.\n  blurred_pdf = linspline.blur_pdf(t, pdf, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points.\n  resampled_weights = resample(tq, t, blurred_pdf)\n\n  return resampled_weights"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Compute the PDF from the weights\n  pdf = weight_to_pdf(t, w)\n\n  # Blur the PDF using a Gaussian kernel\n  blurred_pdf = linspline.gaussian_kernel_blur(pdf, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points\n  resampled_pdf = linspline.resample_stepfun(tq, t, blurred_pdf)\n\n  # Convert the blurred and resampled PDF back to weights\n  resampled_weights = pdf_to_weight(tq, resampled_pdf)\n\n  return resampled_weights"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  utils.assert_valid_stepfun(t, w)\n  w = jnp.asarray(w)\n  t = jnp.asarray(t)\n  tq = jnp.asarray(tq)\n  w_blurred = linspline.blur_and_resample_pdf(tq, t, w, blur_halfwidth)\n  return w_blurred"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(to_homogeneous(vectors), transform))"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(to_homogeneous(vectors), transform))"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(to_homogeneous(vectors), transform))"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(to_homogeneous(vectors), transform))"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(to_homogeneous(vectors), transform.T))"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(to_homogeneous(vectors), transform.T))"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(to_homogeneous(vectors), transform))"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Convert the input vectors to homogeneous coordinates\n  vectors_homogeneous = to_homogeneous(vectors)\n\n  # Apply the transformation to the homogeneous vectors\n  transformed_vectors_homogeneous = matmul(transform, vectors_homogeneous)\n\n  # Convert the transformed vectors back to non-homogeneous coordinates\n  transformed_vectors = from_homogeneous(transformed_vectors_homogeneous)\n\n  return transformed_vectors"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  vectors_homogeneous = to_homogeneous(vectors)\n  transformed_vectors_homogeneous = matmul(transform, vectors_homogeneous)\n  return from_homogeneous(transformed_vectors_homogeneous)"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  homogeneous_vectors = to_homogeneous(vectors)\n  transformed_vectors = matmul(transform, homogeneous_vectors)\n  return from_homogeneous(transformed_vectors)"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  vectors_homogeneous = to_homogeneous(vectors)\n  transformed_homogeneous = matmul(transform, vectors_homogeneous)\n  return from_homogeneous(transformed_homogeneous)"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Convert the input vectors to homogeneous coordinates\n  vectors_homogeneous = to_homogeneous(vectors)\n\n  # Apply the homogeneous transformation to the input vectors\n  transformed_vectors_homogeneous = matmul(transform, vectors_homogeneous)\n\n  # Convert the transformed vectors back to non-homogeneous coordinates\n  transformed_vectors = from_homogeneous(transformed_vectors_homogeneous)\n\n  return transformed_vectors"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  v = to_homogeneous(vectors)\n  v = matmul(transform, v)\n  return from_homogeneous(v)"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  vectors_homogeneous = to_homogeneous(vectors)\n  transformed_vectors_homogeneous = matmul(transform,\n                                           vectors_homogeneous)\n  return from_homogeneous(transformed_vectors_homogeneous)"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  vectors = jnp.concatenate([vectors, jnp.ones_like(vectors[Ellipsis, :1])],\n                             axis=-1)\n  return jnp.einsum('...c,...cd->...d', vectors, transform)"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Convert the input array to a homogeneous representation\n  vectors_homogeneous = to_homogeneous(vectors)\n\n  # Apply the homogeneous transformation to the homogeneous representation of the input array\n  transformed_vectors_homogeneous = matmul(transform, vectors_homogeneous)\n\n  # Convert the transformed points back to non-homogeneous representation\n  transformed_vectors = from_homogeneous(transformed_vectors_homogeneous)\n\n  return transformed_vectors"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  vectors = jnp.concatenate([vectors, jnp.ones_like(vectors[Ellipsis, :1])],\n                             axis=-1)\n  return jnp.einsum('...ij,...j->...i', transform, vectors)\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Add a homogeneous coordinate of 1 to the points.\n  v_homogeneous = to_homogeneous(vectors)\n  # Apply the transformation.\n  v_transformed = matmul(transform, v_homogeneous)\n  # Remove the homogeneous coordinate.\n  return from_homogeneous(v_transformed)"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Convert the input vectors to homogeneous coordinates\n  v = to_homogeneous(vectors)\n\n  # Apply the transformation matrix to the homogeneous vectors\n  v = matmul(transform, v)\n\n  # Convert the transformed vectors back to non-homogeneous coordinates\n  v = from_homogeneous(v)\n\n  return v\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  vectors_h = to_homogeneous(vectors)\n  transformed_vectors_h = matmul(transform, vectors_h)\n  transformed_vectors = from_homogeneous(transformed_vectors_h)\n  return transformed_vectors\n\n"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n  if use_avg:\n    return linspline.blur_stepfun(t, vp, 0.5)[1]\n  else:\n    return linspline.blur_stepfun(t, vp, 0.5)[1]"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n\n  # Create a new step function by resampling the original step function into the new intervals defined by t.\n  vq = jax.vmap(jnp.interp, in_axes=(None, 0, 0))(t, tp, vp)\n\n  # If use_avg is True, average the values of the step function for each interval in t.\n  if use_avg:\n    vq = jax.vmap(jnp.mean, in_axes=(None, 0))(t, vq)\n\n  return vq"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n\n  # The new values are computed by resampling the original step function into the new intervals.\n  new_values = linspline.interpolate_stepfun(t, tp, vp)\n\n  # If use_avg is True, the new values are averaged over the new intervals.\n  if use_avg:\n    new_values = new_values / jnp.diff(t)\n\n  return new_values"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n\n  # Compute the intervals of the original step function.\n  intervals = jnp.stack([tp[Ellipsis, :-1], tp[Ellipsis, 1:]], axis=-1)\n\n  # Compute the widths of the intervals.\n  widths = jnp.diff(intervals, axis=-1)\n\n  # Compute the indices of the intervals that each new time point falls into.\n  indices = jnp.searchsorted(intervals, t[Ellipsis, None, :], side='right') - 1\n\n  # Compute the fraction of each new time point that falls into each interval.\n  fractions = (t[Ellipsis, None, :] - intervals[Ellipsis, indices, 0]) / widths[Ellipsis, indices]\n\n  # Compute the values of the step function at the new time points.\n  if use_avg:\n    values = vp[Ellipsis, indices] * fractions + vp[Ellipsis, indices + 1] * (1 - fractions)\n  else:\n    values = vp[Ellipsis, indices] * fractions * widths[Ellipsis, indices] + vp[Ellipsis, indices + 1] * (1 - fractions) * widths[Ellipsis, indices + 1]\n\n  return values"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n  if use_avg:\n    t_new = (t[..., 1:] + t[..., :-1]) / 2\n    v_new = jnp.diff(vp) / jnp.diff(t)\n  else:\n    t_new = t\n    v_new = jnp.diff(vp)\n  v_new = jnp.concatenate([vp[..., :1], v_new, vp[..., -1:]], axis=-1)\n  return jnp.vectorize(jnp.interp, signature='(n),(n),(n)->(m)')(t_new, tp, v_new)"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n\n  # Convert the step function into a PDF.\n  p = weight_to_pdf(tp, vp)\n\n  # Resample the PDF step function into a piecewise linear spline PDF.\n  t_linspline, p_linspline = linspline.resample_stepfun(t, p)\n\n  # Integrate the spline PDF, then query it to get integrated weights.\n  quad = linspline.compute_integral(t_linspline, p_linspline)\n  acc_wq = linspline.interpolate_integral(t, t_linspline, *quad)\n\n  # Undo the integration to get weights.\n  wq = jnp.diff(acc_wq, axis=-1)\n\n  # Fix negative values to 0, as they should never happen but may due to\n  # numerical issues.\n  wq = jnp.maximum(0, wq)\n\n  # If use_avg is True, return the average value of the step function within each interval.\n  if use_avg:\n    # Compute the average value of the step function within each interval.\n    avg_v = jnp.diff(acc_wq) / jnp.diff(t)\n    return avg_v\n\n  return wq"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n\n  # Compute the width of each interval.\n  dt = jnp.diff(tp)\n\n  # Compute the cumulative sum of dt.\n  cdt = jnp.cumsum(dt, axis=-1)\n\n  # Compute the cumulative sum of vp.\n  cvp = jnp.cumsum(vp, axis=-1)\n\n  # Compute the cumulative sum of dt * vp.\n  cdtvp = jnp.cumsum(dt * vp, axis=-1)\n\n  # Compute the new values of the step function.\n  if use_avg:\n    # Compute the new values of the step function by dividing the cumulative sum of dt * vp by the cumulative sum of dt.\n    vq = cdtvp / cdt\n  else:\n    # Compute the new values of the step function by dividing the cumulative sum of vp by the cumulative sum of dt.\n    vq = cvp / cdt\n\n  # Compute the new time points of the step function.\n  tq = tp[..., :-1] + jnp.diff(t) / 2\n\n  return tq, vq"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n\n  # Compute the cumulative sum of the values at the original time points.\n  vp_cumsum = jnp.cumsum(vp, axis=-1)\n\n  # Compute the cumulative sum of the values at the new time points.\n  vq_cumsum = jnp.cumsum(vp, axis=-1)\n\n  # Compute the new values by subtracting the cumulative sum at the new time points from the cumulative sum at the original time points.\n  vq = vq_cumsum - vp_cumsum\n\n  # If use_avg is True, compute the average value of the step function for each interval in t.\n  if use_avg:\n    # Compute the width of each interval in t.\n    dt = jnp.diff(t)\n\n    # Compute the average value of the step function for each interval in t.\n    vq = vq / dt\n\n  return vq"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n  utils.assert_valid_stepfun(t, tp)\n  utils.assert_valid_stepfun(t, vp)\n\n  # Compute the indices of the original time points that are within each new time point interval.\n  (i0, i1), _ = math.sorted_lookup(t, tp, (), utils.device_is_tpu())\n\n  # Sum or average the values within each interval.\n  if use_avg:\n    vq = jnp.sum(vp[Ellipsis, i0:i1], axis=-1) / jnp.diff(t)\n  else:\n    vq = jnp.sum(vp[Ellipsis, i0:i1], axis=-1)\n\n  return vq"}
{"namespace": "stepfun.resample", "completion": "  # Check input arguments\n  if not isinstance(t, jnp.ndarray):\n    raise TypeError(\"t must be a jax.numpy.ndarray\")\n  if not isinstance(tp, jnp.ndarray):\n    raise TypeError(\"tp must be a jax.numpy.ndarray\")\n  if not isinstance(vp, jnp.ndarray):\n    raise TypeError(\"vp must be a jax.numpy.ndarray\")\n  if not isinstance(use_avg, bool):\n    raise TypeError(\"use_avg must be a bool\")\n\n  # Check if t and tp are sorted\n  if not jnp.all(jnp.diff(t) >= 0):\n    raise ValueError(\"t must be sorted\")\n  if not jnp.all(jnp.diff(tp) >= 0):\n    raise ValueError(\"tp must be sorted\")\n\n  # Check if t and tp have the same last dimension\n  if t.shape[-1] != tp.shape[-1]:\n    raise ValueError(\"t and tp must have the same last dimension\")\n\n  # Check if t and tp have the same first dimensions\n  if t.shape[:-1] != tp.shape[:-1]:\n    raise ValueError(\"t and tp must have the same first dimensions\")\n\n  # Check if t and tp have the same first dimensions\n  if t.shape[:-1] != vp.shape[:-1]:\n    raise ValueError(\"t and vp must have the same first dimensions\")\n\n  # Check if t and tp have the same last dimension\n  if t.shape[-1] != vp.shape[-1]:\n    raise ValueError(\"t and vp must have the same last dimension\")\n\n  # Check if t and tp have the same last dimension\n  if t.shape[-1] < 2:\n    raise ValueError(\"t must have at least 2 elements\")\n\n  # Check if t and tp have the same last dimension\n  if tp.shape[-1] < 2:\n    raise ValueError(\"tp must have at least"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n  tp = tp.squeeze()\n  vp = vp.squeeze()\n  t = t.squeeze()\n\n  # Compute the width of each interval in t.\n  dt = jnp.diff(t)\n\n  # Compute the indices of the intervals in tp that correspond to each interval in t.\n  i = jnp.searchsorted(tp, t)\n\n  # Compute the values of the step function at the new time points t.\n  if use_avg:\n    v = jnp.zeros_like(t)\n    v = v.at[Ellipsis, 1:].set(jnp.diff(vp[i]) / dt)\n  else:\n    v = jnp.zeros_like(t)\n    v = v.at[Ellipsis, 1:].set(jnp.diff(vp[i]) * dt)\n\n  return v"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n  if use_avg:\n    # Compute the width of each interval in `t`.\n    dt = jnp.diff(t)\n\n    # Compute the indices of the intervals in `t` that each value in `tp` belongs to.\n    i0, i1 = math.sorted_lookup(tp, t, utils.device_is_tpu())\n\n    # Compute the weights for each value in `tp` based on the width of its interval in `t`.\n    w = jnp.where(i1 == i0, dt[i0], 0)\n\n    # Compute the average value of each interval in `t`.\n    v = jnp.sum(w * vp[Ellipsis, i0], axis=-1) / jnp.sum(w, axis=-1)\n  else:\n    # Compute the indices of the intervals in `t` that each value in `tp` belongs to.\n    i0, i1 = math.sorted_lookup(tp, t, utils.device_is_tpu())\n\n    # Compute the width of each interval in `t`.\n    dt = jnp.diff(t)\n\n    # Compute the weights for each value in `tp` based on the width of its interval in `t`.\n    w = jnp.where(i1 == i0, dt[i0], 0)\n\n    # Compute the sum of values in each interval in `t`.\n    v = jnp.sum(w * vp[Ellipsis, i0], axis=-1)\n  return v"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n  utils.assert_valid_stepfun(t, None)\n\n  # Compute the width of each interval in t.\n  dt = jnp.diff(t)\n\n  # Compute the width of each interval in tp.\n  dtp = jnp.diff(tp)\n\n  # Compute the indices of the intervals in tp that correspond to each interval in t.\n  i0, i1 = math.sorted_lookup(t, tp, utils.device_is_tpu())\n\n  # Compute the values of the step function at the new time points in t.\n  if use_avg:\n    # Compute the average value of the step function for each interval in t.\n    vq = jnp.where(dtp > jnp.finfo(dtp.dtype).eps,\n                   jnp.sum(vp[..., i0:i1] * dtp[..., i0:i1], axis=-1) / dt,\n                   vp[..., i0:i1])\n  else:\n    # Sum the values of the step function for each interval in t.\n    vq = jnp.sum(vp[..., i0:i1] * dtp[..., i0:i1], axis=-1)\n\n  return vq"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n\n  # Compute the width of each interval.\n  widths = jnp.diff(tp)\n\n  # Compute the cumulative sum of the widths.\n  cwidths = jnp.cumsum(widths)\n\n  # Compute the cumulative sum of the values.\n  cvals = jnp.cumsum(vp * widths)\n\n  # Compute the cumulative sum of the squared values.\n  cvals_sq = jnp.cumsum(vp**2 * widths)\n\n  # Compute the cumulative sum of the squared widths.\n  cwidths_sq = jnp.cumsum(widths**2)\n\n  # Compute the average values for each interval.\n  avg_vals = cvals / cwidths\n\n  # Compute the variance of each interval.\n  var_vals = cvals_sq / cwidths - avg_vals**2\n\n  # Compute the standard deviation of each interval.\n  std_vals = jnp.sqrt(var_vals)\n\n  # Compute the new values for each interval.\n  if use_avg:\n    new_vals = avg_vals\n  else:\n    new_vals = cvals / cwidths_sq * jnp.diff(t)\n\n  # Compute the new standard deviation for each interval.\n  new_std_vals = std_vals / jnp.sqrt(jnp.diff(t))\n\n  # Interpolate the new values for each interval.\n  new_vals_interp = jnp.interp(t, tp, new_vals)\n\n  # Interpolate the new standard deviation for each interval.\n  new_std_vals_interp = jnp.interp(t, tp, new_std_vals)\n\n  return new_vals_interp, new_std_vals_interp"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n  utils.assert_valid_stepfun(t, tp)\n  if use_avg:\n    # Compute the width of each interval in t.\n    dt = jnp.diff(t)\n    # Compute the indices of the intervals in tp that overlap with each interval in t.\n    (i0, i1), _ = math.sorted_lookup(t, tp, ())\n    # Compute the values of the step function at the endpoints of each interval in t.\n    v0 = vp[i0]\n    v1 = vp[i1]\n    # Compute the average value of the step function in each interval in t.\n    vt = (v0 + v1) / 2\n    # Compute the weighted average value of the step function in each interval in t.\n    vt = vt * dt\n    return vt\n  else:\n    # Compute the width of each interval in t.\n    dt = jnp.diff(t)\n    # Compute the indices of the intervals in tp that overlap with each interval in t.\n    (i0, i1), _ = math.sorted_lookup(t, tp, ())\n    # Compute the values of the step function at the endpoints of each interval in t.\n    v0 = vp[i0]\n    v1 = vp[i1]\n    # Compute the sum of the values of the step function in each interval in t.\n    vt = (v1 - v0) * dt\n    return vt\n\n"}
{"namespace": "stepfun.resample", "completion": "  # Check if the input tensors have the same shape.\n  if tp.shape != vp.shape:\n    raise ValueError(f'Input tensors tp and vp must have the same shape.')\n\n  # Check if the input tensors are sorted.\n  if not jnp.all(jnp.diff(tp) >= 0):\n    raise ValueError(f'Input tensor tp must be sorted.')\n\n  # Check if the input tensor t is sorted and has at least two elements.\n  if not jnp.all(jnp.diff(t) >= 0) or t.shape[-1] < 2:\n    raise ValueError(f'Input tensor t must be sorted and have at least two elements.')\n\n  # Check if the input tensor tp has at least two elements.\n  if tp.shape[-1] < 2:\n    raise ValueError(f'Input tensor tp must have at least two elements.')\n\n  # Compute the indices of the new time points in the original time points.\n  indices = jnp.searchsorted(tp, t, side='right') - 1\n\n  # Compute the values of the resampled step function at the new time points.\n  if use_avg:\n    # Compute the average value of the step function for each interval in t.\n    vq = jnp.diff(vp) / jnp.diff(tp)\n    vq = jnp.take_along_axis(vq, indices[Ellipsis, None], axis=-1)\n    vq = jnp.sum(vq, axis=-1)\n  else:\n    # Sum the values of the step function for each interval in t.\n    vq = jnp.take_along_axis(vp, indices[Ellipsis, None], axis=-1)\n    vq = jnp.sum(vq, axis=-1)\n\n  return vq"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n\n  # Compute the width of each interval in tp.\n  tp_width = jnp.diff(tp)\n\n  # Compute the indices of the intervals in tp that each point in t falls into.\n  i0, i1 = math.sorted_lookup(t, tp, utils.device_is_tpu())\n\n  # Compute the width of each interval in t that each point in t falls into.\n  t_width = jnp.diff(t)\n\n  # Compute the fraction of each point in t that falls into each interval in tp.\n  frac = jnp.where(t_width < np.finfo(np.float32).tiny, 0,\n                   math.safe_div(t_width, tp_width))\n\n  # Compute the values of the step function at each point in t.\n  v = jnp.zeros_like(t)\n  v = v.at[Ellipsis, i0].add(vp[Ellipsis, i0] * frac)\n  v = v.at[Ellipsis, i1].add(vp[Ellipsis, i1] * (1 - frac))\n\n  # If use_avg is True, compute the average value of the step function at each point in t.\n  if use_avg:\n    v = v / t_width\n\n  return v"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n  utils.assert_valid_stepfun(t, vp)\n\n  # Compute the width of each interval in `t`.\n  dt = jnp.diff(t)\n\n  # Compute the indices of the intervals in `tp` that are fully contained in each interval in `t`.\n  (i0, i1), _ = math.sorted_lookup(t, tp, (), utils.device_is_tpu())\n\n  # Compute the width of each interval in `t` that is fully contained in each interval in `tp`.\n  dt_contained = jnp.diff(t[Ellipsis, i0], axis=-1)\n\n  # Compute the width of each interval in `t` that is partially contained in each interval in `tp`.\n  dt_partial = jnp.minimum(\n      jnp.diff(t[Ellipsis, i0 + 1], axis=-1),\n      jnp.diff(t[Ellipsis, i1], axis=-1),\n  )\n\n  # Compute the width of each interval in `t` that is not contained in any interval in `tp`.\n  dt_uncontained = dt - dt_contained - dt_partial\n\n  # Compute the values of the step function at the new time points in `t`.\n  vp_resampled = jnp.where(\n      dt_uncontained > 0,\n      jnp.zeros_like(vp),\n      jnp.where(\n          dt_partial > 0,\n          jnp.where(\n              dt_contained > 0,\n              vp[Ellipsis, i0] * dt_contained / dt_partial\n              + vp[Ellipsis, i0 + 1] * dt_partial / dt_partial,\n              vp[Ellipsis, i0 + 1] * dt_partial / dt_partial,\n          ),\n          vp[Ellipsis, i0],\n      ),\n  )\n\n  # Compute the"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n\n  # Compute the width of each original interval.\n  widths = jnp.diff(tp)\n\n  # Compute the width of each new interval.\n  widths_new = jnp.diff(t)\n\n  # Compute the number of intervals in each original interval.\n  num_intervals_per_interval = jnp.ceil(widths / widths_new)\n\n  # Compute the new interval widths.\n  widths_new_new = widths / num_intervals_per_interval\n\n  # Compute the new time points for the resampled step function.\n  tp_new = jnp.concatenate(\n      [tp[Ellipsis, :1], tp[Ellipsis, 1:] - widths_new_new], axis=-1\n  )\n\n  # Compute the new values for the resampled step function.\n  if use_avg:\n    vp_new = jnp.cumsum(vp * widths_new_new, axis=-1)\n    vp_new = jnp.concatenate([jnp.zeros_like(vp[Ellipsis, :1]), vp_new], axis=-1)\n    vp_new = vp_new / jnp.cumsum(widths_new_new, axis=-1)\n  else:\n    vp_new = jnp.cumsum(vp * widths_new_new, axis=-1)\n    vp_new = jnp.concatenate([jnp.zeros_like(vp[Ellipsis, :1]), vp_new], axis=-1)\n\n  # Interpolate the new values into the new time points.\n  vp_new = math.sorted_interp(t, vp_new, tp_new, utils.device_is_tpu())\n\n  return vp_new"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n  tp = tp.reshape((-1,))\n  vp = vp.reshape((-1,))\n\n  # Find the indices of the original time points (tp) that are within each new interval (t).\n  i0, i1 = math.sorted_lookup(t, tp, utils.device_is_tpu())\n\n  # Compute the widths of each interval (t) and the number of original time points (tp) within each interval.\n  widths = jnp.diff(t)\n  num_points = jnp.bincount(i1.reshape((-1,)), minlength=t.shape[-1] - 1)\n\n  # Compute the sum or average of the values (vp) within each interval (t).\n  if use_avg:\n    vq = jnp.bincount(i1.reshape((-1,)), weights=vp, minlength=t.shape[-1] - 1)\n    vq = jnp.divide(vq, num_points, out=jnp.zeros_like(vq), where=num_points != 0)\n  else:\n    vq = jnp.bincount(i1.reshape((-1,)), minlength=t.shape[-1] - 1)\n\n  # Normalize the values (vq) by the widths of each interval (t).\n  vq = jnp.divide(vq, widths, out=jnp.zeros_like(vq), where=widths != 0)\n  return vq"}
{"namespace": "coord.integrated_pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = mean.shape[:-1] + (-1,)\n  scaled_mean = mean[Ellipsis, None] * scales[:, None]\n  scaled_var = var[Ellipsis, None] * scales[:, None] ** 2\n  scaled_mean = jnp.reshape(scaled_mean, shape)\n  scaled_var = jnp.reshape(scaled_var, shape)\n  four_feat = jnp.sin(\n      jnp.concatenate([scaled_mean, scaled_mean + 0.5 * jnp.pi], axis=-1)\n  )\n  return jnp.concatenate([mean, four_feat], axis=-1)"}
{"namespace": "coord.integrated_pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = mean.shape[:-1] + (-1,)\n  scaled_mean = mean[Ellipsis, None, :] * scales[:, None]\n  scaled_mean = jnp.reshape(scaled_mean, shape)\n  scaled_var = var[Ellipsis, None, :] * scales[:, None] ** 2\n  scaled_var = jnp.reshape(scaled_var, shape)\n  scaled_x = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n  return jnp.sin(scaled_x)"}
{"namespace": "coord.integrated_pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = mean.shape[:-1] + (-1,)\n  scaled_mean = mean[Ellipsis, None, :] * scales[:, None]  # (..., s, c).\n  scaled_mean = jnp.reshape(scaled_mean, shape)  # (..., s*c).\n  scaled_var = var[Ellipsis, None, :] * scales[:, None]  # (..., s, c).\n  scaled_var = jnp.reshape(scaled_var, shape)  # (..., s*c).\n  # Note that we're not using safe_sin, unlike IPE.\n  # (..., s*c + s*c).\n  four_feat = jnp.sin(\n      jnp.concatenate([scaled_mean, scaled_mean + 0.5 * jnp.pi], axis=-1)\n  )\n  four_feat = jnp.concatenate([four_feat, scaled_var], axis=-1)\n  return four_feat"}
{"namespace": "coord.integrated_pos_enc", "completion": "  min_deg, max_deg = min_deg, max_deg\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = mean.shape[:-1] + (-1,)\n  scaled_mean = mean[Ellipsis, None, :] * scales[:, None]  # (..., s, c).\n  scaled_var = var[Ellipsis, None, :] * scales[:, None] ** 2  # (..., s, c).\n  scaled_mean = jnp.reshape(scaled_mean, shape)  # (..., s*c).\n  scaled_var = jnp.reshape(scaled_var, shape)  # (..., s*c).\n  four_feat = jnp.sin(\n      jnp.concatenate([scaled_mean, scaled_var, scaled_mean + 0.5 * jnp.pi], axis=-1)\n  )\n  return jnp.concatenate([mean, four_feat], axis=-1)"}
{"namespace": "coord.integrated_pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = mean.shape[:-1] + (-1,)\n  scaled_mean = mean[Ellipsis, None] * scales[:, None]  # (..., s).\n  scaled_mean = jnp.reshape(scaled_mean, shape)  # (..., s*c).\n  scaled_var = var[Ellipsis, None] * scales[:, None]  # (..., s).\n  scaled_var = jnp.reshape(scaled_var, shape)  # (..., s*c).\n  four_feat = jnp.sin(\n      jnp.concatenate([scaled_mean, scaled_mean + 0.5 * jnp.pi], axis=-1)\n  )\n  four_feat = jnp.concatenate([four_feat, four_feat], axis=-1)\n  return four_feat"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Compute the scaling factor for the encoding.\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  # Scale the input coordinates.\n  scaled_mean = mean[..., None] * scales[None, :]\n  scaled_var = var[..., None] * scales[None, :]\n  # Concatenate the scaled coordinates.\n  x = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n  # Apply the sinusoidal encoding.\n  four_feat = jnp.sin(\n      jnp.concatenate([x, x + 0.5 * jnp.pi], axis=-1)\n  )\n  return jnp.concatenate([mean, four_feat], axis=-1)"}
{"namespace": "coord.integrated_pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = mean.shape[:-1] + (-1,)\n  scaled_mean = mean[Ellipsis, None] * scales[None, :]\n  scaled_var = var[Ellipsis, None] * scales[None, :]\n  scaled_mean = jnp.reshape(scaled_mean, shape)\n  scaled_var = jnp.reshape(scaled_var, shape)\n  # Note that we're not using safe_sin, unlike IPE.\n  # (..., s*c + s*c).\n  four_feat = jnp.sin(\n      jnp.concatenate([scaled_mean, scaled_mean + 0.5 * jnp.pi], axis=-1)\n  )\n  four_feat = jnp.concatenate([four_feat, four_feat], axis=-1)\n  return jnp.concatenate([mean, var, four_feat], axis=-1)"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Compute the scale factors for the encoding.\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  # Scale the input coordinates.\n  scaled_mean = mean[Ellipsis, None, :] * scales[:, None]\n  scaled_var = var[Ellipsis, None, :] * scales[:, None] ** 2\n  # Concatenate the scaled coordinates.\n  scaled_concat = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n  # Apply the sinusoidal encoding.\n  sin_encoding = jnp.sin(scaled_concat)\n  # Return the encoded variables.\n  return sin_encoding"}
{"namespace": "coord.integrated_pos_enc", "completion": "  if min_deg < 0 or max_deg < 0 or min_deg > max_deg:\n    raise ValueError('min_deg and max_deg must be non-negative and min_deg <= max_deg.')\n\n  # Scale the mean and variance by powers of 2 within the specified range.\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  scaled_mean = mean[Ellipsis, None, :] * scales[:, None]\n  scaled_var = var[Ellipsis, None, :] * scales[:, None] ** 2\n\n  # Concatenate the scaled mean and variance along the last axis.\n  scaled_mean_var = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply the sinusoidal encoding to the concatenated tensor.\n  four_feat = jnp.sin(scaled_mean_var)\n\n  return four_feat"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scaling factor for the encoding.\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  # Scaling the mean and variance.\n  scaled_mean = mean[..., None] * scales\n  scaled_var = var[..., None] * scales**2\n  # Concatenating the scaled mean and variance.\n  concat_mean_var = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n  # Applying the sinusoidal encoding.\n  encoded_mean_var = jnp.sin(concat_mean_var)\n  # Returning the encoded variables.\n  return encoded_mean_var"}
{"namespace": "coord.integrated_pos_enc", "completion": "  if min_deg < 0 or max_deg <= min_deg:\n    raise ValueError(f'Invalid range [{min_deg}, {max_deg})')\n  if min_deg == 0 and max_deg == 1:\n    return jnp.concatenate([mean, var], axis=-1)\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = mean.shape[:-1] + (-1,)\n  scaled_mean = mean[Ellipsis, None, :] * scales[:, None]  # (..., s, c).\n  scaled_var = var[Ellipsis, None, :] * scales[:, None] ** 2  # (..., s, c).\n  scaled_mean = jnp.reshape(scaled_mean, shape)  # (..., s*c).\n  scaled_var = jnp.reshape(scaled_var, shape)  # (..., s*c).\n  four_feat = jnp.sin(\n      jnp.concatenate([scaled_mean, scaled_mean + 0.5 * jnp.pi], axis=-1)\n  )\n  four_feat = jnp.concatenate([four_feat, jnp.sqrt(scaled_var)], axis=-1)\n  return four_feat"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Compute the scaling factor for the encoding.\n  scale = jnp.sqrt(jnp.maximum(1e-8, var))\n\n  # Scale the mean and variance values.\n  scaled_mean = mean / scale\n  scaled_var = var / scale**2\n\n  # Concatenate the scaled mean and variance values along the last dimension.\n  scaled_mean_var = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Encode the concatenated values using sinusoidal functions.\n  encoded_values = pos_enc(scaled_mean_var, min_deg, max_deg)\n\n  return encoded_values"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Compute the minimum and maximum degrees of the encoding.\n  min_deg = min_deg\n  max_deg = max_deg\n\n  # Compute the scaling factors for the encoding.\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n\n  # Scale the mean and variance of the input coordinates.\n  scaled_mean = mean[Ellipsis, None] * scales[:, None]\n  scaled_var = var[Ellipsis, None] * scales[:, None]\n\n  # Concatenate the scaled mean and variance along the last dimension.\n  concat_input = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply the sinusoidal encoding to the concatenated input.\n  four_feat = jnp.sin(\n      jnp.concatenate([concat_input, concat_input + 0.5 * jnp.pi], axis=-1)\n  )\n\n  # Return the encoded variables.\n  return four_feat"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Compute the minimum and maximum degrees for the encoding.\n  min_deg = min_deg\n  max_deg = max_deg\n\n  # Scale the mean and variance by powers of 2 within the specified range.\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  scaled_mean = mean[Ellipsis, None] * scales[None, :]\n  scaled_var = var[Ellipsis, None] * scales[None, :]\n\n  # Concatenate the scaled mean and variance along the last dimension.\n  scaled_vars = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply a sinusoidal encoding to the concatenated variables.\n  four_feat = jnp.sin(scaled_vars)\n\n  # Concatenate the original mean and variance along the last dimension.\n  four_feat = jnp.concatenate([mean, four_feat], axis=-1)\n\n  return four_feat"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Compute the scaling factor for the encoding.\n  scales = jnp.power(2, jnp.arange(min_deg, max_deg))\n  # Scale the mean and variance.\n  scaled_mean = mean[..., None] * scales[None, :]\n  scaled_var = var[..., None] * scales[None, :]\n  # Concatenate the scaled mean and variance along the last axis.\n  concat_scaled = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n  # Apply the sinusoidal encoding to the concatenated values.\n  sin_encoded = jnp.sin(concat_scaled)\n  # Return the encoded variables.\n  return sin_encoded"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Compute the scaling factor for the encoding.\n  scale = jnp.sqrt(var)\n\n  # Scale the mean and variance by the scaling factor.\n  scaled_mean = mean / scale\n  scaled_var = var / scale**2\n\n  # Concatenate the scaled mean and variance along the last dimension.\n  x = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply the sinusoidal encoding to the concatenated tensor.\n  four_feat = jnp.sin(\n      jnp.concatenate(\n          [\n              x,\n              x + 0.5 * jnp.pi,\n              x + jnp.pi,\n              x + 1.5 * jnp.pi,\n              x + 2 * jnp.pi,\n              x + 3 * jnp.pi,\n              x + 4 * jnp.pi,\n              x + 5 * jnp.pi,\n          ],\n          axis=-1,\n      )\n  )\n\n  # Return the encoded tensor.\n  return four_feat"}
{"namespace": "coord.integrated_pos_enc", "completion": "  if min_deg < 0 or max_deg < 0:\n    raise ValueError(\n        f'min_deg and max_deg must be non-negative, but are {min_deg} and {max_deg}.'\n    )\n  if min_deg >= max_deg:\n    raise ValueError(\n        f'min_deg ({min_deg}) must be less than max_deg ({max_deg}).'\n    )\n  scales = jnp.power(2.0, jnp.arange(min_deg, max_deg))\n  scaled_mean = mean[Ellipsis, None] * scales[:, None]\n  scaled_var = var[Ellipsis, None] * jnp.square(scales)[:, None]\n  scaled_mean = jnp.reshape(scaled_mean, mean.shape[:-1] + (-1,))\n  scaled_var = jnp.reshape(scaled_var, var.shape[:-1] + (-1,))\n  return jnp.sin(jnp.concatenate([scaled_mean, scaled_mean + 0.5 * jnp.pi], axis=-1))"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Compute the scaling factor for the encoding.\n  scale = jnp.sqrt(var)\n  # Concatenate the mean and variance tensors along the last axis.\n  x = jnp.concatenate([mean, scale], axis=-1)\n  # Apply the sinusoidal encoding.\n  x = pos_enc(x, min_deg, max_deg)\n  return x\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scaling the mean and variance by 2**min_deg and 2**max_deg, respectively,\n  # ensures that the encoding covers the full range of input values.\n  min_scale = 2.0**min_deg\n  max_scale = 2.0**max_deg\n  scaled_mean = mean * min_scale\n  scaled_var = var * min_scale**2\n\n  # Concatenating the scaled mean and variance along the last dimension ensures\n  # that the encoding is applied to each coordinate independently.\n  x = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Applying the sinusoidal encoding to the concatenated tensor.\n  x_enc = jnp.sin(x)\n\n  # Returning the encoded tensor.\n  return x_enc"}
{"namespace": "coord.integrated_pos_enc", "completion": "  if min_deg > max_deg:\n    raise ValueError('min_deg must be <= max_deg.')\n  if min_deg < 0:\n    raise ValueError('min_deg must be >= 0.')\n\n  # Compute the number of degrees in the encoding range.\n  num_degrees = max_deg - min_deg\n\n  # Compute the number of encoding functions to use.\n  num_functions = 2 * num_degrees\n\n  # Compute the encoding functions for the specified degrees.\n  degrees = jnp.arange(min_deg, max_deg)\n  radians = 2 * jnp.pi * degrees / num_degrees\n  sin_enc = jnp.sin(radians)\n  cos_enc = jnp.cos(radians)\n  encodings = jnp.concatenate([sin_enc, cos_enc], axis=0)\n\n  # Scale the input mean and variance.\n  scaled_mean = mean / 2.0 ** min_deg\n  scaled_var = var / 2.0 ** (2 * min_deg)\n\n  # Concatenate the scaled mean and variance.\n  scaled_input = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply the encoding functions to the scaled input.\n  encoded_input = jnp.matmul(scaled_input[..., None, :], encodings[None, ...])\n\n  # Reshape the encoded input to have the same shape as the original input.\n  encoded_input = jnp.reshape(encoded_input, mean.shape + (num_functions,))\n\n  return encoded_input"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(sph_harms), jnp.imag(sph_harms)], axis"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(sph_harms), jnp.imag(sph_harms)], axis"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(sph_harms), jnp.imag(sph_harms)], axis"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(sph_harms), jnp.imag(sph_harms)], axis"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(sph_harms), jnp.imag(sph_harms)], axis"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(sph_harms), jnp.imag(sph_harms)], axis"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(sph_harms), jnp.imag(sph_harms)], axis"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(sph_harms), jnp.imag(sph_harms)], axis"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(sph_harms), jnp.imag(sph_harms)], axis"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(sph_harms), jnp.imag(sph_harms)], axis"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(sph_harms), jnp.imag(sph_harms)],"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(sph_harms), jnp.imag(sph_harms)], axis=-1)\n\n  return dir_enc_fn"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"\n    Function that takes a 3D point (or points) as input and returns its directional encoding.\n\n    Input-Output Arguments\n    :param xyz: Array. A 3D point (or points) in the form of a NumPy array.\n    :return: Array. The directional encoding of the input point(s).\n\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(sph_harms), jnp.imag(sph_harms)], axis=-1)"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"\n    Function that evaluates the directional encoding for given inputs.\n\n    Input-Output Arguments\n    :param xyz: Array. A 3D point (or points) in the form of a NumPy array.\n    :return: Array. The directional encoding of the input point(s).\n\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(s"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"\n    Function that evaluates the directional encoding for a given input point.\n\n    Input-Output Arguments\n    :param xyz: JAX array. A 3D point (or points) in the shape [..., 3].\n    :return: JAX array. The directional encoding of the input point(s).\n\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate(["}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"\n    Function to evaluate directional encoding for given inputs.\n\n    Input-Output Arguments\n    :param xyz: JAX array. A 3D point (or points) to evaluate the directional encoding for.\n    :return: JAX array. The directional encoding for the input points.\n\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(sph"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  \"\"\"\n  Generate a directional encoding function based on the specified number of spherical harmonics degrees.\n\n  Input-Output Arguments\n  :param deg_view: Int. The number of spherical harmonics degrees to use for generating the directional encoding function. It determines the complexity and accuracy of the encoding.\n  :return: Function. A function that takes a 3D point (or points) as input and returns its directional encoding. This returned function internals uses a generated integrated directional encoding function with the specified degree of spherical harmonics.\n\n  \"\"\"\n  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the x+iy coordinate Vandermonde matrix,\n  # results in the x+iy component of the encoding.\n  mat_xy = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat_xy[k, i] = sph_harm_coeff(l, m, k).real\n\n  def dir_enc_fn(xyz, kappa_inv):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: ["}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"\n    Function that evaluates the directional encoding for a given 3D point (or points).\n\n    Input-Output Arguments\n    :param xyz: JAX array. A 3D point (or points) in the form of a JAX array.\n    :return: JAX array. The directional encoding of the input point(s).\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Split into real and imaginary parts and return\n    return jnp"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"\n    Function that evaluates the directional encoding for a given 3D point (or points).\n\n    Input-Output Arguments\n    :param xyz: Array. A 3D point (or points) in the form of a numpy array of shape (..., 3).\n    :return: Array. The directional encoding of the input points. The shape of the returned array is (..., 2 * (deg_view + 1)).\n\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"\n    Evaluates the directional encoding for the given 3D points.\n\n    Input-Output Arguments\n    :param xyz: Numpy array of shape (..., 3). The 3D points for which the directional encoding is to be evaluated.\n    :return: Numpy array of shape (..., 2 * (deg_view ** 2)). The directional encoding for the given 3D points.\n\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = np.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = np.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * np.matmul(vmz, mat)\n\n   "}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    #     print(\"*\" * 50)\n    #     print(\"CLEANING LINES\")\n    #     print(\"*\" * 50)\n    #     print()\n\n    #     print(\"*\" * 50)\n    #     print(\"CLEANING LINES\")\n    #     print(\"*\" * 50)\n    #     print()\n\n    #     print(\"*\" * 50)\n    #     print(\"CLEANING LINES\")\n    #     print(\"*\" * 50)\n    #     print()\n\n    #     print(\"*\" * 50)\n    #     print(\"CLEANING LINES\")\n    #     print(\"*\" * 50)\n    #     print()\n\n    #     print(\"*\" * 50)\n    #     print(\"CLEANING LINES\")\n    #     print(\"*\" * 50)\n    #     print()\n\n    #     print(\"*\" * 50)\n    #     print(\"CLEANING LINES\")\n    #     print(\"*\" * 50)\n    #     print()\n\n    #     print(\"*\" * 50)\n    #     print(\"CLEANING LINES\")\n    #     print(\"*\" * 50)\n    #     print()\n\n    #     print(\"*\" * 50)\n    #     print(\"CLEANING LINES\")\n    #     print(\"*\" * 50)\n    #     print()\n\n    #     print(\"*\" * 50)\n    #     print(\"CLEANING LINES\")\n    #     print(\"*\" * 50)\n    #     print()\n\n    #     print(\"*\" * 50)\n    #     print(\"CLEANING LINES\")\n    #     print(\"*\" * 50)\n    #     print()\n\n    #     print(\"*\" * 50)\n    #     print(\"CLEANING LINES\")\n    #     print(\"*\" * 50)\n    #     print()\n\n    #    "}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # print(\"*\" * 50)\n    # print(\"clean_lines\")\n    # print(\"*\" * 50)\n    # print()\n    # print(lines)\n    # print()\n    # print(\"*\" * 50)\n    # print(\"clean_lines\")\n    # print(\"*\" * 50)\n    # print()\n    # print(lines)\n    # print()\n    # print(\"*\" * 50)\n    # print(\"clean_lines\")\n    # print(\"*\" * 50)\n    # print()\n    # print(lines)\n    # print()\n    # print(\"*\" * 50)\n    # print(\"clean_lines\")\n    # print(\"*\" * 50)\n    # print()\n    # print(lines)\n    # print()\n    # print(\"*\" * 50)\n    # print(\"clean_lines\")\n    # print(\"*\" * 50)\n    # print()\n    # print(lines)\n    # print()\n    # print(\"*\" * 50)\n    # print(\"clean_lines\")\n    # print(\"*\" * 50)\n    # print()\n    # print(lines)\n    # print()\n    # print(\"*\" * 50)\n    # print(\"clean_lines\")\n    # print(\"*\" * 50)\n    # print()\n    # print(lines)\n    # print()\n    # print(\"*\" * 50)\n    # print(\"clean_lines\")\n    # print(\"*\" * 50)\n    # print()\n    # print(lines)\n    # print()\n    # print(\"*\" * 50)\n    # print(\"clean_lines\")\n    # print(\"*\" * 50)\n    # print()\n    # print(lines)\n    # print()\n    # print(\"*\" * 50)\n    # print(\"clean_lines\")\n    # print(\"*\" * 50)\n    # print()\n    # print(lines)\n    # print()\n    # print(\"*\" * 50)\n    # print(\"clean_lines\")"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # print(\"*\" * 50)\n    # print(\"CLEANING LINES\")\n    # print(\"*\" * 50)\n    # print()\n    # print(\"*\" * 50)\n    # print(\"CLEANING LINES\")\n    # print(\"*\" * 50)\n    # print()\n    # print(\"*\" * 50)\n    # print(\"CLEANING LINES\")\n    # print(\"*\" * 50)\n    # print()\n\n    # print(\"*\" * 50)\n    # print(\"CLEANING LINES\")\n    # print(\"*\" * 50)\n    # print()\n    # print(\"*\" * 50)\n    # print(\"CLEANING LINES\")\n    # print(\"*\" * 50)\n    # print()\n\n    # print(\"*\" * 50)\n    # print(\"CLEANING LINES\")\n    # print(\"*\" * 50)\n    # print()\n    # print(\"*\" * 50)\n    # print(\"CLEANING LINES\")\n    # print(\"*\" * 50)\n    # print()\n\n    # print(\"*\" * 50)\n    # print(\"CLEANING LINES\")\n    # print(\"*\" * 50)\n    # print()\n    # print(\"*\" * 50)\n    # print(\"CLEANING LINES\")\n    # print(\"*\" * 50)\n    # print()\n\n    # print(\"*\" * 50)\n    # print(\"CLEANING LINES\")\n    # print(\"*\" * 50)\n    # print()\n    # print(\"*\" * 50)\n    # print(\"CLEANING LINES\")\n    # print(\"*\" * 50)\n    # print()\n\n    # print(\"*\" * 50)\n    # print(\"CLEANING LINES\")\n    # print(\"*\" * 50)\n    # print()\n    # print(\"*\" * 50)\n    #"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # print()\n    # print(\"*\" * 50)\n    # print(\"clean_lines\")\n    # print()\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * 50)\n    # print(\"*\" * "}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # print(lines)\n    # print(\"*\"*50)\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n   "}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # print(\"*\"*50)\n    # print(\"clean_lines\")\n    # print(lines)\n    # print()\n    # print(\"*\"*50)\n\n    # print(\"*\"*50)\n    # print(\"clean_lines\")\n    # print(lines)\n    # print()\n    # print(\"*\"*50)\n\n    # print(\"*\"*50)\n    # print(\"clean_lines\")\n    # print(lines)\n    # print()\n    # print(\"*\"*50)\n\n    # print(\"*\"*50)\n    # print(\"clean_lines\")\n    # print(lines)\n    # print()\n    # print(\"*\"*50)\n\n    # print(\"*\"*50)\n    # print(\"clean_lines\")\n    # print(lines)\n    # print()\n    # print(\"*\"*50)\n\n    # print(\"*\"*50)\n    # print(\"clean_lines\")\n    # print(lines)\n    # print()\n    # print(\"*\"*50)\n\n    # print(\"*\"*50)\n    # print(\"clean_lines\")\n    # print(lines)\n    # print()\n    # print(\"*\"*50)\n\n    # print(\"*\"*50)\n    # print(\"clean_lines\")\n    # print(lines)\n    # print()\n    # print(\"*\"*50)\n\n    # print(\"*\"*50)\n    # print(\"clean_lines\")\n    # print(lines)\n    # print()\n    # print(\"*\"*50)\n\n    # print(\"*\"*50)\n    # print(\"clean_lines\")\n    # print(lines)\n    # print()\n    # print(\"*\"*50)\n\n    # print(\"*\"*50)\n    # print(\"clean_lines\")\n    # print(lines)\n    # print()\n    # print(\"*\"*50)\n\n    # print(\"*\"*50)\n    # print(\"clean_lines\")\n    # print(lines)\n    # print()\n    # print(\"*\""}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # print(\"*\" * 100)\n    # print(\"CLEANING LINES\")\n    # print(\"*\" * 100)\n    # print()\n    # print()\n\n    # print(\"*\" * 100)\n    # print(\"CLEANING LINES\")\n    # print(\"*\" * 100)\n    # print()\n    # print()\n    # print(\"*\" * 100)\n    # print(\"CLEANING LINES\")\n    # print(\"*\" * 100)\n    # print()\n    # print()\n    # print(\"*\" * 100)\n    # print(\"CLEANING LINES\")\n    # print(\"*\" * 100)\n    # print()\n    # print()\n    # print(\"*\" * 100)\n    # print(\"CLEANING LINES\")\n    # print(\"*\" * 100)\n    # print()\n    # print()\n    # print(\"*\" * 100)\n    # print(\"CLEANING LINES\")\n    # print(\"*\" * 100)\n    # print()\n    # print()\n    # print(\"*\" * 100)\n    # print(\"CLEANING LINES\")\n    # print(\"*\" * 100)\n    # print()\n    # print()\n    # print(\"*\" * 100)\n    # print(\"CLEANING LINES\")\n    # print(\"*\" * 100)\n    # print()\n    # print()\n    # print(\"*\" * 100)\n    # print(\"CLEANING LINES\")\n    # print(\"*\" * 100)\n    # print()\n    # print()\n    # print(\"*\" * 100)\n    # print(\"CLEANING LINES\")\n    # print(\"*\" * 100)\n    # print()\n    # print()\n    # print(\"*\" * 100)\n    # print(\"CLEANING LINES\")\n    #"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # print(\"clean_lines\")\n    # print(\"*\" * 50)\n    # print(len(lines))\n    # print(\"*\" * 50)\n    # print()\n\n    # print(lines)\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()\n    # print()"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    #     print(\"*\"*50)\n    #     print(\"CLEANING LINES\")\n    #     print(\"*\"*50)\n    #     print()\n    #     print(\"-\"*50)\n    #     print(\"START\")\n    #     print(\"-\"*50)\n    #     print()\n    #     print(\"*\"*50)\n    #     print(\"CLEANING LINES\")\n    #     print(\"*\"*50)\n    #     print()\n    #     print(\"-\"*50)\n    #     print(\"START\")\n    #     print(\"-\"*50)\n    #     print()\n    #     print(\"*\"*50)\n    #     print(\"CLEANING LINES\")\n    #     print(\"*\"*50)\n    #     print()\n    #     print(\"-\"*50)\n    #     print(\"START\")\n    #     print(\"-\"*50)\n    #     print()\n    #     print(\"*\"*50)\n    #     print(\"CLEANING LINES\")\n    #     print(\"*\"*50)\n    #     print()\n    #     print(\"-\"*50)\n    #     print(\"START\")\n    #     print(\"-\"*50)\n    #     print()\n    #     print(\"*\"*50)\n    #     print(\"CLEANING LINES\")\n    #     print(\"*\"*50)\n    #     print()\n    #     print(\"-\"*50)\n    #     print(\"START\")\n    #     print(\"-\"*50)\n    #     print()\n    #     print(\"*\"*50)\n    #     print(\"CLEANING LINES\")\n    #     print(\"*\"*50)\n    #     print()\n    #     print(\"-\"*50)\n    #     print(\"START\")\n    #     print(\"-\"*50)\n    #     print()\n    #     print(\"*\"*50)\n    #     print(\"CLEANING LINES\")\n    #     print(\""}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # remove duplicate lines\n    # lines = list(set(lines))\n    # remove empty lines\n    lines = [line for line in lines if line.strip()]\n    # remove lines with less than 2 characters\n    lines = [line for line in lines if len(line.strip()) >= 2]\n    # remove lines with less than 2 words\n    lines = [line for line in lines if len(line.split()) >= 2]\n    # remove lines with less than 2 characters\n    lines = [line for line in lines if len(line.strip()) >= 2]\n    # remove lines with less than 2 words\n    lines = [line for line in lines if len(line.split()) >= 2]\n    # remove lines with less than 2 characters\n    lines = [line for line in lines if len(line.strip()) >= 2]\n    # remove lines with less than 2 words\n    lines = [line for line in lines if len(line.split()) >= 2]\n    # remove lines with less than 2 characters\n    lines = [line for line in lines if len(line.strip()) >= 2]\n    # remove lines with less than 2 words\n    lines = [line for line in lines if len(line.split()) >= 2]\n    # remove lines with less than 2 characters\n    lines = [line for line in lines if len(line.strip()) >= 2]\n    # remove lines with less than 2 words\n    lines = [line for line in lines if len(line.split()) >= 2]\n    # remove lines with less than 2 characters\n    lines = [line for line in lines if len(line.strip()) >= 2]\n    # remove lines with less than 2 words\n    lines = [line for line in lines if len(line.split()) >= 2]\n    # remove lines with less than 2 characters\n    lines = [line for line in lines if len(line.strip()) >= 2]\n    # remove lines with less than 2 words\n    lines = [line for line in lines if len(line.split()) >= 2]\n    # remove lines with less than 2 characters\n    lines = [line for line in lines if len(line."}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    #     print(\"*\"*50)\n    #     print(\"CLEANING LINES\")\n    #     print(\"*\"*50)\n    #     print(\"-\"*50)\n    #     print(\"INPUT\")\n    #     print(\"-\"*50)\n    #     print(lines)\n    #     print(\"-\"*50)\n    #     print(\"-\"*50)\n    #     print(\"OUTPUT\")\n    #     print(\"-\"*50)\n    #     print()\n    #     print(\"-\"*50)\n    #     print()\n    #     print(\"-\"*50)\n    #     print()\n    #     print(\"-\"*50)\n    #     print()\n    #     print(\"-\"*50)\n    #     print()\n    #     print(\"-\"*50)\n    #     print()\n    #     print(\"-\"*50)\n    #     print()\n    #     print(\"-\"*50)\n    #     print()\n    #     print(\"-\"*50)\n    #     print()\n    #     print(\"-\"*50)\n    #     print()\n    #     print(\"-\"*50)\n    #     print()\n    #     print(\"-\"*50)\n    #     print()\n    #     print(\"-\"*50)\n    #     print()\n    #     print(\"-\"*50)\n    #     print()\n    #     print(\"-\"*50)\n    #     print()\n    #     print(\"-\"*50)\n    #     print()\n    #     print(\"-\"*50)\n    #     print()\n    #     print(\"-\"*50)\n    #     print()\n    #     print(\"-\"*50)\n    #     print()\n    #     print(\"-\"*50)\n    #     print()\n    #     print(\"-\"*50)\n    #     print()\n    #     print(\"-\"*50)\n    #     print()\n    #     print(\"-\"*50)\n    #     print()\n    #"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    #     print(\"-\" * 50)\n    #     print(\"clean_lines\")\n    #     print(len(lines))\n    #     print()\n    #     print(\"-\" * 50)\n    #     print()\n    #     print()\n\n    result = []\n    block_idx = 0\n    header_block_idx = -1\n    text_group_start = True\n    text_group_start_idx = 0\n    block_group = []\n    block_group_dict = defaultdict(list)\n    block_group_list = []\n    block_group_id = 0\n    block_group_dict = defaultdict(list)\n    block_group_list = []\n    block_group_id = 0\n    block_group_dict = defaultdict(list)\n    block_group_list = []\n    block_group_id = 0\n    block_group_dict = defaultdict(list)\n    block_group_list = []\n    block_group_id = 0\n    block_group_dict = defaultdict(list)\n    block_group_list = []\n    block_group_id = 0\n    block_group_dict = defaultdict(list)\n    block_group_list = []\n    block_group_id = 0\n    block_group_dict = defaultdict(list)\n    block_group_list = []\n    block_group_id = 0\n    block_group_dict = defaultdict(list)\n    block_group_list = []\n    block_group_id = 0\n    block_group_dict = defaultdict(list)\n    block_group_list = []\n    block_group_id = 0\n    block_group_dict = defaultdict(list)\n    block_group_list = []\n    block_group_id = 0\n    block_group_dict = defaultdict(list)\n    block_group_list = []\n    block_group_id = 0\n    block_group_dict = defaultdict(list)\n    block_group_list = []\n    block_group_id = 0\n    block_group_dict = defaultdict(list)\n   "}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    #     print(len(lines))\n    #     print(len(lines), \"*\" * 50)\n    #     print(lines[:10])\n    #     print(\"*\" * 50)\n    #     print(lines[-10:])\n    #     print(\"*\" * 50)\n\n    #     print()\n    #     print(\"CLEANING LINES\")\n    #     print(\"*\" * 50)\n    #     print()\n\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)\n    #     print(\"-\" * 50)"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # print(\"*\" * 50)\n    # print(\"CLEAN LINES\")\n    # print(\"*\" * 50)\n    result = []\n    prev_line = None\n    block_idx = 0\n    header_block_idx = -1\n    block_group = []\n    text_group_start = True\n    text_group_start_idx = 0\n    group_id = 0\n    for idx, line in enumerate(lines):\n        # print(idx)\n        if should_skip(line, xml):\n            continue\n\n        curr_line = line_parser.Line(line)\n\n        if prev_line is None:\n            # initialize memory of previous line.\n            # this will update with join decisions\n            block = {\n                \"block_idx\": block_idx,\n                \"block_text\": curr_line.text,\n                \"block_type\": curr_line.line_type,\n                \"header_block_idx\": header_block_idx,\n                \"block_group\": [curr_line.visual_line.text_list],\n                \"fs\": curr_line.visual_line.start_fs,\n                \"text_group_start_idx\": text_group_start_idx,\n                \"block_list\": curr_line.visual_line.text_list,\n                \"line\": curr_line,\n                \"y\": curr_line.visual_line.start_y,\n                \"group_id\": group_id,\n            }\n\n            prev_line = curr_line\n            block_idx += 1\n            result.append(block)\n            continue\n\n        # print(\"--\" * 50)\n        # print(prev_line.line_type, \"\\n\", prev_line.text)\n        # print(prev_line.visual_line.fs, prev_line.visual_line.fw, \"prev_line:\", prev_line.line_type, prev_line.text)\n        # print(curr_line.line_type, \"\\n\", curr_line.text)\n        # print(curr"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    block_idx = 0\n    header_block_idx = -1\n    prev_line = None\n    curr_line = None\n    block_group_list = []\n    group_id = 0\n    text_group_start = True\n    text_group_start_idx = 0\n\n    for idx, line in enumerate(lines):\n        # print(idx)\n        line = clean_line(line)\n        # print(line)\n        if should_skip(line):\n            continue\n\n        if find_floating_chars(line):\n            continue\n\n        curr_line = line_parser.Line(line)\n        if prev_line is None:\n            if curr_line.line_type == \"header\":\n                header_block_idx = block_idx\n            block = {\n                \"block_idx\": block_idx,\n                \"block_text\": curr_line.text,\n                \"block_type\": curr_line.line_type,\n                \"header_block_idx\": header_block_idx,\n                \"block_group\": [curr_line.visual_line.text_list],\n                \"fs\": curr_line.visual_line.start_fs,\n                \"text_group_start_idx\": text_group_start_idx,\n                \"block_list\": curr_line.visual_line.text_list,\n                \"line\": curr_line,\n                \"y\": curr_line.visual_line.start_y,\n                \"group_id\": group_id,\n            }\n            prev_line = curr_line\n            block_idx += 1\n            result.append(block)\n            continue\n\n        # print(\"--\" * 50)\n        # print(prev_line.line_type, \"\\n\", prev_line.text)\n        # print(prev_line.visual_line.fs, prev_line.visual_line.fw, \"prev_line:\", prev_line.line_type, prev_line.text)\n        # print(curr_line.line_type, \"\\"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    blocks = []\n    header_block_idx = -1\n    block_idx = 0\n    text_group_start = True\n    text_group_start_idx = 0\n    group_id = 0\n    for idx, line in enumerate(lines):\n        # print(idx)\n        line = clean_line(line)\n        if should_skip(line, xml=xml):\n            continue\n        # print(\"line\", line)\n        line_obj = line_parser.Line(line)\n        # print(line_obj.text)\n        if idx + 1 < len(lines):\n            next_line = lines[idx + 1]\n            next_line = clean_line(next_line)\n            if should_skip(next_line, xml=xml):\n                continue\n            next_line_obj = line_parser.Line(next_line)\n            if (\n                line_obj.line_type == \"list_item\"\n                and next_line_obj.line_type == \"list_item\"\n            ):\n                line_obj.text = connect(line_obj.text, next_line_obj.text)\n                line_obj.visual_line.text_list.append(next_line_obj.visual_line.text_list)\n                line_obj.visual_line.start_x_list.append(\n                    next_line_obj.visual_line.start_x,\n                )\n                line_obj.visual_line.start_x_list_single_ent.append(\n                    next_line_obj.visual_line.start_x_list_single_ent,\n                )\n                line_obj.visual_line.end_x_list.append(\n                    next_line_obj.visual_line.end_x,\n                )\n                line_obj.visual_line.end_x_list_single_ent.append(\n                    next_line_obj.visual_line.end_x_list_single_ent,\n                )\n                line_obj.visual_line.start_y_list.append("}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # result = []\n    # block_index = 0\n    # header_index = -1\n    # block_list = []\n    # prev_line = \"\"\n    # curr_line = \"\"\n    # block_text = \"\"\n    # block_type = \"\"\n    # block_start_index = 0\n    # block_end_index = 0\n    # block_start_y = 0\n    # block_end_y = 0\n    # block_start_x = 0\n    # block_end_x = 0\n    # block_indent_level = 0\n    # block_header_index = -1\n    # block_list_item = \"\"\n    # block_list_item_index = -1\n    # block_list_item_type = \"\"\n    # block_list_item_start_index = 0\n    # block_list_item_end_index = 0\n    # block_list_item_start_y = 0\n    # block_list_item_end_y = 0\n    # block_list_item_start_x = 0\n    # block_list_item_end_x = 0\n    # block_list_item_indent_level = 0\n    # block_list_item_header_index = -1\n    # block_list_item_list_item = \"\"\n    # block_list_item_list_item_index = -1\n    # block_list_item_list_item_type = \"\"\n    # block_list_item_list_item_start_index = 0\n    # block_list_item_list_item_end_index = 0\n    # block_list_item_list_item_start_y = 0\n    # block_list_item_list_item_end_y = 0\n    # block_list_item_list_item_start_x = 0\n    # block_list_item_list_item_end_x = 0\n    # block_list_item_list_item_indent_level = 0\n    # block_list_item_list_item_header_index = -1\n    # block_list_item_list_item_list"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    curr_line = None\n    prev_line = None\n    curr_line_idx = 0\n    curr_header_idx = -1\n    curr_list_level = 0\n    curr_block_start_idx = 0\n    curr_block_start_y = None\n    curr_block_end_y = None\n    curr_block_text = \"\"\n    curr_block_type = None\n    curr_block_group = []\n    curr_block_list = []\n    curr_block_group_idx = -1\n    curr_block_group_list = []\n    curr_block_group_start_idx = -1\n    curr_block_group_start_y = None\n    curr_block_group_end_y = None\n    curr_block_group_text = \"\"\n    curr_block_group_type = None\n    curr_block_group_list = []\n    curr_block_group_idx = -1\n    curr_block_group_start_idx = -1\n    curr_block_group_start_y = None\n    curr_block_group_end_y = None\n    curr_block_group_text = \"\"\n    curr_block_group_type = None\n    curr_block_group_list = []\n    curr_block_group_idx = -1\n    curr_block_group_start_idx = -1\n    curr_block_group_start_y = None\n    curr_block_group_end_y = None\n    curr_block_group_text = \"\"\n    curr_block_group_type = None\n    curr_block_group_list = []\n    curr_block_group_idx = -1\n    curr_block_group_start_idx = -1\n    curr_block_group_start_y = None\n    curr_block_group_end_y = None\n    curr_block_group_text = \"\"\n    curr_block_group_type = None\n    curr"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # print(\"-\" * 50)\n    # print(\"CLEANING LINES\")\n    # print(\"-\" * 50)\n    # print()\n\n    # print(\"-\" * 50)\n    # print(\"CLEANING LINES\")\n    # print(\"-\" * 50)\n    # print()\n    result = []\n    block_idx = 0\n    header_block_idx = -1\n    line_set = set()\n    for idx, line in enumerate(lines):\n        # print(idx)\n        line = clean_line(line)\n        if line in line_set:\n            continue\n\n        line_set.add(line)\n\n        if should_skip(line, xml):\n            continue\n\n        if is_table_row(line):\n            continue\n\n        if find_floating_chars(line):\n            continue\n\n        line_parser_obj = line_parser.Line(line)\n        line_type = line_parser_obj.line_type\n        line_text = line_parser_obj.text\n        line_text = fix_spaced_characters(line_text)\n\n        if line_type == \"header\":\n            header_block_idx = block_idx\n\n        block = {\n            \"block_idx\": block_idx,\n            \"block_text\": line_text,\n            \"block_type\": line_type,\n            \"header_block_idx\": header_block_idx,\n            \"block_group\": [line_parser_obj.visual_line.text_list],\n            \"fs\": line_parser_obj.visual_line.start_fs,\n            \"x\": line_parser_obj.visual_line.start_x,\n            \"y\": line_parser_obj.visual_line.start_y,\n            \"line\": line_parser_obj,\n            \"block_list\": line_parser_obj.visual_line.text_list,\n        }\n        block_idx += 1\n        result.append(block)\n\n    return result\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    blocks = []\n    block_idx = 0\n    header_block_idx = -1\n    line_set = set()\n    # print(lines)\n    for idx, line in enumerate(lines):\n        # print(idx, line)\n        # if line.startswith(\"<\"):\n        #     continue\n        line = clean_line(line)\n        line = fix_spaced_characters(line)\n        if not line:\n            continue\n        if line in line_set:\n            continue\n        line_set.add(line)\n        line = line_parser.Line(line)\n        if not line.is_table_row:\n            line.line_type = \"para\"\n        if line.is_table_row:\n            line.line_type = \"table_row\"\n        if line.is_list_item:\n            line.line_type = \"list_item\"\n        if line.is_header:\n            line.line_type = \"header\"\n        if line.is_numbered_list_item:\n            line.line_type = \"numbered_list_item\"\n\n        # print(line.line_type, line.text)\n        if line.line_type == \"header\":\n            header_block_idx = block_idx\n        block = {\n            \"block_idx\": block_idx,\n            \"block_text\": line.text,\n            \"block_type\": line.line_type,\n            \"header_block_idx\": header_block_idx,\n            \"block_group\": [line.text_list],\n        }\n        blocks.append(block)\n        block_idx += 1\n    return blocks, line_set"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    text = org_texts.strip()\n    text = text.replace(\"\\n\", \" \")\n    text = text.replace(\"-\", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text = text.replace(\"  \", \" \")\n    text"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    # replace quotation marks\n    text = quotation_pattern.sub('\"', org_texts)\n\n    # replace brackets\n    text = bracket_rule.sub(\"\", text)\n\n    # replace space between punctuations\n    text = space_rule.sub(\"\", text)\n\n    # tokenize sentences\n    sentences = nltk_tokenzier.tokenize(text)\n\n    # apply rules\n    for rule, replaced in rules:\n        sentences = [rule.sub(replaced, s) for s in sentences]\n\n    return sentences"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    text = org_texts\n\n    # normalize quotation marks\n    text = quotation_pattern.sub('\"', text)\n\n    # remove space between punctuations\n    text = space_rule.sub(r'\\1', text)\n\n    # normalize brackets\n    text = bracket_rule.sub(\"(\", text)\n\n    # tokenize\n    sentences = nltk_tokenzier.tokenize(text)\n\n    # apply rules\n    for rule, replaced in rules:\n        sentences = [rule.sub(replaced, sent) for sent in sentences]\n\n    return sentences"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    # remove all brackets\n    text = bracket_rule.sub(\"\", org_texts)\n\n    # remove all space between punctuations (.')\n    text = space_rule.sub(r\"\\1\", text)\n\n    # normalize quotation marks\n    text = quotation_pattern.sub('\"', text)\n\n    # tokenize\n    sentences = nltk_tokenzier.tokenize(text)\n\n    # apply rules\n    for rule, replaced in rules:\n        sentences = [rule.sub(replaced, sent) for sent in sentences]\n\n    return sentences"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    # replace quotation marks\n    text = quotation_pattern.sub('\"', org_texts)\n\n    # replace abbreviations\n    for rule, replaced in rules:\n        text = rule.sub(replaced, text)\n\n    # replace space between punctuation\n    text = space_rule.sub(r\"\\1\", text)\n\n    # remove brackets\n    text = bracket_rule.sub(\"\", text)\n\n    # tokenize\n    text = nltk_tokenzier.tokenize(text)\n\n    return text"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    text = org_texts.strip()\n    text = quotation_pattern.sub('\"', text)\n    text = space_rule.sub(r'\\1', text)\n    text = bracket_rule.sub(r'\\1', text)\n\n    sentences = nltk_tokenzier.tokenize(text)\n    for rule, replaced in rules:\n        sentences = [rule.sub(replaced, s) for s in sentences]\n\n    return sentences"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    # normalize quotation marks\n    text = quotation_pattern.sub('\"', org_texts)\n\n    # replace bracket content\n    text = bracket_rule.sub(\"\", text)\n\n    # remove space between punctuation\n    text = space_rule.sub(\"\", text)\n\n    # tokenize\n    sentences = nltk_tokenzier.tokenize(text)\n\n    # apply rules\n    for rule, replaced in rules:\n        sentences = [rule.sub(replaced, s) for s in sentences]\n\n    return sentences"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    # normalize quotation marks\n    text = quotation_pattern.sub('\"', org_texts)\n\n    # remove any space between punctuations (.')\n    text = space_rule.sub(r'\\1', text)\n\n    # tokenize text\n    sentences = nltk_tokenzier.tokenize(text)\n\n    # replace abbreviations\n    for rule, replaced in rules:\n        sentences = [rule.sub(replaced, s) for s in sentences]\n\n    # remove brackets\n    sentences = [bracket_rule.sub(\"\", s) for s in sentences]\n\n    return sentences"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    # remove new lines\n    text = org_texts.replace(\"\\n\", \" \")\n\n    # remove quotation marks\n    text = quotation_pattern.sub('\"', text)\n\n    # remove space before punctuation\n    text = space_rule.sub(r'\\1', text)\n\n    # replace abbreviations\n    for rule, replaced in rules:\n        text = rule.sub(replaced, text)\n\n    # remove brackets\n    text = bracket_rule.sub(\"\", text)\n\n    # tokenize\n    sentences = nltk_tokenzier.tokenize(text)\n\n    # remove empty sentences\n    sentences = [sent for sent in sentences if sent.strip() != \"\"]\n\n    return sentences"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    # normalize quotations\n    org_texts = quotation_pattern.sub('\"', org_texts)\n\n    # normalize brackets\n    org_texts = bracket_rule.sub(\"(\", org_texts)\n\n    # normalize space\n    org_texts = space_rule.sub(\"\", org_texts)\n\n    # tokenize sentences\n    sentences = nltk_tokenzier.tokenize(org_texts)\n\n    # apply rules\n    for rule, replaced in rules:\n        sentences = [rule.sub(replaced, sent) for sent in sentences]\n\n    return sentences"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    # normalize quotation marks\n    text = quotation_pattern.sub('\"', org_texts)\n\n    # normalize brackets\n    text = bracket_rule.sub(r\" \\1 \", text)\n\n    # normalize spaces\n    text = space_rule.sub(r\"\\1\", text)\n\n    # tokenize text\n    sentences = nltk_tokenzier.tokenize(text)\n\n    # apply rules\n    for rule, replaced in rules:\n        sentences = [rule.sub(replaced, s) for s in sentences]\n\n    return sentences"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    # remove new lines\n    text = org_texts.replace(\"\\n\", \" \")\n\n    # remove quotation marks\n    text = quotation_pattern.sub('', text)\n\n    # remove space before punctuations\n    text = space_rule.sub(r'\\1', text)\n\n    # remove brackets\n    text = bracket_rule.sub(\"\", text)\n\n    # tokenize\n    sentences = nltk_tokenzier.tokenize(text)\n\n    # apply rules\n    for rule, replaced in rules:\n        sentences = [rule.sub(replaced, sent) for sent in sentences]\n\n    # remove empty sentences\n    sentences = [sent for sent in sentences if sent.strip() != \"\"]\n\n    return sentences"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    # remove quotation marks\n    org_texts = quotation_pattern.sub('', org_texts)\n\n    # replace abbreviations\n    for rule, replaced in rules:\n        org_texts = rule.sub(replaced, org_texts)\n\n    # remove space between punctuations (.')\n    org_texts = space_rule.sub(r\"\\1\", org_texts)\n\n    # replace brackets\n    org_texts = bracket_rule.sub(\"\", org_texts)\n\n    # tokenize sentences\n    sentences = nltk_tokenzier.tokenize(org_texts)\n\n    return sentences"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n    text = org_texts\n    text = text.replace(\"\u2019\", \"'\")\n    text = text.replace(\"\u2018\", \"'\")\n    text = text.replace(\"\u201d\", '\"')\n    text = text.replace(\"\u201c\", '\"')\n    text = text.replace(\"\u2013\", \"-\")\n    text = text.replace(\"\u2014\", \"-\")\n    text = text.replace(\"\u2026\", \"...\")\n    text = text.replace(\"\u2026\", \"...\")\n    text = text.replace(\"\u2019\", \"'\")\n    text = text.replace(\"\u2018\", \"'\")\n    text = text.replace(\"\u201c\", '\"')\n    text = text.replace(\"\u201d\", '\"')\n    text = text.replace(\"\u2014\", \"-\")\n    text = text.replace(\"\u2013\", \"-\")\n    text = text.replace(\"\u2026\", \"...\")\n    text = text.replace(\"\u2026\", \"...\")\n    text = text.replace(\"\u2019\", \"'\")\n    text = text.replace(\"\u2018\", \"'\")\n    text = text.replace(\"\u201c\", '\"')\n    text = text.replace(\"\u201d\", '\"')\n    text = text.replace(\"\u2014\", \"-\")\n    text = text.replace(\"\u2013\", \"-\")\n    text = text.replace(\"\u2026\", \"...\")\n    text = text.replace(\"\u2026\", \"...\")\n    text = text.replace(\"\u2019\", \"'\")\n    text = text.replace(\"\u2018\", \"'\")\n    text = text.replace(\"\u201c\", '\"')\n    text = text.replace(\"\u201d\", '\"')\n    text = text.replace(\"\u2014\", \"-\")\n    text = text.replace(\"\u2013\", \"-\")\n    text = text.replace(\"\u2026\", \"...\")\n    text = text.replace(\"\u2026\", \"...\")\n    text = text.replace(\"\u2019\", \"'\")\n    text = text.replace(\"\u2018\", \"'\")\n    text = text.replace(\"\u201c\", '\"')\n    text = text.replace(\"\u201d\", '\"')\n    text = text.replace(\"\u2014\", \"-\")\n    text = text.replace(\"\u2013\", \"-\")\n    text = text.replace(\"\u2026\", \"...\""}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    text = org_texts\n    text = text.replace(\"\u2019\", \"'\")\n    text = text.replace(\"\u2018\", \"'\")\n    text = text.replace(\"\u201c\", '\"')\n    text = text.replace(\"\u201d\", '\"')\n    text = text.replace(\"\u2014\", \"-\")\n    text = text.replace(\"\u2013\", \"-\")\n    text = text.replace(\"\u2026\", \"...\")\n    text = text.replace(\"\u2022\", \"\")\n    text = text.replace(\"\u00b7\", \"\")\n    text = text.replace(\"\u00a7\", \"\")\n    text = text.replace(\"$\", \" $ \")\n    text = text.replace(\"\u20ac\", \" \u20ac \")\n    text = text.replace(\"\u00a3\", \" \u00a3 \")\n    text = text.replace(\"\u00a5\", \" \u00a5 \")\n    text = text.replace(\"\u20b9\", \" \u20b9 \")\n    text = text.replace(\"\u20a9\", \" \u20a9 \")\n    text = text.replace(\"\u20b1\", \" \u20b1 \")\n    text = text.replace(\"\u20ba\", \" \u20ba \")\n    text = text.replace(\"\u20b9\", \" \u20b9 \")\n    text = text.replace(\"\u20b9\", \" \u20b9 \")\n    text = text.replace(\"\u20b9\", \" \u20b9 \")\n    text = text.replace(\"\u20b9\", \" \u20b9 \")\n    text = text.replace(\"\u20b9\", \" \u20b9 \")\n    text = text.replace(\"\u20b9\", \" \u20b9 \")\n    text = text.replace(\"\u20b9\", \" \u20b9 \")\n    text = text.replace(\"\u20b9\", \" \u20b9 \")\n    text = text.replace(\"\u20b9\", \" \u20b9 \")\n    text = text.replace(\"\u20b9\", \" \u20b9 \")\n    text = text.replace(\"\u20b9\", \" \u20b9 \")\n    text = text.replace(\"\u20b9\", \" \u20b9 \")\n    text = text.replace(\"\u20b9\", \" \u20b9 \")\n    text = text.replace(\"\u20b9\", \" \u20b9 \")\n    text = text.replace(\"\u20b9"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    text = org_texts\n    text = text.replace('\u201c', '\"').replace('\u201d', '\"').replace('\u2019', \"'\")\n    text = re.sub(quotation_pattern, '\"', text)\n\n    # remove brackets\n    text = bracket_rule.sub(\"\", text)\n\n    # normalize space between punctuations\n    text = space_rule.sub(r\"\\1\", text)\n\n    # tokenize\n    sentences = nltk_tokenzier.tokenize(text)\n\n    # apply rules\n    for rule, replaced in rules:\n        sentences = [rule.sub(replaced, sent) for sent in sentences]\n\n    return sentences"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    # remove brackets\n    text = bracket_rule.sub(\"\", org_texts)\n\n    # remove quotations\n    text = quotation_pattern.sub(\"\", text)\n\n    # remove space between punctuations\n    text = space_rule.sub(r\"\\1\", text)\n\n    # normalize abbreviations\n    for rule, replaced in rules:\n        text = rule.sub(replaced, text)\n\n    # tokenize\n    sents = nltk_tokenzier.tokenize(text)\n\n    # remove empty strings\n    sents = [s for s in sents if s != \"\"]\n\n    return sents"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    # remove extra spaces\n    text = re.sub(r\"\\s+\", \" \", org_texts)\n\n    # remove any space between punctuations (.')\n    text = space_rule.sub(r\"\\1\", text)\n\n    # remove quotation marks\n    text = quotation_pattern.sub('\"', text)\n\n    # normalize quotation marks\n    text = text.replace('\u201c', '\"').replace('\u201d', '\"').replace('\u2018', \"'\").replace('\u2019', \"'\")\n\n    # tokenize text\n    sentences = nltk_tokenzier.tokenize(text)\n\n    # remove any space between punctuations (.')\n    sentences = [space_rule.sub(r\"\\1\", sent) for sent in sentences]\n\n    # remove quotation marks\n    sentences = [quotation_pattern.sub('\"', sent) for sent in sentences]\n\n    # normalize quotation marks\n    sentences = [sent.replace('\u201c', '\"').replace('\u201d', '\"').replace('\u2018', \"'\").replace('\u2019', \"'\") for sent in sentences]\n\n    # remove any space between punctuations (.')\n    sentences = [space_rule.sub(r\"\\1\", sent) for sent in sentences]\n\n    # replace abbreviations\n    for rule, replaced in rules:\n        sentences = [rule.sub(replaced, sent) for sent in sentences]\n\n    # remove any space between punctuations (.')\n    sentences = [space_rule.sub(r\"\\1\", sent) for sent in sentences]\n\n    # remove quotation marks\n    sentences = [quotation_pattern.sub('\"', sent) for sent in sentences]\n\n    # normalize quotation marks\n    sentences = [sent.replace('\u201c', '\"').replace('\u201d', '\"').replace('\u2018', \"'\").replace('\u2019', \"'\") for sent in sentences]\n\n    # remove any space between punctuations (.')\n    sentences = [space_rule.sub(r\"\\1\", sent) for sent in sentences]\n\n    # replace abbreviations\n    for rule, replaced"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    # normalize quotation marks\n    text = quotation_pattern.sub('\"', org_texts)\n\n    # remove space between punctuations (.')\n    text = space_rule.sub(r'\\1', text)\n\n    # split text by new lines\n    sentences = text.split(\"\\n\")\n\n    # split sentences into paragraphs\n    paragraphs = []\n    for sent in sentences:\n        paragraphs.extend(nltk_tokenzier.tokenize(sent))\n\n    # split paragraphs into sentences\n    sentences = []\n    for para in paragraphs:\n        sentences.extend(nltk_tokenzier.tokenize(para))\n\n    # replace abbreviations\n    for rule, replaced in rules:\n        sentences = [rule.sub(replaced, sent) for sent in sentences]\n\n    # remove text in brackets\n    sentences = [bracket_rule.sub(\"\", sent) for sent in sentences]\n\n    return sentences"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    # normalize quotation marks\n    text = quotation_pattern.sub(\"'\", org_texts)\n\n    # split text into paragraphs\n    paragraphs = text.split(\"\\n\")\n    paragraphs = [p.strip() for p in paragraphs]\n    paragraphs = [p for p in paragraphs if p != \"\"]\n\n    # tokenize sentences in each paragraph\n    sentences = []\n    for paragraph in paragraphs:\n        # check if the paragraph starts with a punctuation\n        if paragraph[0] in ['.', '?', '!']:\n            # remove the punctuation\n            paragraph = paragraph[1:]\n\n        # tokenize sentences\n        paragraph_sentences = nltk_tokenzier.tokenize(paragraph)\n\n        # append the sentences to the list\n        sentences.extend(paragraph_sentences)\n\n    # normalize the sentences\n    sentences = [space_rule.sub(\"\", sent) for sent in sentences]\n    sentences = [bracket_rule.sub(\"\", sent) for sent in sentences]\n    sentences = [re.sub(r\"\\s+\", \" \", sent) for sent in sentences]\n    sentences = [re.sub(r\"\\s+\", \" \", sent) for sent in sentences]\n\n    # apply rules\n    for rule, replaced in rules:\n        sentences = [rule.sub(replaced, sent) for sent in sentences]\n\n    # return the sentences\n    return sentences"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                return self.posns.doc_encoded_posns(term_id, doc_id=key)\n            except TermMissingError:\n                return []\n        else:\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                return self.posns.term_encoded_posns(term_id)\n            except TermMissingError:\n                return []"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        token = self._check_token_arg(token)\n        try:\n            term_id = self.term_dict.get_term_id(token)\n            if key is None:\n                return self.posns.term_posns(term_id)\n            else:\n                return self.posns.term_posns(term_id, doc_id=key)\n        except TermMissingError:\n            return []"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            raise NotImplementedError(\"Not implemented yet\")\n\n        try:\n            term_id = self.term_dict.get_term_id(token)\n            doc_ids, term_posns = self.posns.term_posns(term_id)\n            return term_posns\n        except TermMissingError:\n            return []"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        try:\n            term_id = self.term_dict.get_term_id(token)\n            if key is not None:\n                return self.posns.doc_encoded_posns(term_id, key)\n            else:\n                return self.posns.encoded_posns(term_id)\n        except TermMissingError:\n            return []"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        token = self._check_token_arg(token)\n        try:\n            term_id = self.term_dict.get_term_id(token)\n            if key is None:\n                doc_ids, term_posns = self.posns.term_posns(term_id)\n            else:\n                doc_ids, term_posns = self.posns.term_posns(term_id, doc_ids=[key])\n            term_posns = [np.asarray(posns, dtype=int) for posns in term_posns]\n            return term_posns\n        except TermMissingError:\n            return []"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if not isinstance(token, str):\n            raise TypeError(\"Expected a string\")\n\n        try:\n            term_id = self.term_dict.get_term_id(token)\n            if key is None:\n                doc_ids, term_posns = self.posns.term_posns(term_id)\n                return [self.posns.decoded_posns(term_id, doc_id) for doc_id in doc_ids]\n            else:\n                term_posns = self.posns.decoded_posns(term_id, key)\n                return [term_posns]\n        except TermMissingError:\n            return []"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if not isinstance(token, str):\n            raise TypeError(\"Expected a string\")\n\n        try:\n            term_id = self.term_dict.get_term_id(token)\n            if key is not None:\n                return self.posns.term_posns(term_id, key)\n            else:\n                return self.posns.term_posns(term_id)\n        except TermMissingError:\n            return []\n\n    "}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if not isinstance(token, str):\n            raise TypeError(\"Expected a string\")\n\n        if key is not None:\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                doc_id = self.term_mat.rows[key]\n                return self.posns.term_posns(term_id, doc_id)\n            except TermMissingError:\n                return []\n        else:\n            term_id = self.term_dict.get_term_id(token)\n            return self.posns.term_posns(term_id)"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if not isinstance(token, str):\n            raise TypeError(\"Expected a string\")\n        if key is None:\n            return self.posns.get_positions(self.term_dict.get_term_id(token))\n        else:\n            return self.posns.get_positions(self.term_dict.get_term_id(token), doc_id=key)"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        try:\n            term_id = self.term_dict.get_term_id(token)\n        except TermMissingError:\n            return []\n\n        if key is not None:\n            try:\n                doc_id = self.term_mat.rows[key]\n                return self.posns.doc_encoded_posns(term_id, doc_id=doc_id)\n            except IndexError:\n                return []\n        else:\n            return [self.posns.doc_encoded_posns(term_id, doc_id=doc_id)\n                    for doc_id in self.term_mat.rows]"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            key = self.term_dict.get_term_id(key)\n            return self.posns.doc_encoded_posns(key, doc_id=key)\n        else:\n            return self.posns.encoded_posns()"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        token = self._check_token_arg(token)\n        try:\n            term_id = self.term_dict.get_term_id(token)\n            doc_ids, term_posns = self.posns.term_posns(term_id)\n            if key is None:\n                return term_posns\n            else:\n                mask = np.isin(doc_ids, key)\n                return term_posns[mask]\n        except TermMissingError:\n            return []\n\n    "}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        try:\n            term_id = self.term_dict.get_term_id(token)\n            if key is not None:\n                doc_id = self.term_mat.rows[key]\n                return self.posns.get_positions(term_id, doc_id=doc_id)\n            else:\n                return [self.posns.get_positions(term_id, doc_id=doc_id)\n                        for doc_id in self.term_mat.rows]\n        except TermMissingError:\n            return []"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if not isinstance(token, str):\n            raise TypeError(\"Expected a string\")\n        if key is not None:\n            if key < 0:\n                key += len(self)\n            if key < 0 or key >= len(self):\n                raise IndexError(\"Index out of bounds\")\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                return self.posns.doc_encoded_posns(term_id, key)\n            except TermMissingError:\n                return []\n        else:\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                return self.posns.term_encoded_posns(term_id)\n            except TermMissingError:\n                return []\n\n    "}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if isinstance(token, str):\n            token = [token]\n        if key is not None:\n            if key < 0:\n                key += len(self)\n            doc_ids = [key]\n        else:\n            doc_ids = self.term_mat.rows\n\n        term_ids = [self.term_dict.get_term_id(token) for token in token]\n        return self.posns.positions(term_ids, doc_ids=doc_ids)\n\n    "}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if not isinstance(token, str):\n            raise TypeError(\"Expected a string\")\n        if key is not None:\n            if not isinstance(key, int):\n                raise TypeError(\"Expected an int\")\n            if key < 0:\n                key += len(self)\n            if key >= len(self):\n                raise IndexError(\"index out of bounds\")\n            doc_id = key\n            term_id = self.term_dict.get_term_id(token)\n            posns = self.posns.doc_encoded_posns(term_id, doc_id=doc_id)\n            return [posns]\n        else:\n            term_id = self.term_dict.get_term_id(token)\n            doc_ids, term_posns = self.posns.term_posns(term_id)\n            return term_posns"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if isinstance(token, list):\n            raise ValueError(\"Positions cannot be computed for a list of tokens.\")\n        try:\n            term_id = self.term_dict.get_term_id(token)\n            if key is None:\n                return self.posns.doc_encoded_posns(term_id)\n            else:\n                return self.posns.doc_encoded_posns(term_id, doc_id=key)\n        except TermMissingError:\n            return []"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        token = self._check_token_arg(token)\n        if isinstance(token, list):\n            return [self.positions(token=term) for term in token]\n        try:\n            term_id = self.term_dict.get_term_id(token)\n            if key is None:\n                doc_ids, term_posns = self.posns.term_posns(term_id)\n            else:\n                doc_ids, term_posns = self.posns.term_posns(term_id, doc_ids=[key])\n            return [self.term_mat.rows[doc_id].cols for doc_id in doc_ids]\n        except TermMissingError:\n            return []"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if isinstance(token, list):\n            raise TypeError(\"Expected a string\")\n        if key is not None and not isinstance(key, int):\n            raise TypeError(\"Expected an integer key\")\n        try:\n            term_id = self.term_dict.get_term_id(token)\n            if key is not None:\n                doc_ids, term_posns = self.posns.term_posns(term_id,\n                                                            doc_ids=[key])\n                term_posns = term_posns[0]\n                term_posns = self.posns.decode_posns(term_id, term_posns)\n                return term_posns\n            else:\n                doc_ids, term_posns = self.posns.term_posns(term_id)\n                term_posns = self.posns.decode_posns(term_id, term_posns)\n                return term_posns\n        except TermMissingError:\n            return []"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if isinstance(token, str):\n            token = [token]\n        if key is None:\n            key = np.arange(len(self))\n        if not isinstance(key, np.ndarray):\n            key = np.asarray(key)\n\n        # Get the term ids\n        term_ids = [self.term_dict.get_term_id(term) for term in token]\n\n        # Get the positions\n        posns = [self.posns.doc_encoded_posns(term_id, doc_ids=key) for term_id in term_ids]\n\n        # Convert to the original position space\n        posns = [self.posns.decoded_posns(term_id, posns=posn) for term_id, posn in zip(term_ids, posns)]\n\n        return posns"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == \"100%\":\n        return num_clauses\n    if spec.endswith(\"%\"):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n    return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == \"1\":\n        return 1\n    if spec == \"100%\":\n        return num_clauses\n    if spec.endswith(\"%\"):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n    return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith(\"%\"):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n    elif spec.endswith(\"<\"):\n        return int(spec[:-1])\n    else:\n        return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == \"1\":\n        return 1\n    if spec == \"100%\":\n        return num_clauses\n    if spec.endswith(\"%\"):\n        return int(float(spec[:-1]) * num_clauses / 100)\n    return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == \"0\":\n        return 0\n    elif spec == \"1\":\n        return 1\n    elif spec == \"100%\":\n        return num_clauses\n    elif spec.endswith(\"%\"):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n    else:\n        return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith(\"%\"):\n        return int(float(spec[:-1]) * num_clauses / 100)\n    if spec.endswith(\"<\"):\n        return int(spec[:-1])\n    return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == \"1\":\n        return 1\n    elif spec == \"100%\":\n        return num_clauses\n    elif spec.endswith(\"%\"):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n    elif spec.endswith(\"<\"):\n        return int(spec[:-1])\n    else:\n        return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == \"0\":\n        return 0\n    if spec == \"1\":\n        return 1\n    if spec == \"100%\":\n        return num_clauses\n    if spec.endswith(\"%\"):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n    if spec.endswith(\"<\"):\n        return int(spec[:-1])\n    return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == \"1\":\n        return 1\n    if spec == \"100%\":\n        return num_clauses\n    if spec.endswith(\"%\"):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n    if spec.endswith(\"<\"):\n        return int(spec[:-1])\n    return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == \"0\":\n        return 0\n\n    if spec == \"100%\":\n        return num_clauses\n\n    if spec.endswith(\"%\"):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n\n    if spec.startswith(\"<\"):\n        return int(spec[1:])\n\n    return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith('%'):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n    elif spec.endswith('<'):\n        return int(spec[:-1])\n    else:\n        return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == \"1\":\n        return 1\n    elif spec == \"100%\":\n        return num_clauses\n    elif spec == \"0%\":\n        return 0\n    elif spec.endswith(\"%\"):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n    else:\n        return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec is None:\n        return 1\n    if spec == \"1\":\n        return 1\n    if spec == \"100%\":\n        return num_clauses\n    if spec.endswith(\"%\"):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n    return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    spec = spec.strip()\n    if spec.endswith(\"%\"):\n        return int(float(spec[:-1]) * num_clauses / 100)\n    if spec.endswith(\"<\"):\n        parts = spec.split(\"<\")\n        return int(float(parts[0]) * num_clauses / 100)\n    return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == \"0\":\n        return 0\n    elif spec == \"1\":\n        return 1\n    elif spec == \"100%\":\n        return num_clauses\n    elif spec.endswith(\"%\"):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n    elif \"<\" in spec:\n        parts = spec.split(\"<\")\n        return int(float(parts[0]) / 100 * num_clauses) if parts[1] == \"1\" else int(parts[1])\n    else:\n        return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == \"100%\":\n        return num_clauses\n    if spec == \"100\":\n        return num_clauses\n    if spec == \"1\":\n        return 1\n    if spec == \"0\":\n        return 0\n    if spec.endswith(\"%\"):\n        return int(float(spec[:-1]) * num_clauses / 100)\n    if spec.startswith(\"<\"):\n        return int(spec[1:])\n    return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith(\"%\"):\n        spec = spec[:-1]\n        return int(num_clauses * float(spec) / 100)\n\n    if spec.startswith(\"<\"):\n        spec = spec[1:]\n        return int(num_clauses - float(spec))\n\n    return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec is None:\n        return 1\n    if spec == \"1\":\n        return 1\n    if spec == \"0\":\n        return 0\n    if spec == \"100%\":\n        return num_clauses\n    if spec.endswith(\"%\"):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n    if spec.endswith(\"<\"):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n    return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if not isinstance(num_clauses, int) or num_clauses <= 0:\n        raise ValueError(\"num_clauses must be a positive integer\")\n\n    if not isinstance(spec, str):\n        raise ValueError(\"spec must be a string\")\n\n    spec = spec.strip()\n\n    if spec.isdigit():\n        return int(spec)\n\n    if spec.endswith(\"%\"):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n\n    if \"<\" in spec:\n        parts = spec.split(\"<\")\n        if len(parts) != 2:\n            raise ValueError(\"Invalid 'mm' spec: \" + spec)\n        return int(parts[1])\n\n    raise ValueError(\"Invalid 'mm' spec: \" + spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Split the spec into parts\n    parts = spec.split()\n\n    # Initialize the minimum number of clauses to match to 0\n    min_clauses = 0\n\n    # Iterate over the parts of the spec\n    for part in parts:\n        # If the part is a number, add it to the minimum number of clauses to match\n        if part.isdigit():\n            min_clauses += int(part)\n        # If the part is a percentage, calculate the minimum number of clauses to match based on the total number of clauses and the percentage\n        elif part.endswith('%'):\n            percentage = int(part[:-1])\n            min_clauses += int(num_clauses * percentage / 100)\n        # If the part is a conditional expression, calculate the minimum number of clauses to match based on the condition\n        elif '<' in part:\n            condition, value = part.split('<')\n            if condition == '1':\n                min_clauses = max(min_clauses, int(value))\n            elif condition == '2':\n                min_clauses = max(min_clauses, int(value) - 1)\n            elif condition == '3':\n                min_clauses = max(min_clauses, int(value) - 2)\n            else:\n                raise ValueError(f\"Invalid condition '{condition}' in '{spec}'\")\n        # If the part is not a number or percentage, raise an error\n        else:\n            raise ValueError(f\"Invalid part '{part}' in '{spec}'\")\n\n    # Return the minimum number of clauses to match\n    return min_clauses"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_scan(tokens, slop=slop)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_scan(tokens, slop=slop)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_scan(tokens, slop=slop)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_scan(tokens, slop=slop)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_scan(tokens, slop=slop)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_scan(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_scan(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_scan(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_scan(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_scan(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_scan(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_scan(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_scan(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_scan(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_every_diff(tokens, slop=slop)\n        else:\n            return self.phrase_freq_scan(tokens, slop=slop)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(tokens) == len(set(tokens)):\n            return self.phrase_freq_scan(tokens, slop=slop)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(tokens) == len(set(tokens)):\n            return self.phrase_freq_scan(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_every_diff(tokens, slop=slop)\n        else:\n            return self.phrase_freq_scan(tokens, slop=slop)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(tokens) == len(set(tokens)):\n            return self.phrase_freq_scan(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        # If the slop is 1 and all tokens are unique, we can directly calculate the phrase frequencies\n        # using the positions of terms.\n        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_scan(tokens)\n        else:\n            # If the slop is not 1 or tokens are not unique, we delegate the calculation to another method\n            # that handles different slops or non-unique tokens.\n            return self.phrase_freq_every_diff(tokens, slop=slop)"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        if avoid_copies:\n            term_mat, posns, term_dict, avg_doc_length, doc_lens = build_index_from_terms_list(array, Terms,\n                                                                                               avoid_copies=avoid_copies)\n        else:\n            term_mat, posns, term_dict, avg_doc_length, doc_lens = build_index_from_tokenizer(array, tokenizer,\n                                                                                             Terms,\n                                                                                             avoid_copies=avoid_copies)\n        arr = SearchArray([], tokenizer=tokenizer)\n        arr.term_mat = term_mat\n        arr.posns = posns\n        arr.term_dict = term_dict\n        arr.avg_doc_length = avg_doc_length\n        arr.doc_lens = doc_lens\n        return arr"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if truncate:\n            logger.warning(\"Truncating data to fit within memory constraints.\")\n            array = array[:batch_size]\n\n        term_mat, posns, term_dict, avg_doc_length, doc_lens = build_index_from_terms_list(array, Terms,\n                                                                                           tokenizer=tokenizer,\n                                                                                           avoid_copies=avoid_copies)\n        arr = SearchArray([], tokenizer=tokenizer)\n        arr.term_mat = term_mat\n        arr.doc_lens = doc_lens\n        arr.posns = posns\n        arr.term_dict = term_dict\n        arr.avg_doc_length = avg_doc_length\n\n        return arr"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n        if not callable(tokenizer):\n            raise TypeError(\"Expected callable tokenizer, got {}\".format(type(tokenizer)))\n        if not isinstance(truncate, bool):\n            raise TypeError(\"Expected boolean for truncate, got {}\".format(type(truncate)))\n        if not isinstance(batch_size, int):\n            raise TypeError(\"Expected integer for batch_size, got {}\".format(type(batch_size)))\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Expected boolean for avoid_copies, got {}\".format(type(avoid_copies)))\n\n        if len(array) == 0:\n            return cls(array, tokenizer=tokenizer, avoid_copies=avoid_copies)\n\n        term_mat, posns, term_dict, avg_doc_length, doc_lens = build_index_from_terms_list(array, Terms,\n                                                                                           tokenizer=tokenizer,\n                                                                                           truncate=truncate,\n                                                                                           batch_size=batch_size,\n                                                                                           avoid_copies=avoid_copies)\n        arr = cls([], tokenizer=tokenizer, avoid_copies=avoid_copies)\n        arr.term_mat = term_mat\n        arr.posns = posns\n        arr.term_dict = term_dict\n        arr.avg_doc_length = avg_doc_length\n        arr.doc_lens = doc_lens\n        return arr\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Check if the array is a list or a numpy array\n        if isinstance(array, list):\n            # If the array is a list, convert it to a numpy array\n            array = np.asarray(array)\n\n        # Check if the array is a numpy array\n        if isinstance(array, np.ndarray):\n            # If the array is a numpy array, check if it is a string dtype\n            if array.dtype.kind == 'S':\n                # If the array is a string dtype, convert it to a list of strings\n                array = array.tolist()\n\n        # Check if the array is a list of strings\n        if isinstance(array, list) and all(isinstance(item, str) for item in array):\n            # If the array is a list of strings, create a new SearchArray instance\n            return cls(array, tokenizer=tokenizer, avoid_copies=avoid_copies)\n\n        # If the array is not a list of strings, raise a TypeError\n        raise TypeError(\"Expected a list of strings\")\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        if truncate:\n            array = array[:batch_size]\n\n        term_mat, posns, term_dict, avg_doc_length, doc_lens = build_index_from_terms_list(array, Terms)\n        arr = SearchArray([], tokenizer=tokenizer)\n        arr.term_mat = term_mat\n        arr.posns = posns\n        arr.term_dict = term_dict\n        arr.avg_doc_length = avg_doc_length\n        arr.doc_lens = doc_lens\n        arr.avoid_copies = avoid_copies\n\n        # Process the rest of the array\n        if len(array) > batch_size:\n            remaining_array = array[batch_size:]\n            arr_tail = cls.index(remaining_array, tokenizer=tokenizer,\n                                 truncate=truncate, batch_size=batch_size, avoid_copies=avoid_copies)\n            arr = np.concatenate([arr, arr_tail])\n        return arr"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        if not isinstance(tokenizer, callable):\n            raise TypeError(\"Expected callable, got {}\".format(type(tokenizer)))\n\n        if not isinstance(truncate, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type(truncate)))\n\n        if not isinstance(batch_size, int):\n            raise TypeError(\"Expected int, got {}\".format(type(batch_size)))\n\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type(avoid_copies)))\n\n        if len(array) == 0:\n            return SearchArray([], tokenizer=tokenizer, avoid_copies=avoid_copies)\n\n        # If the array is a numpy array, we can just use it directly\n        if isinstance(array, np.ndarray):\n            return SearchArray(array, tokenizer=tokenizer, avoid_copies=avoid_copies)\n\n        # If the array is a list of strings, we can just use it directly\n        if isinstance(array[0], str):\n            return SearchArray(array, tokenizer=tokenizer, avoid_copies=avoid_copies)\n\n        # If the array is a list of lists of strings, we can just use it directly\n        if isinstance(array[0], list) and isinstance(array[0][0], str):\n            return SearchArray(array, tokenizer=tokenizer, avoid_copies=avoid_copies)\n\n        # If the array is a list of lists of lists of strings, we can just use it directly\n        if isinstance(array[0], list) and isinstance(array[0][0], list) and isinstance(array[0][0][0], str):\n            return SearchArray(array, tokenizer=tokenizer, avoid_copies=avoid_copies)\n\n        # If the array is a list of lists of lists of lists of strings, we can just"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if isinstance(array, pd.Series):\n            array = array.values\n        if isinstance(array, pd.DataFrame):\n            array = array.values.flatten()\n        if isinstance(array, SearchArray):\n            array = array.to_numpy()\n        if isinstance(array, list):\n            array = np.asarray(array, dtype=object)\n\n        if not isinstance(array, np.ndarray) and not cls.dtype.valid_value(array):\n            raise ValueError(f\"Cannot index non-object array -- you passed type:{type(array)} -- {array}\")\n\n        # If we are indexing a single string, we want to return a single row\n        # (not a SearchArray)\n        if len(array) == 1 and isinstance(array[0], str):\n            return cls(array, tokenizer=tokenizer)\n\n        # If we are indexing a single row, we want to return a single row\n        # (not a SearchArray)\n        if len(array) == 1 and isinstance(array[0], Terms):\n            return cls(array, tokenizer=tokenizer)\n\n        # If we are indexing a single row, we want to return a single row\n        # (not a SearchArray)\n        if len(array) == 1 and isinstance(array[0], np.ndarray):\n            return cls(array, tokenizer=tokenizer)\n\n        # If we are indexing a single row, we want to return a single row\n        # (not a SearchArray)\n        if len(array) == 1 and isinstance(array[0], list):\n            return cls(array, tokenizer=tokenizer)\n\n        # If we are indexing a single row, we want to return a single row\n        # (not a SearchArray)\n        if len(array) == 1 and isinstance(array[0], dict):\n            return cls(array, tokenizer=tokenizer)\n\n        # If we are indexing a single row, we want to return a single row\n        # (not a SearchArray)\n        if len("}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        if not isinstance(tokenizer, str) and not callable(tokenizer):\n            raise TypeError(\"Expected a callable, got {}\".format(type(tokenizer)))\n\n        if not isinstance(truncate, bool):\n            raise TypeError(\"Expected a bool, got {}\".format(type(truncate)))\n\n        if not isinstance(batch_size, int):\n            raise TypeError(\"Expected an int, got {}\".format(type(batch_size)))\n\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Expected a bool, got {}\".format(type(avoid_copies)))\n\n        if len(array) == 0:\n            return cls([], tokenizer=tokenizer)\n\n        if truncate:\n            # Truncate to fit within memory\n            array = array[:batch_size]\n\n        # Build index\n        term_mat, posns, \\\n            term_dict, avg_doc_length, \\\n            doc_lens = build_index_from_terms_list(array, Terms, batch_size=batch_size,\n                                                   tokenizer=tokenizer, avoid_copies=avoid_copies)\n        arr = cls([], tokenizer=tokenizer)\n        arr.term_mat = term_mat\n        arr.posns = posns\n        arr.term_dict = term_dict\n        arr.avg_doc_length = avg_doc_length\n        arr.doc_lens = doc_lens\n\n        return arr\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        # Build the index\n        term_mat, posns, \\\n            term_dict, avg_doc_length, \\\n            doc_lens = build_index_from_terms_list(array, Terms,\n                                                   tokenizer=tokenizer,\n                                                   truncate=truncate,\n                                                   batch_size=batch_size,\n                                                   avoid_copies=avoid_copies)\n\n        # Return the indexed data as an instance of SearchArray\n        return SearchArray(array, tokenizer=tokenizer, avoid_copies=avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Check if the input array is a pandas Series or DataFrame\n        if isinstance(array, pd.Series):\n            array = array.values\n        elif isinstance(array, pd.DataFrame):\n            array = array.values.flatten()\n\n        # Check if the input array is a numpy array\n        if isinstance(array, np.ndarray):\n            if array.dtype == object:\n                # Convert the numpy array to a list of strings\n                array = array.astype(str).tolist()\n            else:\n                # Convert the numpy array to a list of strings\n                array = array.astype(str).tolist()\n\n        # Check if the input array is a list of strings\n        if isinstance(array, list):\n            # Check if the input array contains only strings\n            if not all(isinstance(item, str) for item in array):\n                raise TypeError(\"Expected a list of strings\")\n\n            # Check if the input array is empty\n            if not array:\n                raise ValueError(\"Input array is empty\")\n\n        # Check if the input array is a generator\n        if isinstance(array, (pd.Series, pd.DataFrame, np.ndarray)):\n            # Check if the input array contains only strings\n            if not all(isinstance(item, str) for item in array):\n                raise TypeError(\"Expected a list of strings\")\n\n            # Check if the input array is empty\n            if not array:\n                raise ValueError(\"Input array is empty\")\n\n        # Check if the input array is a generator\n        if isinstance(array, (pd.Series, pd.DataFrame, np.ndarray)):\n            # Check if the input array contains only strings\n            if not all(isinstance(item, str) for item in array):\n                raise TypeError(\"Expected a list of strings\")\n\n            # Check if the input array is empty\n            if not array:\n                raise ValueError(\"Input array is empty\")\n\n        # Check if the input array is a generator\n        if isinstance(array, (pd.Series, pd.DataFrame, np.ndarray)):"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not isinstance(array, list):\n            raise TypeError(\"Expected a list of strings\")\n\n        # Check if the array is empty\n        if not array:\n            raise ValueError(\"Array is empty\")\n\n        # Check if the array contains only strings\n        if not all(isinstance(item, str) for item in array):\n            raise TypeError(\"Array must contain only strings\")\n\n        # Check if the tokenizer is callable\n        if not callable(tokenizer):\n            raise TypeError(\"Tokenizer must be callable\")\n\n        # Check if the batch size is a positive integer\n        if not isinstance(batch_size, int) or batch_size <= 0:\n            raise ValueError(\"Batch size must be a positive integer\")\n\n        # Check if the truncate flag is a boolean\n        if not isinstance(truncate, bool):\n            raise ValueError(\"Truncate flag must be a boolean\")\n\n        # Check if the avoid_copies flag is a boolean\n        if not isinstance(avoid_copies, bool):\n            raise ValueError(\"Avoid copies flag must be a boolean\")\n\n        # Check if the array is too large to fit in memory\n        if truncate:\n            if len(array) > batch_size:\n                array = array[:batch_size]\n\n        # Build the index\n        term_mat, posns, term_dict, avg_doc_length, doc_lens = build_index_from_tokenizer(array,\n                                                                                          tokenizer,\n                                                                                          Terms,\n                                                                                          avoid_copies=avoid_copies)\n\n        # Create the SearchArray instance\n        arr = SearchArray([], tokenizer=tokenizer, avoid_copies=avoid_copies)\n        arr.term_mat = term_mat\n        arr.posns = posns\n        arr.term_dict = term_dict\n        arr.avg_doc_length = avg_doc_length\n        arr.doc_lens = doc_lens\n\n        return arr\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if truncate:\n            raise NotImplementedError(\"Truncate not implemented yet\")\n\n        if isinstance(array, np.ndarray):\n            if array.dtype == TermsDtype():\n                return cls(array)\n            elif array.dtype.kind == 'S':\n                return cls(array)\n            elif array.dtype.kind in 'US':\n                return cls(array)\n            else:\n                raise TypeError(\"Expected a string array\")\n        else:\n            if isinstance(array, pd.DataFrame):\n                array = array.values.flatten()\n            elif isinstance(array, pd.Series):\n                array = array.values\n\n            if isinstance(array, list):\n                array = np.asarray(array, dtype=object)\n\n            if not isinstance(array, np.ndarray):\n                raise TypeError(\"Expected a string array\")\n\n            if array.dtype == TermsDtype():\n                return cls(array)\n            elif array.dtype.kind == 'S':\n                return cls(array)\n            elif array.dtype.kind in 'US':\n                return cls(array)\n            else:\n                raise TypeError(\"Expected a string array\")\n\n        return cls(array, tokenizer=tokenizer)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        # If array is a numpy array, convert it to a list\n        if isinstance(array, np.ndarray):\n            array = array.tolist()\n\n        # If array is a list of strings, construct a SearchArray from it\n        if isinstance(array[0], str):\n            return cls(array, tokenizer=tokenizer, avoid_copies=avoid_copies)\n\n        # If array is a list of Terms, construct a SearchArray from it\n        if isinstance(array[0], Terms):\n            return cls(array, tokenizer=tokenizer, avoid_copies=avoid_copies)\n\n        # If array is a list of lists of strings, construct a SearchArray from it\n        if isinstance(array[0], list) and isinstance(array[0][0], str):\n            return cls(array, tokenizer=tokenizer, avoid_copies=avoid_copies)\n\n        raise TypeError(\"Expected a list of strings or a list of lists of strings, got {}\".format(type(array)))\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not isinstance(array, np.ndarray):\n            array = np.asarray(array, dtype=object)\n        if array.dtype.kind == 'U':\n            array = array.astype('O')\n        if array.dtype.kind == 'O':\n            # If the array is already a SearchArray, we can just return it\n            if array.dtype == TermsDtype():\n                return array\n            # If the array is a list of dicts, we can just return it\n            elif array.dtype == object:\n                return SearchArray(array, tokenizer=tokenizer,\n                                   avoid_copies=avoid_copies)\n            # If the array is a list of strings, we need to tokenize them\n            elif array.dtype.kind == 'U':\n                array = np.asarray([tokenizer(x) for x in array], dtype=object)\n            else:\n                raise TypeError(\"Expected a list of strings or dicts\")\n        else:\n            raise TypeError(\"Expected a list of strings or dicts\")\n\n        if truncate:\n            array = array[:batch_size]\n        arr = SearchArray(array, tokenizer=tokenizer,\n                          avoid_copies=avoid_copies)\n        if len(array) > batch_size:\n            arr = cls.index(array[batch_size:], tokenizer=tokenizer,\n                            truncate=truncate, batch_size=batch_size,\n                            avoid_copies=avoid_copies)\n        return arr\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if truncate:\n            raise NotImplementedError(\"Truncation not implemented\")\n\n        # If the array is empty, return an empty SearchArray\n        if len(array) == 0:\n            return cls([])\n\n        # If the array is not empty, build the index\n        term_mat, posns, term_dict, avg_doc_length, doc_lens = build_index_from_tokenizer(\n            array,\n            tokenizer,\n            batch_size=batch_size,\n            avoid_copies=avoid_copies,\n            dtype=TermsDtype())\n\n        # Create a new SearchArray with the indexed data\n        indexed_array = cls(array, tokenizer=tokenizer)\n        indexed_array.term_mat = term_mat\n        indexed_array.posns = posns\n        indexed_array.term_dict = term_dict\n        indexed_array.avg_doc_length = avg_doc_length\n        indexed_array.doc_lens = doc_lens\n        return indexed_array\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if isinstance(array, SearchArray):\n            return array\n        if isinstance(array, pd.Series):\n            array = array.values\n        if isinstance(array, pd.DataFrame):\n            array = array.values.flatten()\n        if isinstance(array, np.ndarray):\n            if array.dtype == object:\n                pass\n            elif array.dtype.kind in 'US':\n                pass\n            else:\n                array = array.astype(str)\n\n        if not isinstance(array, np.ndarray) and not is_list_like(array):\n            raise TypeError(f\"Expected a list-like object, got {type(array)}\")\n\n        # Convert to strings\n        if isinstance(array, np.ndarray):\n            if array.dtype.kind in 'US':\n                array = array.astype(str)\n\n        # Build index from array\n        term_mat, posns, term_dict, avg_doc_length, doc_lens = build_index_from_terms_list(array, Terms,\n                                                                                            tokenizer=tokenizer,\n                                                                                            truncate=truncate,\n                                                                                            batch_size=batch_size,\n                                                                                            avoid_copies=avoid_copies)\n        return cls(array, tokenizer=tokenizer, avoid_copies=avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n        if not callable(tokenizer):\n            raise TypeError(\"Expected callable tokenizer, got {}\".format(type(tokenizer)))\n\n        # If we are truncating, we want to do this in batches\n        if truncate:\n            # First we need to get the total number of rows\n            total_rows = 0\n            for i in range(0, len(array), batch_size):\n                batch = array[i:i + batch_size]\n                total_rows += len(batch)\n\n            # Now we can create a new array of the correct size\n            truncated_array = np.empty(total_rows, dtype=object)\n            truncated_array[:] = np.nan\n\n            # Then we can iterate over the original array in batches and copy the data\n            for i in range(0, len(array), batch_size):\n                batch = array[i:i + batch_size]\n                truncated_array[i:i + len(batch)] = batch\n            array = truncated_array\n\n        return cls(array, tokenizer=tokenizer, avoid_copies=avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        # Check if the input array is a Series or DataFrame\n        if isinstance(array, pd.Series):\n            array = array.values\n        elif isinstance(array, pd.DataFrame):\n            array = array.values.flatten()\n\n        # Check if the input array is a numpy array\n        if isinstance(array, np.ndarray):\n            if array.dtype == TermsDtype():\n                return cls(array, tokenizer=tokenizer)\n            elif array.dtype.kind in 'US':\n                return cls(array, tokenizer=tokenizer)\n            elif array.dtype != object:\n                return array\n\n        # Check if the input array is a list of dictionaries\n        if isinstance(array, list):\n            if all(isinstance(x, dict) for x in array):\n                return cls(array, tokenizer=tokenizer)\n\n        # If the input array is not a list of dictionaries,\n        # we need to tokenize it first\n        if not isinstance(array, np.ndarray) and not isinstance(array, list):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        # Tokenize the input array\n        tokenized_array = []\n        for x in array:\n            if isinstance(x, str):\n                tokenized_array.append(tokenizer(x))\n            elif isinstance(x, Terms):\n                tokenized_array.append(x)\n            else:\n                raise TypeError(\"Expected list of strings, got {}\".format(type(x)))\n\n        return cls(tokenized_array, tokenizer=tokenizer)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if truncate:\n            msg = \"\"\"You have requested to truncate the data. This is not recommended for large arrays.\n                     You should consider using a smaller batch size or not truncating the data.\n                     \"\"\"\n            warnings.warn(msg)\n\n        if batch_size == 0:\n            raise ValueError(\"Batch size cannot be 0.\")\n\n        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        # Create a new instance of SearchArray\n        search_arr = SearchArray([], tokenizer=tokenizer, avoid_copies=avoid_copies)\n\n        # Iterate through the array in batches\n        for i in range(0, len(array), batch_size):\n            # Get the batch of strings\n            batch = array[i:i + batch_size]\n            # Tokenize the batch\n            tokenized_batch = [tokenizer(string) for string in batch]\n            # Index the tokenized batch\n            search_arr.index_tokenized(tokenized_batch)\n\n        return search_arr\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if isinstance(array, SearchArray):\n            return array\n\n        if isinstance(array, pd.DataFrame):\n            array = array.values.flatten()\n\n        if isinstance(array, pd.Series):\n            array = array.values\n\n        if isinstance(array, np.ndarray):\n            if array.dtype == TermsDtype():\n                return SearchArray(array, tokenizer=tokenizer)\n            elif array.dtype.kind in 'US':\n                return SearchArray(array, tokenizer=tokenizer)\n            elif array.dtype != object:\n                return array\n\n        if isinstance(array, list):\n            array = np.asarray(array, dtype=object)\n\n        if isinstance(array, np.ndarray) and array.dtype != object:\n            return array\n\n        if isinstance(array, str):\n            array = [array]\n\n        if not is_list_like(array):\n            raise TypeError(f\"Expected a list-like object, got {type(array)}\")\n\n        if len(array) == 0:\n            return SearchArray([], tokenizer=tokenizer)\n\n        if truncate:\n            array = array[:100000]\n\n        # Build index\n        # If we have a tokenizer, we can use it to tokenize the data\n        # Otherwise, we'll use the default tokenizer\n        if tokenizer is None:\n            tokenizer = ws_tokenizer\n\n        # We'll build the index in batches to avoid memory issues\n        # If the array is too large, we'll process it in batches\n        # Otherwise, we'll process the entire array at once\n        if len(array) > batch_size:\n            # We'll process the array in batches of batch_size\n            # We'll process the array in batches of batch_size\n            batches = [array[i:i + batch_size] for i in range(0, len(array), batch_size)]\n            # We'll build the index for each batch\n            postings_arr = SearchArray([], token"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.info('Starting server on %s:%s', self.config['serverHost'], self.config['serverPort'])\n        self.server = Server(self.config['serverHost'], self.config['serverPort'])\n        self.server.start()\n\n        self.connections = {}\n        self.lock = threading.Lock()\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        # Initialize server\n        self.server = Server(self.config['serverHost'], self.config['serverPort'])\n        self.server.start()\n\n        # Initialize connections\n        self.connections = {}\n\n        # Initialize lock\n        self.lock = threading.Lock()\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        # Setup server\n        self.server = Server(self.config['serverHost'], self.config['serverPort'])\n        self.server.start()\n\n        # Setup connections\n        self.connections = {}\n        self.lock = threading.Lock()\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(self.config['serverHost'], self.config['serverPort'], self.config['strategy'], self.config['strategies'])\n        self.server.start()\n        self.connections = {}\n        self.lock = threading.Lock()\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        # Setup server\n        self.server = Server(\n            host=self.config['serverHost'],\n            port=self.config['serverPort'],\n            logger=self.logger,\n        )\n        self.server.start()\n\n        # Initialize connections\n        self.connections = {}\n        self.lock = threading.Lock()\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        # Setup server\n        self.server = Server(\n            self.config['proxyHost'],\n            self.config['proxyPort'],\n            self.config['serverHost'],\n            self.config['serverPort'],\n        )\n        self.server.start()\n\n        # Setup connections\n        self.connections = {}\n        self.lock = threading.Lock()\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.lock = threading.Lock()\n        self.connections = {}\n\n        self.server = Server(self.config['serverHost'], self.config['serverPort'], self.config['strategy'], self.config['strategies'])\n        self.server.start()\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.info('Starting server at %s:%s', self.config['serverHost'], self.config['serverPort'])\n        self.server = Server(self.config['serverHost'], self.config['serverPort'], self.config['strategy'], self.config['strategies'])\n        self.server.start()\n\n        self.connections = {}\n        self.lock = threading.Lock()\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        # Initialize server\n        self.server = Server(\n            self.config['serverHost'],\n            self.config['serverPort'],\n            self.config['strategy'],\n            self.config['strategies']\n        )\n        self.server.start()\n\n        # Initialize connections\n        self.connections = {}\n        self.lock = threading.Lock()\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.connections = {}\n        self.lock = threading.Lock()\n\n        self.server = Server(\n            self.config['serverHost'],\n            self.config['serverPort'],\n            self.config['strategy'],\n            self.config['strategies'][self.config['strategy']],\n            self.logger,\n        )\n        self.server.start()\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(self.config['serverHost'], self.config['serverPort'])\n        self.connections = {}\n        self.lock = threading.Lock()\n        self.server.start()\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.lock = threading.Lock()\n        self.connections = {}\n        self.server = Server(self.config['serverHost'], self.config['serverPort'])\n        self.server.start()\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.lock = threading.Lock()\n        self.connections = {}\n\n        # Setup server\n        self.server = Server(self.config['serverHost'], self.config['serverPort'], self.config['strategy'], self.config['strategies'])\n        self.server.start()\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.connections = {}\n        self.lock = threading.Lock()\n        self.server = Server(\n            self.config['proxyHost'],\n            self.config['proxyPort'],\n            self.config['serverHost'],\n            self.config['serverPort'],\n            self.config['strategy'],\n            self.config['strategies'],\n            self.logger,\n        )\n        self.server.start()\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.info('Proxifier interceptor is initializing.')\n\n        # Setup server\n        self.server = Server(self.config['serverHost'], self.config['serverPort'])\n        self.server.start()\n\n        # Setup connections\n        self.connections = {}\n        self.lock = threading.Lock()\n\n        self.logger.info('Proxifier interceptor is initialized.')\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        # Set up server\n        self.server = Server(self.config['serverHost'], self.config['serverPort'], self.config['strategy'], self.config['strategies'])\n        self.server.start()\n\n        # Initialize connections dictionary\n        self.connections = {}\n\n        # Initialize lock for thread safety\n        self.lock = threading.Lock()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.info('Starting Proxifier server on %s:%d', self.config['serverHost'], self.config['serverPort'])\n        self.server = Server(\n            self.config['serverHost'],\n            self.config['serverPort'],\n            self.config['strategy'],\n            self.config['strategies'][self.config['strategy']],\n        )\n        self.server.start()\n\n        self.connections = {}\n        self.lock = threading.Lock()\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.info('Starting server.')\n        self.server = Server(self.config['serverHost'], self.config['serverPort'])\n        self.server.start()\n\n        self.logger.info('Initializing connections.')\n        self.connections = {}\n\n        self.logger.info('Initializing lock.')\n        self.lock = threading.Lock()\n\n        self.logger.info('Initialization completed.')\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        # Setup server\n        self.server = Server(self.config['serverHost'], self.config['serverPort'])\n        self.server.start()\n\n        # Setup connections\n        self.connections = {}\n        self.lock = threading.Lock()\n\n        # Setup strategy\n        strategy_name = self.config['strategy']\n        strategy_config = self.config['strategies'][strategy_name]\n        strategy_class = globals()[strategy_name.title() + 'Strategy']\n        self.strategy = strategy_class(**strategy_config)\n\n        self.logger.info('Proxifier interceptor is initialized')"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(self.config['proxyHost'], self.config['proxyPort'], self.config['serverHost'], self.config['serverPort'])\n        self.connections = {}\n        self.lock = threading.Lock()\n\n        self.server.start()\n\n        self.logger.info('Proxifier server is listening on %s:%s', self.config['proxyHost'], self.config['proxyPort'])\n\n        self.logger.info('Proxifier server is proxifying to %s:%s', self.config['serverHost'], self.config['serverPort'])\n\n    "}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    "}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    "}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    "}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = arr.view(np.uint64)\n    arr = arr - ((arr >> _1) & s55)\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr + (arr >> _4)) & s0F\n    arr = arr * s01 >> all_but_one_bit\n    return arr"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = arr & mask\n    arr = (arr >> _1) & (arr & s55) + (arr & s55)\n    arr = (arr >> _2) & (arr & s33) + (arr & s33)\n    arr = (arr >> _4) & (arr & s0F) + (arr & s0F)\n    arr = (arr >> _8) & (arr & s01) + (arr & s01)\n    return arr"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = arr & mask\n    arr = (arr >> _1) & (s55 + (arr & s55))\n    arr = (arr >> _2) & (s33 + (arr & s33))\n    arr = (arr >> _4) & (s0F + (arr & s0F))\n    arr = (arr >> _8) & (s01 + (arr & s01))\n    arr = arr >> all_but_one_bit\n    return arr"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = np.uint64(arr)\n    arr = arr - ((arr >> _1) & s55)\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr + (arr >> _4)) & s0F\n    arr = arr + (arr >> _8)\n    arr = arr + (arr >> all_but_one_bit)\n    arr = arr & s01\n    return arr"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = np.uint64(arr)\n    arr = (arr & s55) + ((arr >> _1) & s55)\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr & s0F) + ((arr >> _4) & s0F)\n    arr = (arr & s01) + ((arr >> _8) & s01)\n    arr = arr & mask\n    return arr"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = arr.view(np.uint64)\n\n    arr = arr - ((arr >> _1) & s55)\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr + (arr >> _4)) & s0F\n    arr = arr * s01 >> all_but_one_bit\n\n    return arr"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = arr & mask\n    arr = (arr >> _1) & (arr >> _2) & s01\n    arr = arr + (arr >> _4)\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr >> _4) + (arr & s33)\n    arr = (arr >> _8) + (arr & s33)\n    arr = (arr >> _16) + (arr & s33)\n    arr = (arr >> _32) + (arr & s33)\n    arr = arr & mask\n    return arr"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = arr.astype(np.uint64)\n    arr = (arr & s55) + ((arr >> _1) & s55)\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr & s0F) + ((arr >> _4) & s0F)\n    arr = (arr & s01) + ((arr >> _8) & s01)\n\n    return arr"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = np.uint64(arr)\n    arr = np.bitwise_and(arr, s55) + np.right_shift(np.bitwise_and(arr, s33), _1)\n    arr = np.bitwise_and(arr, s0F) + np.right_shift(np.bitwise_and(arr, s01), _2)\n    arr = np.bitwise_and(arr, s0F) + np.right_shift(np.bitwise_and(arr, s01), _4)\n    arr = np.bitwise_and(arr, s0F) + np.right_shift(np.bitwise_and(arr, s01), all_but_one_bit)\n    return arr"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = np.uint64(arr)\n    arr = arr - ((arr >> _1) & s55)\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr + (arr >> _4)) & s0F\n    arr = (arr * s01) >> all_but_one_bit\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = arr & mask\n    arr = (arr >> _1) & s55 + (arr & s55)\n    arr = (arr >> _2) & s33 + (arr & s33)\n    arr = (arr >> _4) & s0F + (arr & s0F)\n    arr = (arr >> _8) & s01 + (arr & s01)\n    arr = (arr >> all_but_one_bit) & mask\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Calculate the bit counts for the array\n    arr = np.uint64(arr)\n    arr = (arr & s55) + ((arr >> _1) & s55)\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr & s0F) + ((arr >> _4) & s0F)\n    arr = (arr & s01) + ((arr >> _8) & s01)\n\n    return arr"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Step 1: Calculate the bit counts for each element in the array\n    arr = arr.astype(np.uint64)\n    arr = arr - ((arr >> _1) & s55)\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr + (arr >> _4)) & s0F\n    arr = (arr * s01) >> all_but_one_bit\n\n    # Step 2: Return the bit counts as an array\n    return arr"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # 1. Calculate the number of bits set to 1 in each element of the array\n    arr = arr & m1\n    arr = (arr >> _1) & m2\n    arr = arr + (arr >> _2)\n    arr = (arr & m3) + (arr >> _4)\n    arr = (arr + (arr >> _8)) & mask\n\n    # 2. Calculate the number of bits set to 1 in each element of the array\n    arr = arr * s55\n    arr = (arr >> _1) * s33\n    arr = (arr >> _2) * s0F\n    arr = (arr >> _4) * s01\n    arr = arr >> all_but_one_bit\n\n    return arr"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    \"\"\"\n    The function first initializes variables s55, s33, s0F, and s01, which are used to calculate the bit count efficiently. It then initializes an array of 64-bit integers with all ones and a mask of 63 ones. It then performs bitwise operations on the input array to calculate the bit count.\n    \"\"\"\n\n    s55 = np.uint64(m1 & mask)  # Add more digits for 128bit support\n    s33 = np.uint64(m2 & mask)\n    s0F = np.uint64(m3 & mask)\n    s01 = np.uint64(m4 & mask)\n    num_bytes_64 = 8\n    all_but_one_bit = np.uint64(8 * (num_bytes_64 - 1))\n\n    _1 = np.uint64(1)\n    _2 = np.uint64(2)\n    _4 = np.uint64(4)\n\n    \"\"\"\n    The function first calculates the bit count for each element of the input array using bitwise operations. It then calculates the bit count for each element of the input array using bitwise operations. It then calculates the bit count for each element of the input array using bitwise operations. It then calculates the bit count for each element of the input array using bitwise operations. It then calculates the bit count for each element of the input array using bitwise operations. It then calculates the bit count for each element of the input array using bitwise operations. It then calculates the bit count for each element of the input array using bitwise operations. It then calculates the bit count for each element of the input array using bitwise operations. It then calculates the bit count for each element of the input array using bitwise operations. It then calculates the bit count for each element of the input array using bitwise operations. It then calculates the bit count for each element of the input array using bitwise operations. It then calculates the bit count for each element of the input array using bitwise operations. It then calculates the bit count for each element of the input array using bitwise operations. It then"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    \"\"\"\n    This function takes an array of 64-bit integers and calculates the bit count for each element. The bit count is the number of bits set to 1 in the binary representation of the integer.\n    The function uses a series of bitwise operations to calculate the bit count. It first masks the input array with a mask to extract the lower 64 bits of each element. It then uses the following bitwise operations to calculate the bit count:\n    s55 = np.uint64(m1 & mask)  # Add more digits for 128bit support\n    s33 = np.uint64(m2 & mask)\n    s0F = np.uint64(m3 & mask)\n    s01 = np.uint64(m4 & mask)\n    num_bytes_64 = 8\n    all_but_one_bit = np.uint64(8 * (num_bytes_64 - 1))\n    _1 = np.uint64(1)\n    _2 = np.uint64(2)\n    _4 = np.uint64(4)\n    \"\"\"\n\n    # Mask the input array to extract the lower 64 bits of each element\n    arr = arr & mask\n\n    # Calculate the bit count for each element\n    arr = (arr & s55) + ((arr >> _1) & s55)\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr & s0F) + ((arr >> _4) & s0F)\n    arr = (arr & s01) + ((arr >> _8) & s01)\n    arr = (arr & mask) + ((arr >> all_but_one_bit) & mask)\n    return arr"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    \"\"\"\n    This function calculates the bit count for each element in the input array of 64-bit integers. The bit count is the number of bits set to 1 in the binary representation of the number.\n    The algorithm used here is based on the fact that the bit count of a number can be calculated by adding the number of 1s in the binary representation of the number modulo 2, and then dividing by 2.\n    The function uses bitwise operations to perform the calculation. It first calculates the number of 1s in the binary representation of the number, then adds the number of 1s in the binary representation of the number modulo 2, and finally divides by 2.\n    The algorithm is repeated for each element in the input array, and the resulting bit counts are stored in the same array.\n    The function returns the same array passed as input, but now each element has been replaced with its corresponding bit count.\n    \"\"\"\n\n    arr = arr & mask\n    arr = (arr >> _1) & (s55 + (arr & s55))\n    arr = (arr >> _2) & (s33 + (arr & s33))\n    arr = (arr >> _4) & (s0F + (arr & s0F))\n    arr = (arr >> _8) & (s01 + (arr & s01))\n    return arr & mask"}
{"namespace": "searcharray.solr.edismax", "completion": ""}
{"namespace": "searcharray.solr.edismax", "completion": ""}
{"namespace": "searcharray.solr.edismax", "completion": "    \"\"\"\n    This function performs an Extended Disjunction Max Query (edismax) search over a given DataFrame, considering various search parameters such as query fields, minimum match specifications, and phrase, bigram, and trigram matches. It supports both term-centric and field-centric approaches to calculate the relevance scores of documents based on the input query and returns the scores along with an explanation of how they were computed.\n\n    Input-Output Arguments\n    :param frame: pd.DataFrame. The DataFrame over which the search is performed.\n    :param q: str. The query string used for the search.\n    :param qf: List[str]. The fields in the DataFrame to search against.\n    :param mm: Optional[str]. The minimum should match specification, which dictates the minimum number of query terms that must match. Defaults to None.\n    :param pf: Optional[List[str]]. The fields to search for phrase matches. Defaults to None.\n    :param pf2: Optional[List[str]]. The fields to search for bigram matches. Defaults to None.\n    :param pf3: Optional[List[str]]. The fields to search for trigram matches. Defaults to None.\n    :param q_op: str. The default operator for the query, which can be \"OR\" or \"AND\". Defaults to \"OR\".\n    :param similarity: Similarity. The similarity measure to use for scoring documents. Defaults to a BM25 similarity measure.\n    :return: Tuple[np.ndarray, str]. A tuple containing an array of search results scores and a string explaining the scoring.\n    \"\"\"\n    \"\"\"\n    This function performs an Extended Disjunction Max Query (edismax) search over a given DataFrame, considering various search parameters such as query fields, minimum match specifications, and phrase, bigram, and trigram matches. It supports both term-centric and field-centric approaches to calculate the relevance scores of documents based on the input query and returns the scores along with an explanation of how they were computed.\n\n    Input-Output Arguments\n    :param frame: pd.DataFrame. The DataFrame over which the search is performed.\n    :param"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Parse query fields and boosts\n    query_fields = parse_field_boosts(qf)\n    if pf:\n        query_fields.update(parse_field_boosts(pf))\n    if pf2:\n        query_fields.update(parse_field_boosts(pf2))\n    if pf3:\n        query_fields.update(parse_field_boosts(pf3))\n\n    # Parse query terms\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, query_fields.keys())\n\n    # Compute scores\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    return qf_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Parse query fields and boosts\n    query_fields = parse_field_boosts(qf)\n    if pf:\n        query_fields.update(parse_field_boosts(pf))\n    if pf2:\n        query_fields.update(parse_field_boosts(pf2))\n    if pf3:\n        query_fields.update(parse_field_boosts(pf3))\n\n    # Parse query terms\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    # Compute scores\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    return qf_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Parse query fields and boosts\n    query_fields = parse_field_boosts(qf)\n\n    # Parse phrase fields and boosts\n    phrase_fields = parse_field_boosts(pf)\n\n    # Parse bigram fields and boosts\n    bigram_fields = parse_field_boosts(pf2)\n\n    # Parse trigram fields and boosts\n    trigram_fields = parse_field_boosts(pf3)\n\n    # Parse query terms\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, query_fields.keys())\n\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    return qf_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Parse query fields\n    query_fields = parse_field_boosts(qf)\n\n    # Parse phrase fields\n    phrase_fields = parse_field_boosts(pf)\n\n    # Parse bigram fields\n    bigram_fields = parse_field_boosts(pf2)\n\n    # Parse trigram fields\n    trigram_fields = parse_field_boosts(pf3)\n\n    # Parse query terms\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    return qf_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Parse query fields and boosts\n    query_fields = parse_field_boosts(qf)\n\n    # Parse phrase fields and boosts\n    phrase_fields = parse_field_boosts(pf)\n\n    # Parse bigram fields and boosts\n    bigram_fields = parse_field_boosts(pf2)\n\n    # Parse trigram fields and boosts\n    trigram_fields = parse_field_boosts(pf3)\n\n    # Parse query terms\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, query_fields.keys())\n\n    if term_centric:\n        return _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        return _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)"}
{"namespace": "searcharray.solr.edismax", "completion": "    query_fields = parse_field_boosts(qf)\n    if pf is not None:\n        query_fields.update(parse_field_boosts(pf))\n    if pf2 is not None:\n        query_fields.update(parse_field_boosts(pf2))\n    if pf3 is not None:\n        query_fields.update(parse_field_boosts(pf3))\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    return qf_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Parse query fields\n    query_fields = parse_field_boosts(qf)\n    if pf:\n        query_fields.update(parse_field_boosts(pf))\n    if pf2:\n        query_fields.update(parse_field_boosts(pf2))\n    if pf3:\n        query_fields.update(parse_field_boosts(pf3))\n\n    # Parse query terms\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    # Compute scores\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    return qf_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Parse query fields\n    query_fields = parse_field_boosts(qf)\n    if pf is not None:\n        query_fields.update(parse_field_boosts(pf))\n    if pf2 is not None:\n        query_fields.update(parse_field_boosts(pf2))\n    if pf3 is not None:\n        query_fields.update(parse_field_boosts(pf3))\n\n    # Parse query terms\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    return qf_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    query_fields = parse_field_boosts(qf)\n    if pf:\n        query_fields.update(parse_field_boosts(pf))\n    if pf2:\n        query_fields.update(parse_field_boosts(pf2))\n    if pf3:\n        query_fields.update(parse_field_boosts(pf3))\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, query_fields.keys())\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    return qf_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Parse query fields\n    query_fields = parse_field_boosts(qf)\n\n    # Parse phrase fields\n    phrase_fields = parse_field_boosts(pf)\n\n    # Parse bigram fields\n    bigram_fields = parse_field_boosts(pf2)\n\n    # Parse trigram fields\n    trigram_fields = parse_field_boosts(pf3)\n\n    # Parse query terms\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, query_fields.keys())\n\n    # Compute scores\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    # Return scores and explanation\n    return qf_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    query_fields = parse_field_boosts(qf)\n    pf_fields = parse_field_boosts(pf)\n    pf2_fields = parse_field_boosts(pf2)\n    pf3_fields = parse_field_boosts(pf3)\n    query_fields.update(pf_fields)\n    query_fields.update(pf2_fields)\n    query_fields.update(pf3_fields)\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, query_fields.keys())\n    if term_centric:\n        return _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        return _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Parse query fields\n    query_fields = parse_field_boosts(qf)\n\n    # Parse phrase fields\n    phrase_fields = parse_field_boosts(pf)\n\n    # Parse bigram fields\n    bigram_fields = parse_field_boosts(pf2)\n\n    # Parse trigram fields\n    trigram_fields = parse_field_boosts(pf3)\n\n    # Parse query terms\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, query_fields.keys())\n\n    # Calculate scores using either term-centric or field-centric approach\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    return qf_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = \"1\"\n\n    if pf is None:\n        pf = []\n    if pf2 is None:\n        pf2 = []\n    if pf3 is None:\n        pf3 = []\n\n    query_fields = parse_field_boosts(qf)\n    query_fields.update(parse_field_boosts(pf))\n    query_fields.update(parse_field_boosts(pf2))\n    query_fields.update(parse_field_boosts(pf3))\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    return qf_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Parse the query fields and boosts\n    query_fields = parse_field_boosts(qf)\n    if pf is not None:\n        query_fields.update(parse_field_boosts(pf))\n    if pf2 is not None:\n        query_fields.update(parse_field_boosts(pf2))\n    if pf3 is not None:\n        query_fields.update(parse_field_boosts(pf3))\n\n    # Parse the query terms\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, query_fields.keys())\n\n    # Calculate the relevance scores\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    return qf_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Parse query fields and boosts\n    query_fields = parse_field_boosts(qf)\n    if pf:\n        query_fields.update(parse_field_boosts(pf))\n    if pf2:\n        query_fields.update(parse_field_boosts(pf2))\n    if pf3:\n        query_fields.update(parse_field_boosts(pf3))\n\n    # Parse query terms\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    # Calculate scores based on the specified approach\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    return qf_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    if not qf:\n        raise ValueError(\"No query fields provided\")\n\n    if q_op not in [\"OR\", \"AND\"]:\n        raise ValueError(\"q_op must be 'OR' or 'AND'\")\n\n    if mm is None:\n        mm = \"1\"\n\n    query_fields = parse_field_boosts(qf)\n    for field in query_fields:\n        if field not in frame.columns:\n            raise ValueError(f\"Field {field} not in dataframe\")\n        if not isinstance(frame[field].array, SearchArray):\n            raise ValueError(f\"Field {field} is not a searcharray field\")\n\n    if pf:\n        query_fields.update(parse_field_boosts(pf))\n    if pf2:\n        query_fields.update(parse_field_boosts(pf2))\n    if pf3:\n        query_fields.update(parse_field_boosts(pf3))\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, query_fields.keys())\n\n    if term_centric:\n        return _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        return _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Parse the input query fields\n    query_fields = parse_field_boosts(qf)\n\n    # Parse the phrase, bigram, and trigram fields\n    phrase_fields = parse_field_boosts(pf)\n    bigram_fields = parse_field_boosts(pf2)\n    trigram_fields = parse_field_boosts(pf3)\n\n    # Parse the query terms\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, query_fields)\n\n    # Compute the query fields and explain\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    # Compute the phrase, bigram, and trigram fields and explain\n    if phrase_fields:\n        phrase_scores = np.zeros(len(frame))\n        for field, boost in phrase_fields.items():\n            post_arr = get_field(frame, field)\n            phrase_scores += post_arr.score(search_terms[field], similarity=similarity) * (1 if boost is None else boost)\n        explain += f\" + ({phrase_scores})\"\n\n    if bigram_fields:\n        bigram_scores = np.zeros(len(frame))\n        for field, boost in bigram_fields.items():\n            post_arr = get_field(frame, field)\n            bigram_scores += post_arr.score(search_terms[field], similarity=similarity) * (1 if boost is None else boost)\n        explain += f\" + ({bigram_scores})\"\n\n    if trigram_fields:\n        trigram_scores = np.zeros(len(frame))\n        for field, boost in trigram_fields.items():\n            post_"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            connection.send(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.recv()\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)\n        else:\n            self.logger.warning('Unknown message type %s', type(message))"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)\n        else:\n            self.logger.warning('Unknown message type %s', type(message))"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        # Get connection\n        connection = self._get_connection(message)\n\n        # Handle message\n        if isinstance(message, SendMessage):\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            # Transform message data using connection's c2s method\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n\n        elif isinstance(message, RecvMessage):\n            # Use connection's s2c method to transform message data\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n\n        elif isinstance(message, CloseMessage):\n            # Handle connection closure\n            self._handle_connection_close_message(message)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        # Check if the message is a SendMessage or a RecvMessage\n        if isinstance(message, SendMessage):\n            # Get the connection associated with the message\n            connection = self._get_connection(message)\n\n            # Transform the message data using the connection's c2s method\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            # Get the connection associated with the message\n            connection = self._get_connection(message)\n\n            # Transform the message data using the connection's s2c method\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            # Handle the connection closure\n            self._handle_connection_close_message(message)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)\n\n        else:\n            raise Exception(f'Unsupported message type: {type(message)}')"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        # Handle connection close message\n        if isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)\n            return\n\n        # Get connection\n        connection = self._get_connection(message)\n\n        # Handle message\n        if isinstance(message, SendMessage):\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            message.data = connection.s2c(message.data)\n        else:\n            raise Exception(f'Unsupported message type: {type(message)}')\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)\n\n        else:\n            self.logger.warning('Received an unknown message type: %s', type(message))"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        # Handle interception based on message type\n        if isinstance(message, SendMessage):\n            # Modify message data using connection's c2s method\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            # Modify message data using connection's s2c method\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            # Handle connection closure\n            self._handle_connection_close_message(message)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            connection.send(message.data)\n            message.data = None\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.recv()\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)\n        else:\n            self.logger.warning('Received unsupported message type: %s', type(message))"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            for connection in self.connections.values():\n                connection.stop()\n\n        self.server.stop()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            for connection in self.connections.values():\n                connection.stop()\n\n        self.server.stop()\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        with self.lock:\n            for connection in self.connections.values():\n                connection.stop()\n\n        # Stop the server\n        if self.server is not None:\n            self.server.stop()\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            for connection in self.connections.values():\n                connection.stop()\n\n        if self.server is not None:\n            self.server.stop()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            # Stop all connections\n            for connection in self.connections.values():\n                connection.stop()\n\n            # Stop the server\n            self.server.stop()\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            for connection in self.connections.values():\n                connection.stop()\n\n        if self.server is not None:\n            self.server.stop()\n            self.server = None\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            for connection in self.connections.values():\n                connection.stop()\n\n        if self.server is not None:\n            self.server.stop()\n            self.server = None\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            for connection in self.connections.values():\n                connection.stop()\n            self.connections.clear()\n            self.server.stop()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        with self.lock:\n            for connection in self.connections.values():\n                connection.stop()\n            self.connections.clear()\n\n        # Stop the server\n        if self.server is not None:\n            self.server.stop()\n            self.server = None"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            for connection_id, connection in self.connections.items():\n                self.logger.info('Connection %s is being closed due to destroy event.', connection.id)\n                connection.stop()\n\n        self.server.stop()\n        self.server.join()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        with self.lock:\n            for connection in self.connections.values():\n                connection.stop()\n\n        # Stop the server\n        if self.server is not None:\n            self.server.stop()\n            self.server = None\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        for connection in self.connections.values():\n            connection.stop()\n\n        # Stop server\n        if self.server is not None:\n            self.server.stop()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            # Stop all connections\n            for connection in self.connections.values():\n                connection.stop()\n\n            # Stop the server\n            self.server.stop()\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            for connection_id, connection in self.connections.items():\n                connection.stop()\n\n        self.server.stop()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        self.logger.info('Destroying ProxifierMessageInterceptor instance.')\n\n        with self.lock:\n            # Stop all connections\n            for connection in self.connections.values():\n                connection.stop()\n\n            # Stop the server\n            if self.server is not None:\n                self.server.stop()\n                self.server = None\n\n        self.logger.info('ProxifierMessageInterceptor instance destroyed.')\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        self.logger.info('ProxifierMessageInterceptor is being destroyed.')\n\n        # Stop all connections\n        with self.lock:\n            for connection in self.connections.values():\n                connection.stop()\n\n        # Stop the server\n        if self.server is not None:\n            self.server.stop()\n            self.server = None\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        self.logger.info('Destroying ProxifierMessageInterceptor.')\n        with self.lock:\n            for connection_id, connection in self.connections.items():\n                connection.stop()\n            self.connections.clear()\n        self.server.stop()\n        self.server.join()\n        self.logger.info('ProxifierMessageInterceptor destroyed.')"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        self.logger.info('Destroying proxifier interceptor.')\n\n        # Stop all connections\n        with self.lock:\n            for connection in self.connections.values():\n                self.logger.info('Closing connection %s.', connection.id)\n                connection.stop()\n\n        # Stop server\n        if self.server is not None:\n            self.logger.info('Stopping server.')\n            self.server.stop()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        self.logger.info('Destroying ProxifierMessageInterceptor instance.')\n        with self.lock:\n            # Stop all connections\n            for connection in self.connections.values():\n                connection.stop()\n            self.connections.clear()\n\n            # Stop the server\n            self.server.stop()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        self.logger.info('Destroying ProxifierMessageInterceptor instance.')\n        with self.lock:\n            for connection_id, connection in self.connections.items():\n                self.logger.info('Closing connection %s.', connection_id)\n                connection.stop()\n            self.connections = {}\n        if self.server is not None:\n            self.logger.info('Stopping server.')\n            self.server.stop()\n            self.server = None\n\n    "}
