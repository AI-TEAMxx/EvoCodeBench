{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        raise ValueError(\"The provided output_dir doesn't have a URL.\")\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided directory should start with s3://. Found {output_dir.url}.\")\n\n    if append or overwrite:\n        raise NotImplementedError(\"Appending or overwriting data is not supported at this time.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Prefix=prefix,\n        MaxKeys=1  # We only need to check if at least one object exists\n    )\n\n    if objects.get('Contents'):\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.url}` is not empty.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        raise ValueError(\"The provided output_dir does not have a URL.\")\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided directory should be an S3 bucket. Found {output_dir.url}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    # Check for the existence of index.json in the specified directory\n    index_key = obj.path.lstrip(\"/\") + \"/index.json\"\n    try:\n        s3.head_object(Bucket=obj.netloc, Key=index_key)\n        # If the call succeeds, the index.json file exists and we raise an error\n        raise RuntimeError(f\"The index file 'index.json' already exists in the directory '{output_dir.url}'.\")\n    except botocore.exceptions.ClientError as e:\n        # If a 404 error is returned, the object does not exist and we can proceed\n        if e.response['Error']['Code'] != \"404\":\n            # If the error is not a 404, it means there was another issue accessing the object\n            raise RuntimeError(f\"Error checking for 'index.json' in the directory '{output_dir.url}': {e}\")\n\n    # If index.json does not exist, delete all objects within the specified prefix\n    response = s3.list_objects_v2(Bucket=obj.netloc, Prefix=obj.path.lstrip(\"/\"))\n    if 'Contents' in response:\n        for item in response['Contents']:\n            s3.delete_object(Bucket=obj.netloc, Key=item['Key'])"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait for all workers to finish writing their index files\n        while True:\n            files = os.listdir(self._cache_dir)\n            index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n            if len(index_files) == num_workers:\n                break\n            sleep(1)  # Wait for a second before checking again\n\n        # Only the master node (rank 0) should perform the actual merge\n        if node_rank is None or node_rank == 0:\n            self._merge_no_wait(node_rank)\n\n        # If not the master node, wait until the merged index file is available\n        elif node_rank is not None and node_rank != 0:\n            merged_index_file = os.path.join(self._cache_dir, _INDEX_FILENAME)\n            while not os.path.exists(merged_index_file):\n                sleep(1)  # Wait for a second before checking again"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "\n    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ImportError(\"The lightning SDK is not available, please install it to use this function.\")\n\n    if machine is None:\n        machine = Machine()  # Fetch default machine configuration\n\n    if command is None:\n        # Construct a default command that includes the current working directory and environment variables\n        command = f\"cd {os.getcwd()} && env\"\n\n    # Create a Studio object to interact with the Lightning Studio API\n    studio = Studio()\n\n    # Create a job with the specified parameters\n    job = studio.create_job(\n        name=name,\n        num_nodes=num_nodes,\n        machine_type=machine.type,\n        command=command,\n    )\n\n    # Submit the job\n    job.submit()\n\n    # Wait for the job to start and print the job URL\n    while job.status in [\"PENDING\", \"QUEUED\"]:\n        sleep(5)  # Wait for 5 seconds before checking the status again\n        job.refresh()  # Refresh the job status\n\n    if job.status == \"RUNNING\":\n        print(f\"Job URL: {job.url}\")\n    else:\n        raise RuntimeError(f\"Job failed to start. Status: {job.status}\")\n\n    # Wait for the job to complete\n    while job.status not in [\"COMPLETED\", \"FAILED\"]:\n        sleep(5)  # Wait for 5 seconds before checking the status again\n        job.refresh()  # Refresh the job status\n\n    if job.status == \"FAILED\":\n        raise RuntimeError(f\"Job failed. Status: {job.status}\")"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        try:\n            # Attempt to load the configuration from the cache directory\n            self._config = ChunksConfig.load_from_cache(\n                cache_dir=self._cache_dir,\n                serializers=self._serializers,\n                remote_input_dir=self._remote_input_dir,\n                item_loader=self._item_loader\n            )\n            return self._config\n        except Exception as e:\n            # Log the exception and return None if the configuration could not be loaded\n            logger.error(f\"Failed to load ChunksConfig: {e}\")\n            return None"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration must be defined before accessing it.\")\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The provided index must be an instance of ChunkedIndex.\")\n\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Ensure the prepare thread is running to manage chunk downloads and deletions\n        if self._prepare_thread is None or not self._prepare_thread.is_alive():\n            self._prepare_thread = PrepareChunksThread(\n                config=self._config,\n                item_loader=self._item_loader,\n                distributed_env=self._distributed_env,\n                max_cache_size=self._max_cache_size,\n            )\n            self._prepare_thread.start()\n\n        # Check if the chunk is already downloaded and available\n        if not self._item_loader.is_chunk_available(chunk_index):\n            # If not, request the chunk to be downloaded\n            self._prepare_thread.download([chunk_index])\n\n            # Wait for the chunk to be downloaded\n            while not self._item_loader.is_chunk_available(chunk_index):\n                # Sleep to avoid busy waiting\n                time.sleep(_DEFAULT_TIMEOUT)\n\n        # Once the chunk is available, read the item from the chunk\n        return self._item_loader.load_item(index)"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for root, dirs, files in os.walk(path):\n        for name in files:\n            try:\n                filepath = os.path.join(root, name)\n                if not os.path.islink(filepath):  # Skip symbolic links\n                    total_size += os.path.getsize(filepath)\n            except FileNotFoundError:\n                # If the file was deleted between os.walk() and os.path.getsize(), ignore it\n                pass\n    return total_size"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    # Check if the application is running in a distributed environment\n    lightning_app_external_url = os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\")\n    if lightning_app_external_url:\n        # If in a distributed environment, use the _ImmutableDistributedMap to broadcast the object\n        distributed_map = _ImmutableDistributedMap()\n        return distributed_map.set_and_get(key, obj)\n    else:\n        # If not in a distributed environment, return the object as is\n        return obj"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "\n    # Initialize the bins and their corresponding weights\n    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Pair each item with its weight and sort by weight in descending order\n    items_with_weights = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Distribute items into bins greedily\n    for item, weight in items_with_weights:\n        # Find the bin with the lowest total weight\n        lightest_bin = min(bin_weights, key=bin_weights.get, default=0)\n        \n        # If the number of bins is not yet reached, create a new bin\n        if len(bin_weights) < num_bins:\n            lightest_bin = len(bin_weights)\n            bin_weights[lightest_bin] = 0\n        \n        # Place the item in the bin with the lowest total weight\n        bins[lightest_bin].append(item)\n        bin_weights[lightest_bin] += weight\n\n    return bins, bin_weights"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "\n    # Seed the random number generator to ensure reproducibility\n    rng = np.random.default_rng(seed + current_epoch)\n\n    # Shuffle the chunks within each rank\n    shuffled_chunks_per_ranks = [rng.permutation(chunks) for chunks in chunks_per_ranks]\n\n    # Flatten the list of shuffled chunks across all ranks\n    shuffled_chunks = [chunk for rank_chunks in shuffled_chunks_per_ranks for chunk in rank_chunks]\n\n    return shuffled_chunks"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    suffixes = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']\n    i = 0\n    while num_bytes >= 1000 and i < len(suffixes)-1:\n        num_bytes /= 1000.0\n        i += 1\n    return f\"{num_bytes:.2f} {suffixes[i]}\""}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Extract indexed paths from the first two elements of the input sequence\n    indexed_paths = {}\n    for input_element in inputs[:2]:\n        indexed_paths.update(_get_indexed_paths(input_element))\n\n    # If no valid file paths are found, return None\n    if not indexed_paths:\n        return None\n\n    # Get the list of paths\n    paths = list(indexed_paths.values())\n\n    # Check if all paths are consistent and have the same root\n    common_prefix = os.path.commonprefix(paths)\n    if not common_prefix or not os.path.isdir(common_prefix):\n        raise ValueError(\"Inconsistent file paths found in the input sequence.\")\n\n    # Format the path to include the project root or a specified depth in the file system\n    input_dir = os.path.abspath(common_prefix)\n\n    return input_dir"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        optimize_dns(enable)\n        yield\n    finally:\n        optimize_dns(False)"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "\n    num_chunks = len(indexes)\n    world_size = distributed_env.world_size\n    chunks_per_rank = num_chunks // world_size\n    remainder = num_chunks % world_size\n\n    if drop_last and remainder:\n        num_chunks -= remainder\n\n    chunks_for_ranks = []\n    intervals_for_ranks = []\n\n    for rank in range(world_size):\n        start_index = rank * chunks_per_rank\n        end_index = start_index + chunks_per_rank + (1 if rank < remainder and not drop_last else 0)\n        chunks_for_ranks.append(indexes[start_index:end_index])\n        intervals_for_ranks.append(chunk_intervals[start_index:end_index])\n\n    return chunks_for_ranks, intervals_for_ranks"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_device:\n            kwargs['device'] = self._device\n        if self._contains_is_last:\n            kwargs['is_last'] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            # Attempt to retrieve the file metadata using the head_object method\n            response = s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip('/'))\n            return response\n        except botocore.exceptions.ClientError as e:\n            # If the error code is 404, the file does not exist yet, so wait and try again\n            if e.response['Error']['Code'] == '404':\n                sleep(sleep_time)\n            else:\n                # If a different error occurred, raise it\n                raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    import shutil\n    # Convert the threshold from gigabytes to bytes\n    threshold_in_bytes = threshold_in_gb * (1024 ** 3)\n\n    # Loop indefinitely until the condition is met\n    while True:\n        # Get the disk usage statistics about the given directory\n        total, used, free = shutil.disk_usage(input_dir)\n\n        # Check if the free space is lower than the threshold\n        if free < threshold_in_bytes:\n            break  # Exit the loop if the condition is met\n\n        # Sleep for the specified amount of time before checking again\n        sleep(sleep_time)"}
{"namespace": "litdata.processing.functions.optimize", "completion": "\n    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if isinstance(inputs, StreamingDataLoader):\n            input_dir = Dir()\n        else:\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default_num_workers(),\n            fast_dev_run=fast_dev_run,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_files=reorder_files,\n            weights=weights,\n            reader=reader,\n        )\n        with optimize_dns_context(True):\n            return data_processor.run(LambdaDataChunkRecipe(fn, inputs, chunk_size, chunk_bytes, compression))\n    return _execute(\n        f\"data-prep-optimize-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\",\n        num_nodes,\n        machine,\n    )"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be a non-empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(\"The provided inputs should be non-empty.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if isinstance(output_dir, Dir):\n        _output_dir = output_dir\n    else:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n    if error_when_not_empty and _assert_dir_is_empty(_output_dir) is False:\n        raise ValueError(f\"The output directory '{_output_dir.path}' is not empty.\")\n\n    if not isinstance(inputs, StreamingDataLoader):\n        input_dir = _resolve_dir(_get_input_dir(inputs))\n\n        if isinstance(batch_size, int) and batch_size > 1:\n            inputs = [inputs[pos: pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n    else:\n        input_dir = Dir()\n\n    data_processor = DataProcessor(\n        input_dir=input_dir,\n        output_dir=_output_dir,\n        num_workers=num_workers or _get_default_num_workers(),\n        fast_dev_run=fast_dev_run,\n        num_downloaders=num_downloaders,\n        num_uploaders=num_uploaders,\n        reorder_files=reorder_files,\n        reader=reader,\n    )\n\n    with optimize_dns_context(True):\n        data_processor.run(\n            LambdaDataTransformRecipe(\n                fn,\n                inputs,\n            )\n        )"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3_client = None\n    if input_dir.url and input_dir.url.startswith(\"s3://\"):\n        s3_client = S3Client()\n\n    while True:\n        task = queue_in.get()\n        if task is None:\n            # Signal to the output queue that this downloader is finished\n            queue_out.put(None)\n            break\n\n        index, paths = task\n        for path in paths:\n            # Parse the source path or URL\n            parsed_url = parse.urlparse(path)\n            cache_path = os.path.join(cache_dir, os.path.basename(path))\n\n            # Check if the file is already in the cache\n            if not os.path.exists(cache_path):\n                # Download the file if it's not in the cache\n                if s3_client and parsed_url.scheme == \"s3\":\n                    # Download from S3\n                    s3_client.download_file(parsed_url.netloc, parsed_url.path.lstrip('/'), cache_path)\n                else:\n                    # Copy from a local directory\n                    shutil.copyfile(path, cache_path)\n\n        # Signal to the output queue that the files for this index are available\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "\n    s3_client = None\n    if output_dir.url and output_dir.url.startswith(\"s3://\"):\n        s3_client = S3Client()\n\n    while True:\n        data = upload_queue.get()\n        if data is None:\n            break\n\n        # Handle both single file paths and tuples of (temp_dir, file_path)\n        file_path = data if isinstance(data, str) else data[1]\n\n        # Ensure the file path is absolute by prepending the cache directory if necessary\n        if not file_path.startswith(cache_dir):\n            file_path = os.path.join(cache_dir, file_path)\n\n        # Parse the output directory to determine the upload method\n        obj = parse.urlparse(output_dir.url if output_dir.url else output_dir.path)\n\n        if obj.scheme == \"s3\":\n            # Upload to S3\n            assert s3_client is not None, \"S3 client should be available for S3 uploads.\"\n            s3_client.client.upload_file(\n                file_path, obj.netloc, obj.path.lstrip(\"/\") + os.path.basename(file_path)\n            )\n        elif os.path.isdir(obj.path):\n            # Move within the local filesystem\n            target_path = os.path.join(obj.path, os.path.basename(file_path))\n            shutil.move(file_path, target_path)\n        else:\n            raise ValueError(f\"Unsupported output directory scheme: {obj.scheme}\")\n\n        # Send the file path to the remove queue for cleanup\n        remove_queue.put(file_path)\n\n        # If a temporary directory was provided, add it to the remove queue as well\n        if isinstance(data, tuple):\n            remove_queue.put(data[0])"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    if weights is None:\n        weights = [1] * len(user_items)\n\n    if len(weights) != len(user_items):\n        raise ValueError(\"The length of weights must match the length of user_items.\")\n\n    # Calculate total weight and the target weight per worker\n    total_weight = sum(weights)\n    weight_per_worker = total_weight / world_size\n\n    # Distribute items to workers based on weights\n    worker_items = [[] for _ in range(world_size)]\n    worker_weights = [0] * world_size\n    for item, weight in sorted(zip(user_items, weights), key=lambda x: x[1], reverse=True):\n        # Find the worker with the least accumulated weight\n        worker_idx = min(range(world_size), key=lambda i: worker_weights[i])\n        worker_items[worker_idx].append(item)\n        worker_weights[worker_idx] += weight\n\n    # Shuffle items for each worker\n    for items in worker_items:\n        random.shuffle(items)\n\n    # Only return items for workers on the current node\n    start_idx = node_rank * num_workers\n    end_idx = (node_rank + 1) * num_workers\n    node_worker_items = worker_items[start_idx:end_idx]\n\n    # Print distribution details for workers on the current node\n    for i, items in enumerate(node_worker_items, start=start_idx):\n        total_weight = sum(weights[user_items.index(item)] for item in items)\n        if file_size:\n            print(f\"Worker {i} will process {len(items)} files with a total size of {total_weight / (1024 * 1024):.2f} MB\")\n        else:\n            print(f\"Worker {i} will process {len(items)} items with a total weight of {total_weight}\")\n\n    return node_worker_items"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    total_workers = num_workers * num_nodes\n    num_items = len(user_items)\n\n    # Calculate the number of items per worker and the remainder\n    items_per_worker, remainder = divmod(num_items, total_workers)\n\n    # Initialize the list of items for each worker\n    worker_items = [[] for _ in range(num_workers)]\n\n    # Calculate the start and end indices for each worker's items\n    for worker_id in range(num_workers):\n        global_worker_id = node_rank * num_workers + worker_id\n        start_idx = global_worker_id * items_per_worker + min(global_worker_id, remainder)\n        end_idx = start_idx + items_per_worker + (1 if global_worker_id < remainder else 0)\n\n        # Assign the items to the worker\n        worker_items[worker_id] = user_items[start_idx:end_idx]\n\n    # Ensure that the output list has a length equal to the number of workers\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\"The number of worker item lists does not match the number of workers.\")\n\n    return worker_items"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        # Define the cache directories\n        cache_data_dir = _get_cache_data_dir()\n        cache_chunks_dir = _get_cache_dir()\n\n        # Remove the cache directories if they exist\n        for cache_dir in [cache_data_dir, cache_chunks_dir]:\n            if os.path.exists(cache_dir):\n                shutil.rmtree(cache_dir)\n\n        # Recreate the cache directories\n        os.makedirs(cache_data_dir, exist_ok=True)\n        os.makedirs(cache_chunks_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    def get_file_size(item: Any) -> int:\n        try:\n            # Assuming that the item is a path or contains a path to a file\n            if isinstance(item, str):\n                path = os.path.join(base_path, item)\n            else:\n                # If the item is not a string, it's assumed to be a data structure containing file paths\n                # The _get_num_bytes function is used to handle such cases\n                return _get_num_bytes(item, base_path)\n            if os.path.isfile(path):\n                return os.path.getsize(path)\n            else:\n                return 0\n        except Exception as e:\n            logger.error(f\"Error getting file size for item {item}: {e}\")\n            return 0\n\n    # Determine the number of workers based on the CPU count\n    num_workers = min(32, os.cpu_count() or 1)\n\n    # Use ThreadPoolExecutor to parallelize file size retrieval\n    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n        # Map the get_file_size function to each item\n        file_sizes = list(executor.map(get_file_size, items))\n\n    return file_sizes"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if not isinstance(element, str):\n        return False\n\n    element_path = Path(element)\n    if input_dir:\n        input_dir_path = Path(input_dir)\n        try:\n            # Check if element is a subpath of input_dir\n            element_path.relative_to(input_dir_path)\n            return True\n        except ValueError:\n            # element is not a subpath of input_dir\n            pass\n\n    # Check if element is an existing path\n    return element_path.exists()"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"Number of layers must be greater than 0\"\n        assert n_neurons > 0, \"Number of neurons per layer must be greater than 0\"\n\n        if self.tcnn:\n            # Code for creating a network using tinycudann would go here\n            # This is a placeholder as the actual tinycudann implementation is not provided\n            raise NotImplementedError(\"tinycudann network creation is not implemented in this example.\")\n        else:\n            # Create a PyTorch network\n            layers = []\n            for i in range(n_layers - 1):\n                layers.extend(self._get_torch_layer(n_input_dims, n_neurons, activation))\n                n_input_dims = n_neurons  # Update input dims for the next layer\n\n            # Add the output layer\n            layers.extend(self._get_torch_layer(n_input_dims, n_output_dims, output_activation))\n\n            # Create a Sequential model with the layers\n            model = nn.Sequential(*layers)\n            return model"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "\n    if not polygons:\n        return []\n\n    # Calculate the area for each polygon\n    areas = [cv2.contourArea(polygon) for polygon in polygons]\n\n    # Find the largest area\n    max_area = max(areas)\n\n    # Calculate the relative area threshold based on the largest area\n    rel_area_threshold = rel_tr * max_area\n\n    # Determine the effective threshold by taking the maximum of the relative and absolute thresholds\n    effective_threshold = max(rel_area_threshold, abs_tr)\n\n    # Filter out polygons that don't meet the area criteria\n    filtered_polygons = [polygon for polygon, area in zip(polygons, areas) if area >= effective_threshold]\n\n    return filtered_polygons"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Initialize an empty list to hold the medians\n        medians = []\n\n        # Loop over the signal array while applying the kernel offset\n        for i in range(kernel_offset, len(signal) - kernel_offset):\n            # Extract the window of data to compute the median\n            window = signal[i - kernel_offset:i + kernel_offset + 1]\n            # Compute the median of the window and append to the medians list\n            medians.append(np.median(window))\n\n        # Convert the list of medians to a numpy array and return\n        return np.array(medians)"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "\n    min_HD = float('inf')\n    min_shift = 0\n    total_bits = template_probe.code.size\n\n    # Calculate the square root of total bits for normalization\n    sqrt_totalbits, sqrt_totalbits_top, sqrt_totalbits_bot = count_sqrt_totalbits(\n        total_bits, template_probe.half_width, weights\n    )\n\n    # Iterate over the allowed rotation shifts\n    for shift in range(-rotation_shift, rotation_shift + 1):\n        # Shift the probe template\n        shifted_irisbits = np.roll(template_probe.code, shift, axis=1)\n        shifted_maskbits = np.roll(template_probe.mask, shift, axis=1)\n\n        # Count nonmatch bits and mask bits\n        irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n            [shifted_irisbits, template_gallery.code],\n            [shifted_maskbits, template_gallery.mask],\n            template_probe.half_width,\n            weights\n        )\n\n        # Calculate Hamming distance for top and bottom iris\n        HD_top = irisbitcount_top / maskbitcount_top if maskbitcount_top > 0 else 0\n        HD_bot = irisbitcount_bot / maskbitcount_bot if maskbitcount_bot > 0 else 0\n\n        # Calculate the average Hamming distance\n        HD = (HD_top + HD_bot) / 2\n\n        # Normalize the Hamming distance if nm_dist is provided\n        if nm_dist is not None:\n            HD = normalized_HD(irisbitcount_top + irisbitcount_bot, maskbitcount_top + maskbitcount_bot, sqrt_totalbits, nm_dist)\n\n        # Update the minimum Hamming distance and corresponding shift\n        if HD < min_HD:\n            min_HD = HD\n            min_shift = shift\n\n    return min_HD, min_shift"}
{"namespace": "iris.utils.math.area", "completion": "    if array.ndim != 2 or array.shape[1] != 2:\n        raise ValueError(f\"Input array must have shape (_, 2), got {array.shape}\")\n\n    n = len(array)\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i][0] * array[j][1]\n        area -= array[j][0] * array[i][1]\n    area = abs(area) / 2.0\n    return area"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_points = polygon.shape[0]\n        first_bisectors_point = []\n        second_bisectors_point = []\n\n        for _ in range(self.params.max_iterations):\n            if len(first_bisectors_point) >= self.params.num_bisectors:\n                break\n\n            # Randomly select two distinct points\n            idx1, idx2 = np.random.choice(num_points, 2, replace=False)\n            point1, point2 = polygon[idx1], polygon[idx2]\n\n            # Check if the points are sufficiently far apart\n            if np.linalg.norm(point1 - point2) >= min_distance_between_sector_points_in_px:\n                # Calculate midpoint\n                midpoint = (point1 + point2) / 2\n\n                # Calculate the slope of the perpendicular bisector\n                if point2[0] - point1[0] != 0:\n                    slope = -(point2[1] - point1[1]) / (point2[0] - point1[0])\n                else:\n                    slope = np.inf\n\n                # Calculate points on the perpendicular bisector\n                if slope != np.inf:\n                    # For non-vertical lines, choose arbitrary points along the line\n                    dx = 1\n                    dy = slope\n                else:\n                    # For vertical lines, choose points directly above and below the midpoint\n                    dx = 0\n                    dy = 1\n\n                # Normalize the direction vector\n                direction = np.array([dx, dy])\n                direction /= np.linalg.norm(direction)\n\n                # Define points on the bisector line at a fixed distance from the midpoint\n                bisector_point1 = midpoint + direction * 1000  # Arbitrary large number\n                bisector_point2 = midpoint - direction * 1000  # Arbitrary large number\n\n                first_bisectors_point.append(midpoint)\n                second_bisectors_point.append(midpoint + direction * (2 * 1000))  # Extend in the same direction\n\n        if len(first_bisectors_point) < self.params.num_bisectors:\n            raise EyeCentersEstimationError(\"Failed to find sufficient bisector points.\")\n\n        return np.array(first_bisectors_point), np.array(second_bisectors_point)"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Ensure the polygon is a numpy array with the correct shape\n    if not isinstance(polygon, np.ndarray) or polygon.ndim != 2 or polygon.shape[1] != 2:\n        raise ValueError(\"Input polygon must be a numpy array with shape (N, 2) where N is the number of points.\")\n\n    # Calculate the distances between consecutive points\n    distances = np.linalg.norm(np.diff(polygon, axis=0), axis=1)\n\n    # Exclude distances that exceed the maximum allowed distance\n    valid_distances = distances[distances <= max_point_distance]\n\n    # Sum the valid distances to get the total length\n    total_length = np.sum(valid_distances)\n\n    return total_length"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.all(np.isin(v, [0, 1])):\n        raise ValueError(f\"{cls.__name__}: {field.name} must contain only binary values (0 or 1).\")\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.ndim != 2 or v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points with shape (_, 2).\")\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "\n    # Check if v is an iterable (but not a string, as we want to treat strings as single values)\n    if isinstance(v, Iterable) and not isinstance(v, str):\n        if not all(item > 0 for item in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must contain only positive values.\")\n    else:\n        # Check if v is a single value and if it is positive\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values.get('x_min'), values.get('x_max'), values.get('y_min'), values.get('y_max')\n\n    if x_min is None or x_max is None or y_min is None or y_max is None:\n        raise ValueError(f\"{cls.__name__}: Bounding box values must not be None.\")\n\n    if x_min >= x_max:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max. Received x_min={x_min}, x_max={x_max}\")\n\n    if y_min >= y_max:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max. Received y_min={y_min}, y_max={y_max}\")\n\n    return values"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "\n    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check if the array has the specified number of dimensions.\"\"\"\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. Got {v.ndim} dimensions instead.\")\n        return v\n\n    return __validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "\n    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Check if all arrays in field1 have the same shape as their counterparts in field2.\"\"\"\n        field1_arrays = values.get(field1, [])\n        field2_arrays = values.get(field2, [])\n\n        if len(field1_arrays) != len(field2_arrays):\n            raise ValueError(f\"{cls.__name__}: {field1} and {field2} have different lengths.\")\n\n        for i, (arr1, arr2) in enumerate(zip(field1_arrays, field2_arrays)):\n            if arr1.shape != arr2.shape:\n                raise ValueError(f\"{cls.__name__}: Shapes of arrays at index {i} in {field1} and {field2} do not match ({arr1.shape} != {arr2.shape}).\")\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "\n    def validator(cls: type, values: Dict[str, np.ndarray], field: fields.ModelField) -> Dict[str, np.ndarray]:\n        \"\"\"Check if the shapes of field1 and field2 are equal.\"\"\"\n        shape_field1 = values.get(field1, None)\n        shape_field2 = values.get(field2, None)\n\n        if shape_field1 is None or shape_field2 is None:\n            raise ValueError(f\"{cls.__name__}: Both {field1} and {field2} must be provided.\")\n\n        if shape_field1.shape != shape_field2.shape:\n            raise ValueError(\n                f\"{cls.__name__}: The shapes of {field1} and {field2} do not match. \"\n                f\"Got {shape_field1.shape} and {shape_field2.shape} respectively.\"\n            )\n\n        return values\n\n    return validator"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        # Call pre-execution hooks\n        for callback in self._callbacks:\n            callback.on_pre_execute(self, *args, **kwargs)\n\n        # Run the main algorithm\n        result = self.run(*args, **kwargs)\n\n        # Call post-execution hooks\n        for callback in self._callbacks:\n            callback.on_post_execute(self, *args, **kwargs, result=result)\n\n        return result"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            # Deserialize the JSON string into a Python object\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            # If deserialization fails, the output does not match the type definition\n            return False\n\n        # Use the check_type method to validate the deserialized output against the type definition\n        return self.check_type(deserialized_output, type_definition)"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n\n    \"\"\"\n    Encodes an integer into a single character based on a predefined character set. The character set consists of lowercase letters, digits, and an underscore.\n\n    Input-Output Arguments\n    :param n: Integer. The integer to be encoded. It is used as an index to select a character from the character set.\n    :return: String. The encoded character corresponding to the input integer.\n    \"\"\"\n    if not isinstance(n, int) or n < 0 or n >= len(charset):\n        raise ValueError(\"The input must be a non-negative integer less than the length of the character set.\")\n    return charset[n]"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        # Get the function's signature and type hints\n        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        # Extract input and output type hints\n        input_type_hints = {param.name: param.annotation for param in signature.parameters.values()}\n        output_type_hint = type_hints.get('return', None)\n\n        # Helper function to get class definitions for type hints\n        def get_class_definition(type_hint):\n            origin = get_origin(type_hint)\n            if origin is not None:  # This is a generic type\n                return origin\n            elif not isinstance(type_hint, (type(None), type(Literal))):  # Exclude NoneType and Literals\n                return type_hint\n            return None\n\n        # Fetch class definitions for input and output types\n        input_classes = {name: get_class_definition(hint) for name, hint in input_type_hints.items()}\n        output_class = get_class_definition(output_type_hint)\n\n        # Determine the function type based on the output type hint\n        function_type = FunctionType.SYMBOLIC\n        if inspect.isclass(output_class) and issubclass(output_class, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        elif get_origin(output_type_hint) is Union:\n            # Check if any Union subtype is an Embedding to determine function type\n            for subtype in output_type_hint.__args__:\n                if inspect.isclass(subtype) and issubclass(subtype, Embedding):\n                    function_type = FunctionType.EMBEDDABLE\n                    break\n\n        # Create the FunctionDescription instance\n        function_description = FunctionDescription(\n            name=func_object.__name__,\n            docstring=func_object.__doc__,\n            input_type_hints=input_type_hints,\n            output_type_hint=output_type_hint,\n            input_classes=input_classes,\n            output_class=output_class,\n            type=function_type\n        )\n\n        return function_description"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            # Optionally, you can keep track of the indices set to 1\n            self.indices[index] += 1\n            # Uncomment the following line to enable logging of the add operation\n            # print(f\"Add: Seed={seed}, Digest={index}, BitValue={self.bit_array[index]}\")"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        if len(loaded_bit_array) == self.size:\n            self.bit_array = loaded_bit_array\n        else:\n            logging.warning(\"Loaded bit array size does not match expected size. Reinitializing the bit array.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False  # If any bit is not set, the string is definitely not in the filter\n        return True  # If all bits are set, the string might be in the filter"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros < 0 or zeros > n:\n        raise ValueError(\"The number of zeros must be between 0 and n\")\n\n    # Generate n-zeros random weights from a Dirichlet distribution\n    weights = np.random.dirichlet(np.ones(n - zeros))\n\n    # If zeros are required, extend the weights array by adding zeros\n    if zeros > 0:\n        zero_weights = np.zeros(zeros)\n        weights = np.concatenate((weights, zero_weights))\n\n        # Shuffle the weights to ensure the zeros are randomly distributed\n        np.random.shuffle(weights)\n\n    return weights"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        # Attempt to perform Cholesky decomposition on the matrix.\n        # If the matrix is not positive definite, it will raise a LinAlgError.\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        # The decomposition failed, which means the matrix is not suitable\n        # for Cholesky decomposition.\n        return False"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        if 'distilled_model' in json_dict:\n            self.distilled_model = config_factory.create_model_config(json_dict['distilled_model'])\n        if 'current_model_stats' in json_dict:\n            self.current_model_stats = json_dict['current_model_stats']\n        if 'last_training_run' in json_dict:\n            self.last_training_run = json_dict['last_training_run']\n        if 'current_training_run' in json_dict:\n            self.current_training_run = json_dict['current_training_run']\n        if 'nr_of_training_runs' in json_dict:\n            self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = [config_factory.create_model_config(teacher_model) for teacher_model in json_dict['teacher_models']]\n        return self"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # Validate and prepare the parameters for the API call\n        generation_params = {k: v for k, v in kwargs.items() if k in LLM_GENERATION_PARAMETERS}\n        full_prompt = system_message + prompt\n\n        # Attempt to call the API up to 5 times with exponential backoff\n        max_retries = 5\n        backoff_factor = 2\n        for attempt in range(max_retries):\n            try:\n                response = requests.post(\n                    OPENAI_URL,\n                    headers={\"Authorization\": f\"Bearer {self.api_key}\"},\n                    json={\n                        \"model\": model.model_name,\n                        \"prompt\": full_prompt,\n                        **generation_params\n                    }\n                )\n                response.raise_for_status()  # Raise an HTTPError if the HTTP request returned an unsuccessful status code\n\n                # Process the response\n                response_data = response.json()\n                generated_text = response_data['choices'][0]['message']['content']\n\n                # Remove parsing helper tokens if present\n                if model.parsing_helper_tokens:\n                    for token in model.parsing_helper_tokens:\n                        generated_text = generated_text.replace(token, '')\n\n                return generated_text\n\n            except requests.exceptions.HTTPError as http_err:\n                logging.error(f\"HTTP error occurred: {http_err}\")\n                time.sleep(backoff_factor ** attempt)  # Exponential backoff\n            except Exception as err:\n                logging.error(f\"An error occurred: {err}\")\n                time.sleep(backoff_factor ** attempt)  # Exponential backoff\n\n        raise Exception(\"Failed to generate text after multiple retries.\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2 or x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix must be square (same number of rows and columns)\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)  # Check if the matrix is square\n    if not np.allclose(x, x.T, atol=1e-8):  # Check if the matrix is equal to its transpose\n        raise ValueError(\"The matrix is not symmetric\")"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    assert_is_square(cov)\n\n    std_dev = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std_dev, std_dev)\n    # Ensure the diagonal elements are exactly 1\n    np.fill_diagonal(corr, 1.0)\n\n    return corr, std_dev"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)  # Check if the matrix is square\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric to be a distance matrix.\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal elements of a distance matrix must be close to zero.\")"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "\n        # Check if the function has been initialized and retrieve the models\n        if func_hash not in self.initialized_functions:\n            distilled_model, teacher_models = self.function_modeler.get_models(function_description)\n            self.initialized_functions[func_hash] = {\n                \"distilled_model\": distilled_model,\n                \"teacher_models\": teacher_models,\n                \"examples\": [],\n                \"model\": \"\"\n            }\n        else:\n            distilled_model = self.initialized_functions[func_hash][\"distilled_model\"]\n            teacher_models = self.initialized_functions[func_hash][\"teacher_models\"]\n\n        # Determine if the function is suitable for distillation\n        suitable_for_distillation, _ = self.suitable_for_finetuning_token_check(args, kwargs, function_description, distilled_model)\n\n        # If suitable for distillation, use the distilled model\n        if suitable_for_distillation:\n            prompt = self.construct_prompt(str(function_description), args, kwargs, self.initialized_functions[func_hash][\"examples\"], distilled_model)\n            return prompt, distilled_model, True, False\n\n        # If not suitable for distillation, use a teacher model\n        else:\n            # Choose the most appropriate teacher model based on token count\n            model = self.choose_model_from_tokens(teacher_models, approximate_token_count(str(function_description)))\n            if model is None:\n                raise ValueError(\"No suitable teacher model found for the given token count.\")\n\n            # Construct the prompt for the teacher model\n            prompt = self.construct_prompt(str(function_description), args, kwargs, self.initialized_functions[func_hash][\"examples\"], model)\n\n            # Update examples for fine-tuning if necessary\n            if self.initialized_functions[func_hash][\"model\"] != model.model_name:\n                self.initialized_functions[func_hash][\"examples\"] = self.function_modeler.get_symbolic_alignments(func_hash, max=5)\n                self.initialized_functions[func_hash][\"model\"] = model.model_name\n\n            return prompt, model, False, True"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    assert_is_square(cov)\n    if higham:\n        # Higham algorithm\n        n = cov.shape[0]\n        Y = cov\n        for _ in range(higham_max_iteration):\n            R = np.linalg.cholesky(Y)\n            X = np.linalg.solve(R.T, np.linalg.solve(R, cov))\n            Y = (X + X.T) / 2\n            norm_diff = np.linalg.norm(Y - cov, 'fro')\n            if norm_diff < _CLIPPING_VALUE:\n                break\n        return Y\n    else:\n        # Eigenvalue clipping\n        eigvals, eigvecs = np.linalg.eigh(cov)\n        eigvals_clipped = np.clip(eigvals, _CLIPPING_VALUE, None)\n        cov_nearest = eigvecs @ np.diag(eigvals_clipped) @ eigvecs.T\n        return (cov_nearest + cov_nearest.T) / 2"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for arr in x:\n        if arr.size > 1:  # Ensure the array has more than one element\n            mid = arr.size // 2  # Find the midpoint of the array\n            yield [arr[:mid], arr[mid:]]  # Yield the two halves"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    \n    if percent:\n        x *= 100\n    \n    # Determine the number of decimal places\n    if abs(x) < 0.01:\n        fmt_str = \"{:.4f}\"\n    elif abs(x) < 1:\n        fmt_str = \"{:.2f}\"\n    else:\n        fmt_str = \"{:.1f}\"\n    \n    formatted_str = fmt_str.format(x)\n    \n    if percent:\n        formatted_str += \"%\"\n    \n    return formatted_str"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(f\"When {name} is a dictionary, assets_names cannot be None.\")\n        array = np.full((n_assets,), fill_value, dtype=np.float64)\n        for asset_name, value in items.items():\n            try:\n                index = np.where(assets_names == asset_name)[0][0]\n                array[index] = value\n            except IndexError:\n                raise ValueError(f\"Asset name '{asset_name}' not found in assets_names.\")\n    else:\n        array = np.asarray(items, dtype=np.float64)\n        if array.ndim != dim:\n            raise ValueError(f\"The dimension of {name} must be {dim}, got {array.ndim}.\")\n        if array.shape[-1] != n_assets:\n            raise ValueError(f\"The last dimension of {name} must be {n_assets}, got {array.shape[-1]}.\")\n\n    return array"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get('SKFOLIO_DATA', Path.home().joinpath('skfolio_data'))\n    data_home = Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n    return str(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    if os.path.exists(data_home):\n        shutil.rmtree(data_home)\n        print(f\"Cleared data home cache directory at {data_home}\")\n    else:\n        print(f\"Data home cache directory {data_home} does not exist.\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, collections.abc.Sequence) and not isinstance(obj, (str, bytes)):\n        if isinstance(obj, list):\n            return ListSchema.flatten(obj)\n        elif isinstance(obj, tuple):\n            return TupleSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, (Boxes, ROIMasks)):\n        return TensorWrapSchema.flatten(obj)\n    elif isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    else:\n        raise ValueError(f\"Unsupported type for flattening: {type(obj)}\")"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n\n    if groups.ndim != 2:\n        raise ValueError(f\"The {names[0]} should be a 2D array\")\n    if equations.ndim != 1:\n        raise ValueError(f\"The {names[1]} should be a 1D array\")\n\n    n_assets = groups.shape[1]\n    n_equations = equations.shape[0]\n\n    left_matrix = np.zeros((n_equations, n_assets))\n    right_array = np.zeros(n_equations)\n\n    for i, equation in enumerate(equations):\n        try:\n            left, right = _string_to_equation(groups, equation, sum_to_one)\n            left_matrix[i, :] = left\n            right_array[i] = right\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            else:\n                warnings.warn(str(e), UserWarning)\n\n    return left_matrix, right_array"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    fields_dict = {k: v for k, v in fields}\n    cls_name, s = _gen_instance_module(fields_dict)\n\n    with tempfile.TemporaryDirectory() as d:\n        path = os.path.join(d, f\"{cls_name}.py\")\n        with open(path, \"w\") as f:\n            f.write(s)\n\n        new_module = _import(path)\n        newInstances = getattr(new_module, cls_name)\n\n        _add_instances_conversion_methods(newInstances)\n\n        original_instances = detectron2.structures.Instances\n        detectron2.structures.Instances = newInstances\n\n        try:\n            yield newInstances\n        finally:\n            detectron2.structures.Instances = original_instances\n            _clear_jit_cache()"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "\n    class FreezeTrainingModeContext:\n        def __init__(self, model):\n            self.model = model\n            self.original_training_states = {}\n\n        def __enter__(self):\n            for name, module in self.model.named_modules():\n                # Save the original training state\n                self.original_training_states[name] = module.training\n                # Set the training attribute to False to freeze the module\n                module.training = False\n                # Annotate the training attribute as a constant for TorchScript\n                if hasattr(module, \"__constants__\"):\n                    module.__constants__.append(\"training\")\n                else:\n                    module.__constants__ = [\"training\"]\n\n        def __exit__(self, exc_type, exc_val, exc_tb):\n            # Restore the original training states\n            for name, module in self.model.named_modules():\n                module.training = self.original_training_states.get(name, True)\n                # Remove the annotation if it was added\n                if \"training\" in getattr(module, \"__constants__\", []):\n                    module.__constants__.remove(\"training\")\n\n    return FreezeTrainingModeContext(model)"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear_below = srgb / 12.92\n  linear_above = ((srgb + 0.055) / 1.055) ** 2.4\n  return xnp.where(srgb <= 0.04045, linear_below, linear_above)"}
{"namespace": "resample.resample_3d", "completion": "\n  # Adjust locations for half-pixel centering if necessary\n  if half_pixel_center:\n    locations -= 0.5\n\n  # Handle edge behavior\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    # Pad the volume with the constant value\n    pad_width = [(1, 1) for _ in range(data.ndim)]\n    data = jnp.pad(data, pad_width, mode='constant', constant_values=constant_values)\n    # Adjust locations to account for padding\n    locations += 1\n  elif edge_behavior == 'CLAMP':\n    # Clamp locations to the bounds of the data\n    locations = jnp.clip(locations, 0, jnp.array(data.shape[:3]) - 1)\n\n  # Resample the data\n  if method == 'TRILINEAR':\n    # Compute the integer and fractional parts of the locations\n    int_locations = jnp.floor(locations).astype(jnp.int32)\n    frac_locations = locations - int_locations\n\n    # Gather the eight corner points around each location\n    offsets = jnp.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n                         [1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])\n    corner_points = int_locations[..., None, :] + offsets\n    gathered_data = gather_volume(data, corner_points, coordinate_order)\n\n    # Interpolate along the z-axis\n    z_interp0 = (1 - frac_locations[..., 2]) * gathered_data[..., 0, :] + frac_locations[..., 2] * gathered_data[..., 1, :]\n    z_interp1 = (1 - frac_locations[..., 2]) * gathered_data[..., 2, :] + frac_locations[..., 2] * gathered_data[..., 3, :]\n    z_interp2 = (1 - frac_locations[..., 2]) * gathered_data[..., 4, :] + frac_locations[..., 2] * gathered_data[..., 5, :]\n    z_interp3 = (1 - frac_locations[..., 2]) * gathered_data[..., 6, :] + frac_locations[..., 2] * gathered_data[..., 7, :]\n\n    # Interpolate along the y-axis\n    y_interp0 = (1 - frac_locations[..., 1]) * z_interp0 + frac_locations[..., 1] * z_interp1\n    y_interp1 = (1 - frac_locations[..., 1]) * z_interp2 + frac_locations[..., 1] * z_interp3\n\n    # Interpolate along the x-axis\n    final_interp = (1 - frac_locations[..., 0]) * y_interp0 + frac_locations[..., 0] * y_interp1\n\n  elif method == 'NEAREST':\n    # Round locations to the nearest integer\n    nearest_locations = jnp.round(locations).astype(jnp.int32)\n    # Gather the nearest points\n    final_interp = gather_volume(data, nearest_locations, coordinate_order)\n\n  return final_interp"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  dt = jnp.diff(t)  # Compute the difference between consecutive t values\n  avg_w = (w[..., :-1] + w[..., 1:]) / 2  # Compute the average of consecutive w values\n  integral = jnp.sum(dt * avg_w, axis=-1)  # Apply the trapezoid rule\n  return integral"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n\n  # Clip the query points to the range of the spline\n  tq_clipped = jnp.clip(tq, t[0], t[-1])\n\n  # Find the indices of the segments containing the query points\n  indices = jnp.searchsorted(t, tq_clipped, side='right') - 1\n  indices = jnp.clip(indices, 0, len(t) - 2)\n\n  # Compute the slopes of the segments\n  slopes = (v[1:] - v[:-1]) / (t[1:] - t[:-1])\n\n  # Calculate the interpolated values\n  interpolated_values = v[indices] + slopes[indices] * (tq_clipped - t[indices])\n\n  # Set the extrapolated values to 0\n  extrapolated_mask = (tq < t[0]) | (tq > t[-1])\n  interpolated_values = jnp.where(extrapolated_mask, 0, interpolated_values)\n\n  return interpolated_values"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1_normalized = spin_math.normalize(dir1)\n  dir2_normalized = spin_math.normalize(dir2)\n\n  # Compute the cross product of the normalized direction vectors\n  cross_product = jnp.cross(dir1_normalized, dir2_normalized)\n\n  # Check if the norm of the cross product is close to zero, which indicates parallelism\n  return jnp.linalg.norm(cross_product) < eps"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  # Convert spherical to cartesian coordinates\n  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n\n  return jnp.array([x, y, z])"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError(\"Tessellation factor v must be greater than or equal to 1.\")\n\n  # Generate all combinations of integer weights that sum up to v\n  # These combinations correspond to the barycentric coordinates of the tessellated points\n  weights = []\n  for i in range(v + 1):\n    for j in range(v + 1 - i):\n      k = v - i - j\n      weights.append((i, j, k))\n\n  # Convert to numpy array and normalize to get barycentric coordinates\n  weights = np.array(weights, dtype=np.float64)\n  weights /= v\n\n  return weights"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = onp.sqrt(x**2 + y**2 + z**2)\n  theta = onp.arccos(onp.clip(z / (r + eps), -1.0, 1.0))  # Inclination\n  phi = onp.arctan2(y, x)  # Azimuth\n  phi = onp.where(phi < 0, phi + 2 * onp.pi, phi)  # Ensure phi is in [0, 2*pi]\n  return r, theta, phi"}
{"namespace": "geopoly.compute_sq_dist", "completion": "    if mat1 is None:\n        mat1 = mat0\n\n    # Compute the norms of the vectors in each matrix.\n    norm0 = np.sum(mat0 ** 2, axis=0, keepdims=True)\n    norm1 = np.sum(mat1 ** 2, axis=0, keepdims=True)\n\n    # Compute the dot product between all pairs of vectors.\n    dot_product = np.dot(mat0.T, mat1)\n\n    # Use broadcasting to calculate the squared distances.\n    sq_dists = norm0.T + norm1 - 2 * dot_product\n\n    # Ensure numerical stability by setting any negative distances to zero.\n    sq_dists[sq_dists < 0] = 0\n\n    return sq_dists"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x), jnp.inf)\n  )"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, _, x_dot: x_dot * jnp.exp(x),\n      (min_val, np.log(max_val)),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / jnp.maximum(tiny_val, x),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  # Ensure that x is not negative and does not exceed the maximum float32 value\n  x_safe = jnp.clip(x, 0, max_val)\n\n  # Define the safe square root function with a custom gradient\n  @jax.custom_jvp\n  def safe_sqrt_fn(x):\n    return jnp.sqrt(x)\n\n  # Define the custom JVP (Jacobian-vector product) rule for the safe square root function\n  @safe_sqrt_fn.defjvp\n  def safe_sqrt_jvp(primals, tangents):\n    (x,) = primals\n    (x_dot,) = tangents\n    safe_y = safe_sqrt_fn(x)\n    # The gradient is 0.5 / sqrt(x) with respect to x\n    safe_y_dot = x_dot * (0.5 / safe_y)\n    return safe_y, safe_y_dot\n\n  # Apply the safe square root function to the clamped input\n  return safe_sqrt_fn(x_safe)"}
{"namespace": "math.power_ladder_max_output", "completion": "  # Handle special cases for p\n  if p == 1:\n    # When p is 1, the power_ladder behaves linearly, so the output approaches infinity.\n    return jnp.inf\n  elif p == 0:\n    # When p is 0, the power_ladder is equivalent to log(1 + x), which approaches infinity.\n    return jnp.inf\n  elif p == -jnp.inf:\n    # When p is -infinity, the power_ladder approaches 0 as x approaches infinity.\n    return 0\n  elif p == jnp.inf:\n    # When p is infinity, the power_ladder approaches infinity as x approaches infinity.\n    return jnp.inf\n  else:\n    # For other values of p, calculate the limit based on the power_ladder formula.\n    p_safe = clip_finite_nograd(remove_zero(p))\n    # If p is less than 1, the output approaches a finite limit as x approaches infinity.\n    # If p is greater than 1, the output approaches infinity.\n    return jnp.where(p_safe < 1, jnp.abs(p_safe - 1) / p_safe, jnp.inf)"}
{"namespace": "geopoly.generate_basis", "completion": "\n  # Define the vertices and faces of the base polyhedra\n  if base_shape == 'tetrahedron':\n    base_verts = np.array([[1, 1, 1],\n                           [-1, -1, 1],\n                           [-1, 1, -1],\n                           [1, -1, -1]], dtype=np.float64) / np.sqrt(3)\n    base_faces = np.array([[0, 1, 2],\n                           [0, 3, 1],\n                           [0, 2, 3],\n                           [1, 3, 2]], dtype=np.int32)\n  elif base_shape == 'icosahedron':\n    phi = (1 + np.sqrt(5)) / 2\n    base_verts = np.array([[-1, phi, 0],\n                           [1, phi, 0],\n                           [-1, -phi, 0],\n                           [1, -phi, 0],\n                           [0, -1, phi],\n                           [0, 1, phi],\n                           [0, -1, -phi],\n                           [0, 1, -phi],\n                           [phi, 0, -1],\n                           [phi, 0, 1],\n                           [-phi, 0, -1],\n                           [-phi, 0, 1]], dtype=np.float64) / np.sqrt(1 + phi**2)\n    base_faces = np.array([[0, 11, 5],\n                           [0, 5, 1],\n                           [0, 1, 7],\n                           [0, 7, 10],\n                           [0, 10, 11],\n                           [1, 5, 9],\n                           [5, 11, 4],\n                           [11, 10, 2],\n                           [10, 7, 6],\n                           [7, 1, 8],\n                           [3, 9, 4],\n                           [3, 4, 2],\n                           [3, 2, 6],\n                           [3, 6, 8],\n                           [3, 8, 9],\n                           [4, 9, 5],\n                           [2, 4, 11],\n                           [6, 2, 10],\n                           [8, 6, 7],\n                           [9, 8, 1]], dtype=np.int32)\n  elif base_shape == 'octahedron':\n    base_verts = np.array([[1, 0, 0],\n                           [-1, 0, 0],\n                           [0, 1, 0],\n                           [0, -1, 0],\n                           [0, 0, 1],\n                           [0, 0, -1]], dtype=np.float64)\n    base_faces = np.array([[0, 2, 4],\n                           [2, 1, 4],\n                           [1, 3, 4],\n                           [3, 0, 4],\n                           [0, 5, 2],\n                           [2, 5, 1],\n                           [1, 5, 3],\n                           [3, 5, 0]], dtype=np.int32)\n  else:\n    raise ValueError(f'Unknown base_shape {base_shape}')\n\n  # Tessellate the base polyhedron\n  verts = tesselate_geodesic(base_verts, base_faces, angular_tesselation, eps)\n\n  # Optionally remove symmetries\n  if remove_symmetries:\n    # Find the unique vertices by considering the distance between them\n    sq_dist = compute_sq_dist(verts.T)\n    unique_indices = []\n    for i in range(len(verts)):\n      if not any(sq_dist[i, :i] < eps):\n        unique_indices.append(i)\n    verts = verts[unique_indices, :]\n\n  return verts.T"}
{"namespace": "math.safe_log1p", "completion": "  # Ensure that x is greater than -1 to avoid undefined behavior for log1p\n  safe_x = jnp.maximum(x, -1 + tiny_val)\n  # Compute the natural logarithm of 1 plus the safe input value\n  result = jnp.log1p(safe_x)\n  return result"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = premult * x\n\n  # Handle special cases for the power parameter p\n  y = select(\n      [\n          (p == 1, x),\n          (p == 0, safe_log(x)),\n          (p == -jnp.inf, -safe_expm1(-x)),\n          (p == jnp.inf, safe_expm1(x)),\n      ],\n      (jnp.abs(p - 1) / p) * (x ** p - 1)\n  )\n\n  if postmult is not None:\n    y = postmult * y\n\n  return y"}
{"namespace": "math.inv_power_ladder", "completion": "  if postmult is not None:\n    y = y * postmult\n  p_safe = clip_finite_nograd(remove_zero(p))\n  y_abs = jnp.abs(y)\n  y_sign = safe_sign(y)\n  x = select(\n      [\n          (p == 1, y_abs),\n          (p == 0, safe_expm1(y_abs)),\n          (p == -jnp.inf, -safe_log1p(-y_abs)),\n          (p == jnp.inf, safe_log1p(y_abs)),\n      ],\n      clip_finite_nograd(\n          (y_abs / jnp.maximum(tiny_val, jnp.abs(p_safe - 1)) + 1) ** (1 / p_safe) - 1\n      ) * jnp.maximum(tiny_val, jnp.abs(p_safe - 1))\n  ) * y_sign\n  if premult is not None:\n    x = x * premult\n  return x"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Calculate intersection points with the near plane.\n  t = -(near + origins[..., 2]) / directions[..., 2]\n  t = xnp.expand_dims(t, axis=-1)\n  near_plane_intersections = origins + t * directions\n\n  # Apply the inverse intrinsic matrix to the intersection points.\n  near_plane_intersections = xnp.concatenate(\n      [near_plane_intersections[..., :2], xnp.ones_like(t)], axis=-1)\n  near_plane_intersections_ndc = xnp.matmul(\n      near_plane_intersections, xnp.transpose(pixtocam))\n\n  # Calculate the direction vectors in NDC space.\n  directions_ndc = directions / -directions[..., 2:3]\n  directions_ndc = xnp.concatenate(\n      [directions_ndc[..., :2], xnp.ones_like(directions[..., :1])], axis=-1)\n  directions_ndc = xnp.matmul(directions_ndc, xnp.transpose(pixtocam))\n\n  # Normalize the direction vectors.\n  directions_ndc = directions_ndc / directions_ndc[..., 2:3]\n\n  # The origin in NDC space is the intersection point on the near plane.\n  origins_ndc = near_plane_intersections_ndc / near_plane_intersections_ndc[..., 2:3]\n\n  return origins_ndc, directions_ndc"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Ensure that the spline degree is not greater than the number of points minus one.\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Perform the spline interpolation.\n  tck, u = scipy.interpolate.splprep([x], k=spline_degree, s=smoothness, u=t_input)\n  interpolated_values, = scipy.interpolate.splev(t_output, tck)\n\n  return interpolated_values"}
{"namespace": "math.learning_rate_decay", "completion": "  # Calculate the current progress through the steps as a value in [0, 1]\n  progress = jnp.clip(step / max_steps, 0.0, 1.0)\n\n  # Compute the learning rate via log-linear interpolation\n  lr = log_lerp(progress, lr_init, lr_final)\n\n  # If we are still in the delay period, scale the learning rate\n  if step < lr_delay_steps:\n    # Calculate the delay progress as a value in [0, 1]\n    delay_progress = step / lr_delay_steps\n    # Compute the delayed learning rate\n    lr_delayed = log_lerp(delay_progress, lr_init * lr_delay_mult, lr_init)\n    # Use the delayed learning rate instead of the interpolated one\n    lr = jnp.where(step < lr_delay_steps, lr_delayed, lr)\n\n  return lr"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  # Create the intrinsic matrix using the provided focal lengths and principal point coordinates.\n  intrinsic_matrix = xnp.array([\n    [fx, 0,  cx],\n    [0,  fy, cy],\n    [0,  0,  1]\n  ])\n  \n  return intrinsic_matrix"}
{"namespace": "utils.dummy_rays", "completion": "  # Define the parameters for the random rays\n  n = 1024  # Number of rays\n  origin_lo = -1.0\n  origin_hi = 1.0\n  radius_lo = 0.1\n  radius_hi = 0.2\n  near_lo = 0.5\n  near_hi = 1.0\n  far_lo = 1.5\n  far_hi = 2.0\n\n  # Initialize the random number generator\n  rng = random.PRNGKey(0)\n\n  # Generate the random rays using the predefined function\n  return generate_random_rays(\n      rng,\n      n,\n      origin_lo,\n      origin_hi,\n      radius_lo,\n      radius_hi,\n      near_lo,\n      near_hi,\n      far_lo,\n      far_hi,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  points_homogeneous = xnp.concatenate([points, xnp.ones_like(points[..., :1])], axis=-1)\n  points_camera = math.matmul(camtoworlds, points_homogeneous[..., None])[..., :3, 0]\n\n  # Project points onto the image plane.\n  # For perspective cameras, we divide by the z-coordinate to apply perspective division.\n  if camtype == ProjectionType.PERSPECTIVE:\n    points_image_plane = points_camera[..., :2] / points_camera[..., 2:3]\n  else:\n    raise NotImplementedError(f\"Camera type {camtype} not supported.\")\n\n  # Apply intrinsic matrix to get pixel coordinates.\n  # We assume pixtocams is of shape [..., 3, 3].\n  points_pixels_homogeneous = math.matmul(pixtocams, xnp.concatenate([points_image_plane, xnp.ones_like(points_camera[..., :1])], axis=-1))\n  coordinates = points_pixels_homogeneous[..., :2] / points_pixels_homogeneous[..., 2:3]\n\n  # Correct for distortion if parameters are provided.\n  if distortion_params is not None:\n    coordinates = _radial_and_tangential_distort(\n        coordinates[..., 0], coordinates[..., 1],\n        k1=distortion_params.get('k1', 0),\n        k2=distortion_params.get('k2', 0),\n        k3=distortion_params.get('k3', 0),\n        k4=distortion_params.get('k4', 0),\n        p1=distortion_params.get('p1', 0),\n        p2=distortion_params.get('p2', 0),\n    )\n\n  # The depth is the z-coordinate in the camera space.\n  depth = points_camera[..., 2]\n\n  return coordinates, depth"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[:3]\n  v = screw_axis[3:]\n  theta = _safe_sqrt(jnp.sum(w**2))\n\n  R = exp_so3(w, eps)\n  W = skew(w)\n  W_squared = spin_math.matmul(W, W)\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  V_taylor = jnp.eye(3) + W\n\n  # Prevent bad gradients from propagating back when theta is small.\n  theta_safe = jnp.where(theta > eps, theta, 1.0)\n  W_safe = jnp.where(theta > eps, W, jnp.zeros_like(W))\n  W_squared_safe = spin_math.matmul(W_safe, W_safe)\n\n  V = (\n      jnp.eye(3)\n      + (1.0 - jnp.cos(theta_safe)) / theta_safe * W_safe\n      + (theta_safe - jnp.sin(theta_safe)) / theta_safe**2 * W_squared_safe\n  )\n\n  p = spin_math.matmul(V, v)\n\n  return rp_to_se3(R, p)"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = _safe_sqrt(jnp.sum(axis_angle**2))\n  axis = jnp.where(theta > eps, axis_angle / theta, axis_angle)\n\n  # Rodrigues' rotation formula\n  K = skew(axis)\n  R = jnp.eye(3) + jnp.sin(theta) * K + (1 - jnp.cos(theta)) * jnp.dot(K, K)\n\n  # Ensure R is exactly orthonormal when theta is very small\n  R = jnp.where(theta > eps, R, jnp.eye(3))\n\n  return R"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Calculate the means and variances for the conical frustum\n  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  \n  # Scale the radial variance by the square of the base radius\n  r_var *= base_radius ** 2\n  \n  # Lift the Gaussian from 1D to 3D\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  \n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  # Calculate the mean distance along the cylinder's axis\n  t_mean = (t0 + t1) / 2.0\n\n  # Calculate the variance along the cylinder's axis\n  t_var = ((t1 - t0) ** 2) / 12.0\n\n  # The variance in the radial direction is constant and equal to the radius squared\n  r_var = radius ** 2\n\n  # Use the lift_gaussian function to convert the 1D Gaussian along the axis to a 3D Gaussian\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n\n  return mean, cov"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  pix_x = pix_x_int.astype(xnp.float32)\n  pix_y = pix_y_int.astype(xnp.float32)\n\n  # Apply inverse intrinsics to get normalized device coordinates.\n  pix_coords = xnp.stack([pix_x, pix_y, xnp.ones_like(pix_x)], axis=-1)\n  directions = xnp.einsum('...ij,...j->...i', pixtocams, pix_coords)\n\n  # Correct for distortion if parameters are provided.\n  if distortion_params is not None:\n    directions[..., :2] = _radial_and_tangential_undistort(\n        directions[..., 0], directions[..., 1], **distortion_params, xnp=xnp)\n\n  # For non-perspective cameras, modify the directions accordingly.\n  if camtype == ProjectionType.FISHEYE:\n    # Convert to fisheye coordinates.\n    raise NotImplementedError(\"Fisheye camera model is not implemented.\")\n  elif camtype == ProjectionType.PANORAMIC:\n    # Convert to panoramic coordinates.\n    raise NotImplementedError(\"Panoramic camera model is not implemented.\")\n\n  # Apply extrinsics to get world-space ray directions.\n  directions = xnp.einsum('...ij,...j->...i', camtoworlds[..., :3, :3], directions)\n  origins = camtoworlds[..., :3, 3]\n\n  # Normalize directions to get view directions.\n  viewdirs = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Compute differential ray radii for mipmapping (used in mip-NeRF).\n  dx = xnp.linalg.norm(\n      xnp.cross(directions, xnp.einsum('...ij,...j->...i', camtoworlds[..., :3, :3], xnp.array([1, 0, 0]))),\n      axis=-1, keepdims=True)\n  dy = xnp.linalg.norm(\n      xnp.cross(directions, xnp.einsum('...ij,...j->...i', camtoworlds[..., :3, :3], xnp.array([0, 1, 0]))),\n      axis=-1, keepdims=True)\n  radii = xnp.sqrt(dx**2 + dy**2) / xnp.sqrt(pix_x_int.shape[-1]**2 + pix_y_int.shape[-2]**2)\n\n  # Optionally convert to NDC space.\n  if pixtocam_ndc is not None:\n    origins, directions = convert_to_ndc(origins, directions, pixtocam_ndc, xnp=xnp)\n\n  # Compute image plane coordinates.\n  imageplane = pix_coords[..., :2]\n\n  return origins, directions, viewdirs, radii, imageplane"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Calculate the differences between consecutive elements in t.\n  dt = jnp.diff(t)\n  # Divide the weights by the differences to get the PDF.\n  pdf = w / dt\n  return pdf"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Calculate the norm of the direction vectors\n  dirs_norm = jnp.linalg.norm(dirs, axis=-1, keepdims=True)\n  \n  # Calculate the differences in tdist to get segment lengths in the direction of the ray\n  delta_t = tdist[..., 1:] - tdist[..., :-1]\n  \n  # Adjust segment lengths by the norm of the direction vectors\n  delta_t_norm = delta_t / dirs_norm\n  \n  # Calculate the density deltas, which are the product of density and adjusted segment lengths\n  density_delta = density * delta_t_norm\n  \n  # Compute the alpha weights using the helper function\n  alpha_weights = compute_alpha_weights_helper(density_delta, **kwargs)\n  \n  return alpha_weights"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  # Calculate the differences between consecutive elements in 't'.\n  dt = jnp.diff(t)\n  # Multiply the PDF values by the differences to get the weights.\n  w = p * dt\n  # Normalize the weights to ensure they sum to 1.\n  w_sum = jnp.sum(w)\n  normalized_w = w / w_sum\n  return normalized_w"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  if rng is None:\n    # Deterministic sampling using linspace\n    if deterministic_center:\n      # Sample at the center of each bin\n      centers = (t[..., :-1] + t[..., 1:]) / 2\n      spacing = jnp.linspace(0, 1, num_samples + 2, endpoint=True)[..., 1:-1]\n      samples = jnp.take_along_axis(centers, jnp.argsort(cw[..., :-1], axis=-1), axis=-1)\n      samples = jnp.take_along_axis(samples, jnp.clip(jnp.floor(spacing * samples.shape[-1]), 0, samples.shape[-1] - 1).astype(jnp.int32), axis=-1)\n    else:\n      # Sample uniformly across the entire range\n      spacing = jnp.linspace(0, 1, num_samples, endpoint=False) + 0.5 / num_samples\n      samples = math.sorted_interp(spacing, cw, t, utils.device_is_tpu())\n  else:\n    # Random sampling\n    if single_jitter:\n      # Jitter every sample by the same amount\n      u = jax.random.uniform(rng, shape=(1,), minval=eps, maxval=1 - eps)\n      jitter = jax.random.uniform(rng, shape=(1,), minval=-0.5, maxval=0.5) / num_samples\n      u = jnp.clip(u + jitter * jnp.arange(num_samples), eps, 1 - eps)\n    else:\n      # Jitter each sample independently\n      u = jax.random.uniform(rng, shape=(num_samples,), minval=eps, maxval=1 - eps)\n    samples = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n\n  return samples"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function.\n  sampled_points = sample(\n      rng,\n      t,\n      w_logits,\n      num_samples + 1,  # Sample one more point to create intervals\n      single_jitter=single_jitter\n  )\n\n  # Calculate midpoints between adjacent samples to form intervals.\n  intervals = (sampled_points[..., :-1] + sampled_points[..., 1:]) / 2\n\n  # Adjust the first interval to be within the domain.\n  intervals = jax.ops.index_update(\n      intervals, jax.ops.index[..., 0], jnp.maximum(intervals[..., 0], domain[0])\n  )\n\n  # Adjust the last interval to be within the domain.\n  intervals = jax.ops.index_update(\n      intervals, jax.ops.index[..., -1], jnp.minimum(intervals[..., -1], domain[1])\n  )\n\n  return intervals"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Normalize weights to sum to 1 if they don't already\n  w_normalized = w / jnp.sum(w)\n  # Compute the cumulative sum of the normalized weights\n  cw = integrate_weights(w_normalized)\n  # Convert percentiles from [0, 100] to [0, 1]\n  ps = jnp.asarray(ps) / 100.0\n  # Interpolate into the inverse CDF to find the percentiles\n  percentiles = math.sorted_interp(ps, cw, t, utils.device_is_tpu())\n  return percentiles"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  utils.assert_valid_stepfun(t, w)\n\n  # Convert the weights to a PDF.\n  p = weight_to_pdf(t, w)\n\n  # Apply the blur to the PDF. This is a simple box blur.\n  # We need to handle the blur at the boundaries of the PDF.\n  # For simplicity, we assume the PDF is zero outside its defined range.\n  blur_width = 2 * blur_halfwidth\n  tp_extended = jnp.concatenate([\n      t[Ellipsis, :1] - blur_width,  # Extend the left boundary\n      t,\n      t[Ellipsis, -1:] + blur_width  # Extend the right boundary\n  ], axis=-1)\n  p_extended = jnp.concatenate([\n      jnp.zeros_like(p[Ellipsis, :1]),\n      p,\n      jnp.zeros_like(p[Ellipsis, :1])\n  ], axis=-1)\n  p_blurred = jnp.convolve(p_extended, jnp.ones(blur_halfwidth) / blur_halfwidth, mode='same')\n\n  # Trim the extended parts of the blurred PDF.\n  p_blurred = p_blurred[Ellipsis, blur_halfwidth:-blur_halfwidth]\n\n  # Resample the blurred PDF at the new time points.\n  v_blurred = resample(tq, tp_extended, p_blurred, use_avg=True)\n\n  # Convert the resampled PDF back to weights.\n  w_resampled = pdf_to_weight(tq, v_blurred)\n\n  return w_resampled"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.sqrt(safe_x)"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Convert the vectors to homogeneous coordinates\n  vectors_homogeneous = to_homogeneous(vectors)\n  \n  # Apply the transformation matrix to the vectors\n  transformed_homogeneous = matmul(vectors_homogeneous, transform.T)\n  \n  # Convert back from homogeneous coordinates to standard coordinates\n  transformed_vectors = from_homogeneous(transformed_homogeneous)\n  \n  return transformed_vectors"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n  # Query the original step function at the new endpoints.\n  vp_resampled = query(t, tp, vp)\n  # Compute the differences between consecutive queried values.\n  vp_diff = jnp.diff(vp_resampled, axis=-1)\n  # Compute the widths of the new intervals.\n  t_widths = jnp.diff(t, axis=-1)\n  # If averaging is required, divide the differences by the widths.\n  if use_avg:\n    vp_resampled = vp_diff / jnp.maximum(t_widths, jnp.finfo(t.dtype).tiny)\n  else:\n    vp_resampled = vp_diff\n  return vp_resampled"}
{"namespace": "coord.contract", "completion": "  # Calculate the squared magnitude of the input vectors\n  x_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  # Calculate the scaling factor based on the squared magnitude\n  scale = jnp.where(\n      x_mag_sq < 1,\n      0.5 * (1 + x_mag_sq),\n      1 - jnp.reciprocal(jnp.sqrt(x_mag_sq))\n  )\n  # Apply the scaling factor to the input vectors\n  return x * scale"}
{"namespace": "coord.inv_contract", "completion": "  # Compute the magnitude squared of the input vector z\n  z_mag_sq = jnp.sum(z**2, axis=-1, keepdims=True)\n  \n  # Compute the scaling factor for the inverse operation\n  # This is the inverse of the scale factor used in the contract function\n  inv_scale = z_mag_sq / (2 * jnp.sqrt(z_mag_sq) - 1)\n  \n  # Apply the inverse scaling to the input vector z\n  x = inv_scale * z\n  \n  return x"}
{"namespace": "grid_utils.trilerp", "completion": "    if datastructure == 'grid':\n        # Get the integer part of the coordinates (floor)\n        coords_floor = jnp.floor(coordinates).astype(int)\n        # Get the fractional part of the coordinates\n        coords_frac = coordinates - coords_floor\n\n        # Compute the opposite corner of the fractional part\n        coords_ceil = coords_floor + 1\n\n        # Clamp the coordinates to be within the valid range\n        coords_floor = jnp.clip(coords_floor, 0, values.shape[0:3] - 1)\n        coords_ceil = jnp.clip(coords_ceil, 0, values.shape[0:3] - 1)\n\n        # Gather the values at the corner points\n        c000 = values[coords_floor[..., 0], coords_floor[..., 1], coords_floor[..., 2]]\n        c001 = values[coords_floor[..., 0], coords_floor[..., 1], coords_ceil[..., 2]]\n        c010 = values[coords_floor[..., 0], coords_ceil[..., 1], coords_floor[..., 2]]\n        c011 = values[coords_floor[..., 0], coords_ceil[..., 1], coords_ceil[..., 2]]\n        c100 = values[coords_ceil[..., 0], coords_floor[..., 1], coords_floor[..., 2]]\n        c101 = values[coords_ceil[..., 0], coords_floor[..., 1], coords_ceil[..., 2]]\n        c110 = values[coords_ceil[..., 0], coords_ceil[..., 1], coords_floor[..., 2]]\n        c111 = values[coords_ceil[..., 0], coords_ceil[..., 1], coords_ceil[..., 2]]\n\n        # Interpolate along the z-axis\n        c00 = c000 * (1 - coords_frac[..., 2]) + c001 * coords_frac[..., 2]\n        c01 = c010 * (1 - coords_frac[..., 2]) + c011 * coords_frac[..., 2]\n        c10 = c100 * (1 - coords_frac[..., 2]) + c101 * coords_frac[..., 2]\n        c11 = c110 * (1 - coords_frac[..., 2]) + c111 * coords_frac[..., 2]\n\n        # Interpolate along the y-axis\n        c0 = c00 * (1 - coords_frac[..., 1]) + c01 * coords_frac[..., 1]\n        c1 = c10 * (1 - coords_frac[..., 1]) + c11 * coords_frac[..., 1]\n\n        # Interpolate along the x-axis\n        c = c0 * (1 - coords_frac[..., 0]) + c1 * coords_frac[..., 0]\n\n        return c\n    elif datastructure == 'hash':\n        # For a hash data structure, we would typically use a hash function to\n        # map the coordinates to indices in the values array. However, this\n        # requires more context about the hash function and the data structure\n        # than is provided in this snippet. For now, we'll raise a NotImplementedError.\n        raise NotImplementedError(\"Hash data structure interpolation is not implemented in this snippet.\")\n    else:\n        raise ValueError(\"Invalid data structure. Only 'grid' or 'hash' are supported.\")"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n\n  # Compute the Jacobian of the linearized function\n  jacobian_fn = jax.jacfwd(lin_fn)(mean)\n\n  # Transform the covariances using the Jacobian\n  fn_cov = jnp.matmul(jnp.matmul(jacobian_fn, cov), jnp.swapaxes(jacobian_fn, -1, -2))\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Ensure the input is 3-dimensional\n  if x.shape[-1] != 3:\n    raise ValueError(\"Input must be 3-dimensional\")\n\n  # Compute the squared magnitude of the input vectors\n  x_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  \n  # Compute the scaling factor\n  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, x_mag_sq)\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  \n  # Apply the scaling to the input\n  z = scale * x\n  \n  return z"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2 ** jnp.arange(min_deg, max_deg)\n  x_scaled = x[..., None] * scales[None, :]\n  encoded = jnp.concatenate([jnp.sin(x_scaled), jnp.cos(x_scaled)], axis=-1)\n\n  if append_identity:\n    encoded = jnp.concatenate([x] + [encoded], axis=-1)\n\n  return encoded"}
{"namespace": "coord.integrated_pos_enc", "completion": "  scales = 2**jnp.arange(min_deg, max_deg)\n  mean_enc = []\n  var_enc = []\n  for scale in scales:\n    scaled_mean = mean * scale\n    scaled_var = var * scale**2\n    mean_enc.append(jnp.sin(scaled_mean))\n    mean_enc.append(jnp.cos(scaled_mean))\n    var_enc.append(jnp.sin(scaled_var))\n    var_enc.append(jnp.cos(scaled_var))\n  return jnp.concatenate(mean_enc + var_enc, axis=-1)"}
{"namespace": "coord.isotropize", "completion": "  d = cov.shape[-1]  # Dimension of the covariance matrix\n  eps = jnp.finfo(jnp.float32).tiny  # Small epsilon to prevent log(0)\n\n  if mode == 'fast':\n    # Compute the determinant of the covariance matrix\n    det = jnp.linalg.det(cov)\n    # Compute the isotropic scale factor\n    scale = jnp.cbrt(det) if d == 3 else det ** (1 / d)\n    # Create isotropic covariance matrix with the same determinant\n    isotropic_cov = scale * jnp.eye(d)\n  elif mode == 'accurate':\n    # Compute the log-determinant of the covariance matrix for numerical stability\n    sign, logdet = jnp.linalg.slogdet(cov)\n    # Ensure the determinant is positive to take its logarithm\n    if jnp.any(sign <= 0):\n      raise ValueError(\"Covariance matrix has non-positive determinant.\")\n    # Compute the isotropic scale factor using the log-determinant\n    scale = jnp.exp(logdet / d)\n    # Create isotropic covariance matrix with the same determinant\n    isotropic_cov = scale * jnp.eye(d)\n  else:\n    raise ValueError(\"Invalid mode. Choose 'fast' or 'accurate'.\")\n\n  return isotropic_cov"}
{"namespace": "coord.construct_ray_warps", "completion": "\n  # If the inverse function is not provided, attempt to infer it from the forward function.\n  if fn_inv is None:\n    # Define a dictionary mapping functions to their inverses.\n    known_inverses = {\n      contract: inv_contract,\n      # Add other known function-inverse pairs here as needed.\n    }\n    # Attempt to retrieve the inverse function from the dictionary.\n    fn_inv = known_inverses.get(fn)\n    if fn_inv is None:\n      raise ValueError(\"Inverse function not provided and could not be inferred.\")\n\n  # Define the forward mapping function from metric to normalized distances.\n  def t_to_s(t):\n    # Clip the input distances to the valid range [t_near, t_far].\n    t_clipped = jnp.clip(t, t_near, t_far)\n    # Apply the forward function to the clipped distances.\n    s = fn(t_clipped)\n    # Normalize the output to the range [0, 1].\n    s_normalized = (s - fn(t_near)) / (fn(t_far) - fn(t_near))\n    return s_normalized\n\n  # Define the inverse mapping function from normalized to metric distances.\n  def s_to_t(s):\n    # Scale the input normalized distances to the range of the forward function.\n    s_scaled = s * (fn(t_far) - fn(t_near)) + fn(t_near)\n    # Apply the inverse function to the scaled distances.\n    t = fn_inv(s_scaled)\n    return t\n\n  # Return the pair of mapping functions.\n  return t_to_s, s_to_t"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Calculate the L2 norm squared along the last axis and clamp it to grad_eps\n  norm_sq = jnp.maximum(jnp.sum(x**2, axis=-1, keepdims=True), grad_eps)\n  # Calculate the L2 norm for the forward pass, clamping to a slightly larger epsilon to ensure non-zero division\n  norm = jnp.sqrt(jnp.maximum(norm_sq, jnp.finfo(jnp.float32).tiny))\n  # Normalize the input array\n  normalized_x = x / norm\n  return normalized_x"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    # Check if the word is already in lowercase or uppercase, return as is\n    if word.islower() or word.isupper():\n        return word\n    \n    # If the first character is uppercase and the rest of the word is lowercase, return as is (proper noun or start of sentence)\n    if word[0].isupper() and word[1:].islower():\n        return word\n    \n    # If the first two characters are uppercase, consider it an acronym or initialism and return as is\n    if word[0].isupper() and word[1].isupper():\n        return word\n    \n    # If the first character is lowercase and the rest of the word is uppercase, lowercase the entire word\n    if word[0].islower() and word[1:].isupper():\n        return word.lower()\n    \n    # If the word is mixed case with no specific pattern, convert it to lowercase\n    return word.lower()"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the text\n    cleaned_text = re.sub(r'\\s+', '', line_text)\n\n    # Segment the cleaned text into smaller parts or tokens\n    # This can be done using a spell checker or a segmentation algorithm\n    # Here, we use the SpellUtil instance 'su' which is assumed to be available in the context\n    # If 'su' is not available, you may need to create an instance or use a different segmentation method\n    segmented_text = su.segment(cleaned_text)\n\n    return segmented_text"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Generate the integrated directional encoding function with the specified degree\n  ide_fn = generate_ide_fn(deg_view)\n\n  def dir_enc_fn(directions):\n    \"\"\"\n    Evaluates the directional encoding for the given directions.\n\n    :param directions: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n    :return: An array with the resulting directional encoding.\n    \"\"\"\n    # Normalize the input directions\n    directions_normalized = l2_normalize(directions)\n\n    # Use the reciprocal of the concentration parameter kappa, which is set to 1 here\n    kappa_inv = jnp.ones_like(directions_normalized[..., :1])\n\n    # Evaluate the integrated directional encoding function\n    encoding = ide_fn(directions_normalized, kappa_inv)\n\n    return encoding\n\n  return dir_enc_fn"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    cleaned_lines = []\n    line_set = set()\n    page_blocks = []\n    page_idx = 0\n\n    for line in lines:\n        # Clean the line by removing unwanted characters and whitespace\n        line = clean_line(line)\n\n        # Skip empty lines or lines that should be skipped based on XML flag\n        if should_skip(line, xml):\n            continue\n\n        # Fix spaced characters in the line\n        line = fix_spaced_characters(line)\n\n        # Tokenize the line if not in XML mode\n        if not xml:\n            tokens = nlm_tokenize(line)\n            line = ' '.join(tokens)\n\n        # Check if the line is a floating character line and skip it\n        if find_floating_chars(line):\n            continue\n\n        # Check if the line is a table row and skip it\n        if is_table_row(line):\n            continue\n\n        # Append the cleaned line to the result list\n        cleaned_lines.append(line)\n\n    # Process the cleaned lines into blocks\n    for line in cleaned_lines:\n        # Check if the line is a duplicate and skip it\n        line_without_numbers = re.sub(r\"[^a-zA-Z]+\", \"\", line).strip()\n        if line_without_numbers in line_set:\n            continue\n\n        # Add the line to the set of processed lines\n        line_set.add(line_without_numbers)\n\n        # Organize the lines into blocks\n        blocks, line_set = visual_clean_lines([{'text': line, 'style': {}, 'text_list': []}], line_set=line_set)\n        if blocks:\n            # If there are blocks from the previous page that can be joined with the current one, join them\n            if page_blocks and check_block_join(page_blocks[-1][-1], blocks[0]):\n                page_blocks, blocks = join_blocks(page_blocks, blocks)\n\n            # Append the new blocks to the page blocks\n            page_blocks.append(blocks)\n\n    # Flatten the list of page blocks into a single list of blocks\n    result_blocks = [block for page in page_blocks for block in page]\n\n    return result_blocks"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    # Normalize quotation marks\n    org_texts = quotation_pattern.sub('\"', org_texts)\n\n    # Apply rules for abbreviations and special cases\n    for rule, replaced in rules:\n        org_texts = rule.sub(replaced, org_texts)\n\n    # Tokenize sentences\n    sentences = nltk_tokenzier.tokenize(org_texts)\n\n    # Normalize spaces around punctuation\n    sentences = [space_rule.sub(r'\\1', sent) for sent in sentences]\n\n    # Handle sentences within brackets\n    new_sentences = []\n    for sent in sentences:\n        if bracket_rule.search(sent):\n            # Extract content inside brackets as a separate sentence\n            bracket_content = bracket_rule.findall(sent)\n            sent = bracket_rule.sub(' ', sent)  # Remove bracket content from the sentence\n            new_sentences.extend(bracket_content)\n        new_sentences.append(sent)\n\n    # Restore spaces for abbreviations\n    for abb in abbs:\n        new_sentences = [sent.replace(f\"{abb}_\", f\"{abb}.\") for sent in new_sentences]\n\n    # Restore spaces for special abbreviations\n    for abb in nlm_special_abbs:\n        new_sentences = [sent.replace(f\"{abb}_\", f\"{abb}.\") for sent in new_sentences]\n\n    # Remove any leading or trailing whitespace\n    new_sentences = [sent.strip() for sent in new_sentences]\n\n    return new_sentences"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            # If a specific key is provided, return positions for that document only\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                positions = self.posns.doc_encoded_posns(term_id, doc_id=key)\n                return [positions]\n            except TermMissingError:\n                # If the term is not in the dictionary, return an empty list\n                return []\n        else:\n            # If no key is provided, return positions across all documents\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                positions_list = []\n                for doc_id in range(len(self)):\n                    positions = self.posns.doc_encoded_posns(term_id, doc_id=doc_id)\n                    positions_list.append(positions)\n                return positions_list\n            except TermMissingError:\n                # If the term is not in the dictionary, return an empty list for each document\n                return [[] for _ in range(len(self))]"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Remove any whitespace from the spec\n    spec = spec.replace(\" \", \"\")\n\n    # Check for conditional 'min should match' spec\n    if '<' in spec:\n        # Split the condition and the value\n        condition, value = spec.split('<')\n        # Convert the condition to an integer\n        condition = int(condition)\n        # If the number of clauses is less than the condition, return 1\n        if num_clauses < condition:\n            return 1\n        # Otherwise, continue parsing the value part\n        spec = value\n\n    # Check if the spec is a percentage\n    if spec.endswith('%'):\n        # Convert the percentage to a float and calculate the minimum number\n        percentage = float(spec[:-1]) / 100\n        min_should_match = int(np.ceil(percentage * num_clauses))\n    else:\n        # Otherwise, the spec is an absolute number\n        min_should_match = int(spec)\n\n    # Ensure that the minimum number is at least 1 and does not exceed the number of clauses\n    min_should_match = max(1, min_should_match)\n    min_should_match = min(num_clauses, min_should_match)\n\n    return min_should_match"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        # Check if all tokens are unique, which allows for a direct calculation\n        if slop == 1 and len(tokens) == len(set(tokens)):\n            return self.phrase_freq_every_diff(tokens, slop=slop)\n        else:\n            # For non-unique tokens or slop != 1, use a more general approach\n            return self.phrase_freq_scan(tokens, slop=slop)"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Initialize variables to hold the combined results of each batch\n        combined_term_mat = None\n        combined_posns = None\n        combined_term_dict = None\n        combined_avg_doc_length = 0\n        combined_doc_lens = []\n\n        # Process the array in batches\n        for batch_start in range(0, len(array), batch_size):\n            batch_end = min(batch_start + batch_size, len(array))\n            batch = array[batch_start:batch_end]\n\n            # Tokenize and build index for the current batch\n            term_mat, posns, term_dict, avg_doc_length, doc_lens = build_index_from_tokenizer(\n                batch, tokenizer, Terms)\n\n            # Combine the results with those from previous batches\n            if combined_term_mat is None:\n                combined_term_mat = term_mat\n                combined_posns = posns\n                combined_term_dict = term_dict\n            else:\n                combined_term_mat = np.concatenate((combined_term_mat, term_mat))\n                combined_posns = combined_posns.combine(posns)\n                combined_term_dict = combined_term_dict.combine(term_dict)\n\n            combined_avg_doc_length = ((combined_avg_doc_length * len(combined_doc_lens)) +\n                                       (avg_doc_length * len(doc_lens))) / (len(combined_doc_lens) + len(doc_lens))\n            combined_doc_lens.extend(doc_lens)\n\n            # If truncate is True, break the loop to avoid processing further batches\n            if truncate:\n                break\n\n        # Create a new SearchArray instance with the combined results\n        search_array_instance = cls([], tokenizer=tokenizer, avoid_copies=avoid_copies)\n        search_array_instance.term_mat = combined_term_mat\n        search_array_instance.posns = combined_posns\n        search_array_instance.term_dict = combined_term_dict\n        search_array_instance.avg_doc_length = combined_avg_doc_length\n        search_array_instance.doc_lens = np.array(combined_doc_lens)\n\n        return search_array_instance"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "class ProxifierMessageInterceptor(MessageInterceptor):\n    def __init__(self):\n        \"\"\"\n        Initializes an instance of the ProxifierMessageInterceptor class by setting up a server with specific configurations and starting it. It also initializes a dictionary to keep track of connections and a lock for thread safety.\n\n        Input-Output Arguments\n        :param self: ProxifierMessageInterceptor. An instance of the ProxifierMessageInterceptor class. It uses its configuration and logger attributes to set up the server and for logging purposes.\n        :return: No return values.\n        \"\"\"\n        # Initialize the lock for thread safety\n        self.lock = threading.Lock()\n\n        # Initialize the connections dictionary\n        self.connections = {}\n\n        # Extract the server configuration from the interceptor's configuration\n        server_config = {\n            'host': self.config['serverHost'],\n            'port': self.config['serverPort'],\n            'strategy': self.config['strategy'],\n            'strategies': self.config['strategies'],\n        }\n\n        # Create and start the server\n        self.server = Server(server_config)\n        self.server.start()\n\n        # Log the initialization\n        self.logger.info('ProxifierMessageInterceptor initialized with server at %s:%s', self.config['serverHost'], self.config['serverPort'])"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = (arr & s55) + ((arr >> _1) & s55)\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr & s0F) + ((arr >> _4) & s0F)\n    arr = (arr * s01) >> all_but_one_bit\n    return arr"}
{"namespace": "searcharray.solr.edismax", "completion": "\n    # Parse query fields and boosts\n    query_fields = parse_field_boosts(qf)\n    phrase_fields = parse_field_boosts(pf) if pf else {}\n    bigram_fields = parse_field_boosts(pf2) if pf2 else {}\n    trigram_fields = parse_field_boosts(pf3) if pf3 else {}\n\n    # Parse the query terms\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, list(query_fields.keys()))\n\n    # If no minimum match is specified, default to 100% if q_op is \"AND\", else 0%\n    if mm is None:\n        mm = \"100%\" if q_op.upper() == \"AND\" else \"0%\"\n\n    # Perform term-centric or field-centric search\n    if term_centric:\n        qf_scores, qf_explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, qf_explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    # TODO: Implement phrase, bigram, and trigram scoring (pf, pf2, pf3)\n    # For now, we only consider qf (query fields) scores\n    # Phrase, bigram, and trigram scoring would require additional functions to handle those types of matches\n\n    # Combine scores and explanations\n    scores = qf_scores\n    explain = qf_explain\n\n    return scores, explain"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)\n        else:\n            self.logger.warning(f\"Received an unsupported message type: {type(message).__name__}\")"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            # Close all active connections\n            for connection_id, connection in list(self.connections.items()):\n                self.logger.info('Closing connection %s', connection_id)\n                connection.stop()\n                self.connections.pop(connection_id, None)\n\n            # Stop the server if it's running\n            if self.server:\n                self.logger.info('Stopping the server')\n                self.server.stop()\n                self.server = None"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    assert \"bbox\" in instance\n    bbox = instance[\"bbox\"]\n    bbox_mode = instance.get(\"bbox_mode\", BoxMode.XYXY_ABS)\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n\n    # Calculate the center of the bounding box\n    bbox_center_x = (bbox[0] + bbox[2]) / 2.0\n    bbox_center_y = (bbox[1] + bbox[3]) / 2.0\n\n    # Ensure the crop size is within the image size\n    crop_height, crop_width = crop_size\n    crop_height = min(crop_height, image_size[0])\n    crop_width = min(crop_width, image_size[1])\n\n    # Calculate the top-left corner of the crop so that the instance is centered\n    crop_x1 = max(bbox_center_x - crop_width / 2, 0)\n    crop_y1 = max(bbox_center_y - crop_height / 2, 0)\n\n    # Adjust the crop if it goes beyond the image boundaries\n    if crop_x1 + crop_width > image_size[1]:\n        crop_x1 = image_size[1] - crop_width\n    if crop_y1 + crop_height > image_size[0]:\n        crop_y1 = image_size[0] - crop_height\n\n    return T.CropTransform(int(crop_x1), int(crop_y1), int(crop_width), int(crop_height))"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n\n        # Apply orientation to image according to EXIF data\n        image = _apply_exif_orientation(image)\n\n        # Convert image to numpy array and desired format\n        image = convert_PIL_to_numpy(image, format)\n\n    return image"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if \"bbox\" in annotation:\n        # Transform the bounding box\n        bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n        annotation[\"bbox\"] = transforms.apply_box([bbox])[0]\n        annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    if \"segmentation\" in annotation:\n        # Transform the segmentation mask\n        if isinstance(annotation[\"segmentation\"], list):\n            # Polygon format\n            polygons = [np.asarray(polygon, dtype=\"float32\") for polygon in annotation[\"segmentation\"]]\n            transformed_polygons = [transforms.apply_polygons(polygon) for polygon in polygons]\n            annotation[\"segmentation\"] = transformed_polygons\n        elif isinstance(annotation[\"segmentation\"], dict):\n            # RLE format\n            mask = mask_util.decode(annotation[\"segmentation\"])\n            mask = transforms.apply_segmentation(mask)\n            annotation[\"segmentation\"] = mask_util.encode(np.array(mask, order='F', dtype='uint8'))\n\n    if \"keypoints\" in annotation:\n        # Transform the keypoints\n        keypoints = transform_keypoint_annotations(\n            annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints.flatten().tolist()\n\n    return annotation"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        # Convert the input coordinates to homogeneous coordinates by adding a column of ones\n        coords_homogeneous = np.hstack([coords, np.ones((coords.shape[0], 1), dtype=np.float32)])\n        # Apply the rotation matrix to the homogeneous coordinates\n        transformed_coords = np.dot(coords_homogeneous, self.rm_coords.T)\n        return transformed_coords[:, :2]"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    target = Instances(image_size)\n    boxes = [obj[\"bbox\"] for obj in annos if obj.get(\"iscrowd\", 0) == 0]\n    boxes = [BoxMode.convert(box, obj.get(\"bbox_mode\", BoxMode.XYXY_ABS), BoxMode.XYXY_ABS) for box, obj in zip(boxes, annos)]\n    boxes = torch.as_tensor(boxes).reshape(-1, 4)  # guard against no boxes\n    target.gt_boxes = Boxes(boxes)\n    target.gt_boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos if obj.get(\"iscrowd\", 0) == 0]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if any(\"segmentation\" in obj for obj in annos):\n        if mask_format == \"polygon\":\n            masks = [obj[\"segmentation\"] for obj in annos if obj.get(\"iscrowd\", 0) == 0]\n            masks = PolygonMasks(masks)\n        elif mask_format == \"bitmask\":\n            masks = [mask_util.decode(obj[\"segmentation\"]) for obj in annos if obj.get(\"iscrowd\", 0) == 0]\n            masks = BitMasks(torch.stack([torch.tensor(m, dtype=torch.uint8) for m in masks], dim=0))\n        else:\n            raise ValueError(f\"Unknown mask_format: {mask_format}\")\n        target.gt_masks = masks\n\n    if any(\"keypoints\" in obj for obj in annos):\n        keypoints = [obj[\"keypoints\"] for obj in annos if obj.get(\"iscrowd\", 0) == 0]\n        if len(keypoints) > 0:\n            keypoints = Keypoints(keypoints)\n            target.gt_keypoints = keypoints\n\n    return target"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img.size == 0 or self.angle % 360 == 0:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        # Ensure image is contiguous in memory\n        if any(x < 0 for x in img.strides):\n            img = np.ascontiguousarray(img)\n\n        # Rotate the image using warpAffine\n        rotated_img = cv2.warpAffine(\n            img,\n            self.rm_image,\n            (self.bound_w, self.bound_h),\n            flags=interp,\n            borderMode=cv2.BORDER_CONSTANT,\n            borderValue=0\n        )\n        return rotated_img"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n        masks = None\n\n        if predictions.has(\"pred_masks\"):\n            masks = predictions.pred_masks\n        elif predictions.has(\"pred_masks_rle\"):\n            masks = [mask_util.decode(rle) for rle in predictions.pred_masks_rle]\n\n        if self._instance_mode == ColorMode.SEGMENTATION:\n            colors = [self.metadata.thing_colors[c] for c in classes]\n        else:\n            colors = None  # random colors\n\n        if self._instance_mode == ColorMode.IMAGE_BW:\n            self.output.reset_image(self._create_grayscale_image(predictions.pred_masks.any(dim=0)))\n\n        self.overlay_instances(\n            boxes=boxes,\n            labels=labels,\n            masks=masks,\n            keypoints=keypoints,\n            assigned_colors=colors,\n            alpha=0.5,\n        )\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Retrieve the buffer from the canvas\n        canvas = self.canvas.get_renderer().buffer_rgba()\n        # Convert buffer to a numpy array\n        w, h = self.canvas.get_width_height()\n        image = np.frombuffer(canvas, dtype=np.uint8).reshape(h, w, 4)\n        # Convert RGBA to RGB\n        image_rgb = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n        return image_rgb"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        annos = dic.get(\"annotations\", None)\n        if annos:\n            if \"segmentation\" in annos[0]:\n                masks = [obj[\"segmentation\"] for obj in annos]\n            else:\n                masks = None\n            if \"keypoints\" in annos[0]:\n                keypoints = [obj[\"keypoints\"] for obj in annos]\n            else:\n                keypoints = None\n            boxes = [obj[\"bbox\"] for obj in annos if \"bbox\" in obj]\n            classes = [obj[\"category_id\"] for obj in annos if \"category_id\" in obj]\n            labels = _create_text_labels(classes, None, self.metadata.get(\"thing_classes\", None))\n            self.overlay_instances(masks=masks, boxes=boxes, labels=labels, keypoints=keypoints)\n\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"].astype(\"uint8\"))\n\n        if \"panoptic_seg\" in dic:\n            panoptic_seg, segments_info = dic[\"panoptic_seg\"]\n            self.draw_panoptic_seg(panoptic_seg, segments_info)\n\n        return self.output"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the original module to an in-memory buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    \n    # Load the module back from the in-memory buffer\n    buffer.seek(0)  # Move to the start of the buffer\n    reloaded_module = torch.jit.load(buffer)\n    \n    return reloaded_module"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        # Find contours from the binary mask\n        mask = np.ascontiguousarray(binary_mask)  # some versions of cv2 does not support incontiguous arr\n        contours, hierarchy = cv2.findContours(mask.astype(\"uint8\"), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n\n        # Draw polygons for each contour\n        for idx, contour in enumerate(contours):\n            # Skip small contours\n            if cv2.contourArea(contour) < area_threshold:\n                continue\n\n            # If it has no parent, it's an external contour\n            if hierarchy[0, idx, 3] == -1:\n                polygon = contour.reshape(-1, 2)\n                self.draw_polygon(polygon, color, edge_color=edge_color, alpha=alpha)\n            else:\n                # It's a hole, draw it with a different style\n                polygon = contour.reshape(-1, 2)\n                self.draw_polygon(polygon, color=\"w\", edge_color=edge_color, alpha=alpha)\n\n        # Draw text if provided\n        if text is not None:\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            self._draw_text_in_mask(binary_mask, text, lighter_color)\n\n        return self.output"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), f\"{msg}Input must be an instance of Instances\"\n    assert isinstance(other, Instances), f\"{msg}Other must be an instance of Instances\"\n\n    if size_as_tensor:\n        assert torch.allclose(input.image_size, other.image_size, rtol=rtol), \\\n            f\"{msg}Image sizes are not close: {input.image_size} vs {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, \\\n            f\"{msg}Image sizes are not equal: {input.image_size} vs {other.image_size}\"\n\n    for field in input.get_fields():\n        input_field = input.get(field)\n        other_field = other.get(field)\n\n        if isinstance(input_field, torch.Tensor):\n            assert torch.allclose(input_field, other_field, rtol=rtol), \\\n                f\"{msg}Field '{field}' tensors are not close\"\n        elif isinstance(input_field, Boxes):\n            assert torch.allclose(input_field.tensor, other_field.tensor, rtol=rtol), \\\n                f\"{msg}Field '{field}' Boxes are not close\"\n        elif isinstance(input_field, ROIMasks):\n            assert torch.allclose(input_field.tensor, other_field.tensor, rtol=rtol), \\\n                f\"{msg}Field '{field}' ROIMasks are not close\"\n        else:\n            raise ValueError(f\"{msg}Do not know how to compare field '{field}' of type {type(input_field)}\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        # Try to locate the object using the standard library pydoc.\n        obj = pydoc.locate(name)\n        if obj is not None:\n            return obj\n    except Exception as e:\n        # If an exception occurs, log or handle it as needed.\n        # For example, you might want to log the error message.\n        # print(f\"Error locating object with pydoc: {e}\")\n        pass\n\n    # If the standard method fails, attempt a fallback method.\n    # Split the name by dots to separate modules and attributes.\n    parts = name.split('.')\n    module_name, object_path = parts[0], parts[1:]\n\n    # Import the top-level module.\n    module = __import__(module_name)\n\n    # Traverse the object path to locate the final object.\n    try:\n        for part in object_path:\n            module = getattr(module, part)\n        return module\n    except AttributeError:\n        # If an AttributeError occurs, it means the object could not be found.\n        raise ImportError(f\"Object named '{name}' could not be located.\")"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        # The area of a rotated box is the same as the area of the non-rotated box\n        # with the same width and height. Therefore, we can simply multiply the width\n        # and height of each box to get the area.\n        widths = self.tensor[:, 2]\n        heights = self.tensor[:, 3]\n        areas = widths * heights\n        return areas"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n        gt_boxes = cat([p.gt_boxes.tensor for p in proposals], dim=0)\n        proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)\n\n        # Classification loss\n        loss_cls = cross_entropy(scores, gt_classes, reduction=\"mean\")\n\n        # Box regression loss\n        loss_box_reg = self.box_reg_loss(\n            proposal_boxes, gt_boxes, proposal_deltas, gt_classes\n        )\n\n        # Scale and return the losses\n        return {\n            \"loss_cls\": loss_cls * self.loss_weight.get(\"loss_cls\", 1.0),\n            \"loss_box_reg\": loss_box_reg * self.loss_weight.get(\"loss_box_reg\", 1.0),\n        }"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.MODEL.TRACKER_HEAD.NAME\n    tracker_class = TRACKER_HEADS_REGISTRY.get(tracker_name)\n    if tracker_class is None:\n        raise RuntimeError(f\"No registered tracker head with name '{tracker_name}'\")\n    return tracker_class.from_config(cfg)"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        # Apply deltas to boxes\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        # x1\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        # y1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        # x2\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        # y2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        # Process the image using the general annotation instructions\n        processed_output = self.general_ins(image)\n\n        # If no specific annotation type is requested, return the entire processed output\n        if anno_type is None:\n            return processed_output\n\n        # If a specific annotation type is requested, filter the output\n        if isinstance(anno_type, str):\n            anno_type = [anno_type]\n\n        # Ensure that the requested annotation types are valid\n        assert all(tp in self.anno_type_map.keys() for tp in anno_type), \"Invalid annotation type(s) requested.\"\n\n        # Filter the output to include only the requested annotation types\n        filtered_output = {tp: processed_output[tp] for tp in anno_type if tp in processed_output}\n\n        # If only one annotation type was requested, return just that annotation\n        if len(filtered_output) == 1:\n            return next(iter(filtered_output.values()))\n\n        # Return the filtered dictionary of annotations\n        return filtered_output"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = normalize_string(query)\n        keywords = query.split()\n        scores = defaultdict(float)\n\n        for keyword in keywords:\n            keyword_scores = self.bm25(keyword)\n            for url, score in keyword_scores.items():\n                scores[url] += score\n\n        return dict(scores)"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize angles to be within the range [-180, 180)\n        self.normalize_angles()\n\n        # Identify nearly horizontal boxes based on the clip_angle_threshold\n        angles = self.tensor[:, 4].abs()\n        nearly_horizontal = angles < clip_angle_threshold\n\n        # Get the box centers, widths, and heights\n        ctr_x, ctr_y, widths, heights, _ = self.tensor.unbind(dim=1)\n\n        # Compute half-widths and half-heights\n        half_widths = widths / 2.0\n        half_heights = heights / 2.0\n\n        # Compute the coordinates of the top-left and bottom-right corners\n        x1 = ctr_x - half_widths\n        y1 = ctr_y - half_heights\n        x2 = ctr_x + half_widths\n        y2 = ctr_y + half_heights\n\n        # Clip the coordinates for nearly horizontal boxes\n        height, width = box_size\n        x1[nearly_horizontal] = torch.clamp(x1[nearly_horizontal], min=0, max=width)\n        y1[nearly_horizontal] = torch.clamp(y1[nearly_horizontal], min=0, max=height)\n        x2[nearly_horizontal] = torch.clamp(x2[nearly_horizontal], min=0, max=width)\n        y2[nearly_horizontal] = torch.clamp(y2[nearly_horizontal], min=0, max=height)\n\n        # Convert the clipped coordinates back to the center representation\n        ctr_x[nearly_horizontal] = (x1[nearly_horizontal] + x2[nearly_horizontal]) / 2.0\n        ctr_y[nearly_horizontal] = (y1[nearly_horizontal] + y2[nearly_horizontal]) / 2.0\n        widths[nearly_horizontal] = x2[nearly_horizontal] - x1[nearly_horizontal]\n        heights[nearly_horizontal] = y2[nearly_horizontal] - y1[nearly_horizontal]\n\n        # Update the tensor in place\n        self.tensor[:, 0] = ctr_x\n        self.tensor[:, 1] = ctr_y\n        self.tensor[:, 2] = widths\n        self.tensor[:, 3] = heights"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        # Initialize a dictionary to hold the counts for each type\n        stats = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n\n        # Iterate over each item in the data and increment the count for the corresponding type\n        for item in self.data:\n            if 'type' in item and item['type'] in stats:\n                stats[item['type']] += 1\n\n        return stats"}
{"namespace": "common.bleu4_score", "completion": "    # Define a tokenizer function using jieba\n    tokenizer = lambda text: list(jieba.cut(text))\n\n    # Load the BLEU metric from the evaluate library\n    bleu = evaluate.load('bleu')\n\n    # Compute the BLEU score using the tokenizer function\n    results = bleu.compute(predictions=[tokenizer(continuation)], references=[[tokenizer(reference)]], smooth=with_penalty)\n\n    # Extract the BLEU-4 score from the results\n    score = results['bleu']\n\n    return score"}
{"namespace": "common.rougeL_score", "completion": "    # Define a tokenizer function using jieba\n    tokenizer = lambda text: list(jieba.cut(text))\n\n    # Load the ROUGE metric\n    rouge = evaluate.load('rouge')\n\n    # Compute the ROUGE-L score using the tokenizer\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=tokenizer)\n    score = results['rougeL'].mid.fmeasure  # Use the mid.fmeasure value for the ROUGE-L score\n\n    return score"}
{"namespace": "chat.utils.run_cmd", "completion": "    import subprocess\n    print(f\"Executing command: {cmd}\")\n    result = subprocess.run(cmd, shell=True)\n    return result.returncode"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMDET_LOSSES._module_dict.keys():\n        return MMDET_LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        raise NotImplementedError(f\"Loss {cfg['type']} is not recognized.\")"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        raise NotImplementedError(f\"Segmentor type {cfg['type']} is not recognized.\")"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val = val - torch.floor(val / period + offset) * period\n    return val"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(gt_annos) == len(dt_annos)\n    num_examples = len(gt_annos)\n    if not num_examples:\n        print_log('No ground truth or detection results.', logger=logger)\n        return None\n\n    # Convert annotations to required format\n    pred_annos = {cat: [] for cat in label2cat.values()}\n    gt_annos_dict = {cat: [] for cat in label2cat.values()}\n    for i in range(num_examples):\n        gt_anno = gt_annos[i]\n        dt_anno = dt_annos[i]\n        for label in label2cat:\n            cat_name = label2cat[label]\n            # Filter out the ground truth and detection results for each category\n            gt_annos_dict[cat_name].extend(\n                [obj for obj in gt_anno if obj['label'] == label])\n            pred_annos[cat_name].extend(\n                [obj for obj in dt_anno if obj['label'] == label])\n\n    # Evaluate mAP and recall for each category\n    recalls, precisions, aps = eval_map_recall(pred_annos, gt_annos_dict, metric)\n\n    # Calculate mean AP and mean AR across all categories\n    mean_ap = np.array([np.mean([aps[iou_idx][cat] for cat in label2cat.values()])\n                        for iou_idx in range(len(metric))])\n    mean_recall = np.array([np.mean([recalls[iou_idx][cat] for cat in label2cat.values()])\n                            for iou_idx in range(len(metric))])\n\n    # Prepare results for logging\n    results = {'mAP': mean_ap, 'mAR': mean_recall}\n    for iou_idx, iou_thr in enumerate(metric):\n        for cat_id, cat_name in label2cat.items():\n            results[f'{cat_name}_AP@{iou_thr:.2f}'] = aps[iou_idx][cat_name]\n            results[f'{cat_name}_Recall@{iou_thr:.2f}'] = recalls[iou_idx][cat_name]\n\n    # Log results\n    if logger is not None:\n        log_msg = ['\\n']\n        log_msg.append('Evaluation Results:')\n        log_msg.append('mAP@IoU=' + ', '.join([f'{iou:.2f}' for iou in metric]))\n        log_msg.append('mAR@IoU=' + ', '.join([f'{iou:.2f}' for iou in metric]))\n        for iou_idx, iou_thr in enumerate(metric):\n            log_msg.append(f'AP@{iou_thr:.2f}: {mean_ap[iou_idx]:.4f}')\n            log_msg.append(f'Recall@{iou_thr:.2f}: {mean_recall[iou_idx]:.4f}')\n        print_log('\\n'.join(log_msg), logger=logger)\n\n    return results"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "            from mmdet3d.core.bbox.structures import DepthInstance3DBoxes\n            from mmdet3d.core.bbox.structures import CameraInstance3DBoxes\n            from mmdet3d.core.bbox.structures import LiDARInstance3DBoxes\n    box_type = box_type.lower()\n    if box_type == 'lidar':\n        from mmdet3d.core.bbox.structures import LiDARInstance3DBoxes\n        return LiDARInstance3DBoxes, 'LiDAR'\n    elif box_type == 'camera':\n        from mmdet3d.core.bbox.structures import CameraInstance3DBoxes\n        return CameraInstance3DBoxes, 'Camera'\n    elif box_type == 'depth':\n        from mmdet3d.core.bbox.structures import DepthInstance3DBoxes\n        return DepthInstance3DBoxes, 'Depth'\n    else:\n        raise ValueError(f'Invalid box type: {box_type}. Must be one of \"LiDAR\", \"Camera\", or \"Depth\".')"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if messages:\n      for message in messages:\n        if not isinstance(message, dict):\n          raise TypeError('messages must be a list of dict-like objects')\n        if 'role' not in message or message['role'] not in ['system', 'user', 'assistant']:\n          raise RequestError('each message must contain a \"role\" and it must be one of \"system\", \"user\", or \"assistant\"')\n        if 'content' not in message:\n          raise RequestError('each message must contain \"content\"')\n        if 'images' in message:\n          message['images'] = [_encode_image(image) for image in message['images']]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages or [],\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "\n    if not model:\n      raise RequestError('must provide a model')\n\n    if images:\n      images = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'format': format,\n        'images': images,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client._create_blob", "completion": "\n    # Calculate the SHA-256 checksum of the file\n    sha256sum = sha256()\n    with open(path, 'rb') as f:\n      for chunk in iter(lambda: f.read(8192), b''):\n        sha256sum.update(chunk)\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    # Check if the blob already exists on the server\n    try:\n      response = self._client.head(f'/api/blobs/{digest}')\n      response.raise_for_status()\n    except httpx.HTTPStatusError as e:\n      if e.response.status_code != 404:\n        raise\n\n      # If the blob does not exist, upload it\n      with open(path, 'rb') as f:\n        response = self._client.post(f'/api/blobs/{digest}', content=f)\n        response.raise_for_status()\n\n    # Return the digest\n    return digest"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "\n    if not model:\n      raise RequestError('must provide a model')\n\n    if images:\n      encoded_images = [_encode_image(image) for image in images]\n    else:\n      encoded_images = []\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': encoded_images,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    # Call the _request_stream method with the appropriate parameters\n    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "\n    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "\n    sha256sum = sha256()\n    async with aiofiles.open(path, 'rb') as r:\n      while True:\n        chunk = await r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      async with aiofiles.open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=await r.read())\n\n    return digest"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_path = Path(temp_dir)\n            code_path = temp_path / \"code.py\"\n            code_path.write_text(user_code + test_code, encoding=\"utf-8\")\n\n            # Run Pyright type checker as a subprocess\n            result = subprocess.run(\n                [\"pyright\", str(code_path), \"--outputjson\"],\n                capture_output=True,\n                text=True,\n            )\n\n            # Parse Pyright output\n            try:\n                pyright_output = result.stdout\n                output_json = json.loads(pyright_output)\n                errors = output_json.get(\"generalDiagnostics\", [])\n            except json.JSONDecodeError:\n                return TypeCheckResult(\n                    message=\"Failed to parse Pyright output.\",\n                    passed=False,\n                    debug_info={\"pyright_output\": result.stdout},\n                )\n\n            # Check for expected type errors\n            expected_errors = cls._get_expected_errors(user_code)\n            actual_errors = cls._parse_pyright_errors(errors)\n\n            # Compare expected errors with actual Pyright errors\n            passed, message = cls._compare_errors(expected_errors, actual_errors)\n\n            return TypeCheckResult(\n                message=message,\n                passed=passed,\n                debug_info={\n                    \"expected_errors\": expected_errors,\n                    \"actual_errors\": actual_errors,\n                },\n            )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if path:\n      realpath = _as_path(path)\n      if realpath and realpath.exists():\n        modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n      else:\n        raise RequestError('The provided path does not exist.')\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        # Compile the module with aot_module using the provided compiler function\n        with no_fake_tensor():\n            compiled_module = aot_module(fn, fw_compiler=get_compiler_fn(\"Module Forward\"), bw_compiler=get_compiler_fn(\"Module Backward\"))\n        return compiled_module\n    else:\n        # Compile the function with aot_function using the provided compiler function\n        with no_fake_tensor():\n            compiled_fn = aot_function(fn, fw_compiler=get_compiler_fn(\"Function Forward\"), bw_compiler=get_compiler_fn(\"Function Backward\"))\n        return compiled_fn"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Load the summary file and the config file from the trial directory\n    summary_file_path = os.path.join(trial_path, 'summary.csv')\n    config_file_path = os.path.join(trial_path, 'config.yaml')\n\n    if not os.path.exists(summary_file_path) or not os.path.exists(config_file_path):\n        raise FileNotFoundError(\"The trial directory must contain both a summary.csv and a config.yaml file.\")\n\n    summary_df = load_summary_file(summary_file_path)\n    with open(config_file_path, 'r') as f:\n        config_dict = yaml.safe_load(f)\n\n    # Convert the summary dataframe to a config yaml dictionary\n    best_config_dict = summary_df_to_yaml(summary_df, config_dict)\n\n    # Optionally save the extracted configuration to a YAML file\n    if output_path:\n        if not output_path.lower().endswith(('.yaml', '.yml')):\n            raise ValueError(\"The output file extension must be .yaml or .yml\")\n        with open(output_path, 'w') as f:\n            yaml.dump(best_config_dict, f, default_flow_style=False)\n\n    return best_config_dict"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    cache = {}\n    cache_lock = threading.Lock()\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        nonlocal cache\n        key = (hash_arg(args), hash_arg(kwargs))\n        with cache_lock:\n            if key not in cache:\n                compiler = AutoTraceCompiler(ts_compiler=ts_compiler, **kwargs_)\n                cache[key] = compiler.compile(func, args, kwargs)\n        return cache[key](*args, **kwargs)\n\n    return wrapper"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        # Extract the best configuration from the trial folder\n        best_config = extract_best_config(trial_path)\n        \n        # Set the project directory to the parent directory of the trial folder\n        project_dir = os.path.dirname(trial_path)\n        \n        # Instantiate the Runner with the extracted configuration and project directory\n        return cls(config=best_config, project_dir=project_dir)"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n    assert len(results) == len(value), \"results and value must have the same length.\"\n    assert len(results) == len(metadatas), \"results and metadatas must have the same length.\"\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for result, val, metadata in zip(results, value, metadatas):\n        if val <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "\n    # Ensure the node_line_dir exists\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Initialize a DataFrame to store summary information\n    summary_df = pd.DataFrame(columns=['filename', 'module_name', 'module_params', 'execution_time'])\n\n    # Iterate over each module and its parameters\n    for module, params in zip(modules, module_params):\n        # Measure the speed of the module\n        result_df, execution_time = measure_speed(module, params, previous_result)\n\n        # Save the result to a file\n        filename = f\"{module.__name__}_result.parquet\"\n        result_df.to_parquet(os.path.join(node_line_dir, filename))\n\n        # Append the execution time and other details to the summary\n        summary_df = summary_df.append({\n            'filename': filename,\n            'module_name': module.__name__,\n            'module_params': params,\n            'execution_time': execution_time\n        }, ignore_index=True)\n\n    # Save the summary to a file\n    summary_df.to_csv(os.path.join(node_line_dir, \"summary.csv\"), index=False)\n\n    # Apply strategies to filter and select the best result\n    if 'speed_threshold' in strategies:\n        summary_df = filter_by_threshold(summary_df, strategies['speed_threshold'])\n\n    if 'evaluation_metrics' in strategies:\n        # Evaluate each result using the provided metrics\n        for index, row in summary_df.iterrows():\n            result_df = pd.read_parquet(os.path.join(node_line_dir, row['filename']))\n            result_df = evaluate_retrieval_node(result_df, previous_result, strategies['evaluation_metrics'])\n            # Save the evaluated result back to disk\n            result_df.to_parquet(os.path.join(node_line_dir, row['filename']))\n\n    # Select the best result based on the evaluation metrics\n    best_result_info = select_best_average(summary_df, strategies.get('evaluation_metrics', []))\n    best_result_filename = best_result_info['filename']\n\n    # Load the best result\n    best_result_df = pd.read_parquet(os.path.join(node_line_dir, best_result_filename))\n\n    # Combine the previous result columns with the selected retrieval node's result columns\n    combined_result_df = pd.concat([previous_result, best_result_df], axis=1)\n\n    return combined_result_df"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    if not (len(ids) == len(scores) == len(weights)):\n        raise ValueError(\"The length of ids, scores, and weights must be the same.\")\n\n    # Check if the sum of weights is equal to 1\n    if not sum(weights) == 1:\n        raise ValueError(\"The sum of weights must be equal to 1.\")\n\n    # Create a DataFrame from the ids and scores\n    df = pd.DataFrame()\n    for i, (id_list, score_list) in enumerate(zip(ids, scores)):\n        df_temp = pd.DataFrame({'id': id_list, f'score_{i}': score_list})\n        df = pd.merge(df, df_temp, on='id', how='outer')\n\n    # Normalize the scores and apply weights\n    for i in range(len(weights)):\n        df[f'score_{i}'] = df[f'score_{i}'].fillna(0)\n        df[f'norm_score_{i}'] = (df[f'score_{i}'] - df[f'score_{i}'].min()) / (df[f'score_{i}'].max() - df[f'score_{i}'].min())\n        df[f'weighted_score_{i}'] = df[f'norm_score_{i}'] * weights[i]\n\n    # Sum the weighted scores to get the final score\n    df['final_score'] = df[[f'weighted_score_{i}' for i in range(len(weights))]].sum(axis=1)\n\n    # Sort the results by the final score and select the top_k results\n    df = df.sort_values(by='final_score', ascending=False).head(top_k)\n\n    # Return the top_k ids and their corresponding scores\n    return df['id'].tolist(), df['final_score'].tolist()"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "\n    # Calculate the weighted sum of scores for each ID\n    weighted_scores = {}\n    for i in range(len(ids)):\n        for id, score in zip(ids[i], scores[i]):\n            if id not in weighted_scores:\n                weighted_scores[id] = 0\n            weighted_scores[id] += score * weights[i]\n\n    # Normalize the weighted scores\n    max_score = max(weighted_scores.values())\n    min_score = min(weighted_scores.values())\n    for id in weighted_scores:\n        weighted_scores[id] = (weighted_scores[id] - min_score) / (max_score - min_score)\n\n    # Sort the IDs by their weighted sum in descending order\n    sorted_ids = sorted(weighted_scores, key=weighted_scores.get, reverse=True)\n\n    # Return the top K IDs and their corresponding normalized weighted sums\n    top_ids = sorted_ids[:top_k]\n    top_scores = [weighted_scores[id] for id in top_ids]\n\n    return top_ids, top_scores"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "\n    # Ensure the directory exists\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Initialize a list to store the results\n    results = []\n\n    # Iterate over each module and its corresponding parameters\n    for module, params in zip(modules, module_params):\n        # Measure the speed of the module execution\n        with measure_speed() as timer:\n            expanded_queries = module(previous_result=previous_result, **params)\n\n        # Evaluate the module\n        evaluation_result = evaluate_one_query_expansion_node(\n            retrieval_funcs=strategies['retrieval_funcs'],\n            retrieval_params=strategies['retrieval_params'],\n            expanded_queries=expanded_queries,\n            retrieval_gt=strategies['retrieval_gt'],\n            metrics=strategies['metrics'],\n            project_dir=node_line_dir,\n            previous_result=previous_result\n        )\n\n        # Add execution time to the result\n        evaluation_result['execution_time'] = timer()\n\n        # Append the result to the list\n        results.append(evaluation_result)\n\n    # Filter results by speed threshold if specified\n    if 'speed_threshold' in strategies:\n        results = filter_by_threshold(results, 'execution_time', strategies['speed_threshold'])\n\n    # Select the best result based on the evaluation metrics\n    best_result, best_index = select_best_average(results, strategies['metrics'])\n\n    # Save the best result to the specified directory\n    best_result_path = os.path.join(node_line_dir, 'best_result.csv')\n    best_result.to_csv(best_result_path, index=False)\n\n    # Save a summary of all results\n    summary_path = os.path.join(node_line_dir, 'summary.csv')\n    pd.concat(results).to_csv(summary_path, index=False)\n\n    # Return the best result dataframe\n    return best_result"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "\n    # Ensure the node_line_dir exists\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Run each module with its parameters and collect results\n    results = []\n    for module, params in zip(modules, module_params):\n        result = module(previous_result=previous_result, **params)\n        results.append(result)\n\n    # Evaluate each result using the specified strategies\n    generator_callable, generator_params = make_generator_callable_params(strategies.get('generator', {}))\n    prompts = [result['prompt'].tolist() for result in results]\n    generation_gt = strategies.get('generation_ground_truth', [])\n    metrics = strategies.get('metrics', [])\n    evaluation_results = [\n        evaluate_one_prompt_maker_node(\n            generator_funcs=generator_callable,\n            generator_params=generator_params,\n            prompts=prompt_set,\n            generation_gt=generation_gt,\n            metrics=metrics,\n            project_dir=node_line_dir\n        ) for prompt_set in prompts\n    ]\n\n    # Select the best result based on the evaluation\n    best_result_index = select_best_average(evaluation_results, [metric['metric_name'] for metric in metrics])[1]\n    best_result = results[best_result_index]\n\n    # Save the best result and summary\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'), index=False)\n    summary = {\n        'execution_times': [measure_speed(module, params) for module, params in zip(modules, module_params)],\n        'evaluation_metrics': [cast_metrics(evaluation_result) for evaluation_result in evaluation_results]\n    }\n    pd.DataFrame(summary).to_csv(os.path.join(node_line_dir, 'summary.csv'), index=False)\n\n    # Combine the best result with the previous result and return\n    combined_result = pd.concat([previous_result, best_result], axis=1)\n    return combined_result"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    # Initialize an empty set to store unique values\n    unique_values = set()\n\n    # Iterate over each node in the list of nodes\n    for node in nodes:\n        # Extract values from the current node using the provided key\n        values = extract_values(node, key)\n        # Update the set of unique values with the values extracted from the current node\n        unique_values.update(values)\n\n    # Convert the set of unique values to a list and return it\n    return list(unique_values)"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    # Load the summary file into a pandas DataFrame\n    summary_df = pd.read_csv(summary_path)\n\n    # Set default columns to convert if not provided\n    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    # Convert specified columns that contain dictionary-like strings into actual dictionaries\n    for column in dict_columns:\n        if column in summary_df.columns:\n            summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n\n    return summary_df"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.get('module_type')\n        if not module_type:\n            raise ValueError(\"The 'module_type' key is required to create a Module instance from a dictionary.\")\n        \n        # Create a copy of the dictionary to avoid modifying the original\n        module_param = deepcopy(module_dict)\n        # Remove the 'module_type' key-value pair since it's already extracted\n        module_param.pop('module_type', None)\n        \n        # Create a new Module instance with the extracted type and parameters\n        return cls(module_type=module_type, module_param=module_param)"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    metric_names = []\n    metric_params = []\n\n    for metric in metrics:\n        if isinstance(metric, str):\n            metric_names.append(metric)\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            for key, value in metric.items():\n                if key == 'name':\n                    metric_names.append(value)\n                else:\n                    metric_params.append({key: value})\n        else:\n            raise ValueError(f\"Unsupported metric type: {type(metric)}\")\n\n    # Ensure that the length of metric_params matches the length of metric_names\n    if len(metric_params) < len(metric_names):\n        # Extend metric_params with empty dictionaries for metrics without parameters\n        metric_params.extend([{}] * (len(metric_names) - len(metric_params)))\n\n    return metric_names, metric_params"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    # If no embedding model is provided, use the default 'all-mpnet-base-v2' model\n    if embedding_model is None:\n        embedding_model = embedding_models.get_model('all-mpnet-base-v2')\n\n    # Convert the predicted string and ground truth strings to embeddings\n    pred_embedding = embedding_model.encode([pred])[0]\n    gt_embeddings = embedding_model.encode(generation_gt)\n\n    # Calculate cosine similarity between the predicted embedding and each ground truth embedding\n    similarities = [calculate_cosine_similarity(pred_embedding, gt_embedding) for gt_embedding in gt_embeddings]\n\n    # Return the maximum similarity score\n    return max(similarities)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    global gfpgan_face_restorer\n\n    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer is not set up. Returning the original image.\")\n        return np_image\n\n    try:\n        restored_image = gfpgan_face_restorer.restore(np_image)\n        return restored_image\n    except Exception as e:\n        logger.error(f\"An error occurred while restoring faces: {e}\")\n        return np_image"}
{"namespace": "codeformer_model.setup_model", "completion": "    global codeformer\n    try:\n        codeformer = FaceRestorerCodeFormer(model_path=dirname)\n        codeformer.net = codeformer.load_net()\n    except Exception as e:\n        logger.error(f\"Failed to set up the CodeFormer model: {e}\")\n        codeformer = None\n        raise errors.SetupError(f\"CodeFormer setup failed: {e}\") from e"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n\n    try:\n        # Patch the facexlib with the given directory\n        shared.patch_facexlib_root(dirname)\n\n        # Initialize the GFPGAN face restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(model_path=dirname)\n        gfpgan_face_restorer.net = gfpgan_face_restorer.load_net()\n\n    except errors.ModelNotFoundError as e:\n        logger.error(f\"Model not found: {e}\")\n    except errors.ModelLoadingError as e:\n        logger.error(f\"Error loading model: {e}\")\n    except Exception as e:\n        logger.error(f\"An unexpected error occurred during GFPGAN setup: {e}\")"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector into a quaternion with a zero scalar part\n  v_quat = jnp.concatenate([v, jnp.array([0.0])], axis=-1)\n  \n  # Compute the quaternion representing the rotation\n  q_conj = conjugate(q)\n  v_rotated_quat = multiply(multiply(q, v_quat), q_conj)\n  \n  # Extract the vector part of the resulting quaternion\n  v_rotated = im(v_rotated_quat)\n  \n  return v_rotated"}
{"namespace": "quaternion.from_axis_angle", "completion": "  angle = linalg.norm(axis_angle, axis=-1, keepdims=True)\n  axis = axis_angle / jnp.maximum(angle, eps)\n  \n  sin_half_angle = jnp.sin(angle / 2.0)\n  cos_half_angle = jnp.cos(angle / 2.0)\n  \n  # Construct the quaternion as [axis * sin(theta/2), cos(theta/2)]\n  q = jnp.concatenate([axis * sin_half_angle, cos_half_angle], axis=-1)\n  \n  return q"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    topk_words = model.topk(prefix)\n    num_calls = 1\n\n    # check if idx is already in the top-k\n    if idx in topk_words:\n        return topk_words[idx], num_calls\n\n    # initialize high bias for idx\n    logit_bias = {i: 0 for i in topk_words}  # initialize bias for top-k words\n    logit_bias[idx] = high  # set high bias for target idx\n\n    # search for the bias that brings idx into top-k\n    while idx not in topk_words:\n        topk_words = model.topk(prefix, logit_bias)\n        num_calls += 1\n        if idx in topk_words:\n            break  # if idx is in top-k, stop searching\n        logit_bias[idx] *= 2  # otherwise, increase the bias\n\n    # refine the bias to find the smallest bias that keeps idx in top-k\n    low = 0\n    while high - low > 1e-6:\n        mid = (high + low) / 2\n        logit_bias[idx] = mid\n        topk_words = model.topk(prefix, logit_bias)\n        num_calls += 1\n        if idx in topk_words:\n            high = mid  # if idx is still in top-k, decrease the high bound\n        else:\n            low = mid  # if idx is not in top-k, increase the low bound\n\n    # return the log probability of idx being the top result under the found bias\n    return topk_words[idx], num_calls"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the EGL context to match the camera's dimensions\n        eglctx.resize(camera.W, camera.H)\n\n        # Bind the framebuffer for offscreen rendering\n        eglctx.bind()\n\n        # Set up OpenGL context and camera for rendering\n        common_opengl_options()\n        self.upload_gl_uniforms(camera)\n\n        # Clear the color and depth buffers\n        gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)\n\n        # Render the mesh using the camera's view and projection matrices\n        self.render(camera)\n\n        # Unbind the framebuffer after rendering is done\n        eglctx.unbind()"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    return NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        n_embd=bert_config.hidden_size,\n        n_layer=bert_config.num_hidden_layers,\n        n_head=bert_config.num_attention_heads,\n        n_inner=bert_config.intermediate_size,\n        activation_function=bert_config.hidden_act,\n        resid_pdrop=bert_config.hidden_dropout_prob,\n        attn_pdrop=bert_config.attention_probs_dropout_prob,\n        n_positions=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        # Additional attributes for NomicBertConfig that are not present in BertConfig\n        # can be set to default values or derived from existing attributes\n        # Example:\n        # additional_attribute=bert_config.some_existing_attribute_or_default_value\n    )"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        # Use the appropriate shader program based on the render type\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            self.use_gl_program(self.mesh_program)\n\n        # Upload camera and other uniforms to the shader\n        self.upload_gl_uniforms(camera)\n\n        # Bind the vertex array object (VAO) for drawing\n        gl.glBindVertexArray(self.vao)\n\n        # Draw the mesh based on the render type\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.LINES:\n            if hasattr(self, 'faces'):\n                gl.glDrawElements(gl.GL_LINES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.TRIS:\n            if hasattr(self, 'faces'):\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(gl.GL_TRIANGLES, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.QUADS:\n            if hasattr(self, 'faces'):\n                gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(gl.GL_QUADS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            if hasattr(self, 'faces'):\n                gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))\n\n        # Unbind the VAO after drawing\n        gl.glBindVertexArray(0)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "\n        # If ptr is a PyTorch tensor, convert it to a numpy array\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        # Ensure ptr is a numpy array\n        ptr = np.asarray(ptr)\n\n        # Set default width and height if not specified\n        w = w or self.W\n        h = h or self.H\n\n        # Bind the texture\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Specify the 2D texture image\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Validate input shapes\n    assert R.dim() == 3 and R.shape[1:] == (3, 3), \"R must be a batch of 3x3 rotation matrices\"\n    assert tvec.dim() == 3 and tvec.shape[1:] == (3, 1), \"tvec must be a batch of 3x1 translation vectors\"\n    assert camera_matrix.dim() == 3 and camera_matrix.shape[1:] == (3, 3), \"camera_matrix must be a batch of 3x3 intrinsic matrices\"\n    assert image_size.dim() == 2 and image_size.shape[1] == 2, \"image_size must be a batch of image sizes with shape [batch_size, 2]\"\n\n    # Compute camera position from rotation matrix and translation vector\n    camera_position = -R.transpose(-2, -1) @ tvec  # B, 3, 1\n\n    # Compute focal length and principal point\n    fx = camera_matrix[:, 0, 0]\n    fy = camera_matrix[:, 1, 1]\n    cx = camera_matrix[:, 0, 2]\n    cy = camera_matrix[:, 1, 2]\n\n    # Check if fx and fy differ significantly\n    if torch.abs(fx - fy).max() / fy.max() > 0.01:\n        warn_once_about_pulsar_fxfy()\n\n    # Average focal lengths and normalize by image size\n    focal_length = (fx + fy) * 0.5\n    sensor_width = 2.0 * znear * focal_length / image_size[:, 0]  # Assuming square pixels\n\n    # Adjust principal point offsets\n    principal_point_offset_x = (0.5 * image_size[:, 0] - cx) / image_size[:, 0]\n    principal_point_offset_y = (0.5 * image_size[:, 1] - cy) / image_size[:, 1]\n\n    # Convert rotation matrix to 6D representation\n    rotation_6d = matrix_to_rotation_6d(R)\n\n    # Concatenate all parameters into a single tensor\n    camera_params = torch.cat([\n        camera_position.squeeze(-1),  # B, 3\n        rotation_6d,                  # B, 6\n        sensor_width.unsqueeze(-1),   # B, 1\n        focal_length.unsqueeze(-1),   # B, 1\n        principal_point_offset_x.unsqueeze(-1),  # B, 1\n        principal_point_offset_y.unsqueeze(-1),  # B, 1\n    ], dim=-1)\n\n    return camera_params"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        w = w or self.W\n        h = h or self.H\n\n        # Save the current viewport and scissor box\n        old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        old_scissor_box = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n\n        # Set the viewport and scissor box to the specified dimensions\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Use the shader program for drawing the quad\n        gl.glUseProgram(self.quad_program)\n\n        # Bind the texture\n        gl.glActiveTexture(gl.GL_TEXTURE0)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Bind the vertex array object and draw the quad\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n\n        # Restore the original viewport and scissor box\n        gl.glViewport(*old_viewport)\n        gl.glScissor(*old_scissor_box)\n\n        # Unbind the texture and vertex array object\n        gl.glBindTexture(gl.GL_TEXTURE_2D, 0)\n        gl.glBindVertexArray(0)"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "\n    # Extract OpenCV camera parameters from batch\n    H, W, K, R, T, C = get_opencv_camera_params(batch)\n\n    # Convert OpenCV intrinsic matrix K to PyTorch3D NDC format\n    K_ndc = get_pytorch3d_ndc_K(K, H, W)\n\n    # Adjust rotation matrix to PyTorch3D format\n    R_pytorch3d = R.clone()\n    R_pytorch3d[:, 1:3, :] *= -1  # Flip the y and z axes\n\n    # Adjust translation vector to PyTorch3D format\n    T_pytorch3d = T.clone()\n    T_pytorch3d[:, 1:3, :] *= -1  # Flip the y and z axes\n\n    # Compute the camera center in PyTorch3D format\n    C_pytorch3d = -R_pytorch3d.transpose(1, 2) @ T_pytorch3d\n\n    return H, W, K_ndc, R_pytorch3d, T_pytorch3d, C_pytorch3d"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        # Use the Quad's dimensions if width and height are not specified\n        w = w or self.W\n        h = h or self.H\n\n        # Save the current read framebuffer binding\n        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n\n        # Bind the Quad's framebuffer object as the read framebuffer\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n\n        # Copy the block of pixels from the read framebuffer to the draw framebuffer\n        gl.glBlitFramebuffer(x, y, w, h, x, y, w, h, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n\n        # Restore the previously bound read framebuffer\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Ensure that t1 and y1 are sorted in ascending order along the last dimension\n    sorted_indices = torch.argsort(t1, dim=-1)\n    t1 = torch.gather(t1, dim=-1, index=sorted_indices)\n    y1 = torch.gather(y1, dim=-1, index=sorted_indices)\n\n    # Compute the cumulative sum of y1 to get the outer measure at t1\n    y1_outer = torch.cumsum(y1, dim=-1)\n\n    # To get the inner measure, we shift the cumulative sum one step to the right and pad with zeros\n    y1_inner = F.pad(y1_outer[..., :-1], (1, 0), mode='constant', value=0)\n\n    # Interpolate the inner and outer measures at the target times t0\n    inner = torch.interp(t0, t1, y1_inner)\n    outer = torch.interp(t0, t1, y1_outer)\n\n    return inner, outer"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "\n    # Ensure that the target and environment tensors are matched up correctly\n    t, w = matchup_channels(t, w)\n    t_env, w_env = matchup_channels(t_env, w_env)\n\n    # Normalize the target weights\n    w_normalized = w / torch.clamp_min(t[..., 1:] - t[..., :-1], eps)\n\n    # Compute the inner and outer measures\n    w_inner, w_outer = inner_outer(t_env, t, w_normalized)\n\n    # Calculate the loss as the squared difference between the outer measure and the environment weights\n    # The loss is scaled by the environment weights to form a half-quadratic loss\n    loss = ((w_outer - w_env).clamp(min=0) ** 2 / (w_env + eps)).mean()\n\n    return loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "\n    # Ensure that the last dimension of 't' is one more than that of 'w'\n    assert t.shape[-1] == w.shape[-1] + 1, \"The last dimension of 't' should be one more than that of 'w'.\"\n\n    # Compute the inter-interval distortion loss\n    t_lo = t[..., :-1]\n    t_hi = t[..., 1:]\n    inter_interval_loss = interval_distortion(t_lo, t_hi, t_lo, t_hi)\n\n    # Compute the intra-interval distortion loss\n    # For intra-interval, we consider the distortion within each interval defined by consecutive elements in 't'\n    intra_interval_loss = 0.0\n    for i in range(t.shape[-1] - 1):\n        intra_interval_loss += interval_distortion(t[..., i:i+1], t[..., i+1:i+2], t[..., i:i+1], t[..., i+1:i+2])\n\n    # Combine the inter-interval and intra-interval losses\n    distortion_loss = (w * (inter_interval_loss + intra_interval_loss)).mean()\n\n    return distortion_loss"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Match up the channels of t and w\n    t, w = matchup_channels(t, w)\n\n    # Integrate the weights to get the cumulative distribution function (CDF)\n    cw = integrate_weights(w)\n\n    # Convert the list of percentiles to a tensor\n    percentiles = torch.tensor(ps, dtype=t.dtype, device=t.device)\n\n    # Ensure the percentiles are in the range [0, 1]\n    percentiles = torch.clamp(percentiles, 0.0, 1.0)\n\n    # Interpolate the integrated weights to find the weighted percentiles\n    weighted_pcts = interpolate(percentiles, cw, t)\n\n    return weighted_pcts"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Expand a to match the shape of v except for the last dimension\n    a = a.unsqueeze(-1).expand(*v.shape[:-1], a.size(-1))\n    \n    # Create a boolean array where True indicates that an element of v should be inserted after an element of a\n    greater_than_a = v.unsqueeze(-2) > a\n    \n    # Find the indices where the transitions from False to True occur\n    idx_lo = torch.sum(greater_than_a, dim=-1) - 1\n    idx_hi = idx_lo + 1\n    \n    # Clamp the indices to be within the bounds of a\n    idx_lo = torch.clamp(idx_lo, 0, a.size(-1) - 1)\n    idx_hi = torch.clamp(idx_hi, 0, a.size(-1) - 1)\n    \n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Compute the cumulative distribution function (CDF) from the weights\n    cdf = integrate_weights(w)\n\n    # Generate uniform samples in the range [0, 1)\n    if single_jitter:\n        u = torch.linspace(0., 1. - 1e-5, num_samples, device=t.device)\n        u = u.expand(*t.shape[:-1], num_samples)\n        if perturb:\n            u += torch.rand_like(u[..., :1]) / num_samples\n    else:\n        u = torch.rand(*t.shape[:-1], num_samples, device=t.device)\n        if perturb:\n            u += torch.rand_like(u) / num_samples\n    u = u.clamp(min=0., max=1. - 1e-5)\n\n    # Invert the CDF to get the samples\n    samples = invert_cdf(u, t, w)\n\n    return samples"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Ensure that the dilation is non-negative\n    assert dilation >= 0, \"Dilation must be non-negative\"\n\n    # Calculate the dilated time steps\n    t_dilated = t * dilation\n\n    # Clip the dilated time steps to the domain\n    t_dilated = torch.clamp(t_dilated, min=domain[0], max=domain[1])\n\n    # Initialize the dilated weights tensor\n    w_dilated = torch.zeros_like(w)\n\n    # Perform max-pooling on the weights\n    for i in range(w.size(-1)):\n        # Determine the pooling range\n        start = max(i - dilation, 0)\n        end = min(i + dilation + 1, w.size(-1))\n\n        # Apply max-pooling within the pooling range\n        w_dilated[..., i] = torch.max(w[..., int(start):int(end)], dim=-1).values\n\n    return t_dilated, w_dilated"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Ensure that t is sorted\n    assert torch.all(t[1:] >= t[:-1]), \"t must be sorted\"\n\n    # Find the indices of the bins into which the query times fall\n    idx_lo, idx_hi = searchsorted(t, tq)\n\n    # Check if any query times exactly match a step change time\n    exact_matches = tq[..., None] == t[idx_lo]\n\n    # Interpolate the value at the query times based on the step function\n    # For times that fall exactly on a step change, use the outside_value\n    values = torch.where(exact_matches, outside_value, y[idx_lo])\n\n    return values"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "\n    # Compute the Schlick's bias function\n    bias = train_frac / ((1.0 / (train_frac + eps)) + (1.0 / (1.0 - train_frac + eps)) - 1.0)\n    schlick = (bias ** anneal_slope) / ((bias ** anneal_slope) + ((1.0 - bias) ** anneal_slope) + eps)\n\n    # Adjust weights using the Schlick's bias\n    w_adj = w * schlick\n\n    # Handle cases where adjacent intervals have zero distance\n    zero_intervals = torch.isclose(t[..., 1:], t[..., :-1], atol=eps)\n    w_adj = torch.where(zero_intervals, torch.zeros_like(w_adj), w_adj)\n\n    # Prevent NaN values by using a softmax operation on the adjusted weights\n    w_adj = torch.softmax(w_adj, dim=-1)\n\n    return w_adj"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)) and not ignore_list:\n        return [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device, ignore_list) for k, v in batch.items() if k != 'meta'}\n    elif isinstance(batch, torch.Tensor):\n        return batch.to(device, non_blocking=True)\n    else:\n        return batch  # If the batch is neither a list, tuple, dict, nor tensor, return it as is."}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "\n    # Ensure that the faces tensor has the same batch dimension as the vertices tensor\n    if v.dim() - f.dim() == 1:  # v has batch dimension but f does not\n        f = f[None, ...].expand(v.shape[0], *f.shape)  # Add and expand batch dimension for f\n\n    # Gather the vertices corresponding to each face index\n    gathered_vertices = multi_gather(v, f, dim=dim)\n\n    # Compute the normals for each face\n    # Assuming the vertices are in the order (v0, v1, v2) for each face\n    # Compute the vectors for two edges of the triangle\n    edge1 = gathered_vertices[:, :, 1] - gathered_vertices[:, :, 0]\n    edge2 = gathered_vertices[:, :, 2] - gathered_vertices[:, :, 0]\n\n    # Compute the cross product of the two edge vectors to get the normal\n    normals = torch.cross(edge1, edge2, dim=dim+1)\n\n    # Normalize the normals\n    normals = torch.nn.functional.normalize(normals, dim=dim+1)\n\n    return normals"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        return [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        return {k: add_batch(v) for k, v in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        raise TypeError(f\"Unsupported type for add_batch: {type(batch)}\")"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict(\n            H=torch.tensor(self.H, dtype=torch.float32),\n            W=torch.tensor(self.W, dtype=torch.float32),\n            K=torch.tensor(self.K.to_list(), dtype=torch.float32),\n            R=torch.tensor(self.R.to_list(), dtype=torch.float32),\n            T=torch.tensor(self.T.to_list(), dtype=torch.float32),\n            n=torch.tensor(self.n, dtype=torch.float32),\n            f=torch.tensor(self.f, dtype=torch.float32),\n            t=torch.tensor(self.t, dtype=torch.float32),\n            v=torch.tensor(self.v, dtype=torch.float32),\n            bounds=torch.tensor(self.bounds.to_list(), dtype=torch.float32),\n            mass=torch.tensor(self.mass, dtype=torch.float32),\n            moment_of_inertia=torch.tensor(self.moment_of_inertia, dtype=torch.float32),\n            movement_force=torch.tensor(self.movement_force, dtype=torch.float32),\n            movement_torque=torch.tensor(self.movement_torque, dtype=torch.float32),\n            movement_speed=torch.tensor(self.movement_speed, dtype=torch.float32),\n            origin=torch.tensor(self.origin.to_list(), dtype=torch.float32),\n            world_up=torch.tensor(self.world_up.to_list(), dtype=torch.float32),\n        )\n\n        # Add a nested 'meta' dictionary with the same content for compatibility\n        batch.meta = deepcopy(batch)\n\n        return batch"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        # Assuming the agent object has a method `is_prime()` that returns True if the agent is a prime agent\n        # and a method `is_working()` that returns True if the agent is a working agent.\n        if agent.is_working() and not agent.is_prime():\n            serialized_agent = AgentSerializer.to_dict(agent)\n            self.persistence.save_agent(agent.purpose, serialized_agent)"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        closest_agent = None\n        highest_similarity = float('-inf')\n\n        try:\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n                if similarity > highest_similarity:\n                    highest_similarity = similarity\n                    closest_agent = agent\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            return None, float('-inf')\n\n        return closest_agent, highest_similarity"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        # Create a prime agent with specific attributes\n        prime_agent = MicroAgent(\n            prompt=PRIME_PROMPT,\n            purpose=PRIME_NAME,\n            depth=0,  # Assuming prime agent has a depth of 0\n            lifecycle_manager=self,\n            openai_wrapper=self.openai_wrapper,\n            weight=PRIME_AGENT_WEIGHT,\n            is_prime=True,  # Flag indicating this is the prime agent\n            unspecified_flag=True  # Another unspecified flag, set to True as an example\n        )\n        \n        # Add the prime agent to the agent list\n        self.add_agent(prime_agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        agent_dict = self.persistence.load_agent_by_purpose(purpose)\n        if agent_dict:\n            return AgentSerializer.from_dict(agent_dict, agent_lifecycle, openai_wrapper)\n        return None"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        # Find the start and end indices of the agent invocation\n        start_index = response.find('Use Agent[')\n        end_index = response.find(']', start_index)\n\n        # Extract the agent invocation string\n        agent_invocation = response[start_index:end_index + 1]\n\n        # Remove the 'Use Agent[' and ']' to get the agent name and input text\n        agent_info = agent_invocation[len('Use Agent['):-1].strip()\n\n        # Split the agent name and input text by the colon\n        if ':' in agent_info:\n            agent_name, input_text = agent_info.split(':', 1)\n        else:\n            agent_name = agent_info\n            input_text = ''\n\n        # Strip any whitespace from the agent name and input text\n        agent_name = agent_name.strip()\n        input_text = input_text.strip()\n\n        return agent_name, input_text"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        # Fetch all serialized agents from the database\n        serialized_agents = self.persistence.fetch_all_agents()\n        # Initialize an empty list to hold the loaded agents\n        loaded_agents = []\n        # Iterate over each serialized agent and attempt to deserialize it\n        for serialized_agent in serialized_agents:\n            # Deserialize the agent using the AgentSerializer\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            # If the agent is successfully deserialized, add it to the list of loaded agents\n            if agent:\n                loaded_agents.append(agent)\n        # Return the list of loaded agents\n        return loaded_agents"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()  # Clean up the agents before returning the list\n        return self.agent_lifecycle.agents  # Return the current list of agents"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"usage_count\": agent.usage_count,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n\n        # Convert numpy array to list for serialization if purpose_embedding exists\n        if agent.purpose_embedding is not None and isinstance(agent.purpose_embedding, np.ndarray):\n            agent_dict[\"purpose_embedding\"] = agent.purpose_embedding.tolist()\n        else:\n            agent_dict[\"purpose_embedding\"] = None\n\n        return agent_dict"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            # Combine the engineering system prompt with the specific goal and sample input\n            prompt = f\"{PROMPT_ENGINEERING_SYSTEM_PROMPT}\\n\\n{PROMPT_ENGINEERING_TEMPLATE.format(goal=goal, examples=EXAMPLES, sample_input=sample_input)}\"\n            # Get the completion from the OpenAI API wrapper\n            completion = self.openai_wrapper.get_chat_completion(prompt)\n            return completion\n        except Exception as e:\n            # Log the exception and return an empty string\n            logger.exception(f\"Error generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        # Convert the purpose_embedding back to a numpy array if it exists\n        purpose_embedding = np.array(data['purpose_embedding']) if 'purpose_embedding' in data else None\n\n        # Create a new MicroAgent instance with the lifecycle and OpenAI wrapper\n        agent = MicroAgent(agent_lifecycle, openai_wrapper)\n\n        # Set the attributes of the agent from the dictionary\n        agent.dynamic_prompt = data.get('dynamic_prompt')\n        agent.purpose = data.get('purpose')\n        agent.purpose_embedding = purpose_embedding\n        agent.depth = data.get('depth')\n        agent.max_depth = data.get('max_depth')\n        agent.usage_count = data.get('usage_count')\n        agent.id = data.get('id')\n        agent.parent_id = data.get('parent_id')\n        agent.working_agent = data.get('working_agent')\n        agent.is_prime = data.get('is_prime')\n        agent.evolve_count = data.get('evolve_count')\n        agent.number_of_code_executions = data.get('number_of_code_executions')\n        agent.last_input = data.get('last_input')\n\n        return agent"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"\"\"\n                INSERT INTO agents (id, purpose, data) VALUES (?, ?, ?)\n                ON CONFLICT(id) DO UPDATE SET\n                purpose = excluded.purpose,\n                data = excluded.data\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict['data'])))\n            conn.commit()"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            result = cursor.fetchone()\n            if result:\n                return json.loads(result[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # Serialize the function name, arguments, and keyword arguments to a JSON string\n        # Ensure that the keyword arguments are sorted to maintain consistency\n        serialized_data = json.dumps(\n            [func_name, args, sorted(kwargs.items())],\n            sort_keys=True\n        ).encode('utf-8')\n\n        # Create a SHA-256 hash object\n        hash_obj = hashlib.sha256()\n\n        # Update the hash object with the serialized data\n        hash_obj.update(serialized_data)\n\n        # Return the hexadecimal digest of the hash\n        return hash_obj.hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"SELECT result FROM cache WHERE hash = ?\",\n            (arg_hash,)\n        )\n        row = cursor.fetchone()\n        if row is not None:\n            return json.loads(row[0])\n        return None"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as memoization:\n                return memoization.fetch_or_compute(func, func_name, *args, **kwargs)\n        return wrapper\n    return decorator"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        serialized_result = json.dumps(result)\n        cursor = self.connection.cursor()\n        cursor.execute(\"INSERT OR REPLACE INTO cache (hash, result) VALUES (?, ?)\", (arg_hash, serialized_result))\n        self.connection.commit()"}
{"namespace": "run.execute_command_line_process", "completion": "\n    # Update global configuration with command line arguments\n    for arg in vars(args):\n        setattr(CONFIG, arg, getattr(args, arg))\n\n    # If quiet mode is enabled, redirect stdout to the record directory\n    if quiet_mode and args.record_dir:\n        os.makedirs(args.record_dir, exist_ok=True)\n        log_file_path = os.path.join(args.record_dir, 'execution_log.txt')\n        with open(log_file_path, 'w') as log_file, redirect_stdout(log_file):\n            start_command_line(vars(args))\n    else:\n        # Execute the command line process normally\n        start_command_line(vars(args))"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in str(e):\n                fallback_models = CONFIG.get('fallback_models', {})\n                fallback_model = fallback_models.get(model_name)\n                if fallback_model:\n                    model_name = fallback_model\n                    logger.info(f\"Context length exceeded, retrying with {model_name}\")\n                    chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n                    chatcompletion_kwargs.update(kwargs)\n                    response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n                    response = json.loads(str(response))\n                else:\n                    raise e\n            else:\n                raise e\n\n        return response"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        current_time = time()\n        if self._client is None or (self._last_time is not None and (current_time - self._last_time) > self._refetch_interval):\n            self._create_client()\n            self._last_time = current_time\n        return self._client"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"state_dict cannot be called from within a DataLoader worker process.\")\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path if self.input_dir.path else \"\",\n            \"input_dir_url\": self.input_dir.url if self.input_dir.url else \"\",\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n        return state"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        # Validate the input state_dict before applying it\n        required_keys = {\n            \"num_samples_yielded\",\n            \"num_workers\",\n            \"batch_size\",\n            \"current_epoch\",\n            \"input_dir_path\",\n            \"input_dir_url\",\n            \"item_loader\",\n            \"drop_last\",\n            \"seed\",\n            \"world_size\",\n            \"shuffle\",\n        }\n        if not required_keys.issubset(state_dict.keys()):\n            missing_keys = required_keys - state_dict.keys()\n            raise KeyError(f\"The provided state_dict is missing the following keys: {missing_keys}\")\n\n        # Load the state into the StreamingDataset instance\n        self._state_dict = state_dict\n        self.current_epoch = state_dict[\"current_epoch\"]\n        self.input_dir.path = state_dict[\"input_dir_path\"]\n        self.input_dir.url = state_dict[\"input_dir_url\"]\n        self.drop_last = state_dict[\"drop_last\"]\n        self.seed = state_dict[\"seed\"]\n        self.shuffle = state_dict[\"shuffle\"]\n\n        # Load the item_loader state if it exists\n        if self.item_loader and \"item_loader\" in state_dict:\n            self.item_loader.load_state_dict(state_dict[\"item_loader\"])\n\n        # Update the distributed environment world size\n        self.distributed_env.world_size = state_dict[\"world_size\"]\n\n        # Validate the loaded state to ensure it matches the current configuration\n        self._validate_state_dict()"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if not self._state_dict:\n            raise ValueError(\"No state dictionary is present to validate.\")\n\n        state = self._state_dict\n\n        # Validate shuffle\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(f\"State dictionary shuffle ({state['shuffle']}) does not match current shuffle ({self.shuffle}).\")\n\n        # Validate num_workers\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(f\"State dictionary num_workers ({state['num_workers']}) does not match current num_workers ({self.worker_env.world_size}).\")\n\n        # Validate input directory path\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(f\"State dictionary input_dir_path ({state['input_dir_path']}) does not match current input_dir_path ({self.input_dir.path}).\")\n\n        # Validate input directory URL\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(f\"State dictionary input_dir_url ({state['input_dir_url']}) does not match current input_dir_url ({self.input_dir.url}).\")\n\n        # Validate seed\n        if state[\"seed\"] != self.seed:\n            raise ValueError(f\"State dictionary seed ({state['seed']}) does not match current seed ({self.seed}).\")\n\n        # Validate item_loader state\n        if self.item_loader and state[\"item_loader\"] != self.item_loader.state_dict():\n            raise ValueError(\"State dictionary item_loader state does not match current item_loader state.\")\n\n        # Validate drop_last\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(f\"State dictionary drop_last ({state['drop_last']}) does not match current drop_last ({self.drop_last}).\")\n\n        # Validate world_size\n        if state[\"world_size\"] != self.distributed_env.world_size:\n            raise ValueError(f\"State dictionary world_size ({state['world_size']}) does not match current world_size ({self.distributed_env.world_size}).\")"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"Command line interface for XAgent.\")\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", nargs='*', help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action='store_true', help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action='store_true', help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts, specifying how many times a task can be retried upon failure.\")\n    parser.add_argument(\"--config-file\", type=str, default=os.getenv('CONFIG_FILE', 'assets/config.yml'), help=\"Path to the configuration file, specifying where to find the configuration settings.\")\n\n    return parser.parse_args()"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Generate a unique hash for the input directory\n    dir_hash = hashlib.md5(input_dir.encode('utf-8')).hexdigest()\n\n    # Determine the base directory for the cache\n    base_cache_dir = os.getenv('LITDATA_CACHE_DIR', _DEFAULT_CACHE_DIR)\n\n    # Create the full path for the cache directory\n    cache_dir = os.path.join(base_cache_dir, dir_hash)\n\n    # Try to create the cache directory\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n        logger.info(f\"Cache directory created at {cache_dir}\")\n        return cache_dir\n    except OSError as e:\n        logger.error(f\"Failed to create cache directory {cache_dir}: {e}\")\n        return None"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\":\n        return True\n    special_prefixes = [\"http://\", \"https://\", \"s3://\", \"gs://\", \"hdfs://\"]\n    return any(path.startswith(prefix) for prefix in special_prefixes)"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the total number of batches processed\n    total_batches_processed = num_samples_yielded // batch_size\n\n    # Calculate the number of batches each worker has processed\n    batches_per_worker = total_batches_processed // num_workers\n\n    # Calculate the number of samples each worker has processed\n    samples_per_worker = batches_per_worker * batch_size\n\n    # Initialize the dictionary to store the number of samples processed by each worker\n    samples_processed_by_worker = {worker_idx: samples_per_worker for worker_idx in range(num_workers)}\n\n    # Distribute the remaining samples (if any) among the workers\n    remaining_samples = num_samples_yielded % (batch_size * num_workers)\n    for i in range(remaining_samples):\n        worker_idx = i % num_workers\n        samples_processed_by_worker[worker_idx] += 1\n\n    return samples_processed_by_worker"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        parsed_url = parse.urlparse(remote_filepath)\n        if parsed_url.scheme != 's3':\n            raise ValueError(f\"The URL scheme must be 's3', got '{parsed_url.scheme}' in {remote_filepath}\")\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            return\n\n        # Create the cache directory if it does not exist\n        os.makedirs(os.path.dirname(local_filepath), exist_ok=True)\n\n        # Use a file lock to prevent multiple processes from downloading the same file\n        lock_path = local_filepath + \".lock\"\n        with FileLock(lock_path, timeout=10):\n            # Re-check if the file was created while waiting for the lock\n            if os.path.exists(local_filepath):\n                return\n\n            # Download the file using s5cmd if available, otherwise use the S3 client\n            if self._s5cmd_available:\n                bucket_path = f's3://{parsed_url.netloc}{parsed_url.path}'\n                cmd = f's5cmd cp {bucket_path} {local_filepath}'\n                subprocess.check_call(cmd, shell=True)\n            else:\n                self._client.download_file(parsed_url.netloc, parsed_url.path.lstrip('/'), local_filepath)"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "\n    # Initialize dictionaries to hold the chunks and intervals for each worker\n    workers_chunks = {worker_idx: [] for worker_idx in range(num_workers)}\n    workers_intervals = {worker_idx: [] for worker_idx in range(num_workers)}\n\n    # Distribute chunks and intervals to workers\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        # Determine which worker should receive the current chunk and interval\n        worker_idx = i % num_workers\n        # Assign the chunk and interval to the appropriate worker\n        workers_chunks[worker_idx].append(chunk_index)\n        workers_intervals[worker_idx].append(chunk_interval)\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        # Remove the \"local:\" prefix if it exists\n        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\"):]\n\n        # Call the superclass's download_file method\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "\n    # Initialize dictionaries to hold the updated chunk indexes and indexes within those chunks for each worker\n    updated_chunk_indexes = {}\n    updated_indexes = {}\n\n    # Iterate over each worker's intervals\n    for worker_idx, intervals in workers_intervals.items():\n        # Initialize the current index and chunk index for the worker\n        current_index = indexes[worker_idx]\n        chunk_index = 0\n\n        # Iterate over the intervals for the worker\n        for interval in intervals:\n            # Calculate the size of the current interval\n            interval_size = interval[1] - interval[0]\n\n            # If the current index is within the current interval, break the loop\n            if current_index < interval_size:\n                break\n\n            # Otherwise, update the current index and chunk index\n            current_index -= interval_size\n            chunk_index += 1\n\n        # Update the dictionaries with the new chunk index and index within the chunk\n        updated_chunk_indexes[worker_idx] = chunk_index\n        updated_indexes[worker_idx] = current_index\n\n    return updated_chunk_indexes, updated_indexes"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Ensure the input is a PIL Image\n        if not isinstance(item, Image.Image):\n            raise TypeError(f\"Input item must be a PIL Image. Got {type(item)} instead.\")\n\n        # Get the image's mode, size, and raw data\n        mode = item.mode\n        size = item.size\n        raw_data = item.tobytes()\n\n        # Serialize the mode, size, and raw data into bytes\n        mode_bytes = mode.encode(\"utf-8\")\n        mode_size = len(mode_bytes)\n        serialized = (\n            np.uint32(size[0]).tobytes() +  # width\n            np.uint32(size[1]).tobytes() +  # height\n            np.uint32(mode_size).tobytes() +  # mode size\n            mode_bytes +  # mode\n            raw_data  # image data\n        )\n\n        return serialized, None"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if not isinstance(item, Image.Image):\n            raise TypeError(\"The provided item is not an instance of PIL.Image.Image\")\n\n        if isinstance(item, JpegImageFile) and item.filename and os.path.exists(item.filename):\n            with open(item.filename, 'rb') as f:\n                return f.read(), None\n        else:\n            with io.BytesIO() as buffer:\n                item.save(buffer, format='JPEG')\n                return buffer.getvalue(), None"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        if not _PIL_AVAILABLE:\n            raise ModuleNotFoundError(\"PIL is required for deserialization of images. Please install PIL or Pillow.\")\n\n        # Read the width, height, and mode length from the first 12 bytes\n        ints = np.frombuffer(data[:12], dtype=np.uint32)\n        width, height, mode_length = ints\n\n        # Read the mode string\n        mode = data[12:12 + mode_length].decode(\"utf-8\")\n\n        # Read the raw image data\n        raw_data = data[12 + mode_length:]\n\n        # Create an image from the raw data\n        image = Image.frombytes(mode, (width, height), raw_data)\n\n        return image"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Read the dtype index from the first 4 bytes and get the corresponding dtype\n        dtype_index = np.frombuffer(data[:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_index]\n\n        # Read the number of dimensions from the next 4 bytes\n        num_dims = np.frombuffer(data[4:8], np.uint32).item()\n\n        # Initialize the offset to 8 bytes (4 for dtype index + 4 for number of dimensions)\n        offset = 8\n\n        # Read the shape of the tensor\n        shape = []\n        for _ in range(num_dims):\n            dim = np.frombuffer(data[offset:offset+4], np.uint32).item()\n            shape.append(dim)\n            offset += 4  # Move the offset to the next dimension\n\n        # Read the tensor data using the remaining bytes\n        tensor_data = data[offset:]\n\n        # Convert the byte data to a numpy array with the correct dtype\n        np_array = np.frombuffer(tensor_data, dtype=dtype)\n\n        # Reshape the numpy array to the tensor's shape\n        np_array = np_array.reshape(shape)\n\n        # Convert the numpy array to a PyTorch tensor\n        tensor = torch.from_numpy(np_array)\n\n        return tensor"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        # Get the dtype index from the mapping\n        dtype_index = self._dtype_to_indices[item.dtype]\n        # Serialize the dtype index as a 4-byte unsigned integer\n        dtype_bytes = np.uint32(dtype_index).tobytes()\n        # Serialize the shape of the tensor as a 4-byte unsigned integer for each dimension\n        shape_bytes = np.array(item.shape, dtype=np.uint32).tobytes()\n        # Serialize the raw tensor data\n        data_bytes = item.numpy().tobytes(order=\"C\")\n        # Concatenate the dtype, shape, and data bytes\n        serialized_tensor = dtype_bytes + shape_bytes + data_bytes\n        # Return the serialized tensor and None for the metadata\n        return serialized_tensor, None"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                # Attempt to decode the JPEG data using torchvision\n                image_tensor = decode_jpeg(data)\n                return image_tensor\n            except RuntimeError:\n                # If decoding fails, fall back to PIL\n                pass\n\n        # If torchvision is not available or decoding failed, use PIL\n        if _PIL_AVAILABLE:\n            image = Image.open(io.BytesIO(data))\n            if isinstance(image, JpegImageFile):\n                return image\n            else:\n                # Convert to JPEG format if the original format is not JPEG\n                jpeg_image = image.convert('RGB')\n                return jpeg_image\n        else:\n            raise ModuleNotFoundError(\"Neither torchvision nor PIL (Pillow) is available to deserialize JPEG data.\")"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype is not None, \"The data type for deserialization is not set.\"\n        tensor = torch.frombuffer(data, dtype=self._dtype)\n        return tensor"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Read the dtype index from the first 4 bytes and convert it to dtype\n        dtype_index = np.frombuffer(data[:4], np.uint32)[0]\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_index]\n\n        # Read the shape size from the next 4 bytes\n        shape_size = np.frombuffer(data[4:8], np.uint32)[0]\n        shape = []\n\n        # Calculate the starting index for the actual array data\n        data_start_idx = 8 + 4 * shape_size\n\n        # Read each dimension of the shape from the bytes following the shape size\n        for i in range(shape_size):\n            dim_start_idx = 8 + 4 * i\n            dim_end_idx = dim_start_idx + 4\n            shape.append(np.frombuffer(data[dim_start_idx:dim_end_idx], np.uint32)[0])\n\n        # Read the actual array data using the dtype and shape\n        array_data = data[data_start_idx:]\n        return np.frombuffer(array_data, dtype=dtype).reshape(shape)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype is not None, \"Data type for deserialization is not set.\"\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        serialized_data = item.tobytes(order=\"C\")\n        dtype_identifier = f\"no_header_numpy:{dtype_indice}\"\n        return serialized_data, dtype_identifier"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_index = self._dtype_to_indices[item.dtype.type]\n        dtype_bytes = np.uint32(dtype_index).tobytes()\n        shape_bytes = np.array(item.shape, dtype=np.uint32).tobytes()\n        data_bytes = item.tobytes(order=\"C\")\n        return dtype_bytes + shape_bytes + data_bytes, None"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"current_epoch\": self.current_epoch,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n\n        if isinstance(self.dataset, StreamingDataset):\n            state[\"num_samples_yielded\"] = self._num_samples_yielded_streaming\n            state[\"dataset\"] = self.dataset.state_dict()\n        else:\n            state[\"num_samples_yielded\"] = self._num_samples_yielded_combined\n            state.update(self.dataset.state_dict())\n\n        return state"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "                from torchvision.io import read_video\n        if not _TORCH_VISION_AVAILABLE or not _AV_AVAILABLE:\n            raise ImportError(\"Deserializing video requires torchvision and av to be installed.\")\n\n        from torchvision.io import read_video\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as tmpfile:\n            tmpfile.write(data)\n            tmpfile_name = tmpfile.name\n\n        try:\n            video, audio, info = read_video(tmpfile_name)\n        finally:\n            os.unlink(tmpfile_name)\n\n        return video, audio, info"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self.filled:\n            return []\n\n        written_chunks = []\n\n        # Write any remaining serialized items to a chunk\n        if self._serialized_items:\n            chunk_path = self.write_chunk(on_done=True)\n            written_chunks.append(chunk_path)\n\n        # Write the chunks index to a file\n        index_file_path = self.write_chunks_index()\n        if index_file_path:\n            written_chunks.append(index_file_path)\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        return written_chunks"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if not isinstance(self.dataset, (StreamingDataset, CombinedStreamingDataset)):\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {type(self.dataset)}.\"\n            )\n\n        self.current_epoch = obj['current_epoch']\n        self._latest_worker_idx = obj['latest_worker_idx']\n        self.restore = True\n\n        if isinstance(self.dataset, StreamingDataset):\n            self._num_samples_yielded_streaming = obj['num_samples_yielded']\n            self.dataset.load_state_dict(obj['dataset'])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self._num_samples_yielded_combined = obj['num_samples_yielded']\n            self.dataset.load_state_dict(obj['dataset'])\n\n        # Adjust the worker index iterator to the latest worker index\n        self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n        self._worker_idx_iter = iter(self._worker_idx)\n        for _ in range(self._latest_worker_idx):\n            next(self._worker_idx_iter)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is not None:\n            return self._iterator.state_dict(num_workers=num_workers, batch_size=batch_size)\n\n        return _state_dict(self._datasets, num_samples_yielded, num_workers, batch_size)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        # Load the state of each individual dataset\n        for dataset_idx, dataset in enumerate(self._datasets):\n            dataset_state = state_dict.get(str(dataset_idx))\n            if dataset_state:\n                dataset.load_state_dict(dataset_state)\n\n        # If the state_dict contains the number of samples yielded, update it\n        if __NUM_SAMPLES_YIELDED_KEY__ in state_dict:\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n\n        # If the iterator exists, update its state as well\n        if self._iterator is not None:\n            self._iterator.load_state_dict(state_dict)"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "\n    if isinstance(dir_path, Dir):\n        # If dir_path is already a Dir object, return it as is.\n        return dir_path\n\n    if dir_path is None:\n        # If dir_path is None, return an empty Dir object.\n        return Dir()\n\n    # Normalize the path and remove any trailing slashes for consistency.\n    dir_path = os.path.normpath(dir_path)\n\n    # Check if the path is an S3 URL.\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    # Check if the path is a local file system path.\n    if os.path.isabs(dir_path) or dir_path.startswith(\".\"):\n        # Convert to absolute path and ensure it's a directory.\n        abs_path = os.path.abspath(dir_path)\n        if not os.path.isdir(abs_path):\n            raise ValueError(f\"The provided path `{abs_path}` is not a directory.\")\n        return Dir(path=abs_path)\n\n    # Check for specific project path prefixes and resolve accordingly.\n    if dir_path.startswith(\"/lightning/studio/\"):\n        target_name = dir_path.split(\"/\")[3]\n        target_id = None\n        return _resolve_studio(dir_path, target_name, target_id)\n    elif dir_path.startswith(\"/lightning/data_connections/\"):\n        return _resolve_s3_connections(dir_path)\n    elif dir_path.startswith(\"/lightning/datasets/\"):\n        return _resolve_datasets(dir_path)\n\n    # If none of the above conditions are met, assume it's a local relative path.\n    return Dir(path=os.path.abspath(dir_path))"}
