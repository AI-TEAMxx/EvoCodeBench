{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Executing command: {cmd}\")\n    return os.system(cmd)\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "coord.inv_contract", "completion": ":\n      # A hand-optimized version of the d==3 case.\n      sqrtm_cov_sum = jnp.sum(sqrtm_cov, axis=-1, keepdims=True)\n      offsets = jnp.concatenate(\n          [-sqrtm_cov_sum, 2 * sqrtm_cov - sqrtm_cov_sum / 3], axis=-1\n      )\n      control = mean_ex + jnp.moveaxis(offsets, -1, axis)\n    else:\n      raise ValueError(f'basis={basis} not implemented.')\n\n  return control\n\n\ndef compute_control_points(\n    means,\n    covs,\n    rays,\n    tdist,\n    rng,\n    unscented_mip_basis,\n    unscented_scale_mult,\n):\n  \"\"\"Wrapper to compute unscented control points for the MLP class.\"\"\"\n  if unscented_mip_basis == 'hexify':\n    control, perp_mag = hexify(\n        rng,\n        origins=rays.origins,\n        directions=rays.directions,\n        radii=rays.radii,\n        tdist=tdist,\n    )\n  else:\n    # Use a normal unscented transformation.\n    control = unscented_transform(\n        means,\n        covs,\n        basis=unscented_mip_basis,\n        axis=-2,\n    )\n    if unscented_scale_mult > 0:\n      if rays is None:\n        raise SyntaxError(\n            'Rays are required as input if unscented_scale_mult > 0.'\n        )\n      # Mimic the math used by hexify to produce comparable scales.\n      t_recon = jnp.sum(\n          (control - rays.origins[Ellipsis, None, None, :])\n          * rays.directions[Ellipsis, None, None, :],\n          axis=-1,\n      )\n      perp_mag = jnp.sqrt(0.5) * rays.radii[Ellipsis, None, :] * t_recon\n    else:\n      perp_mag = None\n  return control, perp_mag\n\n\ndef compute_control_points(\n    means,\n    covs,\n    rays,\n    tdist,\n    rng,\n    unsc"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as memoization:\n                return memoization.fetch_or_compute(func, func_name, *args, **kwargs)\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: x_min must be less than x_max. Received x_min={values['x_min']} and x_max={values['x_max']}.\"\n        )\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: y_min must be less than y_max. Received y_min={values['y_min']} and y_max={values['y_max']}.\"\n        )\n    return values\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  return np.sum(mat0**2, 0, keepdims=True) + np.sum(mat1**2, 0, keepdims=True) - 2 * np.matmul(\n      mat0.T, mat1\n  )\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "def _should_replace_path(path: Optional[str]) -> bool:\n    if path is None or path == \"\":\n        return True\n\n    if path.startswith(\"lightning://\"):\n        return True\n\n    return False\n/src/lightning/app/frontend/frontend.py\n# Copyright The Lightning AI team.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport os\nfrom typing import Any, Dict, List, Optional, Union\n\nimport lightning_cloud\nfrom lightning_cloud.openapi import (\n    V1LightningappInstance,\n    V1LightningappInstanceSpec,\n    V1LightningappInstanceStatus,\n    V1LightningappInstanceStatusState,\n    V1LightningappSpec,\n    V1LightningappSpecCluster,\n    V1LightningappSpecClusterSpec,\n    V1LightningappSpecClusterSpecCompute,\n    V1LightningappSpecClusterSpecComputeInstanceType,\n    V1LightningappSpecClusterSpecComputeInstanceTypeInstanceType,\n    V1LightningappSpecClusterSpecComputeInstanceTypeInstanceTypeGpu,\n    V1LightningappSpecClusterSpecComputeInstanceTypeInstanceTypeGpuGpuType,\n    V1LightningappSpecClusterSpecComputeInstanceTypeInstanceTypeGpuGpuTypeGpu,\n    V1LightningappSpecClusterSpecComputeInstanceTypeInstanceTypeGpuGpuTypeGpuGpuType,\n    V1LightningappSpecClusterSpecComputeInstanceTypeInstanceTypeGpuGpuTypeGpuGpuTypeGpu,\n    V1LightningappSpecClusterSpecComputeInstanceTypeInstanceTypeGpuGpuTypeGpuGpuTypeGpuGpu,\n    V1LightningappSpecClusterSpecComputeInstanceTypeInstanceTypeGpuGpuTypeGpuGpuTypeGpuGpuGpu,\n    V1LightningappSpecClusterSpecComputeInstanceTypeInstanceTypeGpuGpuTypeGpuGpuTypeGpuGpuGpuGpu,\n    V1LightningappSpecClusterSpecComputeInstanceTypeInstanceTypeGpu"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                \"When 'items' is a dictionary, 'assets_names' must be provided.\"\n            )\n        if dim == 1:\n            items = np.asarray([items.get(k, fill_value) for k in assets_names])\n        else:\n            items = np.asarray(\n                [\n                    [items.get(k, fill_value) for k in assets_names]\n                    for _ in range(n_assets)\n                ]\n            )\n    else:\n        items = np.asarray(items)\n    if dim == 1:\n        if items.ndim != 1:\n            raise ValueError(\n                f\"When 'dim' is 1, 'items' must be a 1-dimensional array, got {items.ndim}.\"\n            )\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"When 'dim' is 1, 'items' must have shape ({n_assets},), got {items.shape}.\"\n            )\n    else:\n        if items.ndim != 2:\n            raise ValueError(\n                f\"When 'dim' is 2, 'items' must be a 2-dimensional array, got {items.ndim}.\"\n            )\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"When 'dim' is 2, 'items' must have shape ({n_assets}, *), got {items.shape}.\"\n            )\n    return items\n\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\")\n        if isinstance(purpose_embedding, list):\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        return MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\"),\n            purpose=data.get(\"purpose\"),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\"),\n            max_depth=data.get(\"max_depth\"),\n            usage_count=data.get(\"usage_count\"),\n            id=data.get(\"id\"),\n            parent_id=data.get(\"parent_id\"),\n            working_agent=data.get(\"working_agent\"),\n            is_prime=data.get(\"is_prime\"),\n            evolve_count=data.get(\"evolve_count\"),\n            number_of_code_executions=data.get(\"number_of_code_executions\"),\n            last_input=data.get(\"last_input\"),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb / 323 * 25\n  linear1 = ((srgb + 11) / 211) ** (12 / 5)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "      orientation_error_deg,\n      jnp.array([0.0]),"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "def fix_mixedcase_words(word):\n    # if lower no uppers after\n    # if upper no\n\n    \"\"\"\n    This function checks a given word and corrects its casing based on specific rules. If the word is entirely in uppercase or lowercase, it returns the word as is. If the word is mixed case, it applies different rules to correct its casing, such as converting it to lowercase, uppercase, or capitalizing it based on the casing of the first two letters.\n\n    Input-Output Arguments\n    :param word: String. The word to be checked and potentially corrected for mixed casing.\n    :return: String. The word after applying casing corrections based on the function's logic.\n    \"\"\"\n    if word.islower():\n        return word\n    if word.isupper():\n        return word\n    if word[0].isupper():\n        if len(word) > 1 and word[1].isupper():\n            return word\n        return word.capitalize()\n    return word.lower()\n\n\ndef fix_punctuations(word):\n    \"\"\"\n    This function checks a given word and corrects its punctuation based on specific rules. If the word ends with a period, exclamation point, or question mark, it returns the word as is. If the word does not end with a period, exclamation point, or question mark, it adds a period to the end of the word.\n\n    Input-Output Arguments\n    :param word: String. The word to be checked and potentially corrected for punctuation.\n    :return: String. The word after applying punctuation corrections based on the function's logic.\n    \"\"\"\n    if word.endswith(\".\"):\n        return word\n    if word.endswith(\"!\"):\n        return word\n    if word.endswith(\"?\"):\n        return word\n    return word + \".\"\n\n\ndef fix_spaces(word):\n    \"\"\"\n    This function checks a given word and corrects its spacing based on specific rules. If the word starts with a space, it returns the word with the leading space removed. If the word does not start with a space, it returns the word as is.\n\n    Input-Output Arguments\n    :param word: String. The word to be checked and potentially corrected for spacing.\n    :return: String. The word after applying spacing corrections based on the function's logic.\n    \"\"\"\n    if word.startswith(\" \"):\n        return word[1:]\n   "}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a binary array (i.e., containing only True or False values).\"\n        )\n    return v\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Compute the norm of the input\n  norm = jnp.sqrt(jnp.sum(x**2, axis=-1, keepdims=True))\n\n  # Compute the scaled input\n  scaled_x = x / norm\n\n  return scaled_x\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n    return summary_df\n\n"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    return jnp.sqrt(jnp.linalg.det(cov))\n  elif mode == 'accurate':\n    return jnp.exp(0.5 * jnp.linalg.slogdet(cov)[1])\n  else:\n    raise ValueError(f'Invalid mode: {mode}')\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Command line interface for XAgent')\n    parser.add_argument('--task', type=str, required=True, help='The task description, specifying what task should be performed.')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload, allowing multiple files to be specified.')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use.')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs.')\n    parser.add_argument('--mode', type=str, default='auto', choices=['auto', 'manual'], help='Operational mode, which can be \\'auto\\' or \\'manual\\', specifying how the task should be executed.')\n    parser.add_argument('--quiet', action='store_true', help='If set, the program runs in quiet mode with minimal output.')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be.')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution.')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans.')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be.')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts, specifying how many times a task can be retried upon failure.')\n    parser.add_argument('--config-file', type=str, default=os.environ.get('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file, specifying where to find the configuration settings.')\n\n    return parser.parse"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\"\n        )\n\n    return v\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx in range(len(workers_intervals)):\n        interval = workers_intervals[worker_idx]\n        current_index = indexes[worker_idx]\n        chunk_index = current_index - interval[0]\n        chunks_index[worker_idx] = chunk_index\n\n    return chunks_index, indexes"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # For smaller levels (fewer cells), store entries in a dense grid.\n    values = hash_resample.grid_resample(values, coordinates)\n  elif datastructure == 'hash':\n    # For larger levels (more cells), store entries in a hash table.\n    values = hash_resample.hash_resample(values, coordinates)\n  else:\n    raise ValueError(\n        f'Invalid datastructure {datastructure}. '\n        'Only \"grid\" and \"hash\" are supported.'\n    )\n  return values\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError(f'v {v} must be >= 1')\n  if v == 1:\n    return np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n  tri_weights = np.zeros([v + 1, v + 1, v + 1])\n  for i in range(v + 1):\n    for j in range(v + 1 - i):\n      k = v - i - j\n      tri_weights[i, j, k] = 1\n  tri_weights = tri_weights / np.sum(tri_weights, axis=(0, 1, 2), keepdims=True)\n  return tri_weights\n\n"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the linear coefficients corresponding to each input query.\n  idx0, _ = sorted_lookup(tq, t)\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx0 + 1, axis=-1)\n\n  # Interpolate between the linear coefficients.\n  td = tq - t0\n  v = v0 + td * (v1 - v0)\n\n  # Extrapolate to zero outside of the original range.\n  v = jnp.where(tq < t[Ellipsis, :1], 0, jnp.where(tq > math.minus_eps(t[Ellipsis, -1:]), 0, v))\n  return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(value > 0 for value in v):\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must contain only positive values. Got {v}.\"\n            )\n    else:\n        if v <= 0:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must be a positive value. Got {v}.\"\n            )\n\n    return v\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "_cameras(\n    cameras,\n    image_size,\n    projection_type,\n):\n  \"\"\"Converts a camera batch into a JAX camera batch.\"\"\"\n  return jaxcam.Cameras(\n      [jax_camera_from_tuple(c, image_size, projection_type) for c in cameras]\n  )\n\n\ndef cast_rays(\n    cameras,\n    rays,\n    projection_type,\n    scene_bbox = None,\n    xnp = np,\n):\n  \"\"\"Converts a ray batch into a JAX ray batch.\"\"\"\n  return jaxcam.Rays(\n      [\n          cast_ray_batch(\n              cameras,\n              r,\n              projection_type,\n              scene_bbox=scene_bbox,\n              xnp=xnp,\n          )\n          for r in rays\n      ]\n  )\n\n\ndef cast_cameras_and_rays(\n    cameras,\n    rays,\n    projection_type,\n    scene_bbox = None,\n    xnp = np,\n):\n  \"\"\"Converts a camera and ray batch into a JAX camera and ray batch.\"\"\"\n  return (\n      cast_cameras(cameras, xnp=xnp),\n      cast_rays(cameras, rays, projection_type, scene_bbox=scene_bbox, xnp=xnp),\n  )\n\n\ndef cast_pinhole_cameras_and_rays(\n    cameras,\n    rays,\n    scene_bbox = None,\n    xnp = np,\n):\n  \"\"\"Converts a pinhole camera and ray batch into a JAX camera and ray batch.\"\"\"\n  return cast_cameras_and_rays(\n      cameras,\n      rays,\n      projection_type=ProjectionType.PERSPECTIVE,\n      scene_bbox=scene_bbox,\n      xnp=xnp,\n  )\n\n\ndef cast_spherical_cameras_and_rays(\n    cameras,\n    rays,\n    scene_bbox = None,\n    xnp = np,\n):\n  \"\"\"Converts a spherical camera and ray batch into a JAX camera and ray batch.\"\"\"\n  return cast_cameras_and_rays(\n      cameras,\n      rays,\n      projection_type=ProjectionType.PANORAMIC,\n      scene_bbox=scene_bbox,\n      xnp=xnp,\n  )"}
{"namespace": "geometry.are_lines_parallel", "completion": "  return jnp.isclose(jnp.dot(dir1, dir2), 1.0)\n\n"}
{"namespace": "common.bleu4_score", "completion": "    tokenizer = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=tokenizer, use_effective_order=True)\n    score = results['score']\n    if with_penalty:\n        penalty = min(1, len(continuation) / len(reference))\n        score *= penalty\n    return score\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.sqrt(jnp.where(x > eps, x, jnp.full_like(x, value_at_zero)))"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  p = w / jnp.diff(t)\n  return p\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for root, _, files in os.walk(path):\n        for file in files:\n            try:\n                file_path = os.path.join(root, file)\n                total_size += os.path.getsize(file_path)\n            except FileNotFoundError:\n                pass\n    return total_size\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    if isinstance(val, np.ndarray):\n        val = np.asanyarray(val)\n        val = val - np.floor((val + offset * period) / period) * period\n    elif isinstance(val, torch.Tensor):\n        val = val - torch.floor((val + offset * period) / period) * period\n    else:\n        raise TypeError('val should be np.ndarray or torch.Tensor, '\n                        f'but got {type(val)}')\n    return val\n\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding.tolist() if agent.purpose_embedding is not None else None,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    if len(items) != len(weights):\n        raise ValueError(\"The number of items and weights must be equal.\")\n\n    if num_bins <= 0:\n        raise ValueError(\"The number of bins must be positive.\")\n\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"All weights must be positive.\")\n\n    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    for item, weight in sorted(zip(items, weights), key=lambda x: x[1], reverse=True):\n        min_weight_bin = min(bin_weights, key=bin_weights.get)\n        bins[min_weight_bin].append(item)\n        bin_weights[min_weight_bin] += weight\n\n    return bins, bin_weightsfrom collections import defaultdict"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, *args, *kwargs.items())\n        data_str = json.dumps(data)\n        data_bytes = data_str.encode(\"utf-8\")\n        return hashlib.sha256(data_bytes).hexdigest()"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Check if the input polygon is a valid 2D array\n    if not isinstance(polygon, np.ndarray) or polygon.ndim != 2 or polygon.shape[1] != 2:\n        raise ValueError(\"Invalid input polygon. Expected a 2D numpy array with shape (N, 2).\")\n\n    # Check if the input max_point_distance is a valid integer\n    if not isinstance(max_point_distance, int) or max_point_distance <= 0:\n        raise ValueError(\"Invalid input max_point_distance. Expected a positive integer.\")\n\n    # Compute the distances between consecutive points\n    distances = np.linalg.norm(polygon[1:] - polygon[:-1], axis=1)\n\n    # Filter out distances that exceed the specified maximum\n    filtered_distances = distances[distances <= max_point_distance]\n\n    # Compute the total length of the polygon\n    total_length = np.sum(filtered_distances)\n\n    return total_length"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if not polygons:\n        return []\n\n    areas = [area(polygon) for polygon in polygons]\n    max_area = max(areas)\n\n    filtered_polygons = []\n    for polygon, area in zip(polygons, areas):\n        if area > max_area * rel_tr or area > abs_tr:\n            filtered_polygons.append(polygon)\n\n    return filtered_polygons\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    indexes = {}\n\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] = num_samples_yielded % batch_size\n\n    return indexes\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n    assert len(results) == len(value) == len(metadatas), \\\n        \"results, value, and metadatas must have the same length.\"\n    filtered_results = []\n    filtered_metadatas = []\n    for result, val, metadata in zip(results, value, metadatas):\n        if val <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"The input array must have the shape (_, 2), where _ can be any number of points.\")\n\n    return 0.5 * np.abs(np.dot(array[:, 0], np.roll(array[:, 1], 1)) - np.dot(array[:, 1], np.roll(array[:, 0], 1)))\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Find the indices where the elements of v should be inserted into a.\n    idx_lo = torch.searchsorted(a, v, side='left')\n    idx_hi = torch.searchsorted(a, v, side='right')\n\n    # Handle the case where v is out of the range of a.\n    idx_lo = torch.where(v < a[0], torch.zeros_like(idx_lo), idx_lo)\n    idx_hi = torch.where(v > a[-1], torch.full_like(idx_hi, a.shape[-1]), idx_hi)\n\n    return idx_lo, idx_hi\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": ", 2],\n      'translation_y': translation_diffs[Ellipsis, 1],\n      'translation_z': translation_diffs[Ellipsis, 2],\n      'orientation': orientation_diffs,\n  )\n  return cameras_pred.replace(**camera_metrics)\n\n\ndef compute_camera_metrics_batch(\n    cameras_gt, cameras_pred\n):\n  \"\"\"Computes the metrics between two camera batches.\"\"\"\n  camera_metrics = jax.vmap(compute_camera_metrics)(cameras_gt, cameras_pred)\n  return camera_metrics\n\n\ndef compute_ray_metrics(\n    rays_gt, rays_pred\n):\n  \"\"\"Computes the metrics between two ray batches.\"\"\"\n  ray_metrics = {\n      'near': jnp.abs(rays_pred.near - rays_gt.near),\n      'far': jnp.abs(rays_pred.far - rays_gt.far),\n      'lossmult': jnp.abs(rays_pred.lossmult - rays_gt.lossmult),\n      'origins': jnp.linalg.norm(rays_pred.origins - rays_gt.origins, axis=-1),\n      'directions': jnp.linalg.norm(\n          rays_pred.directions - rays_gt.directions, axis=-1\n      ),\n      'viewdirs': jnp.linalg.norm(\n          rays_pred.viewdirs - rays_gt.viewdirs, axis=-1\n      ),\n      'radii': jnp.abs(rays_pred.radii - rays_gt.radii),\n      'imageplane': jnp.linalg.norm(\n          rays_pred.imageplane - rays_gt.imageplane, axis=-1\n      ),\n  }\n  return rays_pred.replace(**ray_metrics)\n\n\ndef compute_ray_metrics_batch(\n    rays_gt, rays_pred\n):\n  \"\"\"Computes the metrics between two ray batches.\"\"\"\n  ray_metrics = jax.vmap(compute_ray_metrics)(rays_gt, rays_pred)\n  return ray_metrics\n\n\ndef compute_metrics(\n    cameras_gt, cameras_pred, rays_gt, rays_pred\n):\n "}
{"namespace": "coord.contract", "completion": ":\n      # A hand-optimized version of the d==3 case.\n      sqrtm_cov_sum = jnp.sum(sqrtm_cov, axis=-1, keepdims=True)\n      offsets = jnp.concatenate(\n          [-sqrtm_cov_sum, 2 * sqrtm_cov - sqrtm_cov_sum / 3], axis=-1\n      )\n      control = mean_ex + jnp.moveaxis(offsets, -1, axis)\n    else:\n      raise ValueError(f'basis={basis} not implemented.')\n\n  return control\n\n\ndef compute_control_points(\n    means,\n    covs,\n    rays,\n    tdist,\n    rng,\n    unscented_mip_basis,\n    unscented_scale_mult,\n):\n  \"\"\"Wrapper to compute unscented control points for the MLP class.\"\"\"\n  if unscented_mip_basis == 'hexify':\n    control, perp_mag = hexify(\n        rng,\n        origins=rays.origins,\n        directions=rays.directions,\n        radii=rays.radii,\n        tdist=tdist,\n    )\n  else:\n    # Use a normal unscented transformation.\n    control = unscented_transform(\n        means,\n        covs,\n        basis=unscented_mip_basis,\n        axis=-2,\n    )\n    if unscented_scale_mult > 0:\n      if rays is None:\n        raise SyntaxError(\n            'Rays are required as input if unscented_scale_mult > 0.'\n        )\n      # Mimic the math used by hexify to produce comparable scales.\n      t_recon = jnp.sum(\n          (control - rays.origins[Ellipsis, None, None, :])\n          * rays.directions[Ellipsis, None, None, :],\n          axis=-1,\n      )\n      perp_mag = jnp.sqrt(0.5) * rays.radii[Ellipsis, None, :] * t_recon\n    else:\n      perp_mag = None\n  return control, perp_mag\n\n\ndef compute_control_points(\n    means,\n    covs,\n    rays,\n    tdist,\n    rng,\n    unsc"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n        if abs(num_bytes) < 1000.0:\n            return f\"{num_bytes:.1f} {unit}\"\n        num_bytes /= 1000.0\n    return f\"{num_bytes:.1f} {unit}\"\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check that the array has the specified number of dimensions.\"\"\"\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. got {v.ndim}\"\n            )\n\n        return v\n\n    return __validator\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(cartesian_vector[..., 2] / (r + eps))\n  phi = jnp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n  return r, theta, phi\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    return results['rouge1'].fmeasure\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return pydoc.locate(name)\n    except ImportError:\n        # Try to locate the object using a fallback method.\n        # This is useful for locating objects in the fvcore library, which is not installed by default.\n        try:\n            return pydoc.locate(f\"fvcore.{name}\")\n        except ImportError:\n            raise ImportError(f\"Could not locate object '{name}'\")\n\n"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    if len(ids) != len(scores):\n        raise ValueError(\"The length of ids and scores must be the same.\")\n\n    if len(ids) != len(weights):\n        raise ValueError(\"The length of ids and weights must be the same.\")\n\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of weights must be 1.\")\n\n    return cc_pure(ids, scores, weights, top_k)"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    if percent:\n        x *= 100\n        return f\"{x:.2f}%\"\n    if x == 0:\n        return \"0\"\n    if x < 1e-3:\n        return f\"{x:.2e}\"\n    if x < 1e-2:\n        return f\"{x:.3f}\"\n    if x < 1e-1:\n        return f\"{x:.4f}\"\n    if x < 1:\n        return f\"{x:.5f}\"\n    if x < 10:\n        return f\"{x:.4f}\"\n    if x < 100:\n        return f\"{x:.3f}\"\n    if x < 1000:\n        return f\"{x:.2f}\"\n    if x < 10000:\n        return f\"{x:.1f}\"\n    return f\"{x:.0f}\"\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": ":\n    \"\"\"The `StreamingDataLoader` provides a way to load data from a remote cloud directory.\"\"\"\n\n    def __init__(self, remote_dir: str, num_workers: int = 1, random_seed: int = 42):\n        \"\"\"The `StreamingDataLoader` provides a way to load data from a remote cloud directory.\n\n        Arguments:\n            remote_dir: The remote cloud directory to load the data from.\n            num_workers: The number of workers to use.\n            random_seed: The random seed to be set before shuffling the data.\n\n        \"\"\"\n        self.remote_dir = remote_dir\n        self.num_workers = num_workers\n        self.random_seed = random_seed\n        self.workers: List[StreamingDataLoaderWorker] = []\n        self.workers_tracker: Dict[int, int] = {}\n        self.progress_queue: Optional[Queue] = None\n        self.error_queue: Queue = Queue()\n        self.stop_queues: List[Queue] = []\n        self.items: List[Any] = []\n        self.items_tracker: Dict[int, int] = {}\n        self.items_sizes: Dict[int, int] = {}\n        self.items_sizes_tracker: Dict[int, int] = {}\n        self.items_sizes_tracker_tracker: Dict[int, int] = {}\n        self.items_sizes_tracker_tracker_tracker: Dict[int, int] = {}\n        self.items_sizes_tracker_tracker_tracker_tracker: Dict[int, int] = {}\n        self.items_sizes_tracker_tracker_tracker_tracker_tracker: Dict[int, int] = {}\n        self.items_sizes_tracker_tracker_tracker_tracker_tracker_tracker: Dict[int, int] = {}\n        self.items_sizes_tracker_tracker_tracker_tracker_tracker_tracker_tracker: Dict[int, int] = {}\n        self.items_sizes_tracker_tracker_tracker_tracker_tracker_tracker_tracker_tracker: Dict[int, int] = {}\n        self.items_sizes_tracker_tracker_tracker_tracker_tracker_tracker_tracker_tracker_tracker: Dict[int, int] = {}\n        self.items_sizes_tracker_tracker_tracker_tracker_tracker_tracker"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\"\\n\", \" \")\n    line_text = line_text.replace(\"\\t\", \" \")\n    line_text = line_text.strip()\n    return line_text\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zeros cannot exceed the total number of weights.\")\n    weights = np.random.rand(n)\n    weights[np.random.choice(n, zeros, replace=False)] = 0\n    weights /= weights.sum()\n    return weights"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop(\"module_type\")\n        return cls(module_type, module_dict)\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    if \"bbox\" not in instance:\n        raise AttributeError(\"Instance does not have a bounding box!\")\n    if \"bbox_mode\" not in instance:\n        raise AttributeError(\"Instance does not have a bounding box mode!\")\n    if \"width\" not in instance:\n        raise AttributeError(\"Instance does not have a width!\")\n    if \"height\" not in instance:\n        raise AttributeError(\"Instance does not have a height!\")\n    if \"width\" not in instance:\n        raise AttributeError(\"Instance does not have a width!\")\n    if \"height\" not in instance:\n        raise AttributeError(\"Instance does not have a height!\")\n    if \"width\" not in instance:\n        raise AttributeError(\"Instance does not have a width!\")\n    if \"height\" not in instance:\n        raise AttributeError(\"Instance does not have a height!\")\n    if \"width\" not in instance:\n        raise AttributeError(\"Instance does not have a width!\")\n    if \"height\" not in instance:\n        raise AttributeError(\"Instance does not have a height!\")\n    if \"width\" not in instance:\n        raise AttributeError(\"Instance does not have a width!\")\n    if \"height\" not in instance:\n        raise AttributeError(\"Instance does not have a height!\")\n    if \"width\" not in instance:\n        raise AttributeError(\"Instance does not have a width!\")\n    if \"height\" not in instance:\n        raise AttributeError(\"Instance does not have a height!\")\n    if \"width\" not in instance:\n        raise AttributeError(\"Instance does not have a width!\")\n    if \"height\" not in instance:\n        raise AttributeError(\"Instance does not have a height!\")\n    if \"width\" not in instance:\n        raise AttributeError(\"Instance does not have a width!\")\n    if \"height\" not in instance:\n        raise AttributeError(\"Instance does not have a height!\")\n    if \"width\" not in instance:\n        raise AttributeError(\"Instance does not have a width!\")\n    if \"height\" not in instance:\n        raise AttributeError(\"Instance does not have a height!\")\n    if \"width\" not in instance:\n        raise AttributeError(\"Instance does not have a width!\")\n    if \"height\" not in instance:\n        raise AttributeError(\"Instance does not have a height!\")\n    if \"width\" not in instance:\n        raise AttributeError(\"Instance does not have a width!\")\n    if \"height\" not in instance:\n        raise AttributeError(\"Instance does not have a height!\")\n    if \"width\" not in instance:\n        raise AttributeError(\"Instance does"}
{"namespace": "ref_utils.l2_normalize", "completion": "  norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  norm = jnp.maximum(norm, grad_eps)\n  return x / norm\n\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name = agent_info.split(\":\")[0]\n        input_text = agent_info.split(\":\")[1] if len(agent_info.split(\":\")) > 1 else \"\"\n        return agent_name, input_text"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    if len(annos) == 0:\n        return Instances(image_size)\n\n    if \"gt_masks\" in annos[0]:\n        gt_masks = [x[\"gt_masks\"] for x in annos]\n        gt_masks = [\n            PolygonMasks(x) if isinstance(x, list) else BitMasks(x) for x in gt_masks\n        ]\n        gt_masks = PolygonMasks(gt_masks)\n    else:\n        gt_masks = None\n\n    if \"gt_keypoints\" in annos[0]:\n        gt_keypoints = [x[\"gt_keypoints\"] for x in annos]\n        gt_keypoints = [Keypoints(x) for x in gt_keypoints]\n        gt_keypoints = Keypoints(gt_keypoints)\n    else:\n        gt_keypoints = None\n\n    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    boxes = [BoxMode.convert(box, BoxMode.XYXY_ABS, BoxMode.XYWH_ABS) for box in boxes]\n    boxes = torch.as_tensor(boxes, dtype=torch.float32)\n    boxes = Boxes(boxes)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n\n    keypoints = [obj.get(\"keypoints\", None) for obj in annos]\n    if keypoints[0] is not None:\n        keypoints = [Keypoints(x) for x in keypoints]\n        keypoints = Keypoints(keypoints)\n\n    return Instances(\n        image_size,\n        gt_boxes=boxes,\n        gt_classes=classes,\n        gt_masks=gt_masks,\n        gt_keypoints=gt_keypoints,\n        keypoints=keypoints,\n    )\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    data_filename = \"ftse100_dataset\"\n    df = download_dataset(\n        data_filename, data_home=data_home, download_if_missing=download_if_missing\n    )\n    return df\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    if cov.shape[0] != cov.shape[1]:\n        raise ValueError(f\"`cov` must be a square matrix, got a {cov.shape} matrix\")\n    std = np.sqrt(np.diag(cov))\n    corr = cov / std / std[:, None]\n    return corr, std"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    def _freeze_training_mode(module):\n        if hasattr(module, \"training\"):\n            module.__class__.training = torch.jit.Final[bool]\n\n    model.apply(_freeze_training_mode)\n    yield\n    model.apply(_freeze_training_mode)\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if the shapes of field1 and field2 match.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1].shape} and {values[field2].shape}.\"\n            )\n\n        return values\n\n    return __root_validator\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "def cast_metrics(metrics: Union[List[str], List[Dict]]) -> Tuple[List[str], List[Dict[str, Any]]]:\n    \"\"\"\n    This function processes a list of metrics, which can either be strings or dictionaries, and returns a tuple containing a list of metric names and a list of dictionaries with metric parameters. It ensures that the input is correctly formatted and transforms it into a standardized format for further processing.\n\n    Input-Output Arguments\n    :param metrics: Union[List[str], List[Dict]]. A list that can either contain strings representing metric names or dictionaries with metric details. It is used to extract metric names and parameters for further processing.\n    :return: Tuple[List[str], List[Dict[str, Any]]]. A tuple where the first element is a list of metric names and the second element is a list of dictionaries containing metric parameters. This standardized format allows for consistent handling of metric information.\n    \"\"\"\n    if isinstance(metrics, list):\n        if isinstance(metrics[0], str):\n            metrics = [\n                {\n                    \"name\": metric,\n                    \"params\": {}\n                } for metric in metrics\n            ]\n        else:\n            metrics = deepcopy(metrics)\n    else:\n        metrics = [metrics]\n\n    metric_names = [metric['name'] for metric in metrics]\n    metric_params = [metric['params'] for metric in metrics]\n\n    return metric_names, metric_params\n\ndef cast_embedding_model(key, value):\n    if key == 'embedding_model':\n        return key, embedding_models[value]\n    else:\n        return key, value\n\ndef cast_metrics(metrics: Union[List[str], List[Dict]]) -> Tuple[List[str], List[Dict[str, Any]]]:\n    \"\"\"\n    This function processes a list of metrics, which can either be strings or dictionaries, and returns a tuple containing a list of metric names and a list of dictionaries with metric parameters. It ensures that the input is correctly formatted and transforms it into a standardized format for further processing.\n\n    Input-Output Arguments\n    :param metrics: Union[List[str], List[Dict]]. A list that can either contain strings representing metric names or dictionaries with metric details. It is used to extract metric names and parameters for further processing.\n    :return: Tuple[List[str], List[Dict[str, Any"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    fn_inv = {\n        contract: contract3_isoscale,\n        contract3_isoscale: contract,\n    }[fn]\n  t_to_s, s_to_t = construct_ray_warps(fn, t_near, t_far, fn_inv=fn_inv)\n  return t_to_s, s_to_t\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  check_zero_endpoints(w)\n  return jnp.trapz(w, t, axis=-1)"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    # Calculate the weighted sum of scores for each ID\n    weighted_sums = {}\n    for i in range(len(ids)):\n        for j in range(len(ids[i])):\n            if ids[i][j] not in weighted_sums:\n                weighted_sums[ids[i][j]] = 0\n            weighted_sums[ids[i][j]] += scores[i][j] * weights[i]\n\n    # Normalize the weighted sums\n    min_sum = min(weighted_sums.values())\n    max_sum = max(weighted_sums.values())\n    if max_sum != min_sum:\n        for key in weighted_sums:\n            weighted_sums[key] = (weighted_sums[key] - min_sum) / (max_sum - min_sum)\n\n    # Sort the IDs by their weighted sums in descending order\n    sorted_ids = sorted(weighted_sums.items(), key=lambda x: x[1], reverse=True)\n\n    # Return the top K IDs and their corresponding weighted sums\n    top_k_ids = [id for id, _ in sorted_ids[:top_k]]\n    top_k_scores = [weighted_sums[id] for id in top_k_ids]\n    return top_k_ids, top_k_scores"}
{"namespace": "coord.track_linearize", "completion": ":\n      # A hand-optimized version of the d==3 case.\n      sqrtm_cov_sum = jnp.sum(sqrtm_cov, axis=-1, keepdims=True)\n      offsets = jnp.concatenate(\n          [-sqrtm_cov_sum, 2 * sqrtm_cov - sqrtm_cov_sum / 3], axis=-1\n      )\n      control = mean_ex + jnp.moveaxis(offsets, -1, axis)\n    else:\n      raise ValueError(f'basis={basis} not implemented.')\n\n  return control\n\n\ndef construct_ray_warps(fn, t_near, t_far, *, fn_inv=None):\n  \"\"\"Construct a bijection between metric distances and normalized distances.\n\n  See the text around Equation 11 in https://arxiv.org/abs/2111.12077 for a\n  detailed explanation.\n\n  Args:\n    fn: the function to ray distances.\n    t_near: a tensor of near-plane distances.\n    t_far: a tensor of far-plane distances.\n    fn_inv: Optional, if not None then it's used as the inverse of fn().\n\n  Returns:\n    t_to_s: a function that maps distances to normalized distances in [0, 1].\n    s_to_t: the inverse of t_to_s.\n  \"\"\"\n  if fn is None:\n    fn_fwd = lambda x: x\n    fn_inv = lambda x: x\n  else:\n    fn_fwd = fn\n    if fn_inv is None:\n      # A simple mapping from some functions to their inverse.\n      inv_mapping = {\n          'reciprocal': jnp.reciprocal,\n          'log': jnp.exp,\n          'exp': jnp.log,\n          'sqrt': jnp.square,\n          'square': jnp.sqrt,\n      }\n      fn_inv = inv_mapping[fn.__name__]\n  fn_t_near, fn_t_far = [fn_fwd(t) for t in (t_near, t_far)]\n  # Forcibly clip t to the range of valid values, to guard against inf's.\n  t_clip = lambda t: jnp.clip(t, t_near, t_far)\n  t_to"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[: len(i) // 2], i[len(i) // 2 :]]\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix must be square\")"}
{"namespace": "coord.pos_enc", "completion": "  if d == 3:\n    # A hand-optimized version of the d==3 case.\n    sqrtm_cov_sum = jnp.sum(sqrtm_cov, axis=-1, keepdims=True)\n    offsets = jnp.concatenate(\n        [-sqrtm_cov_sum, 2 * sqrtm_cov - sqrtm_cov_sum / 3], axis=-1\n    )\n    control = mean_ex + jnp.moveaxis(offsets, -1, axis)\n  else:\n    transform = np.sqrt(d + 1) * np.eye(d) + (1 - np.sqrt(d + 1)) / d\n    #        == sqrt((d+1)) * sqrtm(eye(d) - 1/(d+1))\n    transform1 = np.concatenate([-np.ones((d, 1)), transform], axis=-1)\n    control = mean_ex + jnp.moveaxis(\n        math.matmul(sqrtm_cov, transform1), -1, axis\n    )\n  return control\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Check if len(field1) equals len(field2).\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validatorfrom typing import List, Optional, Tuple\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": ", 0, 0, self.W, self.H, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glDrawBuffer(gl.GL_COLOR_ATTACHMENT0)\n\n    def render_imgui(self, viewer: 'VolumetricVideoViewer', batch: dotdict):\n        super().render_imgui(viewer, batch)\n\n        i = batch.i\n        from imgui_bundle import imgui\n        self.point_smooth = imgui.checkbox(f'Point smooth##{i}', self.point_smooth)[1]\n        self.alpha_blending = imgui.checkbox(f'Alpha blending##{i}', self.alpha_blending)[1]\n        self.radii_mult_volume = imgui.slider_float(f'Radii multiplier volume##{i}', self.radii_mult_volume, 0.1, 3.0)[1]\n        self.radii_mult_solid = imgui.slider_float(f'Radii multiplier solid##{i}', self.radii_mult_solid, 0.1, 3.0)[1]\n\n\nclass Splat2(Mesh):  # FIXME: Not rendering, need to debug this\n    def __init__(self,\n                 *args,\n                 H: int = 512,\n                 W: int = 512,\n                 tex_dtype: str = torch.half,\n\n                 pts_per_pix: int = 24,  # render less for the static background since we're only doing a demo\n                 blit_last_ratio: float = 0.0,\n                 volume_rendering: bool = True,\n                 radii_mult_volume: float = 1.00,  # 2 / 3 is the right integration, but will leave holes, 1.0 will make it bloat, 0.85 looks visually better\n                 radii_mult_solid: float = 0.85,  # 2 / 3 is the right integration, but will leave holes, 1.0 will make it bloat, 0.85 looks visually better\n\n                 point_smooth: bool = True,\n                 alpha_blending: bool = True,\n                 **kwargs):\n        kwargs = dotdict"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    return NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_epsilon=bert_config.layer_norm_epsilon,\n        pad_token_id=bert_config.pad_token_id,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n    )\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": " self.write_lower, self.read_color, self.read_upper, self.read_lower])\n            gl.glDeleteVertexArrays(1, [self.vao])\n\n        # Init textures\n        self.write_fbo = gl.glGenFramebuffers(1)[0]\n        self.read_fbo = gl.glGenFramebuffers(1)[0]\n        self.write_color = gl.glGenTextures(1)[0]\n        self.write_upper = gl.glGenTextures(1)[0]\n        self.write_lower = gl.glGenTextures(1)[0]\n        self.read_color = gl.glGenTextures(1)[0]\n        self.read_upper = gl.glGenTextures(1)[0]\n        self.read_lower = gl.glGenTextures(1)[0]\n        self.vao = gl.glGenVertexArrays(1)[0]\n\n        # Init textures\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, self.write_fbo)\n        gl.glFramebufferTexture2D(gl.GL_COLOR_ATTACHMENT0, gl.GL_TEXTURE_2D, self.write_color, 0)\n        gl.glFramebufferTexture2D(gl.GL_COLOR_ATTACHMENT1, gl.GL_TEXTURE_2D, self.write_upper, 0)\n        gl.glFramebufferTexture2D(gl.GL_COLOR_ATTACHMENT2, gl.GL_TEXTURE_2D, self.write_lower, 0)\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, self.read_fbo)\n        gl.glFramebufferTexture2D(gl.GL_COLOR_ATTACHMENT0, gl.GL_TEXTURE_2D, self.read_color, 0)\n        gl.glFramebufferTexture2D(gl.GL_COLOR_ATTACHMENT1, gl.GL_TEXTURE_2D, self.read_upper, 0)\n        gl.glFramebufferTexture2D(gl.GL_COLOR_ATTACHMENT2, gl.GL_TEXTURE_2D, self.read_lower, 0)\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, 0)\n\n        # Init textures\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.write_color"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": " texture to match the height and width of the output image. It then sets the OpenGL viewport and scissor to only render in the region of the viewport specified by the output image size.\n\n        It concatenates the `xyz`, `rgb`, and `rad` tensors along the last dimension and flattens the result into a 1D tensor.\n\n        The method then uploads the input data to OpenGL for rendering and performs depth peeling using OpenGL. The method uploads the camera parameters to OpenGL and renders the point cloud, saving the output buffer to the `back_fbo` attribute of the class.\n\n        Finally, the method moves the content from the `back_fbo` to the `front_fbo` and returns the `front_fbo` as the rendered image.\n        \"\"\"\n        front_fbo, front_color, front_upper, front_lower = self.read_fbo, self.read_color, self.read_upper, self.read_lower\n        back_fbo, back_color, back_upper, back_lower = self.write_fbo, self.write_color, self.write_upper, self.write_lower\n\n        # Only clear the output once\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, front_fbo)  # for offscreen rendering to textures\n        gl.glClearBufferfv(gl.GL_COLOR, 0, [0.0, 0.0, 0.0, 0.0])\n        # gl.glClearBufferfv(gl.GL_COLOR, 1, [1e9])\n        gl.glClearBufferfv(gl.GL_COLOR, 2, [0.0, 0.0, 0.0, 0.0])\n        gl.glClearBufferfv(gl.GL_DEPTH, 0, [1e9])  # this is for depth testing\n\n        # Only clear the output once\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, back_fbo)  # for offscreen rendering to textures\n        gl.glClearBufferfv(gl.GL_COLOR, 0, [0.0, 0.0, 0.0, 0.0])\n        # gl.glClearBufferfv(gl.GL_COLOR, 1, [1e9])\n        gl"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": ".Tensor, radius: float = 0.05, delta: float = 0.001):\n    # Radius weighted function from structured local radiance field\n    weights = (-d2 / (2 * radius ** 2)).exp().clip(0)  # B, S, K\n    weights = normalize_sum(weights)\n    return weights\n\n\ndef normalize_sum(weights: torch.Tensor):\n    # Normalize sum from structured local sum field\n    weights = weights / weights.sum(dim=-1, keepdim=True)  # B, S, K\n    return weights\n\n\ndef filter_points(pcd_new: torch.Tensor, pcd_old: torch.Tensor, radius: float = 0.05, K: int = 5):\n    # Filter points from structured local points field\n    from pytorch3d.ops import knn_points, ball_query\n\n    # close = ball_query(pcd_new, pcd_old, radius=radius, return_nn=False, K=K)  # B, S, K\n    close = knn_points(pcd_new, pcd_old, return_sorted=False, return_nn=False, K=K)  # B, S, K\n    dists, idx = close.dists, close.idx\n\n    B, S, C = pcd_new.shape\n    B, N, C = pcd_old.shape\n    pcd_new = multi_gather(pcd_old, idx.view(B, S * K)).view(B, S, K, -1)  # B, S, K, 3\n    weights = weight_function(dists, radius)[..., None]  # B, S, K, 1\n    pcd_new = (pcd_new * weights).sum(dim=-2)\n    # pcd_new = pcd_new.mean(dim=-2)\n    return pcd_new\n\n\ndef filter_points_features(pcd_new: torch.Tensor, pcd_old: torch.Tensor, ind_old: torch.Tensor, radius: float = 0.05, K: int = 5):\n    # Filter points features from structured local points features field\n    from pytorch3d.ops import knn_points, ball_query\n\n    #"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "(gl.GL_VIEWPORT)\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n\n        # !: BATCH\n        self.upload_gl_buffers(xyz, rgb, rad, occ)\n        self.upload_gl_uniforms(batch)\n        gl.glDrawArrays(gl.GL_POINTS, 0, xyz.shape[1])\n\n        # !: BATCH\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, 0)\n        gl.glBindVertexArray(0)\n\n        # !: BATCH\n        rgb = self.read_rgb(H, W)\n        occ = self.read_occ(H, W)\n        rad = self.read_rad(H, W)\n\n        # !: BATCH\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, 0)\n        gl.glBindVertexArray(0)\n\n        # !: BATCH\n        return rgb, occ, rad\n\n    def read_rgb(self, H: int, W: int):\n        \"\"\"\n        Reads the RGB image from the `read_color` attribute of the class.\n\n        Args:\n            H (int): The height of the output image.\n            W (int): The width of the output image.\n\n        Returns:\n            A torch.Tensor of shape (1, H, W, 3) containing the RGB image.\n        \"\"\"\n        from cuda import cudart\n        kind = cudart.cudaMemcpyKind.cudaMemcpyDeviceToHost\n        rgb = torch.empty(1, H, W, 3, dtype=self.dtype, device=self.device)\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsMapResources(1, [self.cu_read_color], 0))\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsResourceGetMappedPointer(rgb.data_ptr, self.cu_read_color, 0))\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsUnmapResources(1, [self.cu_read_color], 0))\n        return rgb\n\n    def read_occ(self, H: int, W: int):\n        \"\"\"\n        Reads the accumulated alpha image from the `read_lower` attribute of the class.\n\n        Args:\n            H"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": ".Tensor, radius: float = 0.05):\n    # Radius weighted function from structured local radiance field\n    weights = (-d2 / (2 * radius ** 2)).exp().clip(0)  # B, S, K\n    weights = normalize_sum(weights)\n    return weights\n\n\ndef normalize_sum(weights: torch.Tensor):\n    # Normalize sum\n    weights = weights / weights.sum(dim=-1, keepdim=True)  # B, S, K\n    return weights\n\n\ndef normalize_mean(weights: torch.Tensor):\n    # Normalize mean\n    weights = weights.mean(dim=-1)  # B, S, K\n    return weights\n\n\ndef normalize_max(weights: torch.Tensor):\n    # Normalize max\n    weights = weights.max(dim=-1)  # B, S, K\n    return weights\n\n\ndef normalize_min(weights: torch.Tensor):\n    # Normalize min\n    weights = weights.min(dim=-1)  # B, S, K\n    return weights\n\n\ndef normalize_std(weights: torch.Tensor):\n    # Normalize std\n    weights = weights.std(dim=-1)  # B, S, K\n    return weights\n\n\ndef normalize_var(weights: torch.Tensor):\n    # Normalize var\n    weights = weights.var(dim=-1)  # B, S, K\n    return weights\n\n\ndef normalize_median(weights: torch.Tensor):\n    # Normalize median\n    weights = weights.median(dim=-1)  # B, S, K\n    return weights\n\n\ndef normalize_quantile(weights: torch.Tensor):\n    # Normalize quantile\n    weights = weights.quantile(dim=-1)  # B, S, K\n    return weights\n\n\ndef normalize_prod(weights: torch.Tensor):\n    # Normalize prod\n    weights = weights.prod(dim=-1)  # B, S, K\n    return weights\n\n\ndef normalize_cumprod(weights: torch.Tensor):\n    # Normalize cumprod\n    weights = weights.cumprod(dim=-1)  # B, S, K\n    return weights\n\n\ndef normalize_cumsum(weights: torch.Tensor):\n    # Normalize cumsum\n    weights = weights.cumsum(dim=-1)  # B, S, K\n    return"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": ".GL_SCISSOR, 0, 0, W, H)\n\n        # !: BATCH\n        self.init_gl_buffers(xyz.shape[1])  # maybe init the buffer\n        self.init_textures()  # maybe init the texture\n\n        # !: BATCH\n        self.upload_gl_buffers(xyz, rgb, rad, occ)  # upload the input data to OpenGL for rendering\n        self.upload_gl_uniforms(batch)  # upload the camera parameters to OpenGL\n        gl.glDrawArrays(gl.GL_POINTS, 0, xyz.shape[1])  # number of vertices\n\n        # !: BATCH\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, 0)\n        gl.glBindVertexArray(0)\n\n        # !: BATCH\n        rgb = self.read_rgb(kind)  # read the rendered image from the read_fbo\n        occ = self.read_occ(kind)  # read the accumulated alpha image from the read_fbo\n        rad = self.read_rad(kind)  # read the depth map from the read_fbo\n\n        # !: BATCH\n        rgb = rgb.reshape(1, H, W, 3)  # reshape the rendered image to match the output image size\n        occ = occ.reshape(1, H, W, 1)  # reshape the accumulated alpha image to match the output image size\n        rad = rad.reshape(1, H, W, 1)  # reshape the depth map to match the output image size\n\n        # !: BATCH\n        gl.glViewport(0, 0, old_W, old_H)\n        gl.glScissor(0, 0, old_W, old_H)\n\n        return rgb, occ, rad\n\n    def backward(self, rgb: torch.Tensor, occ: torch.Tensor, rad: torch.Tensor, batch: dotdict):\n        \"\"\"\n        Renders a 3D point cloud using OpenGL and returns the rendered RGB image, accumulated alpha image, and depth map.\n\n        Args:\n            rgb (torch.Tensor): A tensor of shape (B, N, 3) containing the RGB color values of the points.\n            occ (torch.Tensor): A tensor of shape (B, N, 1) containing the accumulated alpha values of"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "87.5)\n        self.eps = eps\n\n    def forward(self, prediction, target, mask):\n        # Deal with the channel dimension, the input dimension may have (B, C, H, W) or (B, H, W)\n        if prediction.ndim == 4: prediction = prediction[:, 0]  # (B, H, W)\n        if target.ndim == 4: target = target[:, 0]  # (B, H, W)\n        if mask.ndim == 4: mask = mask[:, 0]  # (B, H, W)\n\n        total = 0\n        # Maybe there is a better way to do the batching\n        for i in range(prediction.shape[0]):\n            g = torch.log(prediction[i][mask[i]] + self.eps) - torch.log(target[i][mask[i]] + self.eps)  # (N,)\n            Dg = torch.var(g) + self.beta * torch.pow(torch.mean(g), 2)  # scalar\n            total += self.alpha * torch.sqrt(Dg)\n\n        return total\n\n\n# Modified version of Adabins repository\n# https://github.com/shariqfarooq123/AdaBins/blob/0952d91e9e762be310bb4cd055cbfe2448c0ce20/loss.py#L7\nclass ScaleInvariantLogLoss(nn.Module):\n    def __init__(self, alpha=10.0, beta=0.15, eps=0.0):\n        super(ScaleInvariantLogLoss, self).__init__()\n\n        self.alpha = alpha\n        self.beta = beta\n        # The eps is added to avoid log(0) and division by zero\n        # But it should be gauranteed that the network output is always non-negative\n        self.eps = eps\n\n    def forward(self, prediction, target, mask):\n        # Deal with the channel dimension, the input dimension may have (B, C, H, W) or (B, H, W)\n        if prediction.ndim == 4: prediction = prediction[:, 0]  # (B, H, W)\n        if target.ndim"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "87.5)\n        self.eps = eps\n\n    def forward(self, prediction, target, mask):\n        # Deal with the channel dimension, the input dimension may have (B, C, H, W) or (B, H, W)\n        if prediction.ndim == 4: prediction = prediction[:, 0]  # (B, H, W)\n        if target.ndim == 4: target = target[:, 0]  # (B, H, W)\n        if mask.ndim == 4: mask = mask[:, 0]  # (B, H, W)\n\n        total = 0\n        # Maybe there is a better way to do the batching\n        for i in range(prediction.shape[0]):\n            g = torch.log(prediction[i][mask[i]] + self.eps) - torch.log(target[i][mask[i]] + self.eps)  # (N,)\n            Dg = torch.var(g) + self.beta * torch.pow(torch.mean(g), 2)  # scalar\n            total += self.alpha * torch.sqrt(Dg)\n\n        return total\n\n\n# Modified version of Adabins repository\n# https://github.com/shariqfarooq123/AdaBins/blob/0952d91e9e762be310bb4cd055cbfe2448c0ce20/loss.py#L7\nclass ScaleInvariantLogLoss(nn.Module):\n    def __init__(self, alpha=10.0, beta=0.15, eps=0.0):\n        super(ScaleInvariantLogLoss, self).__init__()\n\n        self.alpha = alpha\n        self.beta = beta\n        # The eps is added to avoid log(0) and division by zero\n        # But it should be gauranteed that the network output is always non-negative\n        self.eps = eps\n\n    def forward(self, prediction, target, mask):\n        # Deal with the channel dimension, the input dimension may have (B, C, H, W) or (B, H, W)\n        if prediction.ndim == 4: prediction = prediction[:, 0]  # (B, H, W)\n        if target.ndim"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "6448c0ce20/loss.py#L10\nclass ScaleInvariantLogLoss(nn.Module):\n    def __init__(self, alpha=10.0, beta=0.15, eps=0.0):\n        super(ScaleInvariantLogLoss, self).__init__()\n\n        self.alpha = alpha\n        self.beta = beta\n        # The eps is added to avoid log(0) and division by zero\n        # But it should be gauranteed that the network output is always non-negative\n        self.eps = eps\n\n    def forward(self, prediction, target, mask):\n        # Deal with the channel dimension, the input dimension may have (B, C, H, W) or (B, H, W)\n        if prediction.ndim == 4: prediction = prediction[:, 0]  # (B, H, W)\n        if target.ndim == 4: target = target[:, 0]  # (B, H, W)\n        if mask.ndim == 4: mask = mask[:, 0]  # (B, H, W)\n\n        total = 0\n        # Maybe there is a better way to do the batching\n        for i in range(prediction.shape[0]):\n            g = torch.log(prediction[i][mask[i]] + self.eps) - torch.log(target[i][mask[i]] + self.eps)  # (N,)\n            Dg = torch.var(g) + self.beta * torch.pow(torch.mean(g), 2)  # scalar\n            total += self.alpha * torch.sqrt(Dg)\n\n        return total\n\n\n# Modified version of Adabins repository\n# https://github.com/shariqfarooq123/AdaBins/blob/0952d91e9e762be310bb4cd055cbfe2448c0ce20/loss.py#L10\nclass ScaleInvariantLogLoss(nn.Module):\n    def __init__(self, alpha=10.0, beta=0.15, eps=0.0):\n        super(ScaleInvariantLogLoss, self).__init__()\n\n        self.alpha = alpha\n        self.beta = beta\n        # The eps"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(ps, cw, t)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n    \"\"\"\n    Performs importance sampling on a piecewise-constant probability density function (PDF) defined by bin endpoints and weights. It generates samples according to the specified PDF, with options for perturbation and jittering of samples.\n\n    Input-Output Arguments\n    :param t: torch.Tensor. The tensor containing bin endpoint coordinates, which must be sorted. It represents the domain of the PDF.\n    :param w: torch.Tensor. The tensor containing weights for each bin, used to define the piecewise-constant PDF.\n    :param num_samples: int. The number of samples to generate from the PDF.\n    :param perturb: bool, optional. If True, applies perturbation to the sampling process to avoid sample clustering at bin boundaries. Defaults to True.\n    :param single_jitter: bool, optional. If True, applies the same jitter to every sample along each dimension. If False, applies independent jitter to each sample. Defaults to False.\n    :return: torch.Tensor. A tensor of samples drawn from the specified PDF. The shape of the tensor is determined by the input tensors and the number of samples.\n    \"\"\"\n    # Compute the CDF and PDF for each weight vector.\n    cw = integrate_weights(w)\n    p = w / (t[..., 1:] - t[..., :-1])\n    # Sample uniformly in [0, 1) and invert the CDF.\n    u = torch.rand(num_samples, device=t.device)\n    t_new = invert_cdf(u, t, cw)\n    # Optionally perturb the samples to avoid sample clustering at bin boundaries.\n    if perturb:\n        # Compute the PDF at the sample locations.\n        p_new = query(t_new, t, p)\n        # Compute the CDF at the sample locations.\n        cw_new = query(t_new, t, cw)\n        # Compute the inverse CDF at the sample locations.\n        t_new = invert_cdf(u, t_new, cw_new)\n        # Optionally apply the same jitter to every sample along each dimension.\n        if single_jitter:\n            # Compute the jitter for each sample.\n            jitter = torch.rand(num_samples, device=t.device)\n            # Apply the jitter"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n    t_dilate = torch.linspace(t[..., 0], t[..., -1], steps=dilation * (t.shape[-1] - 1) + 1, device=t.device, dtype=t.dtype)\n    t_dilate = torch.clamp(t_dilate, domain[0], domain[1])\n    w_dilate = torch.max_pool1d(w, kernel_size=dilation, stride=dilation, padding=0)\n    return t_dilate, w_dilate\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check if the query times match a step change time exactly\n    match = torch.any(tq[..., None] == t[..., None], dim=-1)\n\n    # If there are no exact matches, interpolate the values at the query times\n    if not torch.any(match):\n        # Find the indices of the intervals containing the query times\n        indices = torch.searchsorted(t, tq, side='right') - 1\n\n        # Interpolate the values at the query times\n        y_interp = (y[..., indices] * (t[..., indices + 1] - tq) +\n                    y[..., indices + 1] * (tq - t[..., indices])) / (t[..., indices + 1] - t[..., indices])\n\n        # Return the interpolated values\n        return y_interp\n\n    # If there are exact matches, return the outside value\n    else:\n        return torch.where(match, outside_value, query(tq, t, y, outside_value))\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    \"\"\"\n    This function anneals the weights based on the training fraction and an annealing slope using Schlick's bias function. It adjusts the weights of a tensor based on the progression of training, making the weight adjustment more dynamic as training progresses. It ensures stability in the computation by handling cases where adjacent intervals have zero distance, setting their weight to zero, and preventing NaN values by using a softmax operation on the adjusted weights.\n\n    Input-Output Arguments\n    :param t: torch.Tensor. The tensor representing time or another sequential dimension, used to align with the weights tensor.\n    :param w: torch.Tensor. The weights tensor that will be adjusted based on the training fraction and anneal slope.\n    :param train_frac: float. The fraction of training completed, used to calculate the annealing effect on weights.\n    :param anneal_slope: float, optional. The slope of the annealing function, determining how sharply weights are adjusted. Defaults to 10.0.\n    :param eps: torch.float32, optional. A very small number added to prevent division by zero and log of zero in computations. Defaults to a small epsilon value squared.\n    :return: torch.Tensor. The adjusted weights tensor after applying the annealing process.\n    \"\"\"\n    # Compute the annealed weights.\n    t_mid = (t[..., 1:] + t[..., :-1]) / 2\n    t_mid = t_mid.clip(t[..., :-1], t[..., 1:])\n    t_mid = t_mid.clip(t[..., 0], t[..., -1])\n    t_mid = t_mid.clip(t[..., 0], t[..., -1])\n    t_mid = t_mid.clip(t[..., 0], t[..., -1])\n    t_mid = t_mid.clip(t[..., 0], t[..., -1])\n    t_mid = t_mid.clip(t[..., 0], t[..., -1])\n    t_mid = t_mid.clip(t[..., 0], t[..., -1])\n    t_mid = t_mid.clip(t[..., 0], t[..., -1])\n    t_mid = t_mid.clip(t"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "W\n\n\ndef load_ims_bytes_from_disk(ims: np.ndarray, desc=\"Loading image bytes from disk\"):\n    sh = ims.shape\n    ims = ims.ravel()\n    ims_bytes = parallel_execution(list(ims), action=load_image_bytes, desc=desc, print_progress=True)\n    ims_bytes = np.asarray(ims_bytes).reshape(sh)  # reorganize shapes\n    return ims_bytes\n\n\ndef load_resize_undist_im_bytes(imp: str,\n                                K: np.ndarray,\n                                D: np.ndarray,\n                                ratio: Union[float, List[int]] = 1.0,\n                                center_crop_size: List[int] = [-1, -1],\n                                encode_ext='.jpg',\n                                decode_flag=cv2.IMREAD_UNCHANGED,\n                                dist_opt_K: bool = False,\n                                jpeg_quality: int = 100,\n                                png_compression: int = 6\n                                ):\n    # Load image -> resize -> undistort -> save to bytes (jpeg)\n    img = load_image_from_bytes(load_image_bytes(imp), decode_flag=decode_flag)[..., :3]  # cv2 decoding (fast)\n\n    oH, oW = img.shape[:2]\n\n    if dist_opt_K:\n        newCameraMatrix, _ = cv2.getOptimalNewCameraMatrix(K, D, (oW, oH), 0, (oW, oH))\n        img = cv2.undistort(img, K, D, newCameraMatrix=newCameraMatrix)\n        K = newCameraMatrix\n    else:\n        img = cv2.undistort(img, K, D)\n\n    # Maybe update image size\n    if not ((isinstance(ratio, float) and ratio == 1.0)):\n        if isinstance(ratio, float):\n            H, W = int(oH * ratio), int(oW * ratio)\n        else:\n            H, W = ratio  # ratio is actually the target image size\n        rH, rW = H / oH, W / oW\n        K = K.copy()\n        K[0:1] = K[0:"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust the faces tensor to match the batch dimension of the vertices tensor\n    if f.ndim < v.ndim:\n        f = f.unsqueeze(0).expand(v.shape[0], *f.shape)\n\n    # compute the normals of the faces\n    v1 = multi_gather(v, f[:, :, 0], dim)\n    v2 = multi_gather(v, f[:, :, 1], dim)\n    v3 = multi_gather(v, f[:, :, 2], dim)\n    n = torch.cross(v2 - v1, v3 - v1, dim=dim)\n\n    # reshape the result to maintain the original faces tensor structure with additional dimensions for batch processing\n    n = n.reshape(*f.shape[:-1], *n.shape[dim + 1:])\n\n    return n\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "st_im_bytes,\n                             desc=desc,\n                             print_progress=True,\n                             **kwargs)\n    out = np.asarray(out)\n    out = out.reshape(sh + (1, 1))\n    return out\n\n\ndef load_resize_undist_ims_bytes_from_disk(ims: np.ndarray,\n                                           Ks: np.ndarray,\n                                           Ds: np.ndarray,\n                                           ratio: Union[float, List[int], List[float]] = 1.0,\n                                           center_crop_size: List[int] = [-1, -1],\n                                           desc=\"Loading image bytes from disk\",\n                                           **kwargs):\n    # Load image bytes from disk -> resize -> undistort -> save to bytes (jpeg)\n    sh = ims.shape  # V, N\n    # Ks = np.broadcast_to(Ks[:, None], (sh + (3, 3)))\n    # Ds = np.broadcast_to(Ds[:, None], (sh + (1, 5)))\n\n    ims = ims.reshape((np.prod(sh)))\n    # from easyvolcap.utils.dist_utils import get_rank\n    # if not get_rank(): __import__('easyvolcap.utils.console_utils', fromlist=['debugger']).debugger()\n    # else:\n    #     while 1: pass\n    Ks = Ks.reshape((np.prod(sh), 3, 3))\n    Ds = Ds.reshape((np.prod(sh), 1, 5))\n\n    ims = list(ims)\n    Ks = list(Ks)\n    Ds = list(Ds)  # only convert outer most dim to list\n\n    if isinstance(ratio, list) and len(ratio) and isinstance(ratio[0], float):\n        ratio = np.broadcast_to(np.asarray(ratio)[:, None], sh)  # V, N\n        ratio = ratio.reshape((np.prod(sh)))\n        ratio = list(ratio)\n    elif isinstance(ratio, list):\n        ratio = np.asarray(ratio)  # avoid expansion\n\n    if isinstance(center_crop_size, list):\n        center_crop_size = np.asarray(center_crop_size)  # avoid expansion\n\n    # Should we batch these instead of loading?"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "    def to_easymocap(self):\n        batch = self.to_batch()\n        camera = to_numpy(batch)\n        return camera\n\n    def from_easymocap(self, camera: dict):\n        batch = to_tensor(camera)\n        self.from_batch(batch)\n        return self\n\n    def to_string(self) -> str:\n        batch = to_list(self.to_batch().meta)\n        return json.dumps(batch)\n\n    def from_string(self, string: str):\n        batch = to_tensor(dotdict(json.loads(string)), ignore_list=True)\n        self.from_batch(batch)\n\n    def from_batch(self, batch: dotdict):\n        H, W, K, R, T, n, f, t, v, bounds = batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds\n\n        # Batch (network input parameters)\n        self.H = int(H)\n        self.W = int(W)\n        self.K = mat3(*K.mT.ravel())\n        self.R = mat3(*R.mT.ravel())\n        self.T = vec3(*T.ravel())  # 3,\n        self.n = float(n)\n        self.f = float(f)\n        self.t = float(t)\n        self.v = float(v)\n        self.bounds = mat2x3(*bounds.ravel())  # 2, 3\n\n        if 'mass' in batch: self.mass = float(batch.mass)\n        if 'moment_of_inertia' in batch: self.moment_of_inertia = float(batch.moment_of_inertia)\n        if 'movement_force' in batch: self.movement_force = float(batch.movement_force)\n        if 'movement_torque' in batch: self.movement_torque = float(batch.movement_torque)\n        if 'movement_speed' in batch: self.movement_speed = float(batch.movement_speed)\n        if 'origin' in batch: self.origin = vec3(*batch.origin.ravel())  # 3,"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)\n            if serialized_agent:\n                serialized_agent[\"agent_id\"] = agent.id"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if len(self.agents) == 0:\n                return None, -np.inf\n\n            similarities = [cosine_similarity([agent.purpose_embedding], [purpose_embedding])[0][0] for agent in self.agents]\n            max_similarity = max(similarities)\n            max_similarity_index = similarities.index(max_similarity)\n            return self.agents[max_similarity_index], max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            return None, -np.inf"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper, prime=True)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        self.add_agent(prime_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Load an agent with a specified purpose from the database. If an agent with the given purpose is found, it is deserialized and returned; otherwise, None is returned.\n        \"\"\"\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads all agents from the database and returns a list of these agents if they are successfully loaded. Each agent is loaded based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent_lifecycle: The lifecycle manager for agents, used to manage the state and transitions of an agent throughout its lifecycle.\n        :param openai_wrapper: An interface or wrapper for OpenAI functionalities, used to interact with OpenAI services or models in the process of loading an agent.\n        :return: list. A list of agents that have been successfully loaded from the database.\n        \"\"\"\n        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = PROMPT_ENGINEERING_SYSTEM_PROMPT.format(\n                goal=goal,\n                examples=EXAMPLES,\n                sample_input=sample_input,\n                template=PROMPT_ENGINEERING_TEMPLATE\n            )\n            response = self.openai_wrapper.get_chat_completion(prompt)\n            return response.choices[0].message.content\n        except Exception as e:\n            logger.exception(f\"Error in generating prompt: {e}\")\n            return \"\""}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"INSERT OR REPLACE INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            else:\n                return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"SELECT result FROM cache WHERE hash = ?\",\n            (arg_hash,)\n        )\n        row = cursor.fetchone()\n        if row is None:\n            return None\n        result = json.loads(row[0])\n        return result\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update(args.__dict__)\n\n    # If quiet mode is enabled, redirect standard output to a file\n    if quiet_mode:\n        with open(os.path.join(CONFIG.record_dir, \"output.txt\"), \"w\") as f:\n            with redirect_stdout(f):\n                start_command_line(args.__dict__)\n    else:\n        start_command_line(args.__dict__)"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e.message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n                chatcompletion_kwargs.update(kwargs)\n                chatcompletion_kwargs.pop(\"schema_error_retry\", None)\n\n                response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n                response = json.loads(str(response))\n            else:\n                raise e\n\n        return response"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or time() - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n\n        return self._client"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "    if _is_dataloader_worker():\n        raise RuntimeError(\"`state_dict` cannot be called from a DataLoader worker process.\")\n\n    return {\n        \"num_samples_yielded\": num_samples_yielded,\n        \"num_workers\": num_workers,\n        \"batch_size\": batch_size,\n        \"current_epoch\": self.current_epoch,\n        \"input_dir_path\": self.input_dir.path,\n        \"input_dir_url\": self.input_dir.url,\n        \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n        \"drop_last\": self.drop_last,\n        \"seed\": self.seed,\n        \"world_size\": self.distributed_env.world_size,\n        \"shuffle\": self.shuffle,\n    }\n\n    def _validate_state_dict(self) -> None:\n        assert self._state_dict\n        assert self.worker_env\n        assert self.shuffler\n\n        state: Dict[str, Any] = self._state_dict\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                \"The provided `shuffle` state doesn't match the current one. \"\n                f\"Found `{self.shuffle}` instead of `{state['shuffle']}`.\"\n            )\n\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\n                \"The provided `num_workers` state doesn't match the current one. \"\n                f\"Found `{self.worker_env.world_size}` instead of `{state['num_workers']}`.\"\n            )\n\n        # Note: We need to check whether the path has been resolved to its associated cache.\n        # In this case, validate the cache folder is the same.\n        if _should_replace_path(state[\"input_dir_path\"]):\n            cache_path = _try_create_cache_dir(\n                input_dir=state[\"input_dir_path\"] if state[\"input_dir_path\"] else state[\"input_dir_url\"]\n            )\n            if cache_path != self.input_dir.path:\n                raise ValueError(\n                    \"The provided `input_dir` path state doesn't match the current one. \"\n                    f\"Found `{self.input_dir.path}` instead"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "    def _validate_state_dict(self) -> None:\n        assert self._state_dict\n        assert self.worker_env\n        assert self.cache\n\n        state: Dict[str, Any] = self._state_dict\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                \"The provided `shuffle` state doesn't match the current one. \"\n                f\"Found `{self.shuffle}` instead of `{state['shuffle']}`.\"\n            )\n\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\n                \"The provided `num_workers` state doesn't match the current one. \"\n                f\"Found `{self.worker_env.world_size}` instead of `{state['num_workers']}`.\"\n            )\n\n        # Note: We need to check whether the path has been resolved to its associated cache.\n        # In this case, validate the cache folder is the same.\n        if _should_replace_path(state[\"input_dir_path\"]):\n            cache_path = _try_create_cache_dir(\n                input_dir=state[\"input_dir_path\"] if state[\"input_dir_path\"] else state[\"input_dir_url\"]\n            )\n            if cache_path != self.input_dir.path:\n                raise ValueError(\n                    \"The provided `input_dir` path state doesn't match the current one. \"\n                    f\"Found `{self.input_dir.path}` instead of `{cache_path}`.\"\n                )\n        elif state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                \"The provided `input_dir` path state doesn't match the current one. \"\n                f\"Found `{self.input_dir.path}` instead of `{state['input_dir_path']}`.\"\n            )\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                \"The provided `input_dir` URL state doesn't match the current one. \"\n                f\"Found `{self.input_dir.url}` instead of `{state['input_dir_url']}`.\"\n            )\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(\n                \"The provided `seed` state doesn't match the current one. \"\n                f\""}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        state = self._state_dict\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"The state dictionary of the StreamingDataset instance has a different shuffle value ({state['shuffle']}) than the current shuffle value ({self.shuffle}).\"\n            )\n\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\n                f\"The state dictionary of the StreamingDataset instance has a different number of workers ({state['num_workers']}) than the current number of workers ({self.worker_env.world_size}).\"\n            )\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The state dictionary of the StreamingDataset instance has a different input directory path ({state['input_dir_path']}) than the current input directory path ({self.input_dir.path}).\"\n            )\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The state dictionary of the StreamingDataset instance has a different input directory URL ({state['input_dir_url']}) than the current input directory URL ({self.input_dir.url}).\"\n            )\n\n        if state[\"item_loader\"] != self.item_loader.state_dict():\n            raise ValueError(\n                f\"The state dictionary of the StreamingDataset instance has a different item loader state ({state['item_loader']}) than the current item loader state ({self.item_loader.state_dict()}).\"\n            )\n\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                f\"The state dictionary of the StreamingDataset instance has a different drop_last value ({state['drop_last']}) than the current drop_last value ({self.drop_last}).\"\n            )\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(\n                f\"The state dictionary of the StreamingDataset instance has a different seed value ({state['seed']}) than the current seed value ({self.seed}).\"\n            )\n\n        if state[\"world_size\"] != self.distributed_env.world_size:\n            raise ValueError(\n                f\"The state dictionary of the StreamingDataset instance has a different world size ({state['world_size']}) than the current world size ({self.distributed_env"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "def _try_create_cache_dir(input_dir: Optional[str]) -> Optional[str]:\n    \"\"\"\n    Attempts to create a cache directory based on the input directory provided. It generates a unique directory name by hashing the input directory. If certain environment variables are not set, it creates the cache directory in a default location; otherwise, it creates it in a specified location.\n\n    Input-Output Arguments\n    :param input_dir: Optional[str]. The directory path to be hashed and used for creating a unique cache directory. If None, an empty string is used for hashing.\n    :return: Optional[str]. The path of the created cache directory. Returns None if the directory cannot be created, although this behavior is not explicitly handled in the function.\n\n    \"\"\"\n    if input_dir is None or input_dir == \"\":\n        input_dir = \"\"\n\n    cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hashlib.sha256(input_dir.encode()).hexdigest())\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    return cache_dir\n/src/lit/api/types.py\n\"\"\"Types for the LIT API.\"\"\"\n\nfrom typing import Any, Dict, List, Optional, Union\n\nimport numpy as np\n\n# The type of a single example.\nExample = Dict[str, Any]\n\n# The type of a batch of examples.\nBatch = List[Example]\n\n# The type of a single prediction.\nPrediction = Dict[str, Any]\n\n# The type of a batch of predictions.\nPredictions = List[Prediction]\n\n# The type of a single output.\nOutput = Dict[str, Any]\n\n# The type of a batch of outputs.\nOutputs = List[Output]\n\n# The type of a single output.\nOutput = Dict[str, Any]\n\n# The type of a batch of outputs.\nOutputs = List[Output]\n\n# The type of a single output.\nOutput = Dict[str, Any]\n\n# The type of a batch of outputs.\nOutputs = List[Output]\n\n# The type of a single output.\nOutput = Dict[str, Any]\n\n# The type of a batch of outputs.\nOutputs"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"The provided remote_filepath doesn't use the 's3' scheme: {remote_filepath}\")\n\n        if not os.path.exists(local_filepath):\n            if self._s5cmd_available:\n                self._download_file_with_s5cmd(remote_filepath, local_filepath)\n            else:\n                self._download_file_with_boto3(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "def _associate_chunks_to_workers(\n    num_workers: int, worker_env: _WorkerEnv, chunks_replica: List[int], intervals_replica: List[Any]\n) -> Any:\n    \"\"\"\n    This function distributes chunks of data and their corresponding intervals across multiple workers based on the number of workers and a worker environment. It ensures that each worker is assigned a subset of chunks and intervals, following a distribution strategy that depends on the worker's index and the total world size defined in the worker environment.\n\n    Input-Output Arguments\n    :param num_workers: int, the total number of workers among which the chunks and intervals are to be distributed.\n    :param worker_env: _WorkerEnv, an instance or object representing the worker environment, which includes details like world size that are used in the distribution logic.\n    :param chunks_replica: List[int], a list of chunk indices that need to be distributed among the workers.\n    :param intervals_replica: List[Any], a list of intervals corresponding to each chunk in chunks_replica. Each interval represents the range or scope of the chunk it corresponds to.\n    :return: A tuple containing two dictionaries. The first dictionary maps worker indices to their assigned chunks, and the second dictionary maps worker indices to the intervals corresponding to their assigned chunks. There is no explicit data type mentioned for the return value, but it is implied to be a tuple of two dictionaries based on the function's implementation.\n    \"\"\"\n    chunks_per_replica = []\n    intervals_per_replica = []\n\n    for i in range(num_workers):\n        chunks_per_replica.append([])\n        intervals_per_replica.append([])\n\n    for i, (chunk_index, interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        if i % num_workers == worker_env.rank:\n            chunks_per_replica[worker_env.rank].append(chunk_index)\n            intervals_per_replica[worker_env.rank].append(interval)\n\n    return chunks_per_replica, intervals_per_replica\n  def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.save_hyperparameters()\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch,"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[6:]\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        return (\n            item.tobytes(),\n            f\"pil:{item.width}:{item.height}:{item.mode}\",\n        )\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            return item.tobytes(), None\n\n        if not isinstance(item, Image.Image):\n            raise TypeError(f\"Expected Image.Image or its subclasses, got {type(item)}\")\n\n        if item.mode != \"RGB\":\n            item = item.convert(\"RGB\")\n\n        with io.BytesIO() as f:\n            item.save(f, format=\"JPEG\")\n            return f.getvalue(), None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[0:12], np.uint32)\n        width, height, mode_size = ints\n        mode = data[12 : 12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size :]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the tensor bytes\n        tensor = torch.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return torch.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        \"\"\"\n        Serializes a PyTorch tensor into a bytes object containing the tensor's dtype, shape, and raw data. This serialized format can be useful for saving or transmitting tensor data in a compact binary form. The function also returns None as the second element of the tuple, which could be used for additional metadata in future extensions.\n\n        Input-Output Arguments\n        :param item: torch.Tensor, the tensor to be serialized. It is used to extract the dtype, shape, and raw data for serialization.\n        :return: A tuple containing a bytes object and None. The bytes object is the serialized representation of the input tensor, including its dtype, shape, and raw data. The None value indicates that there is no additional metadata associated with this serialization.\n        \"\"\"\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data, \"RGB\")\n            except RuntimeError:\n                pass\n\n        return PILSerializer.deserialize(data)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        array = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if array.shape == shape:\n            return array\n        return np.reshape(array, shape)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and type(item) == np.ndarray and len(item.shape) == 1\n\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "    def __next__(self) -> Any:\n        if isinstance(self.dataset, StreamingDataset):\n            assert self.batch_size\n            self._num_samples_yielded_streaming += self.batch_size\n            return super().__next__()\n        else:\n            self.dataset._set_use_streaming_dataloader(True)\n            assert self.batch_size\n            self._num_samples_yielded_combined[self._latest_worker_idx] = [\n                sample[-1].item() if self.batch_size > 1 else sample.item()\n                for sample in super().__next__()\n            ]\n            return self._num_samples_yielded_combined[self._latest_worker_idx]\n\n    def _get_iterator(self) -> \"_BaseDataLoaderIter\":\n        \"\"\"Overriden to ensure the `Cache.done()` method is triggered on iteration done.\"\"\"\n        if self.num_workers == 0:\n            return _SingleProcessDataLoaderIter(self)\n        self.check_worker_number_rationality()\n        return _StreamingMultiProcessingDataLoaderIter(self)\n\n    def _get_state(self) -> Dict[str, Any]:\n        \"\"\"Overriden to ensure the `Cache.done()` method is triggered on iteration done.\"\"\"\n        if isinstance(self.dataset, StreamingDataset):\n            assert self.batch_size\n            self._num_samples_yielded_streaming += self.batch_size\n            return super()._get_state()\n        else:\n            self.dataset._set_use_streaming_dataloader(True)\n            assert self.batch_size\n            self._num_samples_yielded_combined[self._latest_worker_idx] = [\n                sample[-1].item() if self.batch_size > 1 else sample.item()\n                for sample in super()._get_state()\n            ]\n            return self._num_samples_yielded_combined[self._latest_worker_idx]\n\n    def _get_state(self) -> Dict[str, Any]:\n        \"\"\"Overriden to ensure the `Cache.done()` method is triggered on iteration done.\"\"\"\n        if isinstance(self.dataset, StreamingDataset):\n            assert self.batch_size\n            self._num_samples_yielded_streaming += self.batch_size\n            return super()._get_state()\n        else:\n            self.dataset._set"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\"torchvision is required to deserialize video data.\")\n        if not _AV_AVAILABLE:\n            raise ImportError(\"av is required to deserialize video data.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.flush()\n            return torchvision.io.read_video(f.name)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self.filled:\n            return []\n\n        if self._should_write():\n            self.write_chunk()\n\n        if self._should_write():\n            return []\n\n        self.write_chunks_index()\n        self._is_done = True\n        return [os.path.join(self._cache_dir, f) for f in os.listdir(self._cache_dir) if f.endswith(\".bin\")]\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            assert self.batch_size\n            self.dataset.load_state_dict(\n                obj[\"dataset\"],\n                obj[\"num_samples_yielded\"],\n                self.num_workers,\n                self.batch_size,\n                self.current_epoch,\n            )\n        else:\n            self.dataset._load_state_dict(\n                obj[\"dataset\"],\n                self.num_workers,\n                self.batch_size,\n                obj[\"num_samples_yielded\"],\n                self.current_epoch,\n            )\n\n        self.current_epoch = obj[\"current_epoch\"]\n        self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n        self.restore = True"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is not None:\n            return self._iterator.state_dict(num_workers, batch_size)\n\n        return _state_dict(self._datasets, num_samples_yielded, num_workers, batch_size)\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if self._iterator is None:\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n            self._iterator = _CombinedDatasetIterator(\n                self._datasets,\n                self._seed,\n                self._weights,\n                self._use_streaming_dataloader,\n                self._num_samples_yielded,\n            )\n        self._iterator.load_state_dict(state_dict)\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return _resolve_lightning(dir_path)\n\n    if dir_path.startswith(\"lightning-studio://\"):\n        return _resolve_studio(dir_path, target_name=None, target_id=None)\n\n    if dir_path.startswith(\"lightning-studio-name://\"):\n        return _resolve_studio(dir_path, target_name=dir_path.split(\"/\")[3], target_id=None)\n\n    if dir_path.startswith(\"lightning-studio-id://\"):\n        return _resolve_studio(dir_path, target_name=None, target_id=dir_path.split(\"/\")[3])\n\n    if dir_path.startswith(\"lightning-s3-connection://\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"lightning-datasets://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"lightning-cloud://\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"lightning-cloud-name://\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"lightning-cloud-id://\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"lightning-cloud-space://\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"lightning-cloud-space-name://\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"lightning-cloud-space-id://\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"lightning-cloud-space-url://\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"lightning-cloud-space-url-name://\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"lightning-cloud-space-url-id://\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"lightning-cloud-"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    # No files are found in this folder\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    # Check the index file exists\n    try:\n        s3.head_object(Bucket=obj.netloc, Key=os.path.join(prefix, \"index.json\"))\n        has_index_file = True\n    except botocore.exceptions.ClientError:\n        has_index_file = False\n\n    if has_index_file:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an optimized immutable datasets.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    if not overwrite and not append:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data. \"\n            \"HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    if append:\n        raise NotImplementedError(\"Appending data to an existing directory is not yet supported.\")\n\n    if overwrite:\n        raise NotImplementedError(\"Overwriting data in an existing directory is not yet supported.\")\n\n    bucket_name = obj.netloc\n    s3 = boto3.resource(\"s3\")\n    for obj in s3.Bucket(bucket_name).objects.filter(Prefix=prefix):\n        s3.Object(bucket_name, obj.key).delete()"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # We aren't alloweing to add more data\n    # TODO: Add support for `append` and `overwrite`.\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": " = node_rank is None or node_rank == 0\n        if not node_rank:\n            node_rank = self.rank\n\n        if node_rank == 0:\n            while True:\n                files = os.listdir(self._cache_dir)\n                index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n                if len(index_files) == num_workers:\n                    break\n                sleep(1)\n            self._merge_no_wait(node_rank)\n        else:\n            while True:\n                files = os.listdir(self._cache_dir)\n                index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n                if len(index_files) == num_workers:\n                    break\n                sleep(1)\n            while True:\n                files = os.listdir(self._cache_dir)\n                index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n                if len(index_files) == num_workers:\n                    break\n                sleep(1)\n            self._merge_no_wait(node_rank)"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ImportError(\n            \"The `lightning_sdk` package is required to use this function. Please install it with `pip install lightning-sdk`.\"\n        )\n\n    if not _BOTO3_AVAILABLE:\n        raise ImportError(\n            \"The `boto3` package is required to use this function. Please install it with `pip install boto3`.\"\n        )\n\n    if machine is None:\n        machine = Machine(cpu=1, gpu=0, memory=1)\n\n    if command is None:\n        command = f\"cd {os.getcwd()} && {sys.executable} -m lightning_cloud.operator.execute\"\n\n    studio = Studio()\n\n    job = studio.create_job(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n    )\n\n    job_url = f\"{_get_lightning_cloud_url()}/projects/{studio.project_id}/jobs/{job.id}\"\n\n    print(f\"Job URL: {job_url}\")\n\n    while True:\n        job = studio.get_job(job.id)\n        if job.status == \"RUNNING\":\n            break\n        sleep(1)\n\n    print(f\"Job started. Waiting for it to finish...\")\n\n    while True:\n        job = studio.get_job(job.id)\n        if job.status == \"SUCCEEDED\":\n            print(f\"Job finished successfully.\")\n            break\n        elif job.status == \"FAILED\":\n            raise RuntimeError(f\"Job failed with error: {job.error}\")\n        sleep(1)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        try:\n            self._config = ChunksConfig(\n                self._cache_dir,\n                self._serializers,\n                self._remote_input_dir,\n                self._compression,\n                self._item_loader,\n            )\n            return self._config\n        except FileNotFoundError:\n            return None\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration should be defined before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index should be an instance of ChunkedIndex.\")\n\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index\n        chunk_index = index.chunk_index\n        if chunk_index is None:\n            chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Check if the chunk is already downloaded\n        if chunk_index != self._last_chunk_index:\n            # Download the chunk if needed\n            if self._prepare_thread is None:\n                self._prepare_thread = PrepareChunksThread(\n                    self._config,\n                    self._item_loader,\n                    self._distributed_env,\n                    max_cache_size=self._max_cache_size,\n                )\n                self._prepare_thread.start()\n\n            # Download the chunk\n            self._prepare_thread.download([chunk_index])\n            self._last_chunk_index = chunk_index\n\n        # Read the item\n        return self._item_loader.read(index)\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n\n    return _ImmutableDistributedMap().set_and_get(key, obj)\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Get the number of chunks per rank\n    num_chunks_per_rank = [len(chunks) for chunks in chunks_per_ranks]\n\n    # 2. Get the number of chunks per node\n    num_chunks_per_node = np.array_split(\n        np.arange(sum(num_chunks_per_rank)), distributed_env.world_size\n    )\n\n    # 3. Get the number of chunks per node per rank\n    num_chunks_per_node_per_rank = [\n        np.array_split(num_chunks_per_node[node], distributed_env.world_size)\n        for node in range(distributed_env.world_size)\n    ]\n\n    # 4. Get the number of chunks per rank per node\n    num_chunks_per_rank_per_node = [\n        [\n            num_chunks_per_node_per_rank[node][rank]\n            for node in range(distributed_env.world_size)\n        ]\n        for rank in range(distributed_env.world_size)\n    ]\n\n    # 5. Get the number of chunks per rank per node per chunk\n    num_chunks_per_rank_per_node_per_chunk = [\n        [\n            np.array_split(num_chunks_per_rank_per_node[rank][node], num_chunks_per_rank[rank])\n            for node in range(distributed_env.world_size)\n        ]\n        for rank in range(distributed_env.world_size)\n    ]\n\n    # 6. Get the number of chunks per rank per node per chunk per rank\n    num_chunks_per_rank_per_node_per_chunk_per_rank = [\n        [\n            [\n                np.array_split(\n                    num_chunks_per_rank_per_node_per_chunk[rank][node][chunk],\n                    distributed_env.world_size,\n                )\n                for chunk in range(num_chunks_per_rank[rank])\n            ]\n            for node in range(distributed_env.world_size)\n        ]\n        for rank in range(distributed_env.world_size)\n    ]\n\n    # 7. Get the number of chunks per rank per node per chunk per rank per chunk\n    num_chunks_per_rank_per"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if not _IS_IN_STUDIO:\n        print(\"This method is optimized to run on https://lightning.ai/. Don't use it otherwise.\")\n\n    if isinstance(inputs, StreamingDataLoader):\n        raise ValueError(\"Streaming dataloader isn't supported.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    _output_dir: Dir = _resolve_dir(output_dir)\n\n    if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n        raise ValueError(\n            f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n            \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n        )\n\n    _assert_dir_has_index_file(_output_dir)\n\n    input_dir = _resolve_dir(_get_input_dir(inputs))\n\n    data_processor = DataProcessor(\n        input_dir=input_dir,\n        output_dir=_output_dir,\n        num_workers=num_workers or _get_default_num_workers(),\n        fast_dev_run=fast_dev_run,\n        num_downloaders=num_downloaders,\n        num_uploaders=num_uploaders,\n        reorder_files=reorder_files,\n        reader=reader,\n    )\n\n    with optimize_dns_context(True):\n        return data_processor.run(LambdaDataTransformRecipe(fn, inputs))\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if not _IS_IN_STUDIO:\n        yield\n        return\n\n    with open(\"/etc/resolv.conf\") as f:\n        lines = f.readlines()\n\n    if (enable and any(\"127.0.0.53\" in line for line in lines)) or (\n        not enable and any(\"127.0.0.1\" in line for line in lines)\n    ):\n        cmd = (\n            f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n            f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns({enable})'\"\n        )\n        Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501\n\n    try:\n        yield\n    finally:\n        if not _IS_IN_STUDIO:\n            return\n\n        with open(\"/etc/resolv.conf\") as f:\n            lines = f.readlines()\n\n        write_lines = []\n        for line in lines:\n            if \"nameserver 127\" in line:\n                if enable:\n                    write_lines.append(\"nameserver 127.0.0.1\\n\")\n                else:\n                    write_lines.append(\"nameserver 127.0.0.53\\n\")\n            else:\n                write_lines.append(line)\n\n        with open(\"/etc/resolv.conf\", \"w\") as f:\n            for line in write_lines:\n                f.write(line)"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items each rank should process\n    num_items_per_rank = (\n        len(indexes) // distributed_env.world_size\n        if drop_last\n        else (len(indexes) + distributed_env.world_size - 1) // distributed_env.world_size\n    )\n\n    # assign chunks and their intervals to each rank\n    chunks_per_ranks: List[List[int]] = [[] for _ in range(distributed_env.world_size)]\n    intervals_per_ranks: List[List[Any]] = [[] for _ in range(distributed_env.world_size)]\n    for i in range(len(indexes)):\n        rank = i % distributed_env.world_size\n        chunks_per_ranks[rank].append(indexes[i])\n        intervals_per_ranks[rank].append(chunk_intervals[i])\n\n    return chunks_per_ranks, intervals_per_ranks\n\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "e_dir: str, is_last: bool) -> None:\n        \"\"\"\n        The function 'prepare_item' prepares an item by applying a transformation function ('_fn') to the item's metadata, potentially including additional context like device information and a flag indicating if it's the last item. This is used within a data transformation recipe context, where items are processed and transformed according to a specified lambda function or callable.\n\n        Input-Output Arguments\n        :param item_metadata: Any, the metadata of the item to be transformed. It is used as the first argument in the transformation function.\n        :param output_dir: str, the directory where the transformed item's output should be stored. It is passed as the second argument to the transformation function.\n        :param is_last: bool, a flag indicating whether the item is the last one in the sequence to be processed. It is conditionally added to the keyword arguments passed to the transformation function if '_contains_is_last' is True.\n        :return: No return values. The function directly calls the transformation function ('_fn') with the provided arguments and keyword arguments, affecting external state or outputs rather than returning a value.\n        \"\"\"\n        if self._contains_device:\n            if self._device is None:\n                self._find_device()\n\n            if self._device is not None:\n                kwargs = {\"device\": self._device}\n            else:\n                kwargs = {}\n        else:\n            kwargs = {}\n\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)\n\n    def _find_device(self) -> None:\n        global_rank = os.getenv(\"DATA_OPTIMIZER_GLOBAL_RANK\", None)\n        if torch.cuda.is_available() and global_rank:\n            num_gpus = torch.cuda.device_count()\n            device = int(global_rank) % num_gpus\n            self._device = f\"cuda:{device}\"\n\n\nclass LambdaDataChunkRecipe(DataChunkRecipe):\n    def __init__(\n        self,\n        fn: Callable[[Any], None],\n        inputs: Sequence[Any],\n        chunk_size: Optional[int],\n        chunk_bytes: Optional[Union[int, str]],\n        compression: Optional[str],\n    ):"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": ":\n    \"\"\"The `StreamingDataLoader` provides a way to stream data from a remote cloud directory.\"\"\"\n\n    def __init__(self, remote_dir: str, num_workers: int = 1, random_seed: int = 42):\n        \"\"\"The `StreamingDataLoader` provides a way to stream data from a remote cloud directory.\n\n        Arguments:\n            remote_dir: The remote cloud directory to stream the data from.\n            num_workers: The number of workers to use.\n            random_seed: The random seed to be set before shuffling the data.\n\n        \"\"\"\n        self.remote_dir = remote_dir\n        self.num_workers = num_workers\n        self.random_seed = random_seed\n        self.workers: List[StreamingDataLoaderWorker] = []\n        self.workers_tracker: Dict[int, int] = {}\n        self.progress_queue: Optional[Queue] = None\n        self.error_queue: Queue = Queue()\n        self.stop_queues: List[Queue] = []\n        self.items: List[Any] = []\n        self.items_tracker: Dict[int, int] = {}\n        self.items_sizes: Dict[int, int] = {}\n        self.items_sizes_tracker: Dict[int, int] = {}\n        self.items_sizes_tracker_lock = Lock()\n        self.items_sizes_lock = Lock()\n        self.items_tracker_lock = Lock()\n        self.items_lock = Lock()\n        self.items_sizes_lock = Lock()\n        self.items_sizes_tracker_lock = Lock()\n        self.items_tracker_lock = Lock()\n        self.items_lock = Lock()\n        self.items_sizes_lock = Lock()\n        self.items_sizes_tracker_lock = Lock()\n        self.items_tracker_lock = Lock()\n        self.items_lock = Lock()\n        self.items_sizes_lock = Lock()\n        self.items_sizes_tracker_lock = Lock()\n        self.items_tracker_lock = Lock()\n        self.items_lock = Lock()\n        self.items_sizes_lock = Lock()\n        self.items_sizes_tracker_lock = Lock()\n        self.items_tracker_lock = Lock()\n        self.items_lock = Lock()\n        self.items_sizes"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if error_when_not_empty:\n            _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "_dir()\n\n        # Cleanup the cache data folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_data_dir):\n            shutil.rmtree(cache_data_dir, ignore_errors=True)\n\n        os.makedirs(cache_data_dir, exist_ok=True)\n\n    def _create_process_workers(self, data_recipe: DataRecipe, workers_user_items: List[List[Any]]) -> None:\n        self.progress_queue = Queue()\n        workers: List[DataWorkerProcess] = []\n        stop_queues: List[Queue] = []\n        for worker_idx, worker_user_items in enumerate(workers_user_items):\n            stop_queues.append(Queue())\n            worker = DataWorkerProcess(\n                worker_idx,\n                self.num_workers,\n                _get_node_rank(),\n                data_recipe,\n                self.input_dir,\n                self.output_dir,\n                worker_user_items,\n                self.progress_queue,\n                self.error_queue,\n                stop_queues[-1],\n                self.num_downloaders,\n                self.num_uploaders,\n                self.delete_cached_files,\n                self.reader,\n            )\n            worker.start()\n            workers.append(worker)\n\n        # Note: Don't store within the loop as weakref aren't serializable\n        self.workers = workers\n        self.stop_queues = stop_queues\n\n    def _signal_handler(self, signal: Any, frame: Any) -> None:\n        \"\"\"On temrination, we stop all the processes to avoid leaking RAM.\"\"\"\n        for stop_queue in self.stop_queues:\n            stop_queue.put(None)\n        for w in self.workers:\n            w.join(0)\n        os._exit(0)\n\n    def _cleanup_cache(self) -> None:\n        cache_dir = _get_cache_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)\n\n        cache_data_dir = _get_cache_data_dir()\n\n        # Cleanup the"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "_cache_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)\n\n        cache_data_dir = _get_cache_data_dir()\n\n        # Cleanup the cache data folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_data_dir):\n            shutil.rmtree(cache_data_dir, ignore_errors=True)\n\n        os.makedirs(cache_data_dir, exist_ok=True)\n\n\ndef _get_fast_dev_run() -> Optional[Union[bool, int]]:\n    \"\"\"Whether to run a fast dev run.\"\"\"\n    return _get_env_var(\"fast_dev_run\")\n\n\ndef _get_num_nodes() -> int:\n    \"\"\"The number of nodes.\"\"\"\n    return _get_env_var(\"num_nodes\")\n\n\ndef _get_node_rank() -> int:\n    \"\"\"The node rank.\"\"\"\n    return _get_env_var(\"node_rank\")\n\n\ndef _get_cache_dir() -> str:\n    \"\"\"The cache dir.\"\"\"\n    return _get_env_var(\"cache_dir\")\n\n\ndef _get_cache_data_dir() -> str:\n    \"\"\"The cache data dir.\"\"\"\n    return _get_env_var(\"cache_data_dir\")\n\n\ndef _resolve_dir(dir: Optional[Union[str, Dir]]) -> Dir:\n    \"\"\"Resolve the dir.\"\"\"\n    if dir is None:\n        return Dir(path=None)\n    if isinstance(dir, str):\n        return Dir(path=dir)\n    return dir\n\n\ndef _map_items_to_workers_sequentially(num_workers: int, user_items: List[Any]) -> List[List[Any]]:\n    \"\"\"Map the items to workers sequentially.\"\"\"\n    workers_user_items: List[List[Any]] = [[] for _ in range(num_workers)]\n    for user_item in user_items:\n        workers_user_items[0].append(user_item)\n        workers_user_items = workers_user_items[1"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": ":\n        self.input_dir = input_dir\n        self.output_dir = output_dir\n        self.num_workers = num_workers\n        self.num_downloaders = num_downloaders\n        self.num_uploaders = num_uploaders\n        self.delete_cached_files = delete_cached_files\n        self.fast_dev_run = fast_dev_run\n        self.random_seed = random_seed\n        self.reorder_files = reorder_files\n        self.weights = weights\n        self.reader = reader\n        self.workers: Any = []\n        self.workers_tracker: Dict[int, int] = {}\n        self.progress_queue: Optional[Queue] = None\n        self.error_queue: Queue = Queue()\n        self.stop_queues: List[Queue] = []\n        self.src_resolver = src_resolver\n\n    def run(self, data_recipe: DataRecipe) -> None:\n        \"\"\"The `DataProcessor.run(...)` method triggers the data recipe processing over your dataset.\"\"\"\n        if not isinstance(data_recipe, DataRecipe):\n            raise ValueError(\"The provided value should be a data recipe.\")\n\n        t0 = time()\n        print(f\"Setup started with fast_dev_run={self.fast_dev_run}.\")\n\n        # Force random seed to be fixed\n        random.seed(self.random_seed)\n        np.random.seed(self.random_seed)\n        torch.manual_seed(self.random_seed)\n\n        # Call the setup method of the user\n        user_items: List[Any] = data_recipe.prepare_structure(self.input_dir.path if self.input_dir else None)\n\n        if not isinstance(user_items, (list, StreamingDataLoader)):\n            raise ValueError(\"The `prepare_structure` should return a list of item metadata.\")\n\n        if isinstance(user_items, StreamingDataLoader):\n            self.reader = StreamingDataLoaderReader(user_items)\n\n        if self.reader:\n            user_items = self.reader.remap_items(user_items, self.num_workers)\n\n        if self.weights is not None:\n            if len(self.weights) != len(user_items):\n                raise ValueError(\"The provided weights length should match the inputs' length.\")\n            workers_user_items = _map_items_"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": ":\n        self.input_dir = input_dir\n        self.output_dir = output_dir\n        self.num_workers = num_workers\n        self.num_downloaders = num_downloaders\n        self.num_uploaders = num_uploaders\n        self.delete_cached_files = delete_cached_files\n        self.fast_dev_run = fast_dev_run\n        self.random_seed = random_seed\n        self.reorder_files = reorder_files\n        self.weights = weights\n        self.reader = reader\n        self.workers: Any = []\n        self.workers_tracker: Dict[int, int] = {}\n        self.progress_queue: Optional[Queue] = None\n        self.error_queue: Queue = Queue()\n        self.stop_queues: List[Queue] = []\n        self.src_resolver = src_resolver\n\n    def run(self, data_recipe: DataRecipe) -> None:\n        \"\"\"The `DataProcessor.run(...)` method triggers the data recipe processing over your dataset.\"\"\"\n        if not isinstance(data_recipe, DataRecipe):\n            raise ValueError(\"The provided value should be a data recipe.\")\n\n        t0 = time()\n        print(f\"Setup started with fast_dev_run={self.fast_dev_run}.\")\n\n        # Force random seed to be fixed\n        random.seed(self.random_seed)\n        np.random.seed(self.random_seed)\n        torch.manual_seed(self.random_seed)\n\n        # Call the setup method of the user\n        user_items: List[Any] = data_recipe.prepare_structure(self.input_dir.path if self.input_dir else None)\n\n        if not isinstance(user_items, (list, StreamingDataLoader)):\n            raise ValueError(\"The `prepare_structure` should return a list of item metadata.\")\n\n        if isinstance(user_items, StreamingDataLoader):\n            self.reader = StreamingDataLoaderReader(user_items)\n\n        if self.reader:\n            user_items = self.reader.remap_items(user_items, self.num_workers)\n\n        if self.weights is not None:\n            if len(self.weights) != len(user_items):\n                raise ValueError(\"The provided weights length should match the inputs' length.\")\n            workers_user_items = _map_items_"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        cache_dir = _get_cache_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": ":\n        self.input_dir = input_dir\n        self.output_dir = output_dir\n        self.num_workers = num_workers\n        self.num_downloaders = num_downloaders\n        self.num_uploaders = num_uploaders\n        self.delete_cached_files = delete_cached_files\n        self.fast_dev_run = fast_dev_run\n        self.random_seed = random_seed\n        self.reorder_files = reorder_files\n        self.weights = weights\n        self.reader = reader\n        self.workers: Any = []\n        self.workers_tracker: Dict[int, int] = {}\n        self.progress_queue: Optional[Queue] = None\n        self.error_queue: Queue = Queue()\n        self.stop_queues: List[Queue] = []\n\n    def run(self, data_recipe: DataRecipe) -> None:\n        \"\"\"The `DataProcessor.run(...)` method triggers the data recipe processing over your dataset.\"\"\"\n        if not isinstance(data_recipe, DataRecipe):\n            raise ValueError(\"The provided value should be a data recipe.\")\n\n        t0 = time()\n        print(f\"Setup started with fast_dev_run={self.fast_dev_run}.\")\n\n        # Force random seed to be fixed\n        random.seed(self.random_seed)\n        np.random.seed(self.random_seed)\n        torch.manual_seed(self.random_seed)\n\n        # Call the setup method of the user\n        user_items: List[Any] = data_recipe.prepare_structure(self.input_dir.path if self.input_dir else None)\n\n        if not isinstance(user_items, (list, StreamingDataLoader)):\n            raise ValueError(\"The `prepare_structure` should return a list of item metadata.\")\n\n        if isinstance(user_items, StreamingDataLoader):\n            self.reader = StreamingDataLoaderReader(user_items)\n\n        if self.reader:\n            user_items = self.reader.remap_items(user_items, self.num_workers)\n\n        if self.weights is not None:\n            if len(self.weights) != len(user_items):\n                raise ValueError(\"The provided weights length should match the inputs' length.\")\n            workers_user_items = _map_items_to_workers_weighted(\n                num_workers"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "_run_to_be_there.\n        if os.path.exists(cache_data_dir):\n            shutil.rmtree(cache_data_dir, ignore_errors=True)\n\n        os.makedirs(cache_data_dir, exist_ok=True)\n\n    def _create_process_workers(self, data_recipe: DataRecipe, workers_user_items: List[List[Any]]) -> None:\n        self.progress_queue = Queue()\n        workers: List[DataWorkerProcess] = []\n        stop_queues: List[Queue] = []\n        for worker_idx, worker_user_items in enumerate(workers_user_items):\n            stop_queues.append(Queue())\n            worker = DataWorkerProcess(\n                worker_idx,\n                self.num_workers,\n                _get_node_rank(),\n                data_recipe,\n                self.input_dir,\n                self.output_dir,\n                worker_user_items,\n                self.progress_queue,\n                self.error_queue,\n                stop_queues[-1],\n                self.num_downloaders,\n                self.num_uploaders,\n                self.delete_cached_files,\n                self.reader,\n            )\n            worker.start()\n            workers.append(worker)\n\n        # Note: Don't store within the loop as weakref aren't serializable\n        self.workers = workers\n        self.stop_queues = stop_queues\n\n    def _signal_handler(self, signal: Any, frame: Any) -> None:\n        \"\"\"On temrination, we stop all the processes to avoid leaking RAM.\"\"\"\n        for stop_queue in self.stop_queues:\n            stop_queue.put(None)\n        for w in self.workers:\n            w.join(0)\n        os._exit(0)\n\n    def _cleanup_cache(self) -> None:\n        cache_dir = _get_cache_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)\n\n        cache_data_dir = _get_cache_data_dir()\n\n        # Cleanup the cache data folder to avoid corrupted files from previous run to be"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"n_layers must be greater than 0\"\n        assert n_neurons > 0, \"n_neurons must be greater than 0\"\n\n        if self.tcnn:\n            if n_neurons < 1000:\n                return self._get_tcnn_network(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    n_layers=n_layers,\n                    n_neurons=n_neurons,\n                    activation=activation,\n                    output_activation=output_activation,\n                )\n            else:\n                return self._get_tcnn_network_large(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    n_layers=n_layers,\n                    n_neurons=n_neurons,\n                    activation=activation,\n                    output_activation=output_activation,\n                )\n        else:\n            return self._get_torch_network(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                n_layers=n_layers,\n                n_neurons=n_neurons,\n                activation=activation,\n                output_activation=output_activation,\n            )\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Create a shifted version of the signal by shifting it by the kernel offset in both directions\n        shifted_signal = np.concatenate(\n            (signal[-kernel_offset:], signal, signal[:kernel_offset])\n        )\n\n        # Calculate the rolling median by taking the median of the shifted signals\n        rolling_median = np.median(\n            np.stack(\n                [\n                    shifted_signal[i : i + 2 * kernel_offset + 1]\n                    for i in range(len(shifted_signal) - 2 * kernel_offset)\n                ]\n            ),\n            axis=1,\n        )\n\n        # Trim the rolling median to remove edge effects introduced by the shifting process\n        return rolling_median[kernel_offset:-kernel_offset]"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Convert rotation shift to columns\n    rotation_shift_columns = rotation_shift * template_probe.width\n\n    # Get the iris codes from the probe and gallery templates\n    iris_probe = template_probe.iris_code\n    iris_gallery = template_gallery.iris_code\n\n    # Get the mask codes from the probe and gallery templates\n    mask_probe = template_probe.mask_code\n    mask_gallery = template_gallery.mask_code\n\n    # Get the half width of the iris codes\n    half_width = [iris_probe.shape[1] // 2, iris_gallery.shape[1] // 2]\n\n    # Get the total size of the iris codes\n    total_codesize = iris_probe.shape[1] * 2\n\n    # Calculate the square root of the total bit count\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        total_codesize, half_width, weights\n    )\n\n    # Initialize the minimum Hamming distance and corresponding rotation shift\n    min_hd = np.inf\n    min_rotation_shift = 0\n\n    # Loop over all possible rotation shifts\n    for i in range(rotation_shift_columns + 1):\n        # Shift the iris codes and mask codes by the current rotation shift\n        iris_probe_shifted = np.roll(iris_probe, i, axis=1)\n        mask_probe_shifted = np.roll(mask_probe, i, axis=1)\n        iris_gallery_shifted = np.roll(iris_gallery, -i, axis=1)\n        mask_gallery_shifted = np.roll(mask_gallery, -i, axis=1)\n\n        # Calculate the nonmatch bits for the top and bottom iris codes\n        irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n            [iris_probe_shifted, iris_gallery_shifted],\n            [mask_probe_shifted, mask_gallery_shifted],\n            half_width,\n            weights,\n        )\n\n        # Calculate the normalized Hamming distance for the top and bottom iris codes\n        norm_HD_top = normalized"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        first_bisectors_point = np.zeros((num_bisectors, 2))\n        second_bisectors_point = np.zeros((num_bisectors, 2))\n\n        for i in range(num_bisectors):\n            # Select two random points from the polygon\n            point_1 = polygon[np.random.randint(0, polygon.shape[0])]\n            point_2 = polygon[np.random.randint(0, polygon.shape[0])]\n\n            # Calculate the distance between the two points\n            distance = np.linalg.norm(point_1 - point_2)\n\n            # Check if the distance is greater than the minimum distance\n            if distance > min_distance_between_sector_points_in_px:\n                # Calculate the midpoint between the two points\n                midpoint = (point_1 + point_2) / 2\n\n                # Calculate the perpendicular bisector by rotating the vector from point_1 to point_2 by 90 degrees\n                perpendicular_vector = np.array([point_2[1] - point_1[1], point_1[0] - point_2[0]])\n                perpendicular_vector /= np.linalg.norm(perpendicular_vector)\n\n                # Calculate the endpoints of the perpendicular bisector\n                first_bisectors_point[i] = midpoint + perpendicular_vector * min_distance_between_sector_points_in_px / 2\n                second_bisectors_point[i] = midpoint - perpendicular_vector * min_distance_between_sector_points_in_px / 2\n            else:\n                # If the distance is not greater than the minimum distance, try again\n                i -= 1\n\n            # Check if the maximum number of iterations has been reached\n            if i == max_iterations - 1:\n                raise EyeCentersEstimationError(\n                    f\"Failed to find a sufficient number of point pairs that meet the distance criterion within the maximum number of iterations allowed. This indicates that it may not be possible to accurately estimate the center of the shape.\"\n                )\n\n        return first_bisectors_point, second_"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(self, *args, **kwargs)\n\n        return result\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": ".is_dataclass_instance(target_type):\n            return target_type(**data)\n\n        # Handle other dictionary-like types\n        if self._is_dict_like(target_type) or (isinstance(origin, type) and issubclass(origin, dict)):\n            key_type, value_type = get_args(target_type) if get_args(target_type) else (Any, Any)\n\n            instantiated_dict = {}\n\n            for k, v in data.items():\n                instantiated_dict[self.instantiate(k, key_type)] = self.instantiate(v, value_type)\n\n            # If target_type is a subclass of dict, return an instance of target_type\n            if self._is_subclass_of_generic(target_type, dict) and not self._is_generic(target_type):\n                return target_type(instantiated_dict)\n\n            return instantiated_dict\n\n        # Handle arbitrary class types\n        if inspect.isclass(target_type) and not self.is_base_type(target_type):\n            # Ensure the data is a dictionary\n            if not isinstance(data, dict):\n                raise TypeError(f\"Failed to instantiate {target_type} from provided data.\")\n\n            # Gather type hints from the class and its bases\n            type_hints = {}\n            for cls in reversed(target_type.__mro__):\n                type_hints.update(get_type_hints(cls))\n\n            # Validate each attribute of the class\n            for attr, attr_type in type_hints.items():\n                if attr not in data:\n                    raise TypeError(f\"Failed to instantiate {target_type} from provided data. Missing attribute {attr}.\")\n\n                data[attr] = self.instantiate(data[attr], attr_type)\n\n            # Instantiate the class with the validated data\n            return target_type(**data)\n\n        # Handle arbitrary types\n        return target_type(data)\n\n    def check_type(self, data: Any, target_type: Type) -> bool:\n        \"\"\"\n        Attempts to check if the provided data is of the specified type.\n\n        Args:\n            data: The data to check the type of.\n            target_type: The type to check the data against.\n\n        Returns:\n            A boolean indicating whether the data is of the specified type.\n        \"\"\"\n        # Handle None type\n       "}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = func_object.__doc__\n        source = get_source(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n\n        for param in signature.parameters.values():\n            if param.name in type_hints:\n                input_type_hints[param.name] = type_hints[param.name]\n\n        for param in signature.parameters.values():\n            if param.name in type_hints:\n                output_type_hints[param.name] = type_hints[param.name]\n\n        input_class_definitions = {}\n        output_class_definitions = {}\n\n        for key, value in input_type_hints.items():\n            input_class_definitions[key] = get_class_definition(value)\n\n        for key, value in output_type_hints.items():\n            output_class_definitions[key] = get_class_definition(value)\n\n        if issubclass(output_type_hints[list(output_type_hints.keys())[0]], Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        elif issubclass(output_type_hints[list(output_type_hints.keys())[0]], Union):\n            function_type = FunctionType.EMBEDDABLE\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hints=input_type_hints,\n            output_type_hints=output_type_hints,\n            input_class_definitions=input_class_definitions,\n            output_class_definitions=output_class_definitions,\n            type=function_type,\n            source=source\n        )\n\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        if len(bit_array) != self.size:\n            logging.warning(\"Bit array length does not match expected length. Reinitializing bit array.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "    def lookup_indices(self, string):\n        hash1, hash2 = self.hash_functions(string)\n        indices = []\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            indices.append(index)\n        return indices\n\n    def lookup_bit_array(self, string):\n        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n\n    def lookup_indices_bit_array(self, string):\n        hash1, hash2 = self.hash_functions(string)\n        indices = []\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            indices.append(index)\n            if self.bit_array[index] == 0:\n                return False, indices\n        return True, indices\n\n    def lookup_indices_bit_array_indices(self, string):\n        hash1, hash2 = self.hash_functions(string)\n        indices = []\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            indices.append(index)\n            if self.indices[index] == 0:\n                return False, indices\n        return True, indices\n\n    def lookup_indices_bit_array_indices_2(self, string):\n        hash1, hash2 = self.hash_functions(string)\n        indices = []\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            indices.append(index)\n            if self.indices[index] == 0:\n                return False, indices\n        return True, indices\n\n    def lookup_indices_bit_array_indices_3(self, string):\n        hash1, hash2 = self.hash_functions(string)\n        indices = []\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            indices.append(index)\n            if self.indices[index]"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.load_from_dict(json_dict[\"distilled_model\"])\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        self.teacher_models = [config_factory.load_from_dict(teacher_model) for teacher_model in json_dict[\"teacher_models\"]]\n        return self\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n        # check if the model is a valid OpenAIConfig\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"Invalid model configuration\")\n\n        # check if the system message is a valid string\n        if not isinstance(system_message, str):\n            raise ValueError(\"Invalid system message\")\n\n        # check if the prompt is a valid string\n        if not isinstance(prompt, str):\n            raise ValueError(\"Invalid prompt\")\n\n        # check if the additional parameters are valid\n        for key, value in kwargs.items():\n            if key not in LLM_GENERATION_PARAMETERS:\n                raise ValueError(f\"Invalid parameter: {key}\")\n            if not isinstance(value, (int, float)):\n                raise ValueError(f\"Invalid value for parameter: {key}\")\n\n        # check if the model is a valid OpenAIConfig\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"Invalid model configuration\")\n\n        # check if the system message is a valid string\n        if not isinstance(system_message, str):\n            raise ValueError(\"Invalid system message\")\n\n        # check if the prompt is a valid string\n        if not isinstance(prompt, str):\n            raise ValueError(\"Invalid prompt\")\n\n        # check if the additional parameters are valid\n        for key, value in kwargs.items():\n            if key not in LLM_GENERATION_PARAMETERS:\n                raise ValueError(f\"Invalid parameter: {key}\")\n            if not isinstance(value, (int, float)):\n                raise ValueError(f\"Invalid value for parameter: {key}\")\n\n        # check if the model is a valid OpenAIConfig\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"Invalid model configuration\")\n\n        # check if the system message is a valid string\n        if not isinstance(system_message, str):\n            raise ValueError(\"Invalid system message\")\n\n        # check if the prompt is a valid string\n        if not isinstance(prompt, str):\n            raise ValueError(\"Invalid prompt\")\n\n        # check if the additional parameters are valid\n        for key, value in kwargs.items():\n            if key not in LLM_GENERATION_PARAMETERS:\n                raise ValueError(f\"Invalid parameter: {key}\")\n            if not isinstance(value, (int, float)):\n                raise ValueError(f\"Invalid value for parameter: {key}\")\n\n        # check if the model is a valid OpenAIConfig\n        if not isinstance(model, Open"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if not np.allclose(x, x.T, atol=1e-5):\n        raise ValueError(\"The matrix must be symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(f\"`x` must be a square matrix, got a {x.shape} matrix\")\n    if not np.allclose(x, x.T):\n        raise ValueError(f\"`x` must be a symmetric matrix, got a {x.shape} matrix\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(f\"`x` must be a distance matrix, got a {x.shape} matrix\")\n\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize it\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # get the teacher models\n        teacher_models = self.function_modeler.get_models(function_description)[1]\n        # get the distilled models\n        distilled_models = self.function_modeler.get_models(function_description)[0]\n        # get the examples\n        examples = self.initialized_functions[func_hash][\"examples\"]\n        # get the function description\n        f = str(function_description.__dict__.__repr__() + \"\\n\")\n        # get the token counts\n        input_token_count = approximate_token_count(f\"Function: {f}\\n---\\nInputs:\\nArgs: {args}\\nKwargs: {kwargs}\\nOutput:\")\n        # get the model\n        model = self.choose_model_from_tokens(teacher_models, input_token_count, len(examples))\n        # if the model is not None, then it's a teacher model\n        if model:\n            # construct the prompt\n            prompt = self.construct_prompt(f, args, kwargs, examples, model)\n            # check if the model is suitable for distillation\n            suitable_for_finetune, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs, f, model)\n            # if the model is suitable for distillation, then use the distilled model\n            if suitable_for_finetune:\n                # get the distilled model\n                distilled_model = self.choose_model_from_tokens(distilled_models, input_prompt_token_count, len(examples))\n                # if the distilled model is not None, then it's a distilled model\n                if distilled_model:\n                    # construct the prompt\n                    prompt = self.construct_prompt(f, args, kwargs, examples, distilled_model)\n                    # set the model to the distilled model\n                    model = distilled_model\n                    # set the save_to_finetune flag to False\n                    save_to_finetune = False\n                    # set the is_distilled_model flag to True"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Higham & Nick (2002) algorithm\n        n = cov.shape[0]\n        eigval, eigvec = np.linalg.eigh(cov)\n        eigval[eigval < _CLIPPING_VALUE] = _CLIPPING_VALUE\n        cov_adj = np.dot(eigvec, np.dot(np.diag(eigval), eigvec.T))\n        count = 0\n        while not is_cholesky_dec(cov_adj) and count < higham_max_iteration:\n            eigval[eigval < _CLIPPING_VALUE] *= 1.01\n            cov_adj = np.dot(eigvec, np.dot(np.diag(eigval), eigvec.T))\n            count += 1\n        if count == higham_max_iteration:\n            raise ValueError(\n                \"Higham & Nick (2002) algorithm did not converge after \"\n                f\"{higham_max_iteration} iterations.\"\n            )\n    else:\n        # Clipping eigenvalues\n        eigval, eigvec = np.linalg.eigh(cov)\n        eigval[eigval < _CLIPPING_VALUE] = _CLIPPING_VALUE\n        cov_adj = np.dot(eigvec, np.dot(np.diag(eigval), eigvec.T))\n    return cov_adj"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    path = resources.files(data_module).joinpath(data_filename)\n    with path.open(\"rb\") as compressed_file:\n        compressed_file = gzip.open(compressed_file, mode=\"rt\", encoding=encoding)\n        df = pd.read_csv(compressed_file, sep=\",\", index_col=0)\n        if datetime_index:\n            df.index = pd.to_datetime(df.index, format=\"%Y-%m-%d\")\n        return df\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, str):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, bytes):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, (list, tuple)):\n        schemas = []\n        sizes = []\n        for k in obj:\n            res, schema = flatten_to_tuple(k)\n            schemas.append(schema)\n            sizes.append(len(res))\n        values, _ = ListSchema._concat([k for k in schemas])\n        return values, ListSchema(schemas, sizes)\n    if isinstance(obj, dict):\n        schemas = []\n        sizes = []\n        for k in obj.keys():\n            res, schema = flatten_to_tuple(obj[k])\n            schemas.append(schema)\n            sizes.append(len(res))\n        values, _ = ListSchema._concat([k for k in schemas])\n        return values, DictSchema(schemas, sizes, sorted(obj.keys()))\n    if isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    if isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    raise ValueError(f\"Cannot flatten object of type {type(obj)}\")"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n\n    if groups.ndim != 2:\n        raise EquationToMatrixError(\n            f\"The {names[0]} should be a 2D array, got {groups.ndim}D array instead\"\n        )\n    if equations.ndim != 1:\n        raise EquationToMatrixError(\n            f\"The {names[1]} should be a 1D array, got {equations.ndim}D array instead\"\n        )\n\n    n_groups, n_assets = groups.shape\n    n_equations = len(equations)\n\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n\n    for i, equation in enumerate(equations):\n        try:\n            left[i, :], right[i] = _string_to_equation(\n                groups=groups, string=equation, sum_to_one=sum_to_one\n            )\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            warnings.warn(\n                f\"Unable to find group in equation '{equation}'. Skipping this equation.\"\n            )\n\n    if np.all(left == 0) and np.all(right == 0):\n        return None, None\n\n    return left, right"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    cls_name, cls_def = _gen_instance_module(fields)\n    with tempfile.NamedTemporaryFile(suffix=\".py\") as f:\n        f.write(cls_def.encode())\n        f.flush()\n        newInstances = _import(f.name)\n        _add_instances_conversion_methods(newInstances)\n        yield newInstances\n        _clear_jit_cache()\n        del sys.modules[cls_name]"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": ".\n    \"\"\"\n    return build_augmentation(cfg, is_train)\n\n\ndef build_transform_gen(crop_size, image_size, instance):\n    \"\"\"\n    Generate a CropTransform so that the cropping region contains\n    the center of the given instance.\n\n    Args:\n        crop_size (tuple): h, w in pixels\n        image_size (tuple): h, w\n        instance (dict): an annotation dict of one instance, in Detectron2's\n            dataset format.\n    \"\"\"\n    return gen_crop_transform_with_instance(crop_size, image_size, instance)\n\n\ndef build_transform_list(transforms):\n    \"\"\"\n    Create a :class:`TransformList` from a list of transforms.\n\n    Args:\n        transforms (list[Transform]): a list of transforms\n\n    Returns:\n        TransformList: the transforms\n    \"\"\"\n    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    return transforms\n\n\ndef build_transform_list_gen(transforms):\n    \"\"\"\n    Generate a :class:`TransformList` from a list of transforms.\n\n    Args:\n        transforms (list[Transform]): a list of transforms\n\n    Returns:\n        TransformList: the transforms\n    \"\"\"\n    return gen_transform_list(transforms)\n\n\ndef gen_transform_list(transforms):\n    \"\"\"\n    Generate a :class:`TransformList` from a list of transforms.\n\n    Args:\n        transforms (list[Transform]): a list of transforms\n\n    Returns:\n        TransformList: the transforms\n    \"\"\"\n    return T.TransformList(transforms)\n\n\ndef build_transform_list_from_config(cfg, is_train):\n    \"\"\"\n    Create a :class:`TransformList` from config.\n    Now it includes resizing and flipping.\n\n    Args:\n        cfg: config\n        is_train (bool): whether it's used during training\n\n    Returns:\n        TransformList: the transforms\n    \"\"\"\n    return build_transform_list(cfg, is_train)\n\n\ndef build_transform_list(cfg, is_train):\n    \"\"\"\n    Create a :class:`TransformList` from config.\n    Now it includes resizing and flipping.\n\n    Args:\n        cfg: config\n        is_train (bool): whether it's used during training\n\n    Returns:\n        TransformList: the transforms\n    \"\"\"\n    return build_augmentation(cfg"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        keypoints = transform_keypoint_annotations(\n            keypoints, transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints\n\n    if \"bbox\" in annotation:\n        bbox = annotation[\"bbox\"]\n        bbox = transforms.apply_box(bbox)\n        bbox = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n        annotation[\"bbox\"] = bbox\n        annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    if \"segmentation\" in annotation:\n        segmentation = annotation[\"segmentation\"]\n        if isinstance(segmentation, list):\n            segmentation = transforms.apply_polygons(segmentation)\n        else:\n            segmentation = transforms.apply_rles(segmentation)\n        annotation[\"segmentation\"] = segmentation\n\n    return annotation\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        return cv2.transform(coords[None, None, :], self.rm_coords)[0, 0, :]\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img.shape[:2] == (self.h, self.w) and self.angle % 360 != 0:\n            interp_method = interp if interp is not None else self.interp\n            if img.dtype == np.uint8:\n                if len(img.shape) > 2 and img.shape[2] == 1:\n                    pil_image = Image.fromarray(img[:, :, 0], mode=\"L\")\n                else:\n                    pil_image = Image.fromarray(img)\n                pil_image = pil_image.rotate(self.angle, resample=interp_method)\n                ret = np.asarray(pil_image)\n                if len(img.shape) > 2 and img.shape[2] == 1:\n                    ret = np.expand_dims(ret, -1)\n            else:\n                # PIL only supports uint8\n                if any(x < 0 for x in img.strides):\n                    img = np.ascontiguousarray(img)\n                img = torch.from_numpy(img)\n                shape = list(img.shape)\n                shape_4d = shape[:2] + [1] * (4 - len(shape)) + shape[2:]\n                img = img.view(shape_4d).permute(2, 3, 0, 1)  # hw(c) -> nchw\n                img = F.rotate(img, self.angle, self.rm_image)\n                shape[:2] = (self.bound_h, self.bound_w)\n                ret = img.permute(2, 3, 0, 1).view(shape).numpy()  # nchw -> hw(c)\n        else:\n            ret = img\n        return ret"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "    def draw_keypoints(self, keypoints, color, radius=3):\n        \"\"\"\n        Draw keypoints on the image.\n\n        Args:\n            keypoints (list[int] or Keypoints): a list containing the x and y coordinates\n                of the keypoints.\n            color: color of the keypoints. Refer to `matplotlib.colors` for a full list of\n                formats that are accepted.\n            radius (int): radius of the keypoints.\n\n        Returns:\n            output (VisImage): image object with keypoints drawn.\n        \"\"\"\n        keypoints = self._convert_keypoints(keypoints)\n        for keypoint in keypoints:\n            self.draw_circle(keypoint, color, radius)\n        return self.output\n\n    def draw_soft_masks(self, soft_masks, color=None, *, text=None, alpha=0.5):\n        \"\"\"\n        Draw soft masks on the image.\n\n        Args:\n            soft_masks (list[float] or SoftMasks): a list containing the soft masks.\n            color: color of the masks. Refer to `matplotlib.colors` for a full list of\n                formats that are accepted. If None, will pick a random color.\n            text (str): if None, will be drawn on the object\n            alpha (float): blending efficient. Smaller values lead to more transparent masks.\n\n        Returns:\n            output (VisImage): image object with soft masks drawn.\n        \"\"\"\n        soft_masks = self._convert_masks(soft_masks)\n        for soft_mask in soft_masks:\n            self.draw_soft_mask(soft_mask, color, text=text, alpha=alpha)\n        return self.output\n\n    def draw_binary_masks(\n        self, masks_or_polygons, color=None, *, edge_color=None, text=None, alpha=0.5, area_threshold=10\n    ):\n        \"\"\"\n        Draw binary masks on the image.\n\n        Args:\n            masks_or_polygons (list[int] or PolygonMasks or BitMasks): a list containing the\n                masks or polygons.\n            color: color of the masks. Refer to `matplotlib.colors` for a full list of\n                formats that are accepted. If None, will pick a random color.\n            edge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\n                full"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "    def draw_keypoints(self, keypoints, color, radius=3):\n        \"\"\"\n        Draw keypoints on the image.\n\n        Args:\n            keypoints (list[int] or Keypoints): a list containing the x and y coordinates\n                of the keypoints.\n            color: color of the keypoints. Refer to `matplotlib.colors` for a full list of\n                formats that are accepted.\n            radius (int): radius of the keypoints.\n\n        Returns:\n            output (VisImage): image object with keypoints drawn.\n        \"\"\"\n        keypoints = self._convert_keypoints(keypoints)\n        for keypoint in keypoints:\n            self.draw_circle(keypoint, color, radius)\n        return self.output\n\n    def draw_soft_masks(self, soft_masks, color=None, *, text=None, alpha=0.5):\n        \"\"\"\n        Draw soft masks on the image.\n\n        Args:\n            soft_masks (list[float] or SoftMasks): a list containing the soft masks.\n            color: color of the soft masks. Refer to `matplotlib.colors` for a full list of\n                formats that are accepted. If None, will pick a random color.\n            text (str): if None, will be drawn on the object\n            alpha (float): blending efficient. Smaller values lead to more transparent masks.\n\n        Returns:\n            output (VisImage): image object with soft masks drawn.\n        \"\"\"\n        soft_masks = self._convert_masks(soft_masks)\n        for soft_mask in soft_masks:\n            self.draw_soft_mask(soft_mask, color, text=text, alpha=alpha)\n        return self.output\n\n    def draw_binary_masks(\n        self, masks_or_polygons, color=None, *, edge_color=None, text=None, alpha=0.5, area_threshold=10\n    ):\n        \"\"\"\n        Draw binary masks on the image.\n\n        Args:\n            masks_or_polygons (list[int] or PolygonMasks or BitMasks): a list containing the\n                masks or polygons.\n            color: color of the masks. Refer to `matplotlib.colors` for a full list of\n                formats that are accepted. If None, will pick a random color.\n            edge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\n               "}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "    def draw_soft_mask(self, soft_mask, color=None, *, text=None, alpha=0.5):\n        \"\"\"\n        Draw a soft mask on the image.\n\n        Args:\n            soft_mask (ndarray): float array of shape (H, W), each value in [0, 1].\n            color: color of the mask. Refer to `matplotlib.colors` for a full list of\n                formats that are accepted. If None, will pick a random color.\n            text (str): if None, will be drawn on the object\n            alpha (float): blending efficient. Smaller values lead to more transparent masks.\n\n        Returns:\n            output (VisImage): image object with mask drawn.\n        \"\"\"\n        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        shape2d = (soft_mask.shape[0], soft_mask.shape[1])\n        rgba = np.zeros(shape2d + (4,), dtype=\"float32\")\n        rgba[:, :, :3] = color\n        rgba[:, :, 3] = soft_mask * alpha\n        self.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\n\n        if text is not None:\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            binary_mask = (soft_mask > 0.5).astype(\"uint8\")\n            self._draw_text_in_mask(binary_mask, text, lighter_color)\n        return self.output\n\n    def draw_polygon(self, segment, color, edge_color=None, alpha=0.5):\n        \"\"\"\n        Draw a polygon on the image.\n\n        Args:\n            segment: numpy array of shape Nx2, containing all the points in the polygon.\n            color: color of the polygon. Refer to `matplotlib.colors` for a full list of\n                formats that are accepted.\n            edge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\n                full list of formats that are accepted. If not provided, a darker shade\n                of the polygon color will be used instead.\n            alpha (float): blending efficient. Smaller values lead"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "    def draw_soft_mask(self, soft_mask, color=None, *, text=None, alpha=0.5):\n        \"\"\"\n        Args:\n            soft_mask (ndarray): float array of shape (H, W), each value in [0, 1].\n            color: color of the mask. Refer to `matplotlib.colors` for a full list of\n                formats that are accepted. If None, will pick a random color.\n            text (str): if None, will be drawn on the object\n            alpha (float): blending efficient. Smaller values lead to more transparent masks.\n\n        Returns:\n            output (VisImage): image object with mask drawn.\n        \"\"\"\n        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        shape2d = (soft_mask.shape[0], soft_mask.shape[1])\n        rgba = np.zeros(shape2d + (4,), dtype=\"float32\")\n        rgba[:, :, :3] = color\n        rgba[:, :, 3] = soft_mask * alpha\n        self.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\n\n        if text is not None:\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            binary_mask = (soft_mask > 0.5).astype(\"uint8\")\n            self._draw_text_in_mask(binary_mask, text, lighter_color)\n        return self.output\n\n    def draw_polygon(self, polygon, color, edge_color=None, alpha=0.5):\n        \"\"\"\n        Args:\n            polygon (ndarray): float array of shape (N, 2), each value in [0, 1].\n            color: color of the polygon. Refer to `matplotlib.colors` for a full list of\n                formats that are accepted.\n            edge_color: color of the polygon edges. Refer to `matplotlib.colors` for a\n                full list of formats that are accepted. If not provided, a darker shade\n                of the polygon color will be used instead.\n            alpha (float): blending efficient. Smaller values lead to more transparent masks.\n\n        Returns:\n            output (Vis"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances)\n    assert isinstance(other, Instances)\n    assert input.image_size == other.image_size, (\n        f\"image_size mismatch: {input.image_size} vs {other.image_size}\"\n    )\n    if size_as_tensor:\n        assert torch.equal(input.image_size, other.image_size), (\n            f\"image_size mismatch: {input.image_size} vs {other.image_size}\"\n        )\n    for name in input._fields:\n        if name == \"image_size\":\n            continue\n        val1 = getattr(input, name)\n        val2 = getattr(other, name)\n        if isinstance(val1, Boxes):\n            assert torch.allclose(val1.tensor, val2.tensor, rtol=rtol), (\n                f\"{msg}Boxes field {name} mismatch: {val1.tensor} vs {val2.tensor}\"\n            )\n        elif isinstance(val1, ROIMasks):\n            assert torch.allclose(val1.tensor, val2.tensor, rtol=rtol), (\n                f\"{msg}ROIMasks field {name} mismatch: {val1.tensor} vs {val2.tensor}\"\n            )\n        elif isinstance(val1, torch.Tensor):\n            assert torch.allclose(val1, val2, rtol=rtol), (\n                f\"{msg}Tensor field {name} mismatch: {val1} vs {val2}\"\n            )\n        elif isinstance(val1, (list, tuple)):\n            assert val1 == val2, f\"{msg}List/Tuple field {name} mismatch: {val1} vs {val2}\"\n        else:\n            raise ValueError(f\"Unknown field type {type(val1)} for field {name}\")"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.tensor.area()\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    cls = PROPOSAL_GENERATOR_REGISTRY.get(name)\n    return cls(cfg, input_shape)"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        proposals_with_gt = []\n        for proposals_per_image, gt_per_image in zip(proposals, proposals.gt_boxes):\n            proposals_per_image = proposals_per_image.copy_with(\n                proposal_boxes=proposals_per_image.proposal_boxes.to(proposal_deltas.device)\n            )\n            proposals_with_gt.append(\n                Instances(\n                    image_size=proposals_per_image.image_size,\n                    proposal_boxes=proposals_per_image.proposal_boxes,\n                    gt_boxes=gt_per_image,\n                )\n            )\n\n        # Log the accuracy metrics\n        _log_accuracy_metrics(proposals_with_gt, scores)\n\n        loss_cls = F.cross_entropy(\n            scores, proposals.gt_classes, reduction=\"sum\"\n        ) / max(proposals.gt_classes.numel(), 1.0)\n        loss_box_reg = self.box_reg_loss(\n            proposals.proposal_boxes, proposals.gt_boxes, proposal_deltas, proposals.gt_classes\n        )\n        return {\n            \"loss_cls\": loss_cls * self.loss_weight[\"loss_cls\"],\n            \"loss_box_reg\": loss_box_reg * self.loss_weight[\"loss_box_reg\"],\n        }\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER_HEADS.NAME\n    return TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "    return Boxes(self.apply_deltas(deltas, boxes))\n\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            anno_type = self.anno_type_map.keys()\n        elif isinstance(anno_type, str):\n            assert anno_type in self.anno_type_map.keys()\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        img, k = resize_image(image, 512)\n        img = img.astype(np.float32) / 255.0\n        img = img.transpose(2, 0, 1)\n        img = np.expand_dims(img, 0)\n        img = np.ascontiguousarray(img)\n        img = img.astype(np.float32)\n        img = img.copy()\n\n        with we.no_sync():\n            with we.device_ctx(we.device_id):\n                with we.inference_mode():\n                    output = self.general_ins(img)\n\n        output = output[0]\n        output = resize_image_ori(image.shape[0], image.shape[1], output, k)\n        if len(anno_type) == 1:\n            return output[self.anno_type_map[anno_type[0]]['OUTPUT_KEYS'][0]]\n        else:\n            return {\n                tp: output[self.anno_type_map[tp]['OUTPUT_KEYS'][0]]\n                for tp in anno_type\n            }"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        # Normalize the query string and split it into keywords\n        query_words = normalize_string(query).split(\" \")\n\n        # Calculate the BM25 score for each keyword across URLs\n        scores = [self.bm25(word) for word in query_words]\n\n        # Aggregate the scores for URLs\n        aggregated_scores = defaultdict(float)\n        for score in scores:\n            aggregated_scores = update_url_scores(aggregated_scores, score)\n\n        # Return the aggregated scores as a dictionary where each URL is a key and its aggregated score is the value\n        return dict(aggregated_scores)\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        box = self.tensor\n        box[:, 0] = torch.clamp(box[:, 0], min=0, max=box_size[1] - 1)\n        box[:, 1] = torch.clamp(box[:, 1], min=0, max=box_size[0] - 1)\n        box[:, 2] = torch.clamp(box[:, 2], min=1, max=box_size[1])\n        box[:, 3] = torch.clamp(box[:, 3], min=1, max=box_size[0])\n        box[:, 4] = torch.clamp(box[:, 4], min=-clip_angle_threshold, max=clip_angle_threshold)\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        stats = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0,\n        }\n        for item in self.data:\n            stats[item['type']] += 1\n        return stats"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMDET_LOSSES._module_dict.keys():\n        return MMDET_LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        raise KeyError(f'Unknown loss {cfg[\"type\"]}')"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMSEG_SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    # Initialize the evaluation results\n    eval_results = {}\n    eval_results['mAP'] = {}\n    eval_results['mAR'] = {}\n    for iou in metric:\n        eval_results['mAP'][iou] = {}\n        eval_results['mAR'][iou] = {}\n\n    # Convert the annotations to the required format\n    gt_annos = [\n        {\n            'name': label2cat[anno['name']],\n            'bbox': anno['bbox'],\n            'score': 1.0\n        }\n        for anno in gt_annos\n    ]\n    dt_annos = [\n        {\n            'name': label2cat[anno['name']],\n            'bbox': anno['bbox'],\n            'score': anno['score']\n        }\n        for anno in dt_annos\n    ]\n\n    # Convert the annotations to the required format\n    if box_type_3d is not None:\n        gt_annos = [\n            {\n                'name': anno['name'],\n                'bbox': box_type_3d(anno['bbox'], box_mode_3d).convert(\n                    'xyxy').tensor.numpy(),\n                'score': 1.0\n            }\n            for anno in gt_annos\n        ]\n        dt_annos = [\n            {\n                'name': anno['name'],\n                'bbox': box_type_3d(anno['bbox'], box_mode_3d).convert(\n                    'xyxy').tensor.numpy(),\n                'score': anno['score']\n            }\n            for anno in dt_annos\n        ]\n\n    # Group the annotations by class\n    gt_annos_by_class = {}\n    dt_annos_by_class = {}\n    for anno in gt_annos:\n        if anno['name'] not in gt_annos_by_class:\n            gt_annos_by_class[anno['name']] = []\n        gt_annos_by_class[anno['name']].append(anno)\n    for anno in dt_annos:\n        if anno['name'] not in dt_annos_by_class:\n            dt_annos_by_class[anno['name']] = []\n        dt_annos_by_class[anno['name']].append(anno)\n\n    # Evaluate the detection results for each class\n    for class_name in gt_ann"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        from mmdet3d.core.bbox import LiDARInstance3DBoxes\n        box_class = LiDARInstance3DBoxes\n        mode = 'lidar'\n    elif box_type == 'Camera':\n        from mmdet3d.core.bbox import CameraInstance3DBoxes\n        box_class = CameraInstance3DBoxes\n        mode = 'camera'\n    elif box_type == 'Depth':\n        from mmdet3d.core.bbox import DepthInstance3DBoxes\n        box_class = DepthInstance3DBoxes\n        mode = 'depth'\n    else:\n        raise ValueError(f'Unsupported box type: {box_type}')\n\n    return box_class, mode"}
{"namespace": "ollama._client.Client.chat", "completion": "'\n  >>> _parse_host('example.com:56789')\n  'http://example.com:56789'\n  >>> _parse_host('http://example.com')\n  'http://example.com:80'\n  >>> _parse_host('https://example.com')\n  'https://example.com:443'\n  >>> _parse_host('https://example.com:56789')\n  'https://example.com:56789'\n  >>> _parse_host('example.com/')\n  'http://example.com:11434'\n  >>> _parse_host('example.com:56789/')\n  'http://example.com:56789'\n  \"\"\"\n\n  host, port = host or '', 11434\n  scheme, _, hostport = host.partition('://')\n  if not hostport:\n    scheme, hostport = 'http', host\n  elif scheme == 'http':\n    port = 80\n  elif scheme == 'https':\n    port = 443\n\n  split = urllib.parse.urlsplit('://'.join([scheme, hostport]))\n  host = split.hostname or '127.0.0.1'\n  port = split.port or port\n\n  return f'{scheme}://{host}:{port}'\n\n\ndef _parse_modelfile(modelfile: str, base: Optional[Path] = None) -> str:\n  \"\"\"\n  >>> _parse_modelfile(b'ollama')\n  'b2xsYW1h'\n  >>> _parse_modelfile(io.BytesIO(b'ollama'))\n  'b2xsYW1h'\n  >>> _parse_modelfile('LICENSE')\n  'TUlUIExpY2Vuc2UKCkNvcHlyaWdodCAoYykgT2xsYW1hCgpQZXJtaXNzaW9uIGlzIGhlcmVieSBncmFudGVkLCBmcmVlIG9mIGNoYXJnZSwgdG8gYW55IHBlcnNvbiBvYnRhaW5pbmcgYSBjb3B5Cm9mIHRoaXMgc29mdHdhcmUgYW5kIGFzc29ja"}
{"namespace": "ollama._client.Client.pull", "completion": ".0.0.1:11434'\n  >>> _parse_host('https://1.2.3.4')\n  'https://1.2.3.4:443'\n  >>> _parse_host('https://1.2.3.4:56789')\n  'https://1.2.3.4:56789'\n  >>> _parse_host('example.com')\n  'http://example.com:11434'\n  >>> _parse_host('example.com:56789')\n  'http://example.com:56789'\n  >>> _parse_host('http://example.com')\n  'http://example.com:80'\n  >>> _parse_host('https://example.com')\n  'https://example.com:443'\n  >>> _parse_host('https://example.com:56789')\n  'https://example.com:56789'\n  >>> _parse_host('example.com/')\n  'http://example.com:11434'\n  >>> _parse_host('example.com:56789/')\n  'http://example.com:56789'\n  \"\"\"\n\n  host, port = host or '', 11434\n  scheme, _, hostport = host.partition('://')\n  if not hostport:\n    scheme, hostport = 'http', host\n  elif scheme == 'http':\n    port = 80\n  elif scheme == 'https':\n    port = 443\n\n  split = urllib.parse.urlsplit('://'.join([scheme, hostport]))\n  host = split.hostname or '127.0.0.1'\n  port = split.port or port\n\n  return f'{scheme}://{host}:{port}'\n\n\ndef _parse_modelfile(modelfile: str, base: Optional[Path] = None) -> str:\n  \"\"\"\n  >>> _parse_modelfile(b'ollama')\n  'b2xsYW1h'\n  >>> _parse_modelfile(io.BytesIO(b'ollama'))\n  'b2xsYW1h'\n  >>> _parse_modelfile('LICENSE')\n  'TUl"}
{"namespace": "ollama._client.Client.generate", "completion": "://127.0.0.1:11434'\n  >>> _parse_host('https://1.2.3.4')\n  'https://1.2.3.4:443'\n  >>> _parse_host('https://1.2.3.4:56789')\n  'https://1.2.3.4:56789'\n  >>> _parse_host('example.com')\n  'http://example.com:11434'\n  >>> _parse_host('example.com:56789')\n  'http://example.com:56789'\n  >>> _parse_host('http://example.com')\n  'http://example.com:80'\n  >>> _parse_host('https://example.com')\n  'https://example.com:443'\n  >>> _parse_host('https://example.com:56789')\n  'https://example.com:56789'\n  >>> _parse_host('example.com/')\n  'http://example.com:11434'\n  >>> _parse_host('example.com:56789/')\n  'http://example.com:56789'\n  \"\"\"\n\n  host, port = host or '', 11434\n  scheme, _, hostport = host.partition('://')\n  if not hostport:\n    scheme, hostport = 'http', host\n  elif scheme == 'http':\n    port = 80\n  elif scheme == 'https':\n    port = 443\n\n  split = urllib.parse.urlsplit('://'.join([scheme, hostport]))\n  host = split.hostname or '127.0.0.1'\n  port = split.port or port\n\n  return f'{scheme}://{host}:{port}'\n\n\ndef _parse_modelfile(modelfile: str, base: Optional[Path] = None) -> str:\n  \"\"\"\n  >>> _parse_modelfile(b'ollama')\n  'b2xsYW1h'\n  >>> _parse_modelfile(io.BytesIO(b'ollama'))\n  'b2xsYW1h'\n  >>> _parse_modelfile('LICENSE')"}
{"namespace": "ollama._client.Client.push", "completion": ".0.0.1:11434'\n  >>> _parse_host('https://1.2.3.4')\n  'https://1.2.3.4:443'\n  >>> _parse_host('https://1.2.3.4:56789')\n  'https://1.2.3.4:56789'\n  >>> _parse_host('example.com')\n  'http://example.com:11434'\n  >>> _parse_host('example.com:56789')\n  'http://example.com:56789'\n  >>> _parse_host('http://example.com')\n  'http://example.com:80'\n  >>> _parse_host('https://example.com')\n  'https://example.com:443'\n  >>> _parse_host('https://example.com:56789')\n  'https://example.com:56789'\n  >>> _parse_host('example.com/')\n  'http://example.com:11434'\n  >>> _parse_host('example.com:56789/')\n  'http://example.com:56789'\n  \"\"\"\n\n  host, port = host or '', 11434\n  scheme, _, hostport = host.partition('://')\n  if not hostport:\n    scheme, hostport = 'http', host\n  elif scheme == 'http':\n    port = 80\n  elif scheme == 'https':\n    port = 443\n\n  split = urllib.parse.urlsplit('://'.join([scheme, hostport]))\n  host = split.hostname or '127.0.0.1'\n  port = split.port or port\n\n  return f'{scheme}://{host}:{port}'\n\n\ndef _parse_modelfile(modelfile: str, base: Optional[Path] = None) -> str:\n  \"\"\"\n  >>> _parse_modelfile(b'ollama')\n  'b2xsYW1h'\n  >>> _parse_modelfile(io.BytesIO(b'ollama'))\n  'b2xsYW1h'\n  >>> _parse_modelfile('LICENSE')\n  'TUl"}
{"namespace": "ollama._client.Client.create", "completion": "_host(host)\n  return await self._request('POST', '/api/create', json={'name': model, 'modelfile': modelfile, 'stream': stream}, stream=stream)\n\n\nasync def _parse_modelfile(modelfile: str, base: Optional[Path] = None) -> str:\n  \"\"\"\n  >>> await _parse_modelfile(b'ollama')\n  'b2xsYW1h'\n  >>> await _parse_modelfile(io.BytesIO(b'ollama'))\n  'b2xsYW1h'\n  >>> await _parse_modelfile('LICENSE')\n  'TUlUIExpY2Vuc2UKCkNvcHlyaWdodCAoYykgT2xsYW1hCgpQZXJtaXNzaW9uIGlzIGhlcmVieSBncmFudGVkLCBmcmVlIG9mIGNoYXJnZSwgdG8gYW55IHBlcnNvbiBvYnRhaW5pbmcgYSBjb3B5Cm9mIHRoaXMgc29mdHdhcmUgYW5kIGFzc29jaWF0ZWQgZG9jdW1lbnRhdGlvbiBmaWxlcyAodGhlICJTb2Z0d2FyZSIpLCB0byBkZWFsCmluIHRoZSBTb2Z0d2FyZSB3aXRob3V0IHJlc3RyaWN0aW9uLCBpbmNsdWRpbmcgd2l0aG91dCBsaW1pdGF0aW9uIHRoZSByaWdodHMKdG8gdXNlLCBjb3B5LCBtb2RpZnksIG1lcmdlLCBwdWJsaXNoLCBkaXN0cmlidXRlLCBzdWJsaWNlbnNlLCBhbmQvb3Igc2VsbApjb3BpZXMgb2YgdGhlIFNvZnR3YXJlLCBhbmQgdG8gcGVybWl0IHBlcnNvbnMgdG8gd2hvbSB0aGUgU29mdHdhcmUgaXMKZnVybmlzaGVkIHRvIGRvIHNvLCBzdWJqZWN0IHR"}
{"namespace": "ollama._client.Client._create_blob", "completion": "_host(None)\n  'http://127.0.0.1:11434'\n  >>> _parse_host('')\n  'http://127.0.0.1:11434'\n  >>> _parse_host('1.2.3.4')\n  'http://1.2.3.4:11434'\n  >>> _parse_host(':56789')\n  'http://127.0.0.1:56789'\n  >>> _parse_host('1.2.3.4:56789')\n  'http://1.2.3.4:56789'\n  >>> _parse_host('http://1.2.3.4')\n  'http://1.2.3.4:80'\n  >>> _parse_host('https://1.2.3.4')\n  'https://1.2.3.4:443'\n  >>> _parse_host('https://1.2.3.4:56789')\n  'https://1.2.3.4:56789'\n  >>> _parse_host('example.com')\n  'http://example.com:11434'\n  >>> _parse_host('example.com:56789')\n  'http://example.com:56789'\n  >>> _parse_host('http://example.com')\n  'http://example.com:80'\n  >>> _parse_host('https://example.com')\n  'https://example.com:443'\n  >>> _parse_host('https://example.com:56789')\n  'https://example.com:56789'\n  >>> _parse_host('example.com/')\n  'http://example.com:11434'\n  >>> _parse_host('example.com:56789/')\n  'http://example.com:56789'\n  \"\"\"\n\n  host, port = host or '', 11434\n  scheme, _, hostport = host.partition('://')\n  if not hostport:\n    scheme, hostport = 'http"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "_path(path) and realpath.exists():\n      modelfile = realpath.read_text()\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile, base=base)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return modelfile\n\n\ndef _parse_modelfile(modelfile: str, base: Optional[Path] = None) -> str:\n  \"\"\"\n  >>> _parse_modelfile('LICENSE')\n  'FROM alpine:latest\\nRUN apk add --no-cache python3\\nCMD [\"python3\"]\\n'\n  >>> _parse_modelfile('LICENSE', base=Path('..'))\n  'FROM alpine:latest\\nRUN apk add --no-cache python3\\nCMD [\"python3\"]\\n'\n  >>> _parse_modelfile('LICENSE', base=Path('..'))\n  'FROM alpine:latest\\nRUN apk add --no-cache python3\\nCMD [\"python3\"]\\n'\n  >>> _parse_modelfile('LICENSE', base=Path('..'))\n  'FROM alpine:latest\\nRUN apk add --no-cache python3\\nCMD [\"python3\"]\\n'\n  >>> _parse_modelfile('LICENSE', base=Path('..'))\n  'FROM alpine:latest\\nRUN apk add --no-cache python3\\nCMD [\"python3\"]\\n'\n  >>> _parse_modelfile('LICENSE', base=Path('..'))\n  'FROM alpine:latest\\nRUN apk add --no-cache python3\\nCMD [\"python3\"]\\n'\n  >>> _parse_modelfile('LICENSE', base=Path('..'))\n  'FROM alpine:latest\\nRUN apk add --no-cache python3\\nCMD [\"python3\"]\\n'\n  >>> _parse_modelfile('LICENSE', base=Path('..'))\n  'FROM alpine:latest\\nRUN apk add --no-cache python3\\nCMD [\"python3\"]\\n'\n  >>> _parse_modelfile('LICENSE', base=Path('..'))\n  'FROM alpine:latest\\nRUN apk add --no-cache python3\\nCMD [\"python3\"]\\n'\n  >>> _parse_modelfile('LICENSE', base=Path('..'))"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "  return await self._request_stream(\n    'POST',\n    '/api/pull',\n    json={\n      'name': model,\n      'insecure': insecure,\n      'stream': stream,\n    },\n    stream=stream,\n  )\n\n  async def push(\n    self,\n    model: str,\n    insecure: bool = False,\n    stream: bool = False,\n  ) -> Union[Mapping[str, Any], AsyncIterator[Mapping[str, Any]]]:\n    \"\"\"\n    The function asynchronously requests to push data for a specified model to an API endpoint. It can operate in either a secure or insecure mode and has the option to stream the response. If the request cannot be fulfilled, it raises a `ResponseError`. Depending on the `stream` parameter, it either returns a single `ProgressResponse` or a generator yielding `ProgressResponse` objects.\n\n    Input-Output Arguments\n    :param self: AsyncClient. An instance of the AsyncClient class.\n    :param model: str, The name of the model to push data for.\n    :param insecure: bool, Optional. Determines whether the request should be made in an insecure manner. Defaults to False.\n    :param stream: bool, Optional. Determines whether the response should be streamed. Defaults to False.\n    :return: Union[Mapping[str, Any], AsyncIterator[Mapping[str, Any]]], A single `ProgressResponse` if `stream` is False, or a generator yielding `ProgressResponse` objects if `stream` is True.\n    \"\"\"\n\n    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  async def create(\n    self,\n    model: str,\n    path: Optional[Union[str, PathLike]] = None,\n    modelfile: Optional[str] = None,\n    stream: bool = False,\n  ) -> Union[Mapping[str, Any], AsyncIterator[Mapping[str, Any]]]:\n    \"\"\"\n    The function asynchronously requests to create a specified model in an API endpoint. It can operate in either a secure or insecure mode and has the option to stream the response. If the request cannot be fulfilled, it raises a `ResponseError`. Depending on the `stream` parameter"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "  if not host:\n    return 'http://127.0.0.1:11434'\n\n  if isinstance(host, str):\n    host, port = host, 11434\n    scheme, _, hostport = host.partition('://')\n    if not hostport:\n      scheme, hostport = 'http', host\n    elif scheme == 'http':\n      port = 80\n    elif scheme == 'https':\n      port = 443\n\n    split = urllib.parse.urlsplit('://'.join([scheme, hostport]))\n    host = split.hostname or '127.0.0.1'\n    port = split.port or port\n\n    return f'{scheme}://{host}:{port}'\n\n  raise RequestError('host must be str or path-like object')\n\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "  if p := _as_path(path):\n    return b64encode(p.read_bytes()).decode('utf-8')\n\n  try:\n    b64decode(path, validate=True)\n    return path if isinstance(path, str) else path.decode('utf-8')\n  except (binascii.Error, TypeError):\n    ...\n\n  if b := _as_bytesio(path):\n    return b64encode(b.read()).decode('utf-8')\n\n  raise RequestError('path must be bytes, path-like object, or file-like object')\n\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(mode=\"w\", encoding=\"utf-8\") as temp_file:\n            temp_file.write(user_code + test_code)\n            temp_file.flush()\n\n            # Run Pyright on the temporary file\n            pyright_process = subprocess.run(\n                [\"pyright\", temp_file.name],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                encoding=\"utf-8\",\n            )\n\n            # Parse the output of Pyright to identify lines with expected type errors\n            error_lines = []\n            for line in pyright_process.stdout.splitlines():\n                match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n                if match:\n                    line_number = int(match.group(1))\n                    error_message = match.group(2)\n                    if cls.EXPECT_ERROR_COMMENT in error_message:\n                        error_lines.append(line_number)\n\n            # Check if the type check passed\n            passed = len(error_lines) == 0\n\n            # Create a message based on the outcome of the type check\n            if passed:\n                message = \"Type check passed!\"\n            else:\n                message = (\n                    f\"Type check failed on line(s): {', '.join(str(line) for line in error_lines)}\"\n                )\n\n            # Return the result of the type check\n            return TypeCheckResult(message=message, passed=passed)\n\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "  if p := _as_path(path):\n    return p.read_bytes()\n\n  try:\n    b64decode(path, validate=True)\n    return path if isinstance(path, bytes) else path.encode('utf-8')\n  except (binascii.Error, TypeError):\n    ...\n\n  if b := _as_bytesio(path):\n    return b.read()\n\n  raise RequestError('path must be bytes, path-like object, or file-like object')\n\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, get_compiler_fn(title=\"Forward Graph:\"), get_compiler_fn(title=\"Backward Graph:\"))\n    else:\n        return aot_function(fn, get_compiler_fn(title=\"Forward Graph:\"), get_compiler_fn(title=\"Backward Graph:\"))"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    config_dict = yaml.safe_load(open(os.path.join(trial_path, 'config.yaml'), 'r'))\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n    if output_path is not None:\n        with open(output_path, 'w') as f:\n            yaml.dump(best_config, f)\n    return best_config"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    if not hasattr(func, 'forward'):\n        raise ValueError(\n            'func must be a function or a torch.nn.Module with a forward method'\n        )\n\n    def wrapper(*args, **kwargs):\n        if not hasattr(wrapper, 'cache'):\n            wrapper.cache = {}\n        if not hasattr(wrapper, 'lock'):\n            wrapper.lock = threading.Lock()\n        if not hasattr(wrapper, 'compiler'):\n            wrapper.compiler = AutoTraceCompiler(**kwargs_)\n        inputs_key = wrapper.compiler.get_inputs_key(func, args, kwargs)\n        if inputs_key is None:\n            return func(*args, **kwargs)\n        with wrapper.lock:\n            if inputs_key not in wrapper.cache:\n                wrapper.cache[inputs_key] = wrapper.compiler.compile(\n                    func, args, kwargs)\n            outputs_key = wrapper.compiler.get_outputs_key(\n                wrapper.cache[inputs_key], args, kwargs)\n            if outputs_key is None:\n                return wrapper.cache[inputs_key](*args, **kwargs)\n            if outputs_key not in wrapper.cache:\n                wrapper.cache[outputs_key] = wrapper.cache[inputs_key](\n                    *args, **kwargs)\n            return wrapper.cache[outputs_key]\n\n    return wrapper"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": " return cls.from_yaml(os.path.join(trial_path, 'best_config.yaml'), project_dir=os.path.dirname(trial_path))\n\n    def __add_api_route(self):\n        raise NotImplementedError\n\n    def run(self, query: str, result_column: str = \"generated_texts\"):\n        raise NotImplementedError\n\n    def run_api_server(self, host: str = '0.0.0.0', port: int = 8000, **kwargs):\n        raise NotImplementedError\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    logger.info(f\"Running retrieval node {node_line_dir}\")\n    node_line_dir = pathlib.Path(node_line_dir)\n    node_line_dir.mkdir(parents=True, exist_ok=True)\n\n    # Run each module with given parameters\n    results = []\n    for module, module_param in zip(modules, module_params):\n        result = module(previous_result, **module_param)\n        results.append(result)\n\n    # Measure execution times\n    execution_times = measure_speed(results)\n\n    # Apply strategies\n    best_result = select_best_average(results, execution_times, strategies)\n\n    # Save results and summaries\n    best_result.to_parquet(os.path.join(node_line_dir, \"result.parquet\"))\n    summary_df = pd.DataFrame({\n        'filename': ['result.parquet'],\n        'module_name': [best_result['module_name'].iloc[0]],\n        'module_params': [best_result['module_params'].iloc[0]],\n        'execution_time': [execution_times[best_result['module_name'].iloc[0]]],\n    })\n    summary_df.to_csv(os.path.join(node_line_dir, \"summary.csv\"), index=False)\n    return best_result\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Get the support modules for query expansion\n    support_modules = get_support_modules('query_expansion')\n\n    # Get the query expansion modules and their parameters\n    query_expansion_modules = list(map(lambda module_dict: get_support_modules(module_dict.pop('module_type')),\n                                       module_params))\n    query_expansion_params = list(map(lambda module_dict: module_dict, module_params))\n\n    # Get the retrieval modules and their parameters\n    retrieval_modules = list(map(lambda module_dict: get_support_modules(module_dict.pop('module_type')),\n                                 strategies.pop('retrieval_modules', None)))\n    retrieval_params = list(map(lambda module_dict: module_dict, strategies.pop('retrieval_modules', None)))\n\n    # Get the metrics for evaluation\n    metrics = strategies.pop('metrics', None)\n\n    # Get the top_k for evaluation\n    top_k = strategies.pop('top_k', None)\n\n    # Get the speed threshold for filtering\n    speed_threshold = strategies.pop('speed_threshold', None)\n\n    # Get the threshold for filtering\n    threshold = strategies.pop('threshold', None)\n\n    # Get the retrieval ground truth\n    retrieval_gt = previous_result['retrieval_gt'].tolist()\n\n    # Get the query expansion ground truth\n    query_expansion_gt = previous_result['query_expansion_gt'].tolist()\n\n    # Get the query expansion ground truth\n    query_expansion_gt = previous_result['query_expansion_gt'].tolist()\n\n    # Get the query expansion ground truth\n    query_expansion_gt = previous_result['query_expansion_gt'].tolist()\n\n    # Get the query expansion ground truth\n    query_expansion_gt = previous_result['query_expansion_gt'].tolist()\n\n    # Get the query expansion ground truth\n    query_expansion_gt = previous_result['query_expansion_gt'].tolist()\n\n    # Get the query expansion ground truth\n    query_expansion_gt = previous_result['query_expansion_gt'].tolist()\n\n    # Get the query expansion ground truth\n    query_expansion_gt = previous_result['query_expansion_gt'].tolist()\n\n    # Get the query expansion ground truth\n    query_expansion_gt = previous_result['query_expansion_gt'].tolist()\n\n    # Get"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    node_line_dir = os.path.join(node_line_dir, 'prompt_maker')\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n    generator_funcs, generator_params = make_generator_callable_params(strategies)\n    prompts = previous_result['prompts'].tolist()\n    generation_gt = previous_result['generation_gt'].tolist()\n    metrics = strategies['metrics']\n    best_result = evaluate_one_prompt_maker_node(generator_funcs, generator_params, prompts, generation_gt, metrics, node_line_dir)\n    best_result = cast_metrics(best_result, metrics)\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'), index=False)\n    return best_result\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model('all-mpnet-base-v2')\n\n    def compute_score(gt: str, pred: str) -> float:\n        gt_embedding = embedding_model.embed_query(gt)\n        pred_embedding = embedding_model.embed_query(pred)\n        return calculate_cosine_similarity(gt_embedding, pred_embedding)\n\n    return max(list(map(lambda x: compute_score(x, pred), generation_gt)))\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer not set up\")\n        return np_image\n    return gfpgan_face_restorer.fix_faces(np_image)"}
{"namespace": "codeformer_model.setup_model", "completion": "    global codeformer\n    try:\n        codeformer = FaceRestorerCodeFormer(dirname)\n    except errors.FaceRestorationError as e:\n        logger.error(f\"Error setting up CodeFormer: {e}\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        face_restoration_utils.patch_facexlib(dirname)\n        global gfpgan_face_restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except errors.FaceRestorationError as e:\n        logger.error(f\"Error setting up GFPGAN face restorer: {e}\")\n    except Exception as e:\n        logger.error(f\"Error setting up GFPGAN face restorer: {e}\")"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion\n  v_quat = jnp.array([0.0, v[0], v[1], v[2]])\n\n  # Apply the rotation\n  rotated_quat = multiply(q, v_quat)\n  rotated_quat = multiply(rotated_quat, conjugate(q))\n\n  # Convert the quaternion back to a vector\n  return rotated_quat[Ellipsis, 1:]\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis, angle = jnp.split(axis_angle, 2, axis=-1)\n  half_angle = 0.5 * angle\n  s = jnp.sin(half_angle)\n  c = jnp.cos(half_angle)\n  return jnp.concatenate([s * axis, c], axis=-1)"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    num_calls = k\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n        num_calls += k\n    high = logit_bias[idx]\n\n    # improve estimate\n    low = 0\n    mid = (high + low) / 2\n    while high >= low + eps:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n        num_calls += k\n    return -mid, num_calls"}
{"namespace": "resample.resample_3d", "completion": "  if method == 'TRILINEAR':\n    return resample_3d_trilinear(\n        data,\n        locations,\n        edge_behavior=edge_behavior,\n        constant_values=constant_values,\n        coordinate_order=coordinate_order,\n        half_pixel_center=half_pixel_center,\n    )\n  elif method == 'NEAREST':\n    return resample_3d_nearest(\n        data,\n        locations,\n        edge_behavior=edge_behavior,\n        constant_values=constant_values,\n        coordinate_order=coordinate_order,\n        half_pixel_center=half_pixel_center,\n    )\n  else:\n    raise ValueError(f'Invalid method: {method}')\n\n"}
{"namespace": "math.plus_eps", "completion": "def general_lossfun_grad(x, alpha, scale):\n  \"\"\"The gradient of the general loss function.\"\"\"\n  eps = jnp.finfo(jnp.float32).eps\n  maxval = 1e15\n\n  # A \"safe\" versions of expm1 that will not NaN-out on large inputs.\n  expm1_safe = lambda x: jnp.expm1(jnp.minimum(x, 43))\n\n  # `scale` must be > 0.\n  scale = jnp.maximum(eps, scale)\n\n  # Large values of |x| can cause non-finite gradients.\n  x = fake_clip(x, -maxval, maxval)\n\n  # The loss when alpha == 2. This will get reused repeatedly.\n  loss_two = 0.5 * (x / scale)**2\n\n  # Clamp |alpha| to be >= machine epsilon so that it's safe to divide by.\n  a = jnp.where(alpha >= 0, jnp.ones_like(alpha),\n                -jnp.ones_like(alpha)) * jnp.maximum(eps, jnp.abs(alpha))\n\n  # Clamp |2-alpha| to be >= machine epsilon so that it's safe to divide by.\n  b = jnp.maximum(eps, jnp.abs(a - 2))\n\n  # The loss when not in one of the special casess.\n  loss_ow = (b / a) * ((loss_two / (0.5 * b) + 1)**(0.5 * a) - 1)\n\n  # Select which of the cases of the loss to return as a function of alpha.\n  return jnp.where(\n      alpha == -jnp.inf, -expm1_safe(-loss_two),\n      jnp.where(\n          alpha == 0, jnp.log1p(loss_two),\n          jnp.where(alpha == 2, loss_two,\n                    jnp.where(alpha == jnp.inf, expm1_safe(loss_two),\n                              loss_ow))))\n\n\ndef general_lossfun_grad_grad(x, alpha, scale):\n  \"\"\"The gradient of the gradient of the general loss function.\"\"\"\n  eps = jnp.f"}
{"namespace": "math.minus_eps", "completion": "def general_lossfun_grad(x, alpha, scale):\n  \"\"\"The gradient of the general loss function.\"\"\"\n  eps = jnp.finfo(jnp.float32).eps\n  maxval = 1e15\n\n  # A \"safe\" versions of expm1 that will not NaN-out on large inputs.\n  expm1_safe = lambda x: jnp.expm1(jnp.minimum(x, 43))\n\n  # `scale` must be > 0.\n  scale = jnp.maximum(eps, scale)\n\n  # Large values of |x| can cause non-finite gradients.\n  x = fake_clip(x, -maxval, maxval)\n\n  # The loss when alpha == 2. This will get reused repeatedly.\n  loss_two = 0.5 * (x / scale)**2\n\n  # Clamp |alpha| to be >= machine epsilon so that it's safe to divide by.\n  a = jnp.where(alpha >= 0, jnp.ones_like(alpha),\n                -jnp.ones_like(alpha)) * jnp.maximum(eps, jnp.abs(alpha))\n\n  # Clamp |2-alpha| to be >= machine epsilon so that it's safe to divide by.\n  b = jnp.maximum(eps, jnp.abs(a - 2))\n\n  # The loss when not in one of the special casess.\n  loss_ow = (b / a) * ((loss_two / (0.5 * b) + 1)**(0.5 * a) - 1)\n\n  # Select which of the cases of the loss to return as a function of alpha.\n  return jnp.where(\n      alpha == -jnp.inf, -expm1_safe(-loss_two),\n      jnp.where(\n          alpha == 0, jnp.log1p(loss_two),\n          jnp.where(alpha == 2, loss_two,\n                    jnp.where(alpha == jnp.inf, expm1_safe(loss_two),\n                              loss_ow))))\n\n\ndef general_lossfun_grad_grad(x, alpha, scale):\n  \"\"\"The gradient of the gradient of the general loss function.\"\"\"\n  eps = jnp.f"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, _, x_dot: jnp.exp(x) * x_dot,\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / x,\n      (np.nextafter(np.float32(-1), np.float32(0)), max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  \"\"\"\n  This function creates a safe version of the square root function that can handle edge cases more gracefully. It uses a helper function to generate this safe square root function, which includes a custom gradient for backpropagation in automatic differentiation contexts. The safe version ensures that the input is clamped between 0 and a maximum value to avoid invalid inputs like negative numbers.\n\n  Input-Output Arguments\n  :param x: The input value for which the safe square root will be computed. It is used as the argument for the square root function and its custom gradient computation.\n  :return: The result of applying the safe square root function to the input x. This includes the application of a custom gradient function for automatic differentiation purposes.\n  \"\"\"\n\n  \"\"\"\n  This function creates a safe version of the square root function that can handle edge cases more gracefully. It uses a helper function to generate this safe square root function, which includes a custom gradient for backpropagation in automatic differentiation contexts. The safe version ensures that the input is clamped between 0 and a maximum value to avoid invalid inputs like negative numbers.\n\n  Input-Output Arguments\n  :param x: The input value for which the safe square root will be computed. It is used as the argument for the square root function and its custom gradient computation.\n  :return: The result of applying the safe square root function to the input x. This includes the application of a custom gradient function for automatic differentiation purposes.\n  \"\"\"\n\n  \"\"\"\n  This function creates a safe version of the square root function that can handle edge cases more gracefully. It uses a helper function to generate this safe square root function, which includes a custom gradient for backpropagation in automatic differentiation contexts. The safe version ensures that the input is clamped between 0 and a maximum value to avoid invalid inputs like negative numbers.\n\n  Input-Output Arguments\n  :param x: The input value for which the safe square root will be computed. It is used as the argument for the square root function and its custom gradient computation.\n  :return: The result of applying the safe square root function to the input x. This includes the application of a custom gradient function for automatic differentiation purposes.\n  \"\"\"\n\n  \"\"\"\n  This function creates a safe version of the square root function that can handle edge cases more gracefully. It uses a helper function to generate this safe square root function, which includes a custom gradient for backpropagation in automatic differentiation contexts."}
{"namespace": "math.power_ladder_max_output", "completion": "  return jnp.where(\n      p == -jnp.inf, -expm1_safe(-x),\n      jnp.where(\n          p == 0, jnp.log1p(x),\n          jnp.where(p == 2, x, jnp.where(p == jnp.inf, expm1_safe(x),\n                                            (jnp.abs(p - 1) / p) * ((x / jnp.abs(p - 1) + 1)**p - 1)))))\n\n"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array(\n        [[0, 0, 1], [0, 1, 0], [1, 0, 0], [-1, -1, -1]]\n    ).T\n    base_faces = np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array(\n        [\n            [0, 0, 1],\n            [0, 0, -1],\n            [0, 1, 0],\n            [0, -1, 0],\n            [1, 0, 0],\n            [-1, 0, 0],\n            [0.5, 0.8660254037844386, 0.0],\n            [0.5, -0.8660254037844386, 0.0],\n            [-0.5, 0.8660254037844386, 0.0],\n            [-0.5, -0.8660254037844386, 0.0],\n            [0.8660254037844386, 0.5, 0.0],\n            [-0.8660254037844386, 0.5, 0.0],\n            [0.8660254037844386, -0.5, 0.0],\n            [-0.8660254037844386, -0.5, 0.0],\n            [0.0, 0.5, 0.8660254037844386],\n            [0.0, -0.5, 0.8660254037844386],\n            [0."}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (0, np.nextafter(np.log1p(max_val), np.float32(0))),\n  )(x)\n\n"}
{"namespace": "math.power_ladder", "completion": "def learning_rate_decay(\n    step, lr_init, lr_final, max_steps, lr_delay_steps=0, lr_delay_mult=1\n):\n  \"\"\"Continuous learning rate decay function.\n\n  The returned rate is lr_init when step=0 and lr_final when step=max_steps, and\n  is log-linearly interpolated elsewhere (equivalent to exponential decay).\n  If lr_delay_steps>0 then the learning rate will be scaled by some smooth\n  function of lr_delay_mult, such that the initial learning rate is\n  lr_init*lr_delay_mult at the beginning of optimization but will be eased back\n  to the normal learning rate when steps>lr_delay_steps.\n\n  Args:\n    step: int, the current optimization step.\n    lr_init: float, the initial learning rate.\n    lr_final: float, the final learning rate.\n    max_steps: int, the number of steps during optimization.\n    lr_delay_steps: int, the number of steps to delay the full learning rate.\n    lr_delay_mult: float, the multiplier on the rate when delaying it.\n\n  Returns:\n    lr: the learning for current step 'step'.\n  \"\"\"\n  if lr_delay_steps > 0:\n    # A kind of reverse cosine decay.\n    delay_rate = lr_delay_mult + (1 - lr_delay_mult) * jnp.sin(\n        0.5 * jnp.pi * jnp.clip(step / lr_delay_steps, 0, 1)\n    )\n  else:\n    delay_rate = 1.0\n  return delay_rate * log_lerp(step / max_steps, lr_init, lr_final)\n\n\ndef override_gradient(fval, bval):\n  \"\"\"Use `fval` in the forward pass but `bval` in the backward pass.\"\"\"\n  # Note that the parentheses are needed to avoid catastrophic cancellation.\n  return jax.lax.stop_gradient(fval) + (bval - jax.lax.stop_gradient(bval))\n\n\ndef power_ladder_grad(x, p, premult=None, postmult=None):\n  \"\"\"The gradient of `power_ladder()`.\"\"\"\n  if"}
{"namespace": "math.inv_power_ladder", "completion": "e_fn(x, y, x_dot)\n    return y, y_dot\n\n  return safe_fn\n\n\ndef safe_log(x):\n  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / x,\n      (tiny_val, max_val),\n  )(x)\n\n\ndef safe_exp(x):\n  return generate_safe_fn(\n      jnp.exp,\n      lambda _, y, x_dot: y * x_dot,\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)\n\n\ndef safe_sqrt(x):\n  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, _, x_dot: 0.5 * x_dot / jnp.sqrt(jnp.maximum(tiny_val, x)),\n      (0, max_val),\n  )(x)\n\n\ndef safe_log1p(x):\n  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + x),\n      (np.nextafter(np.float32(-1), np.float32(0)), max_val),\n  )(x)\n\n\ndef safe_expm1(x):\n  return generate_safe_fn(\n      expm1,  # Note that we wrap around our more accurate expm1.\n      lambda x, _, x_dot: jnp.exp(x) * x_dot,\n      (min_val, np.nextafter(np.log1p(max_val), np.float32(0))),\n  )(x)\n\n\ndef safe_arccos(x):\n  \"\"\"jnp.arccos(x) where x is clipped to [-1, 1].\"\"\"\n  y = jnp.arccos(jnp.clip(x, plus_eps(-1), minus_eps(1)))\n  return jnp.where(x >= 1, 0, jnp.where(x <= -1, jnp.pi, y))\n\n\ndef apply_fn_to_grad(grad_fn):\n  \"\"\"Applies a scalar `"}
{"namespace": "math.learning_rate_decay", "completion": "# coding=utf-8\n# Copyright 2022 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Tests for the model.\"\"\"\n\nfrom absl.testing import parameterized\nimport numpy as np\nimport tensorflow as tf\n\nfrom dedal import model\n\n\nclass ModelTest(tf.test.TestCase, parameterized.TestCase):\n\n  def setUp(self):\n    super().setUp()\n    self.batch_size = 2\n    self.num_classes = 3\n    self.num_features = 4\n    self.num_samples = 5\n    self.num_samples_per_class = 6\n    self.num_samples_per_class_per_batch = 7\n    self.num_samples_per_class_per_batch_per_feature = 8\n    self.num_samples_per_class_per_batch_per_feature_per_class = 9\n    self.num_samples_per_class_per_batch_per_feature_per_class_per_sample = 10\n    self.num_samples_per_class_per_batch_per_feature_per_class_per_sample_per_class = 11\n    self.num_samples_per_class_per_batch_per_feature_per_class_per_sample_per_class_per_feature = 12\n    self.num_samples_per_class_per_batch_per_feature_per_class_per_sample_per_class_per_feature_per_sample = 13\n    self.num_samples_per_class_per_batch_per_feature_per_class_per_sample_per_class"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=None,\n      n=1,\n      origin_lo=0.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=0.0,\n      far_hi=1.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "camera_utils.points_to_pixels", "completion": "_length = jnp.where(\n      sigma_focal_length > 0.0,\n      jnp.log(1.0 + sigma_focal_length),\n      jnp.log(1.0 / (1.0 + sigma_focal_length)),\n  )\n  # Perturb focal length.\n  cameras = jaxcam.update_focal_length(\n      cameras,\n      cameras.focal_length + jnp.exp(rng.normal(log_sigma_focal_length.shape))\n      * log_sigma_focal_length,\n  )\n\n  # Perturb principal point.\n  cameras = jaxcam.update_principal_point(\n      cameras,\n      cameras.principal_point\n      + jnp.exp(rng.normal(sigma_position.shape)) * sigma_position,\n  )\n\n  # Perturb look-at position.\n  cameras = jaxcam.update_look_at_position(\n      cameras,\n      cameras.look_at_position\n      + jnp.exp(rng.normal(sigma_look_at.shape)) * sigma_look_at,\n  )\n\n  # Perturb orientation.\n  cameras = jaxcam.update_world_to_camera_matrix(\n      cameras,\n      cameras.world_to_camera_matrix\n      * jnp.exp(rng.normal(sigma_look_at.shape)) * sigma_look_at,\n  )\n\n  return cameras\n\n\ndef perturb_camera_tuple(\n    rng,\n    camera_tuple,\n    sigma_look_at,\n    sigma_position,\n    sigma_focal_length = 0.0,\n    sigma_dolly_z = 0.0,\n    single_dolly = True,\n    dolly_use_average = False,\n):\n  \"\"\"Randomly perturb camera positions and orientations.\n\n  For position the 3D coordinate is simply shifted according to\n  an offset vector. For the orientation an offset angle is calculated based\n  on spherical coordinates. The underlying offsets are randomly chosen using\n  normal distributions absed on the input sigmas.\n\n  Args:\n    rng: A PRNGKey.\n    camera_tuple: A tuple containing `inv_intrinsics`, the inverse intrinsics\n      matrix; `extrinsics`, the camera to world matrix; and `distortion_params`,\n      the dictionary of distortion parameters"}
{"namespace": "rigid_body.exp_se3", "completion": "  w, v = screw_axis[:3], screw_axis[3:]\n  theta_squared = jnp.sum(w**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  X_taylor = rp_to_se3(jnp.eye(3), v)\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta_squared > eps**2, w, 0.0)\n  theta_safe = jnp.where(theta_squared > eps**2, theta, 1.0)\n  W = skew(w_safe)\n  R = exp_so3(w_safe, eps)\n  G = (\n      jnp.eye(3)\n      + (1.0 - jnp.cos(theta_safe)) / theta_safe * W\n      + (theta_safe - jnp.sin(theta_safe)) / theta_safe**2 * spin_math.matmul(W, W)\n  )\n  p = spin_math.matmul(G, v[Ellipsis, jnp.newaxis]).squeeze(-1)\n  # If theta = 0 then the transformation is a pure translation and p = v.\n  # This avoids using the numerically unstable G matrix when theta is near zero.\n  p = jnp.where(theta_squared > eps, p, v)\n  X = rp_to_se3(R, p)\n\n  return jnp.where(theta_squared > eps**2, X, X_taylor)\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = optax.safe_norm(axis_angle, min_norm=eps)\n  w = axis_angle / theta\n  W = skew(w)\n  R = jnp.eye(3) + jnp.sin(theta) * W + (1.0 - jnp.cos(theta)) * spin_math.matmul(\n      W, W\n  )\n  return R"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "_z)\n    else:\n      new_z = z + random.normal(dolly_key, (1,)) * sigma_dolly_z\n\n  # Position.\n  if sigma_position > 0.0:\n    rng, position_key = random.split(rng)\n    position = cameras.position\n    x, y, z = jnp.split(position, 3, -1)\n    new_x = x + random.normal(position_key, (1,)) * sigma_position\n    new_y = y + random.normal(position_key, (1,)) * sigma_position\n    new_position = jnp.stack([new_x, new_y, new_z], axis=-1)\n\n  # Focal length.\n  if sigma_focal_length > 0.0:\n    rng, focal_length_key = random.split(rng)\n    new_focal_length = cameras.focal_length + random.normal(\n        focal_length_key, (1,)\n    ) * sigma_focal_length\n\n  # Look-at.\n  if sigma_look_at > 0.0:\n    rng, look_at_key = random.split(rng)\n    new_look_at = cameras.look_at + random.normal(look_at_key, (3,)) * sigma_look_at\n\n  # Radial distortion.\n  if cameras.has_radial_distortion:\n    rng, radial_distortion_key = random.split(rng)\n    new_radial_distortion = cameras.radial_distortion + random.normal(\n        radial_distortion_key, (4,)\n    ) * sigma_look_at\n\n  # Tangential distortion.\n  if cameras.has_tangential_distortion:\n    rng, tangential_distortion_key = random.split(rng)\n    new_tangential_distortion = cameras.tangential_distortion + random.normal(\n        tangential_distortion_key, (2,)\n    ) * sigma_look_at\n\n  # Principal point.\n  if sigma_principal_point > 0.0:\n    rng, principal_point_key = random.split(rng)\n    new_principal_point = cameras.principal_point + random.normal(\n        principal_point_"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist / jnp.linalg.norm(dirs, axis=-1)\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 0:\n    raise ValueError(f'num_samples must be > 0, is {num_samples}.')\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Generate random samples from the CDF.\n  if rng is None:\n    # Use linspace sampling if rng is None.\n    u = jnp.linspace(0, 1, num_samples, axis=-1)\n  else:\n    # Generate random samples from the uniform distribution.\n    u = jax.random.uniform(rng, (num_samples,), minval=eps, maxval=1 - eps)\n\n  # Interpolate into the inverse CDF.\n  t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n\n  # Jitter the samples if desired.\n  if single_jitter:\n    # Jitter all samples by the same amount.\n    jitter = jax.random.uniform(rng, (1,), minval=eps, maxval=1 - eps)\n    t_new = t_new + jitter\n  else:\n    # Jitter each sample independently.\n    jitter = jax.random.uniform(rng, (num_samples,), minval=eps, maxval=1 - eps)\n    t_new = t_new + jitter\n\n  # If desired, center the samples in each interval.\n  if deterministic_center:\n    t_new = (t_new + t[Ellipsis, 1:]) / 2\n\n  return t_new\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample points from the step function.\n  t_samples = sample(\n      rng, t, w_logits, num_samples, single_jitter, deterministic_center=True\n  )\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[..., 1:] + t_samples[..., :-1]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_midpoints = jnp.concatenate(\n      [\n          jnp.maximum(t_midpoints[..., :1], domain[0]),\n          t_midpoints,\n          jnp.minimum(t_midpoints[..., -1:], domain[1]),\n      ],\n      axis=-1,\n  )\n\n  # Calculate the intervals by subtracting the midpoints from the adjacent samples.\n  t_intervals = t_samples - t_midpoints\n\n  return t_intervals\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Compute the integrated weights.\n  cw = integrate_weights(w)\n  # Interpolate into the integrated weights to get the weighted percentiles.\n  return math.sorted_interp(ps, cw, t, utils.device_is_tpu())\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Convert the weights to a PDF.\n  p = weight_to_pdf(t, w)\n  # Blur the PDF.\n  p_blurred = linspline.blur(p, blur_halfwidth)\n  # Resample the PDF to the new time points.\n  w_resampled = resample(tq, t, p_blurred, use_avg=True)\n  return w_resampled\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(to_homogeneous(vectors), transform))\n\n"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n  # Compute the cumulative sum of the original step function.\n  vp_cumsum = jnp.cumsum(vp, axis=-1)\n  # Compute the cumulative sum of the original step function at the new time points.\n  vp_cumsum_resampled = jnp.interp(t, tp, vp_cumsum)\n  # Compute the difference between consecutive cumulative sums to get the resampled values.\n  vp_resampled = jnp.diff(vp_cumsum_resampled, axis=-1)\n  if use_avg:\n    # Compute the width of each interval in t.\n    t_diff = jnp.diff(t, axis=-1)\n    # Divide the resampled values by the interval width to get the average value.\n    vp_resampled = vp_resampled / t_diff\n  return vp_resampled\n\n"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Compute the mean of sin(x), x ~ N(mean, var).\n  mean_sin = expected_sin(mean, var)\n  # Compute the mean of sin(x), x ~ N(mean, var).\n  mean_sin_var = jnp.exp(-0.5 * var) * mean_sin\n  # Compute the mean of sin(x), x ~ N(mean, var).\n  mean_sin_var_var = jnp.exp(-0.5 * var) * mean_sin_var\n  # Compute the mean of sin(x), x ~ N(mean, var).\n  mean_sin_var_var_var = jnp.exp(-0.5 * var) * mean_sin_var_var\n  # Compute the mean of sin(x), x ~ N(mean, var).\n  mean_sin_var_var_var_var = jnp.exp(-0.5 * var) * mean_sin_var_var_var\n  # Compute the mean of sin(x), x ~ N(mean, var).\n  mean_sin_var_var_var_var_var = jnp.exp(-0.5 * var) * mean_sin_var_var_var_var\n  # Compute the mean of sin(x), x ~ N(mean, var).\n  mean_sin_var_var_var_var_var_var = jnp.exp(-0.5 * var) * mean_sin_var_var_var_var_var\n  # Compute the mean of sin(x), x ~ N(mean, var).\n  mean_sin_var_var_var_var_var_var_var = jnp.exp(-0.5 * var) * mean_sin_var_var_var_var_var_var\n  # Compute the mean of sin(x), x ~ N(mean, var).\n  mean_sin_var_var_var_var_var_var_var_var = jnp.exp(-0.5 * var) * mean_sin_var_var_var_var_var_var_var\n  # Compute the mean of sin(x), x ~ N(mean, var).\n  mean_sin_var_var_var_var_var_var_var_"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz, kappa_inv):\n    \"\"\"Function returning directional encoding (DE).\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n      kappa_inv: [..., 1] reciprocal of the concentration parameter of the von\n        Mises-Fisher distribution.\n\n    Returns:\n      An array with the resulting DE.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Apply attenuation function using the von Mises-Fisher distribution\n    # concentration parameter, kappa.\n    sigma = 0.5 * ml_array[1, :] * (ml_array[1, :] + 1)\n    ide = sph_harms * jnp.exp(-sigma * kappa_inv)\n\n    # Split"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "def check_parentheses(text):\n    return text.count(\"(\") == text.count(\")\")\n\n\ndef line_list_check(prev_line, curr_line, list_char):\n    prev_line_list = prev_line.visual_line.text_list\n    curr_line_list = curr_line.visual_line.text_list\n    prev_line_list_len = len(prev_line_list)\n    curr_line_list_len = len(curr_line_list)\n    prev_line_list_single_ent = prev_line_list[0]\n    curr_line_list_single_ent = curr_line_list[0]\n    prev_line_list_single_ent_len = len(prev_line_list_single_ent)\n    curr_line_list_single_ent_len = len(curr_line_list_single_ent)\n    prev_line_list_single_ent_list = prev_line_list_single_ent.split()\n    curr_line_list_single_ent_list = curr_line_list_single_ent.split()\n    prev_line_list_single_ent_list_len = len(prev_line_list_single_ent_list)\n    curr_line_list_single_ent_list_len = len(curr_line_list_single_ent_list)\n    prev_line_list_single_ent_list_single_ent = prev_line_list_single_ent_list[0]\n    curr_line_list_single_ent_list_single_ent = curr_line_list_single_ent_list[0]\n    prev_line_list_single_ent_list_single_ent_len = len(\n        prev_line_list_single_ent_list_single_ent\n    )\n    curr_line_list_single_ent_list_single_ent_len = len(\n        curr_line_list_single_ent_list_single_ent\n    )\n    prev_line_list_single_ent_list_single_ent_list = prev_line_list_single_ent_list_single_ent.split()\n    curr_line_list_single_ent_list"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "e_text_from_blocks(blocks, debug=False):\n    block_texts = []\n    block_info = []\n    for block in blocks:\n        block_type = block[\"block_type\"]\n        if (\n            block_type == \"list_item\"\n            or block_type == \"para\"\n            or block_type == \"numbered_list_item\"\n            or block_type == \"header\"\n        ):\n            block_texts.append(block[\"block_text\"])\n            block_info.append(block)\n    return block_texts, block_info\n\n\ndef get_block_texts(blocks):\n    block_texts = []\n    block_info = []\n    for block in blocks:\n        block_type = block[\"block_type\"]\n        if (\n            block_type == \"list_item\"\n            or block_type == \"para\"\n            or block_type == \"numbered_list_item\"\n            or block_type == \"header\"\n        ):\n            block_texts.append(block[\"block_text\"])\n            block_info.append(block)\n    return block_texts, block_info\nimport json\nimport re\n\nimport numpy as np\nfrom nltk import load\nfrom nltk import PunktSentenceTokenizer\n\n\nnltk_abbs = load(\"tokenizers/punkt/{}.pickle\".format(\"english\"))._params.abbrev_types\n\n\nclass NpEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        if isinstance(obj, np.floating):\n            return float(obj)\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        return super(NpEncoder, self).default(obj)\n\n\nnlm_abbs = {\n    \"u.s\",\n    \"u.s.a\",\n    \"n.w\",\n    \"p.o\",\n    \"po\",\n    \"st\",\n    \"ave\",\n    \"blvd\",\n    \"ctr\",\n    \"cir\",\n    \"ct\",\n    \"dr\",\n    \"mtn\",\n    \"apt\",\n    \"hwy\",\n    \"esq\",\n    \"fig\",\n    \"no\",\n    \"sec\",\n    \"n.a\",\n    \"s.a.b\",\n    \"non-u.s\",\n   "}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": ":\n        token = self._check_token_arg(token)\n        if isinstance(token, list):\n            return self.phrase_positions(token, key=key)\n        else:\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                return self.posns.positions(term_id, key=key)\n            except TermMissingError:\n                return np.zeros((len(self), 0))\n\n    def phrase_positions(self, tokens: List[str], key=None) -> np.ndarray:\n        if key is None:\n            key = np.arange(len(self))\n        if isinstance(tokens[0], str):\n            tokens = [tokens]\n        if len(tokens) == 0:\n            return np.zeros((len(self), 0))\n\n        # Get term ids\n        term_ids = [self.term_dict.get_term_id(token) for token in tokens]\n        return self.posns.phrase_positions(term_ids, key=key)\n\n    def phrase_positions_scan(self, tokens: List[str], key=None, slop=1) -> np.ndarray:\n        if key is None:\n            key = np.arange(len(self))\n        if isinstance(tokens[0], str):\n            tokens = [tokens]\n        if len(tokens) == 0:\n            return np.zeros((len(self), 0))\n\n        # Get term ids\n        term_ids = [self.term_dict.get_term_id(token) for token in tokens]\n        return self.posns.phrase_positions_scan(term_ids, key=key, slop=slop)\n\n    def phrase_positions_every_diff(self, tokens: List[str], key=None, slop=1) -> np.ndarray:\n        if key is None:\n            key = np.arange(len(self))\n        if isinstance(tokens[0], str):\n            tokens = [tokens]\n        if len(tokens) == 0:\n            return np.zeros((len(self), 0))\n\n        # Get term ids\n        term_ids = [self.term_dict.get_term_id(token) for token in tokens]\n        return self.posns.phrase_positions_every_diff"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec is None:\n        return 0\n\n    if spec.endswith(\"%\"):\n        return int(num_clauses * float(spec[:-1]) / 100)\n\n    if spec.startswith(\"<\"):\n        return int(num_clauses * float(spec[1:]) / 100)\n\n    return int(spec)\n\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(tokens) == len(set(tokens)):\n            return self.phrase_freq_scan(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if isinstance(scalars, np.ndarray) and scalars.dtype == TermsDtype():\n            return cls(scalars)\n        # String types\n        elif isinstance(scalars, np.ndarray) and scalars.dtype.kind in 'US':\n            return cls(scalars)\n        # Other objects\n        elif isinstance(scalars, np.ndarray) and scalars.dtype != object:\n            return scalars\n        return cls(scalars)\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(\n            host=self.config['serverHost'],\n            port=self.config['serverPort'],\n            proxy_host=self.config['proxyHost'],\n            proxy_port=self.config['proxyPort'],\n            strategy=self.config['strategy'],\n            strategies=self.config['strategies'],\n            logger=self.logger,\n        )\n        self.server.start()\n\n        self.connections = {}\n        self.lock = threading.Lock()\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = np.uint64(arr)\n    arr = np.uint64(arr - ((arr >> _1) & s55))\n    arr = np.uint64(arr - ((arr >> _2) & s33))\n    arr = np.uint64(arr - ((arr >> _4) & s0F))\n    arr = np.uint64(arr & s01)\n    return arr\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    if not qf:\n        raise ValueError(\"Must specify at least one query field\")\n\n    query_fields = parse_field_boosts(qf)\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, query_fields)\n\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    if pf:\n        pf_fields = parse_field_boosts(pf)\n        num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, pf_fields)\n        if term_centric:\n            pf_scores, explain = _edismax_term_centric(frame, pf_fields, num_search_terms, search_terms, mm, similarity)\n        else:\n            pf_scores, explain = _edismax_field_centric(frame, pf_fields, num_search_terms, search_terms, mm, similarity)\n        qf_scores = np.maximum(qf_scores, pf_scores)\n        explain = f\"{explain} | {explain}\"\n\n    if pf2:\n        pf2_fields = parse_field_boosts(pf2)\n        num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, pf2_fields)\n        if term_centric:\n            pf2_scores, explain = _edismax_term_centric(frame, pf2_fields, num_search_terms, search_terms, mm, similarity)\n        else:\n            pf2_scores, explain = _edismax_field_centric(frame, pf2_fields, num_search_terms, search_terms, mm, similarity)\n        qf_scores = np.maximum(qf_scores, pf2_scores)\n        explain = f\"{explain} | {explain}\"\n\n    if pf3:\n        pf3_fields = parse_field_boosts(pf3)\n        num_search_terms, search_terms, term_centric = parse_query_terms(frame,"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        for connection in self.connections.values():\n            connection.stop()\n\n        # Stop the server\n        self.server.stop()"}
