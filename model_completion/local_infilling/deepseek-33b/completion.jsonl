{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Executing command: {cmd}\")\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Executing command: {cmd}\")\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Executing command: {cmd}\")\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Executing command: {cmd}\")\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Executing command: {cmd}\")\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Executing command: {cmd}\")\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Executing command: {cmd}\")\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Executing command: {cmd}\")\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Executing command: {cmd}\")\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Executing command: {cmd}\")\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Running command: {cmd}\")\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Executing: {cmd}\")\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Executing command: {cmd}\")\n    return os.system(cmd)"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Executing command: {cmd}\")\n    os.system(cmd)\n    return 0"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Executing command: {cmd}\")\n    os.system(cmd)\n    return os.WEXITSTATUS(os.system(cmd))"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Executing command: {cmd}\")\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Running command: {cmd}\")\n    os.system(cmd)\n    return os.WEXITSTATUS(os.system(cmd))"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Running command: {cmd}\")\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Executing command: {cmd}\")\n    return os.system(cmd)\n\n"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Executing command: {cmd}\")\n    return os.system(cmd)\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "coord.inv_contract", "completion": "  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (1 + jnp.sqrt(1 + 4 * z_mag_sq)) / (2 * z_mag_sq)\n  x = scale * z\n  return x"}
{"namespace": "coord.inv_contract", "completion": "  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (jnp.sqrt(1 + 4 * z_mag_sq) - 1) / (2 * z_mag_sq)\n  x = scale * z\n  return x"}
{"namespace": "coord.inv_contract", "completion": "  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (jnp.sqrt(1 + 4 * z_mag_sq) - 1) / (2 * z_mag_sq)\n  x = scale * z\n  return x"}
{"namespace": "coord.inv_contract", "completion": "  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (jnp.sqrt(1 + 4 * z_mag_sq) - 1) / (2 * z_mag_sq)\n  x = scale * z\n  return x"}
{"namespace": "coord.inv_contract", "completion": "  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (1 + jnp.sqrt(1 + 4 * z_mag_sq)) / (2 * z_mag_sq)\n  x = scale * z\n  return x"}
{"namespace": "coord.inv_contract", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (1 + jnp.sqrt(1 + 4 * x_mag_sq)) / (2 * x_mag_sq)\n  x = scale * z\n  return x"}
{"namespace": "coord.inv_contract", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (1 + jnp.sqrt(1 + 4 * x_mag_sq)) / (2 * x_mag_sq)\n  x = scale * z\n  return x"}
{"namespace": "coord.inv_contract", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (jnp.sqrt(x_mag_sq) + 1) / (2 * jnp.sqrt(x_mag_sq))\n  x = scale * z\n  return x"}
{"namespace": "coord.inv_contract", "completion": "  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (jnp.sqrt(z_mag_sq) + 1) / (2 * jnp.sqrt(z_mag_sq))\n  x = scale * z\n  return x"}
{"namespace": "coord.inv_contract", "completion": "  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (1 + jnp.sqrt(1 + 4 * z_mag_sq)) / (2 * z_mag_sq)\n  x = scale * z\n  return x"}
{"namespace": "coord.inv_contract", "completion": "  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (1 + jnp.sqrt(1 + 4 * z_mag_sq)) / (2 * z_mag_sq)\n  x = scale * z\n  return x"}
{"namespace": "coord.inv_contract", "completion": "  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (1 - 0.5 * z_mag_sq) / (1 - z_mag_sq)\n  x = scale * z\n  return x"}
{"namespace": "coord.inv_contract", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (1 - jnp.sqrt(1 - 2 * x_mag_sq)) / x_mag_sq\n  x = scale * z\n  return x"}
{"namespace": "coord.inv_contract", "completion": "  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (jnp.sqrt(2 * z_mag_sq + 1) - 1) / z_mag_sq\n  x = scale * z\n  return x"}
{"namespace": "coord.inv_contract", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = jnp.sqrt(1 - 2 / x_mag_sq)\n  x = scale * z\n  return x"}
{"namespace": "coord.inv_contract", "completion": "  # Compute the inverse of the contract function for a given input.\n  z_mag_sq = jnp.sum(z**2, axis=-1, keepdims=True)\n  scale = (1 + z_mag_sq) / (2 * jnp.sqrt(z_mag_sq))\n  x = scale * z\n  return x"}
{"namespace": "coord.inv_contract", "completion": "  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (jnp.sqrt(1 + 4 * z_mag_sq) - 1) / (2 * z_mag_sq)\n  x = scale * z\n  return x\n\n"}
{"namespace": "coord.inv_contract", "completion": "  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (jnp.sqrt(z_mag_sq) + 1) / (jnp.sqrt(z_mag_sq) - 1)\n  x = scale * z\n  return x\n\n"}
{"namespace": "coord.inv_contract", "completion": "  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (1 + z_mag_sq) / (2 * jnp.sqrt(z_mag_sq))\n  x = scale * z\n  return x\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Compute the inverse of the contract function for the input vector z\n  # The inverse of the contract function is defined as follows:\n  # inv_contract(z) = z / (1 + sqrt(1 + 4 * (z**2 - 1) / (z**2)))\n  # where z is the input vector\n  # The inverse of the contract function is computed using the above formula\n  inv_z = z / (1 + jnp.sqrt(1 + 4 * (z**2 - 1) / (z**2)))\n\n  return inv_z\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as memo:\n                return memo.fetch_or_compute(func, func_name, *args, **kwargs)\n        return wrapper\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as memo:\n                return memo.fetch_or_compute(func, func_name, *args, **kwargs)\n        return wrapper\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as memo:\n                return memo.fetch_or_compute(func, func_name, *args, **kwargs)\n        return wrapper\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as memo:\n                return memo.fetch_or_compute(func, func_name, *args, **kwargs)\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as memo:\n                return memo.fetch_or_compute(func, func_name, *args, **kwargs)\n        return wrapper\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as memo:\n                return memo.fetch_or_compute(func, func_name, *args, **kwargs)\n        return wrapper\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as memo:\n                return memo.fetch_or_compute(func, func_name, *args, **kwargs)\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as cache:\n                return cache.fetch_or_compute(func, func_name, *args, **kwargs)\n        return wrapper\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as cache:\n                return cache.fetch_or_compute(func, func_name, *args, **kwargs)\n        return wrapper\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as cache:\n                return cache.fetch_or_compute(func, func_name, *args, **kwargs)\n        return wrapper\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as cache:\n                return cache.fetch_or_compute(func, func_name, *args, **kwargs)\n        return wrapper\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as memo:\n                return memo.fetch_or_compute(func, func_name, *args, **kwargs)\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as cache:\n                return cache.fetch_or_compute(func, func_name, *args, **kwargs)\n        return wrapper\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as cache:\n                return cache.fetch_or_compute(func, func_name, *args, **kwargs)\n        return wrapper\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as memo:\n                return memo.fetch_or_compute(func, func_name, *args, **kwargs)\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as memo:\n                return memo.fetch_or_compute(func, func_name, *args, **kwargs)\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as cache:\n                return cache.fetch_or_compute(func, func_name, *args, **kwargs)\n        return wrapper\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as cache:\n                return cache.fetch_or_compute(func, func_name, *args, **kwargs)\n\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as memo:\n                return memo.fetch_or_compute(func, func_name, *args, **kwargs)\n        return wrapper\n    return decorator"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as memoizer:\n                return memoizer.fetch_or_compute(func, func_name, *args, **kwargs)\n\n        return wrapper\n\n    return decorator"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"] or values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: Invalid bounding box values. \"\n            f\"x_min ({values['x_min']}) must be less than x_max ({values['x_max']}) \"\n            f\"and y_min ({values['y_min']}) must be less than y_max ({values['y_max']}).\"\n        )\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"] or values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: invalid bounding box: {values}\")\n\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"] or values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: Invalid bounding box: {values}\")\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"] or values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: Invalid bounding box values. \"\n            f\"x_min: {values['x_min']}, x_max: {values['x_max']}, y_min: {values['y_min']}, y_max: {values['y_max']}\"\n        )\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: x_min ({values['x_min']}) must be less than x_max ({values['x_max']})\"\n        )\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: y_min ({values['y_min']}) must be less than y_max ({values['y_max']})\"\n        )\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"] or values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: invalid bounding box. \"\n            f\"x_min={values['x_min']}, x_max={values['x_max']}, y_min={values['y_min']}, y_max={values['y_max']}\"\n        )\n\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"] or values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: Invalid bounding box values: {values}\")\n\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: x_min must be strictly less than x_max. Received {values['x_min']} and {values['x_max']}\"\n        )\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: y_min must be strictly less than y_max. Received {values['y_min']} and {values['y_max']}\"\n        )\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"] or values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: x_min, y_min must be lower than x_max, y_max respectively. \"\n            f\"Received {values}\"\n        )\n\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: x_min ({values['x_min']}) must be strictly less than x_max ({values['x_max']})\"\n        )\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: y_min ({values['y_min']}) must be strictly less than y_max ({values['y_max']})\"\n        )\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if not (values[\"x_min\"] < values[\"x_max\"] and values[\"y_min\"] < values[\"y_max\"]):\n        raise ValueError(\n            f\"{cls.__name__}: invalid bounding box. \"\n            f\"x_min ({values['x_min']}) must be smaller than x_max ({values['x_max']}) and \"\n            f\"y_min ({values['y_min']}) must be smaller than y_max ({values['y_max']}).\"\n        )\n\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"] or values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: bounding box is invalid. x_min={values['x_min']}, x_max={values['x_max']}, y_min={values['y_min']}, y_max={values['y_max']}\"\n        )\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"] or values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: invalid bounding box. \"\n            f\"x_min={values['x_min']} must be less than x_max={values['x_max']}, \"\n            f\"y_min={values['y_min']} must be less than y_max={values['y_max']}.\"\n        )\n\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values[\"x_min\"], values[\"x_max\"], values[\"y_min\"], values[\"y_max\"]\n\n    if x_min >= x_max:\n        raise ValueError(f\"{cls.__name__}: x_min must be smaller than x_max, got {x_min} and {x_max}.\")\n    if y_min >= y_max:\n        raise ValueError(f\"{cls.__name__}: y_min must be smaller than y_max, got {y_min} and {y_max}.\")\n\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"] or values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: x_min, x_max, y_min, y_max must be respectively lesser than x_max, y_max, x_max, y_max. \"\n            f\"Received {values}\"\n        )\n\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"] or values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: Invalid bounding box values. \"\n            f\"x_min={values['x_min']} must be less than x_max={values['x_max']} and y_min={values['y_min']} \"\n            f\"must be less than y_max={values['y_max']}\"\n        )\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"] or values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: bounding box values are invalid, \"\n            f\"x_min={values['x_min']} must be < x_max={values['x_max']}, \"\n            f\"y_min={values['y_min']} must be < y_max={values['y_max']}\"\n        )\n\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min, x_max, y_min, y_max = values[\"x_min\"], values[\"x_max\"], values[\"y_min\"], values[\"y_max\"]\n\n    if x_min >= x_max or y_min >= y_max:\n        raise ValueError(\n            f\"{cls.__name__}: bounding box values are invalid, \"\n            f\"got x_min={x_min}, x_max={x_max}, y_min={y_min}, y_max={y_max}\"\n        )\n\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] > values[\"x_max\"] or values[\"y_min\"] > values[\"y_max\"]:\n        raise ValueError(\n            f\"{cls.__name__}: Invalid bounding box, \"\n            f\"x_min: {values['x_min']} > x_max: {values['x_max']} \"\n            f\"or y_min: {values['y_min']} > y_max: {values['y_max']}\"\n        )\n    return values"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if not (\n        values[\"x_min\"] < values[\"x_max\"]\n        and values[\"y_min\"] < values[\"y_max\"]\n        and values[\"x_min\"] >= 0\n        and values[\"y_min\"] >= 0\n    ):\n        raise ValueError(f\"{cls.__name__}: bounding box values are invalid\")\n    return values"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  sq_dist = np.sum(mat0**2, 0, keepdims=True) - 2 * np.matmul(mat0.T, mat1) + np.sum(\n      mat1**2, 0, keepdims=True\n  )\n  sq_dist[sq_dist < 0] = 0\n  return sq_dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  mat0_norm = np.sum(mat0**2, 0, keepdims=True)\n  mat1_norm = np.sum(mat1**2, 0, keepdims=True)\n  sq_dist = mat0_norm - 2 * np.matmul(mat0.T, mat1) + mat1_norm.T\n  sq_dist = np.maximum(sq_dist, 0)\n  return sq_dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  sq_dist = np.sum(mat0**2, 0, keepdims=True) - 2 * np.matmul(mat0.T, mat1) + np.sum(\n      mat1**2, 0, keepdims=True\n  )\n  sq_dist[sq_dist < 0] = 0\n  return sq_dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  sq_norm0 = np.sum(mat0**2, 0, keepdims=True)\n  sq_norm1 = np.sum(mat1**2, 0, keepdims=True)\n  inner = np.matmul(mat0.T, mat1)\n  sq_dist = sq_norm0 - 2 * inner + sq_norm1\n  sq_dist[sq_dist < 0] = 0\n  return sq_dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  sq_norm0 = np.sum(mat0**2, 0, keepdims=True)\n  sq_norm1 = np.sum(mat1**2, 0, keepdims=True)\n  dot_prod = np.matmul(mat0.T, mat1)\n  sq_dist = sq_norm0 - 2 * dot_prod + sq_norm1.T\n  sq_dist = np.maximum(sq_dist, 0)\n  return sq_dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  sq_mat0 = np.sum(mat0**2, 0, keepdims=True)\n  sq_mat1 = np.sum(mat1**2, 0, keepdims=True)\n  sq_dist = sq_mat0 - 2 * np.matmul(mat0.T, mat1) + sq_mat1.T\n  sq_dist[sq_dist < 0] = 0\n  return sq_dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  sq_norm0 = np.sum(mat0**2, 0, keepdims=True)\n  sq_norm1 = np.sum(mat1**2, 0, keepdims=True)\n  sq_dist = sq_norm0 - 2 * np.matmul(mat0.T, mat1) + sq_norm1.T\n  sq_dist = np.clip(sq_dist, 0, None)\n  return sq_dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  norm0 = np.sum(mat0**2, 0, keepdims=True)\n  norm1 = np.sum(mat1**2, 0, keepdims=True)\n  inner = np.matmul(mat0.T, mat1)\n  dist = norm0 - 2 * inner + norm1.T\n  dist[dist < 0] = 0\n  return dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  sq_diff = np.sum(mat0**2, 0, keepdims=True) - 2 * np.matmul(mat0.T, mat1) + np.sum(\n      mat1**2, 0, keepdims=True\n  )\n  sq_diff[sq_diff < 0] = 0\n  return sq_diff"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  sq_norm0 = np.sum(mat0**2, 0, keepdims=True)\n  sq_norm1 = np.sum(mat1**2, 0, keepdims=True)\n  inner_prod = np.matmul(mat0.T, mat1)\n  sq_dist = sq_norm0 - 2 * inner_prod + sq_norm1.T\n  sq_dist[sq_dist < 0] = 0\n  return sq_dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  mat0_norms = np.sum(mat0**2, 0, keepdims=True)\n  mat1_norms = np.sum(mat1**2, 0, keepdims=True)\n  sq_dist = mat0_norms - 2 * np.matmul(mat0.T, mat1) + mat1_norms.T\n  sq_dist = np.maximum(sq_dist, 0)\n  return sq_dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  sq_norm0 = np.sum(mat0**2, 0, keepdims=True)\n  sq_norm1 = np.sum(mat1**2, 0, keepdims=True)\n  dot = np.matmul(mat0.T, mat1)\n  sq_dist = sq_norm0 - 2 * dot + sq_norm1\n  sq_dist = np.maximum(sq_dist, 0)\n  return sq_dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  sq_norm0 = np.sum(mat0**2, 0, keepdims=True)\n  sq_norm1 = np.sum(mat1**2, 0, keepdims=True)\n  inner_prod = np.matmul(mat0.T, mat1)\n  sq_dist = sq_norm0 - 2 * inner_prod + sq_norm1.T\n  sq_dist = np.clip(sq_dist, 0, None)\n  return sq_dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  norm0 = np.sum(mat0**2, 0, keepdims=True)\n  norm1 = np.sum(mat1**2, 0, keepdims=True)\n  dot = np.matmul(mat0.T, mat1)\n  sq_dist = norm0 - 2 * dot + norm1\n  sq_dist = np.maximum(sq_dist, 0)\n  return sq_dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  sq_dist = np.sum(mat0**2, 0, keepdims=True) + np.sum(\n      mat1**2, 0, keepdims=True\n  )\n  sq_dist -= 2 * np.matmul(mat0.T, mat1)\n  sq_dist = np.clip(sq_dist, 0, None)\n  return sq_dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  norm0 = np.sum(mat0**2, 0, keepdims=True)\n  norm1 = np.sum(mat1**2, 0, keepdims=True)\n  inner = np.matmul(mat0.T, mat1)\n  sq_dist = norm0 - 2 * inner + norm1\n  sq_dist = np.maximum(sq_dist, 0)\n  return sq_dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  norm0 = np.sum(mat0**2, 0)\n  norm1 = np.sum(mat1**2, 0)\n  dot = np.matmul(mat0.T, mat1)\n  sq_dist = np.expand_dims(norm0, 1) + np.expand_dims(norm1, 0) - 2 * dot\n  sq_dist = np.clip(sq_dist, 0, None)\n  return sq_dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  if mat0.shape[0] != mat1.shape[0]:\n    raise ValueError('mat0 and mat1 must have the same number of rows')\n\n  mat0_sq = np.sum(mat0**2, 0)\n  mat1_sq = np.sum(mat1**2, 0)\n  mat0_dot_mat1 = np.matmul(mat0.T, mat1)\n  sq_dist = np.maximum(mat0_sq[:, None] + mat1_sq[None, :] - 2 * mat0_dot_mat1, 0)\n\n  return sq_dist"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  if mat0.shape[0] != mat1.shape[0]:\n    raise ValueError('mat0 and mat1 must have the same number of rows')\n\n  dist_sq = np.sum(mat0**2, 0, keepdims=True) + np.sum(\n      mat1**2, 0, keepdims=True\n  ) - 2 * np.matmul(mat0.T, mat1)\n  dist_sq = np.maximum(dist_sq, 0)\n  return dist_sq"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  if mat0.shape[0] != mat1.shape[0]:\n    raise ValueError('mat0 and mat1 must have the same number of rows')\n\n  # Compute the squared norms of each column in mat0 and mat1.\n  norms0 = np.sum(mat0**2, axis=0)\n  norms1 = np.sum(mat1**2, axis=0)\n\n  # Compute the dot product between each pair of columns in mat0 and mat1.\n  dot_prods = np.matmul(mat0.T, mat1)\n\n  # Compute the squared Euclidean distance between each pair of columns in mat0 and mat1.\n  sq_dist = norms0[:, np.newaxis] + norms1[np.newaxis, :] - 2 * dot_prods\n\n  # Set negative distances to zero.\n  sq_dist = np.maximum(sq_dist, 0)\n\n  return sq_dist"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\":\n        return False\n    if path.startswith(\"s3://\") or path.startswith(\"gs://\") or path.startswith(\"hdfs://\"):\n        return True\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None:\n        return True\n    if path == \"\":\n        return True\n    if path.startswith(\"s3://\"):\n        return True\n    if path.startswith(\"gs://\"):\n        return True\n    if path.startswith(\"hdfs://\"):\n        return True\n    if path.startswith(\"http://\"):\n        return True\n    if path.startswith(\"https://\"):\n        return True\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None:\n        return True\n    if path == \"\":\n        return True\n    if path.startswith(\"s3://\"):\n        return True\n    if path.startswith(\"gs://\"):\n        return True\n    if path.startswith(\"http://\"):\n        return True\n    if path.startswith(\"https://\"):\n        return True\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None:\n        return True\n    if path == \"\":\n        return True\n    if path.startswith(\"s3://\"):\n        return True\n    if path.startswith(\"gs://\"):\n        return True\n    if path.startswith(\"wasbs://\"):\n        return True\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None:\n        return True\n    if path == \"\":\n        return True\n    if path.startswith(\"s3://\"):\n        return True\n    if path.startswith(\"gs://\"):\n        return True\n    if path.startswith(\"hf://\"):\n        return True\n    if path.startswith(\"http://\"):\n        return True\n    if path.startswith(\"https://\"):\n        return True\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None:\n        return True\n    if path == \"\":\n        return True\n    if path.startswith(\"http://\") or path.startswith(\"https://\"):\n        return True\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None:\n        return False\n\n    if path == \"\":\n        return False\n\n    if path.startswith(\"s3://\") or path.startswith(\"gs://\"):\n        return True\n\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None:\n        return False\n\n    if path == \"\":\n        return False\n\n    if path.startswith(\"s3://\"):\n        return False\n\n    if path.startswith(\"gs://\"):\n        return False\n\n    if path.startswith(\"hdfs://\"):\n        return False\n\n    if path.startswith(\"http://\"):\n        return False\n\n    if path.startswith(\"https://\"):\n        return False\n\n    return True"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None:\n        return True\n    if path == \"\":\n        return True\n    if path.startswith(\"gs://\"):\n        return True\n    if path.startswith(\"s3://\"):\n        return True\n    if path.startswith(\"hdfs://\"):\n        return True\n    if path.startswith(\"http://\"):\n        return True\n    if path.startswith(\"https://\"):\n        return True\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None:\n        return True\n\n    if path == \"\":\n        return True\n\n    return path.startswith(\"s3://\") or path.startswith(\"gs://\") or path.startswith(\"http://\") or path.startswith(\"https://\")"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None:\n        return True\n\n    if path == \"\":\n        return True\n\n    if path.startswith(\"s3://\") or path.startswith(\"gs://\") or path.startswith(\"hf://\"):\n        return True\n\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if not path:\n        return False\n\n    if path.startswith(\"s3://\") or path.startswith(\"gs://\"):\n        return True\n\n    if path.startswith(\"http://\") or path.startswith(\"https://\"):\n        return True\n\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None:\n        return True\n    if path == \"\":\n        return True\n    if path.startswith(\"s3://\"):\n        return True\n    if path.startswith(\"gs://\"):\n        return True\n    if path.startswith(\"https://\"):\n        return True\n    if path.startswith(\"http://\"):\n        return True\n    if path.startswith(\"file://\"):\n        return True\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None:\n        return True\n    if path == \"\":\n        return True\n    if path.startswith(\"s3://\"):\n        return True\n    if path.startswith(\"gs://\"):\n        return True\n    if path.startswith(\"hdfs://\"):\n        return True\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if not path:\n        return False\n    if path == \"\":\n        return False\n    if path.startswith(\"s3://\"):\n        return False\n    if path.startswith(\"gs://\"):\n        return False\n    if path.startswith(\"hf://\"):\n        return False\n    if path.startswith(\"http://\"):\n        return False\n    if path.startswith(\"https://\"):\n        return False\n    return True"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None:\n        return True\n\n    if not path:\n        return True\n\n    if path.startswith(\"s3://\") or path.startswith(\"gs://\") or path.startswith(\"https://\"):\n        return True\n\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if not path:\n        return True\n\n    if path == \"\":\n        return True\n\n    if path.startswith(\"s3://\") or path.startswith(\"gs://\") or path.startswith(\"hdfs://\"):\n        return True\n\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None:\n        return True\n    if path == \"\":\n        return True\n    if path.startswith(\"s3://\"):\n        return True\n    if path.startswith(\"gs://\"):\n        return True\n    if path.startswith(\"wasbs://\"):\n        return True\n    if path.startswith(\"hdfs://\"):\n        return True\n    return False\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\":\n        return False\n\n    # Check if the path starts with specific prefixes\n    prefixes = [\"/tmp/\", \"/home/\", \"/data/\", \"/mnt/\", \"/storage/\"]\n    for prefix in prefixes:\n        if path.startswith(prefix):\n            return True\n\n    # If none of the prefixes match, return False\n    return False"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\":\n        return False\n\n    # Check if the path starts with any of the specified prefixes\n    prefixes = [\n        \"s3://\",\n        \"gs://\",\n        \"http://\",\n        \"https://\",\n        \"hf://\",\n        \"lit://\",\n        \"s3a://\",\n        \"s3n://\",\n        \"s3p://\",\n        \"s3r://\",\n        \"s3s://\",\n        \"s3u://\",\n        \"s3v://\",\n        \"s3w://\",\n        \"s3x://\",\n        \"s3y://\",\n        \"s3z://\",\n        \"s3://\",\n        \"s3b://\",\n        \"s3c://\",\n        \"s3d://\",\n        \"s3e://\",\n        \"s3f://\",\n        \"s3g://\",\n        \"s3h://\",\n        \"s3i://\",\n        \"s3j://\",\n        \"s3k://\",\n        \"s3l://\",\n        \"s3m://\",\n        \"s3n://\",\n        \"s3o://\",\n        \"s3p://\",\n        \"s3q://\",\n        \"s3r://\",\n        \"s3s://\",\n        \"s3t://\",\n        \"s3u://\",\n        \"s3v://\",\n        \"s3w://\",\n        \"s3x://\",\n        \"s3y://\",\n        \"s3z://\",\n        \"s30://\",\n        \"s31://\",\n        \"s32://\",\n        \"s33://\",\n        \"s34://\",\n        \"s35://\",\n        \"s36://\",\n        \"s37://\",\n        \"s38://\",\n        \"s39://\",\n        \"s300://\",\n        \"s301://\",\n        \"s302://\",\n        \"s303://\",\n        \"s304://\",\n        \"s3"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            assets_names = np.array(list(items.keys()))\n        else:\n            assets_names = np.array(assets_names)\n        items = np.array([items.get(k, fill_value) for k in assets_names])\n\n    if dim == 1:\n        if items.ndim != 1:\n            raise ValueError(\n                f\"Expected a 1-dimensional array for {name}, got {items.ndim}\"\n            )\n        if len(items) != n_assets:\n            raise ValueError(\n                f\"Expected {n_assets} elements for {name}, got {len(items)}\"\n            )\n        return items\n\n    if dim == 2:\n        if items.ndim != 2:\n            raise ValueError(\n                f\"Expected a 2-dimensional array for {name}, got {items.ndim}\"\n            )\n        if items.shape[1] != n_assets:\n            raise ValueError(\n                f\"Expected {n_assets} columns for {name}, got {items.shape[1]}\"\n            )\n        return items\n\n    raise ValueError(f\"Expected dim to be 1 or 2, got {dim}\")"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            assets_names = np.asarray(list(items.keys()))\n        else:\n            assets_names = np.asarray(assets_names)\n        items = np.asarray([items.get(k, fill_value) for k in assets_names])\n    if dim == 1:\n        items = np.atleast_1d(items)\n        if items.ndim != 1:\n            raise ValueError(\n                f\"Expected {name} to be 1D array, got {items.ndim}D array instead\"\n            )\n        if items.size != n_assets:\n            raise ValueError(\n                f\"Expected {name} to be of size {n_assets}, got {items.size} instead\"\n            )\n    elif dim == 2:\n        items = np.atleast_2d(items)\n        if items.ndim != 2:\n            raise ValueError(\n                f\"Expected {name} to be 2D array, got {items.ndim}D array instead\"\n            )\n        if items.shape[1] != n_assets:\n            raise ValueError(\n                f\"Expected {name} to have {n_assets} columns, got {items.shape[1]} instead\"\n            )\n    else:\n        raise ValueError(f\"Expected dim to be 1 or 2, got {dim} instead\")\n    return items"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            assets_names = np.array(list(items.keys()))\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"The number of assets ({n_assets}) does not match the number of \"\n                f\"assets in {name} ({len(assets_names)}).\"\n            )\n        items = np.array(\n            [items.get(name, fill_value) for name in assets_names], dtype=object\n        )\n    if not isinstance(items, np.ndarray):\n        items = np.array(items, dtype=object)\n    if dim == 1:\n        if items.ndim != 1:\n            raise ValueError(\n                f\"{name} must be a 1D array-like or a dictionary, got {items.ndim}D.\"\n            )\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"The number of assets ({n_assets}) does not match the number of \"\n                f\"assets in {name} ({items.shape[0]}).\"\n            )\n    elif dim == 2:\n        if items.ndim != 2:\n            raise ValueError(\n                f\"{name} must be a 2D array-like or a dictionary, got {items.ndim}D.\"\n            )\n        if items.shape[1] != n_assets:\n            raise ValueError(\n                f\"The number of assets ({n_assets}) does not match the number of \"\n                f\"assets in {name} ({items.shape[1]}).\"\n            )\n    return items"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(f\"{name} must be a dictionary with asset names as keys\")\n        items = np.array([items.get(name, fill_value) for name in assets_names])\n\n    if not isinstance(items, np.ndarray):\n        items = np.array(items)\n\n    if items.ndim == 0:\n        items = np.array([items])\n\n    if items.ndim == 1:\n        if dim == 1:\n            if items.shape[0] != n_assets:\n                raise ValueError(\n                    f\"Expected {name} to have shape ({n_assets},), got {items.shape}\"\n                )\n        elif dim == 2:\n            items = items.reshape(1, -1)\n        else:\n            raise ValueError(f\"Expected dim to be 1 or 2, got {dim}\")\n    elif items.ndim == 2:\n        if dim == 1:\n            raise ValueError(f\"Expected {name} to have shape ({n_assets},), got {items.shape}\")\n        elif dim == 2:\n            if items.shape[1] != n_assets:\n                raise ValueError(\n                    f\"Expected {name} to have shape (n_groups, {n_assets}), got {items.shape}\"\n                )\n        else:\n            raise ValueError(f\"Expected dim to be 1 or 2, got {dim}\")\n    else:\n        raise ValueError(f\"Expected {name} to have 1 or 2 dimensions, got {items.ndim}\")\n\n    return items"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                \"When 'items' is a dictionary, 'assets_names' cannot be None\"\n            )\n        if len(items) != len(assets_names):\n            raise ValueError(\n                f\"The number of assets in 'items' ({len(items)}) must match the number of assets in 'assets_names' ({len(assets_names)})\"\n            )\n        items = np.asarray(\n            [items.get(asset, fill_value) for asset in assets_names], dtype=float\n        )\n    if not isinstance(items, np.ndarray):\n        items = np.asarray(items, dtype=float)\n    if dim == 1:\n        if items.ndim != 1:\n            raise ValueError(\n                f\"Expected a 1-dimensional array for {name}, got {items.ndim}\"\n            )\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"Expected a 1-dimensional array of shape ({n_assets},) for {name}, got {items.shape}\"\n            )\n    elif dim == 2:\n        if items.ndim != 2:\n            raise ValueError(\n                f\"Expected a 2-dimensional array for {name}, got {items.ndim}\"\n            )\n        if items.shape[1] != n_assets:\n            raise ValueError(\n                f\"Expected a 2-dimensional array of shape (n_groups, {n_assets}) for {name}, got {items.shape}\"\n            )\n    else:\n        raise ValueError(f\"Invalid dimension {dim} for {name}\")\n    return items"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                \"assets_names must be provided when items is a dictionary.\"\n            )\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"assets_names must be of length {n_assets} when items is a dictionary.\"\n            )\n        items = np.array([items.get(name, fill_value) for name in assets_names])\n    if isinstance(items, np.ndarray):\n        if items.ndim != dim:\n            raise ValueError(\n                f\"{name} must be of dimension {dim}, got {items.ndim}.\"\n            )\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"{name} must be of shape ({n_assets},) or ({n_assets}, n_groups),\"\n                f\" got {items.shape}.\"\n            )\n    else:\n        items = np.asarray(items)\n        if items.ndim != dim:\n            raise ValueError(\n                f\"{name} must be of dimension {dim}, got {items.ndim}.\"\n            )\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"{name} must be of shape ({n_assets},) or ({n_assets}, n_groups),\"\n                f\" got {items.shape}.\"\n            )\n    return items"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            assets_names = np.array(list(items.keys()))\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"The number of assets ({n_assets}) does not match the number of \"\n                f\"items in the dictionary ({len(assets_names)}).\"\n            )\n        items = np.array([items.get(k, fill_value) for k in assets_names])\n\n    if not isinstance(items, np.ndarray):\n        items = np.asarray(items)\n\n    if items.ndim != dim:\n        raise ValueError(\n            f\"The {name} must be a {dim}D array, got {items.ndim}D array instead.\"\n        )\n\n    if dim == 1:\n        if items.shape != (n_assets,):\n            raise ValueError(\n                f\"The {name} must have shape ({n_assets},), got {items.shape} \"\n                f\"instead.\"\n            )\n    else:\n        if items.shape[1] != n_assets:\n            raise ValueError(\n                f\"The {name} must have shape (n_groups, {n_assets}), got {items.shape} \"\n                f\"instead.\"\n            )\n    return items"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            assets_names = np.array(list(items.keys()))\n        else:\n            missing = [a for a in assets_names if a not in items]\n            for a in missing:\n                items[a] = fill_value\n        items = np.array([items[a] for a in assets_names])\n    if dim == 1:\n        if items.ndim == 2:\n            items = items.reshape(-1)\n        elif items.ndim > 2:\n            raise ValueError(\n                f\"{name} must be 1-dimensional or 2-dimensional, \"\n                f\"got {items.ndim}-dimensional array\"\n            )\n    elif dim == 2:\n        if items.ndim == 1:\n            items = items.reshape(1, -1)\n        elif items.ndim > 2:\n            raise ValueError(\n                f\"{name} must be 1-dimensional or 2-dimensional, \"\n                f\"got {items.ndim}-dimensional array\"\n            )\n    else:\n        raise ValueError(f\"dim must be 1 or 2, got {dim}\")\n    if items.shape[0] != n_assets:\n        raise ValueError(\n            f\"{name} must have {n_assets} rows, got {items.shape[0]} rows\"\n        )\n    return items"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"{name} is a dictionary, but no assets names were provided\"\n            )\n        if dim == 1:\n            arr = np.full(n_assets, fill_value)\n            for k, v in items.items():\n                if k in assets_names:\n                    arr[np.where(assets_names == k)] = v\n        elif dim == 2:\n            arr = np.full((n_assets, n_assets), fill_value)\n            for k, v in items.items():\n                if k in assets_names:\n                    arr[np.where(assets_names == k)] = v\n        else:\n            raise ValueError(f\"dim must be either 1 or 2, got {dim}\")\n    else:\n        arr = np.asarray(items)\n\n    if dim == 1:\n        if arr.ndim != 1:\n            raise ValueError(\n                f\"Expected {name} to be a 1-dimensional array, got {arr.ndim}\"\n            )\n        if arr.shape[0] != n_assets:\n            raise ValueError(\n                f\"Expected {name} to be of length {n_assets}, got {arr.shape[0]}\"\n            )\n    elif dim == 2:\n        if arr.ndim != 2:\n            raise ValueError(\n                f\"Expected {name} to be a 2-dimensional array, got {arr.ndim}\"\n            )\n        if arr.shape[0] != n_assets or arr.shape[1] != n_assets:\n            raise ValueError(\n                f\"Expected {name} to be of shape ({n_assets}, {n_assets}), got {arr.shape}\"\n            )\n    else:\n        raise ValueError(f\"dim must be either 1 or 2, got {dim}\")\n\n    return arr"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"{name} is a dictionary, but no assets names were provided\"\n            )\n        items = np.asarray([items.get(x, fill_value) for x in assets_names])\n    elif isinstance(items, list):\n        items = np.asarray(items)\n    elif isinstance(items, pd.Series):\n        items = items.to_numpy()\n    if not isinstance(items, np.ndarray):\n        raise TypeError(f\"{name} must be a dictionary, list or numpy array\")\n    if dim == 1:\n        if items.ndim != 1:\n            raise ValueError(f\"{name} must be a 1-dimensional array\")\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"{name} must have {n_assets} elements, \"\n                f\"got {items.shape[0]} elements\"\n            )\n    elif dim == 2:\n        if items.ndim != 2:\n            raise ValueError(f\"{name} must be a 2-dimensional array\")\n        if items.shape[1] != n_assets:\n            raise ValueError(\n                f\"{name} must have {n_assets} columns, \"\n                f\"got {items.shape[1]} columns\"\n            )\n    return items"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                \"When 'items' is a dictionary, 'assets_names' must be provided.\"\n            )\n        if dim == 2:\n            raise ValueError(\n                f\"When 'items' is a dictionary, 'dim' must be equal to 1, got {dim}\"\n            )\n        items = np.asarray([items.get(k, fill_value) for k in assets_names])\n    else:\n        items = np.asarray(items)\n\n    if dim == 1:\n        if items.ndim != 1:\n            raise ValueError(\n                f\"When 'dim' is equal to 1, '{name}' must be a 1-dimensional array, \"\n                f\"got {items.ndim} dimensions.\"\n            )\n        if len(items) != n_assets:\n            raise ValueError(\n                f\"When 'dim' is equal to 1, '{name}' must have {n_assets} elements, \"\n                f\"got {len(items)} elements.\"\n            )\n    elif dim == 2:\n        if items.ndim != 2:\n            raise ValueError(\n                f\"When 'dim' is equal to 2, '{name}' must be a 2-dimensional array, \"\n                f\"got {items.ndim} dimensions.\"\n            )\n        if items.shape[1] != n_assets:\n            raise ValueError(\n                f\"When 'dim' is equal to 2, '{name}' must have {n_assets} columns, \"\n                f\"got {items.shape[1]} columns.\"\n            )\n    else:\n        raise ValueError(\n            f\"When 'dim' is equal to 1, '{name}' must be a 1-dimensional array, \"\n            f\"when 'dim' is equal to 2, '{name}' must be a 2-dimensional array, \"\n            f\"got {dim}.\"\n        )\n    return items"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"{name} is a dictionary, but no assets names were provided.\"\n            )\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"{name} is a dictionary, but the number of assets \"\n                f\"({len(assets_names)}) does not match the number of assets \"\n                f\"({n_assets}).\"\n            )\n        if dim == 1:\n            items = np.asarray([items.get(a, fill_value) for a in assets_names])\n        elif dim == 2:\n            items = np.asarray(\n                [[items.get(a, fill_value) for a in assets_names]] * n_assets\n            )\n        else:\n            raise ValueError(f\"Invalid dimension {dim}\")\n    else:\n        items = np.asarray(items)\n    if items.ndim == 1:\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"{name} is a 1D array, but the number of assets \"\n                f\"({items.shape[0]}) does not match the number of assets \"\n                f\"({n_assets}).\"\n            )\n    elif items.ndim == 2:\n        if items.shape[0] != n_assets or items.shape[1] != n_assets:\n            raise ValueError(\n                f\"{name} is a 2D array, but the shape \"\n                f\"({items.shape}) does not match the number of assets \"\n                f\"({n_assets}).\"\n            )\n    else:\n        raise ValueError(f\"{name} is not a 1D or 2D array.\")\n    return items"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            assets_names = np.asarray(list(items.keys()))\n        else:\n            assets_names = np.asarray(assets_names)\n        n_assets = len(assets_names)\n        if dim == 1:\n            array = np.full((n_assets,), fill_value)\n            for i, name in enumerate(assets_names):\n                if name in items:\n                    array[i] = items[name]\n        else:\n            array = np.full((len(items), n_assets), fill_value)\n            for i, name in enumerate(assets_names):\n                if name in items:\n                    array[:, i] = items[name]\n    else:\n        array = np.asarray(items)\n        if dim == 1:\n            if array.ndim != 1:\n                raise ValueError(f\"Expected a 1d array for {name}, got {array.ndim}d\")\n            if len(array) != n_assets:\n                raise ValueError(\n                    f\"Expected a 1d array of size {n_assets} for {name}, got {len(array)}\"\n                )\n        else:\n            if array.ndim != 2:\n                raise ValueError(f\"Expected a 2d array for {name}, got {array.ndim}d\")\n            if array.shape[1] != n_assets:\n                raise ValueError(\n                    f\"Expected a 2d array with {n_assets} columns for {name}, \"\n                    f\"got {array.shape[1]}\"\n                )\n    return array"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                \"When items is a dictionary, you must provide the assets names\"\n            )\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"The number of assets ({n_assets}) does not match the number of \"\n                f\"assets names ({len(assets_names)}).\"\n            )\n        if dim == 1:\n            arr = np.full(n_assets, fill_value, dtype=object)\n            for i, name in enumerate(assets_names):\n                if name in items:\n                    arr[i] = items[name]\n        else:\n            arr = np.full((n_assets, n_assets), fill_value, dtype=object)\n            for i, name1 in enumerate(assets_names):\n                if name1 in items:\n                    for j, name2 in enumerate(assets_names):\n                        if name2 in items[name1]:\n                            arr[i, j] = items[name1][name2]\n    elif isinstance(items, np.ndarray):\n        arr = np.asarray(items)\n    else:\n        raise TypeError(\n            f\"{name} must be a dictionary or an array-like structure, got \"\n            f\"{type(items)}\"\n        )\n\n    if dim == 1:\n        if arr.ndim != 1:\n            raise ValueError(\n                f\"{name} must be a 1-dimensional array, got a {arr.ndim}-dimensional \"\n                f\"array.\"\n            )\n        if arr.shape[0] != n_assets:\n            raise ValueError(\n                f\"{name} must have {n_assets} elements, got {arr.shape[0]} \"\n                f\"elements.\"\n            )\n    elif dim == 2:\n        if arr.ndim != 2:\n            raise ValueError(\n                f\"{name} must be a 2-dimensional array, got a {arr.ndim}-dimensional \"\n                f\"array.\"\n           "}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            assets_names = np.asarray(list(items.keys()), dtype=object)\n        else:\n            assets_names = np.asarray(assets_names, dtype=object)\n        if assets_names.size != n_assets:\n            raise ValueError(\n                f\"The number of assets ({n_assets}) does not match the number of \"\n                f\"items ({assets_names.size}) in {name}\"\n            )\n        if dim == 1:\n            arr = np.full(n_assets, fill_value, dtype=object)\n            for i, key in enumerate(assets_names):\n                arr[i] = items.get(key, fill_value)\n        elif dim == 2:\n            arr = np.full((n_assets, n_assets), fill_value, dtype=object)\n            for i, key in enumerate(assets_names):\n                arr[i, i] = items.get(key, fill_value)\n        else:\n            raise ValueError(\n                f\"Invalid dimension {dim} for {name}, must be 1 or 2.\"\n            )\n    else:\n        arr = np.asarray(items)\n    if dim == 1:\n        if arr.ndim != 1:\n            raise ValueError(\n                f\"Expected a 1-dimensional array for {name}, got {arr.ndim} \"\n                \"dimensions.\"\n            )\n        if arr.size != n_assets:\n            raise ValueError(\n                f\"The number of assets ({n_assets}) does not match the number of \"\n                f\"items ({arr.size}) in {name}\"\n            )\n    elif dim == 2:\n        if arr.ndim != 2:\n            raise ValueError(\n                f\"Expected a 2-dimensional array for {name}, got {arr.ndim} \"\n                \"dimensions.\"\n            )\n        if arr.shape[0] != n_assets or arr.shape[1] != n_assets:\n            raise Value"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                \"assets_names must be provided when items is a dictionary\"\n            )\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"The number of assets names ({len(assets_names)}) must match the number of assets ({n_assets})\"\n            )\n        items_ = np.full(n_assets, fill_value=fill_value)\n        for i, name in enumerate(assets_names):\n            items_[i] = items.get(name, fill_value)\n    elif isinstance(items, np.ndarray):\n        items_ = items\n    else:\n        items_ = np.asarray(items)\n\n    if dim == 2:\n        if items_.ndim == 1:\n            items_ = items_[np.newaxis, :]\n    else:\n        if items_.ndim != 1:\n            raise ValueError(\n                f\"Expected {name} to be a 1D array, got {items_.ndim}D array instead\"\n            )\n\n    if items_.shape[0] != n_assets:\n        raise ValueError(\n            f\"Expected {name} to have {n_assets} elements, got {items_.shape[0]} instead\"\n        )\n    return items_"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                \"If 'items' is a dictionary, 'assets_names' must be provided\"\n            )\n        if dim == 1:\n            arr = np.full((n_assets,), fill_value, dtype=object)\n            for i, name in enumerate(assets_names):\n                arr[i] = items.get(name, fill_value)\n        elif dim == 2:\n            arr = np.full((len(items), n_assets), fill_value, dtype=object)\n            for i, name in enumerate(assets_names):\n                arr[:, i] = items.get(name, fill_value)\n        else:\n            raise ValueError(f\"Invalid dimension: {dim}\")\n    elif isinstance(items, (pd.DataFrame, pd.Series)):\n        if dim == 1:\n            arr = items.to_numpy()\n        elif dim == 2:\n            arr = items.to_numpy(dtype=object).T\n        else:\n            raise ValueError(f\"Invalid dimension: {dim}\")\n    else:\n        arr = np.asarray(items)\n\n    if dim == 1:\n        if arr.shape != (n_assets,):\n            raise ValueError(\n                f\"Expected array of shape ({n_assets},), got {arr.shape} for '{name}'\"\n            )\n    elif dim == 2:\n        if arr.shape[1] != n_assets:\n            raise ValueError(\n                f\"Expected array of shape (n_groups, {n_assets}), got {arr.shape} for '{name}'\"\n            )\n    else:\n        raise ValueError(f\"Invalid dimension: {dim}\")\n\n    return arr"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(f\"Missing assets names for {name}\")\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"{name} assets names must have {n_assets} elements, got {len(assets_names)}\"\n            )\n        if dim == 1:\n            items = np.full(n_assets, fill_value)\n        else:\n            items = np.full((n_assets, n_assets), fill_value)\n        for key, value in items.items():\n            if key in assets_names:\n                if dim == 1:\n                    items[assets_names == key] = value\n                else:\n                    items[assets_names == key, :] = value\n            else:\n                raise ValueError(f\"Unknown {name} asset '{key}'\")\n    elif isinstance(items, (list, tuple)):\n        items = np.asarray(items)\n    else:\n        items = np.asarray(items)\n\n    if items.ndim != dim:\n        raise ValueError(f\"{name} must have {dim} dimension, got {items.ndim}\")\n\n    if dim == 1:\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"{name} must have {n_assets} elements, got {items.shape[0]}\"\n            )\n    else:\n        if items.shape[0] != n_assets or items.shape[1] != n_assets:\n            raise ValueError(\n                f\"{name} must have shape ({n_assets}, {n_assets}), got {items.shape}\"\n            )\n\n    return items"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                \"You must provide assets names when the input is a dictionary\"\n            )\n        if len(items) != len(assets_names):\n            raise ValueError(\n                f\"The number of assets ({len(assets_names)}) and the number of \"\n                f\"items ({len(items)}) must be equal\"\n            )\n        items = np.asarray(\n            [items.get(name, fill_value) for name in assets_names], dtype=object\n        )\n    if isinstance(items, np.ndarray):\n        if dim == 1:\n            if items.shape != (n_assets,):\n                raise ValueError(\n                    f\"Expected array of shape ({n_assets},), got {items.shape}\"\n                )\n        elif dim == 2:\n            if items.shape != (n_assets, 1):\n                raise ValueError(\n                    f\"Expected array of shape ({n_assets}, 1), got {items.shape}\"\n                )\n        else:\n            raise ValueError(f\"Expected dim to be 1 or 2, got {dim}\")\n    else:\n        if dim == 1:\n            items = np.asarray(items, dtype=object).ravel()\n        elif dim == 2:\n            items = np.asarray(items, dtype=object).reshape(n_assets, 1)\n        else:\n            raise ValueError(f\"Expected dim to be 1 or 2, got {dim}\")\n    return items"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    # Check if 'items' is a dictionary\n    if isinstance(items, dict):\n        # Convert the dictionary into a numpy array\n        if assets_names is None:\n            assets_names = np.asarray(list(items.keys()))\n        else:\n            assets_names = np.asarray(assets_names)\n        # Check if the dictionary keys match the expected assets names\n        if not np.all(assets_names == np.asarray(list(items.keys()))):\n            raise ValueError(\n                f\"The dictionary keys of '{name}' do not match the expected assets \"\n                f\"names. Expected: {assets_names}, got: {list(items.keys())}\"\n            )\n        # Fill in missing values with the 'fill_value'\n        items = np.asarray([items.get(k, fill_value) for k in assets_names])\n    # Check if 'items' is an array-like structure\n    elif not isinstance(items, np.ndarray):\n        # Convert 'items' into a numpy array\n        items = np.asarray(items)\n    # Check if 'items' is a numpy array\n    else:\n        # Check if 'items' has the expected dimensions\n        if items.ndim != dim:\n            raise ValueError(\n                f\"The '{name}' must be {dim}D, got {items.ndim}D.\"\n            )\n        # Check if 'items' has the expected shape\n        if items.shape[dim - 1] != n_assets:\n            raise ValueError(\n                f\"The '{name}' must have shape {(n_assets,)} or \"\n                f\"({n_assets, n_assets}), got {items.shape}.\"\n            )\n    return items"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\", None)\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        agent = MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\", \"\"),\n            purpose=data.get(\"purpose\", \"\"),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\", 0),\n            max_depth=data.get(\"max_depth\", 0),\n            usage_count=data.get(\"usage_count\", 0),\n            id=data.get(\"id\", \"\"),\n            parent_id=data.get(\"parent_id\", \"\"),\n            working_agent=data.get(\"working_agent\", False),\n            is_prime=data.get(\"is_prime\", False),\n            evolve_count=data.get(\"evolve_count\", 0),\n            number_of_code_executions=data.get(\"number_of_code_executions\", 0),\n            last_input=data.get(\"last_input\", \"\"),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n        return agent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\", None)\n        if isinstance(purpose_embedding, list):\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        agent = MicroAgent(\n            dynamic_prompt=data[\"dynamic_prompt\"],\n            purpose=data[\"purpose\"],\n            purpose_embedding=purpose_embedding,\n            depth=data[\"depth\"],\n            max_depth=data[\"max_depth\"],\n            usage_count=data[\"usage_count\"],\n            id=data[\"id\"],\n            parent_id=data[\"parent_id\"],\n            working_agent=data[\"working_agent\"],\n            is_prime=data[\"is_prime\"],\n            evolve_count=data[\"evolve_count\"],\n            number_of_code_executions=data[\"number_of_code_executions\"],\n            last_input=data[\"last_input\"],\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n        return agent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        dynamic_prompt = data.get(\"dynamic_prompt\", \"\")\n        purpose = data.get(\"purpose\", \"\")\n        purpose_embedding = data.get(\"purpose_embedding\", [])\n        depth = data.get(\"depth\", 0)\n        max_depth = data.get(\"max_depth\", 0)\n        usage_count = data.get(\"usage_count\", 0)\n        id = data.get(\"id\", \"\")\n        parent_id = data.get(\"parent_id\", \"\")\n        working_agent = data.get(\"working_agent\", \"\")\n        is_prime = data.get(\"is_prime\", False)\n        evolve_count = data.get(\"evolve_count\", 0)\n        number_of_code_executions = data.get(\"number_of_code_executions\", 0)\n        last_input = data.get(\"last_input\", \"\")\n\n        return MicroAgent(\n            dynamic_prompt=dynamic_prompt,\n            purpose=purpose,\n            purpose_embedding=purpose_embedding,\n            depth=depth,\n            max_depth=max_depth,\n            usage_count=usage_count,\n            id=id,\n            parent_id=parent_id,\n            working_agent=working_agent,\n            is_prime=is_prime,\n            evolve_count=evolve_count,\n            number_of_code_executions=number_of_code_executions,\n            last_input=last_input,\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\")\n        if purpose_embedding is not None and isinstance(purpose_embedding, list):\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        agent = MicroAgent(\n            purpose=data.get(\"purpose\"),\n            dynamic_prompt=data.get(\"dynamic_prompt\"),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\"),\n            max_depth=data.get(\"max_depth\"),\n            usage_count=data.get(\"usage_count\"),\n            id=data.get(\"id\"),\n            parent_id=data.get(\"parent_id\"),\n            working_agent=data.get(\"working_agent\"),\n            is_prime=data.get(\"is_prime\"),\n            evolve_count=data.get(\"evolve_count\"),\n            number_of_code_executions=data.get(\"number_of_code_executions\"),\n            last_input=data.get(\"last_input\"),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n        return agent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\", None)\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        return MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\", \"\"),\n            purpose=data.get(\"purpose\", \"\"),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\", 0),\n            max_depth=data.get(\"max_depth\", 0),\n            usage_count=data.get(\"usage_count\", 0),\n            id=data.get(\"id\", None),\n            parent_id=data.get(\"parent_id\", None),\n            working_agent=data.get(\"working_agent\", False),\n            is_prime=data.get(\"is_prime\", False),\n            evolve_count=data.get(\"evolve_count\", 0),\n            number_of_code_executions=data.get(\"number_of_code_executions\", 0),\n            last_input=data.get(\"last_input\", None),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\")\n        if purpose_embedding is not None and isinstance(purpose_embedding, list):\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        agent = MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\", \"\"),\n            purpose=data.get(\"purpose\", \"\"),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\", 0),\n            max_depth=data.get(\"max_depth\", 0),\n            usage_count=data.get(\"usage_count\", 0),\n            id=data.get(\"id\", \"\"),\n            parent_id=data.get(\"parent_id\", \"\"),\n            working_agent=data.get(\"working_agent\", \"\"),\n            is_prime=data.get(\"is_prime\", False),\n            evolve_count=data.get(\"evolve_count\", 0),\n            number_of_code_executions=data.get(\"number_of_code_executions\", 0),\n            last_input=data.get(\"last_input\", \"\"),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n\n        return agent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        agent = MicroAgent(agent_lifecycle, openai_wrapper)\n        agent.dynamic_prompt = data.get(\"dynamic_prompt\", \"\")\n        agent.purpose = data.get(\"purpose\", \"\")\n        agent.purpose_embedding = np.array(data.get(\"purpose_embedding\", []))  # Convert list to ndarray\n        agent.depth = data.get(\"depth\", 0)\n        agent.max_depth = data.get(\"max_depth\", 0)\n        agent.usage_count = data.get(\"usage_count\", 0)\n        agent.id = data.get(\"id\", None)\n        agent.parent_id = data.get(\"parent_id\", None)\n        agent.working_agent = data.get(\"working_agent\", False)\n        agent.is_prime = data.get(\"is_prime\", False)\n        agent.evolve_count = data.get(\"evolve_count\", 0)\n        agent.number_of_code_executions = data.get(\"number_of_code_executions\", 0)\n        agent.last_input = data.get(\"last_input\", \"\")\n        return agent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\")\n        if purpose_embedding:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        agent = MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\"),\n            purpose=data.get(\"purpose\"),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\"),\n            max_depth=data.get(\"max_depth\"),\n            usage_count=data.get(\"usage_count\"),\n            id=data.get(\"id\"),\n            parent_id=data.get(\"parent_id\"),\n            working_agent=data.get(\"working_agent\"),\n            is_prime=data.get(\"is_prime\"),\n            evolve_count=data.get(\"evolve_count\"),\n            number_of_code_executions=data.get(\"number_of_code_executions\"),\n            last_input=data.get(\"last_input\"),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n\n        return agent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        agent = MicroAgent(\n            purpose=data.get(\"purpose\", \"\"),\n            dynamic_prompt=data.get(\"dynamic_prompt\", \"\"),\n            purpose_embedding=data.get(\"purpose_embedding\", \"\"),\n            depth=data.get(\"depth\", 0),\n            max_depth=data.get(\"max_depth\", 0),\n            usage_count=data.get(\"usage_count\", 0),\n            id=data.get(\"id\", \"\"),\n            parent_id=data.get(\"parent_id\", \"\"),\n            working_agent=data.get(\"working_agent\", \"\"),\n            is_prime=data.get(\"is_prime\", False),\n            evolve_count=data.get(\"evolve_count\", 0),\n            number_of_code_executions=data.get(\"number_of_code_executions\", 0),\n            last_input=data.get(\"last_input\", \"\"),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n        return agent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\")\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        return MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\"),\n            purpose=data.get(\"purpose\"),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\"),\n            max_depth=data.get(\"max_depth\"),\n            usage_count=data.get(\"usage_count\"),\n            id=data.get(\"id\"),\n            parent_id=data.get(\"parent_id\"),\n            working_agent=data.get(\"working_agent\"),\n            is_prime=data.get(\"is_prime\"),\n            evolve_count=data.get(\"evolve_count\"),\n            number_of_code_executions=data.get(\"number_of_code_executions\"),\n            last_input=data.get(\"last_input\"),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\", None)\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        agent = MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\", \"\"),\n            purpose=data.get(\"purpose\", \"\"),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\", 0),\n            max_depth=data.get(\"max_depth\", 0),\n            usage_count=data.get(\"usage_count\", 0),\n            id=data.get(\"id\", None),\n            parent_id=data.get(\"parent_id\", None),\n            working_agent=data.get(\"working_agent\", None),\n            is_prime=data.get(\"is_prime\", False),\n            evolve_count=data.get(\"evolve_count\", 0),\n            number_of_code_executions=data.get(\"number_of_code_executions\", 0),\n            last_input=data.get(\"last_input\", None),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n\n        return agent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\")\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        agent = MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\"),\n            purpose=data.get(\"purpose\"),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\"),\n            max_depth=data.get(\"max_depth\"),\n            usage_count=data.get(\"usage_count\"),\n            id=data.get(\"id\"),\n            parent_id=data.get(\"parent_id\"),\n            working_agent=data.get(\"working_agent\"),\n            is_prime=data.get(\"is_prime\"),\n            evolve_count=data.get(\"evolve_count\"),\n            number_of_code_executions=data.get(\"number_of_code_executions\"),\n            last_input=data.get(\"last_input\"),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n\n        return agent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        agent = MicroAgent(agent_lifecycle, openai_wrapper)\n        agent.dynamic_prompt = data.get(\"dynamic_prompt\", \"\")\n        agent.purpose = data.get(\"purpose\", \"\")\n        agent.purpose_embedding = data.get(\"purpose_embedding\", [])\n        agent.depth = data.get(\"depth\", 0)\n        agent.max_depth = data.get(\"max_depth\", 0)\n        agent.usage_count = data.get(\"usage_count\", 0)\n        agent.id = data.get(\"id\", \"\")\n        agent.parent_id = data.get(\"parent_id\", \"\")\n        agent.working_agent = data.get(\"working_agent\", False)\n        agent.is_prime = data.get(\"is_prime\", False)\n        agent.evolve_count = data.get(\"evolve_count\", 0)\n        agent.number_of_code_executions = data.get(\"number_of_code_executions\", 0)\n        agent.last_input = data.get(\"last_input\", \"\")\n        return agent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        agent = MicroAgent(agent_lifecycle, openai_wrapper)\n\n        agent.dynamic_prompt = data.get(\"dynamic_prompt\")\n        agent.purpose = data.get(\"purpose\")\n        agent.purpose_embedding = data.get(\"purpose_embedding\")\n        agent.depth = data.get(\"depth\")\n        agent.max_depth = data.get(\"max_depth\")\n        agent.usage_count = data.get(\"usage_count\")\n        agent.id = data.get(\"id\")\n        agent.parent_id = data.get(\"parent_id\")\n        agent.working_agent = data.get(\"working_agent\")\n        agent.is_prime = data.get(\"is_prime\")\n        agent.evolve_count = data.get(\"evolve_count\")\n        agent.number_of_code_executions = data.get(\"number_of_code_executions\")\n        agent.last_input = data.get(\"last_input\")\n\n        return agent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\")\n        if isinstance(purpose_embedding, list):\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        agent = MicroAgent(\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n            dynamic_prompt=data.get(\"dynamic_prompt\"),\n            purpose=data.get(\"purpose\"),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\"),\n            max_depth=data.get(\"max_depth\"),\n            usage_count=data.get(\"usage_count\"),\n            id=data.get(\"id\"),\n            parent_id=data.get(\"parent_id\"),\n            working_agent=data.get(\"working_agent\"),\n            is_prime=data.get(\"is_prime\"),\n            evolve_count=data.get(\"evolve_count\"),\n            number_of_code_executions=data.get(\"number_of_code_executions\"),\n            last_input=data.get(\"last_input\"),\n        )\n\n        return agent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        # Create a new MicroAgent instance\n        agent = MicroAgent(agent_lifecycle, openai_wrapper)\n\n        # Set the MicroAgent's attributes based on the input dictionary\n        agent.dynamic_prompt = data.get(\"dynamic_prompt\", \"\")\n        agent.purpose = data.get(\"purpose\", \"\")\n        agent.purpose_embedding = data.get(\"purpose_embedding\", None)\n        agent.depth = data.get(\"depth\", 0)\n        agent.max_depth = data.get(\"max_depth\", 0)\n        agent.usage_count = data.get(\"usage_count\", 0)\n        agent.id = data.get(\"id\", None)\n        agent.parent_id = data.get(\"parent_id\", None)\n        agent.working_agent = data.get(\"working_agent\", None)\n        agent.is_prime = data.get(\"is_prime\", False)\n        agent.evolve_count = data.get(\"evolve_count\", 0)\n        agent.number_of_code_executions = data.get(\"number_of_code_executions\", 0)\n        agent.last_input = data.get(\"last_input\", None)\n\n        return agent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        agent = MicroAgent(agent_lifecycle, openai_wrapper)\n        agent.dynamic_prompt = data.get(\"dynamic_prompt\", None)\n        agent.purpose = data.get(\"purpose\", None)\n        agent.purpose_embedding = data.get(\"purpose_embedding\", None)\n        agent.depth = data.get(\"depth\", None)\n        agent.max_depth = data.get(\"max_depth\", None)\n        agent.usage_count = data.get(\"usage_count\", 0)\n        agent.id = data.get(\"id\", None)\n        agent.parent_id = data.get(\"parent_id\", None)\n        agent.working_agent = data.get(\"working_agent\", False)\n        agent.is_prime = data.get(\"is_prime\", False)\n        agent.evolve_count = data.get(\"evolve_count\", 0)\n        agent.number_of_code_executions = data.get(\"number_of_code_executions\", 0)\n        agent.last_input = data.get(\"last_input\", None)\n        return agent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\", None)\n        if purpose_embedding is not None and isinstance(purpose_embedding, list):\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        agent = MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\", \"\"),\n            purpose=data.get(\"purpose\", \"\"),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\", 0),\n            max_depth=data.get(\"max_depth\", 0),\n            usage_count=data.get(\"usage_count\", 0),\n            id=data.get(\"id\", \"\"),\n            parent_id=data.get(\"parent_id\", \"\"),\n            working_agent=data.get(\"working_agent\", \"\"),\n            is_prime=data.get(\"is_prime\", False),\n            evolve_count=data.get(\"evolve_count\", 0),\n            number_of_code_executions=data.get(\"number_of_code_executions\", 0),\n            last_input=data.get(\"last_input\", \"\"),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n\n        return agent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\")\n        if purpose_embedding is not None and isinstance(purpose_embedding, list):\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        agent = MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\"),\n            purpose=data.get(\"purpose\"),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\"),\n            max_depth=data.get(\"max_depth\"),\n            usage_count=data.get(\"usage_count\"),\n            id=data.get(\"id\"),\n            parent_id=data.get(\"parent_id\"),\n            working_agent=data.get(\"working_agent\"),\n            is_prime=data.get(\"is_prime\"),\n            evolve_count=data.get(\"evolve_count\"),\n            number_of_code_executions=data.get(\"number_of_code_executions\"),\n            last_input=data.get(\"last_input\"),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n\n        return agent\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        # Extract the optional keys from the data dictionary\n        dynamic_prompt = data.get(\"dynamic_prompt\", None)\n        purpose = data.get(\"purpose\", None)\n        purpose_embedding = data.get(\"purpose_embedding\", None)\n        depth = data.get(\"depth\", None)\n        max_depth = data.get(\"max_depth\", None)\n        usage_count = data.get(\"usage_count\", None)\n        id = data.get(\"id\", None)\n        parent_id = data.get(\"parent_id\", None)\n        working_agent = data.get(\"working_agent\", None)\n        is_prime = data.get(\"is_prime\", None)\n        evolve_count = data.get(\"evolve_count\", None)\n        number_of_code_executions = data.get(\"number_of_code_executions\", None)\n        last_input = data.get(\"last_input\", None)\n\n        # Create a new MicroAgent instance with the extracted data\n        microagent = MicroAgent(\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n            dynamic_prompt=dynamic_prompt,\n            purpose=purpose,\n            purpose_embedding=purpose_embedding,\n            depth=depth,\n            max_depth=max_depth,\n            usage_count=usage_count,\n            id=id,\n            parent_id=parent_id,\n            working_agent=working_agent,\n            is_prime=is_prime,\n            evolve_count=evolve_count,\n            number_of_code_executions=number_of_code_executions,\n            last_input=last_input,\n        )\n\n        return microagent\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb / 12.92\n  linear1 = xnp.power((srgb + 0.055) / 1.055, 2.4)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb / 12.92\n  linear1 = xnp.power((srgb + 0.055) / 1.055, 2.4)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb / 12.92\n  linear1 = xnp.power((srgb + 0.055) / 1.055, 2.4)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb / 12.92\n  linear1 = xnp.power((srgb + 0.055) / 1.055, 2.4)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  srgb = xnp.where(srgb <= 0.0404482362771082, srgb / 12.92, xnp.power((srgb + 0.055) / 1.055, 2.4))\n  return srgb"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb / 12.92\n  linear1 = xnp.power((srgb + 0.055) / 1.055, 2.4)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb / 12.92\n  linear1 = xnp.power((srgb + 0.055) / 1.055, 2.4)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  srgb0 = srgb / 12.92\n  srgb1 = xnp.power((srgb + eps) / 1.055, 2.4)\n  return xnp.where(srgb <= 0.04045, srgb0, srgb1)"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb / 12.92\n  linear1 = xnp.power((srgb + 0.055) / 1.055, 2.4)\n  return xnp.where(srgb <= 0.0404482362771082, linear0, linear1)"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  srgb = xnp.array(srgb)\n  linear0 = srgb / 12.92\n  linear1 = xnp.power((srgb + 0.055) / 1.055, 2.4)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  srgb0 = srgb / 12.92\n  srgb1 = xnp.power((srgb + eps) / 1.055, 2.4)\n  return xnp.where(srgb <= 0.04045, srgb0, srgb1)"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb / 12.92\n  linear1 = xnp.power((srgb + 0.055) / 1.055, 2.4)\n  return xnp.where(srgb <= 0.0404482362771082, linear0, linear1)"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb / 12.92\n  linear1 = ((srgb + eps) / 1.055) ** 2.4\n  return xnp.where(srgb <= 0.04045, linear0, linear1)"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  srgb = xnp.where(srgb <= 0.04045, srgb / 12.92, xnp.power((srgb + 0.055) / 1.055, 2.4))\n  return srgb"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  srgb = xnp.array(srgb)\n  linear0 = srgb / 12.92\n  linear1 = ((srgb + 0.055) / 1.055) ** 2.4\n  linear = xnp.where(srgb <= 0.04045, linear0, linear1)\n  return linear"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  srgb = xnp.array(srgb)\n  srgb_linear0 = srgb / 12.92\n  srgb_linear1 = xnp.power((srgb + eps) / (1 + eps), 2.4)\n  return xnp.where(srgb <= 0.04045, srgb_linear0, srgb_linear1)"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  srgb = xnp.array(srgb)\n  srgb0 = srgb / 12.92\n  srgb1 = xnp.power((srgb + 0.055) / 1.055, 2.4)\n  linear = xnp.where(srgb <= 0.04045, srgb0, srgb1)\n  return linear"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  srgb0 = srgb / 12.92\n  srgb1 = ((srgb + eps) / 1.055) ** 2.4\n  return xnp.where(srgb <= 0.0404482362771082, srgb0, srgb1)"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  srgb = jnp.asarray(srgb)\n  linear0 = srgb / 12.92\n  linear1 = ((srgb + 0.055) / 1.055) ** 2.4\n  return xnp.where(srgb <= 0.04045, linear0, linear1)"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  srgb = jnp.clip(srgb, eps, 1 - eps)\n  linear0 = srgb / 12.92\n  linear1 = ((srgb + 0.055) / 1.055) ** 2.4\n  return xnp.where(srgb <= 0.04045, linear0, linear1)\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Ensure that the spline degree is at most one less than the number of points in x.\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Compute the B-spline coefficients using the specified degree and smoothness.\n  tck = scipy.interpolate.splrep(t_input, x, k=spline_degree, s=smoothness)\n\n  # Evaluate the B-spline at the specified output times.\n  x_interp = scipy.interpolate.splev(t_output, tck)\n\n  return x_interp"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Compute the spline coefficients.\n  tck = scipy.interpolate.splrep(t_input, x, k=spline_degree, s=smoothness)\n\n  # Evaluate the spline at the output times.\n  x_output = scipy.interpolate.splev(t_output, tck)\n\n  return x_output\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Check if x is a numpy array\n  if not isinstance(x, np.ndarray):\n    raise ValueError(\"x must be a numpy array\")\n\n  # Check if x is a 1-dimensional array\n  if x.ndim != 1:\n    raise ValueError(\"x must be a 1-dimensional array\")\n\n  # Check if x has at least 2 elements\n  if x.size < 2:\n    raise ValueError(\"x must have at least 2 elements\")\n\n  # Check if t_input is a numpy array\n  if not isinstance(t_input, np.ndarray):\n    raise ValueError(\"t_input must be a numpy array\")\n\n  # Check if t_input is a 1-dimensional array\n  if t_input.ndim != 1:\n    raise ValueError(\"t_input must be a 1-dimensional array\")\n\n  # Check if t_input has the same number of elements as x\n  if t_input.size != x.size:\n    raise ValueError(\"t_input must have the same number of elements as x\")\n\n  # Check if t_output is a numpy array\n  if not isinstance(t_output, np.ndarray):\n    raise ValueError(\"t_output must be a numpy array\")\n\n  # Check if t_output is a 1-dimensional array\n  if t_output.ndim != 1:\n    raise ValueError(\"t_output must be a 1-dimensional array\")\n\n  # Check if t_output is sorted in ascending order\n  if not np.all(np.diff(t_output) > 0):\n    raise ValueError(\"t_output must be sorted in ascending order\")\n\n  # Check if spline_degree is a positive integer\n  if not isinstance(spline_degree, int) or spline_degree < 0:\n    raise ValueError(\"spline_degree must be a positive integer\")\n\n  # Check if smoothness is a non-negative float\n  if not isinstance(smoothness, float) or smoothness < 0:\n    raise ValueError(\"smoothness must be a"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Get the number of points in x.\n  n_points = x.shape[0]\n\n  # Adjust the spline degree to be at most one less than the number of points in x.\n  spline_degree = min(spline_degree, n_points - 1)\n\n  # Create the spline object.\n  tck = scipy.interpolate.splrep(t_input, x, k=spline_degree, s=smoothness)\n\n  # Evaluate the spline at the output times.\n  x_output = scipy.interpolate.splev(t_output, tck)\n\n  return x_output\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Ensure that the degree of the spline is at most one less than the number of points in x.\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Compute the knot vector for the B-spline.\n  knots = np.linspace(0, 1, len(x) + spline_degree - 1)\n\n  # Create the B-spline object.\n  spline = scipy.interpolate.BSpline(knots, x, k=spline_degree, s=smoothness)\n\n  # Evaluate the B-spline at the output times.\n  y = spline(t_output)\n\n  return y\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Check if x is a numpy array\n  if not isinstance(x, np.ndarray):\n    raise TypeError(\"x must be a numpy array\")\n\n  # Check if x is a 1-dimensional array\n  if x.ndim != 1:\n    raise ValueError(\"x must be a 1-dimensional array\")\n\n  # Check if x has at least two elements\n  if x.size < 2:\n    raise ValueError(\"x must have at least two elements\")\n\n  # Check if spline_degree is an integer\n  if not isinstance(spline_degree, int):\n    raise TypeError(\"spline_degree must be an integer\")\n\n  # Check if spline_degree is non-negative\n  if spline_degree < 0:\n    raise ValueError(\"spline_degree must be non-negative\")\n\n  # Check if spline_degree is less than or equal to the number of elements in x minus 1\n  if spline_degree > x.size - 1:\n    raise ValueError(\"spline_degree must be less than or equal to the number of elements in x minus 1\")\n\n  # Check if smoothness is a float or int\n  if not isinstance(smoothness, (float, int)):\n    raise TypeError(\"smoothness must be a float or int\")\n\n  # Check if smoothness is non-negative\n  if smoothness < 0:\n    raise ValueError(\"smoothness must be non-negative\")\n\n  # Check if t_input and t_output are numpy arrays\n  if not isinstance(t_input, np.ndarray) or not isinstance(t_output, np.ndarray):\n    raise TypeError(\"t_input and t_output must be numpy arrays\")\n\n  # Check if t_input and t_output are 1-dimensional arrays\n  if t_input.ndim != 1 or t_output.ndim != 1:\n    raise ValueError(\"t_input and t_output must be 1-dimensional arrays\")\n\n  # Check if t_input and t_output have the same number of elements\n  if t_input.size !"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Ensure that the input and output arrays have the same shape.\n  assert x.shape == t_input.shape\n  assert t_input.shape == t_output.shape\n\n  # Adjust the spline degree to be at most one less than the number of points in x.\n  spline_degree = min(spline_degree, x.shape[0] - 1)\n\n  # Compute the knot sequence using the input time values.\n  tck = scipy.interpolate.splrep(t_input, x, k=spline_degree, s=smoothness)\n\n  # Evaluate the spline at the output time values.\n  x_output = scipy.interpolate.splev(t_output, tck)\n\n  return x_output"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Compute the spline coefficients.\n  tck, _ = scipy.interpolate.splprep(x, k=spline_degree, s=smoothness)\n\n  # Compute the spline at the desired output times.\n  spline_values = scipy.interpolate.splev(t_output, tck)\n\n  # Compute the derivatives of the spline at the desired output times.\n  spline_derivatives = scipy.interpolate.splev(t_output, tck, der=1)\n\n  # Compute the second derivatives of the spline at the desired output times.\n  spline_second_derivatives = scipy.interpolate.splev(t_output, tck, der=2)\n\n  # Compute the third derivatives of the spline at the desired output times.\n  spline_third_derivatives = scipy.interpolate.splev(t_output, tck, der=3)\n\n  # Compute the fourth derivatives of the spline at the desired output times.\n  spline_fourth_derivatives = scipy.interpolate.splev(t_output, tck, der=4)\n\n  # Compute the fifth derivatives of the spline at the desired output times.\n  spline_fifth_derivatives = scipy.interpolate.splev(t_output, tck, der=5)\n\n  # Compute the sixth derivatives of the spline at the desired output times.\n  spline_sixth_derivatives = scipy.interpolate.splev(t_output, tck, der=6)\n\n  # Compute the seventh derivatives of the spline at the desired output times.\n  spline_seventh_derivatives = scipy.interpolate.splev(t_output, tck, der=7)\n\n  # Compute the eighth derivatives of the spline at the desired output times.\n  spline_eighth_derivatives = scipy.interpolate.splev("}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Adapted from https://github.com/google/nerfies/blob/main/nerfies/camera.py\n  # Ensure that the spline degree is at most one less than the number of points.\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Check if the input and output times are increasing.\n  if not np.all(np.diff(t_input) > 0):\n    raise ValueError(\"The input times must be strictly increasing.\")\n  if not np.all(np.diff(t_output) > 0):\n    raise ValueError(\"The output times must be strictly increasing.\")\n\n  # Check if the input and output times are within the valid range.\n  if not np.all(t_input >= t_input[0]) or not np.all(t_input <= t_input[-1]):\n    raise ValueError(\"The input times must be within the valid range.\")\n  if not np.all(t_output >= t_input[0]) or not np.all(t_output <= t_input[-1]):\n    raise ValueError(\"The output times must be within the valid range.\")\n\n  # Check if the input and output times have the same shape.\n  if t_input.shape != x.shape:\n    raise ValueError(\"The input times and x must have the same shape.\")\n\n  # Check if the input and output times have the same shape.\n  if t_output.shape != t_input.shape:\n    raise ValueError(\"The output times and input times must have the same shape.\")\n\n  # Check if the input and output times have the same shape.\n  if t_output.shape != t_input.shape:\n    raise ValueError(\"The output times and input times must have the same shape.\")\n\n  # Check if the input and output times have the same shape.\n  if t_output.shape != t_input.shape:\n    raise ValueError(\"The output times and input times must have the same shape.\")\n\n  # Check if the input and output times have the same shape.\n  if t_output.shape != t_input."}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Ensure that the spline degree is at most one less than the number of points in x.\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Calculate the knot sequence for the B-spline.\n  knots = np.arange(len(x) + spline_degree + 1)\n  knots = np.pad(knots, (spline_degree, spline_degree), mode='edge')\n\n  # Calculate the B-spline coefficients using the input data and knot sequence.\n  tck = scipy.interpolate.splrep(t_input, x, k=spline_degree, t=knots, s=smoothness)\n\n  # Evaluate the B-spline at the output time points.\n  x_output = scipy.interpolate.splev(t_output, tck)\n\n  return x_output\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Adapted from https://github.com/google/nerfies/blob/main/nerfies/camera.py\n  # Make sure that the spline degree is at most one less than the number of\n  # points in x.\n  spline_degree = min(spline_degree, x.shape[0] - 1)\n\n  # Calculate the spline coefficients.\n  tck, _ = scipy.interpolate.splprep(x, k=spline_degree, s=smoothness)\n\n  # Evaluate the spline at the output times.\n  x_interp = scipy.interpolate.splev(t_output, tck)\n\n  return x_interp"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Calculate the number of control points for the spline\n  n_control_points = len(x)\n  spline_degree = min(spline_degree, n_control_points - 1)\n\n  # Calculate the knot vector for the spline\n  knot_vector = np.linspace(0, 1, n_control_points + spline_degree + 1)\n\n  # Create a spline object with the given degree and knot vector\n  spline = UnivariateSpline(t_input, x, k=spline_degree, s=smoothness, t=knot_vector)\n\n  # Evaluate the spline at the given output times\n  return spline(t_output)"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Adapted from https://github.com/google/nerfies/blob/main/nerfies/camera.py\n  # We need to pad the input and output data to avoid boundary effects.\n  pad_width = spline_degree\n  t_input = np.pad(t_input, (pad_width, pad_width), mode='edge')\n  t_output = np.pad(t_output, (pad_width, pad_width), mode='edge')\n  x = np.pad(x, (pad_width, pad_width), mode='edge')\n\n  # Interpolate.\n  tck = scipy.interpolate.splrep(t_input, x, k=spline_degree, s=smoothness)\n  x_interp = scipy.interpolate.splev(t_output, tck)\n\n  # Crop to the original size.\n  x_interp = x_interp[pad_width:-pad_width]\n\n  return x_interp\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Check if x is a numpy array or a list\n  if isinstance(x, np.ndarray):\n    x = x.tolist()\n\n  # Check if t_input and t_output are numpy arrays or lists\n  if isinstance(t_input, np.ndarray):\n    t_input = t_input.tolist()\n  if isinstance(t_output, np.ndarray):\n    t_output = t_output.tolist()\n\n  # Check if t_input and t_output have the same length\n  if len(t_input) != len(x):\n    raise ValueError(\n        'The length of t_input must be equal to the length of x.'\n    )\n\n  # Check if t_output is sorted\n  if not all(t_output[i] <= t_output[i + 1] for i in range(len(t_output) - 1)):\n    raise ValueError('t_output must be sorted.')\n\n  # Check if t_output is within the range of t_input\n  if t_output[0] < t_input[0] or t_output[-1] > t_input[-1]:\n    raise ValueError(\n        't_output must be within the range of t_input.'\n    )\n\n  # Check if t_input is sorted\n  if not all(t_input[i] <= t_input[i + 1] for i in range(len(t_input) - 1)):\n    raise ValueError('t_input must be sorted.')\n\n  # Check if t_input and t_output are not empty\n  if not t_input or not t_output:\n    raise ValueError('t_input and t_output must not be empty.')\n\n  # Check if spline_degree is a positive integer\n  if not isinstance(spline_degree, int) or spline_degree < 0:\n    raise ValueError('spline_degree must be a positive integer.')\n\n  # Check if smoothness is a non-negative float\n  if not isinstance(smoothness, (int, float))"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Ensure that the input array x is a numpy array.\n  x = np.array(x)\n\n  # Ensure that the input array t_input is a numpy array.\n  t_input = np.array(t_input)\n\n  # Ensure that the input array t_output is a numpy array.\n  t_output = np.array(t_output)\n\n  # Check if the input array x is 1-dimensional.\n  if x.ndim != 1:\n    raise ValueError(\"Input array x must be 1-dimensional.\")\n\n  # Check if the input array t_input is 1-dimensional.\n  if t_input.ndim != 1:\n    raise ValueError(\"Input array t_input must be 1-dimensional.\")\n\n  # Check if the input array t_output is 1-dimensional.\n  if t_output.ndim != 1:\n    raise ValueError(\"Input array t_output must be 1-dimensional.\")\n\n  # Check if the input arrays t_input and t_output have the same length.\n  if len(t_input) != len(x):\n    raise ValueError(\"Input arrays t_input and x must have the same length.\")\n\n  # Check if the input arrays t_output and t_input have the same length.\n  if len(t_output) != len(t_input):\n    raise ValueError(\"Input arrays t_output and t_input must have the same length.\")\n\n  # Check if the input arrays t_input and t_output are sorted.\n  if not np.all(np.diff(t_input) > 0):\n    raise ValueError(\"Input array t_input must be sorted in ascending order.\")\n\n  # Check if the input arrays t_output and t_input are sorted.\n  if not np.all(np.diff(t_output) > 0):\n    raise ValueError(\"Input array t_output must be sorted in ascending order.\")\n\n  # Check if the input array t_input is within the range of t_output.\n  if not np.all(t_input >= t_output.min"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Check if the input signal x is 1D.\n  if x.ndim != 1:\n    raise ValueError(\"Input signal x must be 1D.\")\n\n  # Check if the input timesteps t_input are sorted.\n  if not np.all(np.diff(t_input) >= 0):\n    raise ValueError(\"Input timesteps t_input must be sorted.\")\n\n  # Check if the input timesteps t_output are sorted.\n  if not np.all(np.diff(t_output) >= 0):\n    raise ValueError(\"Output timesteps t_output must be sorted.\")\n\n  # Check if the input timesteps t_output are within the range of t_input.\n  if t_output.min() < t_input.min() or t_output.max() > t_input.max():\n    raise ValueError(\"Output timesteps t_output must be within the range of t_input.\")\n\n  # Adjust the spline degree to be at most one less than the number of points in x.\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Compute the B-spline representation of the signal x.\n  tck = scipy.interpolate.splrep(t_input, x, k=spline_degree, s=smoothness)\n\n  # Evaluate the B-spline at the output timesteps t_output.\n  x_output = scipy.interpolate.splev(t_output, tck)\n\n  return x_output"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Check if the input and output time arrays have the same length\n  if len(t_input) != len(x):\n    raise ValueError(\"The length of t_input must be equal to the length of x.\")\n\n  # Check if the input time array is sorted in ascending order\n  if not np.all(np.diff(t_input) > 0):\n    raise ValueError(\"The t_input array must be sorted in ascending order.\")\n\n  # Check if the spline degree is less than or equal to the number of input points\n  if spline_degree >= len(x):\n    raise ValueError(\"The spline degree must be less than the number of input points.\")\n\n  # Check if the smoothness parameter is non-negative\n  if smoothness < 0:\n    raise ValueError(\"The smoothness parameter must be non-negative.\")\n\n  # Compute the knot vector for the B-spline\n  tck = scipy.interpolate.splrep(t_input, x, k=spline_degree, s=smoothness)\n\n  # Evaluate the B-spline at the output time points\n  x_interp = scipy.interpolate.splev(t_output, tck)\n\n  return x_interp"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Check if the input array is a numpy array\n  if not isinstance(x, np.ndarray):\n    raise ValueError(\"Input x must be a numpy array.\")\n\n  # Check if the input array has at least one dimension\n  if x.ndim < 1:\n    raise ValueError(\"Input x must have at least one dimension.\")\n\n  # Check if the input arrays have the same shape\n  if x.shape != t_input.shape:\n    raise ValueError(\"Input arrays x and t_input must have the same shape.\")\n\n  # Check if the spline degree is at least 1\n  if spline_degree < 1:\n    raise ValueError(\"Spline degree must be at least 1.\")\n\n  # Check if the smoothness is non-negative\n  if smoothness < 0:\n    raise ValueError(\"Smoothness must be non-negative.\")\n\n  # Check if the input arrays have the same length\n  if x.shape[0] != t_input.shape[0]:\n    raise ValueError(\"Input arrays x and t_input must have the same length.\")\n\n  # Check if the input arrays are sorted\n  if not np.all(np.diff(t_input) >= 0):\n    raise ValueError(\"Input array t_input must be sorted in ascending order.\")\n\n  # Check if the output times are within the range of the input times\n  if np.min(t_output) < np.min(t_input) or np.max(t_output) > np.max(t_input):\n    raise ValueError(\"Output times t_output must be within the range of the input times t_input.\")\n\n  # Adjust the spline degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, x.shape[0] - 1)\n\n  # Calculate the knot vector\n  knots = np.concatenate(([t_input[0]] * (spline_degree + 1), t_input, [t_input[-1]] * (spline_degree + 1)))\n\n  # Create the spline object\n  spl"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Check if the input and output arrays are both 1-dimensional\n  if x.ndim != 1 or t_input.ndim != 1 or t_output.ndim != 1:\n    raise ValueError(\"Input arrays must be 1-dimensional\")\n\n  # Check if the input and output arrays have the same length\n  if x.shape[0] != t_input.shape[0]:\n    raise ValueError(\"Input arrays must have the same length\")\n\n  # Check if the input array is not empty\n  if x.shape[0] == 0:\n    raise ValueError(\"Input array must not be empty\")\n\n  # Check if the input array has at least two elements\n  if x.shape[0] < 2:\n    raise ValueError(\"Input array must have at least two elements\")\n\n  # Check if the input array is sorted in ascending order\n  if not np.all(np.diff(t_input) > 0):\n    raise ValueError(\"Input array must be sorted in ascending order\")\n\n  # Check if the output array is sorted in ascending order\n  if not np.all(np.diff(t_output) > 0):\n    raise ValueError(\"Output array must be sorted in ascending order\")\n\n  # Check if the output array is within the bounds of the input array\n  if t_output[0] < t_input[0] or t_output[-1] > t_input[-1]:\n    raise ValueError(\"Output array must be within the bounds of the input array\")\n\n  # Check if the spline degree is positive\n  if spline_degree < 0:\n    raise ValueError(\"Spline degree must be positive\")\n\n  # Check if the smoothness is non-negative\n  if smoothness < 0:\n    raise ValueError(\"Smoothness must be non-negative\")\n\n  # Adjust the spline degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, x.shape[0] - 1)\n\n  # Use the scipy.interpolate.splprep function to create a spline representation\n  # of"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Adapted from https://github.com/google/nerfies/blob/main/nerfies/camera.py\n  # Compute the knot locations.\n  n_knots = len(x)\n  k = min(k, n_knots - 1)\n  knot_locations = np.linspace(0, 1, k + 2)[1:-1]\n\n  # Compute the coefficients of the spline.\n  spline_coefficients = np.zeros((k + 1, n_knots))\n  for i in range(n_knots):\n    spline_coefficients[:, i] = np.power(t_input - knot_locations[i], k)\n\n  # Compute the spline.\n  spline_coefficients = np.linalg.inv(spline_coefficients) @ x\n\n  # Compute the spline derivative.\n  spline_derivative = np.zeros_like(spline_coefficients)\n  for i in range(k):\n    spline_derivative[i, :] = (k - i) * spline_coefficients[i + 1, :]\n\n  # Compute the spline second derivative.\n  spline_second_derivative = np.zeros_like(spline_coefficients)\n  for i in range(k - 1):\n    spline_second_derivative[i, :] = (k - i) * (k - i - 1) * spline_coefficients[i + 2, :]\n\n  # Compute the spline third derivative.\n  spline_third_derivative = np.zeros_like(spline_coefficients)\n  for i in range(k - 2):\n    spline_third_derivative[i, :] = (k - i) * (k - i - 1) * (k - i - 2) * spline_coefficients[i + 3, :]\n\n  # Compute the spline fourth derivative.\n  spline_four"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word.lower()\n    if word.islower():\n        return word.upper()\n    if word[0].isupper() and word[1].isupper():\n        return word.upper()\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    return word"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word.lower()\n    if word.islower():\n        return word.upper()\n    if word[0].isupper() and word[1].isupper():\n        return word.lower()\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    return word"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word.lower()\n    if word.islower():\n        return word.upper()\n    if word[0].isupper() and word[1].isupper():\n        return word.upper()\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    return word"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.islower():\n        return word\n    if word.isupper():\n        return word\n    if word[0].isupper() and word[1].isupper():\n        return word.upper()\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    return word.lower()"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n    if word[0].isupper() and word[1].isupper():\n        return word.upper()\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    return word"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word.lower()\n    if word.islower():\n        return word.upper()\n    if word[0].isupper() and word[1].isupper():\n        return word.upper()\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    return word.lower()"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n    if word[0].isupper() and word[1].isupper():\n        return word.capitalize()\n    return word.lower()"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    if word[0].isupper() and word[1].isupper():\n        return word.upper()\n    return word"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word.lower()\n    if word.islower():\n        return word.upper()\n    if len(word) > 1:\n        if word[0].isupper() and word[1].isupper():\n            return word.lower()\n        if word[0].isupper() and word[1].islower():\n            return word.capitalize()\n    return word"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.islower():\n        return word\n    if word.isupper():\n        return word\n    if word[0].isupper() and word[1].isupper():\n        return word.capitalize()\n    return word.lower()"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper() or word.islower():\n        return word\n    if word[0].isupper() and word[1].isupper():\n        return word.lower()\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    return word.lower()"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word.lower()\n    elif word.islower():\n        return word.upper()\n    else:\n        if word[0].isupper() and word[1].isupper():\n            return word.upper()\n        elif word[0].islower() and word[1].islower():\n            return word.lower()\n        else:\n            return word.capitalize()"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word.lower()\n    if word.islower():\n        return word.upper()\n    if word[0].isupper() and word[1].isupper():\n        return word.upper()\n    if word[0].islower() and word[1].islower():\n        return word.lower()\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    return word"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n    if word[0].isupper() and word[1].islower():\n        return word\n    if word[0].isupper() and word[1].isupper():\n        return word.capitalize()\n    return word.lower()"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.islower():\n        return word\n    if word.isupper():\n        return word\n    if word[0].isupper() and word[1].isupper():\n        return word\n    if word[0].isupper() and word[1].islower():\n        return word[0] + word[1:].lower()\n    if word[0].islower() and word[1].isupper():\n        return word.capitalize()\n    return word"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n    if word[0].isupper() and word[1].isupper():\n        return word\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    if word[0].islower() and word[1].isupper():\n        return word.lower()\n    if word[0].islower() and word[1].islower():\n        return word.lower()\n    return word.lower()"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word.lower()\n    if word.islower():\n        return word.upper()\n    if len(word) == 1:\n        return word.upper()\n    if word[0].isupper() and word[1].isupper():\n        return word.lower()\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    if word[0].islower() and word[1].isupper():\n        return word.lower()\n    if word[0].islower() and word[1].islower():\n        return word.capitalize()\n    return word"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n    if word[0].isupper() and word[1].isupper():\n        return word.lower()\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    return word\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n    if word[0].isupper() and word[1].isupper():\n        return word.upper()\n    if word[0].isupper():\n        return word.capitalize()\n    return word.lower()\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n    if word[0].isupper() and word[1].isupper():\n        return word.lower()\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    if word[0].islower() and word[1].isupper():\n        return word.capitalize()\n    if word[0].islower() and word[1].islower():\n        return word.lower()\n    return word\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must contain only boolean values. Received {v.dtype}.\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}: {field.name} must be binary. Received {v.dtype}\")\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be binary. Received {v.dtype}.\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if v.dtype != bool:\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must contain only boolean values. Received {v.dtype}.\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must contain only boolean values. Received {v.dtype}.\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must contain boolean data types. Received {v.dtype}.\"\n        )\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must contain only boolean values. Received {v.dtype}\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if v.dtype != bool:\n        raise ValueError(f\"{cls.__name__}: {field.name} must contain only boolean values. Received {v.dtype}.\")\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must contain boolean data types. Received {v.dtype}\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}: {field.name} must contain only boolean values. Received {v.dtype}\")\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must contain only boolean values. Received {v.dtype}\"\n        )\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must contain boolean data types. Received {v.dtype}.\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if v.dtype != np.bool_:\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be binary. Received {v.dtype}.\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must contain only boolean values. Received {v.dtype}\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if v.dtype != bool:\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must contain boolean values. Received {v.dtype}.\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if v.dtype != bool:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a binary array. Received {v.dtype}.\")\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if v.dtype != bool:\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must contain only boolean values. Got {v.dtype}.\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must contain boolean data types. Found {v.dtype}.\"\n        )\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must contain boolean values. Got {v.dtype}.\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must contain only boolean data types. Received {v.dtype}\"\n        )\n\n    return v"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  fn_x = x / (1 + 2 / 3 * x_norm)\n  return fn_x"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  fn_x = jnp.where(\n      x_norm > 1,\n      x / x_norm,\n      x_norm * (1 - 0.5 * x_norm**2 + 0.125 * x_norm**4),\n  )\n  return fn_x"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  x_norm_sq = x_norm ** 2\n  x_norm_cube = x_norm ** 3\n  x_norm_sq_cube = x_norm_sq * x_norm_cube\n  scale = (2 * jnp.sqrt(x_norm_sq_cube) - x_norm_sq) / x_norm_sq_cube\n  z = scale * x\n  return z"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  fn_x = x / (1 + x_mag)\n  fn_x_mag = jnp.linalg.norm(fn_x, axis=-1, keepdims=True)\n  return fn_x * (1 + fn_x_mag)"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  x_norm_sq = x_norm**2\n  # The following is the analytic form of contract(x) for 3D inputs.\n  scale = (2 * jnp.sqrt(x_norm_sq) - 1) / x_norm_sq\n  z = scale * x\n  return z"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Calculate the norm of the input tensor along the last dimension.\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  # Calculate the scaling factor using the norm and the scaling function.\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  # Apply the scaling factor to the input tensor.\n  z = scale * x\n  return z"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  return jnp.where(x_norm > 1.0, x / x_norm, x)\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  # Avoid dividing by zero.\n  x_norm = jnp.maximum(x_norm, jnp.finfo(jnp.float32).tiny)\n  # Compute the scaling factor based on the norm of x.\n  scale = 2 * jnp.sqrt(x_norm) - 1\n  # Apply the scaling factor to x.\n  z = scale * x\n  return z\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n    for dict_column in dict_columns:\n        summary_df[dict_column] = summary_df[dict_column].apply(lambda x: ast.literal_eval(x))\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n    summary_df = pd.read_csv(summary_path)\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(lambda x: ast.literal_eval(x))\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(lambda x: ast.literal_eval(x))\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    summary_df = pd.read_csv(summary_path)\n    if dict_columns is None:\n        dict_columns = ['module_params']\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n    summary_df = pd.read_csv(summary_path)\n    for column_name in dict_columns:\n        summary_df[column_name] = summary_df[column_name].apply(lambda x: ast.literal_eval(x))\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    df = pd.read_csv(summary_path)\n    for column in dict_columns:\n        df[column] = df[column].apply(lambda x: ast.literal_eval(x))\n    return df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n    summary_df = pd.read_csv(summary_path)\n    for dict_column in dict_columns:\n        summary_df[dict_column] = summary_df[dict_column].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    df = pd.read_csv(summary_path)\n    for dict_column in dict_columns:\n        df[dict_column] = df[dict_column].apply(lambda x: ast.literal_eval(x))\n    return df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    def convert_dict_columns(df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:\n        for column in columns:\n            df[column] = df[column].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n        return df\n\n    summary_df = pd.read_csv(summary_path)\n    summary_df = convert_dict_columns(summary_df, dict_columns)\n    return summary_df"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    summary_df = pd.read_csv(summary_path)\n\n    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    for dict_column in dict_columns:\n        summary_df[dict_column] = summary_df[dict_column].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(lambda x: ast.literal_eval(x))\n    return summary_df\n\n"}
{"namespace": "coord.isotropize", "completion": "  # Compute the determinant of the covariance matrix.\n  det = jnp.linalg.det(cov)\n\n  # Check if the determinant is non-negative.\n  if not jnp.all(det >= 0):\n    raise ValueError('Covariance matrix must have non-negative determinant.')\n\n  # Compute the square root of the determinant.\n  if mode == 'fast':\n    sqrt_det = jnp.sqrt(det)\n  elif mode == 'accurate':\n    sqrt_det = jnp.exp(0.5 * jnp.log(det))\n  else:\n    raise ValueError(f'Unknown mode: {mode}')\n\n  # Compute the isotropic covariance matrix.\n  cov_iso = cov / sqrt_det[Ellipsis, None, None]\n\n  # Check if the resulting covariance matrix is valid.\n  if not jnp.all(jnp.linalg.det(cov_iso) > 0):\n    raise ValueError('Isotropic covariance matrix must have positive determinant.')\n\n  return cov_iso"}
{"namespace": "coord.isotropize", "completion": "  # Compute the determinant of the covariance matrix.\n  determinant = jnp.linalg.det(cov)\n\n  # Check if the determinant is valid.\n  if jnp.any(determinant <= 0):\n    raise ValueError('Invalid determinant encountered.')\n\n  # Compute the logarithm of the determinant.\n  log_determinant = jnp.log(determinant)\n\n  # Check if the logarithm of the determinant is valid.\n  if jnp.any(jnp.isnan(log_determinant)):\n    raise ValueError('Invalid logarithm of determinant encountered.')\n\n  # Compute the scaling factor based on the mode.\n  if mode == 'fast':\n    scaling = determinant**(-1 / cov.shape[-1])\n  elif mode == 'accurate':\n    scaling = jnp.exp(-log_determinant / cov.shape[-1])\n  else:\n    raise ValueError('Invalid mode specified.')\n\n  # Compute the isotropic covariance matrix.\n  isotropic_cov = scaling * jnp.eye(cov.shape[-1])\n\n  return isotropic_cov"}
{"namespace": "coord.isotropize", "completion": "  # Compute the determinant of the input covariance matrix.\n  det = jnp.linalg.det(cov)\n\n  # Check if the determinant is valid (greater than zero).\n  if jnp.any(det <= 0):\n    raise ValueError('Input covariance matrix has invalid determinant.')\n\n  # Compute the logarithm of the determinant.\n  logdet = jnp.log(det)\n\n  # Check if the logarithm of the determinant is valid (not infinite).\n  if jnp.any(jnp.isinf(logdet)):\n    raise ValueError('Input covariance matrix has invalid logarithm of determinant.')\n\n  # Compute the square root of the determinant.\n  if mode == 'fast':\n    sqrt_det = jnp.sqrt(det)\n  elif mode == 'accurate':\n    sqrt_det = jnp.exp(0.5 * logdet)\n  else:\n    raise ValueError('Invalid mode. Choose either \"fast\" or \"accurate\".')\n\n  # Compute the diagonal of the isotropic covariance matrix.\n  diag = jnp.diag(cov)\n  diag_mean = jnp.mean(diag)\n  diag_iso = jnp.ones_like(diag) * diag_mean\n\n  # Compute the isotropic covariance matrix.\n  cov_iso = jnp.diag(diag_iso)\n\n  # Return the isotropic covariance matrix.\n  return cov_iso"}
{"namespace": "coord.isotropize", "completion": "  # Compute the determinant of the covariance matrix.\n  determinant = jnp.linalg.det(cov)\n\n  # Check if the determinant is non-negative.\n  if jnp.any(determinant < 0):\n    raise ValueError('Covariance matrix must have non-negative determinant.')\n\n  # Compute the square root of the determinant.\n  if mode == 'fast':\n    determinant_sqrt = jnp.sqrt(determinant)\n  elif mode == 'accurate':\n    determinant_sqrt = jnp.exp(0.5 * jnp.log(determinant))\n  else:\n    raise ValueError(f'Unsupported mode: {mode}')\n\n  # Compute the inverse of the square root of the covariance matrix.\n  cov_inv_sqrt = jnp.linalg.inv(jnp.sqrt(cov))\n\n  # Compute the isotropic covariance matrix.\n  cov_iso = jnp.eye(cov.shape[-1]) * determinant_sqrt\n\n  # Check if the isotropic covariance matrix is positive definite.\n  if jnp.any(jnp.linalg.eigvals(cov_iso) <= 0):\n    raise ValueError('Isotropic covariance matrix must be positive definite.')\n\n  # Return the isotropic covariance matrix.\n  return cov_iso"}
{"namespace": "coord.isotropize", "completion": "  # Compute the determinant of the covariance matrix.\n  det = jnp.linalg.det(cov)\n\n  # Check if the determinant is less than or equal to zero.\n  if jnp.any(det <= 0):\n    # If the determinant is less than or equal to zero, raise a ValueError.\n    raise ValueError(\"Covariance matrix must be positive definite.\")\n\n  # Compute the logarithm of the determinant of the covariance matrix.\n  log_det = jnp.log(det)\n\n  # Check if the logarithm of the determinant is infinite or NaN.\n  if jnp.any(jnp.isinf(log_det)) or jnp.any(jnp.isnan(log_det)):\n    # If the logarithm of the determinant is infinite or NaN, raise a ValueError.\n    raise ValueError(\"Determinant of covariance matrix is infinite or NaN.\")\n\n  # Compute the square root of the determinant of the covariance matrix.\n  sqrt_det = jnp.sqrt(det)\n\n  # Compute the inverse of the covariance matrix.\n  inv_cov = jnp.linalg.inv(cov)\n\n  # Compute the eigenvalues and eigenvectors of the covariance matrix.\n  eigvals, eigvecs = jnp.linalg.eigh(cov)\n\n  # Compute the square root of the eigenvalues.\n  sqrt_eigvals = jnp.sqrt(eigvals)\n\n  # Compute the diagonal matrix of the square root of the eigenvalues.\n  sqrt_eigvals_diag = jnp.diag(sqrt_eigvals)\n\n  # Compute the matrix of eigenvectors.\n  eigvecs_mat = eigvecs\n\n  # Compute the matrix of the square root of the inverse of the covariance matrix.\n  sqrt_inv_cov = eigvecs_mat @ sqrt_eigvals_diag @ jnp.linalg.inv(eigvecs_mat)\n\n  # Compute the matrix of the square root of the covariance matrix."}
{"namespace": "coord.isotropize", "completion": "  # Compute the determinant of the input covariance matrices.\n  det = jnp.linalg.det(cov)\n\n  # Check if the determinant is negative or zero, and raise an error if it is.\n  if jnp.any(det <= 0):\n    raise ValueError('Covariance matrix must be positive definite.')\n\n  # Compute the logarithm of the determinant of the input covariance matrices.\n  log_det = jnp.log(det)\n\n  # Check if the logarithm of the determinant is negative or zero, and raise an error if it is.\n  if jnp.any(jnp.isnan(log_det)):\n    raise ValueError('Covariance matrix must be positive definite.')\n\n  # Compute the square root of the covariance matrices using the specified mode.\n  if mode == 'fast':\n    # Compute the square root of the covariance matrices using the determinant directly.\n    sqrtm_cov = cov / jnp.sqrt(det)[Ellipsis, None, None]\n  elif mode == 'accurate':\n    # Compute the square root of the covariance matrices using the logarithm of the determinant.\n    sqrtm_cov = cov / jnp.exp(log_det / 2)[Ellipsis, None, None]\n  else:\n    raise ValueError(f'Unknown mode {mode}.')\n\n  # Return the isotropic covariance matrices with the same determinant as the input covariance matrices.\n  return sqrtm_cov"}
{"namespace": "coord.isotropize", "completion": "  # Compute the determinant of the input covariance matrix.\n  det = jnp.linalg.det(cov)\n\n  # Check if the determinant is positive.\n  if mode == 'fast':\n    det_check = det\n  elif mode == 'accurate':\n    det_check = jnp.log(det)\n  else:\n    raise ValueError(f'Unknown mode: {mode}')\n\n  # If the determinant is not positive, raise an error.\n  if jnp.any(det_check <= 0):\n    raise ValueError('Covariance matrix must be positive definite.')\n\n  # Compute the square root of the determinant.\n  if mode == 'fast':\n    det_sqrt = jnp.sqrt(det)\n  elif mode == 'accurate':\n    det_sqrt = jnp.exp(0.5 * jnp.log(det))\n\n  # Compute the inverse of the input covariance matrix.\n  cov_inv = jnp.linalg.inv(cov)\n\n  # Compute the isotropic covariance matrix.\n  cov_iso = det_sqrt * cov_inv\n\n  # Return the isotropic covariance matrix.\n  return cov_iso"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # This is the fastest option, but it's not numerically stable.\n    return cov / jnp.sqrt(jnp.linalg.det(cov))\n  elif mode == 'accurate':\n    # This is the most stable option, but it's also the slowest.\n    # Compute the log-determinant of the covariance matrix.\n    log_det = jnp.linalg.slogdet(cov)[1]\n    # Check if the log-determinant is valid.\n    if jnp.isnan(log_det):\n      raise ValueError('Invalid log-determinant.')\n    # Compute the inverse of the covariance matrix.\n    inv_cov = jnp.linalg.inv(cov)\n    # Check if the inverse of the covariance matrix is valid.\n    if jnp.isnan(inv_cov).any():\n      raise ValueError('Invalid inverse of covariance matrix.')\n    # Compute the isotropic covariance matrix.\n    return inv_cov / jnp.exp(log_det)\n  else:\n    raise ValueError(f'Invalid mode: {mode}')"}
{"namespace": "coord.isotropize", "completion": "  # Compute the determinant of the covariance matrix.\n  det = jnp.linalg.det(cov)\n\n  # Check if the determinant is valid.\n  if jnp.any(det <= 0):\n    raise ValueError('Invalid determinant.')\n\n  # Compute the logarithm of the determinant.\n  log_det = jnp.log(det)\n\n  # Check if the logarithm of the determinant is valid.\n  if jnp.any(jnp.isnan(log_det)):\n    raise ValueError('Invalid logarithm of determinant.')\n\n  # Compute the square root of the covariance matrix.\n  if mode == 'fast':\n    # Compute the square root of the determinant directly.\n    sqrt_det = jnp.sqrt(det)\n  elif mode == 'accurate':\n    # Compute the logarithm of the square root of the determinant.\n    log_sqrt_det = log_det / 2\n\n    # Compute the square root of the determinant using the logarithm.\n    sqrt_det = jnp.exp(log_sqrt_det)\n\n  # Compute the inverse of the covariance matrix.\n  inv_cov = jnp.linalg.inv(cov)\n\n  # Compute the diagonal of the isotropic covariance matrix.\n  diag = jnp.diag(inv_cov)\n\n  # Compute the isotropic covariance matrix.\n  isotropic_cov = sqrt_det * jnp.eye(cov.shape[-1])\n\n  return isotropic_cov"}
{"namespace": "coord.isotropize", "completion": "  # Compute the determinant of the covariance matrix.\n  det = jnp.linalg.det(cov)\n\n  # Check if the determinant is zero or negative.\n  if jnp.any(det <= 0.0):\n    raise ValueError('Covariance matrix must be positive definite.')\n\n  # Compute the logarithm of the determinant.\n  log_det = jnp.log(det)\n\n  # Check if the logarithm of the determinant is infinite.\n  if jnp.any(jnp.isinf(log_det)):\n    raise ValueError('Covariance matrix must be invertible.')\n\n  # Compute the square root of the covariance matrix using the mode specified.\n  if mode == 'fast':\n    sqrt_cov = jnp.sqrt(det) * jnp.linalg.inv(cov)\n  elif mode == 'accurate':\n    sqrt_cov = jnp.linalg.cholesky(cov)\n  else:\n    raise ValueError('Invalid mode. Must be either \"fast\" or \"accurate\".')\n\n  # Compute the isotropic covariance matrix by taking the average of the diagonal elements of the square root of the covariance matrix.\n  isotropic_cov = jnp.mean(jnp.diag(sqrt_cov))\n\n  # Construct an isotropic covariance matrix with the same determinant as the input covariance matrix.\n  isotropic_cov = jnp.eye(cov.shape[-1]) * isotropic_cov\n\n  return isotropic_cov"}
{"namespace": "coord.isotropize", "completion": "  # Compute the determinant of the input covariance matrix or matrices.\n  det = jnp.linalg.det(cov)\n\n  # Check if the determinant is valid (not zero or negative).\n  if mode == 'fast':\n    # If the determinant is invalid, return the input covariance matrix or matrices.\n    if jnp.any(det <= 0):\n      return cov\n  else:\n    # If the determinant is invalid, return the input covariance matrix or matrices.\n    if jnp.any(det <= 0):\n      return cov\n    # Compute the logarithm of the determinant of the input covariance matrix or matrices.\n    log_det = jnp.log(det)\n    # Check if the logarithm of the determinant is valid (not infinite or NaN).\n    if jnp.any(jnp.isinf(log_det)) or jnp.any(jnp.isnan(log_det)):\n      return cov\n\n  # Compute the square root of the input covariance matrix or matrices.\n  sqrt_cov = jnp.linalg.cholesky(cov)\n\n  # Compute the diagonal of the square root of the input covariance matrix or matrices.\n  diag_sqrt_cov = jnp.diagonal(sqrt_cov, axis1=-2, axis2=-1)\n\n  # Compute the product of the diagonal of the square root of the input covariance matrix or matrices with itself.\n  diag_sqrt_cov_prod = jnp.prod(diag_sqrt_cov, axis=-1, keepdims=True)\n\n  # Compute the isotropic covariance matrix or matrices with the same determinant as the input covariance matrix or matrices.\n  isotropic_cov = diag_sqrt_cov_prod * jnp.eye(diag_sqrt_cov.shape[-1])\n\n  return isotropic_cov"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    return cov / jnp.sqrt(jnp.linalg.det(cov))\n  elif mode == 'accurate':\n    # Compute the log determinant of the covariance matrix.\n    log_det = jnp.linalg.slogdet(cov)[1]\n\n    # Check if the determinant is non-zero.\n    if jnp.any(jnp.isinf(log_det)):\n      raise ValueError('Covariance matrix has invalid determinant.')\n\n    # Compute the square root of the covariance matrix.\n    cov_sqrt = jnp.linalg.cholesky(cov)\n\n    # Compute the inverse of the square root of the covariance matrix.\n    cov_sqrt_inv = jnp.linalg.inv(cov_sqrt)\n\n    # Compute the diagonal elements of the isotropic covariance matrix.\n    cov_iso = jnp.diagonal(cov_sqrt_inv @ cov_sqrt, axis1=-2, axis2=-1)\n\n    # Compute the determinant of the isotropic covariance matrix.\n    det_iso = jnp.prod(cov_iso, axis=-1)\n\n    # Check if the determinant of the isotropic covariance matrix is non-zero.\n    if jnp.any(jnp.isinf(jnp.log(det_iso))):\n      raise ValueError('Isotropic covariance matrix has invalid determinant.')\n\n    # Compute the scaling factor for the isotropic covariance matrix.\n    scale = jnp.exp(log_det / (2 * jnp.log(det_iso)))\n\n    # Compute the isotropic covariance matrix.\n    cov_iso = scale * cov_iso\n\n    return cov_iso\n  else:\n    raise ValueError('Invalid mode. Choose either \"fast\" or \"accurate\".')"}
{"namespace": "coord.isotropize", "completion": "  # Compute the determinant of the input covariance matrix.\n  det = jnp.linalg.det(cov)\n\n  # Check if the determinant is valid.\n  if mode == 'fast':\n    # If the determinant is less than or equal to zero, return the input covariance matrix.\n    if jnp.any(det <= 0):\n      return cov\n  elif mode == 'accurate':\n    # If the determinant is less than or equal to zero, return the input covariance matrix.\n    if jnp.any(det <= 0):\n      return cov\n    # If the determinant is infinite, return the input covariance matrix.\n    if jnp.any(jnp.isinf(det)):\n      return cov\n  else:\n    raise ValueError(f'Unknown mode {mode}')\n\n  # Compute the logarithm of the determinant of the input covariance matrix.\n  log_det = jnp.log(det)\n\n  # Compute the inverse of the input covariance matrix.\n  inv_cov = jnp.linalg.inv(cov)\n\n  # Compute the eigenvalues and eigenvectors of the inverse covariance matrix.\n  eigval, eigvec = jnp.linalg.eigh(inv_cov)\n\n  # Compute the diagonal matrix of the eigenvalues.\n  diag = jnp.diag(eigval)\n\n  # Compute the square root of the diagonal matrix of the eigenvalues.\n  diag_sqrt = jnp.sqrt(diag)\n\n  # Compute the matrix of the eigenvectors multiplied by the square root of the diagonal matrix of the eigenvalues.\n  sqrt_cov = jnp.matmul(eigvec, jnp.matmul(diag_sqrt, jnp.transpose(eigvec)))\n\n  # Compute the determinant of the square root of the covariance matrix.\n  sqrt_det = jnp.linalg.det(sqrt_cov)\n\n  # Check if the determinant of the square root of the covariance matrix is valid.\n  if mode == 'fast':\n    # If the determinant is less than or equal to zero, return"}
{"namespace": "coord.isotropize", "completion": "  # Compute the determinant of the input covariance matrix.\n  det = jnp.linalg.det(cov)\n  # Check if the determinant is valid (i.e., not zero or negative).\n  if jnp.any(det <= 0):\n    raise ValueError('Covariance matrix must be positive definite.')\n\n  # Compute the logarithm of the determinant of the input covariance matrix.\n  log_det = jnp.log(det)\n  # Check if the logarithm of the determinant is valid (i.e., not negative or infinite).\n  if jnp.any(jnp.isnan(log_det)) or jnp.any(jnp.isinf(log_det)):\n    raise ValueError('Covariance matrix must have a valid determinant.')\n\n  # Compute the square root of the input covariance matrix using the specified mode.\n  if mode == 'fast':\n    # Compute the square root of the input covariance matrix using the determinant directly.\n    sqrt_cov = jnp.sqrt(det) * cov\n  elif mode == 'accurate':\n    # Compute the square root of the input covariance matrix using the logarithm of the determinant.\n    sqrt_cov = jnp.exp(0.5 * log_det) * cov\n  else:\n    raise ValueError('Invalid mode specified. Choose either \"fast\" or \"accurate\".')\n\n  # Compute the trace of the square root of the input covariance matrix.\n  trace = jnp.trace(sqrt_cov, axis1=-2, axis2=-1)\n  # Check if the trace is valid (i.e., not zero or negative).\n  if jnp.any(trace <= 0):\n    raise ValueError('Covariance matrix must be positive definite.')\n\n  # Compute the inverse of the trace.\n  inv_trace = 1 / trace\n  # Compute the isotropic covariance matrix by multiplying the square root of the input covariance matrix with its inverse.\n  iso_cov = sqrt_cov * inv_trace\n\n  return iso_cov"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # This is the fast way, which uses the determinant directly.\n    # It's faster, but less accurate than the accurate way.\n    # It also doesn't handle invalid determinants.\n    return cov / jnp.sqrt(jnp.linalg.det(cov))\n  elif mode == 'accurate':\n    # This is the accurate way, which uses the logarithm of the determinant.\n    # It's slower, but more accurate than the fast way.\n    # It also handles invalid determinants.\n    # Compute the determinant of the covariance matrix.\n    det = jnp.linalg.det(cov)\n\n    # Check if the determinant is valid.\n    if jnp.any(det <= 0):\n      raise ValueError('Covariance matrix must be positive definite.')\n\n    # Compute the logarithm of the determinant.\n    log_det = jnp.log(det)\n\n    # Check if the logarithm of the determinant is valid.\n    if jnp.any(jnp.isinf(log_det)):\n      raise ValueError('Covariance matrix must be positive definite.')\n\n    # Compute the inverse of the covariance matrix.\n    inv_cov = jnp.linalg.inv(cov)\n\n    # Compute the isotropic covariance matrix.\n    return inv_cov / jnp.sqrt(log_det)\n  else:\n    raise ValueError('Invalid mode.')"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # Fast mode: use the determinant directly.\n    det = jnp.linalg.det(cov)\n    if jnp.any(det <= 0):\n      raise ValueError('Covariance matrix must be positive definite.')\n    # Compute the determinant of the covariance matrix using the Cholesky decomposition.\n    # The determinant of a matrix is the product of the diagonal elements of the Cholesky decomposition.\n    chol = jnp.linalg.cholesky(cov)\n    det = jnp.prod(jnp.diagonal(chol, axis1=-2, axis2=-1), axis=-1)\n    # Compute the isotropic covariance matrix.\n    iso_cov = jnp.eye(cov.shape[-1]) * det\n  elif mode == 'accurate':\n    # Accurate mode: use the logarithm of the determinant.\n    # Compute the logarithm of the determinant of the covariance matrix using the Cholesky decomposition.\n    # The logarithm of the determinant of a matrix is the sum of the logarithms of the diagonal elements of the Cholesky decomposition.\n    chol = jnp.linalg.cholesky(cov)\n    log_det = jnp.sum(jnp.log(jnp.diagonal(chol, axis1=-2, axis2=-1)), axis=-1)\n    # Compute the isotropic covariance matrix.\n    iso_cov = jnp.eye(cov.shape[-1]) * jnp.exp(log_det)\n  else:\n    raise ValueError(f'Invalid mode: {mode}. Must be either \"fast\" or \"accurate\".')\n\n  # Check if the computed isotropic covariance matrix is positive definite.\n  if not jnp.all(jnp.linalg.eigvals(iso_cov) > 0):\n    raise ValueError('Computed isotropic covariance matrix is not positive definite.')\n\n  return iso_cov"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # The determinant of a matrix is the product of its eigenvalues.\n    # The determinant of a diagonal matrix is the product of its diagonal entries.\n    # We use the diagonalization of the matrix to compute the determinant.\n    # This is a fast and stable way to compute the determinant of a matrix.\n    # However, it is not the most accurate way to compute the determinant of a matrix.\n    eigvec, eigval = jax.lax.linalg.eigh(\n        cov, symmetrize_input=False, sort_eigenvalues=False\n    )\n    # The determinant of the matrix is the product of its eigenvalues.\n    # We use the diagonalization of the matrix to compute the determinant.\n    # This is a fast and stable way to compute the determinant of a matrix.\n    # However, it is not the most accurate way to compute the determinant of a matrix.\n    det = jnp.prod(eigval, axis=-1)\n  elif mode == 'accurate':\n    # The determinant of a matrix is the product of its eigenvalues.\n    # The determinant of a diagonal matrix is the product of its diagonal entries.\n    # We use the diagonalization of the matrix to compute the determinant.\n    # This is a fast and stable way to compute the determinant of a matrix.\n    # However, it is not the most accurate way to compute the determinant of a matrix.\n    eigvec, eigval = jax.lax.linalg.eigh(\n        cov, symmetrize_input=False, sort_eigenvalues=False\n    )\n    # The determinant of the matrix is the product of its eigenvalues.\n    # We use the diagonalization of the matrix to compute the determinant.\n    # This is a fast and stable way to compute the determinant of a matrix.\n    # However, it is not the most accurate way to compute the determinant of a matrix.\n    det = jnp.exp(jnp.sum(jnp.log(eigval), axis=-1))\n  else:\n    raise ValueError(f'Invalid mode: {mode}')"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # This is the fast version of the isotropization.\n    eigval, eigvec = jnp.linalg.eigh(cov)\n    # We use the determinant of the original covariance matrix to scale the\n    # isotropic covariance matrix.\n    det = jnp.prod(eigval, axis=-1)\n    # We use the log of the determinant to avoid numerical issues.\n    det = jnp.where(det < 1e-5, jnp.array(1e-5), det)\n    log_det = jnp.log(det)\n    eigval = jnp.maximum(eigval, jnp.zeros_like(eigval))\n    eigval = jnp.exp(log_det / cov.shape[-1] - eigval)\n    cov = jnp.matmul(eigvec * eigval[..., None, :], jnp.swapaxes(eigvec, -1, -2))\n  elif mode == 'accurate':\n    # This is the accurate version of the isotropization.\n    eigval, eigvec = jnp.linalg.eigh(cov)\n    # We use the determinant of the original covariance matrix to scale the\n    # isotropic covariance matrix.\n    det = jnp.prod(eigval, axis=-1)\n    # We use the log of the determinant to avoid numerical issues.\n    det = jnp.where(det < 1e-5, jnp.array(1e-5), det)\n    log_det = jnp.log(det)\n    eigval = jnp.maximum(eigval, jnp.zeros_like(eigval))\n    eigval = jnp.exp(log_det / cov.shape[-1] - eigval)\n    cov = jnp.matmul(eigvec * eigval[..., None, :], jnp.swapaxes(eigvec, -1, -2))\n  else:\n    raise ValueError(f'Unknown mode {mode}.')\n  return"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # This is the fastest way to compute the isotropic covariance matrices, but it can lead to numerical instability when the determinant of the input covariance matrix is very small or very large.\n    det = jnp.linalg.det(cov)\n    det_root = jnp.sqrt(det)\n    return cov / det_root\n  elif mode == 'accurate':\n    # This is a more accurate way to compute the isotropic covariance matrices, but it is slower than the 'fast' mode.\n    # Compute the logarithm of the determinant of the input covariance matrix.\n    log_det = jnp.linalg.slogdet(cov)[1]\n    # Check if the determinant is too small or too large.\n    if jnp.any(jnp.isnan(log_det)):\n        raise ValueError(\"Input covariance matrix has determinant of zero.\")\n    # Compute the logarithm of the square root of the determinant of the input covariance matrix.\n    log_det_root = log_det / 2\n    # Compute the logarithm of the square root of the diagonal elements of the input covariance matrix.\n    log_diag_root = jnp.diag(jnp.linalg.cholesky(cov))\n    # Compute the logarithm of the diagonal elements of the output isotropic covariance matrix.\n    log_diag_iso = log_diag_root + log_det_root\n    # Compute the diagonal elements of the output isotropic covariance matrix.\n    diag_iso = jnp.exp(log_diag_iso)\n    # Construct the output isotropic covariance matrix.\n    cov_iso = jnp.diag(diag_iso)\n    return cov_iso\n  else:\n    raise ValueError(\"Invalid mode. Choose either 'fast' or 'accurate'.\")"}
{"namespace": "coord.isotropize", "completion": "  # Compute the determinant of the input covariance matrices.\n  det = jnp.linalg.det(cov)\n\n  # Check if the determinant is zero or negative.\n  if jnp.any(det <= 0):\n    # If the determinant is zero or negative, we need to handle it.\n    if mode == 'fast':\n      # If the mode is 'fast', we can simply use the determinant directly.\n      det_sqrt = jnp.sqrt(det)\n    else:\n      # If the mode is 'accurate', we use the logarithm of the determinant.\n      det_sqrt = jnp.exp(0.5 * jnp.log(det))\n\n    # Compute the isotropic covariance matrix using the determinant.\n    cov_iso = det_sqrt * jnp.eye(cov.shape[-1])\n  else:\n    # If the determinant is positive, we can use the Cholesky decomposition.\n    cov_iso = jnp.linalg.cholesky(cov)\n\n  return cov_iso\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='XAgent Command Line Interface')\n    parser.add_argument('--task', type=str, required=True, help='Task description')\n    parser.add_argument('--upload-files', type=str, nargs='*', help='Files to upload')\n    parser.add_argument('--model', type=str, help='Model identifier')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs')\n    parser.add_argument('--mode', type=str, default='auto', choices=['auto', 'manual'], help='Operational mode')\n    parser.add_argument('--quiet', action='store_true', help='Run in quiet mode')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Enable asking for human assistance')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts')\n    parser.add_argument('--config-file', type=str, default=os.environ.get('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file')\n\n    return parser.parse_args()"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='XAgent Command Line Interface')\n    parser.add_argument('--task', type=str, required=True, help='Task description')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='Files to upload')\n    parser.add_argument('--model', type=str, help='Model identifier')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs')\n    parser.add_argument('--mode', type=str, default='auto', choices=['auto', 'manual'], help='Operational mode')\n    parser.add_argument('--quiet', action='store_true', help='Run in quiet mode')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Enable asking for human assistance')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts')\n    parser.add_argument('--config-file', type=str, default=os.environ.get('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file')\n\n    return parser.parse_args()"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Command Line Interface')\n    parser.add_argument('--task', type=str, required=True, help='Task description')\n    parser.add_argument('--upload-files', type=str, nargs='*', help='Files to upload')\n    parser.add_argument('--model', type=str, help='Model identifier')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs')\n    parser.add_argument('--mode', type=str, default='auto', choices=['auto', 'manual'], help='Operational mode')\n    parser.add_argument('--quiet', action='store_true', help='Run in quiet mode')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Enable asking for human assistance')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts')\n    parser.add_argument('--config-file', type=str, default=os.environ.get('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file')\n\n    return parser.parse_args()"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='XAgent Command Line Interface')\n    parser.add_argument('--task', type=str, required=True, help='Task description')\n    parser.add_argument('--upload-files', type=str, nargs='*', help='Files to upload')\n    parser.add_argument('--model', type=str, help='Model identifier')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs')\n    parser.add_argument('--mode', type=str, default='auto', choices=['auto', 'manual'], help='Operational mode')\n    parser.add_argument('--quiet', action='store_true', help='Run in quiet mode')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Enable asking for human assistance')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts')\n    parser.add_argument('--config-file', type=str, default=os.environ.get('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file')\n\n    args = parser.parse_args()\n\n    return args"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Command Line Interface for XAgent')\n    parser.add_argument('--task', type=str, required=True, help='Task description')\n    parser.add_argument('--upload-files', nargs='+', type=str, help='List of files to upload')\n    parser.add_argument('--model', type=str, help='Model identifier')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs')\n    parser.add_argument('--mode', type=str, default='auto', choices=['auto', 'manual'], help='Operational mode')\n    parser.add_argument('--quiet', action='store_true', help='Run in quiet mode with minimal output')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts')\n    parser.add_argument('--config-file', type=str, default=os.environ.get('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file')\n\n    return parser.parse_args()"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Command Line Interface for XAgent')\n    parser.add_argument('--task', type=str, required=True, help='Task description')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload')\n    parser.add_argument('--model', type=str, help='Model identifier for the task')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs')\n    parser.add_argument('--mode', type=str, default='auto', choices=['auto', 'manual'], help='Operational mode')\n    parser.add_argument('--quiet', action='store_true', help='Run in quiet mode with minimal output')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Enable asking for human assistance during task execution')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts')\n    parser.add_argument('--config-file', type=str, default=os.environ.get('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file')\n\n    args = parser.parse_args()\n\n    return args"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"XAgent Command Line Interface\")\n\n    parser.add_argument(\"--task\", type=str, required=True, help=\"Task description\")\n    parser.add_argument(\"--upload-files\", nargs=\"*\", type=str, help=\"List of files to upload\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", choices=[\"auto\", \"manual\"], help=\"Operational mode\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"Run in quiet mode with minimal output\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Enable asking for human assistance during task execution\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts\")\n    parser.add_argument(\"--config-file\", type=str, default=os.environ.get(\"CONFIG_FILE\", \"assets/config.yml\"), help=\"Path to the configuration file\")\n\n    return parser.parse_args()"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"Command Line Interface\")\n    parser.add_argument(\"--task\", type=str, required=True, help=\"Task description\")\n    parser.add_argument(\"--upload-files\", nargs='+', type=str, help=\"Files to upload\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", choices=[\"auto\", \"manual\"], help=\"Operational mode\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"Run in quiet mode\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Enable asking for human assistance\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts\")\n    parser.add_argument(\"--config-file\", type=str, default=os.environ.get('CONFIG_FILE', \"assets/config.yml\"), help=\"Path to the configuration file\")\n\n    return parser.parse_args()"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Command Line Interface for XAgent')\n\n    parser.add_argument('--task', type=str, required=True, help='Task description')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload')\n    parser.add_argument('--model', type=str, help='Model identifier for the task')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs')\n    parser.add_argument('--mode', type=str, default='auto', choices=['auto', 'manual'], help='Operational mode')\n    parser.add_argument('--quiet', action='store_true', help='Run in quiet mode')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts')\n    parser.add_argument('--config-file', type=str, default=os.environ.get('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file')\n\n    return parser.parse_args()"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Command Line Interface')\n    parser.add_argument('--task', type=str, required=True, help='Task description')\n    parser.add_argument('--upload-files', type=str, nargs='*', help='List of files to upload')\n    parser.add_argument('--model', type=str, help='Model identifier')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs')\n    parser.add_argument('--mode', type=str, default='auto', choices=['auto', 'manual'], help='Operational mode')\n    parser.add_argument('--quiet', action='store_true', help='Run in quiet mode with minimal output')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Enable asking for human assistance during task execution')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts')\n    parser.add_argument('--config-file', type=str, default=os.environ.get('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file')\n\n    return parser.parse_args()"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"XAgent Command Line Interface\")\n\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", nargs='*', type=str, help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", choices=[\"auto\", \"manual\"], help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts, specifying"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='XAgent Command Line Interface')\n    parser.add_argument('--task', type=str, required=True, help='Task description')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload')\n    parser.add_argument('--model', type=str, help='Model identifier')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs')\n    parser.add_argument('--mode', type=str, default='auto', choices=['auto', 'manual'], help='Operational mode')\n    parser.add_argument('--quiet', action='store_true', help='Run in quiet mode with minimal output')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts')\n    parser.add_argument('--config-file', type=str, default=os.environ.get('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file')\n\n    args = parser.parse_args()\n    return args"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Command line interface for XAgent')\n    parser.add_argument('--task', type=str, required=True, help='Task description')\n    parser.add_argument('--upload-files', type=str, nargs='*', help='List of files to upload')\n    parser.add_argument('--model', type=str, help='Model identifier for the task')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs')\n    parser.add_argument('--mode', type=str, choices=['auto', 'manual'], default='auto', help='Operational mode')\n    parser.add_argument('--quiet', action='store_true', help='Run in quiet mode with minimal output')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts')\n    parser.add_argument('--config-file', type=str, default=os.environ.get('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file')\n\n    return parser.parse_args()"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"XAgent Command Line Interface\")\n    parser.add_argument(\"--task\", type=str, required=True, help=\"Task description\")\n    parser.add_argument(\"--upload-files\", type=str, nargs='+', help=\"Files to upload\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", choices=[\"auto\", \"manual\"], help=\"Operational mode\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"Run in quiet mode with minimal output\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Enable asking for human assistance during task execution\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts\")\n    parser.add_argument(\"--config-file\", type=str, default=os.environ.get(\"CONFIG_FILE\", \"assets/config.yml\"), help=\"Path to the configuration file\")\n    return parser.parse_args()"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Command Line Interface for XAgent')\n    parser.add_argument('--task', type=str, required=True, help='Task description')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload')\n    parser.add_argument('--model', type=str, help='Model identifier for the task')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs')\n    parser.add_argument('--mode', type=str, default='auto', help='Operational mode, can be \"auto\" or \"manual\"')\n    parser.add_argument('--quiet', action='store_true', help='Run in quiet mode with minimal output')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Enable asking for human assistance during task execution')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts')\n    parser.add_argument('--config-file', type=str, default=os.environ.get('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file')\n\n    return parser.parse_args()"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='Command Line Interface')\n    parser.add_argument('--task', type=str, required=True, help='Task description')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload')\n    parser.add_argument('--model', type=str, help='Model identifier')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs')\n    parser.add_argument('--mode', type=str, default='auto', choices=['auto', 'manual'], help='Operational mode')\n    parser.add_argument('--quiet', action='store_true', help='Run in quiet mode with minimal output')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Enable asking for human assistance')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts')\n    parser.add_argument('--config-file', type=str, default=os.environ.get('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file')\n    args = parser.parse_args()\n    return args"}
{"namespace": "run.parse_args", "completion": "    # Create an ArgumentParser object\n    parser = argparse.ArgumentParser(description='Command Line Interface for XAgent')\n\n    # Add arguments to the parser\n    parser.add_argument('--task', type=str, required=True, help='Task description')\n    parser.add_argument('--upload-files', nargs='+', type=str, help='List of files to upload')\n    parser.add_argument('--model', type=str, help='Model identifier for the task')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs')\n    parser.add_argument('--mode', type=str, default='auto', help='Operational mode')\n    parser.add_argument('--quiet', action='store_true', help='Run in quiet mode with minimal output')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts')\n    parser.add_argument('--config-file', type=str, default=os.environ.get('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file')\n\n    # Parse the command line arguments\n    args = parser.parse_args()\n\n    # Return the parsed arguments\n    return args"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='XAgent command line interface')\n\n    parser.add_argument('--task', type=str, required=True, help='The task description, specifying what task should be performed')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload, allowing multiple files to be specified')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs')\n    parser.add_argument('--mode', type=str, default='auto', help='Operational mode, which can be \\'auto\\' or \\'manual\\', specifying how the task should be executed')\n    parser.add_argument('--quiet', action='store_true', default=False, help='If set, the program runs in quiet mode with minimal output')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts, specifying how many times"}
{"namespace": "run.parse_args", "completion": "    # Create an ArgumentParser object\n    parser = argparse.ArgumentParser(description='Process command line arguments.')\n\n    # Add arguments to the parser\n    parser.add_argument('--task', required=True, type=str, help='The task description, specifying what task should be performed.')\n    parser.add_argument('--upload-files', nargs='*', type=str, help='List of files to upload, allowing multiple files to be specified.')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use.')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs.')\n    parser.add_argument('--mode', type=str, default='auto', choices=['auto', 'manual'], help='Operational mode, which can be \\'auto\\' or \\'manual\\', specifying how the task should be executed.')\n    parser.add_argument('--quiet', action='store_true', help='If set, the program runs in quiet mode with minimal output.')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be.')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution.')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans.')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be.')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.')\n   "}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"Command Line Interface for XAgent\")\n\n    # Add arguments for task, upload_files, model, record_dir, mode, quiet, and config_file\n    parser.add_argument(\"--task\", type=str, required=True, help=\"Task description\")\n    parser.add_argument(\"--upload-files\", nargs=\"*\", help=\"Files to upload\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", choices=[\"auto\", \"manual\"], help=\"Operational mode\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"Run in quiet mode\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Flag to enable asking for human assistance\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts\")\n    parser.add_argument(\"--config-file\", type=str, default=os.environ.get(\"CONFIG_FILE\", \"assets/config.yml\"), help=\"Path to the configuration file\")\n\n    # Parse the command line arguments\n    return parser.parse_args()"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. Got shape {v.shape}\")\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. Got shape {v.shape}\")\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. Got shape {v.shape}\")\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a list of 2D points. Got shape {v.shape}.\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a list of 2D points. Got shape {v.shape}.\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. Got {v.shape}\")\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. Got {v.shape}\")\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. Received {v.shape}\")\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. Received {v}\")\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. Received {v}\")\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a list of 2D points. Got shape {v.shape}.\"\n        )\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a list of 2D points. Got shape {v.shape}\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a list of 2D points. Got {v.shape}.\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. Got shape {v.shape}\")\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of points. Got shape {v.shape}\")\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a list of 2D points. Got shape {v.shape}\"\n        )\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of points, i.e. shape (_, 2).\")\n\n    return v"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunks_index[worker_idx] = 0\n        for interval in intervals:\n            if indexes[worker_idx] < interval[1] - interval[0]:\n                break\n            indexes[worker_idx] -= interval[1] - interval[0]\n            chunks_index[worker_idx] += 1\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunks_index[worker_idx] = 0\n        for interval in intervals:\n            interval_size = interval[1] - interval[0]\n            if indexes[worker_idx] >= interval_size:\n                indexes[worker_idx] -= interval_size\n                chunks_index[worker_idx] += 1\n            else:\n                break\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunk_index = 0\n        for interval in intervals:\n            if indexes[worker_idx] < interval[1] - interval[0]:\n                break\n            indexes[worker_idx] -= interval[1] - interval[0]\n            chunk_index += 1\n        chunks_index[worker_idx] = chunk_index\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunks_index[worker_idx] = 0\n        while chunks_index[worker_idx] < len(intervals):\n            interval = intervals[chunks_index[worker_idx]]\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                indexes[worker_idx] -= interval[1] - interval[0]\n                chunks_index[worker_idx] += 1\n            else:\n                break\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunks_index[worker_idx] = 0\n        for interval in intervals:\n            interval_size = interval[1] - interval[0]\n            if indexes[worker_idx] >= interval_size:\n                indexes[worker_idx] -= interval_size\n                chunks_index[worker_idx] += 1\n            else:\n                break\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunks_index[worker_idx] = 0\n\n        while indexes[worker_idx] >= intervals[chunks_index[worker_idx]][1] - intervals[chunks_index[worker_idx]][0]:\n            indexes[worker_idx] -= intervals[chunks_index[worker_idx]][1] - intervals[chunks_index[worker_idx]][0]\n            chunks_index[worker_idx] += 1\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx in workers_intervals:\n        chunks_index[worker_idx] = 0\n\n    for worker_idx in workers_intervals:\n        while True:\n            interval = workers_intervals[worker_idx][chunks_index[worker_idx]]\n            size = interval[1] - interval[0]\n            if indexes[worker_idx] >= size:\n                indexes[worker_idx] -= size\n                chunks_index[worker_idx] += 1\n            else:\n                break\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunks_index[worker_idx] = 0\n\n    for worker_idx, intervals in workers_intervals.items():\n        chunk_size = intervals[chunks_index[worker_idx]][1] - intervals[chunks_index[worker_idx]][0]\n        while indexes[worker_idx] >= chunk_size:\n            indexes[worker_idx] -= chunk_size\n            chunks_index[worker_idx] += 1\n            chunk_size = intervals[chunks_index[worker_idx]][1] - intervals[chunks_index[worker_idx]][0]\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_index = {}\n    for worker_idx in range(len(workers_intervals)):\n        chunk_index[worker_idx] = 0\n\n    for worker_idx, intervals in workers_intervals.items():\n        for interval in intervals:\n            size = interval[1] - interval[0]\n            if indexes[worker_idx] >= size:\n                indexes[worker_idx] -= size\n                chunk_index[worker_idx] += 1\n            else:\n                break\n\n    return chunk_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_indexes = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunk_indexes[worker_idx] = 0\n        for interval in intervals:\n            if indexes[worker_idx] >= (interval[1] - interval[0]):\n                indexes[worker_idx] -= (interval[1] - interval[0])\n                chunk_indexes[worker_idx] += 1\n            else:\n                break\n    return chunk_indexes, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunks_index[worker_idx] = 0\n\n    for worker_idx, intervals in workers_intervals.items():\n        while indexes[worker_idx] >= len(intervals[chunks_index[worker_idx]]):\n            indexes[worker_idx] -= len(intervals[chunks_index[worker_idx]])\n            chunks_index[worker_idx] += 1\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_indexes = {}\n    for worker_idx in range(len(workers_intervals)):\n        chunk_indexes[worker_idx] = 0\n\n    for worker_idx, intervals in workers_intervals.items():\n        current_index = indexes[worker_idx]\n        for interval in intervals:\n            interval_size = interval[1] - interval[0]\n            if current_index >= interval_size:\n                current_index -= interval_size\n                chunk_indexes[worker_idx] += 1\n            else:\n                indexes[worker_idx] = current_index\n                break\n\n    return chunk_indexes, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunks_index[worker_idx] = 0\n\n    for worker_idx, intervals in workers_intervals.items():\n        while True:\n            interval = intervals[chunks_index[worker_idx]]\n            if indexes[worker_idx] < interval[1] - interval[0]:\n                break\n            indexes[worker_idx] -= interval[1] - interval[0]\n            chunks_index[worker_idx] += 1\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunks_index[worker_idx] = 0\n\n        for interval in intervals:\n            size = interval[1] - interval[0]\n            if indexes[worker_idx] >= size:\n                indexes[worker_idx] -= size\n                chunks_index[worker_idx] += 1\n            else:\n                break\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunk_index = 0\n        for interval in intervals:\n            if indexes[worker_idx] < (interval[1] - interval[0]):\n                break\n            indexes[worker_idx] -= interval[1] - interval[0]\n            chunk_index += 1\n        chunks_index[worker_idx] = chunk_index\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        for interval in intervals:\n            size = interval[1] - interval[0]\n            if indexes[worker_idx] < size:\n                break\n            indexes[worker_idx] -= size\n        chunks_index[worker_idx] = intervals.index(interval)\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunk_index = 0\n        while True:\n            interval = intervals[chunk_index]\n            if indexes[worker_idx] >= (interval[1] - interval[0]):\n                indexes[worker_idx] -= interval[1] - interval[0]\n                chunk_index += 1\n            else:\n                break\n        chunks_index[worker_idx] = chunk_index\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunk_index = 0\n        for i, interval in enumerate(intervals):\n            if indexes[worker_idx] < interval[1] - interval[0]:\n                chunk_index = i\n                break\n            else:\n                indexes[worker_idx] -= interval[1] - interval[0]\n\n        chunks_index[worker_idx] = chunk_index\n\n    return chunks_index, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunk_indexes = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunk_indexes[worker_idx] = 0\n\n    for worker_idx, intervals in workers_intervals.items():\n        current_index = indexes[worker_idx]\n        while True:\n            interval = intervals[chunk_indexes[worker_idx]]\n            size = interval[1] - interval[0]\n            if current_index >= size:\n                current_index -= size\n                chunk_indexes[worker_idx] += 1\n            else:\n                indexes[worker_idx] = current_index\n                break\n\n    return chunk_indexes, indexes"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        current_index = indexes[worker_idx]\n        chunk_idx = 0\n        for interval in intervals:\n            if current_index > interval[1] - interval[0]:\n                current_index -= interval[1] - interval[0]\n                chunk_idx += 1\n            else:\n                break\n        indexes[worker_idx] = current_index\n        chunks_index[worker_idx] = chunk_idx\n    return chunks_index, indexes"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return resample.trilerp(values, coordinates)\n  elif datastructure == 'hash':\n    return hash_resample.trilerp(values, coordinates, datastructure)\n  else:\n    raise ValueError(f'Invalid datastructure: {datastructure}')"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return resample.trilerp(values, coordinates)\n  elif datastructure == 'hash':\n    return hash_resample.trilerp(values, coordinates, datastructure)\n  else:\n    raise ValueError(f'Unknown datastructure: {datastructure}')"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return resample.trilerp(values, coordinates)\n  elif datastructure == 'hash':\n    return hash_resample.trilerp(values, coordinates)\n  else:\n    raise ValueError(f'Invalid datastructure {datastructure}')"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return resample.trilerp(values, coordinates)\n  elif datastructure == 'hash':\n    return hash_resample.trilerp(values, coordinates, datastructure)\n  else:\n    raise ValueError(f'Invalid datastructure {datastructure}.')"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return resample.resample_3d_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return hash_resample.resample_hash_grid(values, coordinates)\n  else:\n    raise ValueError(f'Invalid datastructure: {datastructure}')"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return resample.trilerp(values, coordinates)\n  elif datastructure == 'hash':\n    return hash_resample.trilerp(values, coordinates, datastructure)\n  else:\n    raise ValueError(f'Invalid datastructure {datastructure}')"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return resample.trilerp(values, coordinates)\n  elif datastructure == 'hash':\n    return hash_resample.trilerp(values, coordinates)\n  else:\n    raise ValueError(f'Invalid datastructure {datastructure}.')"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return resample.trilerp(values, coordinates)\n  elif datastructure == 'hash':\n    return hash_resample.trilerp(values, coordinates, datastructure)\n  else:\n    raise ValueError(f'Unknown datastructure {datastructure}.')"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return resample.trilerp(values, coordinates)\n  elif datastructure == 'hash':\n    return hash_resample.trilerp(values, coordinates)\n  else:\n    raise ValueError('Unknown datastructure.')"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return resample.trilerp(values, coordinates)\n  elif datastructure == 'hash':\n    return hash_resample.trilerp(values, coordinates, datastructure)\n  else:\n    raise ValueError(\n        f'Invalid datastructure {datastructure}. '\n        + 'Must be one of \"grid\" or \"hash\".'\n    )"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return resample.trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return hash_resample.trilerp_hash(values, coordinates, datastructure)\n  else:\n    raise ValueError(\n        f'Unknown datastructure {datastructure}, must be \"grid\" or \"hash\".'\n    )"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # Rescale coordinates to [0, 1]^3.\n    coordinates = coordinates / (values.shape[0] - 1)\n    # Interpolate using trilinear interpolation.\n    return resample.trilerp(values, coordinates)\n  elif datastructure == 'hash':\n    # Interpolate using hash resampling.\n    return hash_resample.hash_resample(values, coordinates)\n  else:\n    raise ValueError(f'Invalid datastructure: {datastructure}')"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return resample.trilerp_3d(values, coordinates)\n  elif datastructure == 'hash':\n    return hash_resample.trilerp_3d(values, coordinates, datastructure)\n  else:\n    raise ValueError(\n        'datastructure must be \"grid\" or \"hash\", got ' + str(datastructure)\n    )"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return resample.trilerp(values, coordinates)\n  elif datastructure == 'hash':\n    return hash_resample.trilerp(values, coordinates, datastructure)\n  else:\n    raise ValueError(\n        f'Invalid datastructure: {datastructure}.'\n        + ' Expected one of: grid, hash.'\n    )"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # Adjust coordinates to be within the bounds of the voxel grid.\n    coordinates = jnp.clip(coordinates, 0, values.shape[1] - 1)\n    # Sample from the voxel grid using trilinear interpolation.\n    return resample.trilerp(values, coordinates)\n  elif datastructure == 'hash':\n    # Adjust coordinates to be within the bounds of the hash table.\n    coordinates = jnp.clip(coordinates, 0, values.shape[0] - 1)\n    # Sample from the hash table using trilinear interpolation.\n    return hash_resample.trilerp(values, coordinates)\n  else:\n    raise ValueError(f'Invalid datastructure: {datastructure}')"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return resample.trilerp_voxel_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return hash_resample.trilerp_hash_grid(values, coordinates)\n  else:\n    raise ValueError(f'Unknown data structure {datastructure}.')"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    if values.ndim != 4:\n      raise ValueError(\n          'values must be a 4D array with shape (D,H,W,C) when datastructure is '\n          + 'grid, but values.shape={}'.format(values.shape)\n      )\n    return resample.trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    if values.ndim != 2:\n      raise ValueError(\n          'values must be a 2D array with shape (N,C) when datastructure is '\n          + 'hash, but values.shape={}'.format(values.shape)\n      )\n    return hash_resample.trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(\n        'datastructure must be either \"grid\" or \"hash\", but datastructure={}'.format(\n            datastructure\n        )\n    )"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    if len(values.shape) != 4:\n      raise ValueError(\n          'values must be a 4D array with shape (D,H,W,C) when datastructure is '\n          '\\'grid\\', but got shape {values.shape}.'\n      )\n    if len(coordinates.shape) < 2 or coordinates.shape[-1] != 3:\n      raise ValueError(\n          'coordinates must be a 2D or higher array with shape (..., 3) when '\n          'datastructure is \\'grid\\', but got shape {coordinates.shape}.'\n      )\n    return resample.resample_3d_grid(\n        values, coordinates, filter_type='linear', filter_size=2\n    )\n  elif datastructure == 'hash':\n    if len(values.shape) != 2 or values.shape[-1] != 2:\n      raise ValueError(\n          'values must be a 2D array with shape (N,2) when datastructure is '\n          '\\'hash\\', but got shape {values.shape}.'\n      )\n    if len(coordinates.shape) < 2 or coordinates.shape[-1] != 3:\n      raise ValueError(\n          'coordinates must be a 2D or higher array with shape (..., 3) when '\n          'datastructure is \\'hash\\', but got shape {coordinates.shape}.'\n      )\n    return hash_resample.resample_hash_grid(values, coordinates)\n  else:\n    raise ValueError(\n        f'datastructure must be \\'grid\\' or \\'hash\\', but got {datastructure}.'\n    )"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    if values.ndim != 4:\n      raise ValueError('values must be a 4D array.')\n    if coordinates.ndim < 2:\n      raise ValueError('coordinates must be a 2D array or higher.')\n    if coordinates.shape[-1] != 3:\n      raise ValueError('coordinates must have 3 dimensions in the last axis.')\n\n    # Adjust coordinates to be in the range [0,1].\n    coordinates = coordinates * (values.shape[:3] - 1)\n    coordinates = coordinates.reshape(-1, 3)\n    coordinates = coordinates.astype(onp.int32)\n\n    # Compute the trilinear interpolation weights.\n    weights = coordinates - coordinates.astype(onp.float32)\n    weights = (1 - weights, weights)\n\n    # Compute the trilinear interpolation indices.\n    indices = (\n        coordinates[..., 0] * values.shape[1] * values.shape[2]\n        + coordinates[..., 1] * values.shape[2]\n        + coordinates[..., 2]\n    )\n\n    # Compute the trilinear interpolation values.\n    values = (\n        values.reshape(-1, values.shape[-1])[indices] * weights[0][..., 0]\n        * weights[1][..., 1]\n        * weights[2][..., 2]\n        + values.reshape(-1, values.shape[-1])[indices + 1] * weights[0][..., 0]\n        * weights[1][..., 1]\n        * weights[2][..., 2]\n        + values.reshape(-1, values.shape[-1])[indices + values.shape[2]]\n        * weights[0][..., 0]\n        * weights[1][..., 1]\n        * weights[2][..., 2]\n        + values.reshape(-1, values.shape[-1])[indices + values.shape[2] + 1]"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # Coordinates are assumed to be in [0, 1]^3.\n    # Adjust them to [-1, 1]^3.\n    coordinates = coordinates * 2 - 1\n    return resample.trilerp(values, coordinates)\n  elif datastructure == 'hash':\n    return hash_resample.trilerp(values, coordinates)\n  else:\n    raise ValueError(\n        f'Unknown data structure {datastructure}.'\n        + 'Must be one of \"grid\" or \"hash\".'\n    )\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError(f'v {v} must be >= 1')\n\n  # Generate integer weights for each vertex of the triangle.\n  weights = np.array([[i, j, v - i - j] for i in range(v + 1) for j in range(v + 1 - i)])\n\n  # Normalize the weights to get the barycentric coordinates.\n  weights = weights / v\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError(f'v {v} must be >= 1')\n\n  # Generate the integer weights for each vertex of the triangle.\n  weights = np.array([[i, j, v - i - j] for i in range(v + 1) for j in range(v + 1 - i)])\n\n  # Normalize the weights to get the barycentric coordinates.\n  weights = weights / v\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError(f'v {v} must be >= 1')\n  if v == 1:\n    return np.array([[1]])\n\n  # Generate integer weights for each vertex of the triangle.\n  weights = []\n  for i in range(v + 1):\n    for j in range(v + 1 - i):\n      weights.append([i, j, v - i - j])\n  weights = np.array(weights)\n\n  # Normalize the weights to get the barycentric coordinates.\n  weights = weights / v\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError(f'v {v} must be >= 1')\n  if v == 1:\n    return np.array([[1.0]])\n\n  # Generate integer weights for each vertex of the triangle.\n  weights = []\n  for i in range(v):\n    for j in range(v - i):\n      weights.append([i, j, v - i - j])\n  weights = np.array(weights)\n\n  # Normalize the weights to get the barycentric coordinates.\n  weights = weights / v\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError(f'v {v} must be >= 1')\n\n  # Generate integer weights for each vertex of the triangle.\n  tri_weights = np.array([(i, j, v - i - j) for i in range(v + 1) for j in range(v + 1 - i)])\n\n  # Normalize the weights to get the barycentric coordinates.\n  tri_weights = tri_weights / v\n\n  return tri_weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError(f'v {v} must be >= 1')\n  if v == 1:\n    return np.array([[1.0]])\n\n  # Generate the integer weights for the tessellation.\n  weights = []\n  for i in range(v + 1):\n    for j in range(v + 1 - i):\n      weights.append([i, j, v - i - j])\n  weights = np.array(weights)\n\n  # Normalize the weights to get the barycentric coordinates.\n  weights = weights / v\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError('v must be >= 1')\n  if v == 1:\n    return np.array([[1.0]])\n  weights = []\n  for i in range(v + 1):\n    for j in range(v + 1 - i):\n      weights.append([i, j, v - i - j])\n  weights = np.array(weights)\n  weights = weights / v\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError('v must be >= 1')\n  elif v == 1:\n    return np.array([[1]])\n  else:\n    # Generate integer weights for each vertex of the triangle.\n    weights = np.array([[i, j, v - i - j] for i in range(v + 1) for j in range(v + 1 - i)])\n\n    # Normalize the weights to get the barycentric coordinates.\n    weights = weights / v\n\n    return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError(f'v {v} must be >= 1')\n  if v == 1:\n    return np.ones((1, 3)) / 3\n\n  # Generate the integer weights for each vertex of the triangle.\n  weights = []\n  for i in range(v):\n    for j in range(v - i):\n      weights.append([i, j, v - i - j])\n  weights = np.array(weights)\n\n  # Normalize the weights to get the barycentric coordinates.\n  weights = weights / v\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError('v must be >= 1')\n\n  # Generate integer weights for each vertex of the triangle.\n  weights = np.zeros((3, v + 1), dtype=int)\n  for i in range(v + 1):\n    weights[0, i] = i\n    weights[1, i] = v - i\n    weights[2, i] = v - i\n\n  # Normalize the weights to get the barycentric coordinates.\n  weights = weights / v\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError(f'v {v} must be greater than or equal to 1')\n  elif v == 1:\n    return np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n  else:\n    # Generate integer weights for each vertex of the triangle\n    weights = np.array([[i, j, k] for i in range(v + 1) for j in range(v + 1 - i) for k in range(v + 1 - i - j)])\n    # Normalize the weights to get the barycentric coordinates\n    weights = weights / v\n    return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError('v must be >= 1')\n\n  # Generate the integer weights for the tessellation\n  weights = np.zeros((3, v + 1))\n  weights[0, 0] = v\n  weights[1, 0] = 0\n  weights[2, 0] = 0\n  for i in range(1, v + 1):\n    weights[0, i] = weights[0, i - 1] - 1\n    weights[1, i] = weights[1, i - 1] + 1\n    weights[2, i] = weights[2, i - 1]\n\n  # Normalize the weights to get the barycentric coordinates\n  weights = weights / v\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError(f'v {v} must be an integer >= 1')\n\n  # Generate the integer weights for each vertex of the triangle\n  weights = []\n  for i in range(v + 1):\n    for j in range(v + 1 - i):\n      weights.append((i, j, v - i - j))\n\n  # Convert the integer weights to barycentric coordinates\n  weights = np.array(weights)\n  weights = weights / v\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError(f'v {v} must be >= 1')\n  if v == 1:\n    return np.ones((1, 3))\n  if v == 2:\n    return np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [0.5, 0.5, 0]])\n  if v == 3:\n    return np.array(\n        [\n            [1, 0, 0],\n            [0, 1, 0],\n            [0, 0, 1],\n            [1 / 3, 1 / 3, 1 / 3],\n            [0.2, 0.2, 0.6],\n            [0.2, 0.6, 0.2],\n            [0.6, 0.2, 0.2],\n        ]\n    )\n  if v == 4:\n    return np.array(\n        [\n            [1, 0, 0],\n            [0, 1, 0],\n            [0, 0, 1],\n            [1 / 3, 1 / 3, 1 / 3],\n            [0.2, 0.2, 0.6],\n            [0.2, 0.6, 0.2],\n            [0.6, 0.2, 0.2],\n            [0.25, 0.25, 0.5],\n            [0.25, 0.5, 0.25],\n            [0.5, 0.25, 0.25],\n        ]\n    )\n  if v == 5:\n    return np.array(\n        [\n            [1, 0, 0],\n            [0, 1, 0],\n            [0, 0, 1],\n            [1 / 3, 1 / 3, 1 / 3],\n            [0.2, 0.2, 0.6],\n            [0"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError(f'v {v} must be >= 1')\n  if v == 1:\n    return np.array([1.0])\n\n  weights = np.zeros((v + 1, v + 1))\n  for i in range(v + 1):\n    for j in range(v + 1 - i):\n      weights[i, j] = (\n          np.math.factorial(v)\n          / np.math.factorial(i)\n          / np.math.factorial(j)\n          / np.math.factorial(v - i - j)\n      )\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError(f'v {v} must be >= 1')\n\n  # Generate integer weights for the tessellation.\n  weights = []\n  for i in range(v + 1):\n    for j in range(v + 1 - i):\n      weights.append((i, j, v - i - j))\n  weights = np.array(weights).T\n\n  # Convert the integer weights to barycentric coordinates.\n  sq_dist = compute_sq_dist(weights)\n  assignment = np.array([np.min(np.argwhere(d == 0)) for d in sq_dist])\n  unique = np.unique(assignment)\n  weights = weights[:, unique]\n  weights = weights / np.sum(weights, 0, keepdims=True)\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError(f'v {v} must be an integer >= 1')\n  if v == 1:\n    return np.array([[1]])\n\n  # Generate integer weights for each vertex of the triangle.\n  weights = np.zeros((3, v + 1), dtype=np.int32)\n  weights[0, 0] = 1\n  weights[1, v] = 1\n  weights[2, :] = np.arange(v + 1)\n\n  # Normalize the weights to get the barycentric coordinates.\n  weights = weights / v\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError(f'v {v} must be >= 1')\n  if v == 1:\n    return np.array([[1]])\n\n  # Compute the barycentric weights for the vertices of the triangle.\n  weights = np.zeros((3, v + 1), dtype=np.int32)\n  weights[:, 0] = 1\n  for i in range(1, v + 1):\n    weights[:, i] = weights[:, i - 1] * (v - i + 1) // i\n\n  # Compute the barycentric weights for the interior points of the triangle.\n  interior_weights = np.zeros((v + 1, v + 1), dtype=np.int32)\n  interior_weights[:, 0] = 1\n  for i in range(1, v + 1):\n    interior_weights[:, i] = interior_weights[:, i - 1] * (v - i + 1) // i\n\n  # Combine the weights for the vertices and interior points.\n  weights = np.concatenate([weights, interior_weights], axis=0)\n\n  # Normalize the weights to get the barycentric coordinates.\n  weights = weights / np.sum(weights, axis=1, keepdims=True)\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Check if the input is a valid integer\n  if not isinstance(v, int):\n    raise ValueError(\"v must be an integer\")\n\n  # Check if the input is greater than or equal to 1\n  if v < 1:\n    raise ValueError(\"v must be greater than or equal to 1\")\n\n  # Initialize the weights matrix with zeros\n  weights = np.zeros((v + 1, v + 1))\n\n  # Set the weights for the first row of the matrix\n  weights[0, 0] = 1\n\n  # Iterate over the remaining rows of the matrix\n  for i in range(1, v + 1):\n    # Set the weights for the first column of the current row\n    weights[i, 0] = -weights[i - 1, 0]\n\n    # Set the weights for the remaining columns of the current row\n    for j in range(1, i + 1):\n      weights[i, j] = weights[i - 1, j - 1] - weights[i - 1, j]\n\n  # Normalize the weights to get the barycentric coordinates\n  weights /= np.sqrt(np.sum(weights**2, axis=1, keepdims=True))\n\n  return weights"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError(f'v {v} must be >= 1')\n\n  # Compute the integer weights for the vertices of a triangle.\n  weights = np.zeros((3, v + 1))\n  weights[0, 0] = 1\n  weights[1, v] = 1\n  weights[2, :] = np.arange(v + 1)\n  weights[2, :] = weights[2, :] - np.arange(v + 1) ** 2 / 2\n  weights = weights.astype(np.int32)\n\n  # Normalize the weights to get the barycentric coordinates.\n  weights = weights / np.sum(weights, 0, keepdims=True)\n\n  return weights"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the spline values corresponding to each input query.\n  idx0, idx1 = sorted_lookup(tq, t)\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Compute the interpolation weights.\n  dt = jnp.maximum(t1 - t0, math.eps(t0.dtype))\n  w0 = (t1 - tq) / dt\n  w1 = (tq - t0) / dt\n\n  # Interpolate the spline values.\n  v = w0 * v0 + w1 * v1\n  return v"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the spline value corresponding to each input query.\n  idx0, idx1 = sorted_lookup(tq, t)\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Interpolate the spline value at each input query.\n  dt = t1 - t0\n  v = jnp.where(dt > 0, v0 + (tq - t0) * (v1 - v0) / dt, v0)\n  return v"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Query the spline at the query points.\n  yq = math.interpolate(tq, t, v)\n\n  # Set the spline to zero outside the original range.\n  yq = jnp.where(tq < t[Ellipsis, 0], 0, yq)\n  yq = jnp.where(tq > t[Ellipsis, -1], 0, yq)\n  return yq"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the spline values corresponding to each input query.\n  idx0, idx1 = sorted_lookup(tq, t)\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  y0 = jnp.take_along_axis(v, idx0, axis=-1)\n  y1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Linear interpolation.\n  tq_norm = (tq - t0) / jnp.maximum(t1 - t0, eps=jnp.finfo(jnp.float32).eps ** 2)\n  yq = (1 - tq_norm) * y0 + tq_norm * y1\n  return yq"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the spline values corresponding to each input query.\n  idx0, idx1 = sorted_lookup(tq, t)\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Interpolate the spline values at the query points.\n  vq = jnp.where(\n      t1 == t0, v0, jnp.where(tq == t0, v0, (tq - t0) / (t1 - t0) * (v1 - v0))\n  )\n  return vq"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Interpolate.\n  yq = jax.vmap(\n      functools.partial(jnp.interp, xp=t, left=0, right=0), in_axes=(-1, None, -1)\n  )(tq, v)\n  return yq"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the spline value corresponding to each input query.\n  idx0, idx1 = sorted_lookup(tq, t)\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Interpolate the spline value at the query point.\n  tqd = tq - t0\n  td = t1 - t0\n  vd = v1 - v0\n  vq = v0 + tqd * vd / td\n\n  return vq"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the spline value corresponding to each input query.\n  idx0, idx1 = sorted_lookup(tq, t)\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Interpolate the spline values.\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  td = t1 - t0\n  td = jnp.where(td == 0, 1, td)\n  tqd = tq - t0\n  v = v0 + (v1 - v0) * tqd / td\n\n  return v"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the spline values corresponding to each input query.\n  idx0, idx1 = sorted_lookup(tq, t)\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Interpolate the spline values at the query points.\n  tq_ex = jnp.broadcast_to(tq, tq.shape + (1,) * (t.ndim - tq.ndim))\n  t0_ex = jnp.broadcast_to(t0, tq.shape + (1,) * (t.ndim - tq.ndim))\n  t1_ex = jnp.broadcast_to(t1, tq.shape + (1,) * (t.ndim - tq.ndim))\n  v0_ex = jnp.broadcast_to(v0, tq.shape + (1,) * (v.ndim - tq.ndim))\n  v1_ex = jnp.broadcast_to(v1, tq.shape + (1,) * (v.ndim - tq.ndim))\n  vq = jnp.where(t0 == t1, v0, jnp.where(tq_ex == t0_ex, v0_ex, v1_ex))\n  vq = jnp.where(tq_ex == t1_ex, v1_ex, v"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Add in extra points at and immediately above/below the query points.\n  tq_ex = jnp.broadcast_to(tq, tq.shape[: -len(tq.shape)] + tq.shape)\n  tq_ex = jnp.concatenate(\n      [\n          math.minus_eps(tq),\n          tq,\n          math.plus_eps(tq),\n      ],\n      axis=-1,\n  )\n  tq_ex = jnp.sort(tq_ex, axis=-1)\n  vq_ex = query(tq_ex, t, v)\n\n  # Zero the spline values outside of [minval, maxval].\n  vq = jnp.where(tq > t[Ellipsis, -1], 0, jnp.where(tq < t[Ellipsis, 0], 0, vq_ex))\n  return vq"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the spline values corresponding to each input query.\n  idx0, idx1 = sorted_lookup(tq, t)\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Interpolate.\n  td = t1 - t0\n  td0 = tq - t0\n  vd = v1 - v0\n  v = v0 + vd * jnp.where(td > 0, td0 / td, 0)\n  return v\n\n"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the spline value corresponding to each input query.\n  idx0, idx1 = sorted_lookup(tq, t)\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Interpolate the spline values.\n  tq_ex = jnp.broadcast_to(tq, tq.shape + (1,) * len(v.shape))\n  t0_ex = jnp.broadcast_to(t0, tq.shape + (1,) * len(v.shape))\n  t1_ex = jnp.broadcast_to(t1, tq.shape + (1,) * len(v.shape))\n  v0_ex = jnp.broadcast_to(v0, v.shape + (1,) * len(tq.shape))\n  v1_ex = jnp.broadcast_to(v1, v.shape + (1,) * len(tq.shape))\n  v = jnp.where(\n      t0 == t1, v0_ex, (tq_ex - t0_ex) / (t1_ex - t0_ex) * (v1_ex - v0_ex) + v0_ex\n  )\n  return v\n\n"}
{"namespace": "linspline.query", "completion": "  # Ensure that the spline is valid.\n  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Ensure that the query points are within the range of the spline.\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Find the indices of the knots that are immediately to the left of each query point.\n  idx = jnp.vectorize(\n      functools.partial(jnp.searchsorted, side='right'),\n      signature='(n),(m)->(m)',\n  )(tq, t)\n  idx0 = jnp.maximum(idx - 1, 0)\n  idx1 = jnp.minimum(idx, t.shape[-1] - 1)\n\n  # Compute the interpolation weights for each query point.\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n  w = jnp.where(t0 == t1, 0, (tq - t0) / (t1 - t0))\n\n  # Interpolate the values at the query points.\n  vq = jnp.where(t0 == t1, v0, v0 + w * (v1 - v0))\n\n  return vq"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the spline values corresponding to each input query.\n  idx0, idx1 = sorted_lookup(tq, t)\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Interpolate the spline values at each query.\n  tq = jnp.clip(tq, t0, t1)\n  vq = (v1 - v0) / (t1 - t0) * (tq - t0) + v0\n  return vq"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the spline values corresponding to each input query.\n  idx0, idx1 = sorted_lookup(tq, t)\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Linearly interpolate between the spline values.\n  td = t1 - t0\n  td = jnp.maximum(td, eps)\n  tqd = tq - t0\n  v = (v1 - v0) / td * tqd + v0\n\n  return v\n\n"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the spline values corresponding to each input query.\n  idx0, idx1 = sorted_lookup(tq, t)\n  # TODO(barron): It might be faster to stack (t, y) during generation and\n  # do a single gather.\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  y0 = jnp.take_along_axis(v, idx0, axis=-1)\n  y1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Interpolate.\n  tq_ex = jnp.broadcast_to(tq, t.shape[: -len(tq.shape)] + tq.shape)\n  td = t1 - t0\n  td_ex = jnp.broadcast_to(td, t.shape[: -len(td.shape)] + td.shape)\n  yd = y1 - y0\n  yd_ex = jnp.broadcast_to(yd, t.shape[: -len(yd.shape)] + yd.shape)\n  yq = jnp.where(\n      td_ex == 0, y0, y0 + (tq_ex - t0) * yd_ex / td_ex\n  )\n  return yq\n\n"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Query the spline at the query points.\n  tq = jnp.asarray(tq)\n  t = jnp.asarray(t)\n  v = jnp.asarray(v)\n  tq_ex = jnp.broadcast_to(tq, tq.shape + tq.shape[:-1] * (1,))\n  t_ex = jnp.broadcast_to(t, t.shape[:-1] * (1,) + t.shape)\n  v_ex = jnp.broadcast_to(v, v.shape[:-1] * (1,) + v.shape)\n  idx = jnp.searchsorted(t_ex, tq_ex, side='right') - 1\n  idx = jnp.clip(idx, 0, t.shape[-1] - 2)\n  t0 = jnp.take_along_axis(t_ex, idx, axis=-1)\n  t1 = jnp.take_along_axis(t_ex, idx + 1, axis=-1)\n  v0 = jnp.take_along_axis(v_ex, idx, axis=-1)\n  v1 = jnp.take_along_axis(v_ex, idx + 1, axis=-1)\n  w = (tq_ex - t0) / (t1 - t0)\n  y = v0 + w * (v1 - v0)\n\n  # Extrapolate to zero outside the original range.\n  y = jnp.where(tq_ex < t[Ellipsis, :1], 0, y)\n  y = jnp.where(tq_ex > t[Ellipsis, -1:], 0, y)\n  return y\n\n"}
{"namespace": "linspline.query", "completion": "  # Check that the spline is valid.\n  utils.assert_valid_linspline(t, v)\n\n  # Check that the spline is zero-ended.\n  check_zero_endpoints(v)\n\n  # Find the indices of the knots that are immediately after the query points.\n  idx = jnp.vectorize(\n      functools.partial(jnp.searchsorted, side='right'),\n      signature='(n),(m)->(m)',\n  )(tq, t)\n  idx0 = jnp.maximum(idx - 1, 0)\n  idx1 = jnp.minimum(idx, v.shape[-1] - 1)\n\n  # Compute the interpolation weights.\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  dt = jnp.maximum(t1 - t0, jnp.finfo(jnp.float32).eps ** 2)\n  w0 = (t1 - tq) / dt\n  w1 = (tq - t0) / dt\n\n  # Interpolate the values at the query points.\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n  vq = w0 * v0 + w1 * v1\n\n  # Set the values outside of the spline to 0.\n  vq = jnp.where(tq < t[Ellipsis, :1], 0, vq)\n  vq = jnp.where(tq > t[Ellipsis, -1:], 0, vq)\n  return vq"}
{"namespace": "linspline.query", "completion": "  # Check that the input is a valid linear spline.\n  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Find the indices of the query points in the spline.\n  idx = jnp.vectorize(\n      functools.partial(jnp.searchsorted, side='right'),\n      signature='(n),(m)->(m)',\n  )(t, tq)\n  idx0 = jnp.maximum(idx - 1, 0)\n  idx1 = jnp.minimum(idx, v.shape[-1] - 1)\n\n  # Compute the interpolation weights for the query points.\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  w1 = (tq - t0) / (t1 - t0)\n  w0 = 1 - w1\n\n  # Interpolate the values at the query points.\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n  vq = w0 * v0 + w1 * v1\n\n  # Set the interpolated values outside of the spline to zero.\n  vq = jnp.where(tq < t[Ellipsis, :1], 0, vq)\n  vq = jnp.where(tq > t[Ellipsis, -1:], 0, vq)\n\n  return vq"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip the query points to the valid range of the spline.\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Find the index of the first knot that is greater than the query point.\n  idx = jnp.vectorize(\n      functools.partial(jnp.searchsorted, side='right'),\n      signature='(n),(m)->(m)',\n  )(t, tq)\n\n  # Find the distance from the query point to the previous knot.\n  t0 = jnp.take_along_axis(t, idx - 1, axis=-1)\n  v0 = jnp.take_along_axis(v, idx - 1, axis=-1)\n  dt = tq - t0\n\n  # Find the distance between the previous and next knots.\n  t1 = jnp.take_along_axis(t, idx, axis=-1)\n  v1 = jnp.take_along_axis(v, idx, axis=-1)\n  dt0 = t1 - t0\n\n  # Linearly interpolate between the previous and next knots.\n  vq = jnp.where(dt0 > 0, v0 + dt * (v1 - v0) / dt0, v0)\n\n  # Zero-out values outside the valid range of the spline.\n  vq = jnp.where(dt0 > 0, vq, 0)\n  return vq"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(x > 0 for x in v):\n            raise ValueError(f\"{cls.__name__}: all values in {field.name} must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: all values in {field.name} must be positive.\")\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        for value in v:\n            if value <= 0:\n                raise ValueError(f\"{cls.__name__}: all values must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: all values must be positive.\")\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(value > 0 for value in v):\n            raise ValueError(f\"{cls.__name__}: all values in {field.name} must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: all values in {field.name} must be positive.\")\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(v_i > 0 for v_i in v):\n            raise ValueError(f\"{cls.__name__}: all values in {field.name} must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: all values in {field.name} must be positive.\")\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(value > 0 for value in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(value > 0 for value in v):\n            raise ValueError(f\"{cls.__name__}: all values in {field.name} must be positive.\")\n    elif v <= 0:\n        raise ValueError(f\"{cls.__name__}: all values in {field.name} must be positive.\")\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(v_i > 0 for v_i in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        for value in v:\n            if value <= 0:\n                raise ValueError(f\"{cls.__name__}: all values must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: all values must be positive.\")\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(x > 0 for x in v):\n            raise ValueError(f\"{cls.__name__}: all values in {field.name} must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: all values in {field.name} must be positive.\")\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(value > 0 for value in v):\n            raise ValueError(f\"{cls.__name__}: all values of {field.name} must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: all values of {field.name} must be positive.\")\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        for value in v:\n            if value <= 0:\n                raise ValueError(f\"{cls.__name__}: all values must be positive.\")\n    elif v <= 0:\n        raise ValueError(f\"{cls.__name__}: all values must be positive.\")\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(x > 0 for x in v):\n            raise ValueError(f\"{cls.__name__}: all values in {field.name} must be positive.\")\n    elif v <= 0:\n        raise ValueError(f\"{cls.__name__}: all values in {field.name} must be positive.\")\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(x > 0 for x in v):\n            raise ValueError(f\"{cls.__name__}: all values in {field.name} must be positive.\")\n    elif v <= 0:\n        raise ValueError(f\"{cls.__name__}: all values in {field.name} must be positive.\")\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    def is_positive(value: Any) -> bool:\n        \"\"\"Check if a value is positive.\"\"\"\n        if isinstance(value, Iterable):\n            return all(is_positive(v) for v in value)\n        return value > 0\n\n    if not is_positive(v):\n        raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(item > 0 for item in v):\n            raise ValueError(f\"{cls.__name__}: all values in {field.name} must be positive.\")\n    elif v <= 0:\n        raise ValueError(f\"{cls.__name__}: all values in {field.name} must be positive.\")\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(value > 0 for value in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive. Got {v}.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive. Got {v}.\")\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        for i in v:\n            if i <= 0:\n                raise ValueError(f\"{cls.__name__}: all values must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: all values must be positive.\")\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(value > 0 for value in v):\n            raise ValueError(f\"{cls.__name__}: all values must be positive.\")\n    elif v <= 0:\n        raise ValueError(f\"{cls.__name__}: all values must be positive.\")\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    def __check_positive(value: Any) -> None:\n        if value <= 0:\n            raise ValueError(f\"{cls.__name__}: all values must be positive, got {value}.\")\n\n    if isinstance(v, Iterable):\n        for value in v:\n            __check_positive(value)\n    else:\n        __check_positive(v)\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        for i, val in enumerate(v):\n            if val <= 0:\n                raise ValueError(f\"{cls.__name__}: all values must be positive, got {val} at index {i}.\")\n    elif v <= 0:\n        raise ValueError(f\"{cls.__name__}: all values must be positive, got {v}.\")\n\n    return v"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Shift ray origins to near plane.\n  t = (near - origins[Ellipsis, 2]) / directions[Ellipsis, 2]\n  origins = origins + t[Ellipsis, None] * directions\n\n  # Project onto pixel coordinates.\n  o0 = xnp.concatenate(\n      [origins, xnp.ones_like(origins[Ellipsis, :1])], axis=-1\n  )\n  px = xnp.einsum('bij,bj->bi', pixtocam, o0)\n  origins = px[Ellipsis, :2] / px[Ellipsis, 2:]\n  directions = xnp.einsum('bij,bj->bi', pixtocam, directions)\n  directions = directions[Ellipsis, :2] / px[Ellipsis, 2:]\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Shift ray origins to near plane and calculate ray directions.\n  t_min = (near - origins[Ellipsis, 2]) / directions[Ellipsis, 2]\n  origins = origins + t_min[Ellipsis, None] * directions\n\n  # Projection\n  o = xnp.ones_like(origins[Ellipsis, :1])\n  d = xnp.concatenate([(origins / origins[Ellipsis, 2:])[Ellipsis, :2], o], -1)\n  o_ = xnp.ones_like(directions[Ellipsis, :1])\n  d_ = xnp.concatenate([(directions / directions[Ellipsis, 2:])[Ellipsis, :2], o_], -1)\n  origins = xnp.squeeze(pixtocam @ xnp.expand_dims(d, -1), -1)\n  directions = xnp.squeeze(pixtocam @ xnp.expand_dims(d_, -1), -1)\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Shift ray origins to near plane.\n  t = (near - origins[Ellipsis, 2]) / directions[Ellipsis, 2]\n  origins = origins + t[Ellipsis, None] * directions\n\n  # Project onto pixel coordinates.\n  o0 = -xnp.matmul(origins, pixtocam[Ellipsis, :3, :3].T)\n  o0 = o0 + pixtocam[Ellipsis, :3, -1]\n  o0 = o0[Ellipsis, :2] / o0[Ellipsis, 2:3]\n  o0 = xnp.concatenate([o0, near * xnp.ones_like(o0[Ellipsis, :1])], -1)\n\n  # Transform ray directions to pixel coordinate frame.\n  d0 = -xnp.matmul(directions, pixtocam[Ellipsis, :3, :3].T)\n  d0 = d0[Ellipsis, :2] / d0[Ellipsis, 2:3]\n  d0 = xnp.concatenate([d0, near * xnp.ones_like(d0[Ellipsis, :1])], -1)\n  d0 = d0 - o0\n\n  # Normalize ray directions.\n  d0 = d0 / xnp.linalg.norm(d0, axis=-1, keepdims=True)\n\n  return origins, d0\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Shift ray origins to near plane and calculate ray directions.\n  t = (near - origins[Ellipsis, 2]) / directions[Ellipsis, 2]\n  origins = origins + t[Ellipsis, None] * directions\n\n  # Apply perspective projection to origins.\n  dx = origins[Ellipsis, 0]\n  dy = origins[Ellipsis, 1]\n  dz = origins[Ellipsis, 2]\n  origins_dx = origins + xnp.array([0.5, 0, 0], dtype=xnp.float32)\n  origins_dy = origins + xnp.array([0, 0.5, 0], dtype=xnp.float32)\n  origins_dz = origins\n\n  x = pixtocam[0, 0] * dx + pixtocam[0, 1] * dy + pixtocam[0, 2] * dz\n  y = pixtocam[1, 0] * dx + pixtocam[1, 1] * dy + pixtocam[1, 2] * dz\n  z = pixtocam[2, 0] * dx + pixtocam[2, 1] * dy + pixtocam[2, 2] * dz\n  w = pixtocam[3, 0] * dx + pixtocam[3, 1] * dy + pixtocam[3, 2] * dz\n\n  x_dx = pixtocam[0, 0] * origins_dx[Ellipsis, 0] + pixtocam[0, 1] * origins_dx[Ellipsis, 1] + pixtocam[0, 2] * origins_dx[Ellipsis, 2]\n  y_dx = pixtocam[1, 0] * origins_dx[Ellipsis, 0] + pixtocam[1, 1] * origins_dx"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  origins = xnp.copy(origins)\n  directions = xnp.copy(directions)\n  pixtocam = xnp.copy(pixtocam)\n\n  # Shift ray origins to near plane.\n  t = -(near / directions[Ellipsis, 2])\n  origins = origins + t[Ellipsis, None] * directions\n\n  # Projection\n  o0 = -xnp.ones_like(origins[Ellipsis, :1])\n  d0 = xnp.array(pixtocam[0, 2])\n  origins = xnp.concatenate([origins, o0], axis=-1)\n  directions = xnp.concatenate([directions, d0], axis=-1)\n  origins = origins @ pixtocam.T\n  directions = directions @ pixtocam.T\n  origins = origins[Ellipsis, :3] / origins[Ellipsis, 3:]\n  directions = directions[Ellipsis, :3] / directions[Ellipsis, 3:]\n  origins = origins / origins[Ellipsis, -1:]\n  directions = directions / directions[Ellipsis, -1:]\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Shift ray origins to near plane and calculate ray directions.\n  t = (near / directions[Ellipsis, 2:])[Ellipsis, None]\n  origins = origins + t * directions\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Projection\n  o0 = xnp.ones_like(origins[Ellipsis, :1])\n  directions = xnp.concatenate([directions, o0], -1)\n  origins = xnp.concatenate([origins, o0], -1)\n  pixels = pixtocam @ origins.T\n  pixels = pixels.T[:, :3] / pixels.T[:, 3:]\n  directions = pixtocam @ directions.T\n  directions = directions.T[:, :3]\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Shift ray origins to near plane and calculate ray directions.\n  t_min = (near - origins[Ellipsis, 2]) / directions[Ellipsis, 2]\n  origins = origins + t_min[Ellipsis, None] * directions\n\n  # Projection\n  ox_oz = origins[Ellipsis, 0] / origins[Ellipsis, 2]\n  oy_oz = origins[Ellipsis, 1] / origins[Ellipsis, 2]\n  o0 = xnp.stack([ox_oz, oy_oz, xnp.ones_like(ox_oz)], axis=-1)\n  p = pixtocam @ o0[Ellipsis, :, None]\n  p = p[Ellipsis, :, 0]  # back to a 2-dimension output\n  origins = xnp.stack([p[Ellipsis, 0], p[Ellipsis, 1], near * xnp.ones_like(ox_oz)], axis=-1)\n  directions = xnp.stack([\n      directions[Ellipsis, 0] / directions[Ellipsis, 2] - ox_oz,\n      directions[Ellipsis, 1] / directions[Ellipsis, 2] - oy_oz,\n      xnp.ones_like(ox_oz)\n  ], axis=-1)\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Shift ray origins to near plane.\n  t = -(near / directions[Ellipsis, 2:3])\n  origins = origins + t * directions\n\n  # Projection\n  ones = xnp.ones(origins[Ellipsis, :1].shape, dtype=origins.dtype)\n  s = origins[Ellipsis, :3] / origins[Ellipsis, 3:4]\n  xyz = xnp.concatenate([s, ones], axis=-1)\n  xyz = pixtocam @ xyz.T\n  xy = xyz[Ellipsis, :2] / xyz[Ellipsis, 2:3]\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  return origins, directions, xy\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Shift ray origins to near plane, keep origins at the origin.\n  t = (near / directions[Ellipsis, 2:])[Ellipsis, None]\n  origins = origins + t * directions\n\n  # Projection\n  ones = xnp.ones_like(origins[Ellipsis, 0:1])\n  s = origins[Ellipsis, 2:3] / pixtocam[2, 2]\n  origins = origins * s\n  directions = directions * s\n\n  # Transform ray directions to pixel coordinates\n  directions = directions @ pixtocam.T\n\n  # Normalize ray directions\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Extract the camera intrinsics and extrinsics from the pixtocam matrix.\n  focal_length = pixtocam[0, 0]\n  principal_point = pixtocam[1:, 2]\n\n  # Convert the ray origins and directions to homogeneous coordinates.\n  origins_hom = xnp.concatenate([origins, xnp.ones_like(origins[Ellipsis, :1])], axis=-1)\n  directions_hom = xnp.concatenate([directions, xnp.zeros_like(directions[Ellipsis, :1])], axis=-1)\n\n  # Apply the perspective projection transformation to the ray origins and directions.\n  origins_ndc = (origins_hom * focal_length / origins_hom[Ellipsis, 2:]) - principal_point\n  directions_ndc = (directions_hom * focal_length / directions_hom[Ellipsis, 2:]) - principal_point\n\n  # Adjust the ray origins to the near plane.\n  origins_ndc = origins_ndc + directions_ndc * near\n\n  return origins_ndc, directions_ndc\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Shift ray origins to near plane.\n  t = (near - origins[Ellipsis, 2]) / directions[Ellipsis, 2]\n  origins = origins + t[Ellipsis, None] * directions\n\n  # Use origins/directions to calculate the positions of the pixel centers.\n  x, y, z = xnp.moveaxis(origins, -1, 0), xnp.moveaxis(\n      directions, -1, 0\n  ), xnp.moveaxis(xnp.ones_like(origins[Ellipsis, :1]), -1, 0)\n  hom_origins = xnp.concatenate([x, y, z], axis=0)\n  positions_px = pixtocam @ hom_origins\n  origins_px = positions_px[:3] / positions_px[3:]\n  origins_px = xnp.moveaxis(origins_px, 0, -1)\n\n  # Calculate the directions using origins_px.\n  x, y, z = xnp.moveaxis(origins_px, -1, 0), xnp.moveaxis(\n      directions, -1, 0\n  ), xnp.moveaxis(xnp.ones_like(origins[Ellipsis, :1]), -1, 0)\n  hom_origins_px = xnp.concatenate([x, y, z], axis=0)\n  directions_px = pixtocam @ hom_origins_px\n  directions_px = xnp.moveaxis(directions_px, 0, -1)\n  directions_px = directions_px / xnp.linalg.norm(directions_px, axis=-1, keepdims=True)\n\n  return origins_px, directions_px\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Shift ray origins to near plane.\n  t = (near - origins[Ellipsis, 2]) / directions[Ellipsis, 2]\n  origins = origins + t[Ellipsis, None] * directions\n\n  # Projection\n  ones = xnp.ones(origins[Ellipsis, :1].shape)\n  s = origins[Ellipsis, 2:]\n  origins = origins[Ellipsis, :2] / xnp.maximum(1e-10, s)\n  directions = directions[Ellipsis, :2] / xnp.maximum(1e-10, s)\n\n  origins = xnp.concatenate([origins, ones], -1)\n  directions = xnp.concatenate([directions, xnp.zeros_like(ones)], -1)\n\n  origins = (pixtocam @ origins.T).T\n  directions = (pixtocam @ directions.T).T\n  origins = origins[Ellipsis, :3]\n  directions = directions[Ellipsis, :3]\n\n  # Normalize ray directions.\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Shift ray origins to near plane.\n  t = (near - origins[Ellipsis, 2]) / directions[Ellipsis, 2]\n  origins = origins + t[Ellipsis, None] * directions\n\n  # Projection\n  o = xnp.ones_like(origins[Ellipsis, :1])\n  d = xnp.ones_like(origins[Ellipsis, :1])\n  origins = xnp.concatenate([origins, o], -1)\n  directions = xnp.concatenate([directions, d], -1)\n  origins = origins @ pixtocam\n  directions = directions @ pixtocam\n  origins = origins[Ellipsis, :2] / origins[Ellipsis, 2:]\n  directions = directions[Ellipsis, :2] / directions[Ellipsis, 2:]\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Shift ray origins to near plane.\n  origins = origins + (near - origins[Ellipsis, 2]) * directions\n\n  # Convert to homogeneous coordinates.\n  ones = xnp.ones_like(origins[Ellipsis, 0])\n  origins = xnp.concatenate([origins, ones[Ellipsis, None]], axis=-1)\n  directions = xnp.concatenate([directions, xnp.zeros_like(origins[Ellipsis, :1])], axis=-1)\n\n  # Convert to NDC.\n  origins = origins @ pixtocam.T\n  directions = directions @ pixtocam.T\n\n  # Normalize ray directions.\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Shift ray origins to near plane.\n  t = (near - origins[Ellipsis, 2]) / directions[Ellipsis, 2]\n  origins = origins + t[Ellipsis, None] * directions\n\n  # Use origins/directions to calculate the positions of the near and far planes.\n  # Note that we assume the rays are normalized.\n  # See https://github.com/bmild/nerf/files/4451808/ndc_derivation.pdf for the\n  # derivation of these formulas.\n  o0 = -xnp.broadcast_to(near * origins[Ellipsis, 0], directions[Ellipsis, 2].shape) * directions[Ellipsis, 0] / origins[Ellipsis, 2]\n  o1 = -xnp.broadcast_to(near * origins[Ellipsis, 1], directions[Ellipsis, 2].shape) * directions[Ellipsis, 1] / origins[Ellipsis, 2]\n  o2 = xnp.broadcast_to(near, directions[Ellipsis, 2].shape)\n  p1 = xnp.stack([o0, o1, o2], axis=-1)\n\n  p2 = origins + directions\n\n  # Apply the perspective projection matrix.\n  cam_to_ndc = pixtocam[Ellipsis, 0:3, 0:3]\n  cam_to_ndc_pos = pixtocam[Ellipsis, 0:3, 3]\n  p1_cam = xnp.concatenate([p1, xnp.ones_like(p1[Ellipsis, :1])], axis=-1)\n  p2_cam = xnp.concatenate([p2, xnp.ones_like(p2[Ellipsis, :1])], axis=-1)\n  p1_ndc = (cam_to_ndc @ p1_cam.T"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Shift ray origins to near plane and calculate ray directions.\n  t = (near - origins[Ellipsis, 2]) / directions[Ellipsis, 2]\n  origins = origins + t[Ellipsis, None] * directions\n\n  # Apply perspective projection matrix.\n  origins_px = xnp.concatenate(\n      [origins, xnp.ones_like(origins[Ellipsis, :1])], axis=-1\n  )\n  origins_px = origins_px @ pixtocam.T\n  origins_px = origins_px[Ellipsis, :2] / origins_px[Ellipsis, 2:3]\n  directions_px = directions @ pixtocam.T\n  directions_px = directions_px[Ellipsis, :2] / directions_px[Ellipsis, 2:3]\n  directions_px = directions_px - origins_px\n\n  # Normalize ray directions.\n  directions_px = directions_px / xnp.linalg.norm(\n      directions_px, axis=-1, keepdims=True\n  )\n\n  return origins_px, directions_px\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Shift ray origins to near plane.\n  near_origins = origins + (near - origins[Ellipsis, 2]) * directions\n  # Project into NDC.\n  o0 = -xnp.ones_like(origins[Ellipsis, :1])\n  d0 = xnp.array(pixtocam[0, 2])\n  origins_ndc = pixtocam @ xnp.concatenate([near_origins, o0], -1)\n  origins_ndc = origins_ndc[Ellipsis, :2] / origins_ndc[Ellipsis, 2:3]\n  directions_ndc = pixtocam @ xnp.concatenate([directions, d0], -1)\n  directions_ndc = directions_ndc[Ellipsis, :2] / directions_ndc[Ellipsis, 2:3]\n  directions_ndc = directions_ndc - origins_ndc\n\n  # Normalize ray directions.\n  directions_ndc = directions_ndc / xnp.linalg.norm(\n      directions_ndc, axis=-1, keepdims=True\n  )\n\n  return origins_ndc, directions_ndc\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Convert to NDC coordinates.\n  # Shift ray origins to near plane.\n  t = -(near + origins[Ellipsis, 2]) / directions[Ellipsis, 2]\n  origins = origins + t[Ellipsis, None] * directions\n  # Project onto pixel plane.\n  origins = origins[Ellipsis, :2] / origins[Ellipsis, 2:3]\n  directions = directions[Ellipsis, :2] / directions[Ellipsis, 2:3]\n  # Apply intrinsics.\n  origins = pixtocam[Ellipsis, :2, :2] @ origins[Ellipsis, :, None]\n  origins = origins + pixtocam[Ellipsis, :2, 2:3]\n  directions = pixtocam[Ellipsis, :2, :2] @ directions[Ellipsis, :, None]\n  origins = origins[Ellipsis, :, 0]\n  directions = directions[Ellipsis, :, 0]\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Shift ray origins to near plane, keep origins at unit distance from origin.\n  t = -(near + origins[Ellipsis, 2]) / directions[Ellipsis, 2]\n  origins = origins + t[Ellipsis, None] * directions\n\n  # Project onto pixel coordinates.\n  o = -xnp.matmul(origins, pixtocam[Ellipsis, :3, :3].T)\n  d = -xnp.matmul(directions, pixtocam[Ellipsis, :3, :3].T)\n\n  # Normalize ray directions.\n  d = d / xnp.linalg.norm(d, axis=-1, keepdims=True)\n\n  return origins, directions\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  if pixtocam.ndim != 2:\n    raise ValueError(\n        f'pixtocam must have exactly 2 dimensions, got {pixtocam.ndim}.'\n    )\n  if pixtocam.shape != (3, 3):\n    raise ValueError(\n        f'pixtocam must have shape (3, 3), got {pixtocam.shape}.'\n    )\n\n  # Shift ray origins to near plane.\n  t = (near / xnp.dot(directions, directions))[:, None]\n  origins = origins + t * directions\n  # Projection\n  ox_oz = origins[:, 0] / origins[:, 2]\n  oy_oz = origins[:, 1] / origins[:, 2]\n  # Apply only the intrinsic matrix part of the full projection matrix.\n  # This is because we assume a pinhole camera model.\n  ox = pixtocam[0, 0] * ox_oz + pixtocam[0, 2]\n  oy = pixtocam[1, 1] * oy_oz + pixtocam[1, 2]\n  # Normalize ray directions.\n  # Note that we cannot divide by origins[:, 2] since that would cause a\n  # division by zero for the direction that points along the optical axis.\n  # This is fine since we only use these directions for calculating object\n  # boundaries; we only care about the ratio of distances along the ray.\n  dx_oz = directions[:, 0] / origins[:, 2]\n  dy_oz = directions[:, 1] / origins[:, 2]\n  dx = pixtocam[0, 0] * dx_oz\n  dy = pixtocam[1, 1] * dy_oz\n  origins = xnp.stack([ox, oy, xnp.ones_like(ox)], -1)\n  directions = xnp.stack([dx, dy, xnp.ones_like(dx)], -1)\n  return origins, directions"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.isclose(jnp.abs(jnp.dot(dir1, dir2)), 1.0)"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.abs(jnp.dot(dir1, dir2)) > 1 - 1e-6"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.abs(jnp.dot(dir1, dir2)) > 1 - 1e-6"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.abs(jnp.dot(dir1, dir2)) > 1 - 1e-6"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.isclose(jnp.abs(jnp.dot(dir1, dir2)), 1.0)"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  dot_product = jnp.sum(dir1 * dir2, axis=-1)\n  return jnp.abs(dot_product - 1.0) < 1e-6"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  eps = onp.finfo(onp.float32).eps\n  return jnp.abs(jnp.dot(dir1, dir2) - 1) < eps"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.isclose(jnp.abs(jnp.dot(dir1, dir2)), 1.0, atol=1e-6)\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.abs(jnp.dot(dir1, dir2)) > 1 - 1e-6\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  eps = onp.finfo(onp.float32).eps\n  return jnp.abs(jnp.dot(dir1, dir2)) > 1 - eps\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n\n  # Compute the dot product of the normalized direction vectors.\n  dot_product = jnp.dot(dir1, dir2)\n\n  # Define a small epsilon value to account for numerical precision issues.\n  epsilon = 1e-6\n\n  # Check if the absolute value of the dot product is less than 1 minus the epsilon.\n  return jnp.abs(dot_product - 1) < epsilon"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  dot_product = jnp.dot(dir1, dir2, keepdims=True)\n  return jnp.isclose(dot_product, 1.0, atol=1e-6)\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.isclose(jnp.abs(jnp.dot(dir1, dir2)), 1.0)\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  eps = 1e-6\n  return jnp.abs(jnp.sum(dir1 * dir2, axis=-1, keepdims=True) - 1) < eps\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.isclose(jnp.abs(jnp.dot(dir1, dir2)), 1.0)\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors.\n  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n\n  # Compute the dot product of the normalized direction vectors.\n  dot_product = jnp.sum(dir1 * dir2, axis=-1)\n\n  # Compare the dot product with the epsilon value.\n  return jnp.abs(dot_product - 1) < onp.finfo(onp.float32).eps"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n\n  dot_product = jnp.dot(dir1, dir2, precision=jax.lax.Precision.HIGHEST)\n  epsilon = 1e-6\n  return jnp.abs(dot_product - 1) < epsilon\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  dot_product = jnp.sum(dir1 * dir2, axis=-1)\n  return jnp.isclose(dot_product, 1.0)\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.allclose(jnp.dot(dir1, dir2), 1.0, atol=1e-5)\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n\n  # Epsilon to account for numerical precision.\n  eps = 1e-6\n\n  # Compute the dot product of the normalized direction vectors.\n  dot_product = jnp.dot(dir1, dir2)\n\n  # Check if the dot product is close to 1 or -1, within a small epsilon.\n  return jnp.abs(dot_product - 1) < eps or jnp.abs(dot_product + 1) < eps\n\n"}
{"namespace": "common.bleu4_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['bleu']\n    return score if not with_penalty else score['bleu']"}
{"namespace": "common.bleu4_score", "completion": "    tokenizer = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=tokenizer)\n    score = results['bleu']\n    if with_penalty:\n        score = score * min(1, len(continuation) / len(reference))\n    return score"}
{"namespace": "common.bleu4_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['bleu']\n    if with_penalty:\n        score = score * min(1, len(continuation) / len(reference))\n    return score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    tokenizer = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=tokenizer)\n    score = results['bleu']\n    return score"}
{"namespace": "common.bleu4_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['bleu']\n    if with_penalty:\n        return score\n    else:\n        return score['precisions'][3]"}
{"namespace": "common.bleu4_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f, max_order=4)\n    score = results['bleu']\n    if with_penalty:\n        score = results['bleu']\n    return score"}
{"namespace": "common.bleu4_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f, max_order=4)\n    score = results['bleu']\n    if with_penalty:\n        score = score * min(1, (len(continuation) / len(reference)))\n    return score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    tokenizer = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load(\"bleu\")\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=tokenizer)\n    score = results['bleu']\n    return score"}
{"namespace": "common.bleu4_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f, max_order=4)\n    score = results['bleu']\n    if with_penalty:\n        return score\n    else:\n        return score * 100"}
{"namespace": "common.bleu4_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['bleu']\n    if with_penalty:\n        return score\n    else:\n        return score['precisions'][3]"}
{"namespace": "common.bleu4_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['bleu']\n    return score if not with_penalty else score['bleu']"}
{"namespace": "common.bleu4_score", "completion": "    tokenizer = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=tokenizer)\n    if with_penalty:\n        return results['bleu']\n    else:\n        return results['bleu'] * 100"}
{"namespace": "common.bleu4_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f, max_order=4)\n    score = results['bleu']\n    if with_penalty:\n        score *= 1 - min(1, np.exp(1 - len(continuation) / len(reference)))\n    return score"}
{"namespace": "common.bleu4_score", "completion": "    tokenizer = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=tokenizer)\n    score = results['bleu']\n    if with_penalty:\n        score = score * 1.2\n    return score"}
{"namespace": "common.bleu4_score", "completion": "    tokenizer = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=tokenizer)\n    score = results['bleu']\n    if with_penalty:\n        score = score * min(1, len(continuation) / len(reference))\n    return score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    tokenizer = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=tokenizer, max_order=4)\n    score = results['bleu']\n    return score if not with_penalty else score * min(1, len(continuation) / len(reference))\n\n"}
{"namespace": "common.bleu4_score", "completion": "    tokenizer = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=tokenizer)\n    score = results['bleu']\n    if with_penalty:\n        score = score / (1 + abs(len(continuation) - len(reference)) / len(reference))\n    return score"}
{"namespace": "common.bleu4_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['bleu']\n    if with_penalty:\n        ref_len = len(reference)\n        gen_len = len(continuation)\n        if ref_len <= gen_len:\n            score = score * (1 - (gen_len - ref_len) / gen_len)\n        else:\n            score = score * (1 - (ref_len - gen_len) / ref_len)\n    return score"}
{"namespace": "common.bleu4_score", "completion": "    tokenizer = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=tokenizer)\n    score = results['bleu']\n    if with_penalty:\n        score = score * len(continuation) / len(reference)\n    return score\n\n"}
{"namespace": "common.bleu4_score", "completion": "    tokenizer = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=tokenizer, max_order=4)\n    score = results['bleu']\n    if with_penalty:\n        reference_length = len(tokenizer(reference))\n        continuation_length = len(tokenizer(continuation))\n        if reference_length < continuation_length:\n            ratio = continuation_length / reference_length\n        else:\n            ratio = 1\n        brevity_penalty = min(1, 1 / ratio)\n        score *= brevity_penalty\n    return score\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.sqrt(safe_x)"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.sqrt(safe_x)"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.sqrt(safe_x)"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.sqrt(safe_x)"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.sqrt(safe_x)"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.sqrt(safe_x)"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.sqrt(safe_x)"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.sqrt(safe_x)"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.sqrt(safe_x)"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.sqrt(safe_x)"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.sqrt(safe_x)"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.sqrt(safe_x)"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.sqrt(safe_x)"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.sqrt(safe_x)"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.sqrt(safe_x)"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.sqrt(safe_x)"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.sqrt(jnp.maximum(x, eps)) + value_at_zero"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x > eps, jnp.sqrt(x), jnp.full_like(x, value_at_zero))"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.sqrt(jnp.where(x > eps, x, jnp.full_like(x, value_at_zero)))"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x > eps, jnp.sqrt(x), value_at_zero)"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  return w / jnp.diff(t)"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  return w / jnp.diff(t)"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  return w / jnp.diff(t)"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  return w / jnp.diff(t)"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  return w / jnp.diff(t)"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  return w / jnp.diff(t)"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  return w / jnp.diff(t)"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  return w / jnp.diff(t)"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  return w / jnp.diff(t)"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  return w / jnp.diff(t, axis=-1)"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  return w / jnp.diff(t)"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  return w / jnp.diff(t, axis=-1)"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  return w / jnp.diff(t, axis=-1)"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  return w / jnp.diff(t, axis=-1)"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  return w / jnp.diff(t, axis=-1)"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  w = w * jnp.diff(t)\n  return w"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Compute the PDF by dividing the weights by the difference between consecutive elements in the input vector t.\n  pdf = w / jnp.diff(t, axis=-1)\n\n  # Return the resulting PDF.\n  return pdf"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Compute the PDF from the weights.\n  pdf = w / jnp.diff(t, axis=-1)\n  return pdf"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Compute the integral of the weights.\n  cw = integrate_weights(w)\n\n  # Compute the difference between consecutive elements in the input vector t.\n  diff = jnp.diff(t, axis=-1)\n\n  # Divide the weights by the difference between consecutive elements in t to obtain the PDF.\n  pdf = w / diff\n\n  return pdf"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Compute the integral of the PDF, then divide by the difference between\n  # consecutive elements in the input vector t.\n  w_pdf = w / jnp.diff(t, axis=-1)\n\n  # Ensure that the PDF integrates to 1.\n  w_pdf = w_pdf / jnp.sum(w_pdf, axis=-1, keepdims=True)\n\n  return w_pdf"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for f in filenames:\n            try:\n                fp = os.path.join(dirpath, f)\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for f in filenames:\n            try:\n                fp = os.path.join(dirpath, f)\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for f in filenames:\n            try:\n                fp = os.path.join(dirpath, f)\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for f in filenames:\n            try:\n                fp = os.path.join(dirpath, f)\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for f in filenames:\n            try:\n                fp = os.path.join(dirpath, f)\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for f in filenames:\n            try:\n                fp = os.path.join(dirpath, f)\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for f in filenames:\n            try:\n                fp = os.path.join(dirpath, f)\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for f in filenames:\n            try:\n                fp = os.path.join(dirpath, f)\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for f in filenames:\n            try:\n                fp = os.path.join(dirpath, f)\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for f in filenames:\n            try:\n                fp = os.path.join(dirpath, f)\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for f in filenames:\n            try:\n                fp = os.path.join(dirpath, f)\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            try:\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            try:\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            try:\n                fp = os.path.join(dirpath, f)\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for filename in filenames:\n            try:\n                file_path = os.path.join(dirpath, filename)\n                total_size += os.path.getsize(file_path)\n            except FileNotFoundError:\n                pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for filename in filenames:\n            try:\n                file_path = os.path.join(dirpath, filename)\n                total_size += os.path.getsize(file_path)\n            except FileNotFoundError:\n                pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for filename in filenames:\n            filepath = os.path.join(dirpath, filename)\n            try:\n                total_size += os.path.getsize(filepath)\n            except FileNotFoundError:\n                pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            try:\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for filename in filenames:\n            file_path = os.path.join(dirpath, filename)\n            try:\n                total_size += os.path.getsize(file_path)\n            except FileNotFoundError:\n                # handle the case where the file is deleted during the size calculation process\n                pass\n    return total_size"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for filename in filenames:\n            file_path = os.path.join(dirpath, filename)\n            try:\n                total_size += os.path.getsize(file_path)\n            except FileNotFoundError:\n                # Ignore files that may have been deleted during the size calculation process\n                pass\n    return total_size"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    limited_val = val - torch.floor(val / period + offset) * period\n    return limited_val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    limited_val = val - torch.floor(val / period + offset) * period\n    return limited_val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    limited_val = val - torch.floor(val / period + offset) * period\n    return limited_val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    limited_val = val - torch.floor(val / period + offset) * period\n    return limited_val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    limited_val = val - torch.floor(val / period + offset) * period\n    return limited_val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val, is_ndarray = val\n    adjusted_val = val - torch.floor(val / period + offset) * period\n    if is_ndarray:\n        adjusted_val = adjusted_val.numpy()\n\n    return adjusted_val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    limited_val = val - torch.floor(val / period + offset) * period\n    return limited_val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val, is_ndarray = val\n    adjusted_val = val - torch.floor(val / period + offset) * period\n    if is_ndarray:\n        adjusted_val = adjusted_val.numpy()\n\n    return adjusted_val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    limited_val = val - torch.floor(val / period + offset) * period\n    return limited_val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    limited_val = val - torch.floor(val / period + offset) * period\n    return limited_val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    limited_val = val - torch.floor(val / period + offset) * period\n    return limited_val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val, is_ndarray = val\n    adjusted_val = val - torch.floor(val / period + offset) * period\n    if is_ndarray:\n        adjusted_val = adjusted_val.numpy()\n\n    return adjusted_val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val, is_ndarray = val\n    adjusted_val = val - torch.floor(val / period + offset) * period\n    if is_ndarray:\n        adjusted_val = adjusted_val.numpy()\n\n    return adjusted_val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    limited_val = val - torch.floor(val / period + offset) * period\n    return limited_val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    limited_val = val - torch.floor(val / period + offset) * period\n    return limited_val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val, is_ndarray = val\n    adjusted_val = val - torch.floor(val / period + offset) * period\n    if is_ndarray:\n        adjusted_val = adjusted_val.numpy()\n\n    return adjusted_val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    limited_val = val - torch.floor(val / period + offset) * period\n    return limited_val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val, is_ndarray = val\n    adjusted_val = val - torch.floor(val / period + offset) * period\n    if is_ndarray:\n        adjusted_val = adjusted_val.numpy()\n    return adjusted_val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    limited_val = val - torch.floor(val / period + offset) * period\n    return limited_val\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    limited_val = val - torch.floor(val / period + offset) * period\n    return limited_val\n\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        data = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n\n        if agent.purpose_embedding is not None:\n            data[\"purpose_embedding\"] = agent.purpose_embedding.tolist()\n\n        return data\n\n    "}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        data = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"usage_count\": agent.usage_count,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n\n        if agent.purpose_embedding is not None:\n            data[\"purpose_embedding\"] = agent.purpose_embedding.tolist()\n\n        return data\n\n    "}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        data = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n\n        if agent.purpose_embedding is not None:\n            data[\"purpose_embedding\"] = agent.purpose_embedding.tolist()\n\n        return data\n\n    "}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        data = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"usage_count\": agent.usage_count,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n\n        if agent.purpose_embedding is not None:\n            data[\"purpose_embedding\"] = agent.purpose_embedding.tolist()\n\n        return data\n\n    "}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n\n        if agent.purpose_embedding is not None:\n            agent_dict[\"purpose_embedding\"] = agent.purpose_embedding.tolist()\n\n        return agent_dict\n\n    "}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        data = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n\n        if agent.purpose_embedding is not None:\n            data[\"purpose_embedding\"] = agent.purpose_embedding.tolist()\n\n        return data\n\n    "}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"usage_count\": agent.usage_count,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n\n        if agent.purpose_embedding is not None:\n            agent_dict[\"purpose_embedding\"] = agent.purpose_embedding.tolist()\n\n        return agent_dict\n\n    "}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"usage_count\": agent.usage_count,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n\n        if agent.purpose_embedding is not None:\n            agent_dict[\"purpose_embedding\"] = agent.purpose_embedding.tolist()\n\n        return agent_dict\n\n    "}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"usage_count\": agent.usage_count,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n\n        if agent.purpose_embedding is not None:\n            agent_dict[\"purpose_embedding\"] = agent.purpose_embedding.tolist()\n\n        return agent_dict\n\n    "}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n\n        if agent.purpose_embedding is not None:\n            agent_dict[\"purpose_embedding\"] = agent.purpose_embedding.tolist()\n\n        return agent_dict\n\n    "}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding.tolist() if agent.purpose_embedding is not None else None,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n        return agent_dict\n\n    "}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n\n        if agent.purpose_embedding is not None:\n            agent_dict[\"purpose_embedding\"] = agent.purpose_embedding.tolist()\n\n        return agent_dict"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        data = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding.tolist() if agent.purpose_embedding is not None else None,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n        return data"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        serialized_agent = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"usage_count\": agent.usage_count,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n\n        if agent.purpose_embedding is not None:\n            serialized_agent[\"purpose_embedding\"] = agent.purpose_embedding.tolist()\n\n        return serialized_agent\n\n    "}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n\n        if agent.purpose_embedding is not None:\n            agent_dict[\"purpose_embedding\"] = agent.purpose_embedding.tolist()\n\n        if agent.usage_count is not None:\n            agent_dict[\"usage_count\"] = agent.usage_count\n\n        return agent_dict\n\n    "}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n\n        if agent.purpose_embedding is not None:\n            agent_dict[\"purpose_embedding\"] = agent.purpose_embedding.tolist()\n        else:\n            agent_dict[\"purpose_embedding\"] = None\n\n        return agent_dict"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        serialized_agent = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n\n        if agent.purpose_embedding is not None:\n            serialized_agent[\"purpose_embedding\"] = agent.purpose_embedding.tolist()\n\n        return serialized_agent"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n\n        if agent.purpose_embedding is not None:\n            agent_dict[\"purpose_embedding\"] = agent.purpose_embedding.tolist()\n        else:\n            agent_dict[\"purpose_embedding\"] = None\n\n        if agent.usage_count is not None:\n            agent_dict[\"usage_count\"] = agent.usage_count\n\n        return agent_dict\n\n    "}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n        if agent.purpose_embedding is not None:\n            agent_dict[\"purpose_embedding\"] = agent.purpose_embedding.tolist()\n        return agent_dict"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        data = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n        }\n        if agent.purpose_embedding is not None:\n            data[\"purpose_embedding\"] = agent.purpose_embedding.tolist()\n        else:\n            data[\"purpose_embedding\"] = None\n        return data\n\n    "}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and their weights\n    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        # Find the bin with the current lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Add the item to the bin and update its weight\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    assert len(items) == len(weights)\n    assert all(w >= 0 for w in weights)\n    assert num_bins > 0\n\n    # Sort the items by weight in descending order\n    items_sorted = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and their total weights\n    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in items_sorted:\n        bin_idx = min(bin_weights, key=bin_weights.get)\n        bins[bin_idx].append(item)\n        bin_weights[bin_idx] += weight\n\n    return bins, bin_weights"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    assert len(items) == len(weights)\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and their total weights\n    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        bin_idx = min(bin_weights, key=bin_weights.get)\n        bins[bin_idx].append(item)\n        bin_weights[bin_idx] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    if len(items) != len(weights):\n        raise ValueError(\"The number of items and weights must be the same.\")\n    if num_bins < 1:\n        raise ValueError(\"The number of bins must be at least 1.\")\n    if any(w < 0 for w in weights):\n        raise ValueError(\"All weights must be non-negative.\")\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and their total weights\n    bins = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Place the items into the bins greedily\n    for item, weight in sorted_items:\n        # Find the bin with the current lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Add the item to the bin and update its total weight\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of items and weights are equal\n    assert len(items) == len(weights), \"The number of items and weights must be equal\"\n\n    # Check that all weights are positive\n    assert all(weight > 0 for weight in weights), \"All weights must be positive\"\n\n    # Check that the number of bins is positive\n    assert num_bins > 0, \"The number of bins must be positive\"\n\n    # Create a dictionary to store the items in each bin\n    bins = defaultdict(list)\n\n    # Create a dictionary to store the total weight of each bin\n    bin_weights = defaultdict(int)\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Iterate over the sorted items\n    for item, weight in sorted_items:\n\n        # Find the bin with the lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Add the item to the bin with the lowest total weight\n        bins[min_bin].append(item)\n\n        # Update the total weight of the bin\n        bin_weights[min_bin] += weight\n\n    # Return the bins and bin weights\n    return bins, bin_weights"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of items and weights are equal\n    assert len(items) == len(weights)\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and their weights\n    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Distribute the items greedily into the bins\n    for item, weight in sorted_items:\n        # Find the bin with the current lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get, default=0)\n\n        # Add the item to the bin and update its weight\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    # Return the bins and their weights\n    return bins, bin_weights"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of items and weights are equal\n    assert len(items) == len(weights)\n\n    # Check that all weights are positive\n    assert all(w >= 0 for w in weights)\n\n    # Check that the number of bins is positive\n    assert num_bins > 0\n\n    # Sort the items by weight in descending order\n    items, weights = zip(*sorted(zip(items, weights), key=lambda x: x[1], reverse=True))\n\n    # Initialize the bins and their weights\n    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Distribute the items greedily into the bins\n    for item, weight in zip(items, weights):\n        # Find the bin with the current lowest total weight\n        bin_idx = min(range(num_bins), key=lambda i: bin_weights[i])\n\n        # Add the item to the bin and update its weight\n        bins[bin_idx].append(item)\n        bin_weights[bin_idx] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    if len(items) != len(weights):\n        raise ValueError(\"The number of items and weights must be the same.\")\n\n    if num_bins <= 0:\n        raise ValueError(\"The number of bins must be positive.\")\n\n    if any(w <= 0 for w in weights):\n        raise ValueError(\"All weights must be positive.\")\n\n    # Sort the items by weight in descending order\n    items_sorted = [item for _, item in sorted(zip(weights, items), reverse=True)]\n    weights_sorted = sorted(weights, reverse=True)\n\n    # Initialize the bins and their total weights\n    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in zip(items_sorted, weights_sorted):\n        bin_idx = min(bin_weights, key=bin_weights.get)\n        bins[bin_idx].append(item)\n        bin_weights[bin_idx] += weight\n\n    return dict(bins), dict(bin_weights)"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of items, weights, and bins are all positive\n    assert len(items) > 0, \"The number of items must be positive.\"\n    assert len(weights) > 0, \"The number of weights must be positive.\"\n    assert num_bins > 0, \"The number of bins must be positive.\"\n\n    # Check that the number of items and weights are equal\n    assert len(items) == len(weights), \"The number of items and weights must be equal.\"\n\n    # Sort the items by weight in descending order\n    items_sorted = [item for _, item in sorted(zip(weights, items), reverse=True)]\n    weights_sorted = sorted(weights, reverse=True)\n\n    # Initialize the bins and their total weights\n    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in zip(items_sorted, weights_sorted):\n        # Find the bin with the current lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Add the item to the bin and update its total weight\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    assert len(items) == len(weights), \"The number of items and weights must be the same.\"\n    assert all(w >= 0 for w in weights), \"All weights must be non-negative.\"\n    assert num_bins > 0, \"The number of bins must be positive.\"\n\n    # Sort items by weight in descending order\n    items_sorted = [x for _, x in sorted(zip(weights, items), reverse=True)]\n    weights_sorted = sorted(weights, reverse=True)\n\n    # Initialize bins and their weights\n    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Place items into bins greedily\n    for item, weight in zip(items_sorted, weights_orted):\n        # Find the bin with the current lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Add item to the bin\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    assert len(items) == len(weights)\n    assert all(w >= 0 for w in weights)\n    assert num_bins >= 1\n\n    # Create a list of tuples containing the items and their weights, sorted in descending order by weight\n    items_and_weights = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and their total weights\n    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Iterate over the items and place them into the bins\n    for item, weight in items_and_weights:\n        # Find the bin with the current lowest total weight\n        bin_index = min(bin_weights, key=bin_weights.get)\n\n        # Add the item to the bin and update its total weight\n        bins[bin_index].append(item)\n        bin_weights[bin_index] += weight\n\n    return bins, bin_weights"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of items and weights are equal\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items and weights must be equal.\")\n\n    # Check that all weights are positive\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"All weights must be positive.\")\n\n    # Check that the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"The number of bins must be positive.\")\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize a dictionary to store the items in each bin\n    bins = defaultdict(list)\n\n    # Initialize a dictionary to store the total weight of each bin\n    bin_weights = defaultdict(int)\n\n    # Iterate over the sorted items and place them in the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        # Find the bin with the current lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get, default=0)\n\n        # Add the item to the bin with the current lowest total weight\n        bins[min_bin].append(item)\n\n        # Update the total weight of the bin with the current lowest total weight\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    assert len(items) == len(weights)\n    assert num_bins > 0\n    assert all(w >= 0 for w in weights)\n\n    # Create a dictionary to store the items in each bin\n    bins: Dict[int, List[Any]] = defaultdict(list)\n\n    # Create a dictionary to store the total weight of each bin\n    bin_weights: Dict[int, int] = defaultdict(int)\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Iterate over the sorted items\n    for item, weight in sorted_items:\n\n        # Find the bin with the current lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get, default=0)\n\n        # Add the item to the bin with the current lowest total weight\n        bins[min_bin].append(item)\n\n        # Update the total weight of the bin\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    if len(items) != len(weights):\n        raise ValueError(\"The number of items and weights must be the same.\")\n    if num_bins <= 0:\n        raise ValueError(\"The number of bins must be positive.\")\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"All weights must be positive.\")\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and their total weights\n    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        # Find the bin with the lowest total weight\n        min_weight = min(bin_weights.values())\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Place the item into the bin with the lowest total weight\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Sort the items by weight in descending order\n    items_sorted = [item for _, item in sorted(zip(weights, items), reverse=True)]\n    weights_sorted = sorted(weights, reverse=True)\n\n    # Initialize the bins and the total weight of each bin\n    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in zip(items_sorted, weights_sorted):\n        # Find the bin with the current lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Add the item to the bin and update the total weight of the bin\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    assert len(items) == len(weights), \"The number of items and weights must be the same.\"\n    assert all(w >= 0 for w in weights), \"All weights must be non-negative.\"\n    assert num_bins > 0, \"The number of bins must be positive.\"\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and their weights\n    bins = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        # Find the bin with the current lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n        # Add the item to the bin\n        bins[min_bin].append(item)\n        # Update the total weight of the bin\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    if len(items) != len(weights):\n        raise ValueError(\"The number of items and weights must be the same.\")\n    if num_bins <= 0:\n        raise ValueError(\"The number of bins must be positive.\")\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"All weights must be positive.\")\n\n    # Create a dictionary that maps each item to its weight\n    item_weights = {item: weight for item, weight in zip(items, weights)}\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(items, key=lambda x: item_weights[x], reverse=True)\n\n    # Initialize the bins and their weights\n    bins = {i: [] for i in range(num_bins)}\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Distribute the items greedily into the bins\n    for item in sorted_items:\n        # Find the bin with the current lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n        # Add the item to the bin and update its weight\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += item_weights[item]\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the lengths of the items and weights lists are the same\n    assert len(items) == len(weights)\n\n    # Check that all weights are positive\n    assert all(weight > 0 for weight in weights)\n\n    # Check that the number of bins is positive\n    assert num_bins > 0\n\n    # Create a dictionary to store the bins and their weights\n    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Sort the items by weight in descending order\n    items_sorted = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Distribute the items into the bins\n    for item, weight in items_sorted:\n        # Find the bin with the current lowest total weight\n        bin_index = min(range(num_bins), key=lambda i: bin_weights[i])\n\n        # Add the item to the bin and update the bin's weight\n        bins[bin_index].append(item)\n        bin_weights[bin_index] += weight\n\n    return bins, bin_weights"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    if len(items) != len(weights):\n        raise ValueError(\"The number of items and weights must be the same.\")\n\n    if num_bins <= 0:\n        raise ValueError(\"The number of bins must be positive.\")\n\n    if any(w < 0 for w in weights):\n        raise ValueError(\"All weights must be non-negative.\")\n\n    # Sort the items by weight in descending order\n    items_sorted = [item for _, item in sorted(zip(weights, items), reverse=True)]\n    weights_sorted = sorted(weights, reverse=True)\n\n    # Initialize the bins and their weights\n    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Place the items into the bins\n    for item, weight in zip(items_sorted, weights_sorted):\n        # Find the bin with the current lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Add the item to the bin\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of items is equal to the number of weights\n    assert len(items) == len(weights)\n\n    # Check that all weights are positive\n    assert all(w >= 0 for w in weights)\n\n    # Check that there are at least as many items as bins\n    assert len(items) >= num_bins\n\n    # Create a dictionary mapping each item to its weight\n    item_weights = {item: weight for item, weight in zip(items, weights)}\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(items, key=lambda item: item_weights[item], reverse=True)\n\n    # Initialize the bins as a dictionary mapping bin index to a list of items in that bin\n    bins = {i: [] for i in range(num_bins)}\n\n    # Initialize the bin weights as a dictionary mapping bin index to the total weight of items in that bin\n    bin_weights = {i: 0 for i in range(num_bins)}\n\n    # Iterate over the sorted items\n    for item in sorted_items:\n        # Find the bin with the lowest total weight\n        min_bin = min(bin_weights, key=lambda bin: bin_weights[bin])\n\n        # Add the item to the bin with the lowest total weight\n        bins[min_bin].append(item)\n\n        # Update the total weight of the bin with the lowest total weight\n        bin_weights[min_bin] += item_weights[item]\n\n    return bins, bin_weights\n\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        data_bytes = json.dumps(data).encode(\"utf-8\")\n        return hashlib.sha256(data_bytes).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        data_bytes = json.dumps(data).encode(\"utf-8\")\n        return hashlib.sha256(data_bytes).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        data_bytes = json.dumps(data).encode(\"utf-8\")\n        return hashlib.sha256(data_bytes).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        data_bytes = json.dumps(data).encode(\"utf-8\")\n        return hashlib.sha256(data_bytes).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        data_bytes = json.dumps(data).encode(\"utf-8\")\n        return hashlib.sha256(data_bytes).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        data_bytes = json.dumps(data).encode(\"utf-8\")\n        return hashlib.sha256(data_bytes).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        data_bytes = json.dumps(data).encode(\"utf-8\")\n        return hashlib.sha256(data_bytes).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = [func_name, args, kwargs]\n        data_bytes = json.dumps(data).encode(\"utf-8\")\n        return hashlib.sha256(data_bytes).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        return hashlib.sha256(json.dumps(data).encode()).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name,) + args + tuple(sorted(kwargs.items()))\n        return hashlib.sha256(json.dumps(data).encode()).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        data_str = json.dumps(data, sort_keys=True)\n        return hashlib.sha256(data_str.encode(\"utf-8\")).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = [func_name, args, kwargs]\n        data_bytes = json.dumps(data).encode('utf-8')\n        return hashlib.sha256(data_bytes).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = [func_name, args, kwargs]\n        data_str = json.dumps(data, sort_keys=True)\n        return hashlib.sha256(data_str.encode(\"utf-8\")).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        arg_string = json.dumps((func_name, args, kwargs))\n        return hashlib.sha256(arg_string.encode()).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = [func_name, args, kwargs]\n        data_str = json.dumps(data)\n        data_bytes = data_str.encode(\"utf-8\")\n        return hashlib.sha256(data_bytes).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        arg_hash = hashlib.sha256()\n        arg_hash.update(func_name.encode())\n        for arg in args:\n            arg_hash.update(str(arg).encode())\n        for key, value in kwargs.items():\n            arg_hash.update(str(key).encode())\n            arg_hash.update(str(value).encode())\n        return arg_hash.hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        data_str = json.dumps(data, sort_keys=True)\n        arg_hash = hashlib.sha256(data_str.encode()).hexdigest()\n        return arg_hash"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        arg_dict = {\"args\": args, \"kwargs\": kwargs}\n        arg_string = json.dumps(arg_dict, sort_keys=True)\n        data = f\"{func_name}-{arg_string}\".encode(\"utf-8\")\n        return hashlib.sha256(data).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        arg_str = json.dumps({\n            \"func_name\": func_name,\n            \"args\": args,\n            \"kwargs\": kwargs\n        })\n        arg_hash = hashlib.sha256(arg_str.encode(\"utf-8\")).hexdigest()\n        return arg_hash"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        arg_string = json.dumps([func_name, list(args), kwargs])\n        return hashlib.sha256(arg_string.encode('utf-8')).hexdigest()"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the distances between consecutive points\n    distances = np.linalg.norm(polygon[1:] - polygon[:-1], axis=1)\n\n    # Filter out distances that exceed the maximum point distance\n    filtered_distances = distances[distances <= max_point_distance]\n\n    # Compute the total length of the polygon\n    total_length = np.sum(filtered_distances)\n\n    return total_length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the distances between consecutive points\n    distances = np.linalg.norm(np.diff(polygon, axis=0), axis=1)\n\n    # Filter out distances that exceed the maximum distance threshold\n    filtered_distances = distances[distances <= max_point_distance]\n\n    # Compute the total length of the polygon\n    total_length = np.sum(filtered_distances)\n\n    return total_length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    distances = np.linalg.norm(polygon[1:] - polygon[:-1], axis=1)\n    distances[distances > max_point_distance] = 0\n    return np.sum(distances)"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the distances between consecutive points\n    distances = np.linalg.norm(np.diff(polygon, axis=0), axis=1)\n\n    # Filter out distances that exceed the maximum distance\n    filtered_distances = distances[distances <= max_point_distance]\n\n    # Compute the total length of the polygon\n    total_length = np.sum(filtered_distances)\n\n    return total_length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the distances between consecutive points\n    distances = np.linalg.norm(polygon[1:] - polygon[:-1], axis=1)\n\n    # Filter out distances that exceed the maximum point distance\n    distances = distances[distances <= max_point_distance]\n\n    # Compute the total length of the polygon\n    total_length = np.sum(distances)\n\n    return total_length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the pairwise distances between all points in the polygon\n    distances = np.linalg.norm(polygon[:-1] - polygon[1:], axis=1)\n\n    # Exclude distances that exceed the maximum point distance\n    distances = distances[distances <= max_point_distance]\n\n    # Compute the total length of the polygon\n    total_length = np.sum(distances)\n\n    return total_length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the distances between consecutive points\n    distances = np.linalg.norm(polygon[1:] - polygon[:-1], axis=1)\n\n    # Filter out distances that exceed the maximum distance\n    filtered_distances = distances[distances <= max_point_distance]\n\n    # Compute the total length of the polygon\n    polygon_length = np.sum(filtered_distances)\n\n    return polygon_length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    if len(polygon.shape) != 2 or polygon.shape[1] != 2:\n        raise ValueError(f\"Unable to determine the area of a polygon with shape {polygon.shape}. Expecting (_, 2).\")\n\n    distances = np.linalg.norm(polygon[1:] - polygon[:-1], axis=1)\n    distances[distances > max_point_distance] = 0\n    total_length = np.sum(distances)\n\n    return total_length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the distances between consecutive points\n    distances = np.linalg.norm(np.diff(polygon, axis=0), axis=1)\n\n    # Filter out distances that exceed the maximum point distance\n    distances_filtered = distances[distances < max_point_distance]\n\n    # Compute the total length of the polygon\n    total_length = np.sum(distances_filtered)\n\n    return total_length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the pairwise distances between consecutive points\n    point_distances = np.linalg.norm(polygon[1:] - polygon[:-1], axis=1)\n\n    # Filter out distances that exceed the maximum distance threshold\n    filtered_distances = point_distances[point_distances <= max_point_distance]\n\n    # Sum the filtered distances to obtain the total length of the polygon\n    total_length = np.sum(filtered_distances)\n\n    return total_length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Calculate the Euclidean distance between consecutive points in the polygon\n    point_distances = np.linalg.norm(np.diff(polygon, axis=0), axis=1)\n\n    # Filter out distances that exceed the maximum point distance\n    filtered_distances = point_distances[point_distances <= max_point_distance]\n\n    # Compute the total length of the polygon\n    total_length = np.sum(filtered_distances)\n\n    return total_length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    if polygon.shape[0] < 2:\n        return 0.0\n\n    distances = np.linalg.norm(polygon[1:] - polygon[:-1], axis=1)\n    distances = np.where(distances < max_point_distance, distances, 0)\n    total_length = np.sum(distances)\n\n    return total_length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the pairwise distances between consecutive points in the polygon\n    distances = np.linalg.norm(polygon[1:] - polygon[:-1], axis=1)\n\n    # Filter the distances that are below the maximum point distance\n    filtered_distances = distances[distances <= max_point_distance]\n\n    # Compute the total length of the polygon\n    total_length = np.sum(filtered_distances)\n\n    return total_length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the pairwise distances between all points in the polygon\n    point_distances = np.linalg.norm(polygon[1:] - polygon[:-1], axis=1)\n\n    # Compute the total length of the polygon\n    total_length = np.sum(point_distances[point_distances < max_point_distance])\n\n    return total_length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    if len(polygon.shape) != 2 or polygon.shape[1] != 2:\n        raise ValueError(f\"Unable to determine the length of a polygon with shape {polygon.shape}. Expecting (_, 2).\")\n\n    # Compute the distances between consecutive points\n    distances = np.linalg.norm(polygon[1:] - polygon[:-1], axis=1)\n\n    # Filter out distances that exceed the maximum threshold\n    filtered_distances = distances[distances <= max_point_distance]\n\n    # Compute the total length of the polygon\n    total_length = np.sum(filtered_distances)\n\n    return float(total_length)"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Calculate the Euclidean distance between consecutive points\n    distances = np.linalg.norm(np.diff(polygon, axis=0), axis=1)\n\n    # Filter out distances that exceed the maximum distance threshold\n    valid_distances = distances[distances <= max_point_distance]\n\n    # Sum the remaining distances to get the total length of the polygon\n    total_length = np.sum(valid_distances)\n\n    return total_length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    if len(polygon.shape) != 2 or polygon.shape[1] != 2:\n        raise ValueError(f\"Unable to determine the area of a polygon with shape {polygon.shape}. Expecting (_, 2).\")\n\n    diffs = np.diff(polygon, axis=0)\n    distances = np.linalg.norm(diffs, axis=1)\n    distances = np.where(distances > max_point_distance, 0, distances)\n    length = np.sum(distances)\n\n    return length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the pairwise distances between all points in the polygon\n    distances = np.linalg.norm(polygon[:-1] - polygon[1:], axis=1)\n\n    # Filter the distances to exclude those that exceed the maximum distance\n    filtered_distances = distances[distances <= max_point_distance]\n\n    # Compute the total length of the polygon as the sum of the filtered distances\n    total_length = np.sum(filtered_distances)\n\n    return total_length"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Calculate the distance between consecutive points\n    point_distances = np.sqrt(np.sum(np.diff(polygon, axis=0) ** 2, axis=1))\n\n    # Filter out distances that exceed the maximum distance\n    filtered_distances = point_distances[point_distances <= max_point_distance]\n\n    # Return the total length of the polygon\n    return np.sum(filtered_distances)"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the pairwise distances between all points in the polygon\n    point_distances = np.sqrt(np.sum((polygon[:, None, :] - polygon[None, :, :]) ** 2, axis=-1))\n\n    # Set the diagonal elements to a large value to avoid counting the distance between a point and itself\n    point_distances[np.diag_indices_from(point_distances)] = np.inf\n\n    # Set the distances between points that are farther apart than the maximum distance to zero\n    point_distances[point_distances > max_point_distance] = 0\n\n    # Compute the total length of the polygon as the sum of the distances between consecutive points\n    total_length = np.sum(point_distances)\n\n    return total_length"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if not polygons:\n        return polygons\n\n    areas = [area(polygon) for polygon in polygons]\n    max_area = max(areas)\n\n    return [polygon for polygon, area in zip(polygons, areas) if area > max(rel_tr * max_area, abs_tr)]"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    areas = [area(polygon) for polygon in polygons]\n    max_area = max(areas)\n\n    return [polygon for polygon, area in zip(polygons, areas) if area > max(rel_tr * max_area, abs_tr)]"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return []\n\n    areas = [area(polygon) for polygon in polygons]\n    max_area = max(areas)\n\n    filtered_polygons = [\n        polygon for polygon, polygon_area in zip(polygons, areas) if polygon_area > max(rel_tr * max_area, abs_tr)\n    ]\n\n    return filtered_polygons"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    areas = [area(polygon) for polygon in polygons]\n    max_area = max(areas)\n    min_area = max_area * rel_tr + abs_tr\n\n    return [polygon for polygon, area in zip(polygons, areas) if area >= min_area]"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    areas = np.array([area(polygon) for polygon in polygons])\n    max_area = np.max(areas)\n\n    return [polygon for polygon, area in zip(polygons, areas) if area > max(rel_tr * max_area, abs_tr)]"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    areas = [area(polygon) for polygon in polygons]\n    max_area = max(areas)\n    filtered_polygons = [\n        polygon for polygon, polygon_area in zip(polygons, areas) if polygon_area > max(rel_tr * max_area, abs_tr)\n    ]\n    return filtered_polygons"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if not polygons:\n        return polygons\n\n    areas = [area(polygon) for polygon in polygons]\n    max_area = max(areas)\n\n    return [polygon for polygon, area in zip(polygons, areas) if area > max_area * rel_tr or area > abs_tr]"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if not polygons:\n        return polygons\n\n    areas = [area(polygon) for polygon in polygons]\n    max_area = max(areas)\n\n    return [polygon for polygon, area in zip(polygons, areas) if area > max(rel_tr * max_area, abs_tr)]"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    areas = [area(polygon) for polygon in polygons]\n    max_area = max(areas)\n    min_area = max(max_area * rel_tr, abs_tr)\n\n    return [polygon for polygon, polygon_area in zip(polygons, areas) if polygon_area >= min_area]"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    areas = [area(polygon) for polygon in polygons]\n    max_area = max(areas)\n\n    filtered_polygons = []\n    for polygon, polygon_area in zip(polygons, areas):\n        if polygon_area > max(rel_tr * max_area, abs_tr):\n            filtered_polygons.append(polygon)\n\n    return filtered_polygons"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    areas = np.array([area(polygon) for polygon in polygons])\n    max_area = areas.max()\n\n    filtered_polygons = [\n        polygon for polygon, polygon_area in zip(polygons, areas) if polygon_area > max(rel_tr * max_area, abs_tr)\n    ]\n\n    return filtered_polygons"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    areas = [area(polygon) for polygon in polygons]\n    max_area = max(areas)\n    filtered_polygons = [polygon for polygon, area in zip(polygons, areas) if area > max_area * rel_tr and area > abs_tr]\n\n    return filtered_polygons"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    polygons = [polygon for polygon in polygons if area(polygon) > abs_tr]\n\n    if len(polygons) == 0:\n        return polygons\n\n    max_area = max(area(polygon) for polygon in polygons)\n\n    polygons = [polygon for polygon in polygons if area(polygon) > rel_tr * max_area]\n\n    return polygons"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if not polygons:\n        return polygons\n\n    areas = [area(polygon) for polygon in polygons]\n    max_area = max(areas)\n\n    filtered_polygons = [\n        polygon for polygon, polygon_area in zip(polygons, areas) if polygon_area >= abs_tr and polygon_area >= max_area * rel_tr\n    ]\n\n    return filtered_polygons"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    areas = np.array([area(polygon) for polygon in polygons])\n    max_area = areas.max()\n\n    return [\n        polygon\n        for polygon, polygon_area in zip(polygons, areas)\n        if polygon_area > max(rel_tr * max_area, abs_tr)\n    ]"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    areas = np.array([area(p) for p in polygons])\n    max_area = np.max(areas)\n\n    return [p for i, p in enumerate(polygons) if areas[i] > max(rel_tr * max_area, abs_tr)]"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    polygons_areas = [area(polygon) for polygon in polygons]\n    max_area = max(polygons_areas)\n\n    return [\n        polygon\n        for polygon, polygon_area in zip(polygons, polygons_areas)\n        if polygon_area > abs_tr and polygon_area > rel_tr * max_area\n    ]"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if not polygons:\n        return []\n\n    areas = [area(polygon) for polygon in polygons]\n    max_area = max(areas)\n    min_area = max_area * rel_tr + abs_tr\n\n    return [polygon for polygon, polygon_area in zip(polygons, areas) if polygon_area > min_area]"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    areas = [area(polygon) for polygon in polygons]\n    max_area = max(areas)\n    rel_tr_area = rel_tr * max_area\n    abs_tr_area = abs_tr\n\n    return [polygon for polygon, area in zip(polygons, areas) if area > rel_tr_area and area > abs_tr_area]"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if not polygons:\n        return []\n\n    areas = [area(polygon) for polygon in polygons]\n    max_area = max(areas)\n    rel_tr_area = rel_tr * max_area\n    filtered_polygons = [\n        polygon for polygon, polygon_area in zip(polygons, areas) if polygon_area > max(rel_tr_area, abs_tr)\n    ]\n\n    return filtered_polygons"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_per_worker_remainder = num_samples_yielded % num_workers\n\n    num_samples_per_worker = num_samples_per_worker // batch_size\n    num_samples_per_worker_remainder = num_samples_per_worker_remainder // batch_size\n\n    indexes = {}\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] = num_samples_per_worker\n\n    for worker_idx in range(num_samples_per_worker_remainder):\n        indexes[worker_idx] += 1\n\n    return indexes"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_per_worker_list = [num_samples_per_worker] * num_workers\n\n    # Distribute any remaining samples\n    remaining_samples = num_samples_yielded % num_workers\n    for i in range(remaining_samples):\n        num_samples_per_worker_list[i] += 1\n\n    # Calculate the number of samples each worker has processed\n    num_samples_processed = {}\n    for worker_idx in range(num_workers):\n        num_samples_processed[worker_idx] = num_samples_per_worker_list[worker_idx] // batch_size\n\n    return num_samples_processed"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_per_worker_batch = num_samples_per_worker // batch_size\n    num_samples_per_worker_batch_remainder = num_samples_per_worker % batch_size\n\n    samples_per_worker = {}\n    for worker_idx in range(num_workers):\n        samples_per_worker[worker_idx] = num_samples_per_worker_batch\n\n    if num_samples_per_worker_batch_remainder > 0:\n        for worker_idx in range(num_samples_per_worker_batch_remainder):\n            samples_per_worker[worker_idx] += 1\n\n    return samples_per_worker"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_remaining = num_samples_yielded % num_workers\n\n    num_samples_per_worker_list = [num_samples_per_worker] * num_workers\n    for i in range(num_samples_remaining):\n        num_samples_per_worker_list[i] += 1\n\n    indexes = {}\n    for worker_idx, num_samples in enumerate(num_samples_per_worker_list):\n        num_batches = num_samples // batch_size\n        num_samples = num_batches * batch_size\n        indexes[worker_idx] = num_samples\n\n    return indexes"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_per_worker_remainder = num_samples_yielded % num_workers\n\n    num_samples_per_worker_batch = num_samples_per_worker // batch_size\n    num_samples_per_worker_batch_remainder = num_samples_per_worker % batch_size\n\n    num_samples_per_worker_batch_remainder += num_samples_per_worker_remainder\n\n    if num_samples_per_worker_batch_remainder >= batch_size:\n        num_samples_per_worker_batch += num_samples_per_worker_batch_remainder // batch_size\n        num_samples_per_worker_batch_remainder %= batch_size\n\n    indexes = {}\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] = num_samples_per_worker_batch * batch_size\n        if worker_idx < num_samples_per_worker_batch_remainder:\n            indexes[worker_idx] += batch_size\n        else:\n            indexes[worker_idx] += num_samples_per_worker_batch_remainder\n\n    return indexes"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker should process\n    num_samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of remaining samples\n    remaining_samples = num_samples_yielded % num_workers\n\n    # Initialize a dictionary to store the number of samples processed by each worker\n    num_samples_processed = {}\n\n    # Distribute the remaining samples among the workers\n    for i in range(num_workers):\n        num_samples_processed[i] = num_samples_per_worker\n        if remaining_samples > 0:\n            num_samples_processed[i] += 1\n            remaining_samples -= 1\n\n    # Calculate the number of samples each worker should process\n    num_samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of remaining samples\n    remaining_samples = num_samples_yielded % num_workers\n\n    # Initialize a dictionary to store the number of samples processed by each worker\n    num_samples_processed = {}\n\n    # Distribute the remaining samples among the workers\n    for i in range(num_workers):\n        num_samples_processed[i] = num_samples_per_worker\n        if remaining_samples > 0:\n            num_samples_processed[i] += 1\n            remaining_samples -= 1\n\n    # Calculate the number of samples each worker should process\n    num_samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of remaining samples\n    remaining_samples = num_samples_yielded % num_workers\n\n    # Initialize a dictionary to store the number of samples processed by each worker\n    num_samples_processed = {}\n\n    # Distribute the remaining samples among the workers\n    for i in range(num_workers):\n        num_samples_processed[i] = num_samples_per_worker\n        if remaining_"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker has processed\n    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_per_worker_remainder = num_samples_yielded % num_workers\n\n    # Distribute any remaining samples among the workers\n    num_samples_per_worker += num_samples_per_worker_remainder // num_workers\n    num_samples_per_worker_remainder %= num_workers\n\n    # Calculate the number of batches each worker has processed\n    num_batches_per_worker = num_samples_per_worker // batch_size\n    num_batches_per_worker_remainder = num_samples_per_worker % batch_size\n\n    # Distribute any remaining batches among the workers\n    num_batches_per_worker += num_batches_per_worker_remainder // num_workers\n    num_batches_per_worker_remainder %= num_workers\n\n    # Calculate the number of samples each worker has processed\n    num_samples_per_worker = num_batches_per_worker * batch_size\n    num_samples_per_worker_remainder = num_batches_per_worker_remainder * batch_size\n\n    # Distribute any remaining samples among the workers\n    num_samples_per_worker += num_samples_per_worker_remainder // num_workers\n    num_samples_per_worker_remainder %= num_workers\n\n    # Calculate the number of samples each worker has processed\n    num_samples_per_worker += num_samples_per_worker_remainder\n\n    # Create a dictionary to store the number of samples each worker has processed\n    num_samples_per_worker_dict = {}\n\n    # Populate the dictionary with the number of samples each worker has processed\n    for worker_idx in range(num_workers):\n        num_samples_per_worker_dict[worker_idx] = num_s"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_remaining = num_samples_yielded % num_workers\n\n    # Distribute any remaining samples among workers\n    num_samples_per_worker += num_samples_remaining // num_workers\n    num_samples_remaining %= num_workers\n\n    # Calculate the number of samples each worker has processed\n    num_samples_processed = {\n        worker_idx: num_samples_per_worker * batch_size + (num_samples_per_worker % batch_size if worker_idx < num_samples_remaining else 0)\n        for worker_idx in range(num_workers)\n    }\n\n    return num_samples_processed"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker has processed\n    num_samples_per_worker = (num_samples_yielded // num_workers) // batch_size\n\n    # Calculate the remaining samples\n    remaining_samples = num_samples_yielded - (num_samples_per_worker * num_workers * batch_size)\n\n    # Distribute the remaining samples among the workers\n    num_samples_per_worker_with_remainder = num_samples_per_worker + (remaining_samples // num_workers)\n\n    # Calculate the number of samples each worker has processed with the remainder\n    num_samples_per_worker_with_remainder_dict = {\n        worker_idx: num_samples_per_worker_with_remainder + (remaining_samples % num_workers > worker_idx)\n        for worker_idx in range(num_workers)\n    }\n\n    return num_samples_per_worker_with_remainder_dict"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker has processed\n    num_samples_per_worker = (num_samples_yielded // num_workers) // batch_size\n\n    # Calculate the remaining samples\n    remaining_samples = num_samples_yielded % (num_samples_per_worker * batch_size * num_workers)\n\n    # Distribute the remaining samples among workers\n    num_samples_per_worker_with_remaining = num_samples_per_worker + remaining_samples // num_workers\n\n    # Calculate the number of samples each worker has processed\n    num_samples_per_worker = {\n        worker_idx: num_samples_per_worker_with_remaining\n        if worker_idx < remaining_samples % num_workers\n        else num_samples_per_worker\n        for worker_idx in range(num_workers)\n    }\n\n    return num_samples_per_worker"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    samples_per_worker = num_samples_yielded // num_workers\n    samples_per_worker_remainder = num_samples_yielded % num_workers\n\n    # Distribute the remaining samples among the workers\n    for i in range(samples_per_worker_remainder):\n        if i < num_workers:\n            samples_per_worker += 1\n\n    # Calculate the number of samples each worker has processed\n    samples_per_worker_processed = {}\n    for i in range(num_workers):\n        samples_per_worker_processed[i] = samples_per_worker\n\n    # Distribute the remaining samples among the workers\n    for i in range(samples_per_worker_remainder):\n        if i < num_workers:\n            samples_per_worker_processed[i] += 1\n\n    # Calculate the number of batches each worker has processed\n    batches_per_worker_processed = {}\n    for i in range(num_workers):\n        batches_per_worker_processed[i] = samples_per_worker_processed[i] // batch_size\n\n    # Calculate the number of samples each worker has processed\n    samples_per_worker_processed = {}\n    for i in range(num_workers):\n        samples_per_worker_processed[i] = batches_per_worker_processed[i] * batch_size\n\n    return samples_per_worker_processed"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker has processed\n    num_samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples remaining\n    num_samples_remaining = num_samples_yielded % num_workers\n\n    # Initialize a dictionary to store the number of samples processed by each worker\n    num_samples_processed = {}\n\n    # Distribute the remaining samples among the workers\n    for worker_idx in range(num_workers):\n        # Calculate the number of samples to be processed by the current worker\n        num_samples_to_process = num_samples_per_worker + (1 if worker_idx < num_samples_remaining else 0)\n\n        # Update the dictionary with the number of samples processed by the current worker\n        num_samples_processed[worker_idx] = num_samples_to_process\n\n    # Calculate the number of samples each worker has processed\n    num_samples_processed = {\n        worker_idx: num_samples_processed[worker_idx] // batch_size\n        for worker_idx in range(num_workers)\n    }\n\n    # Return the dictionary with the number of samples processed by each worker\n    return num_samples_processed"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_per_worker_batch = num_samples_per_worker // batch_size\n    num_samples_remaining = num_samples_yielded - num_samples_per_worker * num_workers\n\n    num_samples_per_worker_batch_list = [num_samples_per_worker_batch] * num_workers\n\n    for i in range(num_samples_remaining):\n        num_samples_per_worker_batch_list[i] += 1\n\n    num_samples_per_worker_list = [num_samples_per_worker_batch * batch_size for num_samples_per_worker_batch in num_samples_per_worker_batch_list]\n\n    num_samples_per_worker_list[-1] += num_samples_yielded - sum(num_samples_per_worker_list)\n\n    num_samples_per_worker_list_cumsum = np.cumsum(num_samples_per_worker_list)\n\n    num_samples_per_worker_list_cumsum = np.insert(num_samples_per_worker_list_cumsum, 0, 0)\n\n    indexes = {}\n\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] = num_samples_per_worker_list_cumsum[worker_idx]\n\n    return indexes"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    samples_per_worker = num_samples_yielded // num_workers\n    samples_remaining = num_samples_yielded % num_workers\n\n    samples_per_worker = samples_per_worker // batch_size\n    samples_remaining = samples_remaining // batch_size\n\n    indexes = {}\n\n    for worker_idx in range(num_workers):\n        samples_to_yield = samples_per_worker\n\n        if worker_idx < samples_remaining:\n            samples_to_yield += 1\n\n        indexes[worker_idx] = samples_to_yield\n\n    return indexes"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker has processed\n    num_samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples that have been yielded by each worker\n    num_samples_yielded_per_worker = [num_samples_per_worker] * num_workers\n\n    # Distribute any remaining samples among the workers\n    remaining_samples = num_samples_yielded % num_workers\n    for i in range(remaining_samples):\n        num_samples_yielded_per_worker[i] += 1\n\n    # Calculate the number of samples each worker has processed in each batch\n    num_samples_processed_per_worker = [\n        num_samples_yielded_per_worker[i] // batch_size for i in range(num_workers)\n    ]\n\n    # Calculate the number of samples that have been yielded by each worker in each batch\n    num_samples_yielded_per_worker_per_batch = [\n        [num_samples_processed_per_worker[i] * batch_size for i in range(num_workers)]\n        for _ in range(num_samples_processed_per_worker[0])\n    ]\n\n    # Calculate the number of samples that have been yielded by each worker in each batch\n    num_samples_yielded_per_worker_per_batch = [\n        [num_samples_processed_per_worker[i] * batch_size for i in range(num_workers)]\n        for _ in range(num_samples_processed_per_worker[0])\n    ]\n\n    # Calculate the number of samples that each worker has processed in each batch\n    num_samples_processed_per_worker_per_batch = [\n        [num_samples_yielded_per_worker_per_batch[i][j] - num_samples_yielded_per_worker[j] for j in range(num_workers)]\n        for i in"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_per_worker_remainder = num_samples_yielded % num_workers\n\n    num_samples_per_worker_batch = num_samples_per_worker // batch_size\n    num_samples_per_worker_batch_remainder = num_samples_per_worker % batch_size\n\n    num_samples_per_worker_batch_adjusted = num_samples_per_worker_batch\n    if num_samples_per_worker_batch_remainder > 0:\n        num_samples_per_worker_batch_adjusted += 1\n\n    num_samples_per_worker_adjusted = num_samples_per_worker_batch_adjusted * batch_size\n    num_samples_per_worker_adjusted_remainder = num_samples_per_worker_adjusted % num_workers\n\n    num_samples_per_worker_adjusted_batch = num_samples_per_worker_adjusted // num_workers\n    num_samples_per_worker_adjusted_batch_remainder = num_samples_per_worker_adjusted % num_workers\n\n    num_samples_per_worker_adjusted_batch_remainder_adjusted = num_samples_per_worker_adjusted_batch\n    if num_samples_per_worker_adjusted_batch_remainder > 0:\n        num_samples_per_worker_adjusted_batch_remainder_adjusted += 1\n\n    num_samples_per_worker_adjusted_adjusted = (\n        num_samples_per_worker_adjusted_batch_remainder_adjusted * num_workers\n    )\n    num_samples_per_worker_adjusted_adjusted_remainder = (\n        num_samples_per_worker_adjusted_adjust"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker has processed\n    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_remaining = num_samples_yielded % num_workers\n\n    # Distribute any remaining samples among the workers\n    num_samples_per_worker += num_samples_remaining // num_workers\n\n    # Calculate the number of samples each worker has processed, taking into account the batch size\n    num_samples_per_worker = num_samples_per_worker // batch_size * batch_size\n\n    # Distribute any remaining samples among the workers\n    num_samples_remaining = num_samples_per_worker % batch_size\n    num_samples_per_worker = num_samples_per_worker // batch_size\n\n    # Initialize a dictionary to store the number of samples each worker has processed\n    samples_per_worker = {}\n\n    # Calculate the number of samples each worker has processed, taking into account the batch size\n    for worker_idx in range(num_workers):\n        samples_per_worker[worker_idx] = num_samples_per_worker\n\n    # Distribute any remaining samples among the workers\n    for worker_idx in range(num_samples_remaining):\n        samples_per_worker[worker_idx] += 1\n\n    return samples_per_worker"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker should have processed\n    num_samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples that each worker has processed so far\n    num_samples_processed = [num_samples_per_worker] * num_workers\n\n    # Calculate the remaining samples that need to be distributed among the workers\n    remaining_samples = num_samples_yielded % num_workers\n\n    # Distribute the remaining samples among the workers\n    for i in range(remaining_samples):\n        num_samples_processed[i] += 1\n\n    # Calculate the number of samples each worker has processed so far, taking into account the batch size\n    num_samples_processed = [num_samples_processed[i] // batch_size for i in range(num_workers)]\n\n    # Calculate the number of samples each worker has processed so far, taking into account the division of the total samples by the number of workers\n    num_samples_processed = [num_samples_processed[i] * batch_size for i in range(num_workers)]\n\n    # Calculate the number of samples each worker has processed so far, taking into account the division of the total samples by the number of workers and the batch size\n    num_samples_processed = [num_samples_processed[i] * num_workers for i in range(num_workers)]\n\n    # Calculate the number of samples each worker has processed so far, taking into account the division of the total samples by the number of workers and the batch size\n    num_samples_processed = [num_samples_processed[i] // num_workers for i in range(num_workers)]\n\n    # Calculate the number of samples each worker has processed so far, taking into account the division of the total samples by the number of workers and the batch size\n    num_samples_processed = [num_samples_processed[i] * batch_size for i in range(num_workers)]\n\n    # Calculate the"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker has processed\n    num_samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples remaining after distributing the total samples evenly among workers\n    remaining_samples = num_samples_yielded % num_workers\n\n    # Distribute the remaining samples among workers\n    num_samples_per_worker += remaining_samples // num_workers\n\n    # Calculate the number of samples each worker has processed, taking into account the batch size\n    num_samples_per_worker = (num_samples_per_worker // batch_size) * batch_size\n\n    # Distribute any remaining samples among workers\n    remaining_samples = remaining_samples % num_workers\n    for i in range(remaining_samples):\n        num_samples_per_worker += i < remaining_samples\n\n    # Return a dictionary where each key is a worker index (starting from 0) and its value is the number of samples that worker has processed\n    return {i: num_samples_per_worker for i in range(num_workers)}"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker should have processed\n    num_samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples that have been processed by each worker\n    num_samples_processed = [\n        min(num_samples_per_worker, num_samples_yielded - i * num_samples_per_worker) for i in range(num_workers)\n    ]\n\n    # Calculate the number of batches each worker has processed\n    num_batches_processed = [num_samples // batch_size for num_samples in num_samples_processed]\n\n    # Calculate the total number of batches processed\n    total_batches_processed = sum(num_batches_processed)\n\n    # Calculate the number of remaining batches to be processed\n    remaining_batches = total_batches_processed - num_samples_yielded // batch_size\n\n    # Distribute the remaining batches among the workers\n    remaining_batches_per_worker = [remaining_batches // num_workers for _ in range(num_workers)]\n    for i in range(remaining_batches % num_workers):\n        remaining_batches_per_worker[i] += 1\n\n    # Calculate the number of samples each worker has processed\n    num_samples_processed = [\n        (num_batches_processed[i] + remaining_batches_per_worker[i]) * batch_size for i in range(num_workers)\n    ]\n\n    # Calculate the number of samples each worker should have processed\n    num_samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of batches each worker has processed\n    num_batches_processed = [num_samples // batch_size for num_samples in num_samples_processed]\n\n    # Calculate the number of samples that each worker has processed\n    num_samples_processed = [num_batches_process"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n    assert len(results) == len(value) == len(metadatas), \"results, value, and metadatas must have the same length.\"\n    filtered_results = [result for result, value in zip(results, value) if value <= threshold]\n    filtered_metadatas = [metadata for metadata, value in zip(metadatas, value) if value <= threshold]\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    assert len(results) == len(value), \"results and value must have the same length.\"\n    if metadatas is None:\n        metadatas = [None] * len(results)\n    assert len(results) == len(metadatas), \"results and metadatas must have the same length.\"\n    return [result for result, value in zip(results, value) if value <= threshold], \\\n           [metadata for metadata, value in zip(metadatas, value) if value <= threshold]\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    assert len(results) == len(value), \"results and value must have the same length.\"\n    if metadatas is None:\n        metadatas = [None] * len(results)\n    assert len(results) == len(metadatas), \"results and metadatas must have the same length.\"\n    return [result for result, value in zip(results, value) if value <= threshold], \\\n           [metadata for metadata, value in zip(metadatas, value) if value <= threshold]\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n    for i, v in enumerate(value):\n        if v <= threshold:\n            filtered_results.append(results[i])\n            if metadatas is not None:\n                filtered_metadatas.append(metadatas[i])\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n    assert len(results) == len(value) == len(metadatas), \"results, value, and metadatas must have the same length.\"\n    filtered_results = [result for result, value, metadata in zip(results, value, metadatas) if value <= threshold]\n    filtered_metadatas = [metadata for result, value, metadata in zip(results, value, metadatas) if value <= threshold]\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    assert len(results) == len(value), \"results and value must have the same length.\"\n    if metadatas is None:\n        metadatas = [None] * len(results)\n    assert len(results) == len(metadatas), \"results and metadatas must have the same length.\"\n    filtered_results = [result for result, value in zip(results, value) if value <= threshold]\n    filtered_metadatas = [metadata for metadata, value in zip(metadatas, value) if value <= threshold]\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n    assert len(results) == len(value) == len(metadatas), \"results, value, and metadatas must have the same length.\"\n    return [result for result, value, metadata in zip(results, value, metadatas) if value <= threshold], \\\n           [metadata for result, value, metadata in zip(results, value, metadatas) if value <= threshold]\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n    assert len(results) == len(value) == len(metadatas), \"results, value, and metadatas must have the same length.\"\n    assert all([isinstance(v, (int, float)) for v in value]), \"value must be a list of numeric values.\"\n    return_results = []\n    return_metadatas = []\n    for i, v in enumerate(value):\n        if v <= threshold:\n            return_results.append(results[i])\n            return_metadatas.append(metadatas[i])\n    return return_results, return_metadatas"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n    assert len(results) == len(value) == len(metadatas), \"results, value, and metadatas must have the same length.\"\n    return [result for result, value, metadata in zip(results, value, metadatas) if value <= threshold], \\\n           [metadata for result, value, metadata in zip(results, value, metadatas) if value <= threshold]\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    assert len(results) == len(value), \"results and value must have the same length.\"\n    if metadatas is None:\n        metadatas = [None] * len(results)\n    assert len(results) == len(metadatas), \"results and metadatas must have the same length.\"\n    filtered_results = [result for result, value in zip(results, value) if value <= threshold]\n    filtered_metadatas = [metadata for metadata, value in zip(metadatas, value) if value <= threshold]\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if not isinstance(results, list):\n        results = [results]\n    if not isinstance(value, list):\n        value = [value]\n    if metadatas is None:\n        metadatas = [None] * len(results)\n    assert len(results) == len(value) == len(metadatas), \\\n        \"results, value and metadatas must have the same length.\"\n    return [result for result, value, metadata in zip(results, value, metadatas) if value <= threshold], \\\n           [metadata for result, value, metadata in zip(results, value, metadatas) if value <= threshold]"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n    assert len(results) == len(value) == len(metadatas), \"results, value and metadatas must have the same length.\"\n    assert all([isinstance(v, (int, float)) for v in value]), \"value must be numeric.\"\n    assert all([isinstance(v, (int, float)) for v in value if v is not None]), \"value must be numeric.\"\n    assert all([isinstance(m, (int, float, str, list, tuple, dict)) or m is None for m in metadatas]), \\\n        \"metadatas must be numeric, string, list, tuple or dict.\"\n    assert all([isinstance(r, (int, float, str, list, tuple, dict)) for r in results]), \\\n        \"results must be numeric, string, list, tuple or dict.\"\n    assert all([isinstance(r, (int, float, str, list, tuple, dict)) for r in results if r is not None]), \\\n        \"results must be numeric, string, list, tuple or dict.\"\n    assert all([isinstance(v, (int, float)) for v in value]), \"value must be numeric.\"\n    assert all([isinstance(v, (int, float)) for v in value if v is not None]), \"value must be numeric.\"\n    assert all([isinstance(m, (int, float, str, list, tuple, dict)) or m is None for m in metadatas]), \\\n        \"metadatas must be numeric, string, list, tuple or dict.\"\n    assert all([isinstance(r, (int, float, str, list, tuple, dict)) for r in results]), \\\n        \"results must be numeric, string, list, tuple or dict.\"\n    assert all([isinstance(r, (int, float, str, list, tuple, dict)) for r in results if r is not None]), \\\n        \"results must be numeric, string, list, tuple or dict.\"\n    assert all([is"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    assert len(results) == len(value), \"results and value must have the same length.\"\n    if metadatas is None:\n        metadatas = [None] * len(results)\n    assert len(metadatas) == len(results), \"metadatas and results must have the same length.\"\n    return [result for result, value in zip(results, value) if value <= threshold], \\\n           [metadata for metadata, value in zip(metadatas, value) if value <= threshold]\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n    assert len(results) == len(value) == len(metadatas), \"results, value, and metadatas must have the same length.\"\n    assert all([isinstance(value_i, (int, float)) for value_i in value]), \"value must be numeric.\"\n    assert all([isinstance(metadata, dict) or metadata is None for metadata in metadatas]), \"metadatas must be dict or None.\"\n    return [result for result, value_i in zip(results, value) if value_i <= threshold], \\\n           [metadata for metadata, value_i in zip(metadatas, value) if value_i <= threshold]"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n    assert len(results) == len(value) == len(metadatas), \"results, value and metadatas must have the same length.\"\n    filtered_results = []\n    filtered_metadatas = []\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n    assert len(results) == len(value) == len(metadatas), \"results, value, and metadatas must have the same length.\"\n    return [result for result, value, metadata in zip(results, value, metadatas) if value <= threshold], \\\n           [metadata for result, value, metadata in zip(results, value, metadatas) if value <= threshold]\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if not bool(results):\n        return results, metadatas\n    if metadatas is None:\n        metadatas = [None] * len(results)\n    assert len(results) == len(value), \"results and value must have the same length.\"\n    assert len(results) == len(metadatas), \"results and metadatas must have the same length.\"\n    filtered_results = [result for result, value in zip(results, value) if value <= threshold]\n    filtered_metadatas = [metadata for metadata, value in zip(metadatas, value) if value <= threshold]\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    assert len(results) == len(value), \"results and value must have the same length.\"\n    assert all([isinstance(result, pd.DataFrame) for result in results]), \"results must be pd.DataFrame.\"\n    if metadatas is None:\n        metadatas = [None] * len(results)\n    assert len(results) == len(metadatas), \"results and module_filename must have the same length.\"\n    if not results:\n        return [], []\n    else:\n        return [result for result, value in zip(results, value) if value <= threshold], \\\n               [metadata for metadata, value in zip(metadatas, value) if value <= threshold]\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n    assert len(results) == len(value) == len(metadatas), \"results, value and metadatas must have the same length.\"\n    filtered_results = []\n    filtered_metadatas = []\n    for result, v, metadata in zip(results, value, metadatas):\n        if v <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    # Check if the length of the results and values lists are the same\n    assert len(results) == len(value), \"results and value must have the same length.\"\n    # Check if the length of the metadatas list matches the length of the results list\n    if metadatas is None:\n        metadatas = [None] * len(results)\n    assert len(results) == len(metadatas), \"results and metadatas must have the same length.\"\n    # Filter the results and metadatas based on the threshold\n    filtered_results = []\n    filtered_metadatas = []\n    for i in range(len(results)):\n        if value[i] <= threshold:\n            filtered_results.append(results[i])\n            filtered_metadatas.append(metadatas[i])\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f\"Input array must have shape (_, 2), but got {array.shape}\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"The input array must have shape (_, 2), where _ can be any number of points.\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    # Calculate the area using the Shoelace formula\n    area = 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n    return area"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"The input array must have shape (_, 2), where _ can be any number of points.\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    S1 = np.sum(x * np.roll(y, -1))\n    S2 = np.sum(y * np.roll(x, -1))\n\n    area = np.abs((S1 - S2) * 0.5)\n\n    return area"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2)\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    S1 = np.dot(x, np.roll(y, -1))\n    S2 = np.dot(y, np.roll(x, -1))\n\n    area = np.abs((S1 - S2) * 0.5)\n\n    return area"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"The input array must have shape (_, 2), where _ can be any number of points.\")\n\n    xs = array[:, 0]\n    ys = array[:, 1]\n\n    # Apply Shoelace formula\n    return 0.5 * np.abs(np.dot(xs, np.roll(ys, 1)) - np.dot(ys, np.roll(xs, 1)))"}
{"namespace": "iris.utils.math.area", "completion": "    if array.ndim != 2 or array.shape[1] != 2:\n        raise ValueError(f\"This function expects a polygon, i.e. an array of shape (_, 2). Got {array.shape}\")\n\n    # Compute the area using the Shoelace formula\n    x = array[:, 0]\n    y = array[:, 1]\n    area = np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1))) / 2\n\n    return area"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"The input array must have shape (_, 2).\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    # Shoelace formula\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.ndim != 2 or array.shape[1] != 2:\n        raise ValueError(f\"This function expects a polygon, i.e. an array of shape (_, 2). Got {array.shape}\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f\"The input array must have shape (_, 2). Got {array.shape}\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    S1 = np.sum(x * np.roll(y, -1))\n    S2 = np.sum(y * np.roll(x, -1))\n\n    A = 0.5 * np.abs(S1 - S2)\n\n    return A\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2)\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    S1 = np.sum(x * np.roll(y, -1))\n    S2 = np.sum(y * np.roll(x, -1))\n\n    area = np.abs((S1 - S2) * 0.5)\n    return area\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"The input array must have shape (_, 2), where _ is the number of points.\")\n\n    xs: np.ndarray = array[:, 0]\n    ys: np.ndarray = array[:, 1]\n\n    # Shoelace formula\n    return 0.5 * np.abs(np.dot(xs, np.roll(ys, 1)) - np.dot(ys, np.roll(xs, 1)))\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f\"Input array must have shape (_, 2), but got {array.shape}\")\n\n    # Shoelace formula\n    x = array[:, 0]\n    y = array[:, 1]\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2).\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"The input array must have shape (_, 2), where _ can be any number of points.\")\n\n    x: np.ndarray = array[:, 0]\n    y: np.ndarray = array[:, 1]\n\n    # Applying the Shoelace formula\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"The input array must have shape (_, 2), where _ is the number of points.\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    S1 = np.sum(x * np.roll(y, -1))\n    S2 = np.sum(y * np.roll(x, -1))\n\n    area = np.abs((S1 - S2) * 0.5)\n\n    return area\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.ndim != 2 or array.shape[1] != 2:\n        raise ValueError(f\"This function expects a polygon, i.e. an array of shape (_, 2). Got {array.shape}\")\n\n    # fmt: off\n    return 0.5 * np.abs(np.dot(array[:, 0], np.roll(array[:, 1], 1)) - np.dot(array[:, 1], np.roll(array[:, 0], 1)))\n    # fmt: on\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f\"Expected array of shape (_, 2), got {array.shape}\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f\"Input array must have shape (_, 2), but got {array.shape}\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    # Apply the Shoelace formula\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"The input array must have shape (_, 2), where _ can be any number of points.\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    S1 = np.sum(x * np.roll(y, -1))\n    S2 = np.sum(y * np.roll(x, -1))\n\n    A = 0.5 * np.abs(S1 - S2)\n\n    return A\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"The input array must have shape (_, 2)\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx = torch.searchsorted(a, v, right=True)\n    idx_lo = torch.max(torch.zeros_like(idx - 1), idx - 1)\n    idx_hi = torch.min(idx, torch.full_like(idx, a.shape[-1] - 1))\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx = torch.searchsorted(a, v, right=True)\n    idx_lo = torch.max(torch.zeros_like(idx - 1), idx - 1)\n    idx_hi = torch.min(idx, a.shape[-1] - 1)\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx = torch.searchsorted(a, v, right=True)\n    idx_lo = torch.max(torch.zeros_like(idx - 1), idx - 1)\n    idx_hi = torch.min(idx, torch.full_like(idx, a.shape[-1] - 1))\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, side='left')\n    idx_hi = torch.searchsorted(a, v, side='right')\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx = torch.searchsorted(a, v)\n    idx_lo = torch.max(torch.zeros_like(idx), idx - 1)\n    idx_hi = torch.min(idx, a.shape[-1] - 1)\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, side='right') - 1\n    idx_hi = torch.searchsorted(a, v, side='left')\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx = torch.searchsorted(a, v, side='right')\n    idx_lo = idx - 1\n    idx_hi = idx\n\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx = torch.searchsorted(a, v, right=True)\n    idx_lo = torch.maximum(torch.zeros_like(idx), idx - 1)\n    idx_hi = torch.minimum(idx, a.shape[-1] - 1)\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx = torch.searchsorted(a, v, right=True)\n    idx_lo = torch.maximum(torch.tensor(0, device=idx.device), idx - 1)\n    idx_hi = torch.minimum(idx, a.shape[-1] - 1)\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx = torch.searchsorted(a, v, right=True)\n    idx = torch.max(torch.zeros_like(idx), idx - 1)\n    idx_lo = idx\n    idx_hi = torch.min(idx + 1, torch.full_like(idx, a.shape[-1] - 1))\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx = torch.searchsorted(a, v, side='right')\n    idx_lo = idx - 1\n    idx_hi = idx\n\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx = torch.searchsorted(a, v, side='left')\n    idx_lo = idx - 1\n    idx_hi = idx\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # assert t.ndim == w.ndim + 1\n    idx_lo = torch.searchsorted(a, v, side='left')\n    idx_hi = torch.searchsorted(a, v, side='right')\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx = torch.searchsorted(a, v, side='right')\n    idx_lo = idx - 1\n    idx_hi = idx\n\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # prepare for size change\n    sh = *v.shape[:-1], 1\n    v = v.reshape(-1, v.shape[-1])\n    a = a.reshape(-1, a.shape[-1])\n\n    idx_lo = torch.searchsorted(a, v, side='left')\n    idx_hi = torch.searchsorted(a, v, side='right')\n\n    # prepare for size change\n    idx_lo = idx_lo.reshape(sh)\n    idx_hi = idx_hi.reshape(sh)\n\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Ensure that a is sorted.\n    assert torch.all(torch.diff(a) >= 0)\n\n    # Find the indices where v should be inserted into a to maintain order.\n    idx_lo = torch.searchsorted(a, v, side='left')\n    idx_hi = torch.searchsorted(a, v, side='right')\n\n    # Return the lower and upper bounds where each element of v could be inserted into a.\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, side='right') - 1\n    idx_hi = torch.searchsorted(a, v, side='left')\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, side='left')\n    idx_hi = torch.searchsorted(a, v, side='right')\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # handle the case where a is constant\n    if a.shape[-1] == 1:\n        return torch.zeros_like(v, dtype=torch.int32), torch.ones_like(v, dtype=torch.int32)\n\n    # handle the case where v is constant\n    if v.shape[-1] == 1:\n        idx_lo = torch.searchsorted(a, v, side='right') - 1\n        idx_hi = torch.searchsorted(a, v, side='left')\n        return idx_lo, idx_hi\n\n    # handle the case where a and v have the same shape\n    if a.shape == v.shape:\n        idx_lo = torch.searchsorted(a, v, side='right') - 1\n        idx_hi = torch.searchsorted(a, v, side='left')\n        return idx_lo, idx_hi\n\n    # handle the case where a and v have different shapes\n    idx_lo = torch.searchsorted(a, v[..., 0], side='right') - 1\n    idx_hi = torch.searchsorted(a, v[..., 1], side='left')\n    return idx_lo, idx_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, side='left')\n    idx_hi = torch.searchsorted(a, v, side='right')\n    return idx_lo, idx_hi\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ])"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ])"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ])"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ])"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array(\n      [\n          [fx, 0, cx],\n          [0, fy, cy],\n          [0, 0, 1],\n      ],\n      dtype=xnp.float32,\n  )"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array(\n      [\n          [fx, 0, cx],\n          [0, fy, cy],\n          [0, 0, 1],\n      ],\n      dtype=xnp.float32,\n  )"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ])"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ])"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array(\n      [\n          [fx, 0, cx],\n          [0, fy, cy],\n          [0, 0, 1],\n      ]\n  )"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array(\n      [\n          [fx, 0.0, cx],\n          [0.0, fy, cy],\n          [0.0, 0.0, 1.0],\n      ]\n  )"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array(\n      [\n          [fx, 0.0, cx],\n          [0.0, fy, cy],\n          [0.0, 0.0, 1.0],\n      ]\n  )"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array(\n      [\n          [fx, 0.0, cx],\n          [0.0, fy, cy],\n          [0.0, 0.0, 1.0],\n      ]\n  )"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array(\n      [\n          [fx, 0.0, cx],\n          [0.0, fy, cy],\n          [0.0, 0.0, 1.0],\n      ]\n  )"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array(\n      [\n          [fx, 0, cx],\n          [0, fy, cy],\n          [0, 0, 1],\n      ],\n      dtype=xnp.float32,\n  )"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array(\n      [\n          [fx, 0.0, cx],\n          [0.0, fy, cy],\n          [0.0, 0.0, 1.0],\n      ]\n  )"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array(\n      [\n          [fx, 0.0, cx],\n          [0.0, fy, cy],\n          [0.0, 0.0, 1.0],\n      ]\n  )"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0.0, cx],\n      [0.0, fy, cy],\n      [0.0, 0.0, 1.0],\n  ])"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  # Create the intrinsic matrix\n  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1]\n  ])"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  # Create the intrinsic matrix using the provided parameters.\n  return xnp.array(\n      [\n          [fx, 0.0, cx],\n          [0.0, fy, cy],\n          [0.0, 0.0, 1.0],\n      ],\n      dtype=xnp.float32,\n  )"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  # Create an empty matrix of shape (3, 3) with zeros\n  matrix = xnp.zeros((3, 3))\n\n  # Set the diagonal elements of the matrix to the focal lengths\n  matrix[0, 0] = fx\n  matrix[1, 1] = fy\n\n  # Set the off-diagonal elements of the matrix to the principal point coordinates\n  matrix[0, 2] = cx\n  matrix[1, 2] = cy\n\n  # Set the last element of the matrix to 1\n  matrix[2, 2] = 1\n\n  return matrix"}
{"namespace": "coord.contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = 2 * jnp.sqrt(x_mag_sq) - x_mag_sq\n  z = x / scale\n  return z"}
{"namespace": "coord.contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = 2 * jnp.sqrt(x_mag_sq) - x_mag_sq\n  z = x / scale\n  return z"}
{"namespace": "coord.contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = 2 * jnp.sqrt(x_mag_sq) - x_mag_sq\n  z = x / scale\n  return z"}
{"namespace": "coord.contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = 2 * jnp.sqrt(x_mag_sq) - x_mag_sq\n  z = x / scale\n  return z"}
{"namespace": "coord.contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = 2 * jnp.sqrt(x_mag_sq) - x_mag_sq\n  z = x / scale\n  return z"}
{"namespace": "coord.contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = 2 * jnp.sqrt(x_mag_sq) - x_mag_sq\n  z = x / scale\n  return z"}
{"namespace": "coord.contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = 2 * jnp.sqrt(x_mag_sq) - x_mag_sq\n  z = x / scale\n  return z"}
{"namespace": "coord.contract", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = 2 * jnp.sqrt(x_mag_sq) - x_mag_sq\n  z = x / scale\n  return z"}
{"namespace": "coord.contract", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = 2 * jnp.sqrt(x_mag_sq) - x_mag_sq\n  z = x / scale\n  return z"}
{"namespace": "coord.contract", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = 2 * jnp.sqrt(x_mag_sq) - x_mag_sq\n  z = x / scale\n  return z"}
{"namespace": "coord.contract", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = 2 * jnp.sqrt(x_mag_sq) - x_mag_sq\n  z = x / scale\n  return z"}
{"namespace": "coord.contract", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = 2 * jnp.sqrt(x_mag_sq) - x_mag_sq\n  z = x / scale\n  return z"}
{"namespace": "coord.contract", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = 2 * jnp.sqrt(x_mag_sq) - x_mag_sq\n  z = scale * x\n  return z"}
{"namespace": "coord.contract", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = 2 * jnp.sqrt(x_mag_sq) - x_mag_sq\n  z = x * scale\n  return z"}
{"namespace": "coord.contract", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = 1 / jnp.sqrt(x_mag_sq)\n  z = x * scale\n  return z"}
{"namespace": "coord.contract", "completion": "  x_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = 2 * jnp.sqrt(x_mag_sq) - x_mag_sq\n  z = x / scale\n  return z"}
{"namespace": "coord.contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = 1 - 2 / (jnp.sqrt(x_mag_sq) + 1)\n  z = x / scale\n  return z"}
{"namespace": "coord.contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = 1 - jnp.sqrt(1 - x_mag_sq)\n  z = x * scale\n  return z"}
{"namespace": "coord.contract", "completion": "  x_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = jnp.where(\n      x_mag_sq > 1, 2 * jnp.sqrt(x_mag_sq) - x_mag_sq, jnp.ones_like(x_mag_sq)\n  )\n  z = x / scale\n  return z"}
{"namespace": "coord.contract", "completion": "  x_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = 1 - 2 / (3 * jnp.sqrt(x_mag_sq))\n  z = x * scale\n  return z"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n        if abs(num_bytes) < 1000.0:\n            return f\"{num_bytes:3.1f} {unit}\"\n        num_bytes /= 1000.0\n    return f\"{num_bytes:.1f} PB\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n        if abs(num_bytes) < 1000.0:\n            return f\"{num_bytes:3.1f} {unit}\"\n        num_bytes /= 1000.0\n    return f\"{num_bytes:.1f} PB\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n        if num_bytes < 1000.0:\n            return f\"{num_bytes:3.1f} {unit}\"\n        num_bytes /= 1000.0\n    return f\"{num_bytes:.1f} PB\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n        if abs(num_bytes) < 1000.0:\n            return f\"{num_bytes:3.1f} {unit}\"\n        num_bytes /= 1000.0\n    return f\"{num_bytes:.1f} PB\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n        if num_bytes < 1000.0:\n            return f\"{num_bytes:3.1f} {unit}\"\n        num_bytes /= 1000.0\n    return f\"{num_bytes:.1f} PB\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n        if num_bytes < 1024.0:\n            return f\"{num_bytes:.2f} {unit}\"\n        num_bytes /= 1024.0\n    return f\"{num_bytes:.2f} PB\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n        if num_bytes < 1000:\n            return f\"{num_bytes:.2f} {unit}\"\n        num_bytes /= 1000\n    return f\"{num_bytes:.2f} PB\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n        if num_bytes < 1000:\n            return f\"{num_bytes:.1f} {unit}\"\n        num_bytes /= 1000\n    return f\"{num_bytes:.1f} PB\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0B\"\n    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n        if num_bytes < 1000:\n            return f\"{num_bytes:.2f}{unit}\"\n        num_bytes /= 1000\n    return f\"{num_bytes:.2f}PB\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n        if num_bytes < 1024.0:\n            return f\"{num_bytes:.2f}{unit}\"\n        num_bytes /= 1024.0\n    return f\"{num_bytes:.2f}PB\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n        if abs(num_bytes) < 1000.0:\n            return f\"{num_bytes:3.1f}{unit}\"\n        num_bytes /= 1000.0\n    return f\"{num_bytes:.1f}PB\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes < 1000:\n        return str(int(num_bytes)) + \" B\"\n    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n        if num_bytes < 1000:\n            return f\"{num_bytes:.2f} {unit}\"\n        num_bytes /= 1000\n    return f\"{num_bytes:.2f} PB\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0B\"\n    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n    i = 0\n    while num_bytes >= 1000:\n        num_bytes /= 1000\n        i += 1\n    return f\"{num_bytes:.2f}{units[i]}\""}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n        if num_bytes < 1000:\n            return f\"{num_bytes:.2f} {unit}\"\n        num_bytes /= 1000\n    return f\"{num_bytes:.2f} PB\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n        if abs(num_bytes) < 1024.0:\n            return f\"{num_bytes:3.1f}{unit}B\"\n        num_bytes /= 1024.0\n    return f\"{num_bytes:.1f}PB\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n        if num_bytes < 1000.0 or unit == \"PB\":\n            return f\"{num_bytes:3.1f} {unit}\"\n        num_bytes /= 1000.0\n    return f\"{num_bytes:3.1f} PB\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n        if abs(num_bytes) < 1000.0:\n            return f\"{num_bytes:3.1f} {unit}\"\n        num_bytes /= 1000.0\n    return f\"{num_bytes:.1f} PB\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n        if num_bytes < 1000.0:\n            return f\"{num_bytes:.1f} {unit}\"\n        num_bytes /= 1000.0\n    return f\"{num_bytes:.1f} PB\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n        if abs(num_bytes) < 1000.0:\n            return f\"{num_bytes:3.1f}{unit}\"\n        num_bytes /= 1000.0\n    return f\"{num_bytes:.1f}PB\"\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n        if abs(num_bytes) < 1000.0:\n            return f\"{num_bytes:3.1f} {unit}\"\n        num_bytes /= 1000.0\n    return f\"{num_bytes:.1f} PB\"\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __array_n_dimensions(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions.\")\n\n        return v\n\n    return __array_n_dimensions"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __array_n_dimensions(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions.\")\n        return v\n\n    return __array_n_dimensions"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __array_n_dimensions(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions.\")\n\n        return v\n\n    return __array_n_dimensions"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __array_n_dimensions(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions.\")\n\n        return v\n\n    return __array_n_dimensions"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions.\")\n        return v\n\n    return __validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions.\")\n\n        return v\n\n    return __validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __array_n_dimensions(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions.\")\n\n        return v\n\n    return __array_n_dimensions"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __array_n_dimensions(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. Got {v.ndim}.\"\n            )\n\n        return v\n\n    return __array_n_dimensions"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __array_n_dimensions(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check that the array has `nb_dimensions` dimensions.\"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions.\")\n\n        return v\n\n    return __array_n_dimensions"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __array_n_dimensions(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. Received {v.ndim}.\"\n            )\n        return v\n\n    return __array_n_dimensions"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __array_n_dimensions(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check that the array has nb_dimensions dimensions.\"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions.\")\n\n        return v\n\n    return __array_n_dimensions"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __array_n_dimensions(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be {nb_dimensions}D array.\")\n        return v\n\n    return __array_n_dimensions"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validate_array_n_dimensions(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be {nb_dimensions}D array.\")\n\n        return v\n\n    return __validate_array_n_dimensions"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check if array has `nb_dimensions` dimensions.\"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. Got {len(v.shape)}.\")\n\n        return v\n\n    return __validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __array_n_dimensions(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must be a {nb_dimensions}D array, got {v.ndim}D array\"\n            )\n\n        return v\n\n    return __array_n_dimensions"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __array_n_dimensions(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check that the array has a specific number of dimensions.\"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. Got {len(v.shape)}.\"\n            )\n        return v\n\n    return __array_n_dimensions\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must be an array of shape {nb_dimensions} dimensions, \"\n                f\"got an array of shape {len(v.shape)} dimensions.\"\n            )\n        return v\n\n    return __validator"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __array_n_dimensions(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions.\")\n\n        return v\n\n    return __array_n_dimensions\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __array_n_dimensions(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions.\")\n        return v\n\n    return __array_n_dimensions\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must be a {nb_dimensions}D array, got {len(v.shape)}D.\"\n            )\n\n        return v\n\n    return __validator\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(cartesian_vector[..., 2] / (r + eps))\n  phi = jnp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n  return r, theta, phi  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = jnp.linalg.norm(cartesian_vector, axis=-1)\n  theta = jnp.arccos(z / (r + eps))\n  phi = jnp.arctan2(y, x)\n  return r, theta, phi  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = jnp.linalg.norm(cartesian_vector, axis=-1)\n  theta = jnp.arccos(jnp.clip(z / (r + eps), -1, 1))\n  phi = jnp.arctan2(y, x)\n  return r, theta, phi"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x = cartesian_vector[..., 0]\n  y = cartesian_vector[..., 1]\n  z = cartesian_vector[..., 2]\n\n  r = jnp.linalg.norm(cartesian_vector, axis=-1)\n  theta = jnp.arccos(z / (r + eps))\n  phi = jnp.arctan2(y, x)\n\n  return r, theta, phi  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = jnp.split(cartesian_vector, 3, axis=-1)\n  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(z / (r + eps))\n  phi = jnp.arctan2(y, x)\n  return r, theta, phi  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = jnp.sqrt(x**2 + y**2 + z**2)\n  theta = jnp.arccos(z / (r + eps))\n  phi = jnp.arctan2(y, x)\n  return r, theta, phi"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x = cartesian_vector[..., 0]\n  y = cartesian_vector[..., 1]\n  z = cartesian_vector[..., 2]\n\n  r = jnp.linalg.norm(cartesian_vector, axis=-1)\n  theta = jnp.arccos(z / (r + eps))\n  phi = jnp.arctan2(y, x)\n\n  return r, theta, phi  # pytype: disable=bad-return-type  # jax-ndarray\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(jnp.clip(z / (r + eps), -1, 1))\n  phi = jnp.arctan2(y, x)\n  return r, theta, phi  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x = cartesian_vector[..., 0]\n  y = cartesian_vector[..., 1]\n  z = cartesian_vector[..., 2]\n\n  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(z / jnp.clip(r, eps, None))\n  phi = jnp.arctan2(y, x)\n\n  return r, theta, phi"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = jnp.linalg.norm(cartesian_vector, axis=-1)\n  theta = jnp.arccos(z / (r + eps))\n  phi = jnp.arctan2(y, x)\n  return r, theta, phi  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  r = jnp.sqrt(x**2 + y**2 + z**2)\n  theta = jnp.arccos(z / (r + eps))\n  phi = jnp.arctan2(y, x)\n\n  return r, theta, phi  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = jnp.split(cartesian_vector, 3, axis=-1)\n  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(z / (r + eps))\n  phi = jnp.arctan2(y, x)\n\n  return r, theta, phi  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = jnp.linalg.norm(cartesian_vector, axis=-1)\n  theta = jnp.arccos(z / (r + eps))\n  phi = jnp.arctan2(y, x)\n  return r, theta, phi\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(z / (r + eps))\n  phi = jnp.arctan2(y, x)\n  return r, theta, phi  # pytype: disable=bad-return-type  # jax-ndarray\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x = cartesian_vector[..., 0]\n  y = cartesian_vector[..., 1]\n  z = cartesian_vector[..., 2]\n\n  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(jnp.clip(z / (r + eps), -1, 1))\n  phi = jnp.arctan2(y, x)\n\n  return r, theta, phi  # pytype: disable=bad-return-type  # jax-ndarray\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = jnp.split(cartesian_vector, 3, axis=-1)\n\n  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(z / jnp.clip(r, eps, None))\n  phi = jnp.arctan2(y, x)\n\n  return r, theta, phi  # pytype: disable=bad-return-type  # jax-ndarray\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(jnp.clip(z / (r + eps), -1.0, 1.0))\n  phi = jnp.arctan2(y, x)\n\n  return r, theta, phi"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(cartesian_vector[..., 2] / (r + eps))\n  phi = jnp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n  return r, theta, phi  # pytype: disable=bad-return-type  # jax-ndarray\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Convert to cartesian coordinates.\n  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n\n  # Calculate radius.\n  r = jnp.sqrt(x ** 2 + y ** 2 + z ** 2)\n\n  # Calculate inclination.\n  theta = jnp.arccos(z / (r + eps))\n\n  # Calculate azimuth.\n  phi = jnp.arctan2(y, x)\n\n  return r, theta, phi"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(cartesian_vector[..., 2] / (r + eps))\n  phi = jnp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n\n  return r, theta, phi  # pytype: disable=bad-return-type  # jax-ndarray\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[reference], tokenizer=f)\n    return results['rougeL']"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[reference], tokenizer=f)\n    return results['rougeL']"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[reference], tokenizer=f)\n    return results['rougeL']"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[reference], tokenizer=f)\n    return results['rougeL']"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[reference], tokenizer=f)\n    return results['rougeL']"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[reference], tokenizer=f)\n    return results['rougeL']"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[reference], tokenizer=f)\n    return results['rougeL']"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[reference], tokenizer=f)\n    score = results['rougeL']\n    return score"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[reference], tokenizer=f)\n    score = results['rougeL']\n    return score"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[reference], tokenizer=f)\n    score = results['rougeL']\n    return score"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[reference], tokenizer=f)\n    score = results['rougeL']\n    return score"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    return results['rougeL']"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[reference], tokenizer=f)\n    score = results['rougeL']\n    return score"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[reference], tokenizer=f)\n    score = results['rougeL']\n    return score"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[reference], tokenizer=f)\n    score = results['rougeL']\n    return score"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['rougeL']\n    return score"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[reference], tokenizer=f)\n    return results['rougeL']\n\n"}
{"namespace": "common.rougeL_score", "completion": "    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    f = lambda text: list(jieba.cut(text))\n    results = rouge.compute(predictions=[continuation], references=[reference], tokenizer=f)\n    return results['rougeL']"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[reference], tokenizer=f)\n    score = results['rougeL']\n    return score"}
{"namespace": "common.rougeL_score", "completion": "    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    f = lambda text: list(jieba.cut(text))\n    results = rouge.compute(predictions=[continuation], references=[reference], tokenizer=f)\n    return results['rougeL']"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return pydoc.locate(name)  # first try import through pydoc\n    except ImportError:\n        # fallback to import through string\n        components = name.split(\".\")\n        mod = __import__(components[0])\n        for comp in components[1:]:\n            mod = getattr(mod, comp)\n        return mod\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return pydoc.locate(name)\n    except ImportError as e:\n        if name.startswith(\"flamingo.\"):\n            raise e\n        return pydoc.locate(f\"flamingo.{name}\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return pydoc.locate(name)\n    except ImportError as e:\n        if name.startswith(\"my_package.\"):\n            name = name[len(\"my_package.\") :]\n            return pydoc.locate(name)\n        raise e\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return pydoc.locate(name)\n    except ImportError:\n        # fallback to a manual scan.\n        # Remove this if at some point we can drop python 2 support.\n        obj = pydoc.locate(\".\".join(name.split(\".\")[:-1]))\n        return getattr(obj, name.split(\".\")[-1])\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return pydoc.locate(name)  # first try import through pydoc\n    except ImportError:\n        try:\n            return pydoc.locate(f\"{name}.{name}\")  # then try importing the class directly\n        except ImportError:\n            raise ImportError(f\"Cannot locate '{name}'\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # first try importing the target module and getattr it.  If it's a class we\n    # can instantiate it directly\n    try:\n        return pydoc.locate(name)\n    except ImportError:\n        # separate the module from the rest of the path and import it first to\n        # avoid polluting the global namespace with variables from the\n        # module file.\n        components = name.rsplit(\".\", 1)\n        mod = __import__(components[0], globals(), locals(), [components[1]])\n        return getattr(mod, components[1])"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # first try importing the module.\n    try:\n        return pydoc.locate(name)\n    except ImportError as e:\n        # fallback to using __import__\n        # First find the module, then getattr() of it.\n        mod_name, attr_name = name.rsplit(\".\", 1)\n        mod = __import__(mod_name, globals(), locals(), [attr_name])\n        return getattr(mod, attr_name)"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return pydoc.locate(name)\n    except ImportError:\n        # fallback to a manual scan.\n        # Scan this global module.\n        obj = {}\n        for module_name in list(sys.modules.keys()):\n            module = sys.modules[module_name]\n            if not hasattr(module, \"__dict__\"):\n                continue\n            for module_attr in dir(module):\n                if not module_attr.startswith(\"__\"):\n                    obj[module_attr] = getattr(module, module_attr)\n\n        # Scan builtin modules.\n        obj.update(builtins.__dict__)\n\n        return obj[name]"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # first try importing the target module and getattr it.\n    # this is required for the builtin models, which are directly registered as strings.\n    try:\n        return pydoc.locate(name)\n    except ImportError:\n        # fallback method to import the target object from its module.\n        # this is required for the custom models, which are registered as strings.\n        module_name, class_name = name.rsplit(\".\", 1)\n        mod = __import__(module_name, fromlist=[class_name])\n        return getattr(mod, class_name)"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    if callable(name):\n        # name is already a callable\n        return name\n\n    if not isinstance(name, str):\n        # not a string, so can't be located\n        raise ImportError(f\"{name} is not a string.\")\n\n    try:\n        return pydoc.locate(name)  # first try import if there is no \".\" in name\n    except ImportError:\n        components = name.split(\".\")\n        mod = importlib.import_module(components[0])\n        obj = mod\n        for comp in components[1:]:\n            obj = getattr(obj, comp)\n        return obj"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return pydoc.locate(name)\n    except ImportError as e:\n        # fallback to using __import__\n        # NOTE: This is a hack that only works for callable objects that are\n        # defined in top-level modules, and where the class definition is not\n        # hidden behind a function call.\n        components = name.rsplit(\".\", 1)\n        if len(components) == 1:\n            raise e\n        mod, obj = components\n        mod = __import__(mod, fromlist=[obj])\n        return getattr(mod, obj)\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # First try importing the target module and getting the required attribute.\n    try:\n        return pydoc.locate(name)\n    except ImportError:\n        # Fallback method 1: Try to interpret `name` as a:\n        #  module.class_name (assuming class_name is a subclass of `cls`)\n        module_name, _, class_name = name.rpartition(\".\")\n        try:\n            return locate(f\"{module_name}.{class_name}\")\n        except ImportError:\n            # Fallback method 2: Try to interpret `name` as a:\n            #  module.submodule._impl.class (assuming _impl is a submodule\n            #  and class is a subclass of `cls`)\n            try:\n                module_name, _, submodule_name = module_name.rpartition(\".\")\n                return locate(f\"{module_name}.{submodule_name}.{class_name}\")\n            except ImportError:\n                raise ImportError(f\"Error loading '{name}'. No module named '{name}'.\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return pydoc.locate(name)\n    except ImportError:\n        # Fallback method: try to import the object from its module, then getattr from the module.\n        # This is useful for locating objects that are not defined in the module but are defined in\n        # the module's __init__.py file.\n        parts = name.split(\".\")\n        module_name = \".\".join(parts[:-1])\n        class_name = parts[-1]\n        try:\n            module = __import__(module_name, fromlist=[class_name])\n            return getattr(module, class_name)\n        except (ImportError, AttributeError):\n            raise ImportError(f\"Cannot locate object: {name}\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # first try importing the target module and getattr it.  If it's a class we\n    # can instantiate it directly\n    try:\n        return pydoc.locate(name)\n    except ImportError:\n        # separate the module name and class name\n        module_name, _, class_name = name.rpartition(\".\")\n\n        # load the module and get the class\n        try:\n            mod = __import__(module_name, fromlist=[class_name])\n            return getattr(mod, class_name)\n        except (ImportError, AttributeError):\n            # fallback method: try eval.  If it's a class we can instantiate it directly\n            try:\n                return eval(name)\n            except NameError:\n                raise ImportError(f\"cannot locate {name}\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # first try importing the target module and getattr it.\n    try:\n        # Split the module path and target name by last '.' and import the module.\n        module_name, attr_name = name.rsplit(\".\", 1)\n        mod = pydoc.locate(module_name)\n        assert mod is not None, f\"module {module_name} not found\"\n        # Get the target attribute from the module.\n        obj = getattr(mod, attr_name)\n    except (ImportError, AttributeError):\n        # If the import or getattr fails, fall back to using pydoc to locate the object.\n        # This allows for more flexibility in specifying the target object.\n        obj = pydoc.locate(name)\n\n    # If the object could not be located, raise an ImportError.\n    if obj is None:\n        raise ImportError(f\"cannot locate {name}\")\n    return obj"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return pydoc.locate(name)\n    except ImportError as e:\n        # Fallback to a manual scan.\n        try:\n            return _locate_with_import(name)\n        except ImportError as e2:\n            raise ImportError(f\"Cannot locate {name}\") from e2\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # first try importing the target module and getattr it.  If it's a class we\n    # can instantiate it directly\n    try:\n        # for backward compatibility reasons, we still allow the user to\n        # specify the class name instead of a module path + class name.\n        # A warning is raised in this case.\n        parts = name.split(\".\")\n        if len(parts) == 1:\n            return pydoc.locate(name)\n        obj = pydoc.locate(name)\n\n        if obj is None:\n            # raise ValueError(\"Unknown name: %s\" % name)\n            raise ImportError(f\"Cannot locate name: {name}\")\n        elif hasattr(obj, \"__name__\") and obj.__name__ == parts[-1]:\n            # this is a module\n            return obj\n        elif (\n            not hasattr(obj, \"__name__\")\n            and hasattr(obj, \"__class__\")\n            and obj.__class__.__name__ == parts[-1]\n        ):\n            # this is a class\n            return obj()\n        else:\n            return obj\n\n    except ImportError:\n        # the last part is not a class name, so we try to import the module\n        # and get the whole thing\n        try:\n            obj = pydoc.locate(\".\".join(parts[:-1]))\n            return getattr(obj, parts[-1])\n        except Exception:\n            raise ImportError(f\"Cannot locate name: {name}\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return pydoc.locate(name)\n    except ImportError:\n        # Fallback to a manual scan.\n        try:\n            return _locate_name(name)\n        except ImportError as e:\n            raise ImportError(\n                f\"Cannot locate '{name}'. Error: {e}. Fallback to manual scan.\"\n            )\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    if callable(name):\n        # name is already a callable\n        return name\n\n    try:\n        obj = pydoc.locate(name)\n        if obj is None:\n            raise ImportError(f\"Cannot locate object: {name}\")\n    except ImportError as e:\n        # fallback to a manual search\n        # e.g. for objects located in a submodule\n        obj = _manual_load(name)\n        if obj is None:\n            raise e\n\n    # make sure the located callable is the\n    # expected type (subclasses of BatchTask)\n    if not callable(obj):\n        raise TypeError(f\"Located {name} is not callable.\")\n\n    return obj"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        obj = pydoc.locate(name)\n    except ImportError:\n        obj = None\n\n    if obj is None:\n        # The following is a fallback method to locate the object. It assumes that the\n        # object can be located by importing its module and retrieving the object from\n        # the module's namespace dictionary. This is a last resort method that should\n        # only be used if the standard method fails.\n        parts = name.split(\".\")\n        assert len(parts) > 1, \"locate(name): name must be a string with at least one dot\"\n        module_name = \".\".join(parts[:-1])\n        obj_name = parts[-1]\n        module = __import__(module_name, fromlist=[obj_name])\n        try:\n            obj = getattr(module, obj_name)\n        except AttributeError:\n            raise ImportError(f\"Cannot locate object: {name}\")\n    return obj"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the module to an in-memory buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Load the module from the buffer\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buf = io.BytesIO()\n    torch.jit.save(module, buf)\n    buf.seek(0)\n    new_module = torch.jit.load(buf)\n    return new_module"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    return cc_pure(ids, scores, weights, top_k)\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    return cc_pure(ids, scores, weights, top_k)\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    return cc_pure(ids, scores, weights, top_k)\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    return cc_pure(ids, scores, weights, top_k)\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    return cc_pure(ids, scores, weights, top_k)\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    return cc_pure(ids, scores, weights, top_k)\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    return cc_pure(ids, scores, weights, top_k)\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    return cc_pure(ids, scores, weights, top_k)\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    return cc_pure(ids, scores, weights, top_k)\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    return cc_pure(ids, scores, weights, top_k)\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    return cc_pure(ids, scores, weights, top_k)\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    return cc_pure(ids, scores, weights, top_k)\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    return cc_pure(ids, scores, weights, top_k)\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    return cc_pure(ids, scores, weights, top_k)\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    return cc_pure(ids, scores, weights, top_k)\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    return cc_pure(ids, scores, weights, top_k)"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    return cc_pure(ids, scores, weights, top_k)"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    return cc_pure(ids, scores, weights, top_k)"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    return cc_pure(ids, scores, weights, top_k)"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    return cc_pure(ids, scores, weights, top_k)\n\n"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if x < 10:\n        return f\"{x:.2f}\"\n    if x < 100:\n        return f\"{x:.1f}\"\n    return f\"{x:.0f}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if x < 1e-3:\n        return f\"{x:.2e}\"\n    if x < 1:\n        return f\"{x:.2f}\"\n    if x < 10:\n        return f\"{x:.2f}\"\n    if x < 100:\n        return f\"{x:.1f}\"\n    return f\"{x:.0f}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if x < 100:\n        return f\"{x:.2f}\"\n    elif x < 1000:\n        return f\"{x:.1f}\"\n    else:\n        return f\"{x:.0f}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if x == 0:\n        return \"0\"\n    if x < 1:\n        return f\"{x:.3f}\"\n    if x < 10:\n        return f\"{x:.2f}\"\n    if x < 100:\n        return f\"{x:.1f}\"\n    return f\"{x:.0f}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if x == 0:\n        return \"0\"\n    if x < 1:\n        return f\"{x:.2f}\"\n    if x < 10:\n        return f\"{x:.1f}\"\n    return f\"{x:.0f}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if x == 0:\n        return \"0%\"\n    if x < 0.001:\n        return f\"{x:.2e}\"\n    if x < 1:\n        return f\"{x:.3f}\"\n    if x < 10:\n        return f\"{x:.2f}\"\n    if x < 100:\n        return f\"{x:.1f}\"\n    return f\"{x:.0f}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if x < 1e-3 or x > 1e3:\n        return f\"{x:.3e}\"\n    if x < 1:\n        return f\"{x:.3f}\"\n    return f\"{x:.3g}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if x < 1:\n        return f\"{x:.2f}%\"\n    elif x < 10:\n        return f\"{x:.1f}%\"\n    else:\n        return f\"{x:.0f}%\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if x >= 1000:\n        return f\"{x:.0f}\"\n    elif x >= 100:\n        return f\"{x:.1f}\"\n    elif x >= 10:\n        return f\"{x:.2f}\"\n    elif x >= 1:\n        return f\"{x:.3f}\"\n    elif x >= 0.1:\n        return f\"{x:.4f}\"\n    elif x >= 0.01:\n        return f\"{x:.5f}\"\n    elif x >= 0.001:\n        return f\"{x:.6f}\"\n    elif x >= 0.0001:\n        return f\"{x:.7f}\"\n    elif x >= 0.00001:\n        return f\"{x:.8f}\"\n    else:\n        return f\"{x:.9f}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if x == 0:\n        return \"0%\"\n    if abs(x) < 1e-3:\n        return f\"{x:.2e}\"\n    if abs(x) < 1:\n        return f\"{x:.2f}\"\n    if abs(x) < 100:\n        return f\"{x:.1f}\"\n    return f\"{x:.0f}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if x == 0:\n        return \"0\"\n    if x < 0.0001:\n        return f\"{x:.2e}\"\n    if x < 1:\n        return f\"{x:.4f}\"\n    if x < 100:\n        return f\"{x:.2f}\"\n    return f\"{x:.0f}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if abs(x) < 1:\n        return f\"{x:.2f}%\"\n    if abs(x) < 10:\n        return f\"{x:.1f}%\"\n    if abs(x) < 100:\n        return f\"{x:.0f}%\"\n    if abs(x) < 1000:\n        return f\"{x:.0f}%\"\n    if abs(x) < 10000:\n        return f\"{x:.1f}%\"\n    if abs(x) < 100000:\n        return f\"{x:.0f}%\"\n    return f\"{x:.1f}%\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if x < 1:\n        return f\"{x:.2f}%\"\n    if x < 100:\n        return f\"{x:.2f}\"\n    if x < 1000:\n        return f\"{x:.1f}\"\n    return f\"{x:.0f}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n\n    if percent:\n        x *= 100\n\n    if x < 1:\n        # 0.0001 -> 0.0001\n        # 0.001 -> 0.001\n        # 0.01 -> 0.01\n        # 0.1 -> 0.1\n        # 1 -> 1\n        # 10 -> 10\n        # 100 -> 100\n        # 1000 -> 1000\n        # 10000 -> 10000\n        # 100000 -> 100000\n        # 1000000 -> 1000000\n        # 10000000 -> 10000000\n        # 100000000 -> 100000000\n        # 1000000000 -> 1000000000\n        # 10000000000 -> 10000000000\n        # 100000000000 -> 100000000000\n        # 1000000000000 -> 1000000000000\n        # 10000000000000 -> 10000000000000\n        # 100000000000000 -> 100000000000000\n        # 1000000000000000 -> 1000000000000000\n        # 10000000000000000 -> 100000000"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if x == 0:\n        return \"0.00%\" if percent else \"0.00\"\n    if x < 1:\n        return f\"{x:.2f}%\" if percent else f\"{x:.2f}\"\n    if x < 10:\n        return f\"{x:.1f}%\" if percent else f\"{x:.1f}\"\n    if x < 100:\n        return f\"{x:.0f}%\" if percent else f\"{x:.0f}\"\n    if x < 1000:\n        return f\"{x:.0f}\"\n    if x < 10000:\n        return f\"{x:.1f}k\"\n    if x < 100000:\n        return f\"{x/1000:.0f}k\"\n    if x < 1000000:\n        return f\"{x/1000:.1f}k\"\n    if x < 10000000:\n        return f\"{x/1000000:.0f}M\"\n    return f\"{x/1000000:.1f}M\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n\n    if percent:\n        x *= 100\n\n    if x == 0:\n        return \"0%\"\n\n    if abs(x) >= 100:\n        return f\"{x:.0f}%\"\n\n    if abs(x) >= 10:\n        return f\"{x:.1f}%\"\n\n    if abs(x) >= 1:\n        return f\"{x:.2f}%\"\n\n    if abs(x) >= 0.1:\n        return f\"{x:.3f}%\"\n\n    if abs(x) >= 0.01:\n        return f\"{x:.4f}%\"\n\n    return f\"{x:.5f}%\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    if percent:\n        x *= 100\n    if x == 0:\n        return \"0%\" if percent else \"0\"\n    if x < 0.001:\n        return f\"{x:.2e}%\" if percent else f\"{x:.2e}\"\n    if x < 1:\n        return f\"{x:.2f}%\" if percent else f\"{x:.2f}\"\n    if x < 100:\n        return f\"{x:.1f}%\" if percent else f\"{x:.1f}\"\n    return f\"{x:.0f}%\" if percent else f\"{x:.0f}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if x == 0:\n        return \"0\"\n    if percent:\n        x *= 100\n    if x < 0.01:\n        return f\"{x:.2e}\"\n    if x < 10:\n        return f\"{x:.2f}\"\n    if x < 100:\n        return f\"{x:.1f}\"\n    if x < 1000:\n        return f\"{int(x)}\"\n    return f\"{x:.0e}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if x >= 1000000:\n        return f\"{x / 1000000:.2f}M\"\n    elif x >= 1000:\n        return f\"{x / 1000:.2f}K\"\n    elif x >= 100:\n        return f\"{x:.2f}\"\n    elif x >= 10:\n        return f\"{x:.3f}\"\n    elif x >= 0.001:\n        return f\"{x:.4f}\"\n    else:\n        return f\"{x:.6f}\""}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if x < 1e-10:\n        return \"0.0\"\n    if x < 1e-3:\n        return f\"{x:.2e}\"\n    if x < 1e-2:\n        return f\"{x:.3f}\"\n    if x < 1e-1:\n        return f\"{x:.2f}\"\n    if x < 1:\n        return f\"{x:.1f}\"\n    if x < 1e2:\n        return f\"{x:.0f}\"\n    if x < 1e3:\n        return f\"{x:.1f}K\"\n    if x < 1e6:\n        return f\"{x / 1e3:.0f}K\"\n    if x < 1e9:\n        return f\"{x / 1e6:.0f}M\"\n    if x < 1e12:\n        return f\"{x / 1e9:.0f}B\"\n    if x < 1e15:\n        return f\"{x / 1e12:.0f}T\"\n    if x < 1e18:\n        return f\"{x / 1e15:.0f}Q\"\n    if x < 1e21:\n        return f\"{x / 1e18:.0f}Qd\"\n    if x < 1e24:\n        return f\"{x / 1e21:.0f}Sx\"\n    if x < 1e27:\n        return f\"{x / 1e24:.0f}Sp\"\n    if x < 1e30:\n        return f\"{x / 1e27:.0f}O\"\n    if x < 1e33:\n        return f\"{x / 1e30:.0f}N\"\n    if x < 1e36:\n        return f\"{x / 1e33:.0f}D\"\n    if x < 1e39:\n        return f\"{x / 1e36:."}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        disk_usage = shutil.disk_usage(input_dir)\n        free_space_in_gb = disk_usage.free / (1024 ** 3)\n        if free_space_in_gb < threshold_in_gb:\n            sleep(sleep_time)\n        else:\n            break"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        disk_usage = shutil.disk_usage(input_dir)\n        free_space_in_gb = disk_usage.free / (1024 ** 3)\n        if free_space_in_gb < threshold_in_gb:\n            sleep(sleep_time)\n        else:\n            break"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        disk_usage = shutil.disk_usage(input_dir)\n        free_space_in_gb = disk_usage.free / (1024 ** 3)\n        if free_space_in_gb < threshold_in_gb:\n            sleep(sleep_time)\n        else:\n            break"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        disk_usage = shutil.disk_usage(input_dir)\n        free_space_in_gb = disk_usage.free / (1024 ** 3)\n        if free_space_in_gb < threshold_in_gb:\n            sleep(sleep_time)\n        else:\n            break"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        disk_usage = shutil.disk_usage(input_dir)\n        free_space_in_gb = disk_usage.free / (1024 ** 3)\n        if free_space_in_gb < threshold_in_gb:\n            sleep(sleep_time)\n        else:\n            return"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        disk_usage = shutil.disk_usage(input_dir)\n        free_space_in_gb = disk_usage.free / (1024**3)\n\n        if free_space_in_gb < threshold_in_gb:\n            sleep(sleep_time)\n        else:\n            break"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        disk_usage = shutil.disk_usage(input_dir)\n        free_space = disk_usage.free / (1024 ** 3)\n\n        if free_space < threshold_in_gb:\n            sleep(sleep_time)\n        else:\n            break\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        # Get the disk usage of the specified directory\n        disk_usage = shutil.disk_usage(input_dir)\n\n        # Check if the free space is lower than the specified threshold\n        if disk_usage.free < threshold_in_gb * 1024 * 1024 * 1024:\n            # Sleep for the specified time before checking again\n            sleep(sleep_time)\n        else:\n            # Exit the loop if the free space is higher than the threshold\n            break"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        disk_usage = shutil.disk_usage(input_dir)\n        free_space = disk_usage.free / (1024 ** 3)\n        if free_space < threshold_in_gb:\n            sleep(sleep_time)\n        else:\n            break\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        disk_usage = shutil.disk_usage(input_dir)\n        free_space_in_gb = disk_usage.free / (1024 ** 3)\n        if free_space_in_gb < threshold_in_gb:\n            break\n        sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        disk_usage = shutil.disk_usage(\"/\")\n        free_space_in_gb = disk_usage.free / (1024 ** 3)\n        if free_space_in_gb < threshold_in_gb:\n            print(f\"Free space is lower than {threshold_in_gb} GB. Waiting for {sleep_time} seconds...\")\n            sleep(sleep_time)\n        else:\n            break"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        disk_usage = shutil.disk_usage(input_dir)\n        free_space_in_gb = disk_usage.free / (1024 ** 3)\n        if free_space_in_gb < threshold_in_gb:\n            print(f\"Disk usage is {free_space_in_gb:.1f} GB. Waiting for more space to be available...\")\n            sleep(sleep_time)\n        else:\n            print(f\"Disk usage is {free_space_in_gb:.1f} GB. Enough space is available.\")\n            break"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    threshold_in_bytes = threshold_in_gb * 1024 * 1024 * 1024\n    while True:\n        disk_usage = shutil.disk_usage(input_dir)\n        free_space = disk_usage.free\n        if free_space < threshold_in_bytes:\n            break\n        sleep(sleep_time)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        # Get the disk usage of the specified directory\n        disk_usage = shutil.disk_usage(input_dir)\n\n        # Calculate the free disk space in gigabytes\n        free_space = disk_usage.free / (1024 ** 3)\n\n        # Check if the free space is lower than the threshold\n        if free_space < threshold_in_gb:\n            # Wait for a specified amount of time before checking again\n            sleep(sleep_time)\n        else:\n            # The free space is higher than the threshold, so exit the loop\n            break"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    # Initialize variables\n    threshold_in_bytes = threshold_in_gb * 1024 * 1024 * 1024\n    disk_usage = shutil.disk_usage(input_dir)\n    free_space = disk_usage.free\n\n    # Loop until the free space is lower than the threshold\n    while free_space > threshold_in_bytes:\n        # Sleep for the specified time\n        time.sleep(sleep_time)\n\n        # Check the disk usage again\n        disk_usage = shutil.disk_usage(input_dir)\n        free_space = disk_usage.free\n\n    print(f\"Disk usage is now below {threshold_in_gb} GB. Continuing...\")"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        # 1. Get the disk usage of the input directory\n        disk_usage = shutil.disk_usage(input_dir)\n\n        # 2. Check if the free space is lower than the threshold\n        if disk_usage.free / (1024 ** 3) < threshold_in_gb:\n            # 3. If it is, wait for the specified amount of time\n            sleep(sleep_time)\n        else:\n            # 4. If it isn't, exit the loop\n            break"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        # Get the disk usage of the specified directory\n        disk_usage = shutil.disk_usage(input_dir)\n\n        # Check if the free disk space is lower than the threshold\n        if disk_usage.free / (1024**3) < threshold_in_gb:\n            # If the free disk space is lower than the threshold, wait for the specified amount of time before checking again\n            sleep(sleep_time)\n        else:\n            # If the free disk space is higher than the threshold, break out of the loop\n            break\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        try:\n            # Get the disk usage of the specified directory\n            disk_usage = shutil.disk_usage(input_dir)\n\n            # Calculate the free space in gigabytes\n            free_space_gb = disk_usage.free / (1024**3)\n\n            # Check if the free space is lower than the threshold\n            if free_space_gb < threshold_in_gb:\n                # If the free space is lower than the threshold, wait for the specified time before checking again\n                time.sleep(sleep_time)\n            else:\n                # If the free space is higher than the threshold, exit the loop\n                break\n        except Exception as e:\n            # If an error occurs, print the error message and wait for the specified time before checking again\n            print(f\"Error while checking disk usage: {e}\")\n            time.sleep(sleep_time)"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        # Get the disk usage of the specified directory\n        disk_usage = shutil.disk_usage(input_dir)\n\n        # Calculate the free disk space in gigabytes\n        free_space = disk_usage.free / (1024 ** 3)\n\n        # Check if the free disk space is lower than the threshold\n        if free_space < threshold_in_gb:\n            # If the free disk space is lower than the threshold, wait for the specified time before checking again\n            time.sleep(sleep_time)\n        else:\n            # If the free disk space is higher than the threshold, break the loop\n            break\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        # Get the current disk usage in gigabytes\n        total, used, free = shutil.disk_usage(input_dir)\n        free_gb = free // (2**30)\n\n        # Check if the free space is lower than the specified threshold\n        if free_gb < threshold_in_gb:\n            # If the free space is lower than the threshold, print a message and wait for the specified time before checking again\n            print(f\"Free disk space is lower than {threshold_in_gb} GB. Current free space: {free_gb} GB\")\n            sleep(sleep_time)\n        else:\n            # If the free space is higher than the threshold, exit the loop\n            break"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, p * td)"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, p * td)"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return math.safe_div(p, td)"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return math.safe_div(p, td)"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  w = jnp.where(td < np.finfo(np.float32).tiny, 0, p * td)\n  return w"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, p * td)"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return math.safe_div(p * td, jnp.sum(p * td, axis=-1, keepdims=True))"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  w = math.safe_div(p, td)\n  w = jnp.where(td < np.finfo(np.float32).tiny, 0, w)\n  return w"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, p * td)\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  w = p * td\n  return w\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, p * td)\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  w = math.safe_div(p, td)\n  w = jnp.where(td < np.finfo(np.float32).tiny, 0, w)\n  return w\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, p * td)\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  w = jnp.where(td < np.finfo(np.float32).tiny, 0, p * td)\n  return w\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return math.safe_div(p * td, jnp.sum(p * td, axis=-1, keepdims=True))\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, w)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, w * td)\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return math.safe_div(p, td)\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, p * td)\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  w = jnp.where(td < np.finfo(np.float32).tiny, 0, p * td)\n  return w\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return math.safe_div(p, td)\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\" \", \"\")\n    line_text = line_text.replace(\"\\t\", \"\")\n    line_text = line_text.replace(\"\\n\", \"\")\n    line_text = line_text.replace(\"\\r\", \"\")\n    line_text = line_text.replace(\"\\f\", \"\")\n    line_text = line_text.replace(\"\\v\", \"\")\n    line_text = line_text.replace(\"\\u200b\", \"\")\n    line_text = line_text.replace(\"\\u200c\", \"\")\n    line_text = line_text.replace(\"\\u200d\", \"\")\n    line_text = line_text.replace(\"\\u200e\", \"\")\n    line_text = line_text.replace(\"\\u200f\", \"\")\n    line_text = line_text.replace(\"\\u202a\", \"\")\n    line_text = line_text.replace(\"\\u202b\", \"\")\n    line_text = line_text.replace(\"\\u202c\", \"\")\n    line_text = line_text.replace(\"\\u202d\", \"\")\n    line_text = line_text.replace(\"\\u202e\", \"\")\n    line_text = line_text.replace(\"\\u2060\", \"\")\n    line_text = line_text.replace(\"\\u2061\", \"\")\n    line_text = line_text.replace(\"\\u2062\", \"\")\n    line_text = line_text.replace(\"\\u2063\", \"\")\n    line_text = line_text.replace(\"\\u2064\", \"\")\n    line_text = line_text.replace(\"\\u2066\", \"\")\n    line_text = line_text.replace(\"\\u2067\", \"\")\n    line_text = line_text.replace(\"\\u2068\", \"\")\n    line_text = line_text.replace(\"\\u2069\", \"\")\n    line_text = line_text.replace(\"\\u2028\", \"\")\n    line_"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    modified_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = list(modified_text)\n\n    # Return the list of tokens\n    return tokens"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\" \", \"\")\n    line_text = line_text.replace(\"\\t\", \"\")\n    line_text = line_text.replace(\"\\n\", \"\")\n    line_text = line_text.replace(\"\\r\", \"\")\n    line_text = line_text.replace(\"\\f\", \"\")\n    line_text = line_text.replace(\"\\v\", \"\")\n    line_text = line_text.replace(\"\\u00a0\", \"\")\n    line_text = line_text.replace(\"\\u2000\", \"\")\n    line_text = line_text.replace(\"\\u2001\", \"\")\n    line_text = line_text.replace(\"\\u2002\", \"\")\n    line_text = line_text.replace(\"\\u2003\", \"\")\n    line_text = line_text.replace(\"\\u2004\", \"\")\n    line_text = line_text.replace(\"\\u2005\", \"\")\n    line_text = line_text.replace(\"\\u2006\", \"\")\n    line_text = line_text.replace(\"\\u2007\", \"\")\n    line_text = line_text.replace(\"\\u2008\", \"\")\n    line_text = line_text.replace(\"\\u2009\", \"\")\n    line_text = line_text.replace(\"\\u200a\", \"\")\n    line_text = line_text.replace(\"\\u200b\", \"\")\n    line_text = line_text.replace(\"\\u200c\", \"\")\n    line_text = line_text.replace(\"\\u200d\", \"\")\n    line_text = line_text.replace(\"\\u200e\", \"\")\n    line_text = line_text.replace(\"\\u200f\", \"\")\n    line_text = line_text.replace(\"\\u2028\", \"\")\n    line_text = line_text.replace(\"\\u2029\", \"\")\n    line_text = line_text.replace(\"\\u202f\", \"\")\n    line_text"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = \"\".join(line_text.split())\n    line_text = su.segment(line_text)\n    return line_text"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    return line_text"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    return line_text"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\" \", \"\")\n    line_text = line_text.replace(\"\\t\", \"\")\n    line_text = line_text.replace(\"\\n\", \"\")\n    line_text = line_text.replace(\"\\r\", \"\")\n    line_text = line_text.replace(\"\\f\", \"\")\n    line_text = line_text.replace(\"\\v\", \"\")\n    line_text = line_text.replace(\"\\u00A0\", \"\")\n    line_text = line_text.replace(\"\\u2000\", \"\")\n    line_text = line_text.replace(\"\\u2001\", \"\")\n    line_text = line_text.replace(\"\\u2002\", \"\")\n    line_text = line_text.replace(\"\\u2003\", \"\")\n    line_text = line_text.replace(\"\\u2004\", \"\")\n    line_text = line_text.replace(\"\\u2005\", \"\")\n    line_text = line_text.replace(\"\\u2006\", \"\")\n    line_text = line_text.replace(\"\\u2007\", \"\")\n    line_text = line_text.replace(\"\\u2008\", \"\")\n    line_text = line_text.replace(\"\\u2009\", \"\")\n    line_text = line_text.replace(\"\\u200A\", \"\")\n    line_text = line_text.replace(\"\\u200B\", \"\")\n    line_text = line_text.replace(\"\\u200C\", \"\")\n    line_text = line_text.replace(\"\\u200D\", \"\")\n    line_text = line_text.replace(\"\\u200E\", \"\")\n    line_text = line_text.replace(\"\\u200F\", \"\")\n    line_text = line_text.replace(\"\\u2028\", \"\")\n    line_text = line_text.replace(\"\\u2029\", \"\")\n    line_text = line_text.replace(\"\\u202F\", \"\")\n    line_"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    modified_text = \"\".join(line_text.split())\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = list(modified_text)\n\n    # Return the segmented parts of the modified text\n    return tokens"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\" \", \"\")\n    return line_text"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\" \", \"\")\n    line_text = line_text.replace(\"\\n\", \"\")\n    line_text = line_text.replace(\"\\t\", \"\")\n    line_text = line_text.replace(\"\\r\", \"\")\n    line_text = line_text.replace(\"\\f\", \"\")\n    line_text = line_text.replace(\"\\v\", \"\")\n    line_text = line_text.replace(\"\\u00a0\", \"\")\n    line_text = line_text.replace(\"\\u2000\", \"\")\n    line_text = line_text.replace(\"\\u2001\", \"\")\n    line_text = line_text.replace(\"\\u2002\", \"\")\n    line_text = line_text.replace(\"\\u2003\", \"\")\n    line_text = line_text.replace(\"\\u2004\", \"\")\n    line_text = line_text.replace(\"\\u2005\", \"\")\n    line_text = line_text.replace(\"\\u2006\", \"\")\n    line_text = line_text.replace(\"\\u2007\", \"\")\n    line_text = line_text.replace(\"\\u2008\", \"\")\n    line_text = line_text.replace(\"\\u2009\", \"\")\n    line_text = line_text.replace(\"\\u200a\", \"\")\n    line_text = line_text.replace(\"\\u200b\", \"\")\n    line_text = line_text.replace(\"\\u200c\", \"\")\n    line_text = line_text.replace(\"\\u200d\", \"\")\n    line_text = line_text.replace(\"\\u200e\", \"\")\n    line_text = line_text.replace(\"\\u200f\", \"\")\n    line_text = line_text.replace(\"\\u2028\", \"\")\n    line_text = line_text.replace(\"\\u2029\", \"\")\n    line_text = line_text.replace(\"\\u202f\", \"\")\n    line_"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = \"\".join(line_text.split())\n    line_text = \" \".join(su.segment(line_text))\n    return line_text"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = su.segment(line_text)\n\n    # Return the segmented parts of the modified input text\n    return tokens"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    modified_text = \"\".join(line_text.split())\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = list(modified_text)\n\n    # Return the segmented parts or tokens\n    return tokens"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    modified_text = re.sub(r\"\\s+\", \"\", line_text)\n    # Segment the modified text into smaller parts or tokens\n    tokens = re.findall(r\"[A-Za-z]+|\\d+\", modified_text)\n    # Join the tokens back into a single string\n    result = \" \".join(tokens)\n    return result"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    modified_text = re.sub(r\"\\s+\", \"\", line_text)\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = []\n    current_token = \"\"\n    for char in modified_text:\n        if char.isupper() and current_token:\n            tokens.append(current_token)\n            current_token = char\n        else:\n            current_token += char\n    if current_token:\n        tokens.append(current_token)\n\n    # Join the tokens back into a single string\n    result = \" \".join(tokens)\n\n    return result"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    modified_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    segmented_text = su.segment(modified_text)\n\n    return segmented_text"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\" \", \"\")\n    line_text = line_text.replace(\"\\t\", \"\")\n    line_text = line_text.replace(\"\\n\", \"\")\n    line_text = line_text.replace(\"\\r\", \"\")\n    line_text = line_text.replace(\"\\x0c\", \"\")\n    line_text = line_text.replace(\"\\x0b\", \"\")\n    line_text = line_text.replace(\"\\x07\", \"\")\n    line_text = line_text.replace(\"\\x08\", \"\")\n    line_text = line_text.replace(\"\\f\", \"\")\n    line_text = line_text.replace(\"\\v\", \"\")\n    line_text = line_text.replace(\"\\a\", \"\")\n    line_text = line_text.replace(\"\\b\", \"\")\n    line_text = line_text.replace(\"\\f\", \"\")\n    line_text = line_text.replace(\"\\n\", \"\")\n    line_text = line_text.replace(\"\\r\", \"\")\n    line_text = line_text.replace(\"\\t\", \"\")\n    line_text = line_text.replace(\"\\v\", \"\")\n    line_text = line_text.replace(\"\\x0b\", \"\")\n    line_text = line_text.replace(\"\\x0c\", \"\")\n    line_text = line_text.replace(\"\\x1c\", \"\")\n    line_text = line_text.replace(\"\\x1d\", \"\")\n    line_text = line_text.replace(\"\\x1e\", \"\")\n    line_text = line_text.replace(\"\\x85\", \"\")\n    line_text = line_text.replace(\"\\u2028\", \"\")\n    line_text = line_text.replace(\"\\u2029\", \"\")\n    line_text = line_text.replace(\".\", \". \")\n    line_text = line_text.replace(\":\", \": \")\n    line_text = line_text.replace(\";\", \"; \")\n    line_text = line_text.replace(\"?\", \"? \")\n    line_text = line_"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\" \", \"\")\n    line_text = line_text.replace(\"-\", \"\")\n    line_text = line_text.replace(\"/\", \"\")\n    line_text = line_text.replace(\"\u2014\", \"\")\n    line_text = line_text.replace(\"\u2013\", \"\")\n    line_text = line_text.replace(\"_\", \"\")\n    line_text = line_text.replace(\"\u2022\", \"\")\n    line_text = line_text.replace(\"\u27a2\", \"\")\n    line_text = line_text.replace(\"\u0192\", \"\")\n    line_text = line_text.replace(\"\uf0b7\", \"\")\n    line_text = line_text.replace(\"\u2019\", \"\")\n    line_text = line_text.replace(\"'\", \"\")\n    line_text = line_text.replace('\"', \"\")\n    line_text = line_text.replace(\"(\", \"\")\n    line_text = line_text.replace(\")\", \"\")\n    line_text = line_text.replace(\"*\", \"\")\n    line_text = line_text.replace(\".\", \"\")\n    line_text = line_text.replace(\",\", \"\")\n    line_text = line_text.replace(\"?\", \"\")\n    line_text = line_text.replace(\"\u2022\", \"\")\n    line_text = line_text.replace(\"\u27a2\", \"\")\n    line_text = line_text.replace(\"\u0192\", \"\")\n    line_text = line_text.replace(\"\uf0b7\", \"\")\n    line_text = line_text.replace(\"\u2019\", \"\")\n    line_text = line_text.replace(\"'\", \"\")\n    line_text = line_text.replace('\"', \"\")\n    line_text = line_text.replace(\"(\", \"\")\n    line_text = line_text.replace(\")\", \"\")\n    line_text = line_text.replace(\"*\", \"\")\n    line_text = line_text.replace(\".\", \"\")\n    line_text = line_text.replace(\",\", \"\")"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # line_text = line_text.replace(\" \", \"\")\n    # line_text = line_text.replace(\"\\n\", \"\")\n    # line_text = line_text.replace(\"\\t\", \"\")\n    # line_text = line_text.strip()\n    # line_text = line_text.lower()\n    # line_text = line_text.translate(str.maketrans(\"\", \"\", \"\ufffd\\\\(*,.?\u2022\\\\\u27a2\u0192\uf0b7\u2013\\\\)'\\\"\u2014\"))\n    # line_text = line_text.translate(str.maketrans(\"\", \"\", \"\ufffd\\\\(*,.?\u2022\\\\\u27a2\u0192\uf0b7\u2013\\\\)'\\\"\u2014\"))\n    # line_text = patterns.num_unit.sub(r\"\", line_text)\n    # line_text = line_text.translate(str.maketrans(\"\", \"\", \"\ufffd\\\\(*,.?\u2022\\\\\u27a2\u0192\uf0b7\u2013\\\\)'\\\"\u2014\"))\n    # line_text = stem(line_text)\n    # words = line_text.split()\n    # return words"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\" \", \"\")\n    line_text = line_text.replace(\"\\n\", \"\")\n    line_text = line_text.replace(\"\\t\", \"\")\n    line_text = line_text.replace(\".\", \". \")\n    line_text = line_text.replace(\"(\", \" ( \")\n    line_text = line_text.replace(\")\", \" ) \")\n    line_text = line_text.replace(\",\", \", \")\n    line_text = line_text.replace(\":\", \": \")\n    line_text = line_text.replace(\";\", \"; \")\n    line_text = line_text.replace(\"?\", \"? \")\n    line_text = line_text.replace(\"-\", \" - \")\n    line_text = line_text.replace(\"\u2014\", \" \u2014 \")\n    line_text = line_text.replace(\"\u2013\", \" \u2013 \")\n    line_text = line_text.replace(\"\u201c\", \" \u201c \")\n    line_text = line_text.replace(\"\u201d\", \" \u201d \")\n    line_text = line_text.replace(\"\u2018\", \" \u2018 \")\n    line_text = line_text.replace(\"\u2019\", \" \u2019 \")\n    line_text = line_text.replace(\"'\", \" ' \")\n    line_text = line_text.replace(\"'\", \" ' \")\n    line_text = line_text.replace(\"*\", \" * \")\n    line_text = line_text.replace(\"\u2022\", \" \u2022 \")\n    line_text = line_text.replace(\"\u27a2\", \" \u27a2 \")\n    line_text = line_text.replace(\"\u0192\", \" \u0192 \")\n    line_text = line_text.replace(\"\uf0b7\", \" \uf0b7 \")\n    line_text = line_text.replace(\"\u2013\", \" \u2013 \")\n    line_text = line_text.replace(\"\u2026\", \" \u2026 \")\n    line_text = line_text.replace(\"\u2014\", \" \u2014 \")\n    line_text = line_text.replace(\"\u2013\", \" \u2013 \")\n    line_text = line_text"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zero weights cannot exceed the total number of weights.\")\n    weights = np.random.dirichlet(np.ones(n - zeros))\n    if zeros > 0:\n        weights = np.concatenate((weights, np.zeros(zeros)))\n        np.random.shuffle(weights)\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    weights = np.random.dirichlet(np.ones(n))\n    if zeros > 0:\n        if zeros > n:\n            raise ValueError(\"The number of zero weights cannot exceed the total number of weights.\")\n        zero_indices = np.random.choice(np.arange(n), size=zeros, replace=False)\n        weights[zero_indices] = 0.0\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    weights = rand_weights_dirichlet(n)\n    if zeros > 0:\n        weights[np.random.choice(n, zeros, replace=False)] = 0\n    weights /= weights.sum()\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\n            \"The number of zero weights must be less than or equal to the total number of weights.\"\n        )\n    weights = np.random.dirichlet(np.ones(n - zeros))\n    if zeros > 0:\n        weights = np.insert(weights, np.random.choice(n, zeros, replace=False), 0)\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"Number of zero weights cannot exceed number of weights\")\n    weights = np.random.dirichlet(np.ones(n))\n    if zeros > 0:\n        zero_indices = np.random.choice(np.arange(n), size=zeros, replace=False)\n        weights[zero_indices] = 0\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    weights = np.random.dirichlet(np.ones(n - zeros))\n    if zeros > 0:\n        weights = np.insert(weights, np.random.choice(n, size=zeros, replace=False), 0)\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zeros must be less than or equal to the total number of weights\")\n\n    weights = np.random.dirichlet(np.ones(n - zeros))\n    weights = np.concatenate((weights, np.zeros(zeros)))\n    np.random.shuffle(weights)\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zero weights cannot exceed the total number of weights.\")\n    if zeros == 0:\n        weights = np.random.dirichlet(np.ones(n))\n    else:\n        weights = np.zeros(n)\n        weights[:-zeros] = np.random.dirichlet(np.ones(n - zeros))\n        np.random.shuffle(weights)\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    weights = np.zeros(n)\n    if zeros > n:\n        raise ValueError(\"The number of zeros must be less than or equal to n.\")\n    if zeros == n:\n        return weights\n    weights[: n - zeros] = rand_weights_dirichlet(n - zeros)\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zeros must not exceed the total number of weights.\")\n\n    # Generate random weights that sum to one\n    weights = rand_weights_dirichlet(n)\n\n    # Set the specified number of weights to zero\n    if zeros > 0:\n        zero_indices = np.random.choice(n, zeros, replace=False)\n        weights[zero_indices] = 0\n\n    # Normalize the weights to sum to one\n    weights /= np.sum(weights)\n\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(f\"The number of zero-weight elements ({zeros}) cannot exceed the total number of weights ({n}).\")\n    weights = np.random.dirichlet(np.ones(n - zeros))\n    if zeros > 0:\n        weights = np.insert(weights, np.random.choice(n, size=zeros, replace=False), 0)\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if n <= 0:\n        raise ValueError(\"n must be a positive integer\")\n    if zeros < 0 or zeros > n:\n        raise ValueError(\"zeros must be between 0 and n\")\n\n    weights = np.random.dirichlet(np.ones(n - zeros))\n    if zeros > 0:\n        weights = np.insert(weights, np.random.choice(np.arange(n), size=zeros, replace=False), 0)\n\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Generate random weights\n    weights = np.random.rand(n)\n    # Normalize the weights so that they sum up to 1\n    weights /= np.sum(weights)\n    # Set the specified number of weights to zero\n    if zeros > 0:\n        if zeros > n:\n            raise ValueError(\"The number of zero weights cannot exceed the total number of weights.\")\n        weights[np.random.choice(n, zeros, replace=False)] = 0\n    # Normalize the weights so that they sum up to 1 again\n    weights /= np.sum(weights)\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Generate an array of n random weights that sum up to one\n    weights = np.random.dirichlet(np.ones(n))\n\n    # Set a specified number of the weights to zero\n    if zeros > 0:\n        if zeros > n:\n            raise ValueError(\"The number of weights to set to zero must not exceed the total number of weights.\")\n        weights[np.random.choice(range(n), size=zeros, replace=False)] = 0\n\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Generate a random array of weights that sum up to 1\n    weights = np.random.dirichlet(np.ones(n))\n\n    # Set the specified number of weights to 0\n    if zeros > 0:\n        if zeros > n:\n            raise ValueError(\"The number of zero weights must not exceed the total number of weights.\")\n        weights[:zeros] = 0\n\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Initialize an array of zeros with the specified length\n    weights = np.zeros(n)\n\n    # Set the specified number of weights to zero\n    weights[:zeros] = 0\n\n    # Generate the remaining weights using a Dirichlet distribution\n    remaining_weights = np.random.dirichlet(np.ones(n - zeros))\n\n    # Assign the remaining weights to the array\n    weights[zeros:] = remaining_weights\n\n    # Normalize the weights so that they sum to one\n    weights /= np.sum(weights)\n\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zero-weight elements must not exceed n.\")\n\n    # Generate n random weights that sum up to one\n    weights = np.random.dirichlet(np.ones(n))\n\n    # Set the specified number of weights to zero\n    if zeros > 0:\n        zero_indices = np.random.choice(np.arange(n), size=zeros, replace=False)\n        weights[zero_indices] = 0\n\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Generate a numpy array of n random weights that sum up to one\n    weights = rand_weights_dirichlet(n)\n\n    # Set a specified number of the weights to zero\n    if zeros > 0:\n        if zeros > n:\n            raise ValueError(\"The number of zeros cannot exceed the number of weights.\")\n        zero_indices = np.random.choice(n, zeros, replace=False)\n        weights[zero_indices] = 0\n\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\n            f\"The number of zeros ({zeros}) must be less than the total number of weights ({n})\"\n        )\n\n    # Generate the weights for the non-zero elements\n    weights = np.random.dirichlet(np.ones(n - zeros))\n\n    # Add zeros to the weights\n    weights = np.concatenate((weights, np.zeros(zeros)))\n\n    # Shuffle the weights to ensure randomness\n    np.random.shuffle(weights)\n\n    return weights"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    # Generate a random vector of n-1 weights that sum up to 1\n    weights = np.random.dirichlet(np.ones(n - 1))\n\n    # Set the specified number of weights to zero\n    if zeros > 0:\n        if zeros > n:\n            raise ValueError(\n                \"The number of zero weights must not exceed the total number of weights.\"\n            )\n        indices = np.random.choice(n - 1, zeros, replace=False)\n        weights[indices] = 0\n\n    # Ensure that the sum of weights is equal to 1\n    weights = weights / np.sum(weights)\n\n    return weights"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop(\"module_type\")\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop(\"module_type\")\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop(\"module_type\")\n        module_param = deepcopy(module_dict)\n        return cls(module_type=module_type, module_param=module_param)"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = module_dict\n        return cls(module_type, module_param)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop(\"module_type\")\n        return cls(module_type, module_dict)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop(\"module_type\")\n        return cls(module_type, module_dict)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop(\"module_type\")\n        module_param = module_dict\n        return cls(module_type, module_param)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        return cls(module_type, module_dict)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        return cls(module_type, module_dict)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = module_dict\n        return cls(module_type, module_param)\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    crop_y = max(0, round((bbox[1] + bbox[3]) / 2 - crop_size[0] / 2))\n    crop_x = max(0, round((bbox[0] + bbox[2]) / 2 - crop_size[1] / 2))\n    crop_y = min(image_size[0] - crop_size[0], crop_y)\n    crop_x = min(image_size[1] - crop_size[1], crop_x)\n    return T.CropTransform(crop_x, crop_y, crop_size[1], crop_size[0])"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    crop_y = max(0, int(bbox[1] - crop_size[0] / 2))\n    crop_x = max(0, int(bbox[0] - crop_size[1] / 2))\n    crop_y2 = min(image_size[0], crop_y + crop_size[0])\n    crop_x2 = min(image_size[1], crop_x + crop_size[1])\n    crop_y = min(image_size[0] - crop_size[0], crop_y)\n    crop_x = min(image_size[1] - crop_size[1], crop_x)\n    return T.CropTransform(crop_y, crop_x, crop_y2, crop_x2)"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    crop_y = (bbox[1] + bbox[3]) // 2 - crop_size[0] // 2\n    crop_x = (bbox[0] + bbox[2]) // 2 - crop_size[1] // 2\n    crop_y = max(0, min(image_size[0] - crop_size[0], crop_y))\n    crop_x = max(0, min(image_size[1] - crop_size[1], crop_x))\n    return T.CropTransform(crop_y, crop_x, crop_size[0], crop_size[1])"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the bounding box of the instance\n    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n\n    # Calculate the center of the bounding box\n    center_x = (bbox[0] + bbox[2]) / 2\n    center_y = (bbox[1] + bbox[3]) / 2\n\n    # Calculate the top-left corner of the cropping region\n    crop_x = max(0, center_x - crop_size[1] / 2)\n    crop_y = max(0, center_y - crop_size[0] / 2)\n\n    # Adjust the top-left corner if it goes beyond the image boundaries\n    crop_x = min(image_size[1] - crop_size[1], crop_x)\n    crop_y = min(image_size[0] - crop_size[0], crop_y)\n\n    # Create the CropTransform object\n    crop_transform = T.CropTransform(crop_x, crop_y, crop_size[1], crop_size[0])\n\n    return crop_transform"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the bounding box of the instance\n    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n\n    # Calculate the center of the bounding box\n    center_y = 0.5 * (bbox[1] + bbox[3])\n    center_x = 0.5 * (bbox[0] + bbox[2])\n\n    # Calculate the size of the crop region\n    crop_height, crop_width = crop_size\n\n    # Adjust the crop region to fit within the image boundaries\n    crop_y1 = max(0, int(center_y - crop_height / 2))\n    crop_y2 = min(image_size[0], int(center_y + crop_height / 2))\n    crop_x1 = max(0, int(center_x - crop_width / 2))\n    crop_x2 = min(image_size[1], int(center_x + crop_width / 2))\n\n    # Create the CropTransform object\n    crop_transform = T.CropTransform(crop_y1, crop_x1, crop_y2, crop_x2)\n\n    return crop_transform"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the bounding box of the instance\n    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n\n    # Calculate the center of the bounding box\n    center = (bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2\n\n    # Calculate the crop region\n    crop_x1 = max(0, int(center[0] - crop_size[1] / 2))\n    crop_y1 = max(0, int(center[1] - crop_size[0] / 2))\n    crop_x2 = min(image_size[1], int(center[0] + crop_size[1] / 2))\n    crop_y2 = min(image_size[0], int(center[1] + crop_size[0] / 2))\n\n    # Adjust the crop region if it goes beyond the image boundaries\n    if crop_x2 - crop_x1 < crop_size[1]:\n        crop_x2 = min(image_size[1], crop_x1 + crop_size[1])\n    if crop_y2 - crop_y1 < crop_size[0]:\n        crop_y2 = min(image_size[0], crop_y1 + crop_size[0])\n\n    # Create the CropTransform object\n    crop_transform = T.CropTransform(crop_x1, crop_y1, crop_x2, crop_y2)\n\n    return crop_transform"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the bounding box of the instance\n    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n\n    # Get the center of the bounding box\n    center_x = (bbox[0] + bbox[2]) / 2\n    center_y = (bbox[1] + bbox[3]) / 2\n\n    # Calculate the top-left corner of the crop region\n    crop_x1 = max(0, center_x - crop_size[1] / 2)\n    crop_y1 = max(0, center_y - crop_size[0] / 2)\n\n    # Calculate the bottom-right corner of the crop region\n    crop_x2 = min(image_size[1], center_x + crop_size[1] / 2)\n    crop_y2 = min(image_size[0], center_y + crop_size[0] / 2)\n\n    # Adjust the crop region if it exceeds the image boundaries\n    if crop_x2 - crop_x1 < crop_size[1]:\n        crop_x1 = max(0, crop_x2 - crop_size[1])\n    if crop_y2 - crop_y1 < crop_size[0]:\n        crop_y1 = max(0, crop_y2 - crop_size[0])\n\n    # Create a CropTransform object with the calculated parameters\n    crop_transform = T.CropTransform(crop_x1, crop_y1, crop_x2, crop_y2)\n\n    return crop_transform"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    crop_y = max(bbox[1] - crop_size[0] // 2, 0)\n    crop_x = max(bbox[0] - crop_size[1] // 2, 0)\n    crop_y2 = min(crop_y + crop_size[0], image_size[0])\n    crop_x2 = min(crop_x + crop_size[1], image_size[1])\n    crop_y = max(crop_y2 - crop_size[0], 0)\n    crop_x = max(crop_x2 - crop_size[1], 0)\n    return T.CropTransform(crop_x, crop_y, crop_x2, crop_y2)"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    crop_size = np.asarray(crop_size, dtype=np.int32)\n    start_y = max(bbox[1] - crop_size[0] // 2, 0)\n    start_x = max(bbox[0] - crop_size[1] // 2, 0)\n    box_height = bbox[3] - bbox[1]\n    box_width = bbox[2] - bbox[0]\n    crop_y = min(box_height, crop_size[0])\n    crop_x = min(box_width, crop_size[1])\n    crop_y = min(crop_size[0], image_size[0] - start_y)\n    crop_x = min(crop_size[1], image_size[1] - start_x)\n\n    return T.CropTransform(start_x, start_y, crop_x, crop_y)"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Convert instance to a BoxMode.XYXY_ABS format\n    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n\n    # Calculate the center of the bounding box\n    center = (bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2\n\n    # Calculate the top-left corner of the crop region\n    crop_top = max(0, int(center[1] - crop_size[0] / 2))\n    crop_left = max(0, int(center[0] - crop_size[1] / 2))\n\n    # Adjust the top-left corner if it goes beyond the image boundaries\n    if crop_top + crop_size[0] > image_size[0]:\n        crop_top = image_size[0] - crop_size[0]\n    if crop_left + crop_size[1] > image_size[1]:\n        crop_left = image_size[1] - crop_size[1]\n\n    # Create the CropTransform object\n    crop_tfm = T.CropTransform(crop_top, crop_left, crop_size[0], crop_size[1])\n\n    return crop_tfm"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the bounding box of the instance\n    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n\n    # Calculate the center of the bounding box\n    center_x = (bbox[0] + bbox[2]) / 2\n    center_y = (bbox[1] + bbox[3]) / 2\n\n    # Calculate the crop region based on the center and the desired crop size\n    crop_x1 = max(0, center_x - crop_size[1] / 2)\n    crop_y1 = max(0, center_y - crop_size[0] / 2)\n    crop_x2 = min(image_size[1], center_x + crop_size[1] / 2)\n    crop_y2 = min(image_size[0], center_y + crop_size[0] / 2)\n\n    # Adjust the crop region if it exceeds the image boundaries\n    if crop_x1 < 0:\n        crop_x2 -= crop_x1\n        crop_x1 = 0\n    if crop_y1 < 0:\n        crop_y2 -= crop_y1\n        crop_y1 = 0\n    if crop_x2 > image_size[1]:\n        crop_x1 -= crop_x2 - image_size[1]\n        crop_x2 = image_size[1]\n    if crop_y2 > image_size[0]:\n        crop_y1 -= crop_y2 - image_size[0]\n        crop_y2 = image_size[0]\n\n    # Create a CropTransform object with the calculated crop region\n    crop_transform = T.CropTransform(\n        crop_x1, crop_y1, crop_x2, crop_y2, image_size[1], image_size[0]\n    )\n\n    return crop_transform"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    crop_x0 = max(bbox[0] - crop_size[1] / 2, 0)\n    crop_y0 = max(bbox[1] - crop_size[0] / 2, 0)\n    crop_x1 = min(bbox[2] + crop_size[1] / 2, image_size[1])\n    crop_y1 = min(bbox[3] + crop_size[0] / 2, image_size[0])\n    crop_x0 = min(crop_x1 - crop_size[1], crop_x0)\n    crop_y0 = min(crop_y1 - crop_size[0], crop_y0)\n    crop_x0 = int(crop_x0)\n    crop_y0 = int(crop_y0)\n    crop_x1 = int(crop_x1)\n    crop_y1 = int(crop_y1)\n\n    return T.CropTransform(crop_x0, crop_y0, crop_x1, crop_y1)"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the bounding box of the instance\n    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n\n    # Calculate the center of the bounding box\n    center_y = 0.5 * (bbox[1] + bbox[3])\n    center_x = 0.5 * (bbox[0] + bbox[2])\n\n    # Calculate the size of the crop region\n    crop_height, crop_width = crop_size\n\n    # Calculate the top-left corner of the crop region\n    top = center_y - 0.5 * crop_height\n    left = center_x - 0.5 * crop_width\n\n    # Adjust the top-left corner if it goes out of the image boundaries\n    if top < 0:\n        top = 0\n    if left < 0:\n        left = 0\n\n    # Adjust the bottom-right corner if it goes out of the image boundaries\n    bottom = top + crop_height\n    right = left + crop_width\n    if bottom > image_size[0]:\n        bottom = image_size[0]\n        top = bottom - crop_height\n    if right > image_size[1]:\n        right = image_size[1]\n        left = right - crop_width\n\n    # Create a CropTransform object with the calculated crop region\n    crop = T.CropTransform(top, left, bottom, right)\n\n    return crop"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    crop_size = np.asarray(crop_size, dtype=np.int32)\n    crop_size = (crop_size[1], crop_size[0])  # (h, w)\n    image_size = np.asarray(image_size, dtype=np.int32)\n    image_size = (image_size[1], image_size[0])  # (h, w)\n\n    # Compute the center of the bounding box\n    center = (bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2\n\n    # Determine the cropping region\n    crop_y = int(np.clip(center[1] - crop_size[0] / 2, 0, image_size[0] - crop_size[0]))\n    crop_x = int(np.clip(center[0] - crop_size[1] / 2, 0, image_size[1] - crop_size[1]))\n\n    # Adjust the cropping region if it goes beyond the image boundaries\n    crop_y = max(crop_y, 0)\n    crop_x = max(crop_x, 0)\n    crop_y = min(crop_y, image_size[0] - crop_size[0])\n    crop_x = min(crop_x, image_size[1] - crop_size[1])\n\n    # Create the CropTransform object\n    crop_tfm = T.CropTransform(crop_x, crop_y, crop_size[1], crop_size[0])\n\n    return crop_tfm"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the bounding box of the instance\n    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n\n    # Calculate the center of the bounding box\n    center = (bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2\n\n    # Calculate the top-left corner of the crop\n    crop_x1 = max(0, center[0] - crop_size[0] / 2)\n    crop_y1 = max(0, center[1] - crop_size[1] / 2)\n\n    # Adjust the top-left corner if it is too close to the right or bottom edge of the image\n    if crop_x1 + crop_size[0] >= image_size[0]:\n        crop_x1 = image_size[0] - crop_size[0]\n    if crop_y1 + crop_size[1] >= image_size[1]:\n        crop_y1 = image_size[1] - crop_size[1]\n\n    # Create the CropTransform object\n    crop_transform = T.CropTransform(crop_x1, crop_y1, crop_size[0], crop_size[1])\n\n    return crop_transform"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    crop_x1 = max(0, min(bbox[0], bbox[2] - crop_size[1]))\n    crop_y1 = max(0, min(bbox[1], bbox[3] - crop_size[0]))\n    crop_x2 = max(crop_x1 + crop_size[1], min(bbox[2], image_size[1]))\n    crop_y2 = max(crop_y1 + crop_size[0], min(bbox[3], image_size[0]))\n    crop_x1 = max(0, min(crop_x2 - crop_size[1], crop_x1))\n    crop_y1 = max(0, min(crop_y2 - crop_size[0], crop_y1))\n    crop_x2 = min(image_size[1], crop_x1 + crop_size[1])\n    crop_y2 = min(image_size[0], crop_y1 + crop_size[0])\n    crop_w = max(crop_x2 - crop_x1, 1)\n    crop_h = max(crop_y2 - crop_y1, 1)\n    crop_x1 = round(crop_x1)\n    crop_y1 = round(crop_y1)\n    crop_x2 = round(crop_x2)\n    crop_y2 = round(crop_y2)\n    crop_size = (crop_h, crop_w)\n    crop_transform = T.CropTransform(crop_x1, crop_y1, crop_w, crop_h, image_size[1], image_size[0])\n    return crop_transform"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Convert the instance bounding box to a BoxMode.XYXY_ABS format\n    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n\n    # Calculate the center of the bounding box\n    center_x, center_y = (bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2\n\n    # Calculate the cropping region based on the desired crop size\n    crop_x, crop_y = center_x - crop_size[1] / 2, center_y - crop_size[0] / 2\n\n    # Adjust the cropping region if it goes out of the image boundaries\n    crop_x = max(0, crop_x)\n    crop_y = max(0, crop_y)\n    crop_x2 = min(image_size[1], crop_x + crop_size[1])\n    crop_y2 = min(image_size[0], crop_y + crop_size[0])\n\n    # Create a CropTransform object with the calculated cropping region\n    crop = T.CropTransform(crop_x, crop_y, crop_x2, crop_y2)\n\n    return crop"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    crop_size = np.asarray(crop_size, dtype=np.int32)\n    crop_centers = np.stack(\n        [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2], axis=0\n    )\n    # make sure the crop is within the image\n    bottom = crop_centers[1] - crop_size[1] / 2\n    top = crop_centers[1] + crop_size[1] / 2\n    left = crop_centers[0] - crop_size[0] / 2\n    right = crop_centers[0] + crop_size[0] / 2\n    if bottom < 0:\n        bottom = 0\n        top = crop_size[1]\n    if top > image_size[1]:\n        top = image_size[1]\n        bottom = image_size[1] - crop_size[1]\n    if left < 0:\n        left = 0\n        right = crop_size[0]\n    if right > image_size[0]:\n        right = image_size[0]\n        left = image_size[0] - crop_size[0]\n\n    crop_src = np.asarray([(left, bottom, left, top, right, top, right, bottom)])\n    crop_dst = np.tile(crop_centers, (4, 1))\n    crop_transform = T.CropTransform(*image_size, crop_src, crop_dst)\n\n    return crop_transform"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    center_y = 0.5 * (bbox[1] + bbox[3])\n    center_x = 0.5 * (bbox[0] + bbox[2])\n\n    # Check if the crop_size is larger than the image_size\n    if crop_size[0] > image_size[0] or crop_size[1] > image_size[1]:\n        raise ValueError(\"Crop size must be smaller than the image size.\")\n\n    # Adjust the crop_size to fit within the image boundaries\n    crop_size = list(crop_size)\n    if center_y - crop_size[0] * 0.5 < 0:\n        crop_size[0] = center_y\n    if center_x - crop_size[1] * 0.5 < 0:\n        crop_size[1] = center_x\n    if center_y + crop_size[0] * 0.5 > image_size[0]:\n        crop_size[0] = image_size[0] - center_y\n    if center_x + crop_size[1] * 0.5 > image_size[1]:\n        crop_size[1] = image_size[1] - center_x\n\n    # Adjust the crop_size to be even\n    crop_size = [int(round(x)) for x in crop_size]\n    if crop_size[0] % 2 != 0:\n        crop_size[0] += 1\n    if crop_size[1] % 2 != 0:\n        crop_size[1] += 1\n\n    # Calculate the top-left corner of the crop\n    crop_y1 = int(round(center_y - crop_size[0] * 0.5))\n    crop_x1 = int(round(center_x - crop_size[1] * 0.5))\n\n    return T.CropTransform(crop_x1, crop_y1, crop_size["}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Convert the instance's bounding box to XYXY format\n    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n\n    # Calculate the center of the bounding box\n    center_y = 0.5 * (bbox[0] + bbox[2])\n    center_x = 0.5 * (bbox[1] + bbox[3])\n\n    # Calculate the height and width of the bounding box\n    height = bbox[2] - bbox[0]\n    width = bbox[3] - bbox[1]\n\n    # Calculate the top-left corner of the crop region\n    crop_y = center_y - 0.5 * height\n    crop_x = center_x - 0.5 * width\n\n    # Adjust the top-left corner to ensure it stays within the image boundaries\n    crop_y = max(0, crop_y)\n    crop_x = max(0, crop_x)\n\n    # Calculate the height and width of the crop region\n    crop_h = min(height, image_size[0] - crop_y)\n    crop_w = min(width, image_size[1] - crop_x)\n\n    # Adjust the height and width to ensure it matches the desired crop size\n    crop_h = max(crop_h, crop_size[0])\n    crop_w = max(crop_w, crop_size[1])\n\n    # Adjust the top-left corner to ensure it stays within the image boundaries\n    crop_y = max(0, crop_y + crop_h - crop_size[0])\n    crop_x = max(0, crop_x + crop_w - crop_size[1])\n\n    # Create the CropTransform object with the calculated parameters\n    return T.CropTransform(crop_y, crop_x, crop_h, crop_w)"}
{"namespace": "ref_utils.l2_normalize", "completion": "  x_norm = jnp.maximum(jnp.linalg.norm(x, axis=-1, keepdims=True), grad_eps)\n  return x / x_norm, x_norm"}
{"namespace": "ref_utils.l2_normalize", "completion": "  x_sq_norm = jnp.maximum(jnp.sum(x**2, axis=-1, keepdims=True), grad_eps)\n  return x / jnp.sqrt(x_sq_norm)"}
{"namespace": "ref_utils.l2_normalize", "completion": "  x_norm = jnp.maximum(jnp.linalg.norm(x, axis=-1, keepdims=True), grad_eps)\n  x_norm = jnp.clip(x_norm, a_min=grad_eps)\n  return x / x_norm"}
{"namespace": "ref_utils.l2_normalize", "completion": "  return jnp.where(\n      jnp.linalg.norm(x, axis=-1, keepdims=True) > 0.0,\n      x / jnp.maximum(jnp.linalg.norm(x, axis=-1, keepdims=True), grad_eps),\n      jnp.zeros_like(x),\n  )"}
{"namespace": "ref_utils.l2_normalize", "completion": "  x_norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  x_norm = jnp.maximum(x_norm, grad_eps)\n  return x / x_norm"}
{"namespace": "ref_utils.l2_normalize", "completion": "  x_norm = jnp.maximum(jnp.linalg.norm(x, axis=-1, keepdims=True), grad_eps)\n  return x / x_norm, x_norm"}
{"namespace": "ref_utils.l2_normalize", "completion": "  x_norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  x_norm_clamped = jnp.maximum(x_norm, grad_eps)\n  x_norm_clamped_inv = jnp.reciprocal(x_norm_clamped)\n  return x * x_norm_clamped_inv"}
{"namespace": "ref_utils.l2_normalize", "completion": "  x = jnp.asarray(x)\n  return jnp.where(\n      jnp.linalg.norm(x, axis=-1, keepdims=True) > 0.0,\n      x / jnp.maximum(jnp.linalg.norm(x, axis=-1, keepdims=True), grad_eps),\n      x,\n  )"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Forward pass\n  x_norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  x_norm = jnp.clip(x_norm, grad_eps, None)\n  x_norm = x / x_norm\n\n  # Backward pass\n  x_norm_grad = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  x_norm_grad = jnp.clip(x_norm_grad, grad_eps, None)\n  x_norm_grad = x / x_norm_grad\n\n  return x_norm, x_norm_grad"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Clamp the denominator to a minimum value in the forward pass to prevent\n  # division by zero during the backward pass.\n  clamp_denom = jnp.maximum(jnp.finfo(jnp.float32).eps, jnp.sum(x ** 2, axis=-1, keepdims=True))\n  x_norm = x / jnp.sqrt(clamp_denom)\n\n  # Clamp the denominator to a minimum value in the backward pass to prevent\n  # division by zero during the backward pass.\n  clamp_denom = jnp.maximum(grad_eps, jnp.sum(x_norm ** 2, axis=-1, keepdims=True))\n  return x_norm / jnp.sqrt(clamp_denom)"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Compute the square of the norm of x along the last axis.\n  x_sq_norm = jnp.maximum(jnp.sum(x**2, axis=-1, keepdims=True), grad_eps)\n\n  # Compute the inverse of the norm of x along the last axis.\n  x_inv_norm = jnp.sqrt(x_sq_norm)\n\n  # Divide each element of x by the inverse of its norm along the last axis.\n  x_normalized = x / x_inv_norm\n\n  # Return the normalized array.\n  return x_normalized"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Forward pass: normalize the input array to unit length along its last axis.\n  x_norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  x_norm = jnp.maximum(x_norm, grad_eps)\n  x_norm = x / x_norm\n\n  # Backward pass: clamp the squared norm of the input array to a minimum value\n  # to prevent exploding gradients.\n  x_norm_sq = jnp.linalg.norm(x, axis=-1, keepdims=True) ** 2\n  x_norm_sq = jnp.maximum(x_norm_sq, grad_eps)\n  x_norm_sq = x / x_norm_sq\n\n  # Return the normalized array.\n  return x_norm_sq"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Clamp the squared norm to prevent division by zero and exploding gradients\n  x_sq_norm = jnp.maximum(jnp.sum(x ** 2, axis=-1, keepdims=True), grad_eps)\n\n  # Divide by the clamped norm to obtain the normalized vector\n  x_normed = x / jnp.sqrt(x_sq_norm)\n\n  # Clamp the norm to 1 to prevent the norm from exceeding 1 due to numerical errors\n  x_normed = jnp.clip(x_normed, a_min=-1.0, a_max=1.0)\n\n  # Return the normalized vector\n  return x_normed"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Forward pass\n  # Clamp the squared norm to a minimum value to prevent exploding gradients\n  x_norm_squared = jnp.maximum(jnp.sum(x**2, axis=-1, keepdims=True), grad_eps)\n  x_inv_norm = 1.0 / jnp.sqrt(x_norm_squared)\n  x_normalized = x * x_inv_norm\n\n  # Backward pass\n  # Clamp the squared norm to a minimum value to prevent exploding gradients\n  x_inv_norm_squared = jnp.maximum(jnp.sum(x_inv_norm**2, axis=-1, keepdims=True), grad_eps)\n  x_inv_norm_cubed = x_inv_norm_squared * x_inv_norm\n  x_normalized_grad = x_inv_norm_cubed * x\n\n  return x_normalized, x_normalized_grad"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Compute the squared norm of the input array along the last axis.\n  squared_norm = jnp.sum(x**2, axis=-1, keepdims=True)\n\n  # Compute the reciprocal of the squared norm, clamped to a minimum value to prevent division by zero in the backward pass.\n  reciprocal_squared_norm = jnp.reciprocal(jnp.maximum(squared_norm, grad_eps))\n\n  # Multiply the input array by the reciprocal of the squared norm to normalize it.\n  normalized_array = x * jnp.sqrt(reciprocal_squared_norm)\n\n  # Return the normalized array.\n  return normalized_array"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Clamp the denominator to a minimum value during the forward pass.\n  clamped_denom = jnp.maximum(jnp.sqrt(jnp.sum(x**2, axis=-1, keepdims=True)), grad_eps)\n  # Clamp the denominator to a maximum value during the backward pass.\n  clamped_denom_grad = jnp.minimum(clamped_denom, 1.0 / grad_eps)\n  # Divide the numerator by the clamped denominator to get the normalized vector.\n  x_normed = x / clamped_denom_grad\n  return x_normed"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Clamp the denominator to a minimum value to prevent division by zero.\n  denom = jnp.maximum(jnp.sqrt(jnp.sum(x ** 2, axis=-1, keepdims=True)), grad_eps)\n  # Divide the input array by the clamped denominator.\n  x = x / denom\n  # Clamp the norm of the resulting array to 1.\n  x = jnp.clip(x, -1.0, 1.0)\n  # Compute the norm of the clamped array.\n  norm = jnp.sqrt(jnp.sum(x ** 2, axis=-1, keepdims=True))\n  # Normalize the array by dividing it by its norm.\n  x = x / norm\n  return x"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Clamp the squared norm to a minimum value to prevent division by zero in the forward pass.\n  clamped_norm = jnp.maximum(jnp.linalg.norm(x, axis=-1, keepdims=True), grad_eps)\n  # Divide the input array by the clamped norm to obtain the normalized output.\n  normalized_x = x / clamped_norm\n  # Compute the gradients of the clamped norm using the chain rule.\n  grad_clamped_norm = jnp.where(clamped_norm > grad_eps, 1.0 / clamped_norm, 0.0)\n  # Compute the gradients of the normalized output using the chain rule.\n  grad_normalized_x = (jnp.eye(x.shape[-1]) - jnp.einsum('...i,...j->...ij', normalized_x, normalized_x)) / clamped_norm\n  # Compute the gradients of the input array using the chain rule.\n  grad_x = grad_normalized_x * grad_clamped_norm[..., None, None]\n  # Return the normalized output and the gradients of the input array.\n  return normalized_x, grad_x"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Clamp the squared norm to prevent division by zero in the backward pass.\n  # The clamping value is chosen as the default value of grad_eps in JAX.\n  x_2 = jnp.maximum(jnp.sum(x**2, axis=-1, keepdims=True), grad_eps)\n\n  # In the forward pass, we use the default behavior of JAX, which is to return\n  # NaNs if the input is zero.\n  return x / jnp.sqrt(x_2)"}
{"namespace": "ref_utils.l2_normalize", "completion": "  x_norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  x_norm_safe = jnp.maximum(x_norm, grad_eps)\n  x_norm_safe = jnp.minimum(x_norm_safe, 1.0 / grad_eps)\n  return x / x_norm_safe\n\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name, input_text = agent_info.split(\": \")\n        return agent_name, input_text"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split('Use Agent[')[1].split(']')[0]\n        agent_name, input_text = agent_info.split(':')\n        return agent_name, input_text"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name, *input_text = agent_info.split(\": \")\n        input_text = input_text[0] if input_text else \"\"\n        return agent_name, input_text"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name, *input_text = agent_info.split(\":\")\n        input_text = input_text[0] if input_text else \"\"\n        return agent_name, input_text"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name, agent_input = agent_info.split(\":\")\n        return agent_name, agent_input"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_name = response.split('Use Agent[')[1].split(']')[0]\n        input_text = response.split(':')[1].strip() if ':' in response else ''\n        return agent_name, input_text"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_name = response.split('Use Agent[')[1].split(']')[0]\n        input_text = response.split(':')[1].strip() if ':' in response else ''\n        return agent_name, input_text"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name, input_text = agent_info.split(\": \")\n        return agent_name, input_text\n\n    "}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_name = response.split('Use Agent[')[1].split(']')[0]\n        input_text = response.split(':')[1].strip() if ':' in response else ''\n        return agent_name, input_text"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name, agent_input_text = agent_info.split(\": \")\n        return agent_name, agent_input_text"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name, agent_input_text = agent_info.split(\": \")\n        return agent_name, agent_input_text"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name, agent_input = agent_info.split(\": \")\n        return agent_name, agent_input"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name, input_text = agent_info.split(\":\")\n        return agent_name, input_text.strip()"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name, input_text = agent_info.split(\": \") if \":\" in agent_info else (agent_info, \"\")\n        return agent_name, input_text"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name, input_text = agent_info.split(\":\")\n        return agent_name, input_text.strip()\n    "}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_name = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        input_text = response.split(\"Use Agent[\")[1].split(\"]\")[1].split(\":\")[1] if \":\" in response else \"\"\n        return agent_name, input_text\n    "}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name, *input_text = agent_info.split(\": \")\n        input_text = \": \".join(input_text)\n        return agent_name, input_text\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        start_index = response.find(\"Use Agent[\")\n        end_index = response.find(\"]\", start_index)\n        agent_info = response[start_index + 9:end_index]\n        agent_name, *input_text = agent_info.split(\": \")\n        input_text = input_text[0] if input_text else \"\"\n        return agent_name, input_text"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_name_start = response.find(\"Use Agent[\") + len(\"Use Agent[\")\n        agent_name_end = response.find(\"]\", agent_name_start)\n        agent_name = response[agent_name_start:agent_name_end]\n\n        input_text_start = agent_name_end + 1\n        if input_text_start < len(response) and response[input_text_start] == \":\":\n            input_text_start += 1\n        else:\n            input_text_start = len(response)\n\n        input_text = response[input_text_start:].strip()\n\n        return agent_name, input_text"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_name_start = response.find(\"Use Agent[\") + len(\"Use Agent[\")\n        agent_name_end = response.find(\"]\", agent_name_start)\n        agent_name = response[agent_name_start:agent_name_end]\n\n        input_text_start = agent_name_end + 2  # +2 to skip the colon and space after the agent name\n        input_text = response[input_text_start:]\n\n        return agent_name, input_text"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        if mask_format == \"polygon\":\n            masks = [PolygonMasks([rle]) for rle in annos[\"segmentation\"]]\n        elif mask_format == \"bitmask\":\n            masks = []\n            for polygons in annos[\"segmentation\"]:\n                rles = mask_util.frPyObjects(polygons, *image_size)\n                rle = mask_util.merge(rles)\n                masks.append(rle)\n        target.gt_masks = BitMasks(masks)\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = Keypoints(kpts)\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(polygons_to_bitmask(segm, *image_size))\n                elif isinstance(segm, dict):\n                    # COCO RLE\n                    masks.append(mask_util.decode(segm))\n                elif isinstance(segm, np.ndarray):\n                    assert segm.ndim == 2, \"Expects a binary mask with 'L' shape\"\n                    masks.append(segm)\n                else:\n                    raise ValueError(\n                        \"Cannot convert segmentation of type '{}' to a bitmap!\"\n                        \"Supported types are: polygons as list[list[float] or ndarray],\"\n                        \" COCO-style RLE as a dict.\".format(type(segm))\n                    )\n            # torch.from_numpy does not support array with negative stride.\n            masks = BitMasks(\n                torch.stack([torch.from_numpy(np.ascontiguousarray(x)) for x in masks])\n            )\n        target.gt_masks = masks\n\n    if len(annos) and \""}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    # TODO put BoxMode into semantic segmentation?\n    if \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(polygons_to_bitmask(segm, *image_size))\n                else:\n                    # COCO RLE\n                    masks.append(mask_util.decode(segm))\n            masks = BitMasks(masks)\n        target.gt_masks = masks\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = Keypoints(kpts)\n\n    target = target.remove(\"gt_sem_seg\")\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if not annos:\n        return target\n    segms = [obj[\"segmentation\"] for obj in annos]\n    if mask_format == \"polygon\":\n        masks = PolygonMasks(segms)\n    else:\n        assert mask_format == \"bitmask\", mask_format\n        masks = []\n        for segm in segms:\n            if isinstance(segm, list):\n                # polygon\n                masks.append(polygons_to_bitmask(segm, *image_size))\n            elif isinstance(segm, dict):\n                # COCO RLE\n                masks.append(mask_util.decode(segm))\n            elif isinstance(segm, np.ndarray):\n                assert segm.ndim == 2, \"Expects a binary mask with 'L' shape\"\n                masks.append(segm)\n            else:\n                raise ValueError(\n                    \"Cannot convert segmentation of type '{}' to BitMasks!\"\n                    \"Supported types are: polygons as list[list[float] or ndarray],\"\n                    \" COCO-style RLE as a dict.\".format(type(segm))\n                )\n        # torch.from_numpy does not support array with negative stride.\n        masks = BitMasks(\n            torch.stack([torch.from_numpy(np.ascontiguousarray(x)) for x in masks])\n        )\n    target.gt_masks = masks\n\n    if \"keypoints\" in annos[0]:\n        keyp"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    # TODO put back if useful\n    # boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(polygons_to_bitmask(segm, *image_size))\n                elif isinstance(segm, dict):\n                    # COCO RLE\n                    masks.append(mask_util.decode(segm))\n                elif isinstance(segm, np.ndarray):\n                    assert segm.ndim == 2, \"Expects a binary mask with 'L' layout.\"\n                    masks.append(segm)\n                else:\n                    raise ValueError(\n                        \"Cannot convert segmentation of type '{}' to a bitmap!\"\n                        \"Supported types are: polygons as list[list[float] or ndarray],\"\n                        \" COCO-style RLE as a dict.\".format(type(segm))\n                    )\n            # torch.from_numpy does not support array with negative stride.\n            masks = BitMasks(\n                torch.stack([torch.from_numpy(np.ascontiguousarray(x)) for x in masks])\n            )\n        target.gt_masks = masks\n\n   "}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(polygons_to_bitmask(segm, *image_size))\n                else:\n                    # COCO RLE\n                    masks.append(mask_util.decode(segm))\n            # take first to avoid empty list\n            if len(masks) == 0:\n                raise ValueError(\n                    \"No mask for image {}, possibly due to issues with loading the \"\n                    \"annotation data.\".format(annos[0][\"image_id\"])\n                )\n            masks = BitMasks(masks)\n        target.gt_masks = masks\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = Keypoints(kpts)\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if not annos and not keypoints:\n        return target\n\n    if \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(polygons_to_bitmask(segm, *image_size))\n                elif isinstance(segm, dict):\n                    # COCO RLE\n                    masks.append(mask_util.decode(segm))\n                elif isinstance(segm, np.ndarray):\n                    assert segm.ndim == 2, \"Expects a binary mask with 'L' shape\"\n                    masks.append(segm)\n                else:\n                    raise ValueError(\n                        \"Cannot convert segmentation of type '{}' to BitMasks!\"\n                        \"Supported types are: polygons as list[list[float] or ndarray],\"\n                        \" COCO-style RLE as a dict.\".format(type(segm))\n                    )\n            # TODO put the if/else above in a function\n            masks = BitMasks(masks)\n        target.gt_masks = masks\n\n    if \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in ann"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    target.gt_boxes = Boxes(boxes)\n    target.gt_boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        if mask_format == \"polygon\":\n            masks = [PolygonMasks([rle]) for rle in annos[\"segmentation\"]]\n        elif mask_format == \"bitmask\":\n            masks = []\n            for polygons in annos[\"segmentation\"]:\n                rles = mask_util.frPyObjects(polygons, *image_size)\n                rle = mask_util.merge(rles)\n                masks.append(rle)\n        target.gt_masks = BitMasks(masks)\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = Keypoints(kpts)\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    # TODO put BoxMode into semantic segmentation?\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        if mask_format == \"polygon\":\n            masks = [\n                PolygonMasks([PolygonMasks.Polygon(obj[\"segmentation\"])])\n                for obj in annos\n            ]\n        else:\n            masks = []\n            for obj in annos:\n                if isinstance(obj[\"segmentation\"], list):\n                    # polygon\n                    masks.append(polygons_to_bitmask(obj[\"segmentation\"], *image_size))\n                elif isinstance(obj[\"segmentation\"][\"counts\"], list):\n                    # uncompressed RLE\n                    masks.append(mask_util.decode(obj[\"segmentation\"]))\n                else:\n                    # rle\n                    masks.append(mask_util.decode(obj[\"segmentation\"]))\n            # take first because there seems to be only one\n            masks = BitMasks(torch.stack(masks, dim=0))\n        target.gt_masks = masks\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = Keypoints(kpts)\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if not annos and not keypoints:\n        return target\n\n    if \"segmentation\" in annos[0]:\n        if mask_format == \"polygon\":\n            masks = [PolygonMasks([PolygonMasks.polygon_to_mask(obj[\"segmentation\"])]) for obj in annos]\n        else:\n            masks = [BitMasks.from_polygon_masks(obj[\"segmentation\"], *image_size) for obj in annos]\n        target.gt_masks = BitMasks.cat(masks)\n\n    if \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = Keypoints(kpts)\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if not annos:\n        return target\n    segms = [obj[\"segmentation\"] for obj in annos]\n    if mask_format == \"polygon\":\n        masks = PolygonMasks(segms)\n    else:\n        assert mask_format == \"bitmask\", mask_format\n        masks = []\n        for segm in segms:\n            if isinstance(segm, list):\n                # polygon\n                masks.append(polygons_to_bitmask(segm, *image_size))\n            else:\n                # COCO RLE\n                assert isinstance(segm, dict)\n                masks.append(mask_util.decode(segm))\n        # take first to avoid empty list\n        if any(x is not None for x in masks):\n            masks = BitMasks(masks)\n        else:\n            masks = None\n    target.gt_masks = masks\n\n    if annos and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = Keypoints(kpts)\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(polygons_to_bitmask(segm, *image_size))\n                else:\n                    # COCO RLE\n                    masks.append(mask_util.decode(segm))\n            if len(masks) == 0:\n                # This is a workaround of some missing annotations\n                masks = BitMasks(torch.empty(0, *image_size))\n            else:\n                masks = BitMasks(torch.stack(masks, dim=0))\n        target.gt_masks = masks\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = Keypoints(kpts)\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if not annos and not keypoints:\n        return target\n\n    segmentations = [obj[\"segmentation\"] for obj in annos]\n    if mask_format == \"polygon\":\n        masks = []\n        for polygons in segmentations:\n            rles = mask_util.frPyObjects(polygons, *image_size)\n            mask = mask_util.decode(rles)\n            if len(mask.shape) < 3:\n                mask = mask[..., None]\n            mask = torch.as_tensor(mask, dtype=torch.uint8)\n            mask = mask.any(dim=2)\n            masks.append(mask)\n        target.gt_masks = BitMasks(masks)\n    elif mask_format == \"bitmask\":\n        masks = []\n        for rle in segmentations:\n            if isinstance(rle, list):\n                # polygon\n                masks.append(polygons_to_bitmask(rle, *image_size))\n            elif isinstance(rle, dict):\n                # COCO RLE\n                masks.append(mask_util.decode(rle))\n            elif rle is None:\n                # no segmentation\n                masks.append(np.zeros((image_size[0], image_size[1]), dtype=np.uint8))\n            else:\n                raise ValueError(\"Cannot handle {}\".format(type(rle)))\n        target.gt_masks = BitMasks(torch.stack(masks))\n    else:\n        raise ValueError(\"Unknown"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(polygons_to_bitmask(segm, *image_size))\n                elif isinstance(segm, dict):\n                    # COCO RLE\n                    masks.append(mask_util.decode(segm))\n                elif isinstance(segm, np.ndarray):\n                    assert segm.ndim == 3, f\"Expects 3-dimensional mask, got {segm.ndim}.\"\n                    # uncompressed RLE\n                    masks.append(mask_util.decode(mask_util.encode(np.asarray(segm, order=\"F\", dtype=\"uint8\"))))\n                else:\n                    raise ValueError(\n                        \"Cannot convert segmentation of type '{}' to BitMasks!\"\n                        \"Supported types are: polygons as list[list[float] or ndarray],\"\n                        \" COCO-style RLE as a dict.\".format(type(segm))\n                    )\n            # take first elem to avoid merging on CPU\n            masks = BitMasks.cat(masks)\n        target.gt_masks = masks\n\n    if len"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if not annos and not keypoint_hflip_indices:\n        return target\n    if \"segmentation\" in annos[0]:\n        if mask_format == \"polygon\":\n            masks = [\n                PolygonMasks([PolygonMasks.Polygon(obj[\"segmentation\"])])\n                for obj in annos\n            ]\n        elif mask_format == \"bitmask\":\n            masks = []\n            for obj in annos:\n                if isinstance(obj[\"segmentation\"], list):\n                    # polygon\n                    rles = mask_util.frPyObjects(obj[\"segmentation\"], *image_size)\n                    rle = mask_util.merge(rles)\n                elif isinstance(obj[\"segmentation\"][\"counts\"], list):\n                    # uncompressed RLE\n                    rle = mask_util.frPyObjects(obj[\"segmentation\"], *image_size)\n                else:\n                    # rle\n                    rle = obj[\"segmentation\"]\n                mask = mask_util.decode(rle)\n                mask = torch.as_tensor(mask, dtype=torch.uint8)\n                masks.append(mask)\n            target.gt_masks = BitMasks(masks)\n        else:\n            raise ValueError(\"Unknown mask format: {}\".format(mask_format))\n\n    if \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = Keypoints("}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    target.gt_boxes = Boxes(boxes)\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        if mask_format == \"polygon\":\n            masks = []\n            for obj in annos:\n                if isinstance(obj[\"segmentation\"], PolygonMasks):\n                    masks.append(obj[\"segmentation\"])\n                else:\n                    # PolygonMasks cannot handle non-list inputs\n                    masks.append(PolygonMasks([obj[\"segmentation\"]]))\n            target.gt_masks = PolygonMasks(masks)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            # use bitmasks for masks\n            masks = []\n            for obj in annos:\n                if isinstance(obj[\"segmentation\"], PolygonMasks):\n                    masks.append(obj[\"segmentation\"].polygons)\n                elif isinstance(obj[\"segmentation\"], list):\n                    # PolygonMasks cannot handle non-list inputs\n                    masks.append(obj[\"segmentation\"])\n                else:\n                    # bitmasks.py requires list of list of polygons\n                    masks.append([obj[\"segmentation\"]])\n            target.gt_masks = BitMasks.from_polygon_masks(masks, *image_size).tensor\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = Keypoints(kpts)\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        if mask_format == \"polygon\":\n            masks = [obj[\"segmentation\"] for obj in annos]\n            masks = PolygonMasks(masks)\n            target.gt_masks = masks\n        else:\n            # Polygons is the only format that supports bitmask.\n            assert mask_format == \"bitmask\", mask_format\n            masks = [obj[\"segmentation\"] for obj in annos]\n            target.gt_masks = BitMasks.from_polygon_masks(masks, image_size[0], image_size[1]).tensor\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = Keypoints(kpts)\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if not annos and not mask_format == \"polygon\":\n        return target\n    segms = [obj[\"segmentation\"] for obj in annos]\n    if mask_format == \"polygon\":\n        masks = PolygonMasks(segms)\n    else:\n        masks = []\n        for polygons in segms:\n            rles = mask_util.frPyObjects(polygons, *image_size)\n            rle = mask_util.merge(rles)\n            masks.append(mask_util.decode(rle))\n        # PolygonMasks cannot handle empty polygons.\n        if masks:\n            masks = BitMasks(masks)\n        else:\n            masks = PolygonMasks([])\n    target.gt_masks = masks\n\n    keypts = [obj.get(\"keypoints\", []) for obj in annos]  # in Detectron2 Dataset format\n    keypts = Keypoints(keypts)\n    target.gt_keypoints = keypts\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    # TODO put this into Boxes.nonempty()\n    # This is a workaround because the boxes probably got resized\n    # TODO this causes some errors\n    keep = boxes.nonempty(threshold=1)\n    boxes = boxes[keep]\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for polygons in segms:\n                rles = mask_util.frPyObjects(polygons, *image_size)\n                rle = mask_util.merge(rles)\n                masks.append(mask_util.decode(rle))\n            masks = BitMasks(masks)\n        target.gt_masks = masks\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = Keypoints(kpts)\n\n    return target"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(polygons_to_bitmask(segm, *image_size))\n                else:\n                    # COCO RLE\n                    masks.append(mask_util.decode(segm))\n            if len(masks) == 0:\n                # For empty masks, the library returns a float array of all 0s.\n                # This is a workaround to fix the issue.\n                masks = np.zeros((0, image_size[0], image_size[1]))\n            masks = BitMasks(masks)\n        target.gt_masks = masks\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoints = Keypoints(kpts)\n\n    return target"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\n            \"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\")\n        )\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\n            \"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\")\n        )\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\n            \"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\")\n        )\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\n            \"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\")\n        )\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\n            \"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\")\n        )\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\n            \"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\")\n        )\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\n            \"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\")\n        )\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\n            \"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\")\n        )\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\n            \"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\")\n        )\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\n            \"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\")\n        )\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\n            \"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\")\n        )\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\n            \"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\")\n        )\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\n            \"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\")\n        )\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\n            \"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\")\n        )\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\n            \"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\")\n        )\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\"))\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\"))\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\"))\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\"))\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\"))\n    data_home = Path(data_home).expanduser()\n    data_home.mkdir(parents=True, exist_ok=True)\n    return str(data_home)"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    std = np.sqrt(np.diag(cov))\n    corr = cov / std / std[:, None]\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    std = np.sqrt(np.diag(cov))\n    corr = cov / std / std[:, None]\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    std = np.sqrt(np.diag(cov))\n    corr = cov / std / std[:, None]\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    std = np.sqrt(np.diag(cov))\n    corr = cov / std / std[:, None]\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    std = np.sqrt(np.diag(cov))\n    corr = cov / std / std[:, None]\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    std = np.sqrt(np.diag(cov))\n    corr = cov / std / std[:, None]\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    std = np.sqrt(np.diag(cov))\n    corr = cov / std / std[:, None]\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    std = np.sqrt(np.diag(cov))\n    corr = cov / std / std[:, None]\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(\"`cov` must be a 2D array\")\n    std = np.sqrt(np.diag(cov))\n    corr = cov / std / std[:, None]\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(\"`cov` must be a 2D array\")\n    std = np.sqrt(np.diag(cov))\n    corr = cov / std / std[:, None]\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    std = np.sqrt(np.diag(cov))\n    corr = cov / std[:, None] / std[None, :]\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(\"The covariance matrix must be a 2D array\")\n    std = np.sqrt(np.diag(cov))\n    corr = cov / std / std[:, None]\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(\"`cov` must be a 2D array\")\n    std = np.sqrt(np.diag(cov))\n    corr = cov / std / std[:, None]\n    np.fill_diagonal(corr, 1)\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(\"`cov` must be a 2D array\")\n    std = np.sqrt(np.diag(cov))\n    corr = cov / std / std[:, None]\n    np.fill_diagonal(corr, 1)\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert_is_square(cov)\n    assert_is_symmetric(cov)\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    if not np.allclose(cov, cov.T):\n        raise ValueError(\"`cov` must be a symmetric matrix\")\n    std = np.sqrt(np.diag(cov))\n    corr = cov / std / std[:, None]\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    if not np.allclose(cov, cov.T):\n        raise ValueError(\"The covariance matrix must be symmetric\")\n    std = np.sqrt(np.diag(cov))\n    corr = cov / std / std[:, None]\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure the input is a 2D array\n    if cov.ndim != 2:\n        raise ValueError(\"The input array must be 2D.\")\n\n    # Calculate the standard deviation for each variable\n    std = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(std, std)\n\n    # Return the correlation matrix and the standard deviation vector\n    return corr, std"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure the input is a 2D array\n    if cov.ndim != 2:\n        raise ValueError(\"The input array must be 2D.\")\n\n    # Calculate the standard deviation for each variable\n    std = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(std, std)\n\n    # Return the correlation matrix and standard deviation\n    return corr, std"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    training_mode = model.training\n    for m in model.modules():\n        m.__training__ = training_mode\n    yield\n    for m in model.modules():\n        delattr(m, \"__training__\")\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    with ExitStack() as stack:\n        for name, m in model.named_modules():\n            if hasattr(m, \"training\"):\n                stack.enter_context(mock.patch.object(m, \"training\", False))\n        yield\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    training_mode = model.training\n    for m in model.modules():\n        m.__training__ = training_mode\n    yield\n    for m in model.modules():\n        del m.__training__\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    with ExitStack() as stack:\n        for m in model.modules():\n            if hasattr(m, \"training\"):\n                stack.enter_context(mock.patch.object(m, \"training\", False))\n        yield"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    with ExitStack() as stack:\n        for name, module in model.named_modules():\n            if hasattr(module, \"training\"):\n                stack.enter_context(mock.patch.object(module, \"training\"))\n                module.training = False\n        yield"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    with ExitStack() as stack:\n        for name, module in model.named_modules():\n            if isinstance(module, nn.Module):\n                stack.enter_context(mock.patch.object(module, \"training\"))\n                module.training = False\n        yield\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    training_attrs = {}\n    for m in model.modules():\n        if hasattr(m, \"training\"):\n            training_attrs[m] = m.training\n            m.training = False\n    yield\n    for m, v in training_attrs.items():\n        m.training = v\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    training_mode = model.training\n    for module in model.modules():\n        module.__annotations__[\"training\"] = torch.jit.Final[bool]\n    model.train(training_mode)\n    try:\n        yield\n    finally:\n        for module in model.modules():\n            del module.__annotations__[\"training\"]\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # TODO: find a more automatic way to enable import of other classes\n    with ExitStack() as stack:\n        MODULES = [\n            \"detectron2.modeling.backbone\",\n            \"detectron2.modeling.meta_arch\",\n            \"detectron2.modeling.postprocessing\",\n            \"detectron2.modeling.proposal_generator\",\n            \"detectron2.modeling.roi_heads\",\n        ]\n        ctxs = [stack.enter_context(mock.patch(mod + \".training\")) for mod in MODULES]\n        for m in ctxs:\n            m.side_effect = torch.jit.annotate(bool, True)\n        yield\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # TODO: find a more automatic way to enable import of other classes\n    MODULES = [\n        \"detectron2.modeling.backbone\",\n        \"detectron2.modeling.proposal_generator\",\n        \"detectron2.modeling.roi_heads\",\n    ]\n    with ExitStack() as stack:\n        ctxs = [\n            stack.enter_context(mock.patch(mod + \".training\", True))\n            for mod in MODULES\n            if hasattr(model, mod.rsplit(\".\", 1)[-1])\n        ]\n        yield\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # TODO: make it more general\n    # TODO: make it more efficient\n    # TODO: support more types\n    with ExitStack() as stack:\n        for m in model.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                stack.enter_context(m.training.make_constant_(False))\n        yield"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # TODO: support more classes\n    # TODO: support more types\n    with torch.jit.frozen_attribute(model, \"training\"):\n        for m in model.modules():\n            if isinstance(m, (nn.BatchNorm2d, nn.SyncBatchNorm)):\n                m.training = False\n        yield\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    def freeze_training_mode_for_submodule(submodule):\n        if hasattr(submodule, \"training\"):\n            submodule.training = torch.jit.annotate(torch.Tensor, torch.jit.Attribute(submodule.training, torch.Tensor([0])))\n\n    # Freeze the training mode for all submodules\n    for name, submodule in model.named_modules():\n        freeze_training_mode_for_submodule(submodule)\n\n    # Yield control to the context manager\n    yield\n\n    # Unfreeze the training mode for all submodules\n    for name, submodule in model.named_modules():\n        freeze_training_mode_for_submodule(submodule)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    with ExitStack() as stack:\n        for m in model.modules():\n            if hasattr(m, \"training\"):\n                stack.enter_context(mock.patch.object(m, \"training\", True))\n        yield\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    def _freeze_training_mode(module):\n        \"\"\"\n        This function is a helper function that sets the \"training\" attribute of a given module to a constant value. It is used to modify the class definition of a module to make the \"training\" attribute a constant.\n        Input-Output Arguments\n        :param module: The module whose \"training\" attribute is to be annotated as a constant.\n        :return: No return values. This function operates by side effects, modifying the class definition of the module.\n        \"\"\"\n        if hasattr(module, \"training\"):\n            module.training = torch.jit.Final[bool]()\n\n    # Iterate through all submodules in the model and modify their class definitions\n    for m in model.modules():\n        _freeze_training_mode(m)\n\n    # Yield control to the context manager\n    yield\n\n    # Revert the changes made to the class definitions of the model's submodules\n    for m in model.modules():\n        if hasattr(m, \"training\"):\n            del m.training\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # TODO: find a more automatic way to enable import of other classes\n    # TODO: maybe we should consider to add a new function in detectron2.utils.env\n    # to get all subclasses of a given class\n\n    # TODO: we should also consider to add a new function in detectron2.utils.env\n    # to get all submodules of a given module\n\n    # TODO: we should also consider to add a new function in detectron2.utils.env\n    # to get all submodules of a given module\n\n    # TODO: we should also consider to add a new function in detectron2.utils.env\n    # to get all submodules of a given module\n\n    # TODO: we should also consider to add a new function in detectron2.utils.env\n    # to get all submodules of a given module\n\n    # TODO: we should also consider to add a new function in detectron2.utils.env\n    # to get all submodules of a given module\n\n    # TODO: we should also consider to add a new function in detectron2.utils.env\n    # to get all submodules of a given module\n\n    # TODO: we should also consider to add a new function in detectron2.utils.env\n    # to get all submodules of a given module\n\n    # TODO: we should also consider to add a new function in detectron2.utils.env\n    # to get all submodules of a given module\n\n    # TODO: we should also consider to add a new function in detectron2.utils.env\n    # to get all submodules of a given module\n\n    # TODO: we should also consider to add a new function in detectron2.utils.env\n    # to get all submodules of a given module\n\n    # TODO: we should also consider to add a new function in detectron2.utils.env\n    # to get all submodules of a given module\n\n    # TODO: we should also consider to add a new function in detectron2.utils.env\n    # to get all submodules of a given module\n\n    # TODO: we should also consider to add a new function in detectron2.utils.env\n    # to get all sub"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    training_mode = model.training\n    for m in model.modules():\n        m.__annotations__[\"training\"] = torch.jit.Final[bool]\n        m.training = training_mode\n    yield\n    for m in model.modules():\n        del m.__annotations__[\"training\"]\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # Save the original training modes of all submodules\n    old_training_modes = {}\n    for name, module in model.named_modules():\n        old_training_modes[name] = module.training\n\n    # Temporarily set the training mode of all submodules to a constant value\n    for name, module in model.named_modules():\n        module.training = False\n\n    # Execute the context within the with statement\n    yield\n\n    # Revert the training mode of all submodules back to their original values\n    for name, module in model.named_modules():\n        module.training = old_training_modes[name]\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    training_mode = model.training\n    for module in model.modules():\n        module.__training__ = training_mode\n    yield\n    for module in model.modules():\n        delattr(module, \"__training__\")\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # TODO: remove this context manager once the bug is fixed in PyTorch\n    # See https://github.com/pytorch/pytorch/issues/43006\n    # This is a workaround for https://github.com/pytorch/pytorch/issues/43006\n    # We need to set all submodules' training to False in order to compile the model\n    # with jit.script().\n    # TODO: investigate the root cause of this bug, and submit a fix to PyTorch.\n    training_mode = model.training\n\n    def patch_training(module, training):\n        for p in module.parameters():\n            p.requires_grad = training\n\n    def convert_module(module):\n        try:\n            # convert_module is called recursively on every submodule,\n            # so we have to check again in case convert_module was called\n            # on a submodule before.\n            if not hasattr(module, \"_training_mode\"):\n                module._training_mode = module.training\n                module.training = training\n                patch_training(module, training)\n                for submodule in module.children():\n                    convert_module(submodule)\n        except Exception as e:\n            logger = logging.getLogger(__name__)\n            logger.warning(\n                \"Failed to convert module {}; traceback: {}\".format(module, e)\n            )\n\n    convert_module(model)\n    yield\n    for module in model.modules():\n        if hasattr(module, \"_training_mode\"):\n            module.training = module._training_mode\n            patch_training(module, module._training_mode)\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if field1 and field2 have the same shape.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1].shape} and {values[field2].shape}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if field1 and field2 have the same shape.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1].shape} and {values[field2].shape}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if field1 and field2 have the same shape.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1].shape} and {values[field2].shape}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if field1 and field2 have the same shape.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1].shape} and {values[field2].shape}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if field1 and field2 have the same shape.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1].shape} and {values[field2].shape}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if len(field1) equals len(field2) and if every element have the same shape.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1].shape} and {values[field2].shape}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if len(field1) equals len(field2) and if every element have the same shape.\"\"\"\n        shape_field_1 = values[field1].shape\n        shape_field_2 = values[field2].shape\n\n        if shape_field_1 != shape_field_2:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {shape_field_1} and {shape_field_2}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if len(field1) equals len(field2) and if every element have the same shape.\"\"\"\n        shape_field_1 = values[field1].shape\n        shape_field_2 = values[field2].shape\n\n        if shape_field_1 != shape_field_2:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {shape_field_1} and {shape_field_2}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if the shape of field1 equals the shape of field2.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1].shape} and {values[field2].shape}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if field1 and field2 have the same shape.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1].shape} and {values[field2].shape}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if the shape of field1 equals the shape of field2.\"\"\"\n        shape_field_1 = values[field1].shape\n        shape_field_2 = values[field2].shape\n\n        if shape_field_1 != shape_field_2:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {shape_field_1} and {shape_field_2}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if the shape of field1 equals the shape of field2.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1].shape} and {values[field2].shape}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if shape(field1) equals shape(field2).\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1].shape} and {values[field2].shape}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if len(field1) equals len(field2) and if every element have the same shape.\"\"\"\n        shapes_field_1 = [element.shape for element in values[field1]]\n        shapes_field_2 = [element.shape for element in values[field2]]\n\n        if len(values[field1]) != len(values[field2]) or shapes_field_1 != shapes_field_2:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {shapes_field_1} and {shapes_field_2}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if the shape of field1 is equal to the shape of field2.\"\"\"\n        shape_field_1 = values[field1].shape\n        shape_field_2 = values[field2].shape\n\n        if shape_field_1 != shape_field_2:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {shape_field_1} and {shape_field_2}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if the shape of field1 equals the shape of field2.\"\"\"\n        shape_field_1 = values[field1].shape\n        shape_field_2 = values[field2].shape\n\n        if shape_field_1 != shape_field_2:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {shape_field_1} and {shape_field_2}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if shape of field1 equals shape of field2.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if shape of field1 equals shape of field2.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if len(field1) equals len(field2) and if every element have the same shape.\"\"\"\n        shapes_field_1 = [element.shape for element in values[field1]]\n        shapes_field_2 = [element.shape for element in values[field2]]\n\n        if len(values[field1]) != len(values[field2]) or shapes_field_1 != shapes_field_2:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {shapes_field_1} and {shapes_field_2}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        This function is a validator for Pydantic models that checks if two specified fields have the same shape.\n        It is used to validate data structures like NumPy arrays to ensure they are compatible for operations that require matching dimensions.\n\n        Input-Output Arguments\n        :param cls: type, The class of the model being validated.\n        :param values: Dict[str, np.ndarray], A dictionary containing the values of the fields to be checked.\n        :return: Dict[str, np.ndarray], The input values if the shapes of the specified fields match.\n        :raises ValueError: If the shapes of the specified fields do not match.\n        \"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1].shape} and {values[field2].shape}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    metric_names = []\n    metric_params = []\n\n    for metric in metrics:\n        if isinstance(metric, str):\n            metric_names.append(metric)\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            metric_names.append(metric['name'])\n            metric_params.append(deepcopy(metric))\n            metric_params[-1].pop('name')\n        else:\n            raise ValueError(f\"Invalid metric format: {metric}\")\n\n    return metric_names, metric_params"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    metric_names = []\n    metric_params = []\n\n    for metric in metrics:\n        if isinstance(metric, str):\n            metric_names.append(metric)\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            metric_names.append(metric['name'])\n            metric_params.append(deepcopy(metric))\n            metric_params[-1].pop('name')\n        else:\n            raise ValueError(f\"Invalid metric format: {metric}\")\n\n    return metric_names, metric_params"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, str):\n        metrics = [metrics]\n\n    metric_names = []\n    metric_params = []\n\n    for metric in metrics:\n        if isinstance(metric, str):\n            metric_names.append(metric)\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            metric_names.append(metric['name'])\n            metric_params.append(deepcopy(metric))\n            metric_params[-1].pop('name')\n        else:\n            raise ValueError(f'Invalid metric format: {metric}')\n\n    return metric_names, metric_params"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    metric_names = []\n    metric_params = []\n\n    for metric in metrics:\n        if isinstance(metric, str):\n            metric_names.append(metric)\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            metric_names.append(metric['name'])\n            metric_params.append(metric['params'])\n        else:\n            raise ValueError(f\"Invalid metric format: {metric}\")\n\n    return metric_names, metric_params"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, str):\n        metrics = [metrics]\n\n    metric_names = []\n    metric_params = []\n    for metric in metrics:\n        if isinstance(metric, str):\n            metric_names.append(metric)\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            metric_names.append(metric['name'])\n            metric_params.append(deepcopy(metric))\n            metric_params[-1].pop('name')\n        else:\n            raise ValueError(f'Invalid metric type: {type(metric)}')\n\n    return metric_names, metric_params"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list):\n        if all(isinstance(metric, str) for metric in metrics):\n            return metrics, [{} for _ in metrics]\n        elif all(isinstance(metric, dict) for metric in metrics):\n            return [metric['name'] for metric in metrics], [deepcopy(metric) for metric in metrics]\n        else:\n            raise ValueError(\"Invalid metrics format. Metrics must be either a list of strings or a list of dictionaries.\")\n    else:\n        raise ValueError(\"Invalid metrics format. Metrics must be a list.\")"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list):\n        metric_names = []\n        metric_params = []\n        for metric in metrics:\n            if isinstance(metric, str):\n                metric_names.append(metric)\n                metric_params.append({})\n            elif isinstance(metric, dict):\n                metric_names.append(metric['name'])\n                metric_params.append(metric.get('params', {}))\n        return metric_names, metric_params\n    else:\n        raise ValueError('Metrics must be a list of strings or dictionaries.')"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, str):\n        return [metrics], [{}]\n    elif isinstance(metrics, dict):\n        return [metrics['name']], [deepcopy(metrics)]\n    elif isinstance(metrics, list):\n        if all([isinstance(metric, str) for metric in metrics]):\n            return metrics, [{} for _ in range(len(metrics))]\n        elif all([isinstance(metric, dict) for metric in metrics]):\n            return [metric['name'] for metric in metrics], [deepcopy(metric) for metric in metrics]\n        else:\n            raise ValueError('Invalid metrics format')\n    else:\n        raise ValueError('Invalid metrics format')"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    metrics_names = []\n    metrics_params = []\n    for metric in metrics:\n        if isinstance(metric, str):\n            metrics_names.append(metric)\n            metrics_params.append({})\n        elif isinstance(metric, dict):\n            for metric_name, metric_params in metric.items():\n                metrics_names.append(metric_name)\n                metrics_params.append(metric_params)\n        else:\n            raise ValueError(f\"Invalid metric format: {metric}\")\n    return metrics_names, metrics_params\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if all(isinstance(metric, str) for metric in metrics):\n        return metrics, [{} for _ in metrics]\n\n    elif all(isinstance(metric, dict) for metric in metrics):\n        metric_names = []\n        metric_params = []\n        for metric in metrics:\n            metric_names.append(metric['name'])\n            metric_params.append(deepcopy(metric))\n            metric_params[-1].pop('name')\n        return metric_names, metric_params\n\n    else:\n        raise ValueError('Invalid input format. Metrics must be either a list of strings or a list of dictionaries.')"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    # Initialize empty lists to store metric names and parameters\n    metric_names = []\n    metric_params = []\n\n    # Iterate over the input list of metrics\n    for metric in metrics:\n        # If the metric is a string, it represents a metric name\n        if isinstance(metric, str):\n            metric_names.append(metric)\n            metric_params.append({})\n        # If the metric is a dictionary, it contains metric details\n        elif isinstance(metric, dict):\n            # Extract the metric name and parameters\n            metric_name = metric['name']\n            metric_param = deepcopy(metric)\n            del metric_param['name']\n\n            # Add the metric name and parameters to the respective lists\n            metric_names.append(metric_name)\n            metric_params.append(metric_param)\n\n    # Return the list of metric names and a list of dictionaries with metric parameters\n    return metric_names, metric_params"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list):\n        if all([isinstance(metric, str) for metric in metrics]):\n            return metrics, [{}] * len(metrics)\n        elif all([isinstance(metric, dict) for metric in metrics]):\n            metric_names = [metric['name'] for metric in metrics]\n            metric_params = [{key: value for key, value in metric.items() if key != 'name'} for metric in metrics]\n            return metric_names, metric_params\n        else:\n            raise ValueError(\"Invalid metrics format. Metrics should be a list of strings or dictionaries.\")\n    else:\n        raise ValueError(\"Invalid metrics format. Metrics should be a list.\")"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list):\n        metric_names = []\n        metric_params = []\n        for metric in metrics:\n            if isinstance(metric, str):\n                metric_names.append(metric)\n                metric_params.append({})\n            elif isinstance(metric, dict):\n                metric_names.append(metric['name'])\n                metric_params.append(metric.get('params', {}))\n        return metric_names, metric_params\n    else:\n        raise ValueError(\"metrics must be a list of strings or dictionaries\")\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, str):\n        metrics = [metrics]\n\n    metric_names = []\n    metric_params = []\n\n    for metric in metrics:\n        if isinstance(metric, str):\n            metric_names.append(metric)\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            metric_names.append(metric['name'])\n            metric_params.append(metric['params'])\n\n    return metric_names, metric_params\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, str):\n        metrics = [metrics]\n\n    metric_names = []\n    metric_params = []\n\n    for metric in metrics:\n        if isinstance(metric, str):\n            metric_names.append(metric)\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            metric_names.append(metric['name'])\n            metric_params.append(metric.get('params', {}))\n        else:\n            raise ValueError(f\"Invalid metric format: {metric}\")\n\n    return metric_names, metric_params\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list):\n        if all(isinstance(metric, str) for metric in metrics):\n            return metrics, [{} for _ in metrics]\n        elif all(isinstance(metric, dict) for metric in metrics):\n            return [metric.pop('name') for metric in metrics], metrics\n        else:\n            raise ValueError(\"Invalid metric format. Metrics should be either strings or dictionaries.\")\n    else:\n        raise TypeError(\"Metrics should be a list of strings or dictionaries.\")\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, str):\n        metrics = [metrics]\n    if isinstance(metrics, list):\n        if all(isinstance(metric, str) for metric in metrics):\n            return metrics, [{} for _ in metrics]\n        elif all(isinstance(metric, dict) for metric in metrics):\n            return [metric['name'] for metric in metrics], [{key: value for key, value in metric.items() if key != 'name'} for metric in metrics]\n        else:\n            raise ValueError('Metrics must be either strings or dictionaries.')\n    else:\n        raise ValueError('Metrics must be either a list of strings or dictionaries.')\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list):\n        if all(isinstance(metric, str) for metric in metrics):\n            return metrics, [{} for _ in range(len(metrics))]\n        elif all(isinstance(metric, dict) for metric in metrics):\n            return [metric.pop('name') for metric in metrics], [metric for metric in metrics]\n        else:\n            raise ValueError(\"Metrics must be either a list of strings or a list of dictionaries.\")\n    else:\n        raise TypeError(\"Metrics must be a list.\")\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, str):\n        metrics = [metrics]\n\n    metric_names = []\n    metric_params = []\n\n    for metric in metrics:\n        if isinstance(metric, str):\n            metric_names.append(metric)\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            metric_name, metric_param = list(metric.items())[0]\n            metric_names.append(metric_name)\n            metric_params.append(metric_param)\n\n    return metric_names, metric_params\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    # If the input is a list of strings, convert each string to a dictionary with default parameters\n    if isinstance(metrics[0], str):\n        metrics = [{'name': metric} for metric in metrics]\n\n    # Extract metric names and parameters from the dictionaries\n    metric_names = [metric['name'] for metric in metrics]\n    metric_params = [{k: v for k, v in metric.items() if k != 'name'} for metric in metrics]\n\n    # Return the metric names and parameters as a tuple\n    return metric_names, metric_params\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(jax.jacfwd(fn), in_axes=-1, out_axes=-1)(mean)\n\n  # The cube root of the determinant of the Jacobian is the geometric mean\n  # of the eigenvalues of the Jacobian, which gives us the isotropic scaling\n  # implied by `fn` at each mean that `scale` should be multiplied by.\n  eps = jnp.finfo(jnp.float32).tiny  # Guard against an inf gradient at 0.\n  abs_det = jnp.maximum(eps, jnp.abs(jnp.linalg.det(jac)))\n  # Special case d == 3 for speed's sake.\n  fn_scale = scale * (jnp.cbrt(abs_det) if d == 3 else abs_det ** (1 / d))\n  return fn_mean, fn_scale"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(jax.jacfwd(fn), in_axes=-1, out_axes=-1)(mean)\n\n  # The cube root of the determinant of the Jacobian is the geometric mean\n  # of the eigenvalues of the Jacobian, which gives us the isotropic scaling\n  # implied by `fn` at each mean that `scale` should be multiplied by.\n  eps = jnp.finfo(jnp.float32).tiny  # Guard against an inf gradient at 0.\n  abs_det = jnp.maximum(eps, jnp.abs(jnp.linalg.det(jac)))\n  # Special case d == 3 for speed's sake.\n  fn_scale = scale * (jnp.cbrt(abs_det) if d == 3 else abs_det ** (1 / d))\n  return fn_mean, fn_scale\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  def t_to_s(t):\n    \"\"\"Maps metric distances to normalized distances in the range [0, 1].\"\"\"\n    # Clamp `t` to the range [t_near, t_far] to ensure it is within a valid range.\n    t = jnp.clip(t, t_near, t_far)\n    # Compute the normalized distance `s` using the function `fn`.\n    s = fn(t)\n    # Clamp `s` to the range [0, 1] to ensure it is within a valid range.\n    s = jnp.clip(s, 0, 1)\n    return s\n\n  def s_to_t(s):\n    \"\"\"Maps normalized distances back to metric distances.\"\"\"\n    # Clamp `s` to the range [0, 1] to ensure it is within a valid range.\n    s = jnp.clip(s, 0, 1)\n    # Compute the metric distance `t` using the inverse of the function `fn`.\n    t = fn_inv(s)\n    # Clamp `t` to the range [t_near, t_far] to ensure it is within a valid range.\n    t = jnp.clip(t, t_near, t_far)\n    return t\n\n  return t_to_s, s_to_t"}
{"namespace": "coord.construct_ray_warps", "completion": "  def t_to_s(t):\n    \"\"\"\n    Maps metric distances to normalized distances in the range [0, 1].\n\n    Input-Output Arguments\n    :param t: Tensor. Represents metric distances.\n    :return: Tensor. Represents normalized distances in the range [0, 1].\n    \"\"\"\n    t_clipped = jnp.clip(t, t_near, t_far)\n    t_clipped = (t_clipped - t_near) / (t_far - t_near)\n    return fn(t_clipped)\n\n  def s_to_t(s):\n    \"\"\"\n    Maps normalized distances in the range [0, 1] to metric distances.\n\n    Input-Output Arguments\n    :param s: Tensor. Represents normalized distances in the range [0, 1].\n    :return: Tensor. Represents metric distances.\n    \"\"\"\n    t_clipped = fn_inv(s)\n    t_clipped = t_near + (t_far - t_near) * t_clipped\n    return t_clipped\n\n  return t_to_s, s_to_t"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Define the forward mapping from metric to normalized distances.\n  def t_to_s(t):\n    # Clamp t to the valid range to avoid numerical issues.\n    t = jnp.clip(t, t_near, t_far)\n    # Compute the normalized distance using the provided function.\n    s = (fn(t) - fn(t_near)) / (fn(t_far) - fn(t_near))\n    # Clamp s to the valid range to avoid numerical issues.\n    return jnp.clip(s, 0, 1)\n\n  # Define the backward mapping from normalized to metric distances.\n  def s_to_t(s):\n    # Clamp s to the valid range to avoid numerical issues.\n    s = jnp.clip(s, 0, 1)\n    # Compute the metric distance using the inverse of the provided function.\n    t = fn_inv((fn(t_far) - fn(t_near)) * s + fn(t_near))\n    # Clamp t to the valid range to avoid numerical issues.\n    return jnp.clip(t, t_near, t_far)\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(jax.jacfwd(fn), in_axes=-1, out_axes=-1)(mean)\n\n  # The cube root of the determinant of the Jacobian is the geometric mean\n  # of the eigenvalues of the Jacobian, which gives us the isotropic scaling\n  # implied by `fn` at each mean that `scale` should be multiplied by.\n  eps = jnp.finfo(jnp.float32).tiny  # Guard against an inf gradient at 0.\n  abs_det = jnp.maximum(eps, jnp.abs(jnp.linalg.det(jac)))\n  # Special case d == 3 for speed's sake.\n  fn_scale = scale * (jnp.cbrt(abs_det) if d == 3 else abs_det ** (1 / d))\n\n  def t_to_s(t):\n    \"\"\"Map distances `t` to normalized distances in [0, 1].\"\"\"\n    t_clipped = jnp.clip(t, t_near, t_far)\n    t_scaled = (t_clipped - t_near) / (t_far - t_near)\n    return fn(t_scaled)\n\n  def s_to_t(s):\n    \"\"\"Inverse of `t_to_s`.\"\"\"\n    if fn_inv is None:\n      # Try to automatically determine the inverse.\n      if fn == contract:\n        fn_inv = inv_contract\n      else:\n        raise ValueError(\n            f'fn_inv must be provided for fn={fn}, as it cannot be '\n            'automatically determined.'\n        )\n    t_scaled = fn_inv(s)\n    t = t_scaled * (t_far - t_near) + t_near\n    return t\n\n  return t_to_s, s_to_t"}
{"namespace": "coord.construct_ray_warps", "completion": "  # The forward mapping from metric to normalized distances.\n  def t_to_s(t):\n    t_clipped = jnp.clip(t, t_near, t_far)\n    s_clipped = (t_clipped - t_near) / (t_far - t_near)\n    return fn(s_clipped)\n\n  # The inverse mapping from normalized to metric distances.\n  def s_to_t(s):\n    if fn_inv is None:\n      # If the inverse function is not provided, try to automatically determine it based on a predefined mapping of functions to their inverses.\n      if fn == contract:\n        fn_inv = inv_contract\n      elif fn == contract3_isoscale:\n        fn_inv = lambda x: contract(x)\n      else:\n        raise ValueError(f'fn_inv must be provided for fn={fn}.')\n    s_clipped = jnp.clip(s, 0, 1)\n    t_clipped = fn_inv(s_clipped) * (t_far - t_near) + t_near\n    return t_clipped\n\n  return t_to_s, s_to_t"}
{"namespace": "coord.construct_ray_warps", "completion": "  def t_to_s(t):\n    \"\"\"Maps metric distances to normalized distances in the range [0, 1].\"\"\"\n    return (fn(jnp.maximum(t_near, t)) - fn(t_near)) / (\n        fn(t_far) - fn(t_near)\n    )\n\n  def s_to_t(s):\n    \"\"\"Maps normalized distances in the range [0, 1] to metric distances.\"\"\"\n    if fn_inv is None:\n      # This is an approximation that works well for the functions we use.\n      # For more general fn, we'd need to use a numerical solver.\n      return fn_inv(s * (fn(t_far) - fn(t_near)) + fn(t_near))\n    else:\n      return fn_inv(s * (fn(t_far) - fn(t_near)) + fn(t_near))\n\n  return t_to_s, s_to_t"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    fn_inv = {\n        contract: lambda x: inv_contract(x),\n        contract3_isoscale: lambda x: jnp.sqrt(\n            jnp.maximum(1, 8 * x**2 - 8 * x + 1)\n        ),\n    }.get(fn)\n    if fn_inv is None:\n      raise ValueError(\n          f'fn_inv is None and fn={fn} is not in the default mapping.'\n      )\n\n  def t_to_s(t):\n    return (fn(t) - fn(t_near)) / (fn(t_far) - fn(t_near))\n\n  def s_to_t(s):\n    return fn_inv((s * (fn(t_far) - fn(t_near))) + fn(t_near))\n\n  return t_to_s, s_to_t"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(jax.jacfwd(fn), in_axes=-1, out_axes=-1)(mean)\n\n  # The cube root of the determinant of the Jacobian is the geometric mean\n  # of the eigenvalues of the Jacobian, which gives us the isotropic scaling\n  # implied by `fn` at each mean that `scale` should be multiplied by.\n  eps = jnp.finfo(jnp.float32).tiny  # Guard against an inf gradient at 0.\n  abs_det = jnp.maximum(eps, jnp.abs(jnp.linalg.det(jac)))\n  # Special case d == 3 for speed's sake.\n  fn_scale = scale * (jnp.cbrt(abs_det) if d == 3 else abs_det ** (1 / d))\n  return fn_mean, fn_scale\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  # Compute the forward mapping from metric to normalized distances.\n  fn_mean, fn_scale = track_isotropic(fn, t_near, 1.0)\n  t_to_s = lambda t: (fn(t) - fn_mean) / fn_scale\n\n  # Compute the backward mapping from normalized to metric distances.\n  if fn_inv is None:\n    # If the inverse of the function is not provided, attempt to automatically determine it.\n    fn_inv = {\n        contract: inv_contract,\n        contract3_isoscale: lambda x: contract(x) / contract3_isoscale(x),\n    }.get(fn)\n\n  if fn_inv is None:\n    raise ValueError(f'fn_inv must be provided for fn={fn}')\n\n  # Compute the backward mapping from normalized to metric distances.\n  s_to_t = lambda s: fn_inv(fn_mean + fn_scale * s)\n\n  return t_to_s, s_to_t"}
{"namespace": "coord.construct_ray_warps", "completion": "  def t_to_s(t):\n    \"\"\"\n    Maps metric distances to normalized distances in the range [0, 1].\n\n    Input-Output Arguments\n    :param t: Tensor. Represents the metric distances to be mapped.\n    :return: Tensor. The normalized distances corresponding to the input metric distances.\n    \"\"\"\n    t = jnp.clip(t, t_near, t_far)\n    s = fn(t)\n    return jnp.clip(s, 0.0, 1.0)\n\n  def s_to_t(s):\n    \"\"\"\n    Maps normalized distances back to metric distances.\n\n    Input-Output Arguments\n    :param s: Tensor. Represents the normalized distances to be mapped.\n    :return: Tensor. The metric distances corresponding to the input normalized distances.\n    \"\"\"\n    if fn_inv is None:\n      if fn is contract:\n        fn_inv = inv_contract\n      elif fn is contract3_isoscale:\n        fn_inv = lambda x: contract(x)\n      else:\n        raise ValueError(f'fn_inv is None, but fn {fn} is not supported.')\n    s = jnp.clip(s, 0.0, 1.0)\n    t = fn_inv(s)\n    return jnp.clip(t, t_near, t_far)\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  def t_to_s(t):\n    \"\"\"Map metric distances to normalized distances.\"\"\"\n    t_clipped = jnp.clip(t, t_near, t_far)\n    s_clipped = (t_clipped - t_near) / (t_far - t_near)\n    s_mapped = fn(s_clipped)\n    return s_mapped\n\n  def s_to_t(s):\n    \"\"\"Map normalized distances to metric distances.\"\"\"\n    s_clipped = jnp.clip(s, 0.0, 1.0)\n    if fn_inv is None:\n      # Use a pre-computed mapping of functions to their inverses.\n      if fn == contract:\n        s_mapped = s_clipped**2 / (2 * s_clipped - s_clipped**2)\n      else:\n        raise ValueError(f'fn={fn} not supported')\n    else:\n      s_mapped = fn_inv(s_clipped)\n    t_mapped = t_near + s_mapped * (t_far - t_near)\n    return t_mapped\n\n  return t_to_s, s_to_t"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    # Try to find the inverse of fn in a predefined mapping.\n    if fn == contract:\n      fn_inv = inv_contract\n    else:\n      raise ValueError(\n          f'fn_inv must be provided if fn is not one of the predefined '\n          f'functions, but fn={fn} is not in the predefined functions.'\n      )\n\n  # Compute the inverse of the function at the locations of each mean.\n  fn_inv_t_near = fn_inv(t_near)\n  fn_inv_t_far = fn_inv(t_far)\n\n  # Compute the scaling factor for the function at the locations of each mean.\n  scale = (fn_inv_t_far - fn_inv_t_near) / (t_far - t_near)\n\n  # Define the forward and backward mappings.\n  def t_to_s(t):\n    return (fn(t) - fn_inv_t_near) / scale\n\n  def s_to_t(s):\n    return fn_inv(fn_inv_t_near + scale * s)\n\n  return t_to_s, s_to_t"}
{"namespace": "coord.construct_ray_warps", "completion": "  # The following function is the inverse of `fn`.\n  # If `fn_inv` is provided, we use it directly.\n  # Otherwise, we try to automatically determine the inverse based on a predefined mapping of functions to their inverses.\n  if fn_inv is None:\n    if fn == contract:\n      fn_inv = inv_contract\n    elif fn == contract3_isoscale:\n      fn_inv = lambda x: contract(x)\n    else:\n      raise ValueError(f'fn_inv is not provided and fn={fn} is not supported.')\n\n  # The following function maps metric distances to normalized distances.\n  # It is based on the inverse of `fn`.\n  def t_to_s(t):\n    # Clamp `t` to the valid range [t_near, t_far].\n    t = jnp.clip(t, t_near, t_far)\n    # Compute the normalized distances using the inverse of `fn`.\n    s = fn_inv(t)\n    # Clamp the normalized distances to the valid range [0, 1].\n    s = jnp.clip(s, 0, 1)\n    return s\n\n  # The following function maps normalized distances back to metric distances.\n  # It is based on `fn`.\n  def s_to_t(s):\n    # Clamp the normalized distances to the valid range [0, 1].\n    s = jnp.clip(s, 0, 1)\n    # Compute the metric distances using `fn`.\n    t = fn(s)\n    # Clamp the metric distances to the valid range [t_near, t_far].\n    t = jnp.clip(t, t_near, t_far)\n    return t\n\n  return t_to_s, s_to_t"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    # If no inverse function is provided, we try to automatically determine it\n    # based on a predefined mapping of functions to their inverses.\n    fn_inv = {\n        contract: inv_contract,\n        contract3_isoscale: lambda x: contract3_isoscale(x) ** 3,\n    }.get(fn, None)\n    if fn_inv is None:\n      raise ValueError(f'fn_inv must be provided for {fn}')\n\n  def t_to_s(t):\n    \"\"\"Map distances `t` to normalized distances `s`.\"\"\"\n    # Clamp `t` to the nearest intersection with the planes `t_near` and `t_far`.\n    t = jnp.clip(t, t_near, t_far)\n\n    # Normalize `t` to the range [0, 1] using the inverse function `fn_inv`.\n    s_unnormalized = fn_inv(t)\n\n    # Normalize `s_unnormalized` to the range [0, 1] using the min and max values.\n    eps = jnp.finfo(jnp.float32).eps\n    s_normalized = (s_unnormalized - fn_inv(t_near)) / (\n        fn_inv(t_far) - fn_inv(t_near) + eps\n    )\n\n    # Clamp the normalized values to the range [0, 1] and avoid NaNs.\n    s = jnp.clip(s_normalized, 0.0, 1.0)\n    s = jnp.where(jnp.isnan(s), 1.0, s)\n    return s\n\n  def s_to_t(s):\n    \"\"\"Map normalized distances `s` to distances `t`.\"\"\"\n    # Denormalize `s` using the min and max values.\n    t_unnormalized = fn_inv(\n        s * (fn_inv(t_far) - fn_inv(t_near)) + fn_inv(t_near)\n    )"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    # Try to automatically determine the inverse of `fn`.\n    if fn == contract:\n      fn_inv = inv_contract\n    else:\n      raise ValueError(f'fn_inv must be provided for fn={fn}')\n\n  # Compute the scaling factor for `fn` at `t_near` and `t_far`.\n  t_near_scale = fn_inv(t_near)\n  t_far_scale = fn_inv(t_far)\n\n  def t_to_s(t):\n    # Compute the scaling factor for `fn` at `t`.\n    t_scale = fn_inv(t)\n\n    # Compute the normalized distance using the scaling factor.\n    s = (t_scale - t_near_scale) / (t_far_scale - t_near_scale)\n\n    # Clamp the normalized distance to [0, 1].\n    s = jnp.clip(s, 0.0, 1.0)\n\n    return s\n\n  def s_to_t(s):\n    # Compute the scaling factor for `fn` at `s`.\n    t_scale = t_near_scale + s * (t_far_scale - t_near_scale)\n\n    # Compute the metric distance using the scaling factor.\n    t = fn(t_scale)\n\n    # Clamp the metric distance to [t_near, t_far].\n    t = jnp.clip(t, t_near, t_far)\n\n    return t\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    # Try to automatically compute the inverse of `fn`.\n    fn_inv = {\n        contract: inv_contract,\n        contract3_isoscale: lambda x: x / contract3_isoscale(x),\n    }.get(fn, None)\n    if fn_inv is None:\n      raise ValueError('fn_inv must be provided for fn={fn}.')\n\n  def t_to_s(t):\n    # Compute the normalized distances using `fn`.\n    s = fn(t)\n\n    # Clamp the normalized distances to the valid range [0, 1].\n    s = jnp.clip((s - fn(t_near)) / (fn(t_far) - fn(t_near)), 0, 1)\n    return s\n\n  def s_to_t(s):\n    # Compute the metric distances using `fn_inv`.\n    t = fn_inv(s * (fn(t_far) - fn(t_near)) + fn(t_near))\n\n    # Clamp the metric distances to the valid range [t_near, t_far].\n    t = jnp.clip(t, t_near, t_far)\n    return t\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    # Use a predefined mapping of functions to their inverses.\n    if fn == jnp.exp:\n      fn_inv = jnp.log\n    elif fn == jnp.tanh:\n      fn_inv = jnp.arctanh\n    elif fn == jnp.sin:\n      fn_inv = jnp.arcsin\n    else:\n      raise ValueError(\n          f'fn_inv must be provided for fn={fn} not in [exp, tanh, sin].'\n      )\n\n  def t_to_s(t):\n    \"\"\"Transform metric distances to normalized distances.\"\"\"\n    # Map t to the range [0, 1].\n    t_clipped = jnp.clip(t, t_near, t_far)\n    t_scaled = (t_clipped - t_near) / (t_far - t_near)\n\n    # Apply the function to t_scaled to get s.\n    s = fn(t_scaled)\n\n    # Map s back to the range [0, 1].\n    s_clipped = jnp.clip(s, 0, 1)\n    return s_clipped\n\n  def s_to_t(s):\n    \"\"\"Transform normalized distances to metric distances.\"\"\"\n    # Map s to the range [0, 1].\n    s_clipped = jnp.clip(s, 0, 1)\n\n    # Apply the inverse function to s_clipped to get t_scaled.\n    t_scaled = fn_inv(s_clipped)\n\n    # Map t_scaled to the range [t_near, t_far].\n    t = t_near + t_scaled * (t_far - t_near)\n    return t\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  def t_to_s(t):\n    \"\"\"Maps metric distances to normalized distances in the range [0, 1].\"\"\"\n    # Clamp to the near and far planes.\n    t_clamped = jnp.clip(t, t_near, t_far)\n    # Transform the distances using the given function.\n    s_clamped = fn(t_clamped)\n    # Normalize the transformed distances to the range [0, 1].\n    return (s_clamped - fn(t_near)) / (fn(t_far) - fn(t_near))\n\n  def s_to_t(s):\n    \"\"\"Maps normalized distances in the range [0, 1] back to metric distances.\"\"\"\n    # Invert the normalization.\n    t_clamped = s * (fn(t_far) - fn(t_near)) + fn(t_near)\n    # Apply the inverse of the given function to get the metric distances.\n    if fn_inv is not None:\n      t = fn_inv(t_clamped)\n    else:\n      # If the inverse function is not provided, try to use a predefined mapping.\n      if fn == contract:\n        t = inv_contract(t_clamped)\n      elif fn == contract3_isoscale:\n        t = jnp.sqrt(t_clamped / (1 - t_clamped))\n      else:\n        raise ValueError(f'No inverse function available for {fn}.')\n    # Clamp the metric distances to the near and far planes.\n    return jnp.clip(t, t_near, t_far)\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)\n\n"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  return jnp.trapz(w, t, axis=-1)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  return jnp.trapz(w, t, axis=-1)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  return jnp.trapz(w, t, axis=-1)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  check_zero_endpoints(w)\n  return jnp.trapz(w, t, axis=-1)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  return jnp.trapz(w, t, axis=-1)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  return jnp.trapz(w, t, axis=-1)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_stepfun(t, w)\n  check_zero_endpoints(w)\n  return jnp.trapz(w, t, axis=-1)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  check_zero_endpoints(w)\n  return jnp.trapz(w, t)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  check_zero_endpoints(w)\n\n  # Compute the integral of the linear spline using the trapezoid rule.\n  integral = jnp.trapz(w, t, axis=-1)\n\n  return integral"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  check_zero_endpoints(w)\n  return jnp.trapz(w, t)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  check_zero_endpoints(w)\n  dt = jnp.diff(t)\n  return jnp.sum(0.5 * dt * (w[Ellipsis, :-1] + w[Ellipsis, 1:]), axis=-1)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  check_zero_endpoints(w)\n\n  # The integral is the cumulative sum of the trapezoid areas.\n  return jnp.cumsum(jnp.diff(t) * (w[Ellipsis, :-1] + w[Ellipsis, 1:]) / 2, axis=-1)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  check_zero_endpoints(w)\n\n  # Compute the trapezoid rule for each spline individually.\n  return jnp.trapz(w, t, axis=-1)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  check_zero_endpoints(w)\n\n  # Compute the integral of the spline using the trapezoid rule.\n  integral = jnp.trapz(w, x=t, axis=-1)\n\n  # Return the computed integral.\n  return integral"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  check_zero_endpoints(w)\n\n  # Compute the trapezoid rule for each spline.\n  trapezoid = jnp.sum((w[Ellipsis, 1:] + w[Ellipsis, :-1]) / 2 * jnp.diff(t), axis=-1)\n  return trapezoid"}
{"namespace": "linspline.integrate", "completion": "  # Check that the input data points are valid for a linear spline.\n  utils.assert_valid_linspline(t, w)\n\n  # Compute the difference between the x-coordinates of the data points.\n  dt = jnp.diff(t)\n\n  # Compute the area of the trapezoids formed by the data points and the x-axis.\n  area = jnp.sum(dt * (w[Ellipsis, :-1] + w[Ellipsis, 1:]), axis=-1) / 2\n\n  return area"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  check_zero_endpoints(w)\n\n  # Compute the trapezoid rule over the spline.\n  dt = jnp.diff(t)\n  return jnp.sum(jnp.abs(w[Ellipsis, :-1] + w[Ellipsis, 1:]) * dt, axis=-1) / 2"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  check_zero_endpoints(w)\n\n  # Compute the integral of each spline segment.\n  dt = jnp.diff(t)\n  dw = jnp.diff(w)\n  area = (dt * (w[Ellipsis, :-1] + w[Ellipsis, 1:])) / 2\n\n  # Compute the integral of the spline.\n  integral = jnp.sum(area, axis=-1)\n\n  return integral"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  # The spline values are assumed to be zero outside of the spline range.\n  w = jnp.where(t > t[Ellipsis, -1], 0, w)\n  w = jnp.where(t < t[Ellipsis, 0], 0, w)\n  return jnp.trapz(w, t, axis=-1)"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  check_zero_endpoints(w)\n\n  # Compute the integral of the linear spline using the trapezoid rule.\n  # The integral is the cumulative sum of the areas of the trapezoids formed by the linear spline segments.\n  # The areas of the trapezoids are calculated using the formula (b1 + b2) * h / 2, where b1 and b2 are the y-coordinates of the endpoints of the linear spline segment, and h is the difference in x-coordinates between the endpoints.\n  # The cumulative sum is taken along the x-coordinates of the linear spline to get the total integral.\n  # The result is broadcasted to match the shape of the input data points.\n  return jnp.cumsum(jnp.diff(t) * (w[Ellipsis, :-1] + w[Ellipsis, 1:]), axis=-1)"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    id_df = pd.DataFrame({f'id_{i}': id_list for i, id_list in enumerate(ids)})\n    score_df = pd.DataFrame({f'score_{i}': score_list for i, score_list in enumerate(scores)})\n    df = pd.concat([id_df, score_df], axis=1)\n\n    def cc_pure_apply(row):\n        ids_tuple = tuple(row[[f'id_{i}' for i in range(len(ids))]].values)\n        scores_tuple = tuple(row[[f'score_{i}' for i in range(len(scores))]].values)\n        return pd.Series(cc_pure(ids_tuple, scores_tuple, weights, top_k))\n\n    df[['cc_id', 'cc_score']] = df.apply(cc_pure_apply, axis=1)\n    return df['cc_id'].tolist(), df['cc_score'].tolist()"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    id_df = pd.DataFrame({f'id_{i}': id_list for i, id_list in enumerate(ids)})\n    score_df = pd.DataFrame({f'score_{i}': score_list for i, score_list in enumerate(scores)})\n    df = pd.concat([id_df, score_df], axis=1)\n\n    def cc_pure_apply(row):\n        ids_tuple = tuple(row[[f'id_{i}' for i in range(len(ids))]].values)\n        scores_tuple = tuple(row[[f'score_{i}' for i in range(len(scores))]].values)\n        return pd.Series(cc_pure(ids_tuple, scores_tuple, weights, top_k))\n\n    df[['cc_id', 'cc_score']] = df.apply(cc_pure_apply, axis=1)\n    return df['cc_id'].tolist(), df['cc_score'].tolist()"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    id_df = pd.DataFrame({f'id_{i}': id_list for i, id_list in enumerate(ids)})\n    score_df = pd.DataFrame({f'score_{i}': score_list for i, score_list in enumerate(scores)})\n    df = pd.concat([id_df, score_df], axis=1)\n\n    def cc_pure_apply(row):\n        ids_tuple = tuple(row[[f'id_{i}' for i in range(len(ids))]].values)\n        scores_tuple = tuple(row[[f'score_{i}' for i in range(len(scores))]].values)\n        return pd.Series(cc_pure(ids_tuple, scores_tuple, weights, top_k))\n\n    df[['cc_id', 'cc_score']] = df.apply(cc_pure_apply, axis=1)\n    return df['cc_id'].tolist(), df['cc_score'].tolist()"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    id_df = pd.DataFrame({f'id_{i}': id_list for i, id_list in enumerate(ids)})\n    score_df = pd.DataFrame({f'score_{i}': score_list for i, score_list in enumerate(scores)})\n    df = pd.concat([id_df, score_df], axis=1)\n\n    def cc_pure_apply(row):\n        ids_tuple = tuple(row[[f'id_{i}' for i in range(len(ids))]].values)\n        scores_tuple = tuple(row[[f'score_{i}' for i in range(len(scores))]].values)\n        return pd.Series(cc_pure(ids_tuple, scores_tuple, weights, top_k))\n\n    df[['cc_id', 'cc_score']] = df.apply(cc_pure_apply, axis=1)\n    return df['cc_id'].tolist(), df['cc_score'].tolist()"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    id_df = pd.DataFrame({f'id_{i}': id_list for i, id_list in enumerate(ids)})\n    score_df = pd.DataFrame({f'score_{i}': score_list for i, score_list in enumerate(scores)})\n    df = pd.concat([id_df, score_df], axis=1)\n\n    def cc_pure_apply(row):\n        ids_tuple = tuple(row[[f'id_{i}' for i in range(len(ids))]].values)\n        scores_tuple = tuple(row[[f'score_{i}' for i in range(len(scores))]].values)\n        return pd.Series(cc_pure(ids_tuple, scores_tuple, weights, top_k))\n\n    df[['cc_id', 'cc_score']] = df.apply(cc_pure_apply, axis=1)\n    return df['cc_id'].tolist(), df['cc_score'].tolist()"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    id_df = pd.DataFrame({f'id_{i}': id_list for i, id_list in enumerate(ids)})\n    score_df = pd.DataFrame({f'score_{i}': score_list for i, score_list in enumerate(scores)})\n    df = pd.concat([id_df, score_df], axis=1)\n\n    def cc_pure_apply(row):\n        ids_tuple = tuple(row[[f'id_{i}' for i in range(len(ids))]].values)\n        scores_tuple = tuple(row[[f'score_{i}' for i in range(len(scores))]].values)\n        return pd.Series(cc_pure(ids_tuple, scores_tuple, weights, top_k))\n\n    df[['cc_id', 'cc_score']] = df.apply(cc_pure_apply, axis=1)\n    return df['cc_id'].tolist(), df['cc_score'].tolist()"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    id_df = pd.DataFrame({f'id_{i}': id_list for i, id_list in enumerate(ids)})\n    score_df = pd.DataFrame({f'score_{i}': score_list for i, score_list in enumerate(scores)})\n    df = pd.concat([id_df, score_df], axis=1)\n\n    def cc_pure_apply(row):\n        ids_tuple = tuple(row[[f'id_{i}' for i in range(len(ids))]].values)\n        scores_tuple = tuple(row[[f'score_{i}' for i in range(len(scores))]].values)\n        return pd.Series(cc_pure(ids_tuple, scores_tuple, weights, top_k))\n\n    df[['cc_id', 'cc_score']] = df.apply(cc_pure_apply, axis=1)\n    return df['cc_id'].tolist(), df['cc_score'].tolist()"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    id_df = pd.DataFrame({f'id_{i}': id_list for i, id_list in enumerate(ids)})\n    score_df = pd.DataFrame({f'score_{i}': score_list for i, score_list in enumerate(scores)})\n    df = pd.concat([id_df, score_df], axis=1)\n\n    def cc_pure_apply(row):\n        ids_tuple = tuple(row[[f'id_{i}' for i in range(len(ids))]].values)\n        scores_tuple = tuple(row[[f'score_{i}' for i in range(len(scores))]].values)\n        return pd.Series(cc_pure(ids_tuple, scores_tuple, weights, top_k))\n\n    df[['cc_id', 'cc_score']] = df.apply(cc_pure_apply, axis=1)\n    return df['cc_id'].tolist(), df['cc_score'].tolist()"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    id_df = pd.DataFrame({f'id_{i}': id_list for i, id_list in enumerate(ids)})\n    score_df = pd.DataFrame({f'score_{i}': score_list for i, score_list in enumerate(scores)})\n    df = pd.concat([id_df, score_df], axis=1)\n\n    def cc_pure_apply(row):\n        ids_tuple = tuple(row[[f'id_{i}' for i in range(len(ids))]].values)\n        scores_tuple = tuple(row[[f'score_{i}' for i in range(len(scores))]].values)\n        return pd.Series(cc_pure(ids_tuple, scores_tuple, weights, top_k))\n\n    df[['cc_id', 'cc_score']] = df.apply(cc_pure_apply, axis=1)\n    return df['cc_id'].tolist(), df['cc_score'].tolist()"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    id_df = pd.DataFrame({f'id_{i}': id_list for i, id_list in enumerate(ids)})\n    score_df = pd.DataFrame({f'score_{i}': score_list for i, score_list in enumerate(scores)})\n    df = pd.concat([id_df, score_df], axis=1)\n\n    def cc_pure_apply(row):\n        ids_tuple = tuple(row[[f'id_{i}' for i in range(len(ids))]].values)\n        scores_tuple = tuple(row[[f'score_{i}' for i in range(len(scores))]].values)\n        return pd.Series(cc_pure(ids_tuple, scores_tuple, weights, top_k))\n\n    df[['cc_id', 'cc_score']] = df.apply(cc_pure_apply, axis=1)\n    return df['cc_id'].tolist(), df['cc_score'].tolist()\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    # Create a dictionary to store the weighted sum of scores for each ID\n    weighted_sum_dict = {}\n    for i in range(len(ids)):\n        for j in range(len(ids[i])):\n            if ids[i][j] not in weighted_sum_dict:\n                weighted_sum_dict[ids[i][j]] = 0\n            weighted_sum_dict[ids[i][j]] += scores[i][j] * weights[i]\n\n    # Sort the IDs based on their weighted sum in descending order\n    sorted_ids = sorted(weighted_sum_dict.items(), key=lambda x: x[1], reverse=True)\n\n    # Return the top K IDs and their corresponding scores\n    return [x[0] for x in sorted_ids[:top_k]], [x[1] for x in sorted_ids[:top_k]]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    # 1. Calculate the weighted sum of scores for each ID\n    weighted_sum = {}\n    for i in range(len(ids)):\n        for j in range(len(ids[i])):\n            weighted_sum[ids[i][j]] = weighted_sum.get(ids[i][j], 0) + scores[i][j] * weights[i]\n\n    # 2. Normalize the scores\n    total_score = sum(weighted_sum.values())\n    normalized_scores = {k: v / total_score for k, v in weighted_sum.items()}\n\n    # 3. Sort the IDs by their weighted sum in descending order\n    sorted_ids = sorted(normalized_scores.keys(), key=lambda x: normalized_scores[x], reverse=True)\n\n    # 4. Return the top K IDs and their corresponding scores\n    return sorted_ids[:top_k], [normalized_scores[k] for k in sorted_ids[:top_k]]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    # Create a dictionary to store the weighted sum of scores for each ID\n    id_score_dict = {}\n    for i in range(len(ids)):\n        for j in range(len(ids[i])):\n            if ids[i][j] not in id_score_dict:\n                id_score_dict[ids[i][j]] = scores[i][j] * weights[i]\n            else:\n                id_score_dict[ids[i][j]] += scores[i][j] * weights[i]\n\n    # Sort the dictionary based on the weighted sum of scores in descending order\n    sorted_id_score_dict = sorted(id_score_dict.items(), key=lambda x: x[1], reverse=True)\n\n    # Get the top K IDs and their corresponding weighted sums\n    top_k_ids = [item[0] for item in sorted_id_score_dict[:top_k]]\n    top_k_scores = [item[1] for item in sorted_id_score_dict[:top_k]]\n\n    return top_k_ids, top_k_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    # Normalize the scores\n    normalized_scores = [score / sum(score) for score in scores]\n\n    # Calculate the weighted sum of scores for each ID\n    weighted_sum_scores = {\n        id: sum([normalized_scores[i][j] * weights[i] for i, score in enumerate(scores) for j, id in enumerate(score) if\n                id == id])\n        for id in set.union(*[set(ids_list) for ids_list in ids])}\n\n    # Sort the IDs by their weighted sum in descending order\n    sorted_ids = sorted(weighted_sum_scores.items(), key=lambda x: x[1], reverse=True)\n\n    # Return the top K IDs and their corresponding scores\n    return [id for id, score in sorted_ids[:top_k]], [score for id, score in sorted_ids[:top_k]]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    # Create a DataFrame from the input data\n    df = pd.DataFrame({'id': ids[0], 'score': scores[0]})\n\n    # Iterate over the remaining categories or groups of IDs and scores\n    for i in range(1, len(ids)):\n        # Create a temporary DataFrame for the current category or group\n        temp_df = pd.DataFrame({'id': ids[i], 'score': scores[i]})\n\n        # Merge the temporary DataFrame with the main DataFrame using an outer join\n        df = pd.merge(df, temp_df, on='id', how='outer')\n\n    # Fill missing values with 0\n    df = df.fillna(0)\n\n    # Calculate the weighted sum of scores for each ID\n    df['weighted_sum'] = df.iloc[:, 1:].dot(weights)\n\n    # Normalize the weighted sum of scores\n    df['weighted_sum'] = df['weighted_sum'] / df['weighted_sum'].sum()\n\n    # Sort the DataFrame by the weighted sum in descending order\n    df = df.sort_values('weighted_sum', ascending=False)\n\n    # Return the top K IDs and their corresponding scores\n    return df['id'].head(top_k).tolist(), df['weighted_sum'].head(top_k).tolist()"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    # Create a dictionary to store the scores for each ID\n    score_dict = {id: 0 for id in set.union(*[set(id_list) for id_list in ids])}\n\n    # Iterate over each category or group\n    for i, id_list in enumerate(ids):\n        # Iterate over each ID in the current category or group\n        for j, id in enumerate(id_list):\n            # Add the weighted score to the corresponding ID in the dictionary\n            score_dict[id] += scores[i][j] * weights[i]\n\n    # Sort the IDs based on their weighted scores in descending order\n    sorted_ids = sorted(score_dict.keys(), key=lambda x: score_dict[x], reverse=True)\n\n    # Get the top K IDs and their corresponding scores\n    top_ids = sorted_ids[:top_k]\n    top_scores = [score_dict[id] for id in top_ids]\n\n    return top_ids, top_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    # 1. Create a dictionary to store the aggregated scores for each ID\n    aggregated_scores = {}\n\n    # 2. Iterate over each category or group\n    for i in range(len(ids)):\n        # 3. Get the IDs and scores for the current category or group\n        id_list = ids[i]\n        score_list = scores[i]\n\n        # 4. Iterate over each ID in the current category or group\n        for j in range(len(id_list)):\n            # 5. Get the current ID and score\n            id = id_list[j]\n            score = score_list[j]\n\n            # 6. If the ID is not in the aggregated_scores dictionary, add it with a score of 0\n            if id not in aggregated_scores:\n                aggregated_scores[id] = 0\n\n            # 7. Add the weighted score to the aggregated score for the current ID\n            aggregated_scores[id] += score * weights[i]\n\n    # 8. Sort the aggregated scores in descending order\n    sorted_scores = sorted(aggregated_scores.items(), key=lambda x: x[1], reverse=True)\n\n    # 9. Get the top K IDs and their corresponding scores\n    top_ids = [x[0] for x in sorted_scores[:top_k]]\n    top_scores = [x[1] for x in sorted_scores[:top_k]]\n\n    # 10. Return the top K IDs and their corresponding scores\n    return top_ids"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    # Create a dictionary to store the weighted sum of scores for each ID\n    weighted_sums = {}\n\n    # Iterate through each category or group\n    for i in range(len(ids)):\n        # Iterate through each ID in the current category or group\n        for j in range(len(ids[i])):\n            # Get the current ID and score\n            id = ids[i][j]\n            score = scores[i][j]\n\n            # If the ID is not in the dictionary, add it with its weighted score\n            if id not in weighted_sums:\n                weighted_sums[id] = score * weights[i]\n            # If the ID is already in the dictionary, add the weighted score to the existing value\n            else:\n                weighted_sums[id] += score * weights[i]\n\n    # Normalize the weighted sums by dividing them by the sum of all weighted sums\n    total_sum = sum(weighted_sums.values())\n    normalized_sums = {id: score / total_sum for id, score in weighted_sums.items()}\n\n    # Sort the IDs by their normalized weighted sums in descending order\n    sorted_ids = sorted(normalized_sums, key=normalized_sums.get, reverse=True)\n\n    # Return the top K IDs and their corresponding normalized weighted sums\n    return sorted_ids[:top_k], [normalized_sums[id] for id in sorted_ids[:top_k]]"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    # Initialize an empty dictionary to store the weighted sum of scores for each ID\n    weighted_sum_dict = {}\n\n    # Iterate over each category or group in the data\n    for i in range(len(ids)):\n        # Get the IDs and scores for the current category or group\n        id_list = ids[i]\n        score_list = scores[i]\n\n        # Iterate over each ID in the current category or group\n        for j in range(len(id_list)):\n            # Get the current ID and score\n            id = id_list[j]\n            score = score_list[j]\n\n            # If the ID is not in the dictionary, add it with the weighted score as the value\n            if id not in weighted_sum_dict:\n                weighted_sum_dict[id] = score * weights[i]\n            # If the ID is already in the dictionary, add the weighted score to the existing value\n            else:\n                weighted_sum_dict[id] += score * weights[i]\n\n    # Sort the dictionary by the weighted sum of scores in descending order\n    sorted_dict = sorted(weighted_sum_dict.items(), key=lambda x: x[1], reverse=True)\n\n    # Extract the top K IDs and their corresponding scores from the sorted dictionary\n    top_k_ids = [x[0] for x in sorted_dict[:top_k]]\n    top_k_scores = [x[1] for x in sorted_dict[:top_k]]\n\n    # Return the top K IDs and their corresponding scores as a tuple\n    return top_k_ids, top"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Check if the length of ids, scores, and weights are equal\n    assert len(ids) == len(scores) == len(weights), \"The length of ids, scores, and weights must be equal.\"\n\n    # Check if the sum of weights is equal to 1\n    assert sum(weights) == 1, \"The sum of weights must be equal to 1.\"\n\n    # Check if top_k is greater than 0\n    assert top_k > 0, \"top_k must be greater than 0.\"\n\n    # Check if the length of each list within ids and scores is equal\n    for i in range(len(ids)):\n        assert len(ids[i]) == len(scores[i]), \"The length of each list within ids and scores must be equal.\"\n\n    # Initialize an empty dictionary to store the weighted sum of scores for each ID\n    weighted_sum = {}\n\n    # Iterate over each ID and its corresponding scores and weights\n    for i, id_list in enumerate(ids):\n        for j, id in enumerate(id_list):\n            # If the ID is not in the dictionary, add it with its weighted score\n            if id not in weighted_sum:\n                weighted_sum[id] = scores[i][j] * weights[i]\n            # If the ID is already in the dictionary, add the weighted score to the existing value\n            else:\n                weighted_sum[id] += scores[i][j] * weights[i]\n\n    # Sort the dictionary based on the weighted sum in descending order\n    sorted_weighted_sum = sorted(weighted_sum.items(), key=lambda x: x[1], reverse=True)\n\n    # Return the top K IDs and their corresponding scores\n    return [x[0] for x in sorted_weighted_sum[:top_k]], [x[1] for x in sorted_weighted_sum[:top_k]]"}
{"namespace": "coord.track_linearize", "completion": "  if mean.shape[:-1] != cov.shape[:-2]:\n    raise ValueError(\n        f'mean.shape[:-1] {mean.shape}[:-1] != cov.shape[:-2] {cov.shape}[:-2].'\n    )\n  d = mean.shape[-1]\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n\n  if cov is not None:\n    # Compute the Jacobian of fn function at the locations of each mean.\n    jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n        jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n    )\n\n    # The cube root of the determinant of the Jacobian is the geometric mean\n    # of the eigenvalues of the Jacobian, which gives us the isotropic scaling\n    # implied by `fn` at each mean that `cov` should be multiplied by.\n    eps = jnp.finfo(jnp.float32).tiny  # Guard against an inf gradient at 0.\n    abs_det = jnp.maximum(eps, jnp.abs(jnp.linalg.det(jac)))\n    # Special case d == 3 for speed's sake.\n    fn_cov = cov * (jnp.cbrt(abs_det) if d == 3 else abs_det ** (1 / d))\n  else:\n    fn_cov = None\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  if mean.shape[:-1] != cov.shape[:-2]:\n    raise ValueError(\n        f'mean.shape[:-1] {mean.shape}[:-1] != cov.shape[:-2] {cov.shape}[:-2].'\n    )\n  d = mean.shape[-1]\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n\n  if cov is not None:\n    # Compute the Jacobian of fn function at the locations of each mean.\n    jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n        jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n    )\n\n    # The cube root of the determinant of the Jacobian is the geometric mean\n    # of the eigenvalues of the Jacobian, which gives us the isotropic scaling\n    # implied by `fn` at each mean that `scale` should be multiplied by.\n    eps = jnp.finfo(jnp.float32).tiny  # Guard against an inf gradient at 0.\n    abs_det = jnp.maximum(eps, jnp.abs(jnp.linalg.det(jac)))\n    # Special case d == 3 for speed's sake.\n    fn_cov = cov * (jnp.cbrt(abs_det) if d == 3 else abs_det ** (1 / d))\n  else:\n    fn_cov = None\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  d = mean.shape[-1]\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n\n  if cov is not None:\n    # Compute the Jacobian of fn function at the locations of each mean.\n    jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n        jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n    )\n\n    # The cube root of the determinant of the Jacobian is the geometric mean\n    # of the eigenvalues of the Jacobian, which gives us the isotropic scaling\n    # implied by `fn` at each mean that `scale` should be multiplied by.\n    eps = jnp.finfo(jnp.float32).tiny  # Guard against an inf gradient at 0.\n    abs_det = jnp.maximum(eps, jnp.abs(jnp.linalg.det(jac)))\n    # Special case d == 3 for speed's sake.\n    fn_cov = cov * (jnp.cbrt(abs_det) if d == 3 else abs_det ** (1 / d))\n  else:\n    fn_cov = None\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(jax.jacfwd(fn), in_axes=-1, out_axes=-1)(mean)\n\n  # Compute the transformed means and covariances.\n  fn_mean = fn(mean)\n  fn_cov = jnp.matmul(jac, jnp.matmul(cov, jnp.swapaxes(jac, -1, -2)))\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(jax.jacfwd(fn), in_axes=-1, out_axes=-1)(mean)\n\n  # Compute the mean and covariance of the linearized function.\n  fn_mean = fn(mean)\n  fn_cov = jnp.matmul(jac, jnp.matmul(cov, jnp.swapaxes(jac, -1, -2)))\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  d = mean.shape[-1]\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n\n  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n  )\n\n  # The cube root of the determinant of the Jacobian is the geometric mean\n  # of the eigenvalues of the Jacobian, which gives us the isotropic scaling\n  # implied by `fn` at each mean that `cov` should be multiplied by.\n  eps = jnp.finfo(jnp.float32).tiny  # Guard against an inf gradient at 0.\n  abs_det = jnp.maximum(eps, jnp.abs(jnp.linalg.det(jac)))\n  # Special case d == 3 for speed's sake.\n  fn_cov = cov * (jnp.cbrt(abs_det) if d == 3 else abs_det ** (1 / d))\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(jax.jacfwd(fn), in_axes=-1, out_axes=-1)(mean)\n\n  # Compute the mean and covariance of the transformed Gaussian.\n  fn_mean = fn(mean)\n  fn_cov = jnp.matmul(jac, jnp.matmul(cov, jnp.transpose(jac, axes=(0, 2, 1))))\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean.\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n\n  # Compute the Jacobian of the function at the locations of each mean.\n  jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(jnp.eye(mean.shape[-1]), mean.shape + (mean.shape[-1],))\n  )\n\n  # Transform the covariances using the Jacobian.\n  fn_cov = jnp.matmul(jnp.matmul(jac, cov), jnp.swapaxes(jac, -2, -1))\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean.\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n\n  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(jnp.eye(mean.shape[-1]), mean.shape + (mean.shape[-1],))\n  )\n\n  # Compute the transformed covariances.\n  fn_cov = jnp.matmul(jnp.matmul(jac, cov), jnp.swapaxes(jac, -1, -2))\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(jax.jacfwd(fn), in_axes=-1, out_axes=-1)(mean)\n\n  # Compute the transformed mean and covariance.\n  fn_mean = fn(mean)\n  fn_cov = jnp.matmul(jac, jnp.matmul(cov, jnp.swapaxes(jac, -1, -2)))\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Compute the Jacobian of the function at the locations of each mean.\n  jac = jax.vmap(jax.jacfwd(fn), in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(jnp.eye(mean.shape[-1]), mean.shape + (mean.shape[-1],))\n  )\n\n  # Compute the mean and covariance of the transformed Gaussian.\n  fn_mean = fn(mean)\n  fn_cov = jnp.matmul(jac, cov) @ jnp.moveaxis(jac, -1, -2)\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Apply the function to the mean.\n  fn_mean = fn(mean)\n\n  # Compute the Jacobian of the function at the locations of each mean.\n  jac = jax.vmap(jax.jacfwd(fn), in_axes=-1, out_axes=-1)(mean)\n\n  # Compute the transformed covariances.\n  fn_cov = jnp.matmul(jnp.matmul(jac, cov), jnp.swapaxes(jac, -1, -2))\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize fn around mean.\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n\n  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(jnp.eye(mean.shape[-1]), mean.shape + (mean.shape[-1],))\n  )\n\n  # Transform the covariances.\n  fn_cov = jnp.matmul(jnp.matmul(jac, cov), jnp.swapaxes(jac, -1, -2))\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(jax.jacfwd(fn), in_axes=-1, out_axes=-1)(mean)\n\n  # Compute the inverse of the Jacobian.\n  inv_jac = jnp.linalg.inv(jac)\n\n  # Apply the function to the mean.\n  fn_mean = fn(mean)\n\n  # Transform the covariance.\n  fn_cov = jnp.einsum('...ij,...jk,...kl->...il', inv_jac, cov, inv_jac)\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  d = mean.shape[-1]\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n  )\n\n  # The cube root of the determinant of the Jacobian is the geometric mean\n  # of the eigenvalues of the Jacobian, which gives us the isotropic scaling\n  # implied by `fn` at each mean that `scale` should be multiplied by.\n  eps = jnp.finfo(jnp.float32).tiny  # Guard against an inf gradient at 0.\n  abs_det = jnp.maximum(eps, jnp.abs(jnp.linalg.det(jac)))\n  # Special case d == 3 for speed's sake.\n  fn_scale = jnp.cbrt(abs_det) if d == 3 else abs_det ** (1 / d)\n  fn_cov = fn_scale[Ellipsis, None, None] * cov\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Apply the function to the mean and compute the Jacobian of the function at the mean.\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n  jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(jnp.eye(mean.shape[-1]), mean.shape + (mean.shape[-1],))\n  )\n\n  # Compute the transformed covariance by applying the function to the covariance and then transforming it using the Jacobian.\n  fn_cov = jnp.einsum('...ij,...jk,...lk->...il', jac, cov, jac)\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean.\n  lin_fn = jax.linearize(fn, mean)\n\n  # Compute the Jacobian of the function at the mean.\n  jac = lin_fn(jnp.eye(mean.shape[-1]))\n\n  # Compute the transformed mean.\n  fn_mean = lin_fn(mean)\n\n  # Compute the transformed covariance.\n  fn_cov = jnp.einsum('...ij,...jk,...kl->...il', jac, cov, jac)\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  d = mean.shape[-1]\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n\n  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n  )\n\n  # The cube root of the determinant of the Jacobian is the geometric mean\n  # of the eigenvalues of the Jacobian, which gives us the isotropic scaling\n  # implied by `fn` at each mean that `scale` should be multiplied by.\n  eps = jnp.finfo(jnp.float32).tiny  # Guard against an inf gradient at 0.\n  abs_det = jnp.maximum(eps, jnp.abs(jnp.linalg.det(jac)))\n  # Special case d == 3 for speed's sake.\n  fn_scale = jnp.sqrt(abs_det) if d == 3 else abs_det ** (1 / d)\n\n  # Compute the transformed covariances by applying the Jacobian to the\n  # covariances and scaling by the determinant.\n  fn_cov = jnp.matmul(jac, cov) * fn_scale[Ellipsis, None, None]\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  if mean.shape[:-1] != cov.shape[:-1]:\n    raise ValueError(\n        f'mean.shape[:-1] {mean.shape}[:-1] != cov.shape[:-1] {cov.shape}[:-1].'\n    )\n  d = mean.shape[-1]\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n\n  if cov is not None:\n    # Compute the Jacobian of fn function at the locations of each mean.\n    jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n        jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n    )\n\n    # The cube root of the determinant of the Jacobian is the geometric mean\n    # of the eigenvalues of the Jacobian, which gives us the isotropic scaling\n    # implied by `fn` at each mean that `scale` should be multiplied by.\n    eps = jnp.finfo(jnp.float32).tiny  # Guard against an inf gradient at 0.\n    abs_det = jnp.maximum(eps, jnp.abs(jnp.linalg.det(jac)))\n    # Special case d == 3 for speed's sake.\n    fn_scale = scale * (jnp.cbrt(abs_det) if d == 3 else abs_det ** (1 / d))\n\n    # The inverse of the Jacobian is the Jacobian of the inverse of fn.\n    inv_jac = jnp.linalg.inv(jac)\n\n    # The inverse of the covariance of the transformed distribution is the\n    # covariance of the transformed distribution.\n    fn_cov = jnp.matmul(inv_jac, cov, inv_jac.T)\n  else:\n    fn_scale = None\n    fn_cov = None\n  return fn_mean, fn_scale, fn_cov"}
{"namespace": "coord.track_linearize", "completion": "  if mean.shape[:-1] != cov.shape[:-2]:\n    raise ValueError(\n        f'mean.shape[:-1] {mean.shape}[:-1] != cov.shape[:-2] {cov.shape}[:-2].'\n    )\n  d = mean.shape[-1]\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n\n  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n  )\n\n  # The cube root of the determinant of the Jacobian is the geometric mean\n  # of the eigenvalues of the Jacobian, which gives us the isotropic scaling\n  # implied by `fn` at each mean that `scale` should be multiplied by.\n  eps = jnp.finfo(jnp.float32).tiny  # Guard against an inf gradient at 0.\n  abs_det = jnp.maximum(eps, jnp.abs(jnp.linalg.det(jac)))\n  # Special case d == 3 for speed's sake.\n  fn_scale = (\n      jnp.cbrt(abs_det) if d == 3 else abs_det ** (1 / d)\n  )  # (..., 1)\n\n  # Compute the inverse of the Jacobian of fn function at the locations of each mean.\n  inv_jac = jnp.linalg.inv(jac)\n\n  # Compute the inverse of the covariance matrix of the transformed Gaussian.\n  fn_cov = jnp.matmul(inv_jac.swapaxes(-2, -1), cov)\n  fn_cov = jnp.matmul(fn_cov, inv_jac)\n\n  # Multiply the inverse of the covariance matrix by the scaling factor.\n  fn_cov *= fn_scale**2\n\n  return fn_mean,"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for xi in x:\n        if xi.size > 1:\n            yield [xi[: xi.size // 2], xi[xi.size // 2 :]]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for xi in x:\n        if xi.size > 1:\n            yield [xi[: xi.size // 2], xi[xi.size // 2 :]]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for xi in x:\n        if xi.size > 1:\n            yield [xi[: xi.size // 2], xi[xi.size // 2 :]]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for xi in x:\n        if xi.size > 1:\n            yield [xi[: xi.size // 2], xi[xi.size // 2 :]]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for xi in x:\n        if xi.size > 1:\n            yield [xi[: xi.size // 2], xi[xi.size // 2 :]]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for xi in x:\n        if xi.size > 1:\n            yield [xi[: xi.size // 2], xi[xi.size // 2 :]]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for xi in x:\n        if xi.size > 1:\n            yield [xi[: xi.size // 2], xi[xi.size // 2 :]]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for arr in x:\n        if arr.size > 1:\n            yield [arr[: arr.size // 2], arr[arr.size // 2 :]]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for xi in x:\n        if xi.size > 1:\n            yield [xi[: xi.size // 2], xi[xi.size // 2 :]]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i, xi in enumerate(x):\n        if xi.shape[0] > 1:\n            yield [xi[: xi.shape[0] // 2], xi[xi.shape[0] // 2 :]]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in range(len(x)):\n        if x[i].size > 1:\n            yield [x[i][: x[i].size // 2], x[i][x[i].size // 2 :]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i, xi in enumerate(x):\n        if xi.size > 1:\n            n = xi.size // 2\n            yield [xi[:n], xi[n:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for xn in x:\n        if xn.size > 1:\n            xn = np.array_split(xn, 2)\n            yield xn"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for arr in x:\n        if arr.shape[0] > 1:\n            yield np.split(arr, 2)"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i, a in enumerate(x):\n        if a.size > 1:\n            mid = len(a) // 2\n            yield [a[:mid], a[mid:]]"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i, xi in enumerate(x):\n        if xi.size > 1:\n            yield [xi[: xi.size // 2], xi[xi.size // 2 :]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i, xi in enumerate(x):\n        if xi.size > 1:\n            xi = np.array_split(xi, 2)\n            yield [xi[0], xi[1]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for xn in x:\n        if xn.size > 1:\n            yield np.split(xn, 2)\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for arr in x:\n        if arr.size > 1:\n            mid = arr.size // 2\n            yield [arr[:mid], arr[mid:]]\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for arr in x:\n        if arr.size > 1:\n            yield [arr[: len(arr) // 2], arr[len(arr) // 2 :]]\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix must be square\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix must be square\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix must be square\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    n, m = x.shape\n    if n != m:\n        raise ValueError(f\"`x` must be a square matrix, got a {n}x{m} matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix must be square\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(\"`x` must be a 2D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\n            f\"`x` must be a square matrix, got a matrix of shape {x.shape}\"\n        )"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\n            f\"`x` must be a square matrix, got a {x.shape[0]}x{x.shape[1]} matrix\"\n        )"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  sin_x = jnp.sin(scaled_x)\n  cos_x = jnp.cos(scaled_x)\n  if append_identity:\n    return jnp.concatenate([sin_x, cos_x, x], axis=-1)\n  else:\n    return jnp.concatenate([sin_x, cos_x], axis=-1)"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  sin_x = jnp.sin(scaled_x)\n  if append_identity:\n    return jnp.concatenate([sin_x, x[Ellipsis, None]], axis=-1)\n  else:\n    return sin_x"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  sin_x = jnp.sin(scaled_x)\n  if append_identity:\n    return jnp.concatenate([sin_x, x[Ellipsis, None]], axis=-1)\n  else:\n    return sin_x"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  y = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  z = jnp.sin(jnp.concatenate([y, y + 0.5 * jnp.pi], axis=-1))\n  if append_identity:\n    z = jnp.concatenate([z, x[Ellipsis, None]], axis=-1)\n  return z"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  sin = jnp.sin(scaled_x)\n  cos = jnp.cos(scaled_x)\n  if append_identity:\n    return jnp.concatenate([x, sin, cos], axis=-1)\n  else:\n    return jnp.concatenate([sin, cos], axis=-1)"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  sin_x = jnp.sin(scaled_x)\n  if append_identity:\n    sin_x = jnp.concatenate([sin_x, x[Ellipsis, None]], axis=-1)\n  return sin_x"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  encoded = jnp.sin(scaled_x)\n  if append_identity:\n    encoded = jnp.concatenate([encoded, x[Ellipsis, None]], axis=-1)\n  return encoded"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  sin_x = jnp.sin(scaled_x)\n  cos_x = jnp.cos(scaled_x)\n  encoded = jnp.concatenate([sin_x, cos_x], axis=-1)\n  if append_identity:\n    encoded = jnp.concatenate([encoded, x], axis=-1)\n  return encoded"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  sin_x = jnp.sin(scaled_x)\n  cos_x = jnp.cos(scaled_x)\n  pos_enc = jnp.concatenate([sin_x, cos_x], axis=-1)\n  if append_identity:\n    pos_enc = jnp.concatenate([pos_enc, x[Ellipsis, None]], axis=-1)\n  return pos_enc"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  encoded = jnp.sin(scaled_x)\n  if append_identity:\n    encoded = jnp.concatenate([encoded, x[Ellipsis, None]], axis=-1)\n  return encoded"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  if append_identity:\n    return jnp.concatenate([jnp.sin(scaled_x), jnp.cos(scaled_x), x], axis=-1)\n  else:\n    return jnp.concatenate([jnp.sin(scaled_x), jnp.cos(scaled_x)], axis=-1)"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  x = jnp.sin(x)\n  if append_identity:\n    x = jnp.concatenate([x, x[Ellipsis, :1]], axis=-1)\n  return x"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  if append_identity:\n    return jnp.concatenate([scaled_x, scaled_x, x[Ellipsis, None]], axis=-1)\n  else:\n    return scaled_x"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  y = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  y = jnp.sin(jnp.concatenate([y, y + 0.5 * jnp.pi], axis=-1))\n  if append_identity:\n    y = jnp.concatenate([y, x[Ellipsis, None]], axis=-1)\n  return y"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  encoded = jnp.sin(\n      jnp.concatenate([scaled, scaled + 0.5 * jnp.pi], axis=-1)\n  )\n  if append_identity:\n    return jnp.concatenate([encoded, x], axis=-1)\n  else:\n    return encoded"}
{"namespace": "coord.pos_enc", "completion": "  # Compute the scaling factors for each degree.\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  # Reshape the input array to have an additional dimension for the scaling factors.\n  x_scaled = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], x.shape[:-1] + (-1,))\n  # Apply the sine function to the scaled input array.\n  x_sin = jnp.sin(x_scaled)\n  # Concatenate the original input array with the sine-transformed array if append_identity is True.\n  if append_identity:\n    x_sin = jnp.concatenate([x_sin, x[Ellipsis, None]], axis=-1)\n  # Return the result.\n  return x_sin"}
{"namespace": "coord.pos_enc", "completion": "  # Scale the input to be in the range [-pi, pi].\n  scaled_x = (2.0 * jnp.pi) * x\n\n  # Compute the sine of the scaled input.\n  sin_x = jnp.sin(scaled_x)\n\n  # If append_identity is True, concatenate the original input with the sine result.\n  if append_identity:\n    return jnp.concatenate([sin_x, x], axis=-1)\n  else:\n    return sin_x"}
{"namespace": "coord.pos_enc", "completion": "  # Scale the input array by 2^[min_deg, max_deg) and apply a sine function.\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  sin_x = jnp.sin(scaled_x)\n\n  # Optionally concatenate the original input array with the encoded features.\n  if append_identity:\n    sin_x = jnp.concatenate([x[Ellipsis, None], sin_x], axis=-1)\n\n  return sin_x"}
{"namespace": "coord.pos_enc", "completion": "  # Scale the input to the range [-pi, pi]\n  x_scaled = (2.0 ** min_deg) * x\n  # Apply sine function to each dimension of the input\n  x_sin = jnp.sin(x_scaled)\n  # Concatenate the original input with the encoded features if append_identity is True\n  if append_identity:\n    x_encoded = jnp.concatenate([x_sin, x], axis=-1)\n  else:\n    x_encoded = x_sin\n  # Return the encoded features\n  return x_encoded"}
{"namespace": "coord.pos_enc", "completion": "  # Compute the number of dimensions of the input\n  num_dims = x.shape[-1]\n\n  # Compute the number of frequencies to use in the encoding\n  num_freqs = max_deg - min_deg\n\n  # Compute the frequency bands for the encoding\n  freq_bands = 2.0 ** jnp.linspace(min_deg, max_deg, num_freqs)\n\n  # Compute the encoded features\n  x_enc = x[..., None] * freq_bands[None, Ellipsis]\n\n  # Apply the sine function to the encoded features\n  x_enc_sin = jnp.sin(jnp.concatenate([x_enc, x_enc + 0.5 * jnp.pi], axis=-1))\n\n  # Optionally append the original input to the encoded features\n  if append_identity:\n    x_enc_sin = jnp.concatenate([x_enc_sin, x[..., None].repeat(2, axis=-1)], axis=-1)\n\n  return x_enc_sin"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Check if field1.shape equals field2.shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(f\"{cls.__name__}: {field1} and {field2} length mismatch.\")\n        for array1, array2 in zip(values[field1], values[field2]):\n            if array1.shape != array2.shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Check if field1.shape equals field2.shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n        for arr1, arr2 in zip(values[field1], values[field2]):\n            if arr1.shape != arr2.shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Check if field1.shape equals field2.shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n        for arr1, arr2 in zip(values[field1], values[field2]):\n            if arr1.shape != arr2.shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Check if field1.shape equals field2.shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(f\"{cls.__name__}: {field1} and {field2} length mismatch.\")\n        for array1, array2 in zip(values[field1], values[field2]):\n            if array1.shape != array2.shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Check if field1.shape equals field2.shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(f\"{cls.__name__}: {field1} and {field2} length mismatch.\")\n\n        for array1, array2 in zip(values[field1], values[field2]):\n            if array1.shape != array2.shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if field1.shape equals field2.shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n        for arr1, arr2 in zip(values[field1], values[field2]):\n            if arr1.shape != arr2.shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Check if len(field1) equals len(field2) and if each pair of arrays have the same shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n        for array1, array2 in zip(values[field1], values[field2]):\n            if array1.shape != array2.shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Check if all shapes are equal.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for array1, array2 in zip(values[field1], values[field2]):\n            if array1.shape != array2.shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if field1.shape equals field2.shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(f\"{cls.__name__}: {field1} and {field2} length mismatch.\")\n\n        for array1, array2 in zip(values[field1], values[field2]):\n            if array1.shape != array2.shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Check if len(field1) equals len(field2) and if each corresponding pair of arrays has the same shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n        for arr1, arr2 in zip(values[field1], values[field2]):\n            if arr1.shape != arr2.shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if len(field1) equals len(field2) and if each array in field1 has the same shape as the corresponding array in field2.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n        for array1, array2 in zip(values[field1], values[field2]):\n            if array1.shape != array2.shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Check if len(field1) equals len(field2) and if each pair of arrays in field1 and field2 have the same shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for array1, array2 in zip(values[field1], values[field2]):\n            if array1.shape != array2.shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Check if len(field1) equals len(field2) and if each pair of array in field1 and field2 have the same shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for arr1, arr2 in zip(values[field1], values[field2]):\n            if arr1.shape != arr2.shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if len(field1) equals len(field2) and if field1[i].shape equals field2[i].shape for all i.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Check if all shapes are equal.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(f\"{cls.__name__}: {field1} and {field2} length mismatch.\")\n\n        for i, (array1, array2) in enumerate(zip(values[field1], values[field2])):\n            if array1.shape != array2.shape:\n                raise ValueError(f\"{cls.__name__}: {field1}[{i}] and {field2}[{i}] shape mismatch.\")\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"\n        This function is a Pydantic validator that checks if two lists of numpy arrays (specified by field names) have the same length and if each corresponding pair of arrays within these lists has the same shape.\n\n        Input-Output Arguments\n        :param cls: type. The class type of the Pydantic model being validated.\n        :param values: Dict[str, List[np.ndarray]]. A dictionary containing the values of the fields to be validated.\n        :return: Dict[str, List[np.ndarray]]. The validated values of the fields if the check passes.\n        \"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n        for arr1, arr2 in zip(values[field1], values[field2]):\n            if arr1.shape != arr2.shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"\n        This function is a Pydantic model validator that checks if two lists of numpy arrays (specified by field names) have the same length and if each corresponding pair of arrays within these lists has the same shape.\n\n        Input-Output Arguments\n        :param cls: type. The class type of the Pydantic model.\n        :param values: Dict[str, List[np.ndarray]]. A dictionary containing the values of the fields to be validated.\n        :return: Dict[str, List[np.ndarray]]. The validated values of the fields.\n\n        \"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n        for arr1, arr2 in zip(values[field1], values[field2]):\n            if arr1.shape != arr2.shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Check if len(field1) equals len(field2) and if each pair of arrays within the lists have the same shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for i, (array1, array2) in enumerate(zip(values[field1], values[field2])):\n            if array1.shape != array2.shape:\n                raise ValueError(\n                    f\"{cls.__name__}: {field1}[{i}] and {field2}[{i}] shape mismatch, \"\n                    f\"resp. {array1.shape} and {array2.shape}\"\n                )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Check if all shapes of field1 are equal to the shapes of field2.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n        for v1, v2 in zip(values[field1], values[field2]):\n            if v1.shape != v2.shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"\n        This function acts as a Pydantic model validator. It checks if two lists of numpy arrays (specified by field names) have the same length and if each corresponding pair of arrays within these lists has the same shape.\n\n        Input-Output Arguments\n        :param cls: type. The class type of the Pydantic model being validated.\n        :param values: Dict[str, List[np.ndarray]]. A dictionary containing the values of the fields to be validated. The keys are the field names and the values are lists of numpy arrays.\n        :return: Dict[str, List[np.ndarray]]. The validated values of the fields.\n        \"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for arr1, arr2 in zip(values[field1], values[field2]):\n            if arr1.shape != arr2.shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n\n        return values\n\n    return __root_validator"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize(camera.W, camera.H)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize(camera.W, camera.H)\n        self.render(camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize(camera.W, camera.H)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(camera)\n\n        # Reset the rendering context to its original size\n        eglctx.resize(eglctx.width, eglctx.height)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize(camera.W, camera.H)\n\n        # Render the mesh using the camera's settings\n        self.render(camera)\n\n        # Get the rendered image as a numpy array\n        image = eglctx.get_image()\n\n        return image"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize(camera.W, camera.H)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(camera)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize(camera.W, camera.H)\n\n        # Render the mesh using the camera's settings\n        self.render(camera)\n\n        # Reset the rendering context to its original size\n        eglctx.resize(eglctx.width, eglctx.height)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize(camera.W, camera.H)\n\n        # Render the mesh using the camera's settings\n        self.render(camera)\n\n        # Reset the rendering context to its original size\n        eglctx.resize(eglctx.W, eglctx.H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glUseProgram(self.point_program)\n            self.use_gl_program(self.point_program)\n        else:\n            gl.glUseProgram(self.mesh_program)\n            self.use_gl_program(self.mesh_program)\n\n        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize(camera.W, camera.H)\n\n        # Render the mesh using the camera's settings\n        self.upload_gl_uniforms(camera)\n        gl.glBindVertexArray(self.vao)\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glBindBuffer(gl.GL_ELEMENT_ARRAY_BUFFER, self.ebo)\n            gl.glDrawElements(gl.GL_LINES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of indices\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glBindBuffer(gl.GL_ELEMENT_ARRAY_BUFFER, self.ebo)\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of indices\n        elif self.render_type == Mesh.RenderType.QUADS:\n            gl.glBindBuffer(gl.GL_ELEMENT_ARRAY_BUFFER, self.ebo)\n            gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes."}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize(camera.W, camera.H)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(camera)\n\n        # Resize the rendering context back to its original dimensions\n        eglctx.resize(eglctx.width, eglctx.height)"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize(camera.W, camera.H)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize(camera.W, camera.H)\n\n        # Render the mesh using the camera's settings\n        self.render(camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize(camera.W, camera.H)\n\n        # Render the mesh using the camera's settings\n        self.render(camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize(camera.W, camera.H)\n\n        # Render the mesh using the camera's settings\n        self.render(camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize_context(camera.W, camera.H)\n\n        # Render the mesh using the camera's settings\n        self.render(camera)\n\n        # Reset the rendering context to its original size\n        eglctx.resize_context(eglctx.W, eglctx.H)\n\n        # Render the mesh using the camera's settings\n        self.render(camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize(camera.W, camera.H)\n\n        # Render the mesh using the camera's settings\n        self.render(camera)\n\n        # Get the rendered image as a numpy array\n        image = eglctx.get_image()\n\n        return image\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize(camera.W, camera.H)\n\n        # Render the mesh using the camera's settings\n        self.render(camera)\n\n        # Get the rendered image from the OpenGL framebuffer\n        image = gl.glReadPixels(0, 0, camera.W, camera.H, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE)\n\n        # Convert the image to a NumPy array\n        image = np.frombuffer(image, dtype=np.uint8)\n        image = image.reshape((camera.H, camera.W, 4))\n        image = image[:, :, :3]\n        image = image[::-1, :, :]\n        image = np.ascontiguousarray(image)\n\n        return image\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        eglctx.resize_context(camera.W, camera.H)\n        self.render(camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize_context(camera.W, camera.H)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize_context(camera.W, camera.H)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(camera)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize(camera.W, camera.H)\n\n        # Render the mesh using the camera's settings\n        self.render(camera)\n\n        # Get the rendered image from the rendering context\n        image = eglctx.get_image()\n\n        # Return the rendered image\n        return image\n"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object by copying the attributes from the BertConfig\n    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        # The following attributes do not have a direct equivalent in BertConfig\n        # and are set to commonly used defaults for NomicBertConfig\n        pad_token_id=0,\n        position_embedding_type=\"absolute\",\n        use_cache=True,\n    )\n\n    return nomic_config"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object by copying the attributes from the BertConfig\n    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        # The following attributes do not have a direct equivalent in BertConfig\n        # and are set to commonly used defaults for Nomic models\n        pad_token_id=0,\n        position_embedding_type=\"absolute\",\n        use_cache=True,\n    )\n\n    # Return the new NomicBertConfig object\n    return nomic_config"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        # The following attributes do not have a direct equivalent in BertConfig\n        # and are set to commonly used defaults for NomicBertConfig\n        pad_token_id=bert_config.pad_token_id,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n    )\n\n    return nomic_config"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        # The following attributes do not have a direct equivalent in BertConfig\n        # and are set to commonly used defaults for NomicBertConfig\n        orig_vocab_size=bert_config.vocab_size,\n        pad_vocab_size_multiple=1,\n        last_layer_subset=False,\n    )\n\n    return config"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        pad_token_id=bert_config.pad_token_id,\n        bos_token_id=bert_config.bos_token_id,\n        eos_token_id=bert_config.eos_token_id,\n        sep_token_id=bert_config.sep_token_id,\n        # The following attributes do not have a direct equivalent in BertConfig\n        # and are set to commonly used defaults for Nomic models\n        activation_function=\"gelu_new\",\n        resid_pdrop=0.0,\n        embd_pdrop=0.0,\n        attn_pdrop=0.0,\n        n_positions=bert_config.max_position_embeddings,\n        n_ctx=bert_config.max_position_embeddings,\n        n_embd=bert_config.hidden_size,\n        n_layer=bert_config.num_hidden_layers,\n        n_head=bert_config.num_attention_"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # The following attributes do not have a direct equivalent in NomicBertConfig\n    # and are set to commonly used defaults for BertConfig\n    config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        # The following attributes do not have a direct equivalent in BertConfig\n        # and are set to commonly used defaults for NomicBertConfig\n        pad_token_id=0,\n        position_embedding_type=\"absolute\",\n        use_cache=True,\n        orig_vocab_size=bert_config.vocab_size,\n        last_layer_subset=False,\n    )\n\n    return config"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object with the same values as the BertConfig\n    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        # Additional attributes specific to Nomic\n        orig_vocab_size=bert_config.vocab_size,\n        activation_function=bert_config.hidden_act,\n        resid_pdrop=bert_config.hidden_dropout_prob,\n        attn_pdrop=bert_config.attention_probs_dropout_prob,\n        n_positions=bert_config.max_position_embeddings,\n        n_embd=bert_config.hidden_size,\n        n_layer=bert_config.num_hidden_layers,\n        n_head=bert_config.num_attention_heads,\n        n_inner=bert_config.intermediate_size,\n        type_vocab_size=bert_config.type_vocab_size,\n        layer_norm_epsilon="}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object with the same settings as the BertConfig\n    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        # Additional arguments specific to the Nomic model\n        orig_vocab_size=bert_config.vocab_size,\n        last_layer_subset=False,\n        pad_vocab_size_multiple=1,\n    )\n\n    return nomic_config"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object with the same settings as the BertConfig\n    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        # Additional attributes specific to the Nomic model\n        n_inner=bert_config.intermediate_size,\n        n_positions=bert_config.max_position_embeddings,\n        resid_pdrop=bert_config.hidden_dropout_prob,\n        attn_pdrop=bert_config.attention_probs_dropout_prob,\n        n_embd=bert_config.hidden_size,\n        n_layer=bert_config.num_hidden_layers,\n        n_head=bert_config.num_attention_heads,\n        n_ctx=bert_config.max_position_embeddings,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        initializer_range=bert_config.initializer_range,\n        activation_function=bert_config.hidden"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Initialize a new NomicBertConfig object with the inherited settings from the BertConfig\n    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        # Additional configurations specific to the Nomic model\n        n_inner=bert_config.intermediate_size,\n        n_embd=bert_config.hidden_size,\n        n_layer=bert_config.num_hidden_layers,\n        n_head=bert_config.num_attention_heads,\n        n_positions=bert_config.max_position_embeddings,\n        resid_pdrop=bert_config.hidden_dropout_prob,\n        attn_pdrop=bert_config.attention_probs_dropout_prob,\n        embd_pdrop=bert_config.hidden_dropout_prob,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        initializer_range=bert_config.initializer_range,\n        output_attentions="}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # The following attributes do not have a direct equivalent in BertConfig\n    # and are set to commonly used defaults for Nomic\n    return NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        # The following attributes do not have a direct equivalent in BertConfig\n        # and are set to commonly used defaults for Nomic\n        orig_vocab_size=bert_config.vocab_size,\n        activation_function=bert_config.hidden_act,\n        resid_pdrop=bert_config.hidden_dropout_prob,\n        embd_pdrop=0.0,\n        attn_pdrop=bert_config.attention_probs_dropout_prob,\n        n_positions=bert_config.max_position_embeddings,\n        n_embd=bert_config.hidden_size,\n        n_layer=bert_config.num_hidden_layers,\n        n_head=bert_config.num_attention_heads,\n        n_inner=bert_config."}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Construct a new NomicBertConfig object with the same attributes as the BertConfig\n    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        # Add new attributes specific to the Nomic model\n        n_embd=bert_config.hidden_size,\n        n_layer=bert_config.num_hidden_layers,\n        n_head=bert_config.num_attention_heads,\n        n_inner=bert_config.intermediate_size,\n        activation_function=bert_config.hidden_act,\n        resid_pdrop=bert_config.hidden_dropout_prob,\n        embd_pdrop=bert_config.hidden_dropout_prob,\n        attn_pdrop=bert_config.attention_probs_dropout_prob,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        initializer_range=bert_config.initializer_range,\n        summary_type=\"cls_index\","}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object with the same values as the BertConfig\n    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n    )\n\n    # Set additional attributes specific to the Nomic model\n    nomic_config.last_layer_subset = False\n    nomic_config.orig_vocab_size = bert_config.vocab_size\n    nomic_config.pad_vocab_size_multiple = 1\n    nomic_config.num_labels = 1\n    nomic_config.problem_type = \"regression\"\n    nomic_config.label_smoothing_factor = 0.0\n    nomic_config.use_cache = False\n\n    return nomic_config"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Mapping from BertConfig attributes to NomicBertConfig attributes\n    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        pad_token_id=bert_config.pad_token_id,\n        bos_token_id=bert_config.bos_token_id,\n        eos_token_id=bert_config.eos_token_id,\n        sep_token_id=bert_config.sep_token_id,\n        # Additional Nomic-specific attributes\n        # TODO: Add more attributes as needed\n    )\n\n    return nomic_config"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object by copying the attributes from the BertConfig object\n    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        # The following attributes do not have a direct equivalent in BertConfig\n        # and are set to commonly used defaults for NomicBertConfig\n        pad_token_id=0,\n        position_embedding_type=\"absolute\",\n        use_cache=True,\n        # Additional attributes specific to NomicBertConfig\n        orig_vocab_size=bert_config.vocab_size,\n        pad_vocab_size_multiple=1,\n        use_flash_attn=True,\n        use_fused_mlp=True,\n        use_fused_dropout_add_ln=True,\n        use_fused_ln_rpe=True,\n        use_fused_attention_ln_rpe=True,\n        use_fused_attention_rpe=True,\n        use_fused_attention_rpe_qk=True,\n        use_fused_attention_rpe_mlp=True,\n        use_fused_attention_rpe_qkv=True"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object with inherited settings from the BertConfig\n    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        pad_token_id=bert_config.pad_token_id,\n        bos_token_id=bert_config.bos_token_id,\n        eos_token_id=bert_config.eos_token_id,\n        sep_token_id=bert_config.sep_token_id,\n        # The following attributes are specific to the Nomic model\n        # and not present in the original BertConfig\n        pad_vocab_size_multiple=bert_config.pad_vocab_size_multiple,\n        num_key_value_heads=bert_config.num_key_value_heads,\n        num_attention_heads_per_virtual_gpu=bert_config.num_attention_heads_per_virtual_gpu,\n        num_hidden_layers_per_virtual_gpu=bert_config.num_hidden_layers_per_virtual_"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Map the configuration attributes from BertConfig to NomicBertConfig\n    config_mapping = {\n        \"hidden_size\": \"n_embd\",\n        \"num_attention_heads\": \"n_head\",\n        \"num_hidden_layers\": \"n_layer\",\n        \"intermediate_size\": \"n_inner\",\n        \"hidden_act\": \"activation_function\",\n        \"max_position_embeddings\": \"n_positions\",\n        \"type_vocab_size\": \"type_vocab_size\",\n        \"initializer_range\": \"initializer_range\",\n        \"layer_norm_eps\": \"layer_norm_epsilon\",\n        \"vocab_size\": \"vocab_size\",\n        \"pad_token_id\": \"pad_token_id\",\n        \"position_embedding_type\": \"position_embedding_type\",\n        \"use_cache\": \"use_cache\",\n        \"torch_dtype\": \"torch_dtype\",\n        \"revision\": \"revision\",\n    }\n\n    # Create a new NomicBertConfig object with the mapped attributes\n    nomic_config = NomicBertConfig(\n        **{\n            nomic_attr: getattr(bert_config, bert_attr)\n            for bert_attr, nomic_attr in config_mapping.items()\n        }\n    )\n\n    # Set additional attributes that are specific to the Nomic model\n    nomic_config.n_head_kv = nomic_config.n_head\n    nomic_config.rotary = True\n    nomic_config.rotary_dim = 64\n    nomic_config.n_positions = 2048\n    nomic_config.n_ctx = 2048\n    nomic_config.n_embd = 4096\n    nomic_config.activation_function = \"gelu_new\"\n    nomic_config.resid_pdrop = 0.0\n    nomic_config.embd_pdrop = 0.0\n    nomic_config.attn_pdrop = 0"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object with inherited settings from the BertConfig\n    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        # The following attributes are specific to the Nomic model\n        pad_token_id=bert_config.pad_token_id,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        orig_vocab_size=bert_config.vocab_size,\n        last_layer_subset=False,\n        use_nomic_query_embedding=False,\n        use_nomic_token_type_embeddings=False,\n        use_nomic_position_embeddings=False,\n        use_nomic_embeddings=False,\n        use_nomic_layernorm=False,\n        use_nomic_mlp=False,\n        use_nomic_attention=False,\n        use_nomic_encoder=False,\n        use_nomic_decoder=False,\n        use_nomic_output_layer=False,\n        use_nomic_classifier=False,\n    )\n\n    return nomic_"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # In the BertConfig, some arguments are not used in the NomicBertConfig, so we remove them\n    # We also add some new arguments that are specific to the NomicBertConfig\n    # The new arguments are:\n    #   - last_layer_subset: bool. Whether to use the last layer subset. Default: False\n    #   - orig_vocab_size: int. The original vocab size. Default: 30522\n    #   - pad_vocab_size_multiple: int. The padding vocab size multiple. Default: 1\n    #   - activation_function: str. The activation function. Default: \"gelu\"\n    #   - resid_pdrop: float. The dropout probability for the residual. Default: 0.0\n    #   - embd_pdrop: float. The dropout probability for the embedding. Default: 0.0\n    #   - attn_pdrop: float. The dropout probability for the attention. Default: 0.0\n    #   - layer_norm_epsilon: float. The epsilon for the layer normalization. Default: 1e-5\n    #   - initializer_range: float. The initializer range. Default: 0.02\n    #   - output_past: bool. Whether to output the past. Default: True\n    #   - output_attentions: bool. Whether to output the attentions. Default: False\n    #   - output_hidden_states: bool. Whether to output the hidden states. Default: False\n    #   - use_cache: bool. Whether to use the cache. Default: True\n    #   - summary_type: str. The summary type. Default: \"cls_index\"\n    #   - summary_use_proj: bool. Whether to use the projection. Default: True\n    #   - summary_activation: str. The summary activation. Default: None\n    #   - summary_proj_to_labels: bool. Whether to project to labels. Default: True\n    #   - summary_first_dropout: float. The first dropout. Default: 0.1\n    #   - scale_attn_weights"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Inherit from BertConfig\n    config = NomicBertConfig.from_pretrained(\"bert-base-uncased\", torchscript=True)\n    config.update(bert_config.to_dict())\n\n    # Add new attributes\n    config.attention_type = \"global\"\n    config.hidden_dropout = 0.0\n    config.attention_probs_dropout = 0.0\n    config.inner_hidden_size = None\n    config.hidden_size_per_attention_head = None\n    config.hidden_act = \"gelu_new\"\n    config.layer_norm_eps = 1e-12\n    config.pre_ln = True\n    config.adaptive_input = False\n    config.adaptive_input_cutoff = None\n    config.adaptive_input_init_std = 0.02\n    config.adaptive_input_averaging = False\n    config.skip_init = True\n    config.qkv_bias = True\n    config.enable_torch_ref = False\n    config.tensor_parallel_output = False\n    config.tensor_parallel_input = False\n    config.sequence_parallel = False\n    config.gradient_checkpointing = False\n    config.enable_bias_skip_layer_norm = False\n    config.enable_bias_gelu_fusion = False\n    config.bias_gelu_fusion_use_hardware_fusion = False\n    config.bias_dropout_fusion = True\n    config.bias_dropout_add_fusion = True\n    config.apply_query_key_layer_scaling = True\n    config.attention_softmax_in_fp32 = True\n    config.fp32_residual_connection = True\n    config.preallocate_weights = False\n    config.apply_residual_connection_post_layernorm = False\n    config.qkv_no_dense = True\n    config.custom_all_reduce_op = False\n    config.pad_vocab_size_multiple = 1\n    config."}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            self.use_gl_program(self.mesh_program)\n\n        self.upload_gl_uniforms(camera)\n\n        gl.glBindVertexArray(self.vao)\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.QUADS:\n            gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        else:\n            raise NotImplementedError\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            self.use_gl_program(self.mesh_program)\n        self.upload_gl_uniforms(camera)\n\n        gl.glBindVertexArray(self.vao)\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of faces\n        elif self.render_type == Mesh.RenderType.QUADS:\n            gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of faces\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of faces\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            self.use_gl_program(self.mesh_program)\n\n        self.upload_gl_uniforms(camera)\n\n        gl.glBindVertexArray(self.vao)\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.TRIS:\n            if self.faces.dtype == torch.int32:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of faces\n            else:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_SHORT, ctypes.c_void_p(0))  # number of faces\n        elif self.render_type == Mesh.RenderType.QUADS:\n            if self.faces.dtype == torch.int32:\n                gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of faces\n            else:\n                gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_SHORT, ctypes.c_void_p(0))"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            self.use_gl_program(self.mesh_program)\n        self.upload_gl_uniforms(camera)\n\n        gl.glBindVertexArray(self.vao)\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of faces\n        elif self.render_type == Mesh.RenderType.QUADS:\n            gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of faces\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of faces\n        else:\n            raise NotImplementedError\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            self.use_gl_program(self.mesh_program)\n        self.upload_gl_uniforms(camera)\n\n        gl.glBindVertexArray(self.vao)\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.LINES:\n            if self.faces.dtype == torch.int32:\n                gl.glDrawElements(gl.GL_LINES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n            else:\n                gl.glDrawElements(gl.GL_LINES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_SHORT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.TRIS:\n            if self.faces.dtype == torch.int32:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n            else:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_SHORT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.QUADS:\n            if self.faces.dtype == torch.int32:\n                gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT,"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            self.use_gl_program(self.mesh_program)\n\n        self.upload_gl_uniforms(camera)\n        gl.glBindVertexArray(self.vao)\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.TRIS:\n            if len(self.faces):\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n            else:\n                gl.glDrawArrays(gl.GL_TRIANGLES, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.QUADS:\n            if len(self.faces):\n                gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n            else:\n                gl.glDrawArrays(gl.GL_QUADS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            if len(self.faces):\n                gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n            else:\n                gl.glDraw"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            self.use_gl_program(self.mesh_program)\n        self.upload_gl_uniforms(camera)\n\n        gl.glBindVertexArray(self.vao)\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawElements(gl.GL_LINES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.QUADS:\n            gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            self.use_gl_program(self.mesh_program)\n        self.upload_gl_uniforms(camera)\n\n        gl.glBindVertexArray(self.vao)\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.QUADS:\n            gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        else:\n            raise NotImplementedError\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            self.use_gl_program(self.mesh_program)\n\n        self.upload_gl_uniforms(camera)\n\n        gl.glBindVertexArray(self.vao)\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawElements(gl.GL_LINES, len(self.faces) * 2, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * 3, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.QUADS:\n            gl.glDrawElements(gl.GL_QUADS, len(self.faces) * 4, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * 3, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        else:\n            raise NotImplementedError\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            self.use_gl_program(self.mesh_program)\n\n        self.upload_gl_uniforms(camera)\n\n        gl.glBindVertexArray(self.vao)\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * 3, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.QUADS:\n            gl.glDrawElements(gl.GL_QUADS, len(self.faces) * 4, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * 3, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        else:\n            raise NotImplementedError\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            program = self.point_program\n        else:\n            program = self.mesh_program\n\n        self.use_gl_program(program)\n        self.upload_gl_uniforms(camera)\n\n        gl.glBindVertexArray(self.vao)\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.TRIS:\n            if self.faces.dtype == torch.int32:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n            else:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_SHORT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.QUADS:\n            if self.faces.dtype == torch.int32:\n                gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n            else:\n                gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_SHORT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            if self.faces.dtype =="}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            self.use_gl_program(self.mesh_program)\n        self.upload_gl_uniforms(camera)\n\n        gl.glBindVertexArray(self.vao)\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.TRIS:\n            if hasattr(self, 'faces'):\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of indices\n            else:\n                gl.glDrawArrays(gl.GL_TRIANGLES, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.QUADS:\n            if hasattr(self, 'faces'):\n                gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of indices\n            else:\n                gl.glDrawArrays(gl.GL_QUADS, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            if hasattr(self, 'faces'):\n                gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * self."}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            self.use_gl_program(self.mesh_program)\n\n        self.upload_gl_uniforms(camera)\n        gl.glBindVertexArray(self.vao)\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawElements(gl.GL_LINES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of faces\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of faces\n        elif self.render_type == Mesh.RenderType.QUADS:\n            gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of faces\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of faces\n        else:\n            raise NotImplementedError\n\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            self.use_gl_program(self.mesh_program)\n\n        self.upload_gl_uniforms(camera)\n\n        gl.glBindVertexArray(self.vao)\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.TRIS:\n            if self.faces is not None:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n            else:\n                gl.glDrawArrays(gl.GL_TRIANGLES, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.QUADS:\n            if self.faces is not None:\n                gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n            else:\n                gl.glDrawArrays(gl.GL_QUADS, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            if self.faces is not None:\n                gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * self.face_size, gl.GL_UNSIGN"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n        program = self.mesh_program if self.render_type != Mesh.RenderType.POINTS else self.point_program\n        self.use_gl_program(program)\n        self.upload_gl_uniforms(camera)\n\n        gl.glBindVertexArray(self.vao)\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawElements(gl.GL_LINES, len(self.faces) * 2, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * 3, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.QUADS:\n            gl.glDrawElements(gl.GL_QUADS, len(self.faces) * 4, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * 3, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        else:\n            raise NotImplementedError\n\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            program = self.point_program\n        else:\n            program = self.mesh_program\n        use_gl_program(program)\n        self.upload_gl_uniforms(camera)\n\n        gl.glBindVertexArray(self.vao)\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * 3, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of faces\n        elif self.render_type == Mesh.RenderType.QUADS:\n            gl.glDrawElements(gl.GL_QUADS, len(self.faces) * 4, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of faces\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * 3, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of faces\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            self.use_gl_program(self.mesh_program)\n\n        self.upload_gl_uniforms(camera)\n        gl.glBindVertexArray(self.vao)\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.TRIS:\n            if self.faces.dtype == torch.int32:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of faces\n            elif self.faces.dtype == torch.int64:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of faces\n            else:\n                raise NotImplementedError\n        elif self.render_type == Mesh.RenderType.QUADS:\n            if self.faces.dtype == torch.int32:\n                gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of faces\n            elif self.faces.dtype == torch.int64:\n                gl.glDrawElements(gl.GL_QUADS, len"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            self.use_gl_program(self.mesh_program)\n\n        self.upload_gl_uniforms(camera)\n\n        gl.glBindVertexArray(self.vao)\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawElements(gl.GL_LINES, len(self.faces) * 2, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of indices\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * 3, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of indices\n        elif self.render_type == Mesh.RenderType.QUADS:\n            gl.glDrawElements(gl.GL_QUADS, len(self.faces) * 4, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of indices\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * 3, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of indices\n        else:\n            raise NotImplementedError\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            self.use_gl_program(self.mesh_program)\n\n        self.upload_gl_uniforms(camera)\n\n        gl.glBindVertexArray(self.vao)\n\n        if self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawElements(gl.GL_LINES, len(self.faces) * 2, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * 3, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.QUADS:\n            gl.glDrawElements(gl.GL_QUADS, len(self.faces) * 4, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            for i in range(len(self.faces) - 1):\n                gl.glDrawElements(gl.GL_TRIANGLE_STRIP, 2, gl.GL_UNSIGNED_INT, ctypes.c_void_p(i * 2 * self.faces.element_size()))\n        elif self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        else:\n            raise NotImplementedError\n\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        # Select the shader program based on the render type\n        if self.render_type == Mesh.RenderType.POINTS:\n            program = self.point_program\n        else:\n            program = self.mesh_program\n\n        # Bind the shader program and upload uniforms\n        self.use_gl_program(program)\n        self.upload_gl_uniforms(camera)\n\n        # Bind the vertex array object and draw the mesh\n        gl.glBindVertexArray(self.vao)\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))  # number of vertices\n        elif self.render_type == Mesh.RenderType.TRIS:\n            if self.faces.dtype == torch.int32:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of faces\n            else:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_SHORT, ctypes.c_void_p(0))  # number of faces\n        elif self.render_type == Mesh.RenderType.QUADS:\n            if self.faces.dtype == torch.int32:\n                gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of faces\n            else:\n                gl.glDrawElements(gl.GL_QU"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n        w = w or self.W\n        h = h or self.H\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n        w = w or self.W\n        h = h or self.H\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n        w = w or self.W\n        h = h or self.H\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.cpu().numpy()\n        w = w or self.W\n        h = h or self.H\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        w = w or self.W\n        h = h or self.H\n        if ptr.shape[-1] == 3:\n            ptr = torch.cat([ptr, ptr.new_ones(ptr.shape[:-1] + (1,)) * 255], dim=-1)  # add alpha channel\n\n        old = gl.glGetInteger(gl.GL_TEXTURE_BINDING_2D)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr.data_ptr())\n        gl.glBindTexture(gl.GL_TEXTURE_2D, old)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if self.use_quad_cuda:\n            from cuda import cudart\n            if self.compose:\n                # Both reading and writing of this resource is required\n                flags = cudart.cudaGraphicsRegisterFlags.cudaGraphicsRegisterFlagsNone\n            else:\n                flags = cudart.cudaGraphicsRegisterFlags.cudaGraphicsRegisterFlagsWriteDiscard\n            try:\n                self.cu_tex = CHECK_CUDART_ERROR(cudart.cudaGraphicsGLRegisterImage(self.tex, gl.GL_TEXTURE_2D, flags))\n            except RuntimeError as e:\n                log(red('Failed to initialize Quad with CUDA-GL interop, will use slow upload: '), e)\n                self.use_quad_cuda = False\n\n        if not hasattr(self, 'cu_tex'):\n            self.init_texture()\n\n        w = w or self.W\n        h = h or self.H\n        if ptr.shape[-1] == 3:\n            ptr = torch.cat([ptr, ptr.new_ones(ptr.shape[:-1] + (1,)) * 255], dim=-1)  # add alpha channel\n\n        from cuda import cudart\n        kind = cudart.cudaMemcpyKind.cudaMemcpyHostToDevice\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsMapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n        cu_tex_arr = CHECK_CUDART_ERROR(cudart.cudaGraphicsSubResourceGetMappedArray(self.cu_tex, 0, 0))\n\n        if self.compose:\n            \"\"\"\n            Blit current framebuffer to this texture (self.tex)\n            Read content of this texture into a cuda buffer\n            Perform alpha blending based on the frame's alpha channel\n            Copy the blended image back into the texture (self.tex)\n            \"\"\"\n            old = gl.glGetInteger(gl.GL_DRAW_FRAMEBUFFER"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if ptr is None:\n            return\n\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        w = w or self.W\n        h = h or self.H\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        w = w or self.W\n        h = h or self.H\n        if ptr.shape[-1] == 3:\n            ptr = np.concatenate([ptr, np.ones_like(ptr[..., :1]) * 255], axis=-1)  # add alpha channel\n        ptr = np.asarray(ptr, dtype=np.uint8, order='C')\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        w = w or self.W\n        h = h or self.H\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.cpu().numpy()\n        if ptr.shape[-1] == 3:\n            ptr = np.concatenate([ptr, np.ones(ptr.shape[:-1] + (1,)) * 255], axis=-1)  # add alpha channel\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        w = w or self.W\n        h = h or self.H\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n        assert ptr.shape[-1] == 4, \"Only RGBA is supported\"\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n        ptr = np.asarray(ptr, dtype=np.uint8, order='C')\n\n        w = w or self.W\n        h = h or self.H\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        w = w or self.W\n        h = h or self.H\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n        ptr = np.asarray(ptr, dtype=np.uint8, order='C')\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        w = w or self.W\n        h = h or self.H\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n        ptr = np.asarray(ptr, dtype=np.uint8, order='C')\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        w = w or self.W\n        h = h or self.H\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n        assert ptr.shape[-1] == 4, \"Only RGBA textures are supported\"\n        ptr = np.asarray(ptr, dtype=np.uint8, order='C')\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        w = w or self.W\n        h = h or self.H\n        if ptr.shape[-1] == 3:\n            ptr = torch.cat([ptr, ptr.new_ones(ptr.shape[:-1] + (1,)) * 255], dim=-1)  # add alpha channel\n\n        # Convert the PyTorch tensor to a numpy array\n        ptr = ptr.detach().cpu().numpy()\n\n        # Upload the numpy array to the texture\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n\n        # Unbind the texture\n        gl.glBindTexture(gl.GL_TEXTURE_2D, 0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        # Convert the input data to a numpy array if it is a PyTorch tensor\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        # Ensure the input data is a numpy array\n        assert isinstance(ptr, np.ndarray), \"Input data must be a numpy array or a PyTorch tensor\"\n\n        # Reshape the input data to match the dimensions of the texture\n        ptr = ptr.reshape((h, w, -1))\n\n        # Upload the data to the texture\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        # Convert the input tensor to a numpy array if it is a PyTorch tensor\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        # Determine the width and height of the portion of the texture to be updated\n        w = w or self.W\n        h = h or self.H\n\n        # Upload the data to the texture using OpenGL\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        # Convert the input data to a numpy array if it is a PyTorch tensor\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        # Get the width and height of the texture if not provided\n        w = w or self.W\n        h = h or self.H\n\n        # Get the dimensions of the input data\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        # Bind the texture and upload the data to it\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n\n        # Restore the original viewport and scissor settings\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        w = w or self.W\n        h = h or self.H\n        if ptr.shape[-1] == 3:\n            ptr = np.concatenate([ptr, np.ones_like(ptr[..., :1]) * 255], axis=-1)  # add alpha channel\n        ptr = np.asarray(ptr, dtype=np.uint8, order='C')  # H, W, 4\n\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        # Convert the input to a numpy array if it is a PyTorch tensor\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        # Ensure the input is a numpy array\n        assert isinstance(ptr, np.ndarray), f\"Input must be a numpy array or a PyTorch tensor, got {type(ptr)}\"\n\n        # Get the dimensions of the input array\n        h, w, c = ptr.shape\n\n        # If width and height are not provided, use the object's dimensions\n        w = w or self.W\n        h = h or self.H\n\n        # Convert the input array to a byte array\n        ptr = ptr.astype(np.uint8)\n\n        # Upload the texture data to the GPU\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n\n        # Unbind the texture\n        gl.glBindTexture(gl.GL_TEXTURE_2D, 0)\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    if len(R.shape) != 3:\n        raise ValueError(\"Expected R to be of shape (N, 3, 3). Got {}\".format(R.shape))\n    if len(tvec.shape) != 2:\n        raise ValueError(\"Expected tvec to be of shape (N, 3). Got {}\".format(tvec.shape))\n    if len(camera_matrix.shape) != 3:\n        raise ValueError(\"Expected camera_matrix to be of shape (N, 3, 3). Got {}\".format(camera_matrix.shape))\n    if len(image_size.shape) != 2:\n        raise ValueError(\"Expected image_size to be of shape (N, 2). Got {}\".format(image_size.shape))\n    if R.shape[0] != tvec.shape[0]:\n        raise ValueError(\"Expected R and tvec to have the same batch dimension. Got {} and {}\".format(R.shape[0], tvec.shape[0]))\n    if R.shape[0] != camera_matrix.shape[0]:\n        raise ValueError(\"Expected R and camera_matrix to have the same batch dimension. Got {} and {}\".format(R.shape[0], camera_matrix.shape[0]))\n    if R.shape[0] != image_size.shape[0]:\n        raise ValueError(\"Expected R and image_size to have the same batch dimension. Got {} and {}\".format(R.shape[0], image_size.shape[0]))\n    if R.shape[1:] != (3, 3):\n        raise ValueError(\"Expected R to be of shape (N, 3, 3). Got {}\".format(R.shape))\n    if tvec.shape[1:] != (3,):\n        raise ValueError(\"Expected tvec to be of shape (N, 3). Got {}\".format(tvec.shape))\n    if camera_matrix.shape[1:] != (3, 3):\n        raise ValueError(\"Expected camera_matrix to be of shape (N, 3, 3). Got {}\".format(camera_matrix"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    batch_size = R.shape[0]\n    assert R.shape == (batch_size, 3, 3), f\"Expected R to have shape ({batch_size}, 3, 3), but got {R.shape}\"\n    assert tvec.shape == (batch_size, 3, 1), f\"Expected tvec to have shape ({batch_size}, 3, 1), but got {tvec.shape}\"\n    assert camera_matrix.shape == (batch_size, 3, 3), f\"Expected camera_matrix to have shape ({batch_size}, 3, 3), but got {camera_matrix.shape}\"\n    assert image_size.shape == (batch_size, 2), f\"Expected image_size to have shape ({batch_size}, 2), but got {image_size.shape}\"\n\n    # Calculate focal lengths\n    fx = camera_matrix[..., 0, 0]\n    fy = camera_matrix[..., 1, 1]\n    fx_fy_mean = (fx + fy) / 2\n    fx_ratio = fx / fx_fy_mean\n    fy_ratio = fy / fx_fy_mean\n    if torch.any(torch.abs(fx_ratio - fy_ratio) > 0.01):\n        warn_once_about_pulsar_fxfy()\n\n    # Calculate principal point offsets\n    cx = camera_matrix[..., 0, 2]\n    cy = camera_matrix[..., 1, 2]\n    w = image_size[..., 0]\n    h = image_size[..., 1]\n    px_offset = (cx - w / 2) / w\n    py_offset = (cy - h / 2) / h\n\n    # Calculate camera position\n    cam_pos = -R.transpose(-2, -1) @ tvec\n\n    # Calculate camera rotation\n    cam_rot = matrix_to_rotation_6d(R.transpose(-2, -1))"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    if len(R.shape) == 2:\n        R = R[None]\n    if len(tvec.shape) == 2:\n        tvec = tvec[None]\n    if len(camera_matrix.shape) == 2:\n        camera_matrix = camera_matrix[None]\n    if len(image_size.shape) == 1:\n        image_size = image_size[None, :]\n\n    # Focal length\n    fx = camera_matrix[..., 0, 0] / 2\n    fy = camera_matrix[..., 1, 1] / 2\n    if not torch.allclose(fx, fy, rtol=0.01):\n        warn_once_about_pulsar_fxfy()\n    f = (fx + fy) / 2\n    fx = fx / image_size[..., 0] * 2\n    fy = fy / image_size[..., 1] * 2\n\n    # Principal point offset\n    cx = camera_matrix[..., 0, 2] - image_size[..., 0] / 2\n    cy = camera_matrix[..., 1, 2] - image_size[..., 1] / 2\n    cx = cx / image_size[..., 0]\n    cy = cy / image_size[..., 1]\n\n    # Sensor size\n    sensor_width = 2 * f * torch.sqrt(1 + cx ** 2 + cy ** 2) / torch.sqrt(1 + (1 - cx ** 2 - cy ** 2) / (fx ** 2))\n\n    # Camera position\n    cam_pos = -R.transpose(-2, -1) @ tvec\n\n    # Camera rotation\n    cam_rot = matrix_to_rotation_6d(R.transpose(-2, -1))\n\n    # Near and far clipping plane\n    znear = torch.full_like(cam_pos[..., :1], znear)\n    zfar = znear +"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    if R.ndim == 2:\n        R = R[None]\n    if tvec.ndim == 2:\n        tvec = tvec[None]\n    if camera_matrix.ndim == 2:\n        camera_matrix = camera_matrix[None]\n    if image_size.ndim == 1:\n        image_size = image_size[None, None]\n\n    batch_size = R.shape[0]\n    focal_length = camera_matrix[..., 0:2, 0:2].mean(dim=(-2, -1))\n    principal_point = camera_matrix[..., 0:2, 2]\n    camera_position = -R.transpose(-1, -2) @ tvec\n\n    camera_params = torch.cat(\n        [\n            camera_position,\n            matrix_to_rotation_6d(R),\n            focal_length,\n            principal_point,\n            image_size,\n            torch.tensor([znear] * batch_size, dtype=R.dtype, device=R.device),\n        ],\n        dim=-1,\n    )\n\n    return camera_params"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure inputs are batched and valid\n    assert R.ndim == 3 and R.shape[-2:] == (3, 3), f\"R must be a batch of 3x3 matrices, but got {R.shape}\"\n    assert tvec.ndim == 3 and tvec.shape[-1] == 3, f\"tvec must be a batch of 3-vectors, but got {tvec.shape}\"\n    assert camera_matrix.ndim == 3 and camera_matrix.shape[-2:] == (3, 3), f\"camera_matrix must be a batch of 3x3 matrices, but got {camera_matrix.shape}\"\n    assert image_size.ndim == 2 and image_size.shape[-1] == 2, f\"image_size must be a batch of 2-vectors, but got {image_size.shape}\"\n\n    # Extract fx, fy, cx, cy from camera_matrix\n    fx = camera_matrix[..., 0, 0]\n    fy = camera_matrix[..., 1, 1]\n    cx = camera_matrix[..., 0, 2]\n    cy = camera_matrix[..., 1, 2]\n\n    # Ensure fx and fy are close\n    if not torch.allclose(fx, fy, rtol=0.01):\n        warn_once_about_pulsar_fxfy()\n\n    # Compute focal length and principal point\n    focal_length = (fx + fy) * 0.5\n    principal_point = torch.stack([cx, cy], dim=-1)\n\n    # Compute camera position\n    camera_position = -R.transpose(-2, -1) @ tvec\n\n    # Compute camera rotation\n    camera_rotation = matrix_to_rotation_6d(R)\n\n    # Compute sensor width\n    sensor_width = 2 * focal_length.abs().tan() * znear\n\n    # Compute image size\n    image_size = image_size.flip(-1)\n\n    # Stack camera"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    assert R.shape[-2:] == (3, 3), \"R must be of shape (*, 3, 3)\"\n    assert tvec.shape[-1] == 3, \"tvec must be of shape (*, 3)\"\n    assert camera_matrix.shape[-2:] == (3, 3), \"camera_matrix must be of shape (*, 3, 3)\"\n    assert image_size.shape[-1] == 2, \"image_size must be of shape (*, 2)\"\n    assert znear > 0.0, \"znear must be greater than 0.0\"\n\n    # Extract fx, fy, cx, cy from camera_matrix\n    fx = camera_matrix[..., 0, 0]\n    fy = camera_matrix[..., 1, 1]\n    cx = camera_matrix[..., 0, 2]\n    cy = camera_matrix[..., 1, 2]\n\n    # Compute focal lengths for x and y directions\n    fx = fx.mean()\n    fy = fy.mean()\n\n    # Compute principal point offsets for x and y directions\n    cx_offset = (cx - image_size[..., 0] / 2.0) / image_size[..., 0]\n    cy_offset = (cy - image_size[..., 1] / 2.0) / image_size[..., 1]\n\n    # Compute camera position\n    cam_pos = -torch.matmul(R.transpose(-1, -2), tvec[..., None])[..., 0]\n\n    # Compute camera rotation\n    cam_rot = matrix_to_rotation_6d(R)\n\n    # Compute camera parameters\n    cam_params = torch.cat(\n        [cam_pos, cam_rot, fx[..., None], fy[..., None], cx_offset[..., None], cy_offset[..., None], znear * torch.ones_like(fx[..., None])],\n        dim=-1"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Check if the input tensors are batched and have the correct shapes\n    assert R.ndim >= 3 and R.shape[-2:] == (3, 3), f\"R should be a batch of rotation matrices of shape (*, 3, 3), but got {R.shape}\"\n    assert tvec.ndim >= 2 and tvec.shape[-1] == 3, f\"tvec should be a batch of translation vectors of shape (*, 3), but got {tvec.shape}\"\n    assert camera_matrix.ndim >= 3 and camera_matrix.shape[-2:] == (3, 3), f\"camera_matrix should be a batch of camera intrinsic matrices of shape (*, 3, 3), but got {camera_matrix.shape}\"\n    assert image_size.ndim >= 1 and image_size.shape[-1] == 2, f\"image_size should be a batch of image sizes of shape (*, 2), but got {image_size.shape}\"\n\n    # Extract focal lengths and principal points from camera matrix\n    fx = camera_matrix[..., 0, 0]\n    fy = camera_matrix[..., 1, 1]\n    cx = camera_matrix[..., 0, 2]\n    cy = camera_matrix[..., 1, 2]\n\n    # Compute sensor width\n    sensor_width = 2 * (cx / fx).abs()\n\n    # Compute rotation in 6D representation\n    rotation_6d = matrix_to_rotation_6d(R)\n\n    # Compute camera position\n    camera_position = -(R.transpose(-2, -1) @ tvec)\n\n    # Compute camera parameters\n    camera_params = torch.cat([camera_position, rotation_6d, sensor_width.unsqueeze(-1)], dim=-1)\n\n    # Check if the computed camera parameters have the correct shape\n    assert camera_params.shape[-1] == 9, f\"camera_params should have shape (*, 9), but got {camera_params.shape}\"\n\n    return camera_params"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    batch_size = R.shape[0]\n    assert R.shape == (batch_size, 3, 3)\n    assert tvec.shape == (batch_size, 3, 1)\n    assert camera_matrix.shape == (batch_size, 3, 3)\n    assert image_size.shape == (batch_size, 2)\n\n    # focal lengths\n    fx = camera_matrix[..., 0, 0]\n    fy = camera_matrix[..., 1, 1]\n\n    # principal points\n    px = camera_matrix[..., 0, 2]\n    py = camera_matrix[..., 1, 2]\n\n    # sensor width\n    sw = image_size[..., 0]\n\n    # calculate focal length from fx and fy\n    f = (fx + fy) / 2.0\n\n    # calculate principal point offset from px and py\n    px_offset = (px - sw / 2.0) / f\n    py_offset = (py - sw / 2.0) / f\n\n    # calculate camera position from tvec\n    c = -R.transpose(-1, -2) @ tvec\n\n    # calculate rotation from R\n    rot = matrix_to_rotation_6d(R)\n\n    # calculate camera parameters\n    camera_params = torch.cat(\n        [\n            f,\n            px_offset,\n            py_offset,\n            c,\n            rot,\n        ],\n        dim=-1,\n    )\n\n    return camera_params"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Validate input shapes and values\n    assert R.shape[-2:] == (3, 3), \"Invalid rotation matrix shape\"\n    assert tvec.shape[-1] == 3, \"Invalid translation vector shape\"\n    assert camera_matrix.shape[-2:] == (3, 3), \"Invalid camera matrix shape\"\n    assert image_size.shape == (2,), \"Invalid image size\"\n\n    # Extract focal lengths from camera matrix\n    fx = camera_matrix[..., 0, 0]\n    fy = camera_matrix[..., 1, 1]\n\n    # Calculate principal point offsets\n    cx = camera_matrix[..., 0, 2] - image_size[0] / 2.0 + 0.5\n    cy = camera_matrix[..., 1, 2] - image_size[1] / 2.0 + 0.5\n\n    # Calculate average focal length and principal point\n    f = (fx + fy) * 0.5\n    cx = (cx + cy) * 0.5\n    cy = cx\n\n    # Calculate camera position\n    cam_pos = -torch.matmul(R.transpose(-1, -2), tvec[..., None])[..., 0]\n\n    # Convert rotation matrix to 6D representation\n    rot = matrix_to_rotation_6d(R)\n\n    # Calculate camera direction\n    cam_dir = R[..., :3, 2]\n\n    # Calculate camera up vector\n    cam_up = R[..., :3, 1]\n\n    # Calculate horizontal field of view (in radians)\n    h_fov = 2.0 * torch.atan(image_size[0] / (2.0 * f))\n\n    # Calculate vertical field of view (in radians)\n    v_fov = 2.0 * torch.atan(image_size[1] / (2.0 * f))\n\n    # Calculate near and far clipping planes\n    znear = znear\n    zfar = z"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Validate inputs\n    assert R.shape[-2:] == (3, 3), \"Invalid rotation matrix shape\"\n    assert tvec.shape[-1] == 3, \"Invalid translation vector shape\"\n    assert camera_matrix.shape[-2:] == (3, 3), \"Invalid camera matrix shape\"\n    assert image_size.shape == (2,), \"Invalid image size\"\n\n    # Extract focal lengths from camera matrix\n    fx = camera_matrix[..., 0, 0]\n    fy = camera_matrix[..., 1, 1]\n\n    # Calculate principal point offset\n    cx = camera_matrix[..., 0, 2] - image_size[0] / 2 + 0.5\n    cy = camera_matrix[..., 1, 2] - image_size[1] / 2 + 0.5\n\n    # Calculate sensor width\n    sensor_width = 2 * (fx + cx).abs().atan().mul(2).tan()\n\n    # Check if focal lengths are close to each other\n    if (fx - fy).abs() / fy > 0.01:\n        warn_once_about_pulsar_fxfy()\n\n    # Calculate average focal length\n    f = (fx + fy) * 0.5\n\n    # Calculate rotation in 6D representation\n    rot = matrix_to_rotation_6d(R)\n\n    # Calculate camera position\n    campos = -(R.transpose(-1, -2) @ tvec)\n\n    # Stack all parameters together\n    camparams = torch.cat([campos, rot, f, cx, cy, sensor_width, znear * torch.ones_like(znear)], dim=-1)\n\n    return camparams"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    assert R.ndim == 3 and R.shape[-2:] == (3, 3), \"Invalid rotation matrix shape\"\n    assert tvec.ndim == 3 and tvec.shape[-1:] == (3,), \"Invalid translation vector shape\"\n    assert camera_matrix.ndim == 3 and camera_matrix.shape[-2:] == (3, 3), \"Invalid camera matrix shape\"\n    assert image_size.ndim == 2 and image_size.shape[-1] == 2, \"Invalid image size shape\"\n\n    # Extract focal lengths and principal point offsets from camera matrix\n    fx, fy = camera_matrix[..., 0, 0], camera_matrix[..., 1, 1]\n    cx, cy = camera_matrix[..., 0, 2], camera_matrix[..., 1, 2]\n\n    # Normalize focal lengths and principal point offsets\n    image_size = image_size.float()\n    fx = fx / image_size[..., 0]\n    fy = fy / image_size[..., 1]\n    cx = 2 * (cx / image_size[..., 0]) - 1.0\n    cy = 2 * (cy / image_size[..., 1]) - 1.0\n\n    # Average focal lengths for x and y if they differ by more than 1%\n    f_mean = (fx + fy) / 2.0\n    f_diff = torch.abs(fx - fy) / f_mean\n    if (f_diff > 0.01).any():\n        warn_once_about_pulsar_fxfy()\n    fx = f_mean\n    fy = f_mean\n\n    # Convert rotation matrix to 6D representation\n    rotation = matrix_to_rotation_6d(R)\n\n    # Invert translation vector to obtain camera position\n    position = -(R @ tvec)\n\n    # Calculate camera parameters\n    camera_params = torch.cat([position, rotation, fx, fy, cx, cy], dim=-1)"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    if len(R.shape) != 3 or R.shape[-1] != 3 or R.shape[-2] != 3:\n        raise ValueError(\"Expected R to have shape (N, 3, 3), but got {}\".format(R.shape))\n\n    if len(tvec.shape) != 2 or tvec.shape[-1] != 3:\n        raise ValueError(\"Expected tvec to have shape (N, 3), but got {}\".format(tvec.shape))\n\n    if len(camera_matrix.shape) != 2 or camera_matrix.shape[-1] != 3 or camera_matrix.shape[-2] != 3:\n        raise ValueError(\"Expected camera_matrix to have shape (N, 3, 3), \"\n                         \"but got {}\".format(camera_matrix.shape))\n\n    if len(image_size.shape) != 1 or image_size.numel() != 2:\n        raise ValueError(\"Expected image_size to have shape (2,), but got {}\".format(image_size.shape))\n\n    if znear <= 0.0:\n        raise ValueError(\"Expected znear to be positive, but got {}\".format(znear))\n\n    # The principal point offset determines the center of projection.\n    # By convention, we assume that the principal point is at the image center.\n    # However, the principal point offset is not necessarily equal to the image center.\n    # The principal point offset is given by the (cx, cy) vector, where cx and cy are the x and y coordinates of the principal point, respectively.\n    # The principal point offset is used to correct for the difference between the image center and the center of projection.\n    # The principal point offset is given by the (cx, cy) vector, where cx and cy are the x and y coordinates of the principal point, respectively.\n    # The principal point offset is used to correct for the difference between the image center and the center of projection.\n    # The principal point offset is given by the (cx, cy) vector, where cx and cy are the x and y coordinates of the principal point, respectively.\n    #"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    if R.dim() == 2:\n        R = R[None]\n    if tvec.dim() == 1:\n        tvec = tvec[None, :, None]  # (1, 3, 1)\n    if camera_matrix.dim() == 2:\n        camera_matrix = camera_matrix[None]\n    if image_size.dim() == 1:\n        image_size = image_size[None, :, None]  # (1, 2, 1)\n\n    focal_length = camera_matrix[..., 0:1, 0:1]\n    principal_point = camera_matrix[..., 0:2, 2:3]\n    image_size_wh = image_size.flip(-1)\n\n    # flipping x-y axes safety check\n    if focal_length.shape != principal_point.shape:\n        raise ValueError(\"Expected focal_length and principal_point to have \"\n                         \"the same shape but got {} and {}\".format(focal_length.shape, principal_point.shape))\n    if focal_length.shape != image_size_wh.shape:\n        raise ValueError(\"Expected focal_length and image_size_wh to have \"\n                         \"the same shape but got {} and {}\".format(focal_length.shape, image_size_wh.shape))\n    if (focal_length[..., 0, 0] * image_size_wh[..., 0, 0]).abs().max() > 1e-4:\n        warn_once_about_pulsar_fxfy()\n    if (focal_length[..., 1, 0] * image_size_wh[..., 1, 0]).abs().max() > 1e-4:\n        warn_once_about_pulsar_fxfy()\n\n    # adjust focal length to match to given image_size\n    focal_length[..., 0, 0] = focal_length[..., 0, 0] * image_size_wh[..., 0, 0]\n    focal_length[..., "}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    batch_size = R.shape[0]\n    if R.shape != (batch_size, 3, 3):\n        raise ValueError(\"Expected R to have shape (batch_size, 3, 3). Got {}\".format(R.shape))\n    if tvec.shape != (batch_size, 3, 1):\n        raise ValueError(\"Expected tvec to have shape (batch_size, 3, 1). Got {}\".format(tvec.shape))\n    if camera_matrix.shape != (batch_size, 3, 3):\n        raise ValueError(\"Expected camera_matrix to have shape (batch_size, 3, 3). Got {}\".format(camera_matrix.shape))\n    if image_size.shape != (batch_size, 2):\n        raise ValueError(\"Expected image_size to have shape (batch_size, 2). Got {}\".format(image_size.shape))\n    if znear <= 0:\n        raise ValueError(\"znear should be positive. Got {}\".format(znear))\n\n    focal_length = camera_matrix[..., 0, 0]\n    principal_point = camera_matrix[..., 0:2, 2]\n    sensor_width = camera_matrix[..., 0, 2]\n\n    # average focal length for x and y\n    focal_length = (focal_length[..., 0] + focal_length[..., 1]) / 2.0\n    # check that focal lengths for x and y are close\n    ratio = (focal_length[..., 0] / focal_length[..., 1]).abs()\n    if (ratio - 1).abs() > 0.01:\n        warn_once_about_pulsar_fxfy()\n\n    # adjust principal point offset due to cropping\n    principal_point = principal_point - 0.5\n    # adjust focal length due to cropping\n    focal_length = focal_length * (image_size / sensor_width).mean(-1)\n    # get camera rotation\n    rotation = matrix_to_rotation_"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    assert R.shape[-1] == 3 and R.shape[-2] == 3\n    assert tvec.shape[-1] == 3 and tvec.shape[-2] == 1\n    assert camera_matrix.shape[-1] == 3 and camera_matrix.shape[-2] == 3\n    assert image_size.shape[-1] == 2 and image_size.shape[-2] == 1\n\n    # Focal length\n    fx = camera_matrix[..., 0, 0]\n    fy = camera_matrix[..., 1, 1]\n    if (fx / fy - 1.0).abs().max() > 0.01:\n        warn_once_about_pulsar_fxfy()\n    fx = fy = 0.5 * (fx + fy)\n\n    # Principal point offset\n    cx = camera_matrix[..., 0, 2]\n    cy = camera_matrix[..., 1, 2]\n    # cx = (cx - (image_size[..., 0] - 1) / 2.0) / fx\n    # cy = (cy - (image_size[..., 1] - 1) / 2.0) / fy\n    cx = (cx - 0.5 * image_size[..., 0]) / fx\n    cy = (cy - 0.5 * image_size[..., 1]) / fy\n\n    # Sensor size\n    sensor_width = 2.0 * fx / fy * image_size[..., 1] / image_size[..., 0]\n\n    # Camera position\n    cam_pos = -R.mT @ tvec\n\n    # Camera rotation\n    cam_rot = matrix_to_rotation_6d(R)\n\n    # Near and far planes\n    near = torch.full_like(cam_pos[..., :1], znear)\n    far = cam_pos[..., 2:]\n\n    # Camera parameters\n    cam_params = torch.cat([cam_pos, cam_rot, near,"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Validate inputs\n    assert R.shape[-2:] == (3, 3), \"Invalid rotation matrix shape\"\n    assert tvec.shape[-1] == 3, \"Invalid translation vector shape\"\n    assert camera_matrix.shape[-2:] == (3, 3), \"Invalid camera intrinsic matrix shape\"\n    assert image_size.shape == (2,), \"Invalid image size\"\n\n    # Extract fx, fy, cx, cy from camera_matrix\n    fx = camera_matrix[..., 0, 0]\n    fy = camera_matrix[..., 1, 1]\n    cx = camera_matrix[..., 0, 2]\n    cy = camera_matrix[..., 1, 2]\n\n    # Calculate focal length and principal point offsets\n    fx_mean = (fx + fy) / 2\n    fy_mean = fx_mean\n    cx_offset = (cx - image_size[0] / 2) / image_size[0]\n    cy_offset = (cy - image_size[1] / 2) / image_size[1]\n\n    # Calculate camera position\n    cam_pos = -R.transpose(-2, -1) @ tvec\n\n    # Calculate camera rotation\n    cam_rot = matrix_to_rotation_6d(R)\n\n    # Calculate near and far clipping planes\n    near_clip = znear\n    far_clip = 1000.0\n\n    # Calculate focal length\n    focal_length = fx_mean / 2\n\n    # Calculate sensor width\n    sensor_width = 2 * focal_length * torch.tan(torch.atan(image_size[0] / (2 * fx_mean)) * 2)\n\n    # Create camera parameters tensor\n    camera_params = torch.cat([cam_pos, cam_rot, focal_length.unsqueeze(-1), sensor_width.unsqueeze(-1),\n                               cx_offset.unsqueeze(-1), cy_offset.unsqueeze(-"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Validate inputs\n    assert R.ndim == 3 and R.shape[-2:] == (3, 3)\n    assert tvec.ndim == 2 and tvec.shape[-1] == 3\n    assert camera_matrix.ndim == 2 and camera_matrix.shape[-2:] == (3, 3)\n    assert image_size.ndim == 1 and image_size.shape[0] == 2\n\n    # Extract camera parameters from inputs\n    focal_length = camera_matrix[..., 0, 0]\n    principal_point = camera_matrix[..., :2, 2]\n    sensor_width = camera_matrix[..., 0, 2]\n\n    # Compute camera position\n    camera_position = -R.transpose(-1, -2) @ tvec[..., None]\n\n    # Compute camera rotation\n    camera_rotation = matrix_to_rotation_6d(R)\n\n    # Compute camera intrinsic parameters\n    focal_length = (focal_length[..., 0] + focal_length[..., 1]) / 2\n    sensor_width = sensor_width[..., 0]\n    principal_point = principal_point[..., 0]\n\n    # Adjust focal length and principal point based on image size\n    focal_length = focal_length * image_size[..., 0] / sensor_width\n    principal_point = principal_point * image_size / image_size[..., 0]\n\n    # Warn if focal lengths differ by more than 1%\n    if torch.any(torch.abs(focal_length[0] - focal_length) > 0.01 * focal_length[0]):\n        warn_once_about_pulsar_fxfy()\n\n    # Compute camera parameters for Pulsar\n    camera_params = torch.cat(\n        [camera_position[..., 0], camera_rotation, focal_length, principal_point, znear],\n        dim=-1,\n    )\n\n    return camera_params"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Check and validate input shapes\n    assert R.shape[-2:] == (3, 3), \"Invalid rotation matrix shape\"\n    assert tvec.shape[-1] == 3, \"Invalid translation vector shape\"\n    assert camera_matrix.shape[-2:] == (3, 3), \"Invalid camera matrix shape\"\n    assert image_size.shape[-1] == 2, \"Invalid image size shape\"\n\n    # Extract camera intrinsics\n    fx = camera_matrix[..., 0, 0]\n    fy = camera_matrix[..., 1, 1]\n    cx = camera_matrix[..., 0, 2]\n    cy = camera_matrix[..., 1, 2]\n    sensor_width = camera_matrix[..., 0, 2] * 2\n\n    # Calculate camera position\n    cam_pos = -torch.matmul(R.transpose(-1, -2), tvec)\n\n    # Calculate rotation\n    rotation = matrix_to_rotation_6d(R)\n\n    # Calculate focal length and principal point offset\n    focal_length = (fx + fy) * 0.5\n    offset = torch.stack([cx - sensor_width * 0.5, cy - sensor_width * 0.5], dim=-1)\n\n    # Calculate near and far clipping planes\n    near = torch.full_like(focal_length, znear)\n    far = cam_pos[..., 2] + offset[..., 1] * 2.0\n\n    # Normalize focal length and principal point offset\n    focal_length = focal_length / focal_length.max()\n    offset = offset / (sensor_width * 0.5)\n\n    # Check and warn about focal lengths if they differ by more than 1%\n    if (fx / fy > 1.01).any() or (fx / fy < 0.99).any():\n        warn_once_about_pulsar_fxfy()\n\n    # Stack and return camera parameters\n    return torch.stack([cam"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    assert R.shape[-2:] == (3, 3)\n    assert tvec.shape[-1:] == (3, )\n    assert camera_matrix.shape[-2:] == (3, 3)\n    assert image_size.shape[-1:] == (2, )\n\n    # Make sure batch dimensions are the same for all tensors\n    assert R.shape[:-2] == tvec.shape[:-1] == camera_matrix.shape[:-2] == image_size.shape[:-1]\n\n    # Extract fx, fy, cx, cy from camera_matrix\n    fx = camera_matrix[..., 0, 0]\n    fy = camera_matrix[..., 1, 1]\n    cx = camera_matrix[..., 0, 2]\n    cy = camera_matrix[..., 1, 2]\n\n    # Compute focal length and sensor width\n    f = (fx + fy) * 0.5\n    if (f - fx).abs().max() / f.max() > 0.01:\n        warn_once_about_pulsar_fxfy()\n    w = f / cx * image_size[..., 0]\n\n    # Compute camera position\n    cam_pos = -R.transpose(-2, -1) @ tvec\n\n    # Compute camera rotation\n    cam_rot = matrix_to_rotation_6d(R.transpose(-2, -1))\n\n    # Compute near and far clipping planes\n    znear = torch.full(batch_dim, znear, device=R.device, dtype=R.dtype)\n    zfar = znear + w / (2 * f)\n\n    # Stack camera parameters into a single tensor\n    camera_params = torch.stack([cam_pos, cam_rot, znear, zfar, w], dim=-1)\n\n    return camera_params"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    if R.ndim == 2:\n        R = R[None]\n    if tvec.ndim == 2:\n        tvec = tvec[None]\n    if camera_matrix.ndim == 2:\n        camera_matrix = camera_matrix[None]\n    if image_size.ndim == 1:\n        image_size = image_size[None, :]\n\n    batch_size = R.shape[0]\n    focal_length = camera_matrix[..., 0, 0]\n    principal_point = camera_matrix[..., :2, 2]\n    image_size_wh = image_size[..., ::-1]\n\n    assert focal_length.shape == (batch_size,)\n    assert principal_point.shape == (batch_size, 2)\n    assert image_size_wh.shape == (batch_size, 2)\n\n    # We estimate the camera center by projecting the translation vector\n    # (tvec) with the rotation matrix.\n    camera_center = -R.transpose(-1, -2) @ tvec\n\n    # For the rotation, we transform the rotation matrix from OpenCV\n    # convention to Pulsar convention.\n    # In OpenCV, the coordinate system is left to right (x points right)\n    # and up to down (y points down) while, in Pulsar, the coordinate\n    # system is right to left (x points left) and up to down (y points down).\n    # See https://github.com/facebookresearch/pulsar/issues/15 for\n    # a visualization.\n    # We can convert the camera rotation from OpenCV convention to Pulsar\n    # convention by flipping the sign of the first column of the rotation\n    # matrix.\n    R_pulsar = R.clone()\n    R_pulsar[..., 0] *= -1\n\n    # The 6D representation is not unique so we stick to a certain\n    # representation for convenience.\n    # We always choose the one which has positive z values in the last\n    # column.\n    # See https://github.com/facebookresearch/p"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        w = w or self.W\n        h = h or self.H\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        gl.glUseProgram(self.quad_program)\n\n        # Prepare to render to textures\n        gl.glBindVertexArray(self.vao)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glBindVertexArray(old_vao)\n        gl.glUseProgram(old_prog)\n\n        # Restore the original state\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        gl.glUseProgram(self.quad_program)\n        self.upload_gl_uniforms(camera)  # should be a noop\n\n        # Prepare to render to textures\n        gl.glBindVertexArray(self.vao)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glBindVertexArray(old_vao)\n        gl.glUseProgram(old_prog)\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        gl.glUseProgram(self.quad_program)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Prepare to render to textures\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glBindVertexArray(old_vao)\n        gl.glUseProgram(old_prog)\n\n        # Restore the original state\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        gl.glUseProgram(self.quad_program)\n\n        # Prepare to render to textures\n        gl.glBindVertexArray(self.vao)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n        gl.glEnable(gl.GL_SCISSOR_TEST)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glDisable(gl.GL_SCISSOR_TEST)\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n        gl.glBindVertexArray(old_vao)\n        gl.glUseProgram(old_prog)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        gl.glUseProgram(self.quad_program)\n\n        w = w or self.W\n        h = h or self.H\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Prepare to render to textures\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glBindVertexArray(old_vao)\n        gl.glUseProgram(old_prog)\n\n        # Restore the original state\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        gl.glUseProgram(self.quad_program)\n\n        # Prepare to render to textures\n        gl.glBindVertexArray(self.vao)\n\n        # Setup the viewport and scissor box\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Activate the shader program\n        gl.glActiveTexture(gl.GL_TEXTURE0)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Draw the quadrilateral\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n\n        # Restore the viewport and scissor box to their original sizes\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n\n        # Restore the original state\n        gl.glBindVertexArray(old_vao)\n        gl.glUseProgram(old_prog)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Prepare to render to textures\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        gl.glUseProgram(self.quad_program)\n        self.upload_gl_uniforms(camera)\n\n        w = w or self.W\n        h = h or self.H\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Prepare to render to textures\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glBindVertexArray(old_vao)\n        gl.glUseProgram(old_prog)\n\n        # Restore the original state\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        old_scissor = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n\n        w = w or self.W\n        h = h or self.H\n\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n        gl.glUseProgram(self.quad_program)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glBindVertexArray(old_vao)\n        gl.glUseProgram(old_prog)\n\n        gl.glViewport(*old_viewport)\n        gl.glScissor(*old_scissor)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        w = w or self.W\n        h = h or self.H\n\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        gl.glUseProgram(self.quad_program)\n        self.upload_gl_uniforms(camera)  # should be a noop\n\n        # Prepare to render to textures\n        gl.glBindVertexArray(self.vao)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glBindVertexArray(old_vao)\n        gl.glUseProgram(old_prog)\n\n        # Restore states\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        old_scissor = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n        w = w or self.W\n        h = h or self.H\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n        gl.glUseProgram(self.quad_program)\n\n        # Prepare to render to textures\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glBindVertexArray(old_vao)\n        gl.glUseProgram(old_prog)\n\n        # Restore the original state\n        gl.glViewport(old_viewport[0], old_viewport[1], old_viewport[2], old_viewport[3])\n        gl.glScissor(old_scissor[0], old_scissor[1], old_scissor[2], old_scissor[3])"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        old_scissor = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n\n        gl.glUseProgram(self.quad_program)\n        gl.glBindVertexArray(self.vao)\n\n        w = w or self.W\n        h = h or self.H\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n\n        gl.glViewport(old_viewport[0], old_viewport[1], old_viewport[2], old_viewport[3])\n        gl.glScissor(old_scissor[0], old_scissor[1], old_scissor[2], old_scissor[3])\n        gl.glBindVertexArray(old_vao)\n        gl.glUseProgram(old_prog)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        w = w or self.W\n        h = h or self.H\n\n        # Set the viewport and scissor box\n        old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        old_scissor = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Set the shader program\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        gl.glUseProgram(self.quad_program)\n\n        # Set the texture\n        old_tex = gl.glGetIntegerv(gl.GL_TEXTURE_BINDING_2D)\n        gl.glActiveTexture(gl.GL_TEXTURE0)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Prepare to render to textures\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glBindVertexArray(old_vao)\n\n        # Restore the original state\n        gl.glViewport(*old_viewport)\n        gl.glScissor(*old_scissor)\n        gl.glUseProgram(old_prog)\n        gl.glActiveTexture(gl.GL_TEXTURE0)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, old_tex)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        old_scissor = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n\n        w = w or self.W\n        h = h or self.H\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        gl.glUseProgram(self.quad_program)\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glBindVertexArray(old_vao)\n        gl.glUseProgram(old_prog)\n\n        gl.glViewport(*old_viewport)\n        gl.glScissor(*old_scissor)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        old_scissor = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n\n        gl.glUseProgram(self.quad_program)\n        gl.glBindVertexArray(self.vao)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Draw the quad\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n\n        gl.glViewport(*old_viewport)\n        gl.glScissor(*old_scissor)\n        gl.glBindVertexArray(old_vao)\n        gl.glUseProgram(old_prog)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Setup viewport and scissor box\n        old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        old_scissor = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Setup shader program\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        gl.glUseProgram(self.quad_program)\n\n        # Setup texture\n        old_tex = gl.glGetIntegerv(gl.GL_TEXTURE_BINDING_2D)\n        gl.glActiveTexture(gl.GL_TEXTURE0)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Prepare to render to textures\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glBindVertexArray(0)\n\n        # Restore states\n        gl.glBindTexture(gl.GL_TEXTURE_2D, old_tex)\n        gl.glUseProgram(old_prog)\n        gl.glViewport(*old_viewport)\n        gl.glScissor(*old_scissor)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Prepare to render to textures\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        gl.glUseProgram(self.quad_program)\n\n        # Prepare to render to textures\n        gl.glBindVertexArray(self.vao)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Setup viewport and scissor box\n        old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        old_scissor = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n        gl.glEnable(gl.GL_SCISSOR_TEST)\n\n        # Draw the quadrilateral\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n\n        # Restore the original state\n        gl.glDisable(gl.GL_SCISSOR_TEST)\n        gl.glViewport(*old_viewport)\n        gl.glScissor(*old_scissor)\n        gl.glBindVertexArray(old_vao)\n        gl.glUseProgram(old_prog)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Prepare for write frame buffers\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        old_scissor = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n\n        w = w or self.W\n        h = h or self.H\n        gl.glViewport(x, y, w, h)  # set the viewport to the specified dimensions\n        gl.glScissor(x, y, w, h)  # set the scissor box to the specified dimensions\n        gl.glUseProgram(self.quad_program)  # use a different program\n        gl.glActiveTexture(gl.GL_TEXTURE0)  # activate texture unit 0\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)  # bind the texture to the active texture unit\n\n        # Prepare to render to textures\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glBindVertexArray(old_vao)\n        gl.glUseProgram(old_prog)\n\n        # Restore states\n        gl.glViewport(*old_viewport)  # restore the original viewport\n        gl.glScissor(*old_scissor)  # restore the original scissor box"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        gl.glUseProgram(self.quad_program)\n\n        # Prepare to render to textures\n        gl.glBindVertexArray(self.vao)\n        gl.glViewport(x, y, w, h)  # the height is flipped\n        gl.glScissor(x, y, w, h)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n\n        # Restore states\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n        gl.glBindVertexArray(old_vao)\n        gl.glUseProgram(old_prog)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Prepare to render to textures\n        old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        old_scissor = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Draw the quad\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        gl.glUseProgram(self.quad_program)\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glBindVertexArray(old_vao)\n        gl.glUseProgram(old_prog)\n\n        # Restore the original state\n        gl.glViewport(*old_viewport)\n        gl.glScissor(*old_scissor)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        old_scissor = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n\n        # Setup the viewport and scissor box for rendering\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Activate the shader program\n        gl.glUseProgram(self.quad_program)\n\n        # Bind the texture\n        gl.glActiveTexture(gl.GL_TEXTURE0)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Draw the quadrilateral\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glBindVertexArray(0)\n\n        # Restore the viewport and scissor box to their original sizes\n        gl.glViewport(*old_viewport)\n        gl.glScissor(*old_scissor)\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    return H, W, K, R, T, C"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    return H, W, K, R, T, C"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    return H, W, K, R, T, C"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    K = get_pytorch3d_ndc_K(K, H, W)\n    R = R.permute(0, 2, 1)\n    T = -R.mT @ T\n    return H, W, K, R, T, C"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    # R = batch.R.mT\n    # T = -batch.R.mT @ batch.T\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    K = get_pytorch3d_ndc_K(K, H, W)\n    R = R.permute(0, 2, 1)\n    T = T[..., 0]\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    K = get_pytorch3d_ndc_K(K, H, W)  # B, 4, 4\n    R = R.mT  # B, 3, 3\n    T = -R @ T  # B, 3, 1\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n\n    # Adjust R and T to match PyTorch3D's coordinate system and conventions\n    R_trans = R.permute(0, 2, 1)\n    T_trans = -R_trans @ T\n\n    # Compute intrinsic matrix for normalized device coordinates (NDC)\n    focal_length_px = K[:, 0, 0] / W\n    sensor_width_px = K[:, 1, 1] / H\n    focal_length = focal_length_px * sensor_width_px\n    cx = K[:, 0, 2] / W - 0.5\n    cy = K[:, 1, 2] / H - 0.5\n\n    return H, W, K, R_trans, T_trans, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n\n    # Adjust R and T to match PyTorch3D's coordinate system and conventions\n    R_trans = R.permute(0, 2, 1)  # B, 3, 3\n    T_trans = -R.mT @ T  # B, 3, 1\n\n    # Calculate the intrinsic matrix for normalized device coordinates (NDC)\n    focal_length_px = K[:, 0, 0] / W\n    sensor_width_px = K[:, 1, 1] / H\n    focal_length = torch.as_tensor([1], dtype=torch.float32, device=R.device)\n    sensor_width = focal_length / focal_length_px\n    cx = K[:, 0, 2] / W - 0.5\n    cy = K[:, 1, 2] / H - 0.5\n    K_ndc = torch.stack([focal_length, sensor_width, cx, cy], dim=1)\n\n    return H, W, K_ndc, R_trans, T_trans, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    # !: BATCH\n    # Convert to PyTorch3D coordinate system\n    R = R.permute(0, 2, 1)\n    T = -R @ T\n    # !: BATCH\n    # Convert to PyTorch3D camera convention\n    R[..., 0, 1] *= -1\n    R[..., 1, 0] *= -1\n    R[..., 2, 0] *= -1\n    T[..., 0, 0] *= -1\n    T[..., 1, 0] *= -1\n    T[..., 2, 0] *= -1\n    # !: BATCH\n    # Convert to PyTorch3D NDC convention\n    K = get_pytorch3d_ndc_K(K, H, W)\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    # R = R.transpose(-1, -2)  # B, 3, 3\n    # T = T.transpose(-1, -2)  # B, 3, 1\n    K = get_pytorch3d_ndc_K(K, H, W)\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    # R = R.mT\n    # T = -R @ T\n    # R = R.mT\n    # T = -R @ T\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    # R = R.permute(0, 2, 1)  # B, 3, 3\n    # T = T.permute(0, 2, 1)  # B, 3, 1\n    # C = -R.mT @ T  # B, 3, 1\n    # K = torch.tensor([\n    #     [W / 2, 0, W / 2],\n    #     [0, H / 2, H / 2],\n    #     [0, 0, 1],\n    # ], device=R.device, dtype=R.dtype).repeat(batch.B, 1, 1)\n    # K[..., 0, 0] = K[..., 0, 0] * 2.0 / max(H, W)  # fx\n    # K[..., 1, 1] = K[..., 1, 1] * 2.0 / max(H, W)  # fy\n    # K[..., 0, 2] = -(K[..., 0, 2] - W / 2.0) * 2.0 / max(H, W)  # px\n    # K[..., 1, 2] = -(K[..., 1, 2] - H / 2.0) * 2.0 / max(H, W)  # py\n    # K[..., 0, 0] = K[..., 0, 0] * 2.0 / W  # fx\n    # K[..., 1, 1] = K[..., 1, 1] * 2.0 / H  # fy"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n\n    # Transform the camera parameters to PyTorch3D format\n    R_trans = R.permute(0, 2, 1)  # B, 3, 3\n    T_trans = -R_trans @ T  # B, 3, 1\n    K_trans = torch.eye(4, device=R.device)  # 4, 4\n    K_trans[0, 0] = -K[:, 0, 0] / W\n    K_trans[1, 1] = K[:, 1, 1] / H\n    K_trans[0, 2] = 1 - 2 * K[:, 0, 2] / W\n    K_trans[1, 2] = 2 * K[:, 1, 2] / H - 1\n    K_trans[2, 2] = -1\n    K_trans[3, 3] = 1\n\n    # Adjust the camera parameters to PyTorch3D's coordinate system and conventions\n    R_trans[:, :2, :] *= -1  # Flip x and y axes\n    T_trans[:, :2, :] *= -1  # Flip x and y axes\n    K_trans[1, 1] *= -1  # Flip y axis\n    K_trans[1, 2] *= -1  # Flip y axis\n\n    return H, W, K_trans, R_trans, T_trans, C\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)  # read buffer defaults to 0\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)  # now self.tex contains the content of the already rendered frame\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)  # read buffer defaults to 0\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)  # now self.tex contains the content of the already rendered frame\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)  # read buffer defaults to 0\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)  # now self.tex contains the content of the already rendered frame\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)  # read buffer defaults to 0\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)  # read buffer defaults to 0\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)  # now self.tex contains the content of the already rendered frame\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)  # read buffer defaults to 0\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)  # now self.tex contains the content of the already rendered frame\n\n        # Some house keepings\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)  # read buffer defaults to 0\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)  # read buffer defaults to 0\n\n        old_fbo = gl.glGetIntegerv(gl.GL_DRAW_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, old_fbo)  # read buffer defaults to 0\n\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)  # now self.tex contains the content of the already rendered frame\n\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)\n        gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, old_fbo)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)  # read buffer defaults to 0\n\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)  # now self.tex contains the content of the already rendered frame\n\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)  # read buffer defaults to 0\n\n        # Prepare to render to textures\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glBindVertexArray(0)\n\n        # Restore the original state\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)  # read buffer defaults to 0\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old_read_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBlitFramebuffer(x, y, w, h, x, y, w, h, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_read_fbo)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old = gl.glGetInteger(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)  # now self.tex contains the content of the already rendered frame\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old = gl.glGetInteger(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Find the indices of the source times that are less than or equal to the target time.\n    indices = torch.sum(t1[..., None, :] <= t0[..., :, None], dim=-1)\n\n    # Calculate the inner measure by summing the corresponding values from the source times.\n    inner = torch.take_along_dim(y1, indices, dim=-1)\n\n    # Calculate the outer measure by subtracting the inner measure from the target time.\n    outer = t0 - inner\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Compute the inner measure using cumulative sums.\n    inner_measure = torch.cumsum(y1, dim=-1) * (t1[..., 1:] - t1[..., :-1])\n\n    # Compute the outer measure using cumulative sums.\n    outer_measure = torch.cumsum(inner_measure, dim=-1)\n\n    return inner_measure, outer_measure\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Compute the cumulative sum of y1\n    y1_cum = torch.cumsum(y1, dim=-1)\n\n    # Find the indices of t0 in t1\n    t0_indices = searchsorted(t1, t0)\n\n    # Compute the inner and outer measures\n    y1_inner = y1_cum[..., t0_indices - 1]\n    y1_outer = y1_cum[..., t0_indices]\n\n    return y1_inner, y1_outer"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Sort the source times and values based on the source times.\n    t1, y1 = t1.sort()\n\n    # Find the indices where the target times are located in the sorted source times.\n    t1_idx = searchsorted(t1, t0)\n\n    # Compute the inner and outer measures based on the sorted source times and values.\n    inner = y1.take_along_dim(t1_idx, dim=-1)\n    outer = y1.take_along_dim(torch.clip(t1_idx - 1, 0), dim=-1)\n\n    return inner, outer"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Sort the source times and values by increasing order of source times.\n    t1, y1 = matchup_channels(t1, y1)\n    t1, y1 = t1.sort()\n\n    # Compute the cumulative sum of the source values.\n    c = torch.cumsum(y1, dim=-1)\n\n    # Find the indices of the source times that are less than or equal to the target time.\n    idx = searchsorted(t1, t0)\n\n    # Compute the inner and outer measures based on the cumulative sum of the source values.\n    inner = c.gather(-1, idx)\n    outer = c.gather(-1, torch.clamp(idx - 1, min=0))\n\n    return inner, outer"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Sort the source times and values based on the source times\n    t1, t1_idx = torch.sort(t1, dim=-1)\n    y1 = y1.gather(-1, t1_idx)\n\n    # Calculate the cumulative sum of the source values\n    y1_cum = torch.cumsum(y1, dim=-1)\n\n    # Calculate the inner measure using the cumulative sum of the source values\n    y0_inner = torch.zeros_like(t0)\n    y0_inner[t0 > t1[..., 0]] = y1_cum[..., -1]\n    y0_inner[t0 < t1[..., -1]] = y1_cum[..., 0]\n    y0_inner[(t0 > t1[..., 0]) & (t0 < t1[..., -1])] = y1_cum[..., searchsorted(t1, t0[(t0 > t1[..., 0]) & (t0 < t1[..., -1])])]\n\n    # Calculate the outer measure using the cumulative sum of the source values\n    y0_outer = torch.zeros_like(t0)\n    y0_outer[t0 > t1[..., 0]] = y1_cum[..., -1]\n    y0_outer[t0 < t1[..., -1]] = y1_cum[..., 0]\n    y0_outer[(t0 > t1[..., 0]) & (t0 < t1[..., -1])] = y1_cum[..., searchsorted(t1, t0[(t0 > t1[..., 0]) & (t0 < t1[..., -1])]) - 1]\n\n    return y0_inner, y0_outer"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Get the indices of the source times that are less than or equal to the target time.\n    indices = torch.sum(t1[..., None, :] <= t0[..., None], dim=-1)\n\n    # Compute the inner measure using the indices and the source values.\n    inner = torch.gather(y1, dim=-1, index=indices)\n\n    # Compute the outer measure using the indices and the source values.\n    outer = torch.gather(y1, dim=-1, index=indices - 1)\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Find the indices of the source times that are closest to the target times.\n    t1_idx = searchsorted(t1, t0)\n    t1_idx = torch.clamp(t1_idx, 0, t1.shape[-1] - 1)\n\n    # Compute the inner measure by taking the difference between the target time and the closest source time.\n    t1_lo = t1.take_along_dim(t1_idx, dim=-1)\n    inner = t0 - t1_lo\n\n    # Compute the outer measure by taking the difference between the target time and the next closest source time.\n    t1_hi = t1.take_along_dim(torch.clamp(t1_idx + 1, 0, t1.shape[-1] - 1), dim=-1)\n    outer = t1_hi - t0\n\n    return inner, outer"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Sort the source times and values based on the source times.\n    t1, y1 = t1.sort()\n\n    # Compute the cumulative sum of the source values.\n    c1 = torch.cumsum(y1, dim=-1)\n\n    # Find the indices of the target times in the sorted source times.\n    idx = searchsorted(t1, t0)\n\n    # Compute the inner and outer measures based on the cumulative sums.\n    inner = torch.take_along_dim(c1, idx, dim=-1)\n    outer = torch.take_along_dim(c1, idx - 1, dim=-1)\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Sort the source times and values based on the source times.\n    t1, t1_idx = torch.sort(t1, dim=-1)\n    y1 = y1.take_along_dim(t1_idx, dim=-1)\n\n    # Calculate the cumulative sum of the values for each source time.\n    cdf1 = torch.cumsum(y1, dim=-1)\n\n    # Find the indices of the target times in the sorted source times.\n    t1_idx = searchsorted(t1, t0)\n\n    # Calculate the inner measure for each target time.\n    inner = torch.gather(cdf1, dim=-1, index=t1_idx)\n\n    # Calculate the outer measure for each target time.\n    outer = torch.gather(cdf1, dim=-1, index=t1_idx - 1)\n\n    return inner, outer"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Sort the source times and values based on the source times.\n    t1_sorted, y1_sorted = t1.sort()\n\n    # Calculate the cumulative sum of the source values.\n    y1_cumsum = torch.cumsum(y1_sorted, dim=-1)\n\n    # Find the indices of the target times in the sorted source times.\n    t0_indices = searchsorted(t1_sorted, t0)\n\n    # Compute the inner measure using the cumulative sum and the target indices.\n    y0_inner = y1_cumsum.gather(-1, t0_indices)\n\n    # Compute the outer measure using the cumulative sum and the target indices.\n    y0_outer = y1_cumsum.gather(-1, t0_indices - 1)\n\n    return y0_inner, y0_outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Find the indices of the source times that are smaller than the target time.\n    mask = t1[..., None, :] <= t0[..., :, None]\n\n    # Find the indices of the source times that are closest to the target time.\n    idx = torch.max(torch.where(mask, t1[..., None, :], t1[..., :1, None]), -2).indices\n\n    # Calculate the inner measure as the sum of the values associated with the source times.\n    inner = y1.gather(-1, idx).sum(-1)\n\n    # Calculate the outer measure as the difference between the target time and the sum of the values associated with the source times.\n    outer = t0 - inner\n\n    # Return the inner and outer measures.\n    return inner, outer"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Compute the cumulative sum of y1\n    cumsum_y1 = torch.cumsum(y1, dim=-1)\n\n    # Find the indices of the t1 values that are closest to t0\n    t1_indices = searchsorted(t1, t0)\n\n    # Compute the inner measure\n    inner_measure = cumsum_y1[..., t1_indices]\n\n    # Compute the outer measure\n    outer_measure = torch.cat([cumsum_y1[..., :1], cumsum_y1[..., t1_indices]], dim=-1)\n\n    # Return the inner and outer measures\n    return inner_measure, outer_measure"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Compute the inner and outer measures using cumulative sums\n    t1 = torch.cat([t1, t0[-1:]], dim=-1)\n    y1 = torch.cat([y1, torch.zeros_like(y1[..., :1])], dim=-1)\n    y1 = torch.cumsum(y1, dim=-1)\n    y0 = y1.gather(-1, searchsorted(t1, t0))\n    y0 = torch.cat([y0, y1[..., -1:]], dim=-1)\n    y0 = y0.gather(-1, searchsorted(t0, t1))\n\n    return y0, y1\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Get the indices of the source times that are less than or equal to the target time.\n    # This is used to determine the indices of the source times that are relevant for the target time.\n    indices = searchsorted(t1, t0)\n\n    # Calculate the inner measure for the target time.\n    # The inner measure is the sum of the values associated with the source times that are less than or equal to the target time.\n    inner = torch.where(indices == 0,\n                        torch.zeros_like(t0),\n                        torch.cumsum(y1[indices - 1], dim=-1))\n\n    # Calculate the outer measure for the target time.\n    # The outer measure is the sum of the values associated with the source times that are greater than the target time.\n    outer = torch.where(indices == t1.shape[-1],\n                        torch.zeros_like(t0),\n                        torch.cumsum(y1[t1.shape[-1] - 1 - indices], dim=-1))\n\n    # Return the inner and outer measures for the target time.\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Sort the source times and values in ascending order based on the source times\n    t1, idx = torch.sort(t1, dim=-1)\n    y1 = y1.take_along_dim(idx, dim=-1)\n\n    # Compute the cumulative sums of the values for the sorted source times\n    cs = torch.cumsum(y1, dim=-1)\n\n    # Find the indices where the target times are less than or equal to the sorted source times\n    idx = searchsorted(t1, t0)\n\n    # Compute the inner measure by subtracting the cumulative sums at the corresponding indices\n    inner = cs.take_along_dim(idx - 1, dim=-1)\n\n    # Compute the outer measure by subtracting the cumulative sums at the corresponding indices\n    outer = cs.take_along_dim(idx, dim=-1)\n\n    return inner, outer"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Compute the inner measure by finding the index of the first time in t1 that is greater than or equal to each time in t0.\n    # Then, we compute the cumulative sum of the corresponding values in y1 up to that index.\n    # Finally, we subtract the cumulative sum of the values in y1 up to the index before the first time in t1 that is greater than or equal to each time in t0.\n    # This gives us the inner measure for each time in t0.\n    inner_idx = searchsorted(t1, t0)\n    inner_cumsum = torch.cumsum(y1, dim=-1)\n    inner = inner_cumsum.gather(-1, inner_idx) - inner_cumsum.gather(-1, inner_idx - 1)\n\n    # Compute the outer measure by finding the index of the last time in t1 that is less than each time in t0.\n    # Then, we compute the cumulative sum of the corresponding values in y1 up to that index.\n    # Finally, we subtract the cumulative sum of the values in y1 up to the index before the last time in t1 that is less than each time in t0.\n    # This gives us the outer measure for each time in t0.\n    outer_idx = searchsorted(t1, t0, side='right')\n    outer_cumsum = torch.cumsum(y1, dim=-1)\n    outer = outer_cumsum.gather(-1, outer_idx - 1) - outer_cumsum.gather(-1, outer_idx - 2)\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Sort the source times and values in ascending order\n    t1, t1_idx = torch.sort(t1, dim=-1)\n    y1 = y1.gather(-1, t1_idx)\n\n    # Compute the differences between consecutive source times\n    dt = torch.diff(t1, dim=-1, append=torch.tensor(0.0, device=t1.device))\n\n    # Compute the cumulative sum of the values multiplied by the differences\n    cum_y1 = torch.cumsum(y1 * dt, dim=-1)\n\n    # Compute the inner measure by interpolating the cumulative sum at the target times\n    y0 = torch.gather(cum_y1, -1, searchsorted(t1, t0, side='right'))\n\n    # Compute the outer measure by interpolating the cumulative sum at the target times\n    y0_outer = torch.gather(cum_y1, -1, searchsorted(t1, t0, side='left'))\n\n    return y0, y0_outer"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Sort the source times and values in ascending order based on the source times.\n    t1, y1 = matchup_channels(t1, y1)\n    t1, y1 = torch.sort(torch.stack([t1, y1], dim=-1), dim=-1)[0].unbind(-1)\n\n    # Compute the cumulative sum of the source values.\n    y1_cumsum = torch.cumsum(y1, dim=-1)\n\n    # Find the indices of the source times that are smaller than the target time.\n    idx = searchsorted(t1, t0)\n\n    # Compute the inner measure by summing the cumulative sum of the source values up to the corresponding index.\n    y_inner = y1_cumsum.gather(-1, idx)\n\n    # Compute the outer measure by subtracting the inner measure from the cumulative sum of the source values.\n    y_outer = y1_cumsum[..., -1:] - y_inner\n\n    return y_inner, y_outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Sort the source times (t1) and corresponding values (y1) based on the source times.\n    t1, y1 = matchup_channels(t1, y1)\n    t1_sorted, y1_sorted = torch.sort(torch.cat([t1, y1], dim=-1), dim=-1)\n\n    # Compute the cumulative sum of the sorted values (y1_sorted) using the sorted source times (t1_sorted).\n    csum_y1_sorted = torch.cumsum(y1_sorted, dim=-1)\n\n    # Find the indices of the target times (t0) in the sorted source times (t1_sorted).\n    t0_idx = searchsorted(t1_sorted, t0)\n\n    # Compute the inner measure by selecting the corresponding cumulative sum values (csum_y1_sorted) at the indices (t0_idx) of the target times.\n    inner = csum_y1_sorted.gather(-1, t0_idx)\n\n    # Compute the outer measure by adding the cumulative sum values (csum_y1_sorted) at the indices (t0_idx - 1) of the target times.\n    outer = csum_y1_sorted.gather(-1, t0_idx - 1)\n\n    return inner, outer"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    t, w = matchup_channels(t, w)\n    t_env, w_env = matchup_channels(t_env, w_env)\n\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], eps)\n\n    t_, w_ = blur_stepfun(t, w_normalize, 0.01)\n    w_ = torch.clip(w_, min=0.)\n    assert (w_ >= 0.0).all()\n\n    # piecewise linear pdf to piecewise quadratic cdf\n    area = 0.5 * (w_[..., 1:] + w_[..., :-1]) * (t_[..., 1:] - t_[..., :-1])\n\n    cdf = torch.cat([torch.zeros_like(area[..., :1]), torch.cumsum(area, dim=-1)], dim=-1)\n\n    # query piecewise quadratic interpolation\n    cdf_interp = sorted_interp_quad(t_env, t_, w_, cdf)\n    # difference between adjacent interpolated values\n    w_s = torch.diff(cdf_interp, dim=-1)\n\n    return ((w_s - w_env).clip(0.).pow(2) / (w_env + eps)).mean()"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    t, w = matchup_channels(t, w)\n    t_env, w_env = matchup_channels(t_env, w_env)\n\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], eps)\n\n    t_, w_ = blur_stepfun(t, w_normalize, 0.5)\n    w_ = torch.clip(w_, min=0.)\n    assert (w_ >= 0.0).all()\n\n    # piecewise linear pdf to piecewise quadratic cdf\n    area = 0.5 * (w_[..., 1:] + w_[..., :-1]) * (t_[..., 1:] - t_[..., :-1])\n\n    cdf = torch.cat([torch.zeros_like(area[..., :1]), torch.cumsum(area, dim=-1)], dim=-1)\n\n    # query piecewise quadratic interpolation\n    cdf_interp = sorted_interp_quad(t_env, t_, w_, cdf)\n    # difference between adjacent interpolated values\n    w_s = torch.diff(cdf_interp, dim=-1)\n\n    return ((w_s - w_env).clip(0.).pow(2) / (w_env + eps)).mean()"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    t, w = matchup_channels(t, w)\n    t_env, w_env = matchup_channels(t_env, w_env)\n\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], eps)\n\n    t_, w_ = blur_stepfun(t, w_normalize, 0.01)\n    w_ = torch.clip(w_, min=0.)\n    assert (w_ >= 0.0).all()\n\n    # piecewise linear pdf to piecewise quadratic cdf\n    area = 0.5 * (w_[..., 1:] + w_[..., :-1]) * (t_[..., 1:] - t_[..., :-1])\n\n    cdf = torch.cat([torch.zeros_like(area[..., :1]), torch.cumsum(area, dim=-1)], dim=-1)\n\n    # query piecewise quadratic interpolation\n    cdf_interp = sorted_interp_quad(t_env, t_, w_, cdf)\n    # difference between adjacent interpolated values\n    w_s = torch.diff(cdf_interp, dim=-1)\n\n    return ((w_s - w_env).clip(0.).pow(2) / (w_env + eps)).mean()"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    t, w = matchup_channels(t, w)\n    t_env, w_env = matchup_channels(t_env, w_env)\n\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], eps)\n\n    t_, w_ = blur_stepfun(t, w_normalize, eps)\n    w_ = torch.clip(w_, min=0.)\n    assert (w_ >= 0.0).all()\n\n    # piecewise linear pdf to piecewise quadratic cdf\n    area = 0.5 * (w_[..., 1:] + w_[..., :-1]) * (t_[..., 1:] - t_[..., :-1])\n\n    cdf = torch.cat([torch.zeros_like(area[..., :1]), torch.cumsum(area, dim=-1)], dim=-1)\n\n    # query piecewise quadratic interpolation\n    cdf_interp = sorted_interp_quad(t_env, t_, w_, cdf)\n    # difference between adjacent interpolated values\n    w_s = torch.diff(cdf_interp, dim=-1)\n\n    return ((w_s - w_env).clip(0.).pow(2) / (w_env + eps)).mean()"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    w = w / torch.clamp_min(t[..., 1:] - t[..., :-1], eps)\n\n    # piecewise linear pdf to piecewise quadratic cdf\n    area = 0.5 * (w[..., 1:] + w[..., :-1]) * (t[..., 1:] - t[..., :-1])\n\n    cdf = torch.cat([torch.zeros_like(area[..., :1]), torch.cumsum(area, dim=-1)], dim=-1)\n\n    # query piecewise quadratic interpolation\n    cdf_interp = sorted_interp_quad(t_env, t, w, cdf)\n    # difference between adjacent interpolated values\n    w_s = torch.diff(cdf_interp, dim=-1)\n\n    return ((w_s - w_env).clip(0.).pow(2) / (w_env + eps)).mean()"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    t, w = matchup_channels(t, w)\n    t_env, w_env = matchup_channels(t_env, w_env)\n\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], eps)\n\n    t_, w_ = blur_stepfun(t, w_normalize, 1e-4)\n    w_ = torch.clip(w_, min=0.)\n    assert (w_ >= 0.0).all()\n\n    # piecewise linear pdf to piecewise quadratic cdf\n    area = 0.5 * (w_[..., 1:] + w_[..., :-1]) * (t_[..., 1:] - t_[..., :-1])\n\n    cdf = torch.cat([torch.zeros_like(area[..., :1]), torch.cumsum(area, dim=-1)], dim=-1)\n\n    # query piecewise quadratic interpolation\n    cdf_interp = sorted_interp_quad(t_env, t_, w_, cdf)\n    # difference between adjacent interpolated values\n    w_s = torch.diff(cdf_interp, dim=-1)\n\n    return ((w_s - w_env).clip(0.).pow(2) / (w_env + eps)).mean()"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # The loss incurred between all pairs of intervals.\n    ut = (t[..., 1:] + t[..., :-1]) / 2  # 64\n    dut = torch.abs(ut[..., :, None] - ut[..., None, :])  # 64\n    loss_inter = torch.sum(w * torch.sum(w[..., None, :] * dut, dim=-1), dim=-1)\n\n    # The loss incurred within each individual interval with itself.\n    loss_intra = torch.sum(w**2 * (t[..., 1:] - t[..., :-1]), dim=-1) / 3\n\n    # The loss incurred between the target intervals and the environment intervals.\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], eps)\n    t_env, w_env = matchup_channels(t_env, w_env)\n    y0_inner, y0_outer = inner_outer(ut, t_env, w_env)\n    loss_outer = torch.sum(w_normalize * (y0_inner + y0_outer), dim=-1)\n\n    return loss_inter + loss_intra + loss_outer"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # The loss incurred between all pairs of intervals.\n    ut = (t[..., 1:] + t[..., :-1]) / 2  # 64\n    dut = torch.abs(ut[..., :, None] - ut[..., None, :])  # 64\n    loss_inter = torch.sum(w * torch.sum(w[..., None, :] * dut, dim=-1), dim=-1)\n\n    # The loss incurred within each individual interval with itself.\n    loss_intra = torch.sum(w**2 * (t[..., 1:] - t[..., :-1]), dim=-1) / 3\n\n    # The loss incurred by the proposal weight acting as an upper envelope over the nerf weight.\n    y0_inner, y0_outer = inner_outer(t_env, t, w)\n    loss_env = torch.sum(y0_inner * w_env, dim=-1) + torch.sum(y0_outer * w_env, dim=-1)\n\n    loss = loss_inter + loss_intra + loss_env\n\n    return loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # The loss incurred between all pairs of intervals.\n    ut = (t[..., 1:] + t[..., :-1]) / 2  # 64\n    dut = torch.abs(ut[..., :, None] - ut[..., None, :])  # 64\n    loss_inter = torch.sum(w * torch.sum(w[..., None, :] * dut, dim=-1), dim=-1)\n\n    # The loss incurred within each individual interval with itself.\n    loss_intra = torch.sum(w**2 * (t[..., 1:] - t[..., :-1]), dim=-1) / 3\n\n    # The loss incurred between the target interval and the envelope.\n    y0_inner, y0_outer = inner_outer(ut, t_env, w_env)\n    loss_env = torch.sum(w * (y0_inner + y0_outer), dim=-1)\n\n    # The loss incurred within the envelope.\n    loss_env_intra = torch.sum(w_env**2 * (t_env[..., 1:] - t_env[..., :-1]), dim=-1) / 3\n\n    loss = loss_inter + loss_intra + loss_env + loss_env_intra\n    return loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    t, w = matchup_channels(t, w)\n    t_env, w_env = matchup_channels(t_env, w_env)\n\n    # Normalize weights by the width of the corresponding interval.\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], eps)\n\n    # Blur the weights and positions.\n    t_, w_ = blur_stepfun(t, w_normalize, 0.05)\n    w_ = torch.clip(w_, min=0.)\n    assert (w_ >= 0.0).all()\n\n    # piecewise linear pdf to piecewise quadratic cdf\n    area = 0.5 * (w_[..., 1:] + w_[..., :-1]) * (t_[..., 1:] - t_[..., :-1])\n\n    cdf = torch.cat([torch.zeros_like(area[..., :1]), torch.cumsum(area, dim=-1)], dim=-1)\n\n    # query piecewise quadratic interpolation\n    cdf_interp = sorted_interp_quad(t_env, t_, w_, cdf)\n    # difference between adjacent interpolated values\n    w_s = torch.diff(cdf_interp, dim=-1)\n\n    return ((w_s - w_env).clip(0.).pow(2) / (w_env + eps)).mean()"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # Compute the upper envelope weights\n    y0_inner, y0_outer = inner_outer(t, t_env, w_env)\n\n    # Calculate the loss based on the difference between target weights and the upper envelope\n    loss = (w - y0_inner - y0_outer).clip(0.)**2 / (w + eps)\n\n    return loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], eps)\n    # w_normalize = w / (t[..., 1:] - t[..., :-1]).clamp_min(eps)\n    w_normalize = w / (t[..., 1:] - t[..., :-1]).abs().clamp_min(eps)\n\n    # t_, w_ = blur_stepfun(t, w_normalize, pulse_width)\n    # w_ = torch.clip(w_, min=0.)\n    # assert (w_ >= 0.0).all()\n\n    # # piecewise linear pdf to piecewise quadratic cdf\n    # area = 0.5 * (w_[..., 1:] + w_[..., :-1]) * (t_[..., 1:] - t_[..., :-1])\n\n    # cdf = torch.cat([torch.zeros_like(area[..., :1]), torch.cumsum(area, dim=-1)], dim=-1)\n\n    # # query piecewise quadratic interpolation\n    # cdf_interp = sorted_interp_quad(t_env, t_, w_, cdf)\n    # # difference between adjacent interpolated values\n    # w_s = torch.diff(cdf_interp, dim=-1)\n\n    # return ((w_s - w_env).clip(0.).pow(2) / (w_env + eps)).mean()\n\n    # w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], eps)\n    # w_normalize = w / (t[..., 1:] - t[..., :-1]).abs().clamp_min(eps)\n\n    # t_, w_ = blur_stepfun(t, w_normalize, pulse_width)\n    # w_ = torch.clip(w_, min=0.)\n    # assert"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # w.shape[-1] = t.shape[-1] - 1\n\n    # The loss incurred between all pairs of intervals.\n    ut = (t[..., 1:] + t[..., :-1]) / 2  # 64\n    dut = torch.abs(ut[..., :, None] - ut[..., None, :])  # 64\n    loss_inter = torch.sum(w * torch.sum(w[..., None, :] * dut, dim=-1), dim=-1)\n\n    # The loss incurred within each individual interval with itself.\n    loss_intra = torch.sum(w**2 * (t[..., 1:] - t[..., :-1]), dim=-1) / 3\n\n    # The loss incurred between the proposal weight and the nerf weight.\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], eps)\n    y0_inner, y0_outer = inner_outer(t_env, t, w_normalize)\n    loss_outer = torch.sum(w_env * y0_inner, dim=-1) + torch.sum(w_env * y0_outer, dim=-1)\n\n    return loss_inter + loss_intra + loss_outer"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # The loss incurred between all pairs of intervals.\n    ut = (t[..., 1:] + t[..., :-1]) / 2  # 64\n    dut = torch.abs(ut[..., :, None] - ut[..., None, :])  # 64\n    loss_inter = torch.sum(w * torch.sum(w[..., None, :] * dut, dim=-1), dim=-1)\n\n    # The loss incurred within each individual interval with itself.\n    loss_intra = torch.sum(w**2 * (t[..., 1:] - t[..., :-1]), dim=-1) / 3\n\n    # The loss incurred by the proposal weight acting as an upper envelope over the nerf weight.\n    y0_inner, y0_outer = inner_outer(ut, t_env, w_env)\n    loss_outer = torch.sum(w * (y0_inner + y0_outer), dim=-1)\n\n    # The total loss is the sum of the three components.\n    loss = loss_inter + loss_intra + loss_outer\n\n    # The loss is scaled by a half-quadratic function of the proposal weight.\n    loss = loss * (w**2 + eps) / 2\n\n    # The loss is returned as a scalar value.\n    return loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # Normalize the target weights to be proportional to the target intervals.\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], eps)\n\n    # Blur the target weights and positions using a step function.\n    t_, w_ = blur_stepfun(t, w_normalize, 0.01)\n\n    # Clip the target weights to be non-negative.\n    w_ = torch.clip(w_, min=0.)\n\n    # Ensure that the target weights are non-negative.\n    assert (w_ >= 0.0).all()\n\n    # Calculate the area under the curve of the target weights and positions.\n    area = 0.5 * (w_[..., 1:] + w_[..., :-1]) * (t_[..., 1:] - t_[..., :-1])\n\n    # Calculate the cumulative sum of the area under the curve.\n    cdf = torch.cat([torch.zeros_like(area[..., :1]), torch.cumsum(area, dim=-1)], dim=-1)\n\n    # Interpolate the cumulative sum of the area under the curve at the environment positions.\n    cdf_interp = sorted_interp_quad(t_env, t_, w_, cdf)\n\n    # Calculate the difference between adjacent interpolated values.\n    w_s = torch.diff(cdf_interp, dim=-1)\n\n    # Calculate the loss based on the difference between target weights and the upper envelope.\n    loss = ((w_s - w_env).clip(0.).pow(2) / (w_env + eps)).mean()\n\n    return loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # The loss incurred between all pairs of intervals.\n    ut = (t[..., 1:] + t[..., :-1]) / 2  # 64\n    dut = torch.abs(ut[..., :, None] - ut[..., None, :])  # 64\n    loss_inter = torch.sum(w * torch.sum(w[..., None, :] * dut, dim=-1), dim=-1)\n\n    # The loss incurred within each individual interval with itself.\n    loss_intra = torch.sum(w**2 * (t[..., 1:] - t[..., :-1]), dim=-1) / 3\n\n    # The loss incurred between the target and the environment.\n    loss_env = torch.sum(w_env * torch.sum(w_env[..., None, :] * dut, dim=-1), dim=-1)\n\n    # The loss incurred within the environment.\n    loss_env_intra = torch.sum(w_env**2 * (t_env[..., 1:] - t_env[..., :-1]), dim=-1) / 3\n\n    # The loss incurred between the target and the environment, scaled by the target weights.\n    loss_env_scale = loss_env / torch.clamp_min(w, eps)\n\n    # The loss incurred within the environment, scaled by the target weights.\n    loss_env_intra_scale = loss_env_intra / torch.clamp_min(w, eps)\n\n    # The total loss is the sum of the above losses.\n    loss = loss_inter + loss_intra + loss_env_scale + loss_env_intra_scale\n\n    return loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # TODO: Maybe there is a better way to do the batching\n    # But `torch.quantile` does not support multiple `dim` argument for now\n    for i in range(t.shape[0]):\n        t[i] = median_normalize(t[i], w[i])  # (N+1,)\n        t_env[i] = median_normalize(t_env[i], w_env[i])  # (N+1,)\n\n    # Compute the scale-and-shift invariant MAE loss\n    total = lossfun_mae(t, t_env, w, w_env)\n\n    # Add regularization if needed\n    if self.__alpha > 0:\n        total += self.__alpha * self.__regularization_loss(self.prediction, target, mask)\n\n    return total\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # TODO: Implement the function to calculate the loss based on the difference between target weights and the upper envelope.\n\n    # Calculate the difference between target weights and the upper envelope\n    w_diff = w - inner_outer(t, t_env, w_env)[0]  # 128\n\n    # Calculate the loss based on the difference between target weights and the upper envelope\n    loss = torch.sum(w_diff.clip_min(0.)**2) / (w.sum() + eps)\n\n    return loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the outer weights of the target weights\n    w_outer = w[..., 1:] - w[..., :-1]\n\n    # calculate the inner weights of the target weights\n    w_inner = w[..., 1:] + w[..., :-1]\n\n    # calculate the inner and outer weights of the environment weights\n    w_env_inner, w_env_outer = inner_outer(t, t_env, w_env)\n\n    # calculate the loss based on the difference between the target and environment weights\n    loss = torch.sum(torch.clamp(w_inner - w_env_outer, min=0)**2, dim=-1) / (w_outer + eps)\n\n    return loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # Normalize the weights by the time interval.\n    # This ensures that the weights are comparable across different time intervals.\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], eps)\n\n    # Calculate the inner and outer weights for the target weights.\n    # The inner weights represent the weights that are inside the envelope,\n    # while the outer weights represent the weights that are outside the envelope.\n    w0_inner, w0_outer = inner_outer(t_env, t, w_normalize)\n\n    # Calculate the inner and outer weights for the environment weights.\n    w1_inner, w1_outer = inner_outer(t, t_env, w_env)\n\n    # Calculate the difference between the inner and outer weights for the target and environment weights.\n    # This difference represents the amount by which the target weights exceed the envelope.\n    w0_diff = w0_outer - w0_inner\n    w1_diff = w1_outer - w1_inner\n\n    # Calculate the loss based on the difference between the inner and outer weights.\n    # The loss is calculated using a half-quadratic function, which penalizes values greater than 0.\n    # The loss is scaled by a factor of 2 to ensure that the loss is positive.\n    loss_outer = (w0_diff + w1_diff).clip(0.)**2 / 2\n\n    # Return the mean of the loss over the batch.\n    return loss_outer.mean()"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # Calculate the inter-interval loss\n    inter_interval_loss = interval_distortion(t[..., 1:], t[..., :-1], t[..., 1:], t[..., :-1])\n\n    # Calculate the intra-interval loss\n    intra_interval_loss = interval_distortion(t[..., :-1], t[..., 1:], t[..., :-1], t[..., 1:])\n\n    # Combine the inter-interval and intra-interval losses\n    distortion_loss = inter_interval_loss + intra_interval_loss\n\n    return distortion_loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n\n    # Compute the inner and outer measures on (t, w).\n    w_inner, w_outer = inner_outer(t, t, w)\n\n    # Compute the intra-interval loss.\n    loss_intra = (w_inner - w).clip(0.).pow(2) / (w + eps)\n\n    # Compute the inter-interval loss.\n    loss_inter = (w_outer - w).clip(0.).pow(2) / (w + eps)\n\n    # Combine the intra-interval and inter-interval losses.\n    loss = loss_intra + loss_inter\n\n    return loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n\n    # Compute the inter-interval loss\n    inter_interval_loss = lossfun_outer(t, w, t, w, 0.05)\n\n    # Compute the intra-interval loss\n    intra_interval_loss = lossfun_zip_outer(t, w, t, w, 0.05)\n\n    # Combine the inter-interval and intra-interval losses\n    distortion_loss = inter_interval_loss + intra_interval_loss\n\n    return distortion_loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # Inter-interval loss\n    inter_interval_loss = interval_distortion(t[..., :-1], t[..., 1:], w[..., :-1], w[..., 1:])\n\n    # Intra-interval loss\n    intra_interval_loss = (w[..., :-1] + w[..., 1:]) * (t[..., 1:] - t[..., :-1])\n\n    # Combine the two losses\n    distortion_loss = inter_interval_loss + intra_interval_loss\n\n    return distortion_loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n\n    # Compute the inter-interval loss\n    t_lo, t_hi, w_lo, w_hi = t[..., :-1], t[..., 1:], w[..., :-1], w[..., 1:]\n    inter_interval_loss = interval_distortion(t_lo, t_hi, t_lo, t_hi)\n\n    # Compute the intra-interval loss\n    intra_interval_loss = interval_distortion(t_lo, t_hi, t_lo, t_hi)\n\n    # Combine the inter-interval and intra-interval losses\n    distortion_loss = inter_interval_loss + intra_interval_loss\n\n    return distortion_loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n\n    # Compute the distortion loss for the inter-interval distances\n    inter_interval_distortion = interval_distortion(t[..., :-1], t[..., 1:], t[..., :-1], t[..., 1:])\n\n    # Compute the distortion loss for the intra-interval distances\n    intra_interval_distortion = interval_distortion(t[..., :-1], t[..., 1:], t[..., :-1], t[..., 1:])\n\n    # Combine the inter-interval and intra-interval distortion losses\n    distortion_loss = inter_interval_distortion + intra_interval_distortion\n\n    return distortion_loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # Compute the inner and outer measures on (t, w) for t.\n    w_inner, w_outer = inner_outer(t, t, w)\n\n    # Compute the distortion loss using the inner and outer measures.\n    loss_inner = w_inner.clip(0.).pow(2) / (w + eps)\n    loss_outer = w_outer.clip(0.).pow(2) / (w + eps)\n\n    # Combine the inner and outer losses to produce the final distortion loss.\n    loss = loss_inner + loss_outer\n\n    return loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n\n    # Compute the inter-interval loss\n    inter_loss = interval_distortion(t[..., :-1], t[..., 1:], t[..., 1:], t[..., 2:])\n    inter_loss = inter_loss * w[..., :-1]\n\n    # Compute the intra-interval loss\n    intra_loss = interval_distortion(t[..., 1:-1], t[..., 2:], t[..., :-2], t[..., 1:-1])\n    intra_loss = intra_loss * w[..., 1:]\n\n    # Combine the inter- and intra-interval losses\n    distortion_loss = inter_loss + intra_loss\n\n    return distortion_loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # Compute the distortion loss for the inter-interval regions\n    inter_interval_loss = inter_interval_distortion(t, w)\n\n    # Compute the distortion loss for the intra-interval regions\n    intra_interval_loss = intra_interval_distortion(t, w)\n\n    # Combine the inter-interval and intra-interval losses\n    distortion_loss = inter_interval_loss + intra_interval_loss\n\n    return distortion_loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n\n    # Compute the inner and outer measures on (t, w) for t.\n    w_inner, w_outer = inner_outer(t, t, w)\n\n    # Compute the distortion loss for the inner measures.\n    # We assume w_inner <= w <= w_outer. We don't penalize w_inner because it's\n    # more effective to pull w_outer up than it is to push w_inner down.\n    # Scaled half-quadratic loss that gives a constant gradient at w_outer = 0.\n    loss_inner = (w - w_outer).clip(0.).pow(2) / (w + eps)\n\n    # Compute the distortion loss for the outer measures.\n    # We assume w_inner <= w_outer. We don't penalize w_inner because it's\n    # more effective to pull w_outer up than it is to push w_inner down.\n    # Scaled half-quadratic loss that gives a constant gradient at w_outer = 0.\n    loss_outer = (w_inner - w_outer).clip(0.).pow(2) / (w_outer + eps)\n\n    return loss_inner + loss_outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n\n    # Inter-interval loss\n    t_lo, t_hi = t[..., :-1], t[..., 1:]\n    w_lo, w_hi = w[..., :-1], w[..., 1:]\n    t_mid = (t_lo + t_hi) / 2\n    w_mid = (w_lo + w_hi) / 2\n    distortion_inter = interval_distortion(t_lo, t_hi, t_mid, w_mid)\n\n    # Intra-interval loss\n    t_lo, t_hi = t[..., :-1], t[..., 1:]\n    w_lo, w_hi = w[..., :-1], w[..., 1:]\n    distortion_intra = (t_hi - t_lo) * torch.abs(w_hi - w_lo)\n\n    # Combine the two losses\n    distortion = distortion_inter + distortion_intra\n\n    return distortion"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # Normalize the weights by the length of the intervals.\n    t, w = matchup_channels(t, w)\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], eps)\n\n    # Compute the inner and outer measures on the normalized weights.\n    w_inner, w_outer = inner_outer(t, t, w_normalize)\n\n    # Compute the distortion loss using the inner and outer measures.\n    loss_inner = w_inner.clip(0.).pow(2) / (w_normalize + eps)\n    loss_outer = w_outer.clip(0.).pow(2) / (w_normalize + eps)\n\n    # Combine the inner and outer loss to get the total distortion loss.\n    loss = (loss_inner + loss_outer).mean()\n\n    return loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # Calculate the inter-interval and intra-interval losses using the provided tensors and weights.\n    # Inter-interval loss is calculated by taking the difference between adjacent weights and the target tensor, and squaring the result.\n    # Intra-interval loss is calculated by taking the difference between adjacent target values and squaring the result.\n    inter_interval_loss = (w[..., 1:] - w[..., :-1] - (t[..., 1:] - t[..., :-1])).pow(2)\n    intra_interval_loss = (t[..., 1:] - t[..., :-1]).pow(2)\n\n    # Combine the inter-interval and intra-interval losses to produce the total distortion loss.\n    distortion_loss = inter_interval_loss + intra_interval_loss\n\n    # Return the calculated distortion loss as a tensor.\n    return distortion_loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n\n    # Compute the inter-interval loss\n    # The first and last intervals are treated as special cases\n    # The first interval is considered as the left endpoint of the second interval\n    # The last interval is considered as the right endpoint of the second-to-last interval\n    t_lo = t[..., :-1]\n    t_hi = t[..., 1:]\n    w_lo = w[..., :-1]\n    w_hi = w[..., 1:]\n\n    # Compute the intra-interval loss\n    # The first and last intervals are treated as special cases\n    # The first interval is considered as the left endpoint of the second interval\n    # The last interval is considered as the right endpoint of the second-to-last interval\n    t_lo = t[..., :-1]\n    t_hi = t[..., 1:]\n    w_lo = w[..., :-1]\n    w_hi = w[..., 1:]\n\n    # Compute the intra-interval loss\n    # The first and last intervals are treated as special cases\n    # The first interval is considered as the left endpoint of the second interval\n    # The last interval is considered as the right endpoint of the second-to-last interval\n    t_lo = t[..., :-1]\n    t_hi = t[..., 1:]\n    w_lo = w[..., :-1]\n    w_hi = w[..., 1:]\n\n    # Compute the intra-interval loss\n    # The first and last intervals are treated as special cases\n    # The first interval is considered as the left endpoint of the second interval\n    # The last interval is considered as the right endpoint of the second-to-last interval\n    t_lo = t[..., :-1]\n    t_hi = t[..., 1:]\n    w_lo = w[..., :-1]\n    w_hi = w[..., 1:]\n\n    # Compute the intra-interval loss\n    # The first and last intervals are treated as special"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n\n    # Compute the distortion loss for inter-interval regions\n    inter_loss = distortion_inter(t, w)\n\n    # Compute the distortion loss for intra-interval regions\n    intra_loss = distortion_intra(t, w)\n\n    # Combine the inter-interval and intra-interval losses\n    distortion_loss = inter_loss + intra_loss\n\n    return distortion_loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # Calculate the inner and outer intervals for the given targets and weights\n    w_inner, w_outer = inner_outer(t, t, w)\n\n    # Calculate the distortion loss for the inner intervals\n    loss_inner = (w_inner - w).clip(0.).pow(2) / (w + eps)\n\n    # Calculate the distortion loss for the outer intervals\n    loss_outer = (w_outer - w).clip(0.).pow(2) / (w + eps)\n\n    # Combine the losses for the inner and outer intervals\n    loss = loss_inner + loss_outer\n\n    # Return the total distortion loss\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # Compute the inter-interval loss\n    # The loss is computed by taking the difference between the target weights and the inner weights,\n    # and then taking the square of the difference. The result is summed over the last dimension of the\n    # input tensor.\n    inter_loss = (w - inner_outer(t, t, w)[0]).pow(2).sum(dim=-1)\n\n    # Compute the intra-interval loss\n    # The loss is computed by taking the square of the difference between the target weights and the outer\n    # weights, and then taking the square of the result. The result is summed over the last dimension of\n    # the input tensor.\n    intra_loss = (w - inner_outer(t, t, w)[1]).pow(2).sum(dim=-1)\n\n    # Combine the inter-interval and intra-interval losses\n    # The loss is computed by taking the sum of the inter-interval and intra-interval losses, and then\n    # dividing by the number of elements in the last dimension of the input tensor.\n    loss = (inter_loss + intra_loss) / w.shape[-1]\n\n    return loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # First, compute the inter-interval loss\n    t, w = matchup_channels(t, w)\n    # Calculate the differences between consecutive elements of 't'\n    # and the differences between consecutive elements of 'w'\n    # and take the absolute values of these differences.\n    # The resulting tensor 'delta' has the same shape as 't' and 'w'.\n    delta = torch.abs(torch.diff(t, dim=-1)) * torch.abs(torch.diff(w, dim=-1))\n    # Calculate the sum of 'delta' along the last dimension of 'delta'\n    # and divide it by the sum of 'w' along the last dimension of 'w'.\n    # The resulting tensor 'loss_inter' has the same shape as 't' and 'w',\n    # but with the last dimension removed.\n    loss_inter = torch.sum(delta, dim=-1) / torch.sum(w, dim=-1)\n\n    # Next, compute the intra-interval loss\n    # Calculate the differences between consecutive elements of 'w'\n    # and take the absolute values of these differences.\n    # The resulting tensor 'delta' has the same shape as 'w'.\n    delta = torch.abs(torch.diff(w, dim=-1))\n    # Calculate the sum of 'delta' along the last dimension of 'delta'\n    # and divide it by the sum of 'w' along the last dimension of 'w'.\n    # The resulting tensor 'loss_intra' has the same shape as 't' and 'w',\n    # but with the last dimension removed.\n    loss_intra = torch.sum(delta, dim=-1) / torch.sum(w, dim=-1)\n\n    # Combine the inter-interval and intra-interval losses\n    # by taking the maximum of the two losses element-wise.\n    # The resulting tensor 'loss' has the same shape as 't' and 'w',\n    # but with the last dimension removed.\n    loss = torch.max(loss_inter, loss_intra)\n\n    # Return the"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n    # Compute the difference between the target values at the end of each interval\n    # and the target values at the start of the next interval\n    diff = (t[..., 1:] - t[..., :-1])  # 128\n\n    # Compute the difference between the weights at the end of each interval\n    # and the weights at the start of the next interval\n    w_diff = (w[..., 1:] - w[..., :-1])  # 128\n\n    # Compute the absolute difference between the target values at the end of each interval\n    # and the target values at the start of the next interval\n    abs_diff = torch.abs(diff)  # 128\n\n    # Compute the inner product of the weights at the end of each interval\n    # and the weights at the start of the next interval\n    w_inner = w_diff * diff  # 128\n\n    # Compute the outer product of the weights at the end of each interval\n    # and the weights at the start of the next interval\n    w_outer = torch.abs(w_diff) * abs_diff  # 128\n\n    # Compute the inner and outer distortion losses for the intervals\n    inner_distortion = w_inner * torch.abs(w_inner)  # 128\n    outer_distortion = w_outer * torch.abs(w_outer)  # 128\n\n    # Compute the total distortion loss by summing the inner and outer distortion losses\n    distortion = inner_distortion + outer_distortion  # 128\n\n    # Compute the total distortion loss by taking the mean across all intervals\n    total_distortion = torch.mean(distortion)  # 1\n\n    return total_distortion"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # Compute the distortion loss for the intervals between the target points.\n    # The first and last interval are not considered since they are not defined.\n    # The intervals are defined as (t_i, t_{i+1}) and the corresponding weights are (w_i, w_{i+1}).\n    # The distortion loss is calculated as the square of the difference between the interval length and the sum of the weights.\n    # The interval length is calculated as t_{i+1} - t_i, and the sum of the weights is calculated as w_i + w_{i+1}.\n    # The distortion loss is then squared and summed over all intervals.\n    # The final distortion loss is the average of this sum.\n    # The intra-interval loss is calculated as the square of the difference between the interval length and the weight.\n    # The interval length is calculated as t_{i+1} - t_i, and the weight is w_i.\n    # The intra-interval loss is then squared and summed over all intervals.\n    # The final intra-interval loss is the average of this sum.\n    # The final loss is the sum of the distortion loss and the intra-interval loss.\n\n    # Calculate the distortion loss for the intervals between the target points.\n    # The first and last interval are not considered since they are not defined.\n    # The intervals are defined as (t_i, t_{i+1}) and the corresponding weights are (w_i, w_{i+1}).\n    # The distortion loss is calculated as the square of the difference between the interval length and the sum of the weights.\n    # The interval length is calculated as t_{i+1} - t_i, and the sum of the weights is calculated as w_i + w_{i+1}.\n    # The distortion loss is then squared and summed over all intervals.\n    # The final distortion loss is the average of this sum.\n    distortion_loss = ((t[..., 1:] - t[..., :-1]) - (w[..., 1:] + w[..., :-1])).pow(2).mean()\n\n   "}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, device=t.device), cw, t)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, device=t.device), cw, t)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, device=t.device, dtype=t.dtype), cw, t)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, device=t.device), cw, t)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, device=t.device), cw, t)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    ps = torch.tensor(ps, dtype=cw.dtype, device=cw.device)\n    return interpolate(ps, cw, t)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Match the batch shape of t, w, and ps.\n    t, w = matchup_channels(t, w)\n    # Compute the CDF along the last axis of w.\n    cw = integrate_weights(w)\n    # Interpolate into the inverse CDF.\n    t_new = interpolate(ps, cw, t)\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n    # Interpolate each CDF at the query points.\n    cw_p = interpolate(ps, cw, t)\n    return cw_p\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Match up the channels of the tensors.\n    t, w = matchup_channels(t, w)\n\n    # Compute the integral of the weights.\n    cw = integrate_weights(w)\n\n    # Interpolate the integrated weights to find the weighted percentiles.\n    ps = torch.tensor(ps, device=cw.device)\n    t_percentiles = interpolate(ps[None, :], cw, t)\n\n    return t_percentiles\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    # Compute the CDF.\n    cw = integrate_weights(w)\n    # Interpolate into the inverse CDF.\n    return interpolate(torch.tensor(ps, device=t.device), cw, t)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw0 = integrate_weights(w)\n    cw1 = cw0[..., 1:]\n    idx_lo, idx_hi = searchsorted(cw0, ps)\n    denom = cw1 - cw0\n    denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom)\n    t_lo = torch.take_along_dim(t, idx_lo, dim=-1)\n    t_hi = torch.take_along_dim(t, idx_hi, dim=-1)\n    offset = (ps - cw0) / denom\n    t_mids = t_lo + offset * (t_hi - t_lo)\n    return t_mids\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n\n    # Compute the CDF.\n    cw = integrate_weights(w)\n    # Interpolate into the inverse CDF.\n    p = interpolate(torch.tensor(ps, device=t.device), cw, t)\n    return p\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    t = t.reshape(-1, t.shape[-1])\n    w = w.reshape(-1, w.shape[-1])\n\n    cw = integrate_weights(w)\n    cw = cw.reshape(-1, cw.shape[-1])\n    t = t.reshape(-1, t.shape[-1])\n    ps = torch.tensor(ps, device=t.device)\n    ps = ps.reshape(-1, ps.shape[-1])\n\n    t_lo = interpolate(ps, cw, t)\n    t_hi = interpolate(ps + 1e-8, cw, t)\n\n    return t_lo, t_hi\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n\n    cw = integrate_weights(w)\n    cw0 = cw[..., :-1]\n    cw1 = cw[..., 1:]\n\n    # Values of t where the CDFs change.\n    t0 = torch.where(cw0 == cw1, t[..., :1],\n                     interpolate(torch.tensor(ps, device=cw.device), cw0, t))\n    # Values of t where the CDFs change.\n    t1 = torch.where(cw0 == cw1, t[..., -1:],\n                     interpolate(torch.tensor(ps, device=cw.device), cw1, t))\n    return t0, t1\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n\n    # Compute the PDF and CDF for each weight vector, while ensuring that the CDF\n    # starts with exactly 0 and ends with exactly 1.\n    cw = integrate_weights(w)\n    # Interpolate each percentile w/r/t the CDF.\n    # torch.linspace(0, 1, 128)\n    return interpolate(torch.tensor(ps, device=t.device), cw, t)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, device=t.device), cw, t)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, device=t.device), cw, t)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    t, w = t.contiguous(), w.contiguous()\n    cw = integrate_weights(w)\n    cw0 = cw[..., :-1]\n    cw1 = cw[..., 1:]\n    t0 = t[..., :-1]\n    t1 = t[..., 1:]\n    i = torch.searchsorted(cw0, ps)\n    i0 = torch.max(torch.zeros_like(i), i - 1)\n    i1 = torch.min(torch.full_like(i, cw0.shape[-1] - 1), i)\n    t_lo = interpolate(ps, cw0[..., i0], t0[..., i0])\n    t_hi = interpolate(ps, cw1[..., i1], t1[..., i1])\n    return torch.where(t_lo == t_hi, t_lo, torch.where(cw0[..., i0] == cw1[..., i1], t_lo, (ps - cw0[..., i0]) * (t_hi - t_lo) / (cw1[..., i1] - cw0[..., i0]) + t_lo))\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n\n    cw = integrate_weights(w)\n    ps = torch.tensor(ps, device=cw.device)\n    idx = torch.searchsorted(cw, ps, side='right')\n    idx = idx.clip(1, cw.shape[-1] - 1)\n    cw0 = cw.gather(dim=-1, index=idx - 1)\n    cw1 = cw.gather(dim=-1, index=idx)\n    t0 = t.gather(dim=-1, index=idx - 1)\n    t1 = t.gather(dim=-1, index=idx)\n    alpha = (ps - cw0) / (cw1 - cw0)\n    return alpha * (t1 - t0) + t0\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Match the shapes of t and w.\n    t, w = matchup_channels(t, w)\n\n    # Compute the CDF.\n    cw = integrate_weights(w)\n\n    # Convert percentiles to fractions in [0, 1].\n    ps = torch.tensor(ps, device=t.device)\n    ps = ps.reshape(ps.shape + (1,) * t.ndim)\n\n    # Interpolate into the inverse CDF.\n    t_new = interpolate(ps, cw, t)\n\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Draw uniform samples.\n    u = torch.linspace(0.0, 1.0, num_samples, device=t.device)\n    # Invert the CDF.\n    t_new = invert_cdf(u, t, w)\n    # Perturb the samples.\n    if perturb:\n        mid = (u[:-1] + u[1:]) / 2\n        t_new = torch.sort(torch.cat([t_new, mid], dim=-1), dim=-1)[0]\n    # Jitter the samples.\n    if single_jitter:\n        jitter = torch.rand(t_new.shape[:-1], device=t.device) * (1.0 / num_samples)\n        t_new = t_new + jitter[..., None]\n    else:\n        jitter = torch.rand(t_new.shape, device=t.device) * (1.0 / num_samples)\n        t_new = t_new + jitter\n    return t_new"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n\n    # Draw uniform samples.\n    u = torch.linspace(0., 1., steps=num_samples, device=t.device)\n    if not perturb:\n        u = (u + torch.rand_like(u)) / (num_samples + 1)\n    elif perturb and not single_jitter:\n        u = u + (torch.rand_like(u) * (1. / num_samples))\n    else:\n        u = u + torch.rand_like(u[..., :1]) * (1. / num_samples)\n    u = u.clip(0., 1. - 1e-6)\n\n    # Invert CDF.\n    t_new = invert_cdf(u, t, w)\n\n    return t_new"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n    # Draw uniform samples.\n    u = torch.rand(list(w.shape[:-1]) + [num_samples], device=w.device)\n    # Invert the CDF.\n    t_new = invert_cdf(u, t, w)\n    # Perturb the samples.\n    if perturb:\n        t_new = perturb_rand(t_new)\n    # Jitter the samples.\n    if single_jitter:\n        t_new = jitter_rand(t_new)\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Draw uniform samples.\n    u = torch.rand(list(t.shape[:-1]) + [num_samples], device=t.device)\n\n    # Invert the CDF.\n    t_new = invert_cdf(u, t, w)\n\n    # Perturb samples.\n    if perturb:\n        mid = (t_new[..., 1:] + t_new[..., :-1]) / 2\n        if single_jitter:\n            jitter = torch.rand(t_new.shape[:-1] + [1], device=t.device)\n            jitter = (t_new[..., 1:] - t_new[..., :-1]) * jitter\n        else:\n            jitter = torch.rand(t_new.shape, device=t.device)\n            jitter = (t_new[..., 1:] - t_new[..., :-1]) * jitter\n        t_new = torch.stack([t_new[..., :-1], mid, t_new[..., 1:]], dim=-1)\n        t_new = t_new + jitter\n        t_new = torch.sort(t_new, dim=-1)[0]\n        t_new = torch.clip(t_new, t[..., 0], t[..., -1])\n\n    return t_new"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n    # Draw uniform samples.\n    u = torch.linspace(0., 1., num_samples + 1, device=t.device)\n    u = u[..., :-1] + torch.rand(list(u.shape[:-1]) + [num_samples], device=t.device) / num_samples\n    # Draw samples from the PDF.\n    t_new = invert_cdf(u, t, w)\n    # Perturb and resample.\n    if perturb:\n        mids = 0.5 * (t_new[..., 1:] + t_new[..., :-1])\n        upper = torch.cat([mids, t_new[..., -1:]], dim=-1)\n        lower = torch.cat([t_new[..., :1], mids], dim=-1)\n        # Stratified samples in those intervals.\n        u = torch.rand(list(t_new.shape[:-1]) + [num_samples + 1], device=t.device)\n        t_new = lower + (upper - lower) * u\n    # Jitter.\n    if single_jitter:\n        jitter = torch.rand(t_new.shape, device=t.device) * (t[..., 1:] - t[..., :-1]) / num_samples\n    else:\n        jitter = torch.rand(list(t_new.shape[:-1]) + [num_samples + 1], device=t.device) * (t[..., 1:] - t[..., :-1]) / num_samples\n    t_new = t_new + jitter\n    return t_new"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Compute the PDF and CDF for each weight vector.\n    p = weight_to_pdf(t, w)\n    c = integrate_weights(p)\n\n    # Draw uniform samples.\n    if perturb:\n        # Match the shape of c, but move the last dimension to the front.\n        shape = list(c.shape)\n        shape[0], shape[-1] = shape[-1], shape[0]\n        u = torch.rand(shape)\n    else:\n        shape = list(c.shape)\n        shape[-1] = num_samples\n        u = torch.linspace(0., 1., num_samples)\n        u = torch.broadcast_to(u, shape)\n\n    # Invert the CDF.\n    t_new = invert_cdf(u, t, w)\n\n    if perturb:\n        # Perturb the samples and then sort them.\n        t_new = perturb_samples(t_new)\n        t_new = torch.sort(t_new, dim=-1)[0]\n\n    if single_jitter:\n        # Apply the same jitter to every sample.\n        mids = .5 * (t_new[..., 1:] + t_new[..., :-1])\n        upper = torch.cat([mids, t_new[..., -1:]], dim=-1)\n        lower = torch.cat([t_new[..., :1], mids], dim=-1)\n        t_new = torch.lerp(upper, lower, torch.rand_like(t_new))\n    else:\n        # Apply independent jitter to each sample.\n        t_new += (torch.rand_like(t_new) - .5) * (t[..., 1:] - t[..., :-1])\n\n    return t_new"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Compute the PDF and CDF for each weight vector.\n    p = weight_to_pdf(t, w)\n    c = torch.cumsum(p, dim=-1)\n    c = torch.cat([c.new_zeros(c.shape[:-1] + (1,)), c], dim=-1)\n\n    # Take uniform samples.\n    if perturb:\n        # Draw uniform samples.\n        u = torch.rand(list(c.shape[:-1]) + [num_samples], device=c.device)\n        # Perturb them by a random amount.\n        if single_jitter:\n            jitter = torch.rand(c.shape[:-1] + [1], device=c.device)\n        else:\n            jitter = torch.rand(c.shape[:-1] + [num_samples], device=c.device)\n        u = (u - 0.5 + jitter)\n        # Noise in the CDF domain can cause out-of-bounds samples.\n        u = u.clip(0., 1.)\n    else:\n        # Draw uniform samples.\n        u = torch.linspace(0., 1., num_samples, device=c.device)\n        u = u[None, ...].expand(list(c.shape[:-1]) + [num_samples])\n\n    # Invert the CDF.\n    u = u.contiguous()\n    inds = searchsorted(c, u)\n    below = torch.max(torch.zeros_like(inds[0]), inds[0] - 1)\n    above = torch.min(inds[1], c.shape[-1] - 1)\n    inds_g = torch.stack([below, above], dim=-1)\n    matched_shape = [inds_g.shape[0], inds_g.shape[1], c.shape[-1]]\n    c_g = torch.gather(c.expand(matched_shape), 2, inds_g)\n    bins_"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n\n    # Draw uniform samples.\n    u = torch.rand(list(t.shape[:-1]) + [num_samples], device=t.device)\n\n    # Invert the CDF.\n    t_new = invert_cdf(u, t, w)\n\n    # Perturb the samples and then sort them.\n    if perturb:\n        t_new = perturb_and_sort(t_new, single_jitter=single_jitter)\n\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Draw uniform samples.\n    u = torch.linspace(0.0, 1.0, num_samples, device=t.device)\n\n    # Invert the CDF.\n    t_new = invert_cdf(u, t, w)\n\n    # Perturb samples to avoid numerical issues.\n    if perturb:\n        # Sample uniform noise from [0, 1) to match the shape of t_new.\n        u = torch.rand_like(t_new)\n        # Shift the samples to the right if the .5 falls to the right of t_new.\n        t_new += u * (t_new[..., 1:] - t_new[..., :-1])\n\n    # Jitter samples in time to avoid numerical issues due to large gaps.\n    if single_jitter:\n        # Apply the same jitter to all samples.\n        u = torch.rand(t_new.shape[:-1] + (1,), device=t_new.device)\n        t_new += u * (t_new[..., 1:] - t_new[..., :-1])\n    else:\n        # Apply a different jitter to each sample.\n        u = torch.rand_like(t_new)\n        t_new += u * (t_new[..., 1:] - t_new[..., :-1])\n\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n    # Draw uniform samples.\n    u = torch.linspace(0., 1., num_samples, device=t.device)\n    # Invert the CDF.\n    t_new = invert_cdf(u, t, w)\n    # Perturb the samples.\n    if perturb:\n        # Interpolate into the PDF.\n        t0 = torch.max(t[..., :-1], t_new - 1e-5)\n        t1 = torch.min(t[..., 1:], t_new + 1e-5)\n        u0 = torch.interp(t_new, t0, torch.zeros_like(t0), -100)\n        u1 = torch.interp(t_new, t1, torch.zeros_like(t1), -100)\n        # Draw uniform samples.\n        u_new = u0 + torch.rand(u0.shape, device=t.device) * (u1 - u0)\n        # Invert the CDF.\n        t_new = invert_cdf(u_new, t, w)\n    # Jitter by a random offset.\n    if single_jitter:\n        j = torch.rand(t_new.shape[:-1] + (1,), device=t.device)\n    else:\n        j = torch.rand(t_new.shape, device=t.device)\n    t_new = t_new + j * (t[..., 1:] - t[..., :-1]) / t.shape[-1]\n    return t_new"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n    # Draw uniform samples.\n    u = torch.linspace(0., 1., num_samples, device=w.device)\n    # Invert the CDF.\n    t_new = invert_cdf(u, t, w)\n    # Perturb the samples.\n    if perturb:\n        # Interpolate into the PDF.\n        t0 = torch.max(t[..., :-1], t_new - 1e-5)\n        t1 = torch.min(t[..., 1:], t_new + 1e-5)\n        u_lo = (t_new - t0) / (t1 - t0)\n        u_hi = (t1 - t_new) / (t1 - t0)\n        w_lo = torch.gather(w, -1, idx_lo)\n        w_hi = torch.gather(w, -1, idx_hi)\n        w_pert = u_lo * w_lo + u_hi * w_hi\n        # Draw samples from the perturbed PDF.\n        t_new = invert_cdf(u, t_new, w_pert)\n    # Jitter by a random amount in [0, 1).\n    if single_jitter:\n        j = torch.rand(t.shape[:-1] + (1,), device=t.device)\n    else:\n        j = torch.rand(t_new.shape, device=t.device)\n    return torch.clip(t_new + j, t[..., 0], t[..., -1])"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n    # Draw uniform samples.\n    u = torch.linspace(0., 1., num_samples, device=t.device)\n    # Match the batch shape of u to that of t.\n    shape = list(t.shape[:-1]) + [num_samples]\n    u = torch.reshape(u, shape)\n    # Draw samples from the inverse CDF.\n    t_new = invert_cdf(u, t, w)\n    # Perturb the samples.\n    if perturb:\n        # Interpolate into the PDF.\n        t0 = torch.max(t[..., :-1], t_new - 1e-5)\n        t1 = torch.min(t[..., 1:], t_new + 1e-5)\n        u_lo = (t_new - t0) / (t1 - t0)\n        u_hi = (t1 - t_new) / (t1 - t0)\n        cdf_lo = torch.cumsum(w * u_lo[..., None], dim=-1)\n        cdf_hi = torch.cumsum(w * u_hi[..., None], dim=-1)\n        cdf_hat = cdf_lo[..., :-1] + (u - cdf_lo[..., :-1]) / (cdf_hi[..., 1:] - cdf_lo[..., :-1] + 1e-8)\n        cdf_hat = torch.minimum(cdf_hat, torch.full_like(cdf_hat, 1.))\n        # Compute the inverse CDF.\n        t_new = invert_cdf(cdf_hat, t, w)\n    # Jitter by a random amount.\n    if single_jitter:\n        jitter = torch.rand(list(t.shape[:-1]) + [1], device=t.device)\n        t_new = (t_new + jitter) % 1.\n   "}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n\n    # Draw uniform samples.\n    u = torch.linspace(0., 1., num_samples, device=t.device)\n    if not single_jitter:\n        u = u[..., None] + torch.empty_like(t[..., :1]).uniform_(0., 1. / num_samples)\n        u = u.clip(0., 1. - 1. / num_samples)\n    else:\n        u = u + torch.empty_like(t[..., :1]).uniform_(0., 1. / num_samples)\n        u = u.clip(0., 1. - 1. / num_samples)\n\n    # Draw samples from the PDF.\n    t_new = invert_cdf(u, t, w)\n\n    # Perturb the samples.\n    if perturb:\n        # This is equivalent to upsampling the step function and then downsampling\n        # it to num_samples.\n        midpoints = (t[..., :-1] + t[..., 1:]) / 2\n        t_new = torch.gather(midpoints, -1,\n                             torch.searchsorted(t[..., 1:], t_new, right=True))\n\n    return t_new"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Match the shapes of t, w, and u.\n    t, w = matchup_channels(t, w)\n    u = torch.linspace(0., 1., num_samples, device=t.device)\n\n    # Draw uniform samples.\n    if perturb:\n        # Match the shapes of u and t.\n        if u.ndim == 1:\n            u = u.expand(t.shape[:-1] + (num_samples,))\n\n        # Draw uniform samples.\n        if single_jitter:\n            # Add a random offset to each sample -- the same offset is used for all\n            # dimensions of each sample.\n            jitter = torch.rand(list(t.shape[:-1]) + [1], device=t.device)\n        else:\n            # Add random offsets to each sample -- each dimension of each sample\n            # gets a different offset.\n            jitter = torch.rand(list(t.shape[:-1]) + [t.shape[-1], 1], device=t.device)\n        u = (u + jitter) / (1 + num_samples)\n\n    # Invert the CDF.\n    t_new = invert_cdf(u, t, w)\n\n    return t_new"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Match the shapes of t and w.\n    t, w = matchup_channels(t, w)\n\n    # Generate samples from the PDF.\n    u = torch.linspace(0, 1, num_samples + 1, device=t.device)\n    t_new = invert_cdf(u, t, w)\n\n    # Perturb the samples to avoid sample clustering at bin boundaries.\n    if perturb:\n        # Sample uniform noise to match the shape of t_new, then take the sign of\n        # the noise and multiply it by the difference between adjacent samples.\n        # This avoids sample clustering at bin boundaries.\n        noise = torch.rand_like(t_new) * (t_new[..., 1:] - t_new[..., :-1])\n        noise = torch.sign(noise) * torch.cat([noise, torch.zeros_like(noise[..., :1])], dim=-1)\n        t_new = t_new + noise\n\n    # Jitter the samples to avoid sample clustering at bin boundaries.\n    if single_jitter:\n        # Sample uniform noise to match the shape of t_new, then add it to the\n        # samples. This avoids sample clustering at bin boundaries.\n        noise = torch.rand_like(t_new) * (t_new[..., 1:] - t_new[..., :-1])\n        noise = torch.cat([noise, torch.zeros_like(noise[..., :1])], dim=-1)\n        t_new = t_new + noise\n\n    return t_new"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Draw uniform samples.\n    u = torch.linspace(0., 1., num_samples + 1, device=t.device)\n    # Invert the CDF.\n    t_new = invert_cdf(u, t, w)\n    # Perturb the samples.\n    if perturb:\n        # Take a finite difference of the CDF to get the PDF.\n        t0 = torch.cat([t[..., :1], t], dim=-1)\n        t1 = torch.cat([t, t[..., -1:]], dim=-1)\n        cdf = integrate_weights(w)\n        pdf = (cdf[..., 1:] - cdf[..., :-1]) / (t1[..., 1:] - t0[..., :-1])\n        # Perturb the samples and make sure we don't go outside the unit interval.\n        if single_jitter:\n            m = torch.distributions.Bernoulli(probs=0.5)\n            d = m.sample(t.shape[:-1] + (num_samples,)) * 2 - 1\n            t_new = (t_new[..., None, :] + d * pdf[..., None, :] / (pdf.sum(-1)[..., None] + 1e-10)).clip(0., 1.)\n        else:\n            d = torch.rand(t.shape[:-1] + (num_samples,), device=t.device)\n            t_new = (t_new[..., None, :] + d[..., None, :] * pdf[..., None, :] / (pdf.sum(-1)[..., None] + 1e-10)).clip(0., 1.)\n    return t_new"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n    # Draw uniform samples.\n    u = torch.linspace(0.0, 1.0, num_samples + 1, device=t.device)[:-1]\n    if perturb:\n        # Linearly interpolate between the samples following Section 5.2 of the\n        # paper \"A Multidimensional Version of Importance Sampling\" by Matt Pharr.\n        u = u + (u - torch.rand_like(u)) / num_samples\n    else:\n        u = u + torch.rand_like(u) / num_samples\n    u = u.clip(0.0, 1.0 - 1e-8)\n\n    # Invert the CDF.\n    t_new = invert_cdf(u, t, w)\n\n    # Perturb and resample.\n    if perturb:\n        t_new = t_new + (torch.rand_like(t_new) * (t[..., 1] - t[..., :-1]))\n\n    if single_jitter:\n        t_new = t_new + (torch.rand_like(t_new) * (t[..., 1] - t[..., :-1]))\n    else:\n        t_new = t_new + (torch.rand_like(t_new) * (t[..., 1:] - t_new + t_new - t[..., :-1]))\n\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n\n    # Draw uniform samples.\n    if perturb:\n        # Match the shape of cw[:-1] with the shape of t.\n        shape = cw[:-1].shape + (t.shape[-1],)\n        # Sample continuous positions u without any jittering or perturbations.\n        u = torch.rand(shape, device=t.device)\n        # Perturb the positions [t0, t1], ..., [tn-1, tn], to make up for the\n        # fact that our values are sampled at discrete positions.\n        if single_jitter:\n            # Apply the same perturbation to every sample.\n            jitter = torch.rand((t.shape[-1],), device=t.device)\n            u += jitter\n        else:\n            # Apply a different perturbation to each sample.\n            jitter = torch.rand(shape, device=t.device)\n            u += jitter\n        u /= t.shape[-1]  # t.shape[-1] = 128\n    else:\n        # Match the shape of cw[:-1] with the shape of t.\n        shape = cw[:-1].shape + (num_samples,)\n        # Sample continuous positions u without any jittering or perturbations.\n        u = torch.linspace(0., 1., num_samples, device=t.device)\n        u = torch.broadcast_to(u, shape)\n\n    # Invert the CDF.\n    t_new = invert_cdf(u, t, w)\n    return t_new"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n    # Draw uniform samples.\n    u = torch.linspace(0., 1., num_samples, device=w.device)\n    # Match the batch shape of u to that of t.\n    # u = torch.zeros_like(t[..., 0, None]) + u\n    u = u.expand(t.shape[:-1] + (num_samples,))\n    # Draw samples from the inverse CDF.\n    t_new = invert_cdf(u, t, w)\n    # Perturb the samples.\n    if perturb:\n        # This is equivalent to using sample_uniform((0, 1)) to select a bin and\n        # then sample_uniform((t[i], t[i+1])) to sample uniformly from that bin.\n        # This means that we don't have to worry about the case where a sample\n        # can fall outside the domain due to finite floating point inaccuracies.\n        t_lo, t_hi = torch.min(t[..., :-1], t[..., 1:]), torch.max(t[..., :-1], t[..., 1:])\n        t_pad = torch.tensor([t_lo, t_hi], device=t.device)\n        t_new = torch.clip(t_new, t_pad[0], t_pad[1])\n        bins = torch.stack([t_new, t_new], dim=-1)\n        cdf_hat = integrate_weights(w)\n        cdf_hat = torch.cat([torch.zeros_like(cdf_hat[..., :1]), cdf_hat], dim=-1)\n        index_samples = torch.searchsorted(cdf_hat, u, right=True) - 1\n        bin_indices = torch.stack([index_samples, index_samples + 1], dim=-1)\n        t_new = sample_uniform((t_lo, t_hi), bins, bin_indices)"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n    # Draw uniform samples.\n    u = torch.rand(list(t.shape[:-1]) + [num_samples], device=t.device)\n    # Draw samples from the PDF.\n    t_new = invert_cdf(u, t, w)\n\n    # Perturb samples.\n    if perturb:\n        t_new = perturb_samples(t_new)\n\n    # Apply the same jitter to all samples.\n    if single_jitter:\n        jitter = torch.rand((t.shape[0],), device=t.device)\n        t_new = t_new + jitter[..., None] * (t[..., -1] - t[..., 0])\n\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t_dilate = t[..., :-1] + dilation * (t[..., 1:] - t[..., :-1])\n    t_dilate = torch.cat([t_dilate, torch.full_like(t_dilate[..., :1], domain[1])], dim=-1)\n    t_dilate = torch.clamp(t_dilate, domain[0], domain[1])\n\n    p_dilate = torch.maximum(w[..., :-1], w[..., 1:])\n    p_dilate = torch.cat([p_dilate, torch.zeros_like(p_dilate[..., :1])], dim=-1)\n\n    return t_dilate, p_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute the dilated time steps.\n    t_dilate = t[..., :-1] + dilation * (t[..., 1:] - t[..., :-1])\n\n    # Clip the dilated time steps to the specified domain.\n    t_dilate = torch.maximum(t_dilate, domain[0])\n    t_dilate = torch.minimum(t_dilate, domain[1])\n\n    # Compute the adjusted weights.\n    w_dilate = torch.maximum(w, 0)\n\n    # Return the dilated time steps and adjusted weights.\n    return t_dilate, w_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n    t_dilate = t[..., :-1] + dilation * (t[..., 1:] - t[..., :-1])\n    t_dilate = torch.cat([t_dilate, torch.full_like(t_dilate[..., :1], 1e10)], dim=-1)\n    t_dilate = torch.clamp(t_dilate, domain[0], domain[1])\n    p_dilate = torch.maximum(w, torch.cat([w[..., 1:], torch.zeros_like(w[..., :1])], dim=-1))\n    return t_dilate, p_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute the dilated time steps.\n    t_dilate = t[..., :-1] + dilation * (t[..., 1:] - t[..., :-1])\n\n    # Clip the dilated time steps to the specified domain.\n    t_dilate = torch.max(t_dilate, domain[0])\n    t_dilate = torch.min(t_dilate, domain[1])\n\n    # Compute the weights for the dilated time steps.\n    w_dilate = torch.max(w, torch.where(t_dilate[..., :-1] <= t[..., 1:], w, 0))\n    w_dilate = torch.max(w_dilate, torch.where(t_dilate[..., 1:] >= t[..., :-1], w, 0))\n\n    return t_dilate, w_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n    t_dilate = t[..., :-1] + dilation * (t[..., 1:] - t[..., :-1])\n    t_dilate = torch.cat([t_dilate, torch.full_like(t_dilate[..., :1], domain[1])], dim=-1)\n    t_dilate = torch.clamp(t_dilate, domain[0], domain[1])\n    w_dilate = torch.max(torch.where(t_dilate[..., None, :] > t[..., :, None], w[..., None, :], 0), dim=-2)[0]\n    return t_dilate, w_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute the dilation factor and clamp to the domain.\n    dilated_t = t[..., 1:] - t[..., :-1]\n    dilated_t = torch.clamp(dilation * dilated_t, min=domain[0], max=domain[1])\n\n    # Compute the new time steps and weights.\n    t_dilate = torch.cat([t[..., :1], torch.cumsum(dilated_t, dim=-1)], dim=-1)\n    w_dilate = torch.max(w, dim=-1, keepdim=True)[0]\n\n    return t_dilate, w_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t_dilate = torch.cat([t[..., :1], t[..., 1:] * dilation], dim=-1)\n    t_dilate = torch.clamp(t_dilate, *domain)\n    p_dilate = torch.maximum(w, torch.maximum(w - t_dilate[..., :-1], t_dilate[..., 1:] - w))\n    return t_dilate, p_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Dilate the time steps.\n    t_dilate = t[..., None, :] + dilation * (t[..., 1:] - t[..., :-1])[..., None]\n\n    # Clip the dilated time steps to the specified domain.\n    t_dilate = torch.cat([torch.full_like(t_dilate[..., :1], domain[0]), t_dilate, torch.full_like(t_dilate[..., :1], domain[1])], dim=-1)\n    t_dilate = torch.clamp(t_dilate, domain[0], domain[1])\n\n    # Adjust the weights to match the dilated time steps.\n    w_dilate = query(t_dilate, t, w)\n\n    return t_dilate, w_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t_dilate = t[..., :-1] + dilation * (t[..., 1:] - t[..., :-1])\n    t_dilate = torch.cat([t_dilate, t[..., -1:]], dim=-1)\n    t_dilate = torch.clip(t_dilate, domain[0], domain[1])\n\n    p = weight_to_pdf(t, w)\n    p_dilate = torch.maximum(p[..., :-1], p[..., 1:])\n    p_dilate = torch.cat([p_dilate, torch.zeros_like(p[..., :1])], dim=-1)\n\n    return t_dilate, p_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Dilate the time steps.\n    t_dilate = t[..., :-1] + dilation * (t[..., 1:] - t[..., :-1])\n\n    # Clip the dilated time steps to the specified domain.\n    t_dilate = torch.cat([t_dilate[..., :1], t_dilate, t_dilate[..., -1:]], dim=-1)\n    t_dilate = torch.clip(t_dilate, domain[0], domain[1])\n\n    # Compute the weights for the dilated time steps.\n    p = weight_to_pdf(t, w)\n    t_dilate, p_dilate = max_dilate(t, p, dilation, domain=domain)\n    w_dilate = pdf_to_weight(t_dilate, p_dilate)\n\n    return t_dilate, w_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n    t_dilate = t[..., 0] + dilation * (t[..., 1:] - t[..., :-1])\n    t_dilate = torch.cat([t_dilate, torch.full_like(t_dilate[..., :1], 2)], dim=-1)\n    t_dilate = torch.clamp(t_dilate, domain[0], domain[1])\n    w_dilate = torch.maximum(w[..., 1:], w[..., :-1])\n    return t_dilate, w_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute the dilated time steps.\n    t_dilate = t[..., 1:] - dilation * (t[..., 1:] - t[..., :-1])\n\n    # Clip the dilated time steps to the specified domain.\n    t_dilate = torch.clip(t_dilate, domain[0], domain[1])\n\n    # Compute the weights associated with the dilated time steps.\n    w_dilate = torch.maximum(w, torch.where(t_dilate[..., :-1] < t[..., 1:], w, 0.0))\n    w_dilate = torch.where(t_dilate[..., 1:] > t[..., :-1], w_dilate, 0.0)\n\n    # Return the dilated time steps and weights.\n    return t_dilate, w_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute the dilation factor based on the specified dilation parameter.\n    dilation_factor = 1 + dilation\n\n    # Compute the dilated time steps by multiplying the original time steps with the dilation factor.\n    t_dilate = dilation_factor * t\n\n    # Clip the dilated time steps to the specified domain.\n    t_dilate = torch.clip(t_dilate, domain[0], domain[1])\n\n    # Compute the weights of the dilated time steps by taking the maximum of the weights of the original time steps.\n    w_dilate = torch.max(w, dim=-1, keepdim=True)[0]\n\n    # Return the dilated and clipped time steps and the adjusted weights.\n    return t_dilate, w_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute the dilated time steps.\n    t_dilate = t[..., :-1] + dilation * (t[..., 1:] - t[..., :-1])\n\n    # Clip the dilated time steps to the specified domain.\n    t_dilate = torch.clip(t_dilate, domain[0], domain[1])\n\n    # Compute the dilated weights.\n    w_dilate = torch.max(w[..., :-1], w[..., 1:])\n\n    # Ensure that the weights at the beginning and end of the interval are zero.\n    w_dilate = torch.cat([torch.zeros_like(w_dilate[..., :1]), w_dilate, torch.zeros_like(w_dilate[..., :1])], dim=-1)\n\n    # Return the dilated time steps and weights.\n    return t_dilate, w_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n    # Compute the dilation of the time steps.\n    t_dilate = t[..., 1:] - t[..., :-1]\n    t_dilate = torch.cat([t_dilate[..., :1], t_dilate], dim=-1)\n    t_dilate = torch.cumsum(t_dilate, dim=-1)\n    t_dilate = torch.cat([torch.zeros_like(t_dilate[..., :1]), t_dilate], dim=-1)\n    t_dilate = torch.clamp(t_dilate, domain[0], domain[1])\n\n    # Compute the dilation of the weights.\n    w_dilate = torch.maximum(w[..., 1:], w[..., :-1])\n    w_dilate = torch.cat([w_dilate[..., :1], w_dilate], dim=-1)\n\n    # Clip the dilation of the time steps.\n    t_dilate = torch.clamp(t_dilate, domain[0], domain[1])\n\n    # Compute the dilation of the weights.\n    w_dilate = torch.maximum(w[..., 1:], w[..., :-1])\n    w_dilate = torch.cat([w_dilate[..., :1], w_dilate], dim=-1)\n\n    return t_dilate, w_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute the dilation of the time steps.\n    t_dilate = t[..., 1:] - dilation * (t[..., 1:] - t[..., :-1])\n    # Clip the dilated time steps to the specified domain.\n    t_dilate = torch.cat([torch.full_like(t_dilate[..., :1], domain[0]), t_dilate, torch.full_like(t_dilate[..., :1], domain[1])], dim=-1)\n\n    # Compute the adjusted weights based on the dilated time steps.\n    p = weight_to_pdf(t, w)\n    p_dilate = query(t_dilate, t, p, outside_value=0)\n    w_dilate = pdf_to_weight(t_dilate, p_dilate)\n\n    return t_dilate, w_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute the dilated time steps.\n    t_dilate = dilation * t\n\n    # Clip the dilated time steps to the specified domain.\n    t_dilate = torch.clip(t_dilate, domain[0], domain[1])\n\n    # Compute the weights associated with the dilated time steps.\n    p_dilate = torch.maximum(torch.maximum(t_dilate[..., :-1], t[..., :-1]), torch.maximum(t_dilate[..., 1:], t[..., 1:])) - torch.minimum(torch.minimum(t_dilate[..., :-1], t[..., :-1]), torch.minimum(t_dilate[..., 1:], t[..., 1:]))\n    p_dilate = torch.where(p_dilate > 0, p_dilate, torch.zeros_like(p_dilate))\n\n    # Return the dilated and clipped time steps and the adjusted weights.\n    return t_dilate, p_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute the dilation factor based on the specified dilation parameter.\n    dilation_factor = 2**dilation\n\n    # Compute the dilated time steps by multiplying the original time steps with the dilation factor.\n    t_dilate = t[..., :-1] * dilation_factor\n\n    # Clip the dilated time steps to the specified domain.\n    t_dilate = torch.clip(t_dilate, domain[0], domain[1])\n\n    # Compute the weights for the dilated time steps by taking the maximum of the original weights.\n    w_dilate = torch.max(w, dim=-1, keepdim=True)[0]\n\n    # Return the dilated and clipped time steps and the weights.\n    return t_dilate, w_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    t, w = matchup_channels(t, w)\n\n    # Compute the dilation factor for each interval.\n    t_dilate = torch.cat([t[..., :1], t[..., 1:] + dilation], dim=-1)\n    t_dilate = torch.clamp(t_dilate, domain[0], domain[1])\n\n    # Compute the weights for each dilation.\n    p_dilate = torch.max(torch.where(t_dilate[..., :-1, None] <= t[..., None, :], w[..., None, :], 0), dim=-1)[0]\n\n    return t_dilate, p_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Compute the dilation factor for each time step.\n    dilation_factor = (t[..., 1:] - t[..., :-1]) * dilation\n\n    # Compute the dilated time steps.\n    t_dilate = torch.cat([t[..., :1], t[..., 1:] + dilation_factor], dim=-1)\n\n    # Clip the dilated time steps to the specified domain.\n    t_dilate = torch.clip(t_dilate, domain[0], domain[1])\n\n    # Compute the adjusted weights.\n    p = weight_to_pdf(t, w)\n    p_dilate = torch.max(p[..., None, :], torch.zeros_like(t_dilate[..., :, None]))\n    p_dilate = torch.where(t_dilate[..., None, :] > t_dilate[..., None, :-1], p_dilate, torch.zeros_like(p_dilate))\n    p_dilate = torch.where(t_dilate[..., None, :] < t_dilate[..., None, 1:], p_dilate, torch.zeros_like(p_dilate))\n    p_dilate = torch.sum(p_dilate, dim=-1)\n\n    return t_dilate, p_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Interpolate the values at the query times.\n    yq = torch.interp(tq, t, y, left=outside_value, right=outside_value)\n\n    # If a query time matches a step change time, return the outside value.\n    yq = torch.where(torch.isclose(tq, t), outside_value, yq)\n\n    return yq"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    tq, t, y = matchup_channels(tq, t, y)\n\n    # Find the indices of the step function changes that are to the left of each query time.\n    idx = torch.searchsorted(t, tq, side='left')\n\n    # If a query time matches a step change, return the outside value.\n    # Otherwise, interpolate the value at the query time.\n    yq = torch.where(\n        (idx > 0) & (idx < t.shape[-1]),\n        torch.lerp(y[..., idx - 1], y[..., idx], (tq - t[..., idx - 1]) / (t[..., idx] - t[..., idx - 1])),\n        outside_value)\n\n    return yq"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    tq, t, y = matchup_channels(tq, t, y)\n    y_out = torch.full_like(tq, outside_value)\n    mask = (tq >= t[..., :-1]) & (tq < t[..., 1:])\n    tq_sel = torch.where(mask, tq, torch.full_like(tq, torch.nan))\n    y_sel = torch.where(mask, y[..., :-1], torch.full_like(y[..., :-1], torch.nan))\n    y_out = torch.where(~torch.isnan(tq_sel), y_sel, y_out)\n    y_out = torch.where(tq == t[..., -1], y[..., -1], y_out)\n    return y_out"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    tq, t, y = matchup_channels(tq, t, y)\n\n    # Find the indices of the time steps where the query time is between two step function times.\n    indices = torch.sum(torch.ge(tq[..., :, None], t[..., None, :]), -1) - 1\n    indices = torch.clamp(indices, 0, y.shape[-1] - 1)\n\n    # Compute the interpolation weights for each query time.\n    weights = (tq - t.gather(dim=-1, index=indices)) / (t.gather(dim=-1, index=indices + 1) - t.gather(dim=-1, index=indices) + 1e-8)\n\n    # Interpolate the step function values at the query times.\n    yq = y.gather(dim=-1, index=indices) + weights * (y.gather(dim=-1, index=indices + 1) - y.gather(dim=-1, index=indices))\n\n    # Set the interpolated values to the outside value for query times that exactly match a step change time.\n    yq = torch.where(torch.eq(tq, t.gather(dim=-1, index=indices)), outside_value, yq)\n\n    return yq"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    t, y = matchup_channels(t, y)\n    y = torch.cat([y[..., :1], y, y[..., -1:]], dim=-1)\n    t = torch.cat([t[..., :1], t, t[..., -1:]], dim=-1)\n    y = torch.where((tq[..., None, :] >= t[..., None, :-1]) & (tq[..., None, :] < t[..., None, 1:]),\n                   y[..., None, 1:], y[..., None, :1])\n    y = torch.where(tq[..., None, :] == t[..., None, :-1], y[..., None, :-1], y)\n    y = y[..., 0]\n    return y\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    tq, t, y = matchup_channels(tq, t, y)\n\n    # Compute the weights for the linear interpolation.\n    w = torch.where(t[..., :-1] <= tq[..., :, None],\n                    torch.where(tq[..., :, None] < t[..., 1:],\n                               (tq[..., :, None] - t[..., :-1, None]) / (t[..., 1:, None] - t[..., :-1, None]),\n                               torch.zeros_like(t[..., :-1, None])),\n                    torch.zeros_like(t[..., :-1, None]))\n\n    # Interpolate the values using the weights.\n    yq = torch.sum(w * y[..., :-1, None], dim=-2)\n\n    # Set the values to the outside value where the query times exactly match a step change time.\n    yq = torch.where(t[..., :-1] == tq[..., :, None], outside_value, yq)\n\n    return yq"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    tq, t, y = matchup_channels(tq, t, y)\n    y = torch.cat([y[..., :1], y], dim=-1)\n    t = torch.cat([t[..., :1], t], dim=-1)\n    indices = torch.searchsorted(t, tq)\n    t0 = torch.maximum(t[..., indices - 1], tq)\n    t1 = torch.minimum(t[..., indices], tq)\n    y0 = y[..., indices - 1]\n    y1 = y[..., indices]\n    y_query = torch.where(t0 == t1, y0, (t1 - tq) * y0 + (tq - t0) * y1)\n    return y_query"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Find the indices of the time values that are immediately before each query time.\n    indices = torch.searchsorted(t, tq, side='right') - 1\n\n    # Clamp the indices to the valid range of indices for the time values.\n    indices = torch.clamp(indices, 0, len(t) - 2)\n\n    # Compute the weights for each query time based on the distance between the query time and the time values immediately before and after it.\n    weights = (tq - t[indices]) / (t[indices + 1] - t[indices])\n\n    # Compute the interpolated values of the step function at the query times based on the weights and the values of the step function at the time values immediately before the query times.\n    yq = torch.where(tq == t[indices + 1], outside_value, y[indices] + weights * (y[indices + 1] - y[indices]))\n\n    return yq"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # The query times are sorted, so we can use searchsorted to find the index of the\n    # first time that is greater than or equal to each query time.\n    idx = torch.searchsorted(t, tq, side='right')\n\n    # If the query time is exactly equal to a step change time, return the\n    # outside value. Otherwise, interpolate between the two adjacent step function\n    # values.\n    return torch.where(\n        idx == 0, outside_value,\n        torch.where(idx == t.shape[-1], outside_value,\n                    (tq - t[idx - 1]) / (t[idx] - t[idx - 1]) * (y[idx] - y[idx - 1]) + y[idx - 1]))"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # tq: [..., num_query_times]\n    # t: [..., num_steps]\n    # y: [..., num_steps]\n    tq, t, y = matchup_channels(tq, t, y)\n\n    # Compute the indices of the step function changes that are to the left of each query time.\n    indices = torch.sum(torch.ge(tq[..., :, None], t[..., None, :]), dim=-1) - 1\n    indices = torch.clamp(indices, 0, y.shape[-1] - 2)\n\n    # Compute the interpolation weights for each query time.\n    weights = (tq - t[..., indices]) / (t[..., indices + 1] - t[..., indices] + 1e-8)\n\n    # Compute the interpolated values for each query time.\n    yq = torch.where(\n        torch.logical_and(tq >= t[..., indices], tq < t[..., indices + 1]),\n        (1 - weights) * y[..., indices] + weights * y[..., indices + 1],\n        outside_value)\n\n    return yq"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # tq, t, y = map(torch.as_tensor, (tq, t, y))\n    # assert tq.ndim == t.ndim == y.ndim == 1\n    # assert t.shape[0] == y.shape[0]\n    # assert t.min() <= tq.min() and tq.max() <= t.max()\n\n    # Find the indices of the step changes that occur before each query time.\n    indices = torch.searchsorted(t, tq, side='right') - 1\n\n    # If a query time exactly matches a step change time, return the outside value.\n    # Otherwise, interpolate the value at the query time based on the step function.\n    return torch.where(\n        torch.abs(tq - t[indices]) < 1e-6,\n        outside_value,\n        y[indices] + (y[indices + 1] - y[indices]) * (tq - t[indices]) / (t[indices + 1] - t[indices])\n    )\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Interpolate the values at the query times.\n    y_interp = interpolate(tq, t, y)\n\n    # Find the indices of the query times that exactly match a step change time.\n    t_lo, t_hi = searchsorted(t, tq)\n    t_eq = torch.where(t[..., t_lo] == tq, t_lo, -1)\n\n    # Replace the interpolated values at the exact step change times with the outside value.\n    y_interp = torch.where(t_eq[..., None] == torch.arange(y.shape[-1])[None, ...],\n                           outside_value, y_interp)\n\n    return y_interp\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check that the query times are within the range of the step function.\n    assert torch.all(tq >= t[0]), f\"Query times must be >= {t[0]}\"\n    assert torch.all(tq <= t[-1]), f\"Query times must be <= {t[-1]}\"\n\n    # Find the indices of the step function changes that occur before each query time.\n    indices = torch.searchsorted(t, tq, side='right') - 1\n\n    # If a query time matches a step change, return the outside value.\n    # Otherwise, interpolate the value at the query time based on the step function.\n    t0, t1 = t[indices], t[indices + 1]\n    y0, y1 = y[indices], y[indices + 1]\n    y_query = torch.where(tq == t0, outside_value, (t1 - tq) * y0 + (tq - t0) * y1) / (t1 - t0)\n\n    return y_query\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    t, y = matchup_channels(t, y)\n\n    # If the query times are outside the range of the step function, return the\n    # outside value.\n    if tq.min() < t.min() or tq.max() > t.max():\n        return outside_value\n\n    # Find the indices of the step function times that are less than or equal to\n    # the query times.\n    indices = torch.sum(torch.ge(tq[..., None], t[..., :, None]), -1) - 1\n    indices = indices.clip(0, t.shape[-1] - 2)\n\n    # Compute the interpolation weights for the query times.\n    t0 = t.gather(-1, indices)\n    t1 = t.gather(-1, indices + 1)\n    alpha = (tq - t0) / (t1 - t0)\n\n    # Interpolate the step function values at the query times.\n    y0 = y.gather(-1, indices)\n    y1 = y.gather(-1, indices + 1)\n    yq = (1 - alpha) * y0 + alpha * y1\n\n    return yq\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # tq: [B, P, I], t: [B, P, T], y: [B, P, T]\n\n    # Compute the values of the step function at the query times.\n    tq_shape = tq.shape\n    tq = tq.reshape(-1, tq.shape[-1])\n    t = t.reshape(-1, t.shape[-1])\n    y = y.reshape(-1, y.shape[-1])\n\n    # Compute the values of the step function at the query times.\n    yq = torch.where(\n        tq[..., None, :] >= t[..., :, None],\n        y[..., :, None],\n        y[..., :1, None],\n    )\n    yq = yq[..., -1]\n\n    # If a query time matches a step change, return the outside value.\n    yq = torch.where(\n        torch.logical_and(\n            tq[..., None, :] == t[..., :, None],\n            y[..., :, None] != y[..., 1:, None],\n        ),\n        outside_value,\n        yq,\n    )\n\n    # Reshape the output to the original shape of the query times.\n    yq = yq.reshape(tq_shape)\n    return yq\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    t, y = matchup_channels(t, y)\n    # tq = tq.reshape(tq.shape[:-1] + (1,))\n    # t = t.reshape(t.shape[:-1] + (1,))\n    # y = y.reshape(y.shape[:-1] + (1,))\n\n    # Compute the indices of the time changes that occur before each query time.\n    indices = torch.sum(torch.ge(tq[..., None, :], t[..., :, None]), -1) - 1\n    indices = torch.clamp(indices, 0, y.shape[-1] - 1)\n\n    # Compute the interpolation weights for each query time.\n    weights = (tq - t.gather(dim=-1, index=indices)) / (t.gather(dim=-1, index=indices + 1) - t.gather(dim=-1, index=indices) + 1e-8)\n    weights = torch.where(torch.logical_or(torch.le(tq, t.gather(dim=-1, index=indices)), torch.ge(tq, t.gather(dim=-1, index=indices + 1))), outside_value, weights)\n\n    # Interpolate the values of the step function at the query times.\n    yq = weights * y.gather(dim=-1, index=indices + 1) + (1 - weights) * y.gather(dim=-1, index=indices)\n\n    return yq\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # tq: [..., num_query], t: [..., num_steps + 1], y: [..., num_steps]\n    tq, t, y = matchup_channels(tq, t, y)\n    # tq: [..., num_query], t: [..., num_steps + 1], y: [..., num_steps + 1]\n    t = torch.cat([t, torch.ones_like(t[..., -1:]) * 2], dim=-1)\n    y = torch.cat([y, y[..., -1:]], dim=-1)\n\n    # Compute the indices of the step changes that are immediately before each query time.\n    indices = torch.searchsorted(t, tq, side='right') - 1\n    indices = torch.clamp(indices, 0, t.shape[-1] - 2)\n\n    # Compute the query values by interpolating between the step function values.\n    t0 = t.gather(dim=-1, index=indices)\n    t1 = t.gather(dim=-1, index=indices + 1)\n    y0 = y.gather(dim=-1, index=indices)\n    y1 = y.gather(dim=-1, index=indices + 1)\n    yq = (tq - t0) / (t1 - t0) * (y1 - y0) + y0\n\n    # Set the query values to the outside value where the query time matches a step change.\n    yq = torch.where(tq == t0, outside_value, yq)\n\n    return yq"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    t, y = matchup_channels(t, y)\n    tq = tq[..., None]\n    t, tq = torch.broadcast_tensors(t, tq)\n    y = torch.where(t[..., :-1] < tq, y[..., 1:], y[..., :-1])\n    y = torch.where(t[..., 1:] > tq, y[..., :-1], y[..., 1:])\n    y = torch.where(t[..., :-1] == tq, outside_value, y)\n    return y\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    tq, t, y = matchup_channels(tq, t, y)\n    t_lo = t[..., :-1]\n    t_hi = t[..., 1:]\n    y_lo = y[..., :-1]\n    y_hi = y[..., 1:]\n\n    # Get indices where the query time is within a step change interval.\n    indices = torch.where((t_lo <= tq) & (tq <= t_hi),\n                          torch.arange(t.shape[-1] - 1, device=t.device),\n                          torch.tensor(-1, device=t.device))\n\n    # If the query time exactly matches a step change time, return the outside value.\n    y_outside = torch.where(t_lo == tq, y_lo, outside_value)\n\n    # Interpolate the step function value at the query time.\n    t_interp = (tq - t_lo) / (t_hi - t_lo)\n    y_interp = (1 - t_interp) * y_lo + t_interp * y_hi\n\n    # Use the interpolated values where the query time matches a step change interval.\n    y_interp = torch.where(indices >= 0, y_interp, y_outside)\n\n    return y_interp\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # tq, t, y = matchup_channels(tq, t, y)\n    tq, t, y = tq.reshape(-1), t.reshape(-1), y.reshape(-1)\n    tq, t, y = torch.as_tensor(tq), torch.as_tensor(t), torch.as_tensor(y)\n    y = torch.cat([y, y[[-1]]], dim=0)\n    # print(tq.shape, t.shape, y.shape)\n    # assert tq.shape[-1] == t.shape[-1] + 1\n    # assert t.shape[-1] == y.shape[-1]\n\n    # Find the indices of the step changes that are immediately to the left of each query time.\n    indices = torch.searchsorted(t, tq)\n\n    # If a query time exactly matches a step change time, return the outside value.\n    # Otherwise, interpolate the value at the query time based on the step function.\n    yq = torch.where(tq == t[indices], outside_value,\n                     y[indices] + (y[indices + 1] - y[indices]) * (tq - t[indices]) / (t[indices + 1] - t[indices]))\n\n    return yq\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    t_lo = t[..., :-1]\n    t_hi = t[..., 1:]\n    t_intersect = torch.maximum(t_lo, train_frac)\n    t_union = torch.minimum(t_hi, train_frac)\n    intersect_width = t_intersect - t_union\n    intersect_width = torch.where(intersect_width > 0, intersect_width, torch.zeros_like(intersect_width))\n    intersect_width = torch.where(intersect_width > 0, intersect_width, torch.zeros_like(intersect_width))\n    intersect_width = torch.where(intersect_width > 0, intersect_width, torch.zeros_like(intersect_width))\n    intersect_width = torch.where(intersect_width > 0, intersect_width, torch.zeros_like(intersect_width))\n    intersect_width = torch.where(intersect_width > 0, intersect_width, torch.zeros_like(intersect_width))\n    intersect_width = torch.where(intersect_width > 0, intersect_width, torch.zeros_like(intersect_width))\n    intersect_width = torch.where(intersect_width > 0, intersect_width, torch.zeros_like(intersect_width))\n    intersect_width = torch.where(intersect_width > 0, intersect_width, torch.zeros_like(intersect_width))\n    intersect_width = torch.where(intersect_width > 0, intersect_width, torch.zeros_like(intersect_width))\n    intersect_width = torch.where(intersect_width > 0, intersect_width, torch.zeros_like(intersect_width))\n    intersect_width = torch.where(intersect_width > 0, intersect_width, torch.zeros_like(intersect_width))\n    intersect_width = torch.where(intersect_width > 0,"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # accepts t.shape[-1] = w.shape[-1] + 1\n    t, w = matchup_channels(t, w)\n    t_lo = t[..., :-1]\n    t_hi = t[..., 1:]\n    w_lo = w[..., :-1]\n    w_hi = w[..., 1:]\n    t_mid = (t_lo + t_hi) / 2\n    w_mid = (w_lo + w_hi) / 2\n    t_delta = t_hi - t_lo\n    w_delta = w_hi - w_lo\n    w_mid_next = torch.where(t_delta > 0, torch.abs(w_hi - w_mid) / t_delta, torch.zeros_like(w_mid))\n    w_mid_prev = torch.where(t_delta > 0, torch.abs(w_mid - w_lo) / t_delta, torch.zeros_like(w_mid))\n    w_mid_next = torch.where(t_delta > 0, torch.abs(w_hi - w_mid) / t_delta, torch.zeros_like(w_mid))\n    w_mid_prev = torch.where(t_delta > 0, torch.abs(w_mid - w_lo) / t_delta, torch.zeros_like(w_mid))\n    w_mid_next = torch.where(t_delta > 0, torch.abs(w_hi - w_mid) / t_delta, torch.zeros_like(w_mid))\n    w_mid_prev = torch.where(t_delta > 0, torch.abs(w_mid - w_lo) / t_delta, torch.zeros_like(w_mid))\n    w_mid_next = torch.where(t_delta > 0, torch.abs(w_hi - w_mid) / t_delta, torch.zeros_like(w_mid))\n    w_mid_"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    # Compute the fraction of training completed.\n    train_frac = torch.tensor(train_frac, device=t.device, dtype=t.dtype)\n    # Compute the annealing effect on weights.\n    anneal_effect = torch.sigmoid(train_frac * anneal_slope)\n    # Compute the adjusted weights.\n    w_adjusted = w * (1 - anneal_effect)\n    # Handle cases where adjacent intervals have zero distance.\n    w_adjusted = torch.where(\n        torch.abs(t[..., 1:] - t[..., :-1]) < eps,\n        torch.zeros_like(w_adjusted[..., :-1]),\n        w_adjusted[..., :-1])\n    # Prevent NaN values by using a softmax operation.\n    w_adjusted = torch.nn.functional.softmax(w_adjusted, dim=-1)\n    return t, w_adjusted"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    w_max = torch.max(w, dim=-1)[0]\n    w_min = torch.min(w, dim=-1)[0]\n    w_range = (w_max - w_min).clip(eps)\n    w_range_inv = w_range.reciprocal()\n    w_range_inv_slope = w_range_inv * anneal_slope\n    w_range_inv_slope_train_frac = w_range_inv_slope * train_frac\n    w_range_inv_slope_train_frac_clamped = w_range_inv_slope_train_frac.clip(0, 1)\n    w_range_inv_slope_train_frac_clamped_squared = w_range_inv_slope_train_frac_clamped ** 2\n    w_range_inv_slope_train_frac_clamped_squared_clamped = w_range_inv_slope_train_frac_clamped_squared.clip(0, 1)\n    w_range_inv_slope_train_frac_clamped_squared_clamped_scaled = w_range_inv_slope_train_frac_clamped_squared_clamped * w_range\n    w_range_inv_slope_train_frac_clamped_squared_clamped_scaled_shifted = w_range_inv_slope_train_frac_clamped_squared_clamped_scaled + w_min\n    w_range_inv_slope_train_frac_clamped_squared_clamped_scaled_shifted_softmaxed = torch.softmax(w_range_inv_slope_train_frac_clamped_squared_clamped_scaled_shifted, dim=-1)\n    w_range_inv_slope_train_frac_clamped_squared_clamped_scaled_shifted_softmaxed_normalized = w_range_inv_slope_train_"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # Compute the fraction of the training that has been completed.\n    t_frac = t[..., :-1] / t[..., -1:]\n\n    # Compute the weight adjustment based on the training fraction and anneal slope.\n    weight_adjustment = torch.where(\n        t_frac < train_frac,\n        torch.exp(-anneal_slope * (train_frac - t_frac)),\n        torch.ones_like(t_frac),\n    )\n\n    # Adjust the weights based on the weight adjustment.\n    w_adjusted = w * weight_adjustment[..., None]\n\n    # Handle cases where adjacent intervals have zero distance.\n    w_adjusted = torch.where(\n        torch.abs(t[..., 1:] - t[..., :-1]) < eps,\n        torch.zeros_like(w_adjusted),\n        w_adjusted,\n    )\n\n    # Ensure stability in the computation by using a softmax operation.\n    w_adjusted = torch.nn.functional.softmax(w_adjusted, dim=-1)\n\n    return w_adjusted"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    # Compute the weight adjustment based on the training fraction and anneal slope.\n    weight_adjustment = 1 - torch.exp(-train_frac * anneal_slope)\n\n    # Compute the distance between adjacent intervals.\n    dist = t[..., 1:] - t[..., :-1]\n\n    # Compute the adjusted weights.\n    w_adjusted = torch.where(dist == 0, 0, torch.exp(torch.log(w[..., 1:]) * weight_adjustment))\n\n    # Ensure stability in the computation by handling cases where adjacent intervals have zero distance.\n    w_adjusted = torch.where(dist == 0, 0, w_adjusted)\n\n    # Prevent NaN values by using a softmax operation on the adjusted weights.\n    w_adjusted = torch.nn.functional.softmax(w_adjusted, dim=-1)\n\n    return w_adjusted"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    # Compute the weights that reach 0 at the endpoints of each interval.\n    w_lo = torch.where(\n        (t[..., :-1] < train_frac) & (t[..., 1:] >= train_frac),\n        torch.sigmoid((train_frac - t[..., :-1]) * anneal_slope),\n        0,\n    )\n    w_hi = torch.where(\n        (t[..., :-1] < train_frac) & (t[..., 1:] >= train_frac),\n        torch.sigmoid((t[..., 1:] - train_frac) * anneal_slope),\n        0,\n    )\n    w_lo = w_lo / (w_lo + w_hi + 1e-8)\n    w_hi = 1 - w_lo\n    w_lo = w_lo * (w[..., :-1] + eps)\n    w_hi = w_hi * (w[..., 1:] + eps)\n    w_new = torch.cat([w_lo, w_hi], dim=-1)\n    w_new = w_new / torch.sum(w_new, dim=-1, keepdim=True)\n    return t, w_new"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n\n    # Compute the fraction of the training that's completed.\n    t_frac = (t - t[..., 0]) / (t[..., -1] - t[..., 0])\n    # Compute the annealed weights.\n    anneal = torch.sigmoid((t_frac - train_frac) * anneal_slope)\n    anneal = torch.where(t_frac < train_frac, torch.zeros_like(anneal), anneal)\n    anneal = torch.where(t_frac > 1 - train_frac, torch.ones_like(anneal), anneal)\n    # Apply the annealed weights to the weights.\n    w_anneal = w * anneal\n    # Normalize the weights so they sum to 1.\n    w_anneal /= torch.sum(w_anneal, dim=-1, keepdim=True).clip(eps)\n    return t, w_anneal"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    w = w.clip(eps)\n    t_interp = torch.sort(torch.cat([t, 2 * t[..., -1:] - t[..., :-1]], dim=-1), dim=-1)[0]\n    w_interp = torch.max(\n        torch.where(\n            (t_interp[..., None, :] <= t[..., None])\n            & (t_interp[..., None, :] > t[..., None]),\n            w[..., None, :],\n            0,\n        ),\n        dim=-1)[0]\n    w_interp = w_interp.clip(eps)\n    w_interp = w_interp / torch.sum(w_interp, dim=-1, keepdim=True)\n    w_interp = w_interp * torch.clamp(1 - torch.exp(anneal_slope * (train_frac - t_interp)), min=0)\n    w_interp = w_interp / torch.sum(w_interp, dim=-1, keepdim=True)\n    return t_interp, w_interp"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    t_lo = t[..., :-1]\n    t_hi = t[..., 1:]\n    # Distance between adjacent intervals.\n    d = t_hi - t_lo\n    # Relative distance from the left end of each interval.\n    z = (t_lo - t[..., :1]) / d\n    # Schlick's bias function.\n    bias = 1 / (1 + torch.exp(-(z - train_frac) * anneal_slope))\n    # Renormalize to preserve the total weight.\n    w_biased = w * bias\n    w_biased /= torch.sum(w_biased, dim=-1, keepdim=True).clip(eps)\n    return w_biased"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    # Compute the fraction of the training that has completed.\n    t_frac = (t[..., 1:] - t[..., :-1]) / (t[..., -1] - t[..., 0])\n    # Compute the annealing effect on weights.\n    w_anneal = torch.clamp(1 - torch.exp(anneal_slope * (t_frac - train_frac)), min=0)\n    # Apply the annealing effect to the weights.\n    w_anneal = w_anneal * w\n    # Ensure that adjacent intervals have non-zero distance.\n    w_anneal = torch.where(w_anneal[..., 1:] == w_anneal[..., :-1], 0, w_anneal)\n    # Prevent NaN values by using a softmax operation on the adjusted weights.\n    w_anneal = torch.softmax(w_anneal, dim=-1)\n    # Normalize the weights to sum to 1.\n    w_anneal = w_anneal / (torch.sum(w_anneal, dim=-1, keepdim=True) + eps)\n    return t, w_anneal"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n\n    # Compute the fraction of the training that is complete.\n    t_max = t[..., -1]\n    t_min = t[..., 0]\n    t_range = t_max - t_min\n    t_mid = t_min + train_frac * t_range\n\n    # Compute the Schlick's bias function.\n    bias = (t_mid - t[..., :-1]) * (t_mid - t[..., 1:])\n    bias = torch.exp(-bias / (2 * anneal_slope**2 + eps))\n\n    # Compute the adjusted weights.\n    w_adj = w[..., :-1] * bias\n\n    # Prevent NaNs due to division by zero.\n    w_adj = torch.where(bias == 0, w[..., :-1], w_adj)\n\n    # Ensure that the weights sum to 1.\n    w_adj = w_adj / (torch.sum(w_adj, dim=-1, keepdim=True) + eps)\n\n    return t, w_adj"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    t0 = t[..., :-1]\n    t1 = t[..., 1:]\n    z = torch.zeros_like(w)\n    # Compute the annealing weights.\n    w_anneal = (t1 - t0) * torch.sigmoid(train_frac * anneal_slope - (t0 + t1) / 2)\n    # Compute the adjusted weights.\n    w_adj = torch.where(w_anneal > 0, w / w_anneal, z)\n    # Prevent NaNs.\n    w_adj = torch.where(w_adj == w_adj, w_adj, z)\n    # Normalize the weights.\n    w_adj = w_adj / (torch.sum(w_adj, dim=-1, keepdim=True) + eps)\n    return w_adj"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    # Compute the integral of the weights.\n    cw = integrate_weights(w)\n\n    # Compute the annealed weights.\n    # Note that the bias function is biased towards the right, so we subtract 1\n    # from the train_frac to make it biased towards the left.\n    bias = (1 - torch.exp(-(train_frac - 1) * anneal_slope)) / (1 - torch.exp(-anneal_slope))\n    w_anneal = bias * cw + (1 - bias) * w\n\n    # Normalize the weights.\n    w_anneal /= torch.sum(w_anneal, dim=-1, keepdim=True).clip(eps)\n\n    return t, w_anneal"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    # Compute the distance between adjacent weights.\n    # Note that we use a small epsilon to prevent division by zero.\n    dist = t[..., 1:] - t[..., :-1]\n    dist = torch.where(dist > 0, dist, torch.ones_like(dist) * eps)\n\n    # Compute the bias function using Schlick's bias.\n    bias = torch.sigmoid(train_frac * anneal_slope)\n\n    # Compute the annealed weights using the bias function.\n    w_anneal = w * bias\n\n    # Set weights for adjacent intervals with zero distance to zero.\n    w_anneal = torch.where(dist > 0, w_anneal, torch.zeros_like(w_anneal))\n\n    # Normalize the weights to sum to one.\n    w_anneal = w_anneal / torch.sum(w_anneal, dim=-1, keepdim=True)\n\n    return t, w_anneal"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n\n    # Compute the fraction of the way through the training that we are.\n    t_fraction = (t[..., 1:] - t[..., :-1]) / (t[..., -1] - t[..., 0])\n    t_fraction = t_fraction.clip(0, 1)\n\n    # Compute the bias function.\n    bias = (t_fraction - train_frac) * anneal_slope\n    bias = torch.sigmoid(bias)\n    bias = torch.where(t_fraction < train_frac, bias, torch.zeros_like(bias))\n\n    # Adjust the weights and normalize.\n    w_adjusted = w * (1 - bias)\n    w_adjusted = w_adjusted / (torch.sum(w_adjusted, dim=-1, keepdim=True).clip(eps))\n\n    return t, w_adjusted"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    t_lo = t[..., :-1]\n    t_hi = t[..., 1:]\n    w_lo = w[..., :-1]\n    w_hi = w[..., 1:]\n    t_range = t_hi - t_lo\n    t_range = torch.where(t_range == 0, 1, t_range)  # prevent division by zero\n    w_range = w_hi - w_lo\n    w_range = torch.where(w_range == 0, 1, w_range)  # prevent division by zero\n    t_mid = (t_lo + t_hi) / 2\n    w_mid = (w_lo + w_hi) / 2\n    # Schlick's bias function\n    bias = torch.exp(-torch.log(train_frac) / anneal_slope)\n    w_mid = bias * w_mid + (1 - bias) * w_lo\n    w_range = bias * w_range + (1 - bias) * w_mid - w_mid\n    w_range = torch.where(w_range == 0, 1, w_range)  # prevent division by zero\n    w_mid = bias * w_mid + (1 - bias) * w_lo\n    w_lo = w_mid - w_range / 2\n    w_hi = w_mid + w_range / 2\n    w_lo = torch.where(t_lo <= train_frac, w_lo, 0)\n    w_hi = torch.where(t_hi > train_frac, w_hi, 0)\n    w = torch.cat([w_lo, w_hi], dim=-1)\n    w = torch.softmax(w, dim=-1)\n    return t, w"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # accepts t.shape[-1] = w.shape[-1] + 1\n    t, w = matchup_channels(t, w)\n    w = torch.cat([w, torch.zeros_like(w[..., :1])], dim=-1)\n    w = w[..., :-1]\n    t = torch.cat([t, torch.full_like(t[..., :1], t[..., -1])], dim=-1)\n    t = t[..., :-1]\n    # Compute the distance between adjacent intervals.\n    dist = torch.where(t[..., 1:] != t[..., :-1], t[..., 1:] - t[..., :-1], torch.ones_like(t[..., 1:]))\n    # Compute the bias function based on the training fraction and anneal slope.\n    bias = (1 - torch.exp(-train_frac * anneal_slope))\n    # Compute the annealed weights by multiplying the original weights with the bias function.\n    w_anneal = w * bias\n    # Set weights for adjacent intervals with zero distance to zero.\n    w_anneal = torch.where(dist == 0, torch.zeros_like(w_anneal), w_anneal)\n    # Normalize the annealed weights to ensure they sum to 1.\n    w_anneal = w_anneal / (torch.sum(w_anneal, dim=-1, keepdim=True).clip(eps) + 1e-8)\n    return t, w_anneal"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # Compute the fraction of the training that's been completed.\n    t_frac = (t[..., :-1] - t[..., 0]) / (t[..., -1] - t[..., 0])\n\n    # Compute the weight adjustment based on the training fraction and anneal slope.\n    w_adj = torch.where(t_frac < train_frac,  # if t_frac < train_frac\n                        (1 - t_frac) / (1 - train_frac),  # then (1 - t_frac) / (1 - train_frac)\n                        1 / (1 - train_frac))  # else 1 / (1 - train_frac)\n    w_adj = w_adj * (1 - torch.exp(-w_adj * anneal_slope))\n\n    # Handle cases where adjacent intervals have zero distance.\n    w_adj = torch.where(torch.isclose(t[..., 1:] - t[..., :-1], torch.zeros_like(t[..., :-1])),  # if t[..., 1:] - t[..., :-1] is close to zero\n                        torch.zeros_like(w_adj),  # then zero\n                        w_adj)  # else w_adj\n\n    # Prevent NaN values by using a softmax operation on the adjusted weights.\n    w_adj = torch.nn.functional.softmax(w_adj, dim=-1)\n\n    # Adjust the weights based on the weight adjustment.\n    w_adj = torch.where(torch.isnan(w_adj),  # if w_adj is NaN\n                        torch.zeros_like(w_adj),  # then zero\n                        w_adj)  # else w_adj\n\n    # Normalize the weights to sum to 1.\n    w_adj = w_adj / torch.sum(w_adj, dim=-1, keepdim=True).clip(eps)\n\n    return w_adj"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # accepts t.shape[-1] = w.shape[-1] + 1\n    t, w = matchup_channels(t, w)\n    t_diff = t[..., 1:] - t[..., :-1]\n    # t_diff = torch.where(t_diff > 0, t_diff, torch.ones_like(t_diff))\n    t_diff = torch.max(t_diff, torch.full_like(t_diff, 1e-8))\n    # t_diff = torch.where(t_diff == 0, torch.ones_like(t_diff), t_diff)\n    t_mid = (t[..., 1:] + t[..., :-1]) / 2\n    t_diff_inverse = torch.reciprocal(t_diff)\n    # t_diff_inverse = torch.where(t_diff_inverse > 0, t_diff_inverse, torch.ones_like(t_diff_inverse))\n    t_diff_inverse = torch.max(t_diff_inverse, torch.full_like(t_diff_inverse, 1e-8))\n    # t_diff_inverse = torch.where(t_diff_inverse == 0, torch.ones_like(t_diff_inverse), t_diff_inverse)\n\n    # compute bias\n    bias = torch.sigmoid(t_mid * anneal_slope * (train_frac - 1))\n\n    # compute weight adjustment\n    w_adjust = torch.where(t_diff > 0, bias * t_diff_inverse, torch.zeros_like(w))\n\n    # apply weight adjustment\n    w = w + w_adjust\n\n    # handle cases where adjacent intervals have zero distance\n    w = torch.where(t_diff == 0, torch.zeros_like(w), w)\n\n    # ensure stability in the computation\n    w = torch.softmax(w, dim=-1)\n    w = tor"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)) and not ignore_list:\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    else:\n        pass  # do nothing here, used for typed in to_cuda for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)) and not ignore_list:\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)) and not ignore_list:\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    else:\n        pass  # do nothing here, used for typed in to_cuda for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)) and not ignore_list:\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)) and not ignore_list:\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)) and not ignore_list:\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)) and not ignore_list:\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    else:\n        pass  # do nothing here, used for typed in to_cuda for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)) and not ignore_list:\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    else:\n        pass  # do nothing here, used for typed in to_cuda for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)) and not ignore_list:\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = torch.as_tensor(batch, device=device)\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)) and not ignore_list:\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    else:\n        pass  # do nothing here, used for typed in to_cuda for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)) and not ignore_list:\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = torch.as_tensor(batch, device=device)\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)) and not ignore_list:\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)) and not ignore_list:\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    else:\n        pass  # do nothing here, used for typed in to_cuda for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)) and not ignore_list:\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = torch.as_tensor(batch, device=device)\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)) and not ignore_list:\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)) and not ignore_list:\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = torch.as_tensor(batch, device=device)\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        if batch.device != device:\n            batch = batch.to(device, non_blocking=True)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        if batch.device != device:\n            batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)) and not ignore_list:\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    else:\n        batch = torch.as_tensor(batch, device=device)\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    def to_cuda_if(batch, cond):\n        if isinstance(batch, (tuple, list)):\n            batch = [to_cuda(b, device, ignore_list) for b in batch]\n        elif isinstance(batch, dict):\n            batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n        elif isinstance(batch, torch.Tensor):\n            if cond(batch):\n                batch = batch.to(device, non_blocking=True)\n        elif isinstance(batch, np.ndarray):  # numpy and others\n            if cond(batch):\n                batch = torch.as_tensor(batch).to(device, non_blocking=True)\n        else:\n            pass  # do nothing here, used for typed in to_cuda for methods\n            # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n        return batch\n\n    def to_cuda(batch, device=\"cuda\", ignore_list=False):\n        if isinstance(batch, (tuple, list)) and not ignore_list:\n            batch = [to_cuda(b, device, ignore_list) for b in batch]\n        elif isinstance(batch, dict):\n            batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n        elif isinstance(batch, torch.Tensor):\n            batch = batch.to(device, non_blocking=True)\n        elif isinstance(batch, np.ndarray):  # numpy and others\n            batch = torch.as_tensor(batch).to(device, non_blocking=True)\n        else:\n            pass  # do nothing here, used for typed in to_cuda for methods\n            # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n        return batch\n\n    return to_cuda_if(batch, lambda x: x.device != device)\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # index will first be augmented to match the values' dimentionality at the back\n    # take care of batch dimension of, and acts like a linear indexing in the target dimention\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    return v.gather(dim, multi_indexing(f, v.shape, dim))"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # index will first be augmented to match the values' dimentionality at the back\n    # take care of batch dimension of, and acts like a linear indexing in the target dimention\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    return v.gather(dim, multi_indexing(f, v.shape, dim))"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # index will first be augmented to match the values' dimentionality at the back\n    # take care of batch dimension of, and acts like a linear indexing in the target dimention\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    if f.ndim != v.ndim:\n        f = f.expand(v.shape[:-2] + f.shape[-2:])\n    return v.gather(dim, f)"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # index will first be augmented to match the values' dimentionality at the back\n    # take care of batch dimension of, and acts like a linear indexing in the target dimention\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    f = f.expand(*v.shape[:-2], -1, -1)\n    return multi_gather(v, f, dim)"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # index will first be augmented to match the values' dimentionality at the back\n    # take care of batch dimension of, and acts like a linear indexing in the target dimention\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    # f = f.expand(-1, *v.shape[:-2], -1, -1)\n    # return v.gather(dim, multi_indexing(f, v.shape, dim))\n    return v.gather(dim, multi_indexing(f, v.shape, dim))"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # index will first be augmented to match the values' dimentionality at the back\n    # take care of batch dimension of, and acts like a linear indexing in the target dimention\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    if f.ndim == v.ndim:\n        f = f.unsqueeze(0)\n    f = f.expand(v.shape[0], *f.shape[1:])\n    f = f.reshape(v.shape[0], -1, 3)\n    return v.gather(dim, multi_indexing(f, v.shape, dim))\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # index will first be augmented to match the values' dimentionality at the back\n    # take care of batch dimension of, and acts like a linear indexing in the target dimention\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    if f.ndim == 1: f = f.unsqueeze(0)\n    if f.ndim == 2: f = f.unsqueeze(0)\n    if f.ndim == 3: f = f.unsqueeze(0)\n    if v.ndim == 2: v = v.unsqueeze(0)\n    if v.ndim == 3: v = v.unsqueeze(0)\n    if v.ndim == 4: v = v.unsqueeze(0)\n    if f.shape[0] != v.shape[0]:\n        f = f.expand(v.shape[0], *f.shape[1:])\n    return v.gather(dim, multi_indexing(f, v.shape, dim)).view(*f.shape, *v.shape[dim + 1:])"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # index will first be augmented to match the values' dimentionality at the back\n    # take care of batch dimension of, and acts like a linear indexing in the target dimention\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    f = f.expand(*v.shape[:-2], *f.shape[1:])\n    return multi_gather(v, f, dim)\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    if f.ndim == 2:\n        # expand the faces tensor to match the batch dimension of the vertices tensor\n        f = f[None].expand(v.shape[0], -1, -1)\n    elif f.ndim == 3:\n        # check if the batch dimension of the vertices tensor matches the batch dimension of the faces tensor\n        if v.shape[0] != f.shape[0]:\n            raise ValueError(\n                f\"The batch dimension of the vertices tensor ({v.shape[0]}) does not match the batch dimension of the faces tensor ({f.shape[0]}).\")\n    else:\n        raise ValueError(f\"The faces tensor must have either 2 or 3 dimensions, but got {f.ndim} dimensions.\")\n\n    # gather the vertices at the specified dimension\n    v = multi_gather(v, f, dim)\n\n    # reshape the gathered vertices to match the original faces tensor structure with additional dimensions for batch processing\n    v = v.reshape(*f.shape, *v.shape[f.ndim:])\n\n    return v"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust the dimensions of the faces tensor to match the batch dimension of the vertices tensor\n    if v.ndim > f.ndim:\n        f = f.unsqueeze(0)\n    elif v.ndim < f.ndim:\n        f = f.expand(v.shape[:-2] + (-1, -1))\n\n    # reshape the result to maintain the original faces tensor structure with additional dimensions for batch processing\n    return multi_gather(v, f, dim).reshape(*f.shape[:-1], *v.shape[-2:])\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # index will first be augmented to match the values' dimentionality at the back\n    # take care of batch dimension of, and acts like a linear indexing in the target dimention\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    if v.ndim != f.ndim:\n        f = f.expand(*v.shape[:-2], *f.shape)\n    return v.gather(dim, multi_indexing(f, v.shape, dim))\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust the dimension of the faces tensor to match the batch dimension of the vertices tensor\n    if f.ndim == v.ndim - 1:\n        f = f.unsqueeze(0)\n    elif f.ndim == v.ndim:\n        f = f.unsqueeze(-2)\n    else:\n        raise ValueError(\"The dimensions of the vertices and faces tensors do not match.\")\n\n    # reshape the faces tensor to match the batch dimension of the vertices tensor\n    f = f.expand(v.shape[0], *f.shape[1:])\n\n    # gather the vertices using the faces tensor\n    v = multi_gather(v, f, dim)\n\n    # reshape the result to maintain the original faces tensor structure with additional dimensions for batch processing\n    v = v.reshape(*f.shape[:-1], *v.shape[1:])\n\n    return v"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust the faces tensor to match the batch dimension of the vertices tensor\n    if f.ndim == v.ndim - 1:\n        f = f[None].expand(v.shape[0], *f.shape)\n\n    # gather the vertices for each face\n    v0 = multi_gather(v, f[..., 0], dim)\n    v1 = multi_gather(v, f[..., 1], dim)\n    v2 = multi_gather(v, f[..., 2], dim)\n\n    # compute the face normals\n    n = torch.cross(v1 - v0, v2 - v0, dim=-1)  # B, F, 3\n\n    # reshape the result to maintain the original faces tensor structure\n    return n.reshape(*f.shape, 3)"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # index will first be augmented to match the values' dimentionality at the back\n    # take care of batch dimension of, and acts like a linear indexing in the target dimention\n    # we assume that the values's second to last dimension is the dimension to be indexed on\n    if f.ndim == 2:\n        f = f[None].expand(v.shape[0], *f.shape)\n    elif f.ndim == 3:\n        pass\n    else:\n        raise RuntimeError(f'Unsupported faces dimension: {f.shape}')\n    return v.gather(dim, multi_indexing(f, v.shape, dim)).view(*f.shape, 3)  # B, F, 3, 3"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust the dimension of faces tensor to match the batch dimension of vertices tensor\n    if f.ndim == 2:\n        f = f[None].expand(v.shape[0], *f.shape)\n\n    # gather the vertices for each face\n    v0 = v.gather(dim, f[:, :, 0][:, :, None].expand(*f.shape[:-1], 3))\n    v1 = v.gather(dim, f[:, :, 1][:, :, None].expand(*f.shape[:-1], 3))\n    v2 = v.gather(dim, f[:, :, 2][:, :, None].expand(*f.shape[:-1], 3))\n\n    # compute the normals of the faces\n    normals = torch.cross(v1 - v0, v2 - v0, dim=-1)\n\n    # reshape the normals tensor to maintain the original faces tensor structure with additional dimensions for batch processing\n    normals = normals.view(*f.shape[:-1], *normals.shape[1:])\n\n    return normals"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    if f.ndim == 1:\n        f = f[None]\n    if v.ndim == 2:\n        v = v[None]\n    if f.ndim == 3:\n        f = f[:, None]\n    if v.ndim == 3:\n        v = v[:, None]\n    assert f.ndim == v.ndim, f'dimension mismatch: {f.ndim} vs {v.ndim}'\n    return v.gather(dim, f.expand(-1, *v.shape[:-2], -1, -1))"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    if f.ndim == 1: f = f[None]\n    if v.ndim == 2: v = v[None]\n    assert f.ndim == v.ndim\n    assert f.shape[0] == v.shape[0]\n    if f.shape[0] > 1:\n        f = f.expand(v.shape[0], *f.shape[1:])\n    return v.gather(dim, f)\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    assert v.ndim >= 2\n    assert f.ndim >= 2\n\n    if f.ndim == 2:\n        f = f.unsqueeze(0).expand(v.shape[0], *f.shape)\n    assert f.ndim == v.ndim\n\n    # gather the vertices\n    v = multi_gather(v, f, dim)\n    # compute the normals\n    normals = compute_normals(v)\n    # reshape the normals to match the original faces tensor structure\n    shape = list(f.shape)\n    shape[dim] = 3\n    normals = normals.reshape(*shape)\n    return normals"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    if f.ndim == 2:\n        f = f.unsqueeze(0)\n    if v.ndim == 2:\n        v = v.unsqueeze(0)\n    assert f.ndim == 3 and v.ndim == 3\n    assert f.shape[0] == v.shape[0]\n    assert f.shape[2] == 3\n    assert v.shape[2] == 3\n\n    # expand the faces tensor to match the batch dimension of the vertices tensor\n    if f.shape[0] == 1:\n        f = f.expand(v.shape[0], -1, -1)\n\n    # compute the faces normals\n    v0 = v.gather(1, f[:, :, 0:1])\n    v1 = v.gather(1, f[:, :, 1:2])\n    v2 = v.gather(1, f[:, :, 2:3])\n    face_normals = torch.cross(v1 - v0, v2 - v0, dim=-1)\n\n    # reshape the result to maintain the original faces tensor structure with additional dimensions for batch processing\n    face_normals = face_normals.view(*f.shape[:-1], -1)\n\n    return face_normals"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    if f.ndim == 2: f = f[None].expand(v.shape[0], -1, -1)\n    v = v[..., None, :, :]  # B, P, 1, 3\n    f = f[..., :, None, :]  # B, F, 3, 1\n    return torch.gather(v, dim, f).reshape(*f.shape[:-1], -1)  # B, F, 3, 3 -> B, F, 9\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None]\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = batch[None]\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None]\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = batch[None]\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None]\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = batch[None]\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None]\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = batch[None]\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None]\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = batch[None]\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None]\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = batch[None]\n    else:\n        batch = torch.as_tensor(batch)[None]\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None, ...]\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = batch[None, ...]\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None, ...]\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = batch[None, ...]\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None]\n    else:  # numpy and others\n        batch = torch.as_tensor(batch)[None]\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.unsqueeze(0)\n    else:  # numpy and others\n        batch = np.expand_dims(batch, axis=0)\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None, ...]\n    elif isinstance(batch, np.ndarray):\n        batch = batch[None, ...]\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = batch[None]\n    else:  # others, keep as is\n        pass\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = np.expand_dims(batch, axis=0)\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.unsqueeze(0)\n    else:  # numpy and others\n        batch = np.expand_dims(batch, axis=0)\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None]\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = batch[None]\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None, ...]\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = batch[None, ...]\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = batch[None]\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None, ...]\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = batch[None, ...]\n    else:\n        pass  # do nothing here, used for typed in to_x for methods\n        # FIXME: Incosistent behavior here, might lead to undebuggable bugs\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None]\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = batch[None]\n    else:  # others, keep as is\n        pass\n    return batch\n\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None]\n    elif isinstance(batch, np.ndarray):  # numpy and others\n        batch = batch[None]\n    else:  # others, keep as is\n        pass\n    return batch\n\n\n\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        batch.meta = dotdict()\n        batch.meta.H, batch.meta.W, batch.meta.K, batch.meta.R, batch.meta.T, batch.meta.n, batch.meta.f, batch.meta.t, batch.meta.v, batch.meta.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        batch.meta.mass, batch.meta.moment_of_inertia, batch.meta.movement_force, batch.meta.movement_torque, batch.meta.movement_speed, batch.meta.origin, batch.meta.world_up = self.mass, self.moment_of_inertia, self.movement_force, self.movement_torque, self.movement_speed, self.origin, self.world_up\n        return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        batch.meta = dotdict(batch)\n        return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        batch.meta = dotdict()\n        batch.meta.H, batch.meta.W, batch.meta.K, batch.meta.R, batch.meta.T, batch.meta.n, batch.meta.f, batch.meta.t, batch.meta.v, batch.meta.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        batch.meta = dotdict(batch)\n        return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Batch (network input parameters)\n        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n\n        # Other configurables\n        batch.origin = self.origin\n        batch.world_up = self.world_up\n        batch.movement_speed = self.movement_speed\n        batch.movement_force = self.movement_force\n        batch.drag_coeff_mult = self.drag_coeff_mult\n        batch.constant_drag = self.constant_drag\n        batch.mass = self.mass\n        batch.moment_of_inertia = self.moment_of_inertia\n        batch.movement_torque = self.movement_torque\n        batch.angular_friction = self.angular_friction\n        batch.constant_torque = self.constant_torque\n        batch.min_interval = self.min_interval\n        batch.pause_physics = self.pause_physics\n\n        # Meta\n        batch.meta = dotdict()\n        batch.meta.H, batch.meta.W, batch.meta.K, batch.meta.R, batch.meta.T, batch.meta.n, batch.meta.f, batch.meta.t, batch.meta.v, batch.meta.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n\n        # Other configurables\n        batch.meta.origin = self.origin\n        batch.meta.world_up = self.world_up\n        batch.meta.movement_speed = self.movement_speed\n        batch.meta.movement_force = self.movement_force\n        batch.meta.drag_coeff_"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H = self.H\n        batch.W = self.W\n        batch.K = mat3(*self.K.mT.ravel())\n        batch.R = mat3(*self.R.mT.ravel())\n        batch.T = vec3(*self.T.ravel())  # 3,\n        batch.n = self.n\n        batch.f = self.f\n        batch.t = self.t\n        batch.v = self.v\n        batch.bounds = mat2x3(*self.bounds.ravel())  # 2, 3\n\n        batch.meta = dotdict()\n        batch.meta.H = self.H\n        batch.meta.W = self.W\n        batch.meta.K = mat3(*self.K.mT.ravel())\n        batch.meta.R = mat3(*self.R.mT.ravel())\n        batch.meta.T = vec3(*self.T.ravel())  # 3,\n        batch.meta.n = self.n\n        batch.meta.f = self.f\n        batch.meta.t = self.t\n        batch.meta.v = self.v\n        batch.meta.bounds = mat2x3(*self.bounds.ravel())  # 2, 3\n\n        batch.meta.mass = self.mass\n        batch.meta.movement_force = self.movement_force\n        batch.meta.movement_speed = self.movement_speed\n        batch.meta.origin = vec3(*self.origin.ravel())  # 3,\n        batch.meta.world_up = vec3(*self.world_up.ravel())  # 3,\n        batch.meta.drag_coeff_mult = self.drag_coeff_mult\n        batch.meta.constant_drag = self.constant_drag\n        batch.meta.pause_physics = self.pause_physics\n        batch.meta.min_interval = self.min_interval\n\n        batch.meta.moment"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.meta = dotdict()\n        batch.meta.H, batch.meta.W, batch.meta.K, batch.meta.R, batch.meta.T, batch.meta.n, batch.meta.f, batch.meta.t, batch.meta.v, batch.meta.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        batch.meta.origin, batch.meta.world_up, batch.meta.movement_speed, batch.meta.movement_force, batch.meta.drag_coeff_mult, batch.meta.constant_drag, batch.meta.mass, batch.meta.moment_of_inertia, batch.meta.movement_torque, batch.meta.angular_friction, batch.meta.constant_torque, batch.meta.min_interval, batch.meta.pause_physics = self.origin, self.world_up, self.movement_speed, self.movement_force, self.drag_coeff_mult, self.constant_drag, self.mass, self.moment_of_inertia, self.movement_torque, self.angular_friction, self.constant_torque, self.min_interval, self.pause_physics\n\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        batch.origin, batch.world_up, batch.movement_speed, batch.movement_force, batch.drag_coeff_mult, batch.constant_drag, batch.mass, batch.moment_of_inertia, batch.movement_torque, batch.angular_friction, batch.constant_torque, batch.min_interval, batch."}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        batch.meta = dotdict(batch)\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        batch.meta = dotdict()\n        batch.meta.H, batch.meta.W, batch.meta.K, batch.meta.R, batch.meta.T, batch.meta.n, batch.meta.f, batch.meta.t, batch.meta.v, batch.meta.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        batch.meta.mass = self.mass\n        batch.meta.movement_force = self.movement_force\n        batch.meta.movement_speed = self.movement_speed\n        batch.meta.origin = self.origin\n        batch.meta.world_up = self.world_up\n        batch.meta.movement_torque = self.movement_torque\n        batch.meta.moment_of_inertia = self.moment_of_inertia\n        batch.meta.angular_friction = self.angular_friction\n        batch.meta.constant_torque = self.constant_torque\n        batch.meta.constant_drag = self.constant_drag\n        batch.meta.drag_coeff_mult = self.drag_coeff_mult\n        batch.meta.min_interval = self.min_interval\n        batch.meta.pause_physics = self.pause_physics\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        batch.meta = dotdict()\n        batch.meta.H, batch.meta.W, batch.meta.K, batch.meta.R, batch.meta.T, batch.meta.n, batch.meta.f, batch.meta.t, batch.meta.v, batch.meta.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        batch.meta.mass, batch.meta.moment_of_inertia, batch.meta.movement_force, batch.meta.movement_torque, batch.meta.movement_speed = self.mass, self.moment_of_inertia, self.movement_force, self.movement_torque, self.movement_speed\n        batch.meta.origin, batch.meta.world_up = self.origin, self.world_up\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H = self.H\n        batch.W = self.W\n        batch.K = self.K\n        batch.R = self.R\n        batch.T = self.T\n        batch.n = self.n\n        batch.f = self.f\n        batch.t = self.t\n        batch.v = self.v\n        batch.bounds = self.bounds\n\n        batch.meta = dotdict()\n        batch.meta.H = self.H\n        batch.meta.W = self.W\n        batch.meta.K = self.K\n        batch.meta.R = self.R\n        batch.meta.T = self.T\n        batch.meta.n = self.n\n        batch.meta.f = self.f\n        batch.meta.t = self.t\n        batch.meta.v = self.v\n        batch.meta.bounds = self.bounds\n        batch.meta.mass = self.mass\n        batch.meta.moment_of_inertia = self.moment_of_inertia\n        batch.meta.movement_force = self.movement_force\n        batch.meta.movement_torque = self.movement_torque\n        batch.meta.movement_speed = self.movement_speed\n        batch.meta.origin = self.origin\n        batch.meta.world_up = self.world_up\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H = self.H\n        batch.W = self.W\n        batch.K = mat3(*self.K.mT.ravel())\n        batch.R = mat3(*self.R.mT.ravel())\n        batch.T = vec3(*self.T.ravel())  # 3,\n        batch.n = self.n\n        batch.f = self.f\n        batch.t = self.t\n        batch.v = self.v\n        batch.bounds = mat2x3(*self.bounds.ravel())  # 2, 3\n        batch.meta = dotdict()\n        batch.meta.H = self.H\n        batch.meta.W = self.W\n        batch.meta.K = mat3(*self.K.mT.ravel())\n        batch.meta.R = mat3(*self.R.mT.ravel())\n        batch.meta.T = vec3(*self.T.ravel())  # 3,\n        batch.meta.n = self.n\n        batch.meta.f = self.f\n        batch.meta.t = self.t\n        batch.meta.v = self.v\n        batch.meta.bounds = mat2x3(*self.bounds.ravel())  # 2, 3\n\n        batch.meta.mass = self.mass\n        batch.meta.movement_force = self.movement_force\n        batch.meta.movement_speed = self.movement_speed\n        batch.meta.origin = vec3(*self.origin.ravel())  # 3,\n        batch.meta.world_up = vec3(*self.world_up.ravel())  # 3,\n        batch.meta.drag_coeff_mult = self.drag_coeff_mult\n        batch.meta.constant_drag = self.constant_drag\n        batch.meta.pause_physics = self.pause_physics\n        batch.meta.min_interval = self.min_interval\n\n        batch.meta.moment_"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Batch (network input parameters)\n        batch = dotdict()\n        batch.H = torch.tensor([self.H], dtype=torch.int)\n        batch.W = torch.tensor([self.W], dtype=torch.int)\n        batch.K = torch.tensor(self.K, dtype=torch.float32)\n        batch.R = torch.tensor(self.R, dtype=torch.float32)\n        batch.T = torch.tensor(self.T, dtype=torch.float32)\n        batch.n = torch.tensor([self.n], dtype=torch.float32)\n        batch.f = torch.tensor([self.f], dtype=torch.float32)\n        batch.t = torch.tensor([self.t], dtype=torch.float32)\n        batch.v = torch.tensor([self.v], dtype=torch.float32)\n        batch.bounds = torch.tensor(self.bounds, dtype=torch.float32)\n\n        # Other configurables\n        batch.origin = torch.tensor(self.origin, dtype=torch.float32)\n        batch.world_up = torch.tensor(self.world_up, dtype=torch.float32)\n        batch.movement_speed = torch.tensor([self.movement_speed], dtype=torch.float32)\n        batch.movement_force = torch.tensor([self.movement_force], dtype=torch.float32)\n        batch.drag_coeff_mult = torch.tensor([self.drag_coeff_mult], dtype=torch.float32)\n        batch.constant_drag = torch.tensor([self.constant_drag], dtype=torch.float32)\n        batch.mass = torch.tensor([self.mass], dtype=torch.float32)\n        batch.moment_of_inertia = torch.tensor([self"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Batch (network input parameters)\n        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n\n        # GUI related elements\n        batch.mass = self.mass\n        batch.movement_force = self.movement_force\n        batch.movement_torque = self.movement_torque\n        batch.movement_speed = self.movement_speed\n        batch.origin = self.origin\n        batch.world_up = self.world_up\n\n        # Meta data\n        batch.meta = dotdict()\n        batch.meta.H, batch.meta.W, batch.meta.K, batch.meta.R, batch.meta.T, batch.meta.n, batch.meta.f, batch.meta.t, batch.meta.v, batch.meta.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n\n        batch.meta.mass = self.mass\n        batch.meta.movement_force = self.movement_force\n        batch.meta.movement_torque = self.movement_torque\n        batch.meta.movement_speed = self.movement_speed\n        batch.meta.origin = self.origin\n        batch.meta.world_up = self.world_up\n\n        return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H = self.H\n        batch.W = self.W\n        batch.K = to_tensor(self.K)\n        batch.R = to_tensor(self.R)\n        batch.T = to_tensor(self.T)\n        batch.n = self.n\n        batch.f = self.f\n        batch.t = self.t\n        batch.v = self.v\n        batch.bounds = to_tensor(self.bounds)\n        batch.meta = dotdict()\n        batch.meta.H = self.H\n        batch.meta.W = self.W\n        batch.meta.K = to_tensor(self.K)\n        batch.meta.R = to_tensor(self.R)\n        batch.meta.T = to_tensor(self.T)\n        batch.meta.n = self.n\n        batch.meta.f = self.f\n        batch.meta.t = self.t\n        batch.meta.v = self.v\n        batch.meta.bounds = to_tensor(self.bounds)\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Batch (network input parameters)\n        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n\n        # Other configurables\n        batch.origin = self.origin\n        batch.world_up = self.world_up\n        batch.movement_speed = self.movement_speed\n        batch.movement_force = self.movement_force\n        batch.drag_coeff_mult = self.drag_coeff_mult\n        batch.constant_drag = self.constant_drag\n        batch.mass = self.mass\n        batch.moment_of_inertia = self.moment_of_inertia\n        batch.movement_torque = self.movement_torque\n        batch.angular_friction = self.angular_friction\n        batch.constant_torque = self.constant_torque\n        batch.min_interval = self.min_interval\n        batch.pause_physics = self.pause_physics\n\n        # Convert to tensor\n        batch = to_tensor(batch)\n\n        # Add a meta for easy access\n        batch.meta = batch\n\n        return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H = self.H\n        batch.W = self.W\n        batch.K = torch.as_tensor(self.K.mT.ravel())  # 3, 3\n        batch.R = torch.as_tensor(self.R.mT.ravel())  # 3, 3\n        batch.T = torch.as_tensor(self.T.ravel())  # 3,\n        batch.n = self.n\n        batch.f = self.f\n        batch.t = self.t\n        batch.v = self.v\n        batch.bounds = torch.as_tensor(self.bounds.ravel())  # 2, 3\n\n        batch.mass = self.mass\n        batch.moment_of_inertia = self.moment_of_inertia\n        batch.movement_force = self.movement_force\n        batch.movement_torque = self.movement_torque\n        batch.movement_speed = self.movement_speed\n        batch.origin = torch.as_tensor(self.origin.ravel())  # 3,\n        batch.world_up = torch.as_tensor(self.world_up.ravel())  # 3,\n\n        batch.meta = dotdict(batch)\n        return batch\n\n    "}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.meta = dotdict()\n        batch.H = self.H\n        batch.W = self.W\n        batch.K = torch.as_tensor(self.K.mT.ravel())\n        batch.R = torch.as_tensor(self.R.mT.ravel())\n        batch.T = torch.as_tensor(self.T.ravel())  # 3,\n        batch.n = torch.as_tensor(self.n)\n        batch.f = torch.as_tensor(self.f)\n        batch.t = torch.as_tensor(self.t)\n        batch.v = torch.as_tensor(self.v)\n        batch.bounds = torch.as_tensor(self.bounds.ravel())  # 2, 3\n\n        batch.meta.H = self.H\n        batch.meta.W = self.W\n        batch.meta.K = torch.as_tensor(self.K.mT.ravel())\n        batch.meta.R = torch.as_tensor(self.R.mT.ravel())\n        batch.meta.T = torch.as_tensor(self.T.ravel())  # 3,\n        batch.meta.n = torch.as_tensor(self.n)\n        batch.meta.f = torch.as_tensor(self.f)\n        batch.meta.t = torch.as_tensor(self.t)\n        batch.meta.v = torch.as_tensor(self.v)\n        batch.meta.bounds = torch.as_tensor(self.bounds.ravel())  # 2, 3\n\n        batch.meta.mass = torch.as_tensor(self.mass)\n        batch.meta.movement_force = torch.as_tensor(self.movement_force)\n        batch.meta.movement_torque = torch.as_tensor(self.movement_torque)\n        batch.meta.movement_speed = torch.as_tensor("}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Batch (network input parameters)\n        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n\n        # Other configurables\n        batch.origin = self.origin\n        batch.world_up = self.world_up\n        batch.movement_speed = self.movement_speed\n        batch.movement_force = self.movement_force\n        batch.drag_coeff_mult = self.drag_coeff_mult\n        batch.constant_drag = self.constant_drag\n        batch.mass = self.mass\n        batch.moment_of_inertia = self.moment_of_inertia\n        batch.movement_torque = self.movement_torque\n        batch.angular_friction = self.angular_friction\n        batch.constant_torque = self.constant_torque\n        batch.min_interval = self.min_interval\n        batch.pause_physics = self.pause_physics\n\n        # Add a nested 'meta' dictionary to preserve the original batch structure\n        batch.meta = batch\n        return batch\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # The camera parameters are converted into tensors and organized into a structured dictionary format for easy access and manipulation.\n        # The 'meta' dictionary is a nested dictionary that contains the same content as the main dictionary, but with different keys for better readability.\n        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = to_tensor(\n            self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds, ignore_list=True)\n        batch.mass, batch.movement_force, batch.movement_speed, batch.origin, batch.world_up = to_tensor(\n            self.mass, self.movement_force, self.movement_speed, self.origin, self.world_up, ignore_list=True)\n\n        # The nested 'meta' dictionary is created by copying the content of the main dictionary.\n        batch.meta = dotdict(batch)\n        return batch\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent and not agent.is_prime_agent:\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent and not agent.is_prime_agent:\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent and not agent.is_prime_agent:\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent and not agent.is_prime_agent:\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent and not agent.is_prime_agent:\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent and not agent.is_prime_agent:\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent and not agent.is_prime_agent:\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent and not agent.is_prime_agent:\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)\n            "}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)\n            "}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)\n            "}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)\n            "}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working() and not agent.is_prime():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if not agent.is_prime_agent and agent.is_working_agent:\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if not agent.is_working_agent() or agent.is_prime_agent():\n            return\n\n        serialized_agent = AgentSerializer.to_dict(agent)\n        if serialized_agent:\n            self.persistence.save_agent(serialized_agent)\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            max_similarity = -float('inf')\n            closest_agent = None\n\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n\n                if similarity > max_similarity:\n                    max_similarity = similarity\n                    closest_agent = agent\n\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            max_similarity = -float('inf')\n            closest_agent = None\n\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n\n                if similarity > max_similarity:\n                    max_similarity = similarity\n                    closest_agent = agent\n\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            max_similarity = -float('inf')\n            closest_agent = None\n\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n\n                if similarity > max_similarity:\n                    max_similarity = similarity\n                    closest_agent = agent\n\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            max_similarity = -np.inf\n            closest_agent = None\n\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n\n                if similarity > max_similarity:\n                    max_similarity = similarity\n                    closest_agent = agent\n\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            max_similarity = -np.inf\n            closest_agent = None\n\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n\n                if similarity > max_similarity:\n                    max_similarity = similarity\n                    closest_agent = agent\n\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            max_similarity = -np.inf\n            closest_agent = None\n\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n\n                if similarity > max_similarity:\n                    max_similarity = similarity\n                    closest_agent = agent\n\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            max_similarity = -float('inf')\n            closest_agent = None\n\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n\n                if similarity > max_similarity:\n                    max_similarity = similarity\n                    closest_agent = agent\n\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            max_similarity = -np.inf\n            closest_agent = None\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n                if similarity > max_similarity:\n                    max_similarity = similarity\n                    closest_agent = agent\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            max_similarity = -float('inf')\n            closest_agent = None\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n                if similarity > max_similarity:\n                    max_similarity = similarity\n                    closest_agent = agent\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            max_similarity = -np.inf\n            closest_agent = None\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n                if similarity > max_similarity:\n                    max_similarity = similarity\n                    closest_agent = agent\n\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            max_similarity = -float('inf')\n            closest_agent = None\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n                if similarity > max_similarity:\n                    max_similarity = similarity\n                    closest_agent = agent\n\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            max_similarity = -float('inf')\n            closest_agent = None\n\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n\n                if similarity > max_similarity:\n                    max_similarity = similarity\n                    closest_agent = agent\n\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            max_similarity = -np.inf\n            closest_agent = None\n\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n                if similarity > max_similarity:\n                    max_similarity = similarity\n                    closest_agent = agent\n\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            highest_similarity = -np.inf\n            closest_agent = None\n\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n\n                if similarity > highest_similarity:\n                    highest_similarity = similarity\n                    closest_agent = agent\n\n            return closest_agent, highest_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            max_similarity = -float('inf')\n            closest_agent = None\n\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n\n                if similarity > max_similarity:\n                    max_similarity = similarity\n                    closest_agent = agent\n\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            return None, -float('inf')"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            max_similarity = -float('inf')\n            closest_agent = None\n\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n\n                if similarity > max_similarity:\n                    max_similarity = similarity\n                    closest_agent = agent\n\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            max_similarity = -np.inf\n            closest_agent = None\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n                if similarity > max_similarity:\n                    max_similarity = similarity\n                    closest_agent = agent\n\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            max_similarity = -float('inf')\n            closest_agent = None\n\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n                if similarity > max_similarity:\n                    max_similarity = similarity\n                    closest_agent = agent\n\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            return None, -float('inf')"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if len(self.agents) == 0:\n                return None, -float('inf')\n\n            similarities = [cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0] for agent in self.agents]\n            closest_agent_index = np.argmax(similarities)\n            return self.agents[closest_agent_index], similarities[closest_agent_index]\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if not self.agents:\n                logger.error(\"No agents found\")\n                return None, -np.inf\n\n            similarities = [cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0] for agent in self.agents]\n            max_index = np.argmax(similarities)\n            return self.agents[max_index], similarities[max_index]\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.prime = True\n        prime_agent.usage_count = 1\n        self.agents.append(prime_agent)"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.prime = True\n        prime_agent.usage_count = 1\n        self.agents.append(prime_agent)"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper, prime=True, unspecified=True)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        self.agents.append(prime_agent)"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.prime = True\n        prime_agent.usage_count = 1\n        self.agents.append(prime_agent)"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.prime = True\n        prime_agent.usage_count = 1\n        self.agents.append(prime_agent)\n\n    "}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.prime = True\n        prime_agent.usage_count = 1\n        self.agents.append(prime_agent)\n\n    "}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 1, self, self.openai_wrapper)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.prime = True\n        prime_agent.usage_count = 1\n        self.agents.append(prime_agent)"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.prime = True\n        prime_agent.usage_count = 1\n        self.agents.append(prime_agent)"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.prime = True\n        prime_agent.other_flag = True\n        self.agents.append(prime_agent)"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.prime = True\n        prime_agent.prime_flag = True\n        self.agents.append(prime_agent)"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper, prime=True, unspecified=True)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        self.add_agent(prime_agent)"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper, prime=True, working_agent=True)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        self.agents.append(prime_agent)"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.prime = True\n        prime_agent.usage_count = 1\n        prime_agent.flags = {\"prime\": True, \"unspecified\": True}\n        self.agents.append(prime_agent)\n\n    "}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 1, self, self.openai_wrapper)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.prime = True\n        prime_agent.usage_count = 1\n        prime_agent.working_agent = True\n        self.agents.append(prime_agent)"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 1, self, self.openai_wrapper)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.prime = True\n        prime_agent.unspecified = True\n        self.agents.append(prime_agent)"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 1, self, self.openai_wrapper)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.prime_agent = True\n        prime_agent.usage_count = 1\n        prime_agent.other_flag = True\n        self.agents.append(prime_agent)\n\n    "}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper, prime=True, working_agent=True)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.prime = True\n        prime_agent.usage_count = 1\n        self.agents.append(prime_agent)\n\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.prime_agent = True\n        prime_agent.prime_agent_flag = True\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.prime_agent = True\n        prime_agent.unspecified_agent = True\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads an agent with a specified purpose from the database. If an agent with the given purpose is found, it is deserialized and returned; otherwise, None is returned.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str, The purpose of the agent to be loaded. It is used to identify the agent in the database.\n        :param agent_lifecycle: The lifecycle state of the agent. It is passed to the deserializer to properly initialize the agent.\n        :param openai_wrapper: An instance or interface used for interacting with OpenAI services. It is passed to the deserializer for initializing the agent with OpenAI functionalities.\n        :return: An instance of the deserialized agent if found, otherwise None.\n        \"\"\"\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads an agent with a specified purpose from the database. If an agent with the given purpose is found, it is deserialized and returned; otherwise, None is returned.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str, The purpose of the agent to be loaded. It is used to identify the agent in the database.\n        :param agent_lifecycle: The lifecycle state of the agent. It is passed to the deserializer to properly initialize the agent.\n        :param openai_wrapper: An instance or interface used for interacting with OpenAI services. It is passed to the deserializer for initializing the agent with OpenAI functionalities.\n        :return: An instance of the deserialized agent if found, otherwise None.\n        \"\"\"\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads an agent with a specified purpose from the database. If an agent with the given purpose is found, it is deserialized and returned; otherwise, None is returned.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str, The purpose of the agent to be loaded. It is used to identify the agent in the database.\n        :param agent_lifecycle: The lifecycle state of the agent. It is passed to the deserializer to properly initialize the agent.\n        :param openai_wrapper: An instance or interface used for interacting with OpenAI services. It is passed to the deserializer for initializing the agent with OpenAI functionalities.\n        :return: An instance of the deserialized agent if found, otherwise None.\n        \"\"\"\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads an agent with a specified purpose from the database. If an agent with the given purpose is found, it is deserialized and returned; otherwise, None is returned.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str, The purpose of the agent to be loaded. It is used to identify the agent in the database.\n        :param agent_lifecycle: The lifecycle state of the agent. It is passed to the deserializer to properly initialize the agent.\n        :param openai_wrapper: An instance or interface used for interacting with OpenAI services. It is passed to the deserializer for initializing the agent with OpenAI functionalities.\n        :return: An instance of the deserialized agent if found, otherwise None.\n        \"\"\"\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads an agent with a specified purpose from the database. If an agent with the given purpose is found, it is deserialized and returned; otherwise, None is returned.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str, The purpose of the agent to be loaded. It is used to identify the agent in the database.\n        :param agent_lifecycle: The lifecycle state of the agent. It is passed to the deserializer to properly initialize the agent.\n        :param openai_wrapper: An instance or interface used for interacting with OpenAI services. It is passed to the deserializer for initializing the agent with OpenAI functionalities.\n        :return: An instance of the deserialized agent if found, otherwise None.\n        \"\"\"\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads an agent with a specified purpose from the database. If an agent with the given purpose is found, it is deserialized and returned; otherwise, None is returned.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str, The purpose of the agent to be loaded. It is used to identify the agent in the database.\n        :param agent_lifecycle: The lifecycle state of the agent. It is passed to the deserializer to properly initialize the agent.\n        :param openai_wrapper: An instance or interface used for interacting with OpenAI services. It is passed to the deserializer for initializing the agent with OpenAI functionalities.\n        :return: An instance of the deserialized agent if found, otherwise None.\n        \"\"\"\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Load an agent with a specified purpose from the database.\n        \"\"\"\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Load an agent with a specified purpose from the database.\n        \"\"\"\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Load an agent with a specified purpose from the database.\n        \"\"\"\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads an agent with a specified purpose from the database. If an agent with the given purpose is found, it is deserialized and returned; otherwise, None is returned.\n        \"\"\"\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads an agent with a specified purpose from the database. If an agent with the given purpose is found, it is deserialized and returned; otherwise, None is returned.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str, The purpose of the agent to be loaded. It is used to identify the agent in the database.\n        :param agent_lifecycle: The lifecycle state of the agent. It is passed to the deserializer to properly initialize the agent.\n        :param openai_wrapper: An instance or interface used for interacting with OpenAI services. It is passed to the deserializer for initializing the agent with OpenAI functionalities.\n        :return: An instance of the deserialized agent if found, otherwise None.\n        \"\"\"\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:\n            return AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n        return None\n\n    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Load all agents from the database.\n        \"\"\"\n        purposes = self.persistence.load_all_purposes()\n        agents = []\n        for purpose in purposes:\n            agent = self.load_agent(purpose, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        return agents\n\n    def remove_agent(self, agent):\n        \"\"\"\n        Remove an agent from the database.\n        \"\"\"\n        self.persistence.remove_agent(agent.id)\n\n    def save_agent(self, agent):\n        \"\"\"\n        Serialize and save the agent state if it is a working agent and not a prime agent.\n        \"\"\"\n\n        if agent.is_working_agent() and not agent.is_prime_agent():\n           "}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads an agent with a specified purpose from the database. If an agent with the given purpose is found, it is deserialized and returned; otherwise, None is returned.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str, The purpose of the agent to be loaded. It is used to identify the agent in the database.\n        :param agent_lifecycle: The lifecycle state of the agent. It is passed to the deserializer to properly initialize the agent.\n        :param openai_wrapper: An instance or interface used for interacting with OpenAI services. It is passed to the deserializer for initializing the agent with OpenAI functionalities.\n        :return: An instance of the deserialized agent if found, otherwise None.\n        \"\"\"\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:\n            # "}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Load an agent from the database based on its purpose.\n        \"\"\"\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite(maxsize=100)\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Load an agent with a specified purpose from the database.\n        \"\"\"\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        serialized_agents = self.persistence.fetch_all_agents()\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        serialized_agents = self.persistence.fetch_all_agents()\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        serialized_agents = self.persistence.fetch_all_agents()\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        serialized_agents = self.persistence.fetch_all_agents()\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        serialized_agents = self.persistence.fetch_all_agents()\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:\n                "}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        serialized_agents = self.persistence.fetch_all_agents()\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:\n                "}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        serialized_agents = self.persistence.fetch_all_agents()\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:\n                "}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        for serialized_agent in self.persistence.fetch_all_agents():\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        for serialized_agent in self.persistence.fetch_all_agents():\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        for purpose in self.persistence.fetch_all_agents():\n            agent = self.load_agent(purpose, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        for purpose in AgentPurpose:\n            agent = self.load_agent(purpose, agent_lifecycle, openai_wrapper)\n            if agent:\n                "}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        for purpose in self.persistence.fetch_all_agents():\n            agent = self.load_agent(purpose, agent_lifecycle, openai_wrapper)\n            if agent:\n                "}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        for purpose in self.persistence.fetch_all_agents():\n            agent = self.load_agent(purpose, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:\n                "}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        for purpose in self.persistence.fetch_all_agents():\n            agent = self.load_agent(purpose, agent_lifecycle, openai_wrapper)\n            if agent:\n                "}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        for purpose in AgentPurpose:\n            agent = self.load_agent(purpose, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        for purpose in self.persistence.fetch_all_agents():\n            agent = self.load_agent(purpose, agent_lifecycle, openai_wrapper)\n            if agent:\n                "}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise\n\n    "}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise\n\n    "}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise\n\n    "}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents\n    "}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents\n    "}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents\n    "}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents\n    "}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = self.openai_wrapper.get_chat_completion(\n                PROMPT_ENGINEERING_SYSTEM_PROMPT,\n                PROMPT_ENGINEERING_TEMPLATE,\n                EXAMPLES,\n                goal,\n                sample_input\n            )\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = self.openai_wrapper.get_chat_completion(\n                PROMPT_ENGINEERING_SYSTEM_PROMPT,\n                PROMPT_ENGINEERING_TEMPLATE.format(goal=goal, sample_input=sample_input, examples=EXAMPLES)\n            )\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = self.openai_wrapper.get_chat_completion(\n                PROMPT_ENGINEERING_SYSTEM_PROMPT,\n                PROMPT_ENGINEERING_TEMPLATE.format(goal=goal, sample_input=sample_input, examples=EXAMPLES)\n            )\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = self.openai_wrapper.get_chat_completion(\n                prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n                messages=[\n                    {\"role\": \"user\", \"content\": PROMPT_ENGINEERING_TEMPLATE.format(goal=goal, sample_input=sample_input, examples=EXAMPLES)}\n                ]\n            )\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = self.openai_wrapper.get_chat_completion(\n                PROMPT_ENGINEERING_SYSTEM_PROMPT, \n                PROMPT_ENGINEERING_TEMPLATE, \n                EXAMPLES, \n                goal, \n                sample_input\n            )\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = self.openai_wrapper.get_chat_completion(\n                PROMPT_ENGINEERING_SYSTEM_PROMPT,\n                PROMPT_ENGINEERING_TEMPLATE.format(goal=goal, sample_input=sample_input, examples=EXAMPLES),\n            )\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = PROMPT_ENGINEERING_SYSTEM_PROMPT.format(goal=goal, sample_input=sample_input, examples=EXAMPLES)\n            return self.openai_wrapper.get_chat_completion(prompt, PROMPT_ENGINEERING_TEMPLATE)\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = PROMPT_ENGINEERING_SYSTEM_PROMPT + PROMPT_ENGINEERING_TEMPLATE.format(goal=goal, sample_input=sample_input, examples=EXAMPLES)\n            response = self.openai_wrapper.get_chat_completion(prompt)\n            return response\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = self.openai_wrapper.get_chat_completion(\n                PROMPT_ENGINEERING_SYSTEM_PROMPT, \n                PROMPT_ENGINEERING_TEMPLATE, \n                EXAMPLES, \n                goal, \n                sample_input\n            )\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        prompt = PROMPT_ENGINEERING_SYSTEM_PROMPT + PROMPT_ENGINEERING_TEMPLATE.format(goal=goal, sample_input=sample_input, examples=EXAMPLES)\n        try:\n            response = self.openai_wrapper.get_chat_completion(prompt)\n            return response\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = PROMPT_ENGINEERING_SYSTEM_PROMPT.format(goal=goal, sample_input=sample_input, examples=EXAMPLES)\n            response = self.openai_wrapper.get_chat_completion(prompt)\n            return PROMPT_ENGINEERING_TEMPLATE.format(response=response)\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = self.openai_wrapper.get_chat_completion(\n                prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n                messages=[\n                    {\"role\": \"user\", \"content\": PROMPT_ENGINEERING_TEMPLATE.format(goal=goal, sample_input=sample_input)}\n                ]\n            )\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = self.openai_wrapper.get_chat_completion(\n                PROMPT_ENGINEERING_SYSTEM_PROMPT,\n                PROMPT_ENGINEERING_TEMPLATE.format(goal=goal, example=sample_input),\n                EXAMPLES,\n            )\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = PROMPT_ENGINEERING_SYSTEM_PROMPT.format(goal=goal, example_input=sample_input, examples=EXAMPLES)\n            response = self.openai_wrapper.get_chat_completion(prompt)\n            return PROMPT_ENGINEERING_TEMPLATE.format(goal=goal, example_input=sample_input, examples=EXAMPLES, response=response)\n        except Exception as e:\n            logger.exception(f\"Error in generating prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = self.openai_wrapper.get_chat_completion(\n                prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n                messages=[\n                    {\"role\": \"user\", \"content\": PROMPT_ENGINEERING_TEMPLATE.format(goal=goal, sample_input=sample_input, examples=EXAMPLES)}\n                ],\n                temperature=0.5,\n                max_tokens=500,\n                model=\"gpt-3.5-turbo\",\n            )\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n                purpose=goal,\n                examples=EXAMPLES,\n                sample_input=sample_input,\n                prime=PRIME_PROMPT\n            )\n            response = self.openai_wrapper.get_chat_completion(prompt, PROMPT_ENGINEERING_SYSTEM_PROMPT)\n            return response\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = PROMPT_ENGINEERING_SYSTEM_PROMPT.format(goal=goal, example_input=sample_input)\n            response = self.openai_wrapper.get_chat_completion(prompt)\n            return PROMPT_ENGINEERING_TEMPLATE.format(example_input=sample_input, example_output=response) + EXAMPLES\n        except Exception as e:\n            logger.exception(f\"Error in generating prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n                prime_prompt=PRIME_PROMPT,\n                prime_name=PRIME_NAME,\n                examples=EXAMPLES,\n                goal=goal,\n                sample_input=sample_input\n            )\n            response = self.openai_wrapper.get_chat_completion(prompt)\n            return response.content\n        except Exception as e:\n            logger.exception(f\"Error in generating prompt: {e}\")\n            return \"\""}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = self.openai_wrapper.get_chat_completion(\n                PROMPT_ENGINEERING_SYSTEM_PROMPT,\n                PROMPT_ENGINEERING_TEMPLATE,\n                goal,\n                sample_input,\n                EXAMPLES\n            )\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating prompt: {e}\")\n            return \"\"\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = self.openai_wrapper.get_chat_completion(\n                PROMPT_ENGINEERING_SYSTEM_PROMPT,\n                PROMPT_ENGINEERING_TEMPLATE.format(goal=goal, sample_input=sample_input, examples=EXAMPLES)\n            )\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating prompt: {e}\")\n            return \"\"\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT id FROM agents WHERE id = ?\", (agent_dict['id'],))\n            row = cursor.fetchone()\n            if row:\n                cursor.execute(\"UPDATE agents SET purpose = ?, data = ? WHERE id = ?\", (agent_dict['purpose'], json.dumps(agent_dict), agent_dict['id']))\n            else:\n                cursor.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT id FROM agents WHERE id = ?\", (agent_dict['id'],))\n            row = cursor.fetchone()\n            if row:\n                cursor.execute(\"UPDATE agents SET purpose = ?, data = ? WHERE id = ?\", (agent_dict['purpose'], json.dumps(agent_dict), agent_dict['id']))\n            else:\n                cursor.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT id FROM agents WHERE id = ?\", (agent_dict['id'],))\n            row = cursor.fetchone()\n            if row:\n                cursor.execute(\"UPDATE agents SET purpose = ?, data = ? WHERE id = ?\", (agent_dict['purpose'], json.dumps(agent_dict), agent_dict['id']))\n            else:\n                cursor.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict[\"id\"], agent_dict[\"purpose\"], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT id FROM agents WHERE id = ?\", (agent_dict['id'],))\n            row = cursor.fetchone()\n            if row:\n                # Update existing record\n                cursor.execute(\"UPDATE agents SET purpose = ?, data = ? WHERE id = ?\", (agent_dict['purpose'], json.dumps(agent_dict), agent_dict['id']))\n            else:\n                # Insert new record\n                cursor.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT id FROM agents WHERE id = ?\", (agent_dict[\"id\"],))\n            row = cursor.fetchone()\n            if row:\n                cursor.execute(\"UPDATE agents SET purpose = ?, data = ? WHERE id = ?\", (agent_dict[\"purpose\"], json.dumps(agent_dict), agent_dict[\"id\"]))\n            else:\n                cursor.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict[\"id\"], agent_dict[\"purpose\"], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT id FROM agents WHERE id = ?\", (agent_dict['id'],))\n            row = cursor.fetchone()\n            if row:\n                # Update existing record\n                cursor.execute(\"UPDATE agents SET purpose = ?, data = ? WHERE id = ?\",\n                               (agent_dict['purpose'], json.dumps(agent_dict), agent_dict['id']))\n            else:\n                # Insert new record\n                cursor.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?, ?, ?)\",\n                               (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\n                \"INSERT OR REPLACE INTO agents (id, purpose, data) VALUES (?, ?, ?)\",\n                (agent_dict[\"id\"], agent_dict[\"purpose\"], json.dumps(agent_dict))\n            )"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT id FROM agents WHERE id = ?\", (agent_dict['id'],))\n            row = cursor.fetchone()\n            if row:\n                cursor.execute(\"UPDATE agents SET purpose = ?, data = ? WHERE id = ?\", (agent_dict['purpose'], json.dumps(agent_dict), agent_dict['id']))\n            else:\n                cursor.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT id FROM agents WHERE id = ?\", (agent_dict['id'],))\n            row = cursor.fetchone()\n            if row:\n                # Update the existing record\n                cursor.execute(\"UPDATE agents SET purpose = ?, data = ? WHERE id = ?\", (agent_dict['purpose'], json.dumps(agent_dict), agent_dict['id']))\n            else:\n                # Insert a new record\n                cursor.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT COUNT(*) FROM agents WHERE id = ?\", (agent_dict[\"id\"],))\n            count = cursor.fetchone()[0]\n            if count > 0:\n                cursor.execute(\"UPDATE agents SET purpose = ?, data = ? WHERE id = ?\", (agent_dict[\"purpose\"], json.dumps(agent_dict), agent_dict[\"id\"]))\n            else:\n                cursor.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict[\"id\"], agent_dict[\"purpose\"], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        agent_id = agent_dict['id']\n        agent_purpose = agent_dict['purpose']\n        agent_data = json.dumps(agent_dict)\n\n        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT COUNT(*) FROM agents WHERE id = ?\", (agent_id,))\n            count = cursor.fetchone()[0]\n\n            if count > 0:\n                cursor.execute(\"UPDATE agents SET purpose = ?, data = ? WHERE id = ?\", (agent_purpose, agent_data, agent_id))\n            else:\n                cursor.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_id, agent_purpose, agent_data))\n\n        conn.commit()"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT id FROM agents WHERE id = ?\", (agent_dict['id'],))\n            row = cursor.fetchone()\n            if row:\n                # Update existing agent record\n                conn.execute(\"UPDATE agents SET purpose = ?, data = ? WHERE id = ?\", (agent_dict['purpose'], json.dumps(agent_dict), agent_dict['id']))\n            else:\n                # Insert new agent record\n                conn.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row is not None:\n                return json.loads(row[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row is not None:\n                return json.loads(row[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row is not None:\n                return json.loads(row[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row is not None:\n                return json.loads(row[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row is not None:\n                return json.loads(row[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            return [row[0] for row in cursor.fetchall()]"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n        result = cursor.fetchone()\n        if result is None:\n            return None\n        return json.loads(result[0])"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n        result = cursor.fetchone()\n        if result is None:\n            return None\n        return json.loads(result[0])"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n        result = cursor.fetchone()\n        if result is not None:\n            return json.loads(result[0])\n        return None"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"SELECT result FROM cache WHERE hash = ?\",\n            (arg_hash,)\n        )\n        result = cursor.fetchone()\n        if result is not None:\n            return json.loads(result[0])\n        return None"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n        result = cursor.fetchone()\n        if result is not None:\n            return json.loads(result[0])\n        return None"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"SELECT result FROM cache WHERE hash = ?\",\n            (arg_hash,)\n        )\n        result = cursor.fetchone()\n        if result is None:\n            return None\n        return json.loads(result[0])"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"SELECT result FROM cache WHERE hash = ?\",\n            (arg_hash,)\n        )\n        result = cursor.fetchone()\n        if result is not None:\n            return json.loads(result[0])\n        return None"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"SELECT result FROM cache WHERE hash = ?\",\n            (arg_hash,)\n        )\n        result = cursor.fetchone()\n        if result is not None:\n            return json.loads(result[0])\n        return None"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n        row = cursor.fetchone()\n        if row is None:\n            return None\n        return json.loads(row[0])"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n        row = cursor.fetchone()\n        if row is not None:\n            return json.loads(row[0])\n        return None"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n        row = cursor.fetchone()\n        if row is None:\n            return None\n\n        return json.loads(row[0])"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n        result = cursor.fetchone()\n        if result is None:\n            return None\n\n        return json.loads(result[0])"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"SELECT result FROM cache WHERE hash = ?\",\n            (arg_hash,)\n        )\n        result = cursor.fetchone()\n        if result is None:\n            return None\n        return json.loads(result[0])"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"SELECT result FROM cache WHERE hash = ?\",\n            (arg_hash,)\n        )\n        result = cursor.fetchone()\n        if result is not None:\n            return json.loads(result[0])\n        return None"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"SELECT result FROM cache WHERE hash = ?\",\n            (arg_hash,)\n        )\n        result = cursor.fetchone()\n        if result is None:\n            return None\n        return json.loads(result[0])"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n        result = cursor.fetchone()\n        if result is not None:\n            return json.loads(result[0])\n        return None"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n        result = cursor.fetchone()\n        if result is not None:\n            return json.loads(result[0])\n        return None"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n        result = cursor.fetchone()\n        if result is None:\n            return None\n\n        return json.loads(result[0])"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"SELECT result FROM cache WHERE hash = ?\",\n            (arg_hash,)\n        )\n        row = cursor.fetchone()\n        if row is not None:\n            return json.loads(row[0])\n\n        return None"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n        result = cursor.fetchone()\n        if result is None:\n            return None\n\n        return json.loads(result[0])\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        self.connection.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        self.connection.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        self.connection.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        self.connection.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        self.connection.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        self.connection.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        self.connection.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_config(args)\n\n    # If quiet mode is enabled, redirect standard output to a file\n    if quiet_mode:\n        with open(os.devnull, 'w') as devnull:\n            with redirect_stdout(devnull):\n                start_command_line(vars(args))\n    else:\n        start_command_line(vars(args))"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with command line arguments\n    ARGS.update(vars(args))\n\n    # Redirect standard output to a file if quiet mode is enabled\n    if quiet_mode:\n        with open(os.devnull, 'w') as devnull:\n            with redirect_stdout(devnull):\n                start_command_line(ARGS)\n    else:\n        start_command_line(ARGS)"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with command line arguments\n    CONFIG.update_from_args(args)\n\n    # Redirect output to a file if quiet mode is enabled\n    if quiet_mode:\n        with open(os.devnull, 'w') as devnull:\n            with redirect_stdout(devnull):\n                start_command_line(deepcopy(vars(args)))\n    else:\n        start_command_line(deepcopy(vars(args)))"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with command line arguments\n    ARGS.update(vars(args))\n\n    # Redirect standard output to a file if quiet mode is enabled\n    if quiet_mode:\n        with open(os.devnull, 'w') as devnull:\n            with redirect_stdout(devnull):\n                start_command_line(ARGS)\n    else:\n        start_command_line(ARGS)"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with command line arguments\n    CONFIG.update(vars(args))\n\n    # Redirect standard output to a file if quiet mode is enabled\n    if quiet_mode:\n        with open(os.devnull, 'w') as devnull:\n            with redirect_stdout(devnull):\n                start_command_line(args)\n    else:\n        start_command_line(args)"}
{"namespace": "run.execute_command_line_process", "completion": "    args_dict = vars(args)\n    global_config = CONFIG\n    global_config.update(args_dict)\n\n    if quiet_mode:\n        with open(os.devnull, \"w\") as devnull:\n            with redirect_stdout(devnull):\n                start_command_line(args_dict)\n    else:\n        start_command_line(args_dict)"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update(vars(args))\n\n    # If quiet mode is enabled, redirect the standard output to a file instead of displaying it in the terminal\n    if quiet_mode:\n        with open(os.devnull, 'w') as devnull:\n            with redirect_stdout(devnull):\n                start_command_line(args)\n    else:\n        start_command_line(args)"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update the global configuration with the provided arguments\n    CONFIG.update(args)\n\n    # If quiet mode is enabled, redirect standard output to a file\n    if quiet_mode:\n        with open(os.devnull, 'w') as devnull:\n            with redirect_stdout(devnull):\n                start_command_line(args)\n    else:\n        start_command_line(args)"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with provided arguments\n    CONFIG.update(vars(args))\n\n    # If quiet mode is enabled, redirect output to a file instead of displaying it in the terminal\n    if quiet_mode:\n        with open(os.devnull, 'w') as devnull:\n            with redirect_stdout(devnull):\n                start_command_line(args)\n    else:\n        start_command_line(args)"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with command line arguments\n    CONFIG.update_config_from_args(args)\n\n    # If quiet mode is enabled, redirect standard output to a file\n    if quiet_mode:\n        with open(os.devnull, 'w') as devnull:\n            with redirect_stdout(devnull):\n                start_command_line(vars(args))\n    else:\n        start_command_line(vars(args))"}
{"namespace": "run.execute_command_line_process", "completion": "    global CONFIG, ARGS\n\n    # Update the global configuration with the provided arguments\n    CONFIG.update(vars(args))\n    ARGS = args\n\n    # If quiet mode is enabled, redirect the standard output to a file\n    if quiet_mode:\n        with open(os.devnull, 'w') as devnull:\n            with redirect_stdout(devnull):\n                start_command_line(CONFIG)\n    else:\n        start_command_line(CONFIG)"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with provided arguments\n    CONFIG.update(args)\n\n    # If quiet mode is enabled, redirect output to a file instead of displaying it in the terminal\n    if quiet_mode:\n        with open(os.devnull, 'w') as devnull:\n            with redirect_stdout(devnull):\n                start_command_line(args)\n    else:\n        start_command_line(args)"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update the global configuration parameters with the provided arguments\n    CONFIG.update_config(vars(args))\n\n    # If quiet mode is enabled, redirect the standard output to a file instead of displaying it in the terminal\n    if quiet_mode:\n        with open(os.devnull, 'w') as devnull:\n            with redirect_stdout(devnull):\n                start_command_line(deepcopy(vars(args)))\n    else:\n        start_command_line(deepcopy(vars(args)))"}
{"namespace": "run.execute_command_line_process", "completion": "    args_dict = vars(args)\n    update_global_config(args_dict)\n\n    if quiet_mode:\n        with open(os.devnull, 'w') as devnull:\n            with redirect_stdout(devnull):\n                start_command_line(args_dict)\n    else:\n        start_command_line(args_dict)\n\n"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with command line arguments\n    CONFIG.update_from_args(args)\n\n    if quiet_mode:\n        # Redirect standard output to a file\n        with open('output.txt', 'w') as f:\n            with redirect_stdout(f):\n                start_command_line(vars(args))\n    else:\n        start_command_line(vars(args))"}
{"namespace": "run.execute_command_line_process", "completion": "    global ARGS\n    ARGS = deepcopy(args)\n    if quiet_mode:\n        with open(os.path.join(args.record_dir, 'record.log'), 'w') as f:\n            with redirect_stdout(f):\n                start_command_line(vars(args))\n    else:\n        start_command_line(vars(args))"}
{"namespace": "run.execute_command_line_process", "completion": "    global CONFIG, ARGS\n\n    # Update global configuration parameters with the provided arguments\n    CONFIG.update(vars(args))\n    ARGS = args\n\n    # If quiet mode is enabled, redirect output to a file instead of displaying it in the terminal\n    if quiet_mode:\n        with open(os.devnull, \"w\") as f, redirect_stdout(f):\n            start_command_line(deepcopy(vars(args)))\n    else:\n        start_command_line(deepcopy(vars(args)))"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters\n    CONFIG.update_config(args)\n\n    # If quiet mode is enabled, redirect output to a file instead of displaying it in the terminal\n    if quiet_mode:\n        with open(os.devnull, 'w') as devnull:\n            with redirect_stdout(devnull):\n                start_command_line(args.__dict__)\n    else:\n        start_command_line(args.__dict__)"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update the global configuration parameters with the provided arguments\n    CONFIG.update_from_args(args)\n\n    if quiet_mode:\n        # Redirect standard output to a file\n        with open('output.txt', 'w') as f:\n            with redirect_stdout(f):\n                start_command_line(vars(args))\n    else:\n        start_command_line(vars(args))"}
{"namespace": "run.execute_command_line_process", "completion": "    global CONFIG\n    global ARGS\n\n    # Update global configuration parameters\n    CONFIG.update_from_args(args)\n    ARGS = deepcopy(args)\n\n    # Redirect output to a file if quiet mode is enabled\n    if quiet_mode:\n        with open(os.path.join(CONFIG.log_dir, \"command_line.log\"), \"w\") as f:\n            with redirect_stdout(f):\n                start_command_line(vars(args))\n    else:\n        start_command_line(vars(args))"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": ""}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": ""}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": ""}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": ""}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": ""}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_ap"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_ap"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_ap"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (self._last_time is not None and time() - self._last_time > self._refetch_interval):\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._last_time is None or time() - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._last_time is None or time() - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None:\n            self._create_client()\n        elif self._last_time is not None and time() - self._last_time > self._refetch_interval:\n            self._create_client()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if not self._client or (self._last_time and time() - self._last_time > self._refetch_interval):\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None:\n            self._create_client()\n        elif self._last_time is not None and time() - self._last_time > self._refetch_interval:\n            self._create_client()\n        self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (self._last_time is not None and time() - self._last_time > self._refetch_interval):\n            self._create_client()\n            self._last_time = time()\n        return self._client\n\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None:\n            self._create_client()\n        elif self._last_time is not None and time() - self._last_time > self._refetch_interval:\n            self._create_client()\n\n        self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._last_time is None or (time() - self._last_time) > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._last_time is None or time() - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n        return self._client\n\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (self._last_time is not None and time() - self._last_time > self._refetch_interval):\n            self._create_client()\n            self._last_time = time()\n        return self._client\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if not self._client or (self._last_time and time() - self._last_time > self._refetch_interval):\n            self._create_client()\n            self._last_time = time()\n        return self._client\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (\n            self._last_time is not None and time() - self._last_time > self._refetch_interval\n        ):\n            self._create_client()\n            self._last_time = time()\n        return self._client\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (self._last_time is not None and time() - self._last_time > self._refetch_interval):\n            self._create_client()\n            self._last_time = time()\n        return self._client\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._last_time is None or time() - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n        return self._client\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (self._last_time is not None and time() - self._last_time > self._refetch_interval):\n            self._create_client()\n            self._last_time = time()\n        return self._client\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if not self._client:\n            self._create_client()\n        elif self._last_time and time() - self._last_time > self._refetch_interval:\n            self._create_client()\n\n        return self._client\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._last_time is None or time() - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n\n        return self._client\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (self._last_time is not None and time() - self._last_time > self._refetch_interval):\n            self._create_client()\n            self._last_time = time()\n        return self._client\n\n"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (self._last_time is not None and time() - self._last_time > self._refetch_interval):\n            self._create_client()\n            self._last_time = time()\n        return self._client\n\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\n                \"The `state_dict` method should not be called from a DataLoader worker process.\"\n            )\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n    "}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\n                \"The `state_dict` method should not be called from within a DataLoader worker process.\"\n            )\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return state\n\n    "}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\n                \"The `state_dict` method should not be called from a DataLoader worker process. \"\n                \"It should only be called from the main process.\"\n            )\n\n        if self.item_loader is not None:\n            item_loader_state = self.item_loader.state_dict()\n        else:\n            item_loader_state = None\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": item_loader_state,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\n                \"The `state_dict` method should not be called from a DataLoader worker process. \"\n                \"It should be called from the main process or a process that is not a DataLoader worker.\"\n            )\n\n        if self.item_loader is not None:\n            item_loader_state = self.item_loader.state_dict()\n        else:\n            item_loader_state = None\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": item_loader_state,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n    "}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\n                \"The `state_dict` method should not be called from a DataLoader worker process. \"\n                \"It should only be called from the main process.\"\n            )\n\n        if self.item_loader is None:\n            item_loader_state = None\n        else:\n            item_loader_state = self.item_loader.state_dict()\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": item_loader_state,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise ValueError(\n                \"The `state_dict` method should not be called from within a DataLoader worker process.\"\n            )\n\n        if self.item_loader is not None:\n            item_loader_state = self.item_loader.state_dict()\n        else:\n            item_loader_state = None\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": item_loader_state,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\n                \"The state_dict method should not be called from a DataLoader worker process. \"\n                \"Please call it from the main process.\"\n            )\n\n        if self.item_loader is not None:\n            item_loader_state = self.item_loader.state_dict()\n        else:\n            item_loader_state = None\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": item_loader_state,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\n                \"The `state_dict` method should not be called from a DataLoader worker process.\"\n            )\n\n        assert self.worker_env is not None\n        assert self.shuffler is not None\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\n                \"The `state_dict` method should not be called from a DataLoader worker process. \"\n                \"It should be called from the main process or from a worker process that is not a DataLoader worker.\"\n            )\n\n        if self.item_loader is None:\n            item_loader_state = None\n        else:\n            item_loader_state = self.item_loader.state_dict()\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": item_loader_state,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise ValueError(\n                \"The `state_dict` method should not be called from a DataLoader worker process. \"\n                \"Instead, use the `load_state_dict` method to restore the state of the StreamingDataset instance.\"\n            )\n\n        state_dict = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"seed\": self.seed,\n            \"drop_last\": self.drop_last,\n            \"shuffle\": self.shuffle,\n            \"world_size\": self.distributed_env.world_size,\n        }\n\n        if self.item_loader:\n            state_dict[\"item_loader\"] = self.item_loader.state_dict()\n\n        return state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise ValueError(\n                \"The `state_dict` method cannot be called from a DataLoader worker process. \"\n                \"It should be called from the main process only.\"\n            )\n\n        if self.cache is None:\n            self.worker_env = _WorkerEnv.detect()\n            self.cache = self._create_cache(worker_env=self.worker_env)\n            self.shuffler = self._create_shuffler(self.cache)\n\n        state_dict = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return state_dict\n\n    "}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\n                \"The `state_dict` method should not be called from a DataLoader worker process. \"\n                \"Please call it from the main process.\"\n            )\n\n        assert self.cache\n        assert self.worker_env\n        assert self.shuffler\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n        return state"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\n                \"The `state_dict` method should not be called from a DataLoader worker process. \"\n                \"It should be called from the main process or from the parent process of the DataLoader worker process.\"\n            )\n\n        state_dict = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\n                \"The `state_dict` method should not be called from a DataLoader worker process. \"\n                \"Please call it from the main process or from a worker process that is not used by the DataLoader.\"\n            )\n\n        state_dict = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"seed\": self.seed,\n            \"drop_last\": self.drop_last,\n            \"shuffle\": self.shuffle,\n            \"world_size\": self.distributed_env.world_size,\n        }\n\n        if self.item_loader:\n            state_dict[\"item_loader\"] = self.item_loader.state_dict()\n\n        return state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise ValueError(\n                \"The `state_dict` method should not be called from a DataLoader worker process. \"\n                \"This method is intended to be used by the DataLoader to save the state of the dataset.\"\n            )\n\n        item_loader_state = None\n        if self.item_loader:\n            item_loader_state = self.item_loader.state_dict()\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": item_loader_state,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\n                \"`state_dict` should be called only from the main process. \"\n                \"If you're using the StreamingDataset within a DataLoader, \"\n                \"the state dict will be automatically handled for you.\"\n            )\n\n        assert self.worker_env\n        assert self.cache\n\n        state_dict = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n            \"drop_last\": self.drop_last,\n        }\n\n        if self.item_loader:\n            state_dict[\"item_loader\"] = self.item_loader.state_dict()\n\n        return state_dict\n\n    "}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\n                \"`state_dict` should only be called from the main process. \"\n                \"Calling `state_dict` from within a DataLoader worker process is not supported.\"\n            )\n\n        assert self.worker_env\n        assert self.cache\n\n        item_loader_state = None\n        if self.item_loader:\n            item_loader_state = self.item_loader.state_dict()\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": item_loader_state,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\n                \"The `state_dict` method should not be called from a DataLoader worker process. \"\n                \"Please use the `load_state_dict` method instead.\"\n            )\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"seed\": self.seed,\n            \"shuffle\": self.shuffle,\n            \"drop_last\": self.drop_last,\n            \"world_size\": self.distributed_env.world_size,\n        }\n\n        if self.item_loader is not None:\n            state[\"item_loader\"] = self.item_loader.state_dict()\n\n        return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\n                \"`state_dict` should be called outside of the DataLoader worker process.\"\n                \" This is to avoid re-downloading the data.\"\n            )\n\n        if self.item_loader is None:\n            item_loader_state = None\n        else:\n            item_loader_state = self.item_loader.state_dict()\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": item_loader_state,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise ValueError(\n                \"The `state_dict` method should not be called from a DataLoader worker process. \"\n                \"It is intended to be used in the main process, not in the worker processes.\"\n            )\n\n        assert self.worker_env\n        assert self.cache\n\n        state_dict = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"shuffle\": self.shuffle,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n        }\n\n        if self.item_loader is not None:\n            state_dict[\"item_loader\"] = self.item_loader.state_dict()\n\n        return state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._state_dict = state_dict"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._state_dict = state_dict"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._state_dict = state_dict"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._state_dict = state_dict\n\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict\n\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict\n\n        self.input_dir = Dir(path=state_dict[\"input_dir_path\"], url=state_dict[\"input_dir_url\"])\n        self.item_loader = BaseItemLoader.load_state_dict(state_dict[\"item_loader\"]) if state_dict[\"item_loader\"] else None\n        self.shuffle = state_dict[\"shuffle\"]\n        self.drop_last = state_dict[\"drop_last\"]\n        self.seed = state_dict[\"seed\"]\n        self.distributed_env = _DistributedEnv(state_dict[\"world_size\"])"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict\n\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._state_dict = state_dict\n\n        self.current_epoch = state_dict[\"current_epoch\"]\n        self.seed = state_dict[\"seed\"]\n        self.drop_last = state_dict[\"drop_last\"]\n        self.distributed_env = _DistributedEnv.detect()\n\n        if state_dict[\"input_dir_path\"] is not None:\n            self.input_dir.path = state_dict[\"input_dir_path\"]\n\n        if state_dict[\"input_dir_url\"] is not None:\n            self.input_dir.url = state_dict[\"input_dir_url\"]\n\n        if state_dict[\"item_loader\"] is not None:\n            self.item_loader = BaseItemLoader.load_state_dict(state_dict[\"item_loader\"])\n\n        if state_dict[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                \"The provided `shuffle` state doesn't match the current one. \"\n                f\"Found `{self.shuffle}` instead of `{state_dict['shuffle']}`.\"\n            )\n\n        self.shuffler = self._create_shuffler(self._create_cache(worker_env=_WorkerEnv.detect()))\n\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        assert self._state_dict is not None\n        assert self.worker_env is not None\n        assert self.cache is not None\n\n        state = self._state_dict\n\n        if self.shuffle != state[\"shuffle\"]:\n            raise ValueError(\n                f\"The shuffle parameter of the StreamingDataset instance ({self.shuffle}) does not match the state dictionary ({state['shuffle']}).\"\n            )\n\n        if self.worker_env.world_size != state[\"world_size\"]:\n            raise ValueError(\n                f\"The world size of the StreamingDataset instance ({self.worker_env.world_size}) does not match the state dictionary ({state['world_size']}).\"\n            )\n\n        if self.seed != state[\"seed\"]:\n            raise ValueError(\n                f\"The seed of the StreamingDataset instance ({self.seed}) does not match the state dictionary ({state['seed']}).\"\n            )\n\n        if self.input_dir.path != state[\"input_dir_path\"]:\n            raise ValueError(\n                f\"The input directory path of the StreamingDataset instance ({self.input_dir.path}) does not match the state dictionary ({state['input_dir_path']}).\"\n            )\n\n        if self.input_dir.url != state[\"input_dir_url\"]:\n            raise ValueError(\n                f\"The input directory URL of the StreamingDataset instance ({self.input_dir.url}) does not match the state dictionary ({state['input_dir_url']}).\"\n            )\n\n        if self.item_loader is not None and self.item_loader.state_dict() != state[\"item_loader\"]:\n            raise ValueError(\n                f\"The item_loader state of the StreamingDataset instance ({self.item_loader.state_dict()}) does not match the state dictionary ({state['item_loader']}).\"\n            )\n\n        if self.drop_last != state[\"drop_last\"]:\n            raise ValueError(\n                f\"The drop_last parameter of"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        assert self._state_dict is not None\n        assert self.worker_env is not None\n        assert self.cache is not None\n\n        state = self._state_dict\n\n        if self.shuffle != state[\"shuffle\"]:\n            raise ValueError(\n                f\"The shuffle parameter of the state dictionary ({state['shuffle']}) does not match the current state ({self.shuffle}).\"\n            )\n\n        if self.worker_env.world_size != state[\"world_size\"]:\n            raise ValueError(\n                f\"The world size of the state dictionary ({state['world_size']}) does not match the current state ({self.worker_env.world_size}).\"\n            )\n\n        if self.cache._reader._input_dir.path != state[\"input_dir_path\"]:\n            raise ValueError(\n                f\"The input directory path of the state dictionary ({state['input_dir_path']}) does not match the current state ({self.cache._reader._input_dir.path}).\"\n            )\n\n        if self.cache._reader._input_dir.url != state[\"input_dir_url\"]:\n            raise ValueError(\n                f\"The input directory URL of the state dictionary ({state['input_dir_url']}) does not match the current state ({self.cache._reader._input_dir.url}).\"\n            )\n\n        if self.seed != state[\"seed\"]:\n            raise ValueError(\n                f\"The seed of the state dictionary ({state['seed']}) does not match the current state ({self.seed}).\"\n            )\n\n        if self.drop_last != state[\"drop_last\"]:\n            raise ValueError(\n                f\"The drop_last parameter of the state dictionary ({state['drop_last']}) does not match the current state ({self.drop_last}).\"\n            )\n\n        if self.item_loader is not None and self.item_loader.state_dict() != state[\"item_loader\"]:\n            raise ValueError(\n                f\"The item_loader state of the state dictionary ({state['"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        assert self._state_dict\n        state = self._state_dict\n\n        if self.shuffle != state[\"shuffle\"]:\n            raise ValueError(\n                f\"The shuffle parameter of the state dictionary ({state['shuffle']}) does not match the current state of the StreamingDataset ({self.shuffle}).\"\n            )\n\n        if self.worker_env.world_size != state[\"world_size\"]:\n            raise ValueError(\n                f\"The world size parameter of the state dictionary ({state['world_size']}) does not match the current state of the StreamingDataset ({self.worker_env.world_size}).\"\n            )\n\n        if self.input_dir.path != state[\"input_dir_path\"]:\n            raise ValueError(\n                f\"The input directory path parameter of the state dictionary ({state['input_dir_path']}) does not match the current state of the StreamingDataset ({self.input_dir.path}).\"\n            )\n\n        if self.input_dir.url != state[\"input_dir_url\"]:\n            raise ValueError(\n                f\"The input directory URL parameter of the state dictionary ({state['input_dir_url']}) does not match the current state of the StreamingDataset ({self.input_dir.url}).\"\n            )\n\n        if self.seed != state[\"seed\"]:\n            raise ValueError(\n                f\"The seed parameter of the state dictionary ({state['seed']}) does not match the current state of the StreamingDataset ({self.seed}).\"\n            )\n\n        if self.drop_last != state[\"drop_last\"]:\n            raise ValueError(\n                f\"The drop_last parameter of the state dictionary ({state['drop_last']}) does not match the current state of the StreamingDataset ({self.drop_last}).\"\n            )\n\n        if self.item_loader is not None and self.item_loader.state_dict() != state[\"item_loader\"]:\n            raise ValueError(\n                f\"The item_loader parameter of the state dictionary ({state['"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        assert self._state_dict is not None\n        assert self.worker_env is not None\n        assert self.cache is not None\n\n        state = self._state_dict\n\n        if self.shuffle != state[\"shuffle\"]:\n            raise ValueError(\n                f\"The shuffle parameter is different between the state dictionary and the current state of the StreamingDataset instance. \"\n                f\"State dictionary: {state['shuffle']}, Current state: {self.shuffle}\"\n            )\n\n        if self.worker_env.world_size != state[\"world_size\"]:\n            raise ValueError(\n                f\"The world size parameter is different between the state dictionary and the current state of the StreamingDataset instance. \"\n                f\"State dictionary: {state['world_size']}, Current state: {self.worker_env.world_size}\"\n            )\n\n        if self.cache._reader.input_dir.path != state[\"input_dir_path\"]:\n            raise ValueError(\n                f\"The input directory path is different between the state dictionary and the current state of the StreamingDataset instance. \"\n                f\"State dictionary: {state['input_dir_path']}, Current state: {self.cache._reader.input_dir.path}\"\n            )\n\n        if self.cache._reader.input_dir.url != state[\"input_dir_url\"]:\n            raise ValueError(\n                f\"The input directory URL is different between the state dictionary and the current state of the StreamingDataset instance. \"\n                f\"State dictionary: {state['input_dir_url']}, Current state: {self.cache._reader.input_dir.url}\"\n            )\n\n        if self.seed != state[\"seed\"]:\n            raise ValueError(\n                f\"The seed parameter is different between the state dictionary and the current state of the StreamingDataset instance. \"\n                f\"State dictionary: {state['seed']}, Current state: {self.seed}\"\n            )\n\n        if self.drop_last != state[\"drop_last\"]:\n            raise ValueError(\n                f\"The drop_last parameter is different between the"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        assert self._state_dict\n\n        if self.shuffle != self._state_dict[\"shuffle\"]:\n            raise ValueError(\n                f\"The shuffle parameter of the state dictionary ({self._state_dict['shuffle']}) does not match the current state ({self.shuffle}).\"\n            )\n\n        if self.input_dir.path != self._state_dict[\"input_dir_path\"]:\n            raise ValueError(\n                f\"The input directory path of the state dictionary ({self._state_dict['input_dir_path']}) does not match the current state ({self.input_dir.path}).\"\n            )\n\n        if self.input_dir.url != self._state_dict[\"input_dir_url\"]:\n            raise ValueError(\n                f\"The input directory URL of the state dictionary ({self._state_dict['input_dir_url']}) does not match the current state ({self.input_dir.url}).\"\n            )\n\n        if self.seed != self._state_dict[\"seed\"]:\n            raise ValueError(\n                f\"The seed of the state dictionary ({self._state_dict['seed']}) does not match the current state ({self.seed}).\"\n            )\n\n        if self.drop_last != self._state_dict[\"drop_last\"]:\n            raise ValueError(\n                f\"The drop_last parameter of the state dictionary ({self._state_dict['drop_last']}) does not match the current state ({self.drop_last}).\"\n            )\n\n        if self.item_loader is not None:\n            if self.item_loader.state_dict() != self._state_dict[\"item_loader\"]:\n                raise ValueError(\n                    f\"The item_loader state of the state dictionary ({self._state_dict['item_loader']}) does not match the current state ({self.item_loader.state_dict()}).\"\n                )\n\n        if self.distributed_env.world_size != self._state_dict[\"world_size\"]:\n            raise ValueError(\n                f\"The world_"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            return\n\n        state = self._state_dict\n\n        if self.shuffle != state[\"shuffle\"]:\n            raise ValueError(\n                f\"The state dictionary has a different shuffle value than the current state. \"\n                f\"Expected {self.shuffle}, got {state['shuffle']}.\"\n            )\n\n        if self.seed != state[\"seed\"]:\n            raise ValueError(\n                f\"The state dictionary has a different seed value than the current state. \"\n                f\"Expected {self.seed}, got {state['seed']}.\"\n            )\n\n        if self.input_dir.path != state[\"input_dir_path\"]:\n            raise ValueError(\n                f\"The state dictionary has a different input directory path than the current state. \"\n                f\"Expected {self.input_dir.path}, got {state['input_dir_path']}.\"\n            )\n\n        if self.input_dir.url != state[\"input_dir_url\"]:\n            raise ValueError(\n                f\"The state dictionary has a different input directory URL than the current state. \"\n                f\"Expected {self.input_dir.url}, got {state['input_dir_url']}.\"\n            )\n\n        if self.item_loader and self.item_loader.state_dict() != state[\"item_loader\"]:\n            raise ValueError(\n                f\"The state dictionary has a different item_loader than the current state. \"\n                f\"Expected {self.item_loader.state_dict()}, got {state['item_loader']}.\"\n            )\n\n        if self.drop_last != state[\"drop_last\"]:\n            raise ValueError(\n                f\"The state dictionary has a different drop_last value than the current state. \"\n                f\"Expected {self.drop_last}, got {state['drop_last']}.\"\n            )\n\n        if self.distributed_env.world_size != state[\"world_size\"]:\n            raise ValueError(\n                f\"The state dictionary has a different world_size value than the current state."}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        assert self._state_dict is not None\n        state = self._state_dict\n\n        if self.shuffle != state[\"shuffle\"]:\n            raise ValueError(\n                f\"The `shuffle` parameter of the StreamingDataset ({self.shuffle}) does not match the state dictionary ({state['shuffle']}).\"\n            )\n\n        if self.seed != state[\"seed\"]:\n            raise ValueError(\n                f\"The `seed` parameter of the StreamingDataset ({self.seed}) does not match the state dictionary ({state['seed']}).\"\n            )\n\n        if self.drop_last != state[\"drop_last\"]:\n            raise ValueError(\n                f\"The `drop_last` parameter of the StreamingDataset ({self.drop_last}) does not match the state dictionary ({state['drop_last']}).\"\n            )\n\n        if self.input_dir.path != state[\"input_dir_path\"]:\n            raise ValueError(\n                f\"The `input_dir.path` parameter of the StreamingDataset ({self.input_dir.path}) does not match the state dictionary ({state['input_dir_path']}).\"\n            )\n\n        if self.input_dir.url != state[\"input_dir_url\"]:\n            raise ValueError(\n                f\"The `input_dir.url` parameter of the StreamingDataset ({self.input_dir.url}) does not match the state dictionary ({state['input_dir_url']}).\"\n            )\n\n        if self.item_loader is not None:\n            if self.item_loader.state_dict() != state[\"item_loader\"]:\n                raise ValueError(\n                    f\"The `item_loader` parameter of the StreamingDataset ({self.item_loader.state_dict()}) does not match the state dictionary ({state['item_loader']}).\"\n                )\n        else:\n            if state[\"item_loader\"] is not None:\n                raise ValueError(\n                    f\"The `item_loader` parameter of the StreamingDataset ({self.item"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        assert self._state_dict is not None\n\n        # Validate the state dictionary against the current state of the instance\n        if self.shuffle != self._state_dict[\"shuffle\"]:\n            raise ValueError(\n                f\"The state dictionary's shuffle value ({self._state_dict['shuffle']}) does not match the current state's shuffle value ({self.shuffle}).\"\n            )\n\n        if self.seed != self._state_dict[\"seed\"]:\n            raise ValueError(\n                f\"The state dictionary's seed value ({self._state_dict['seed']}) does not match the current state's seed value ({self.seed}).\"\n            )\n\n        if self.input_dir.path != self._state_dict[\"input_dir_path\"]:\n            raise ValueError(\n                f\"The state dictionary's input directory path ({self._state_dict['input_dir_path']}) does not match the current state's input directory path ({self.input_dir.path}).\"\n            )\n\n        if self.input_dir.url != self._state_dict[\"input_dir_url\"]:\n            raise ValueError(\n                f\"The state dictionary's input directory URL ({self._state_dict['input_dir_url']}) does not match the current state's input directory URL ({self.input_dir.url}).\"\n            )\n\n        if self.drop_last != self._state_dict[\"drop_last\"]:\n            raise ValueError(\n                f\"The state dictionary's drop_last value ({self._state_dict['drop_last']}) does not match the current state's drop_last value ({self.drop_last}).\"\n            )\n\n        if self.distributed_env.world_size != self._state_dict[\"world_size\"]:\n            raise ValueError(\n                f\"The state dictionary's world size ({self._state_dict['world_size']}) does not match the current state's world size ({self.distributed_env.world_size}).\"\n            )\n\n        if self."}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            return\n        state = self._state_dict\n\n        if self.shuffle != state[\"shuffle\"]:\n            raise ValueError(\n                f\"The shuffle parameter in the state dictionary ({state['shuffle']}) does not match the current state of the StreamingDataset ({self.shuffle}).\"\n            )\n\n        if self.input_dir.path != state[\"input_dir_path\"]:\n            raise ValueError(\n                f\"The input directory path in the state dictionary ({state['input_dir_path']}) does not match the current state of the StreamingDataset ({self.input_dir.path}).\"\n            )\n\n        if self.input_dir.url != state[\"input_dir_url\"]:\n            raise ValueError(\n                f\"The input directory URL in the state dictionary ({state['input_dir_url']}) does not match the current state of the StreamingDataset ({self.input_dir.url}).\"\n            )\n\n        if self.seed != state[\"seed\"]:\n            raise ValueError(\n                f\"The seed in the state dictionary ({state['seed']}) does not match the current state of the StreamingDataset ({self.seed}).\"\n            )\n\n        if self.drop_last != state[\"drop_last\"]:\n            raise ValueError(\n                f\"The drop_last parameter in the state dictionary ({state['drop_last']}) does not match the current state of the StreamingDataset ({self.drop_last}).\"\n            )\n\n        if self.item_loader and self.item_loader.state_dict() != state[\"item_loader\"]:\n            raise ValueError(\n                f\"The item_loader state in the state dictionary ({state['item_loader']}) does not match the current state of the StreamingDataset ({self.item_loader.state_dict()}).\"\n            )\n\n        if self.distributed_env.world_size != state[\"world_size\"]:\n            raise ValueError(\n                f\"The world size in the state dictionary ({state"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        assert self._state_dict\n        state = self._state_dict\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The input_dir_path in the state dictionary ({state['input_dir_path']}) does not match the current input_dir_path ({self.input_dir.path}).\"\n            )\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The input_dir_url in the state dictionary ({state['input_dir_url']}) does not match the current input_dir_url ({self.input_dir.url}).\"\n            )\n\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                f\"The drop_last in the state dictionary ({state['drop_last']}) does not match the current drop_last ({self.drop_last}).\"\n            )\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(f\"The seed in the state dictionary ({state['seed']}) does not match the current seed ({self.seed}).\")\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"The shuffle in the state dictionary ({state['shuffle']}) does not match the current shuffle ({self.shuffle}).\"\n            )\n\n        if state[\"item_loader\"] != self.item_loader.state_dict():\n            raise ValueError(\n                f\"The item_loader in the state dictionary ({state['item_loader']}) does not match the current item_loader ({self.item_loader.state_dict()}).\"\n            )\n\n        if state[\"world_size\"] != self.distributed_env.world_size:\n            raise ValueError(\n                f\"The world_size in the state dictionary ({state['world_size']}) does not match the current world_size ({self.distributed_env.world_size}).\"\n            )"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            raise ValueError(\"The state dict is None.\")\n\n        state = self._state_dict\n\n        if self.shuffle != state[\"shuffle\"]:\n            raise ValueError(\n                f\"The state dict shuffle is {state['shuffle']} while the current state shuffle is {self.shuffle}.\"\n            )\n\n        if self.distributed_env.world_size != state[\"world_size\"]:\n            raise ValueError(\n                f\"The state dict world size is {state['world_size']} while the current state world size is {self.distributed_env.world_size}.\"\n            )\n\n        if self.drop_last != state[\"drop_last\"]:\n            raise ValueError(\n                f\"The state dict drop_last is {state['drop_last']} while the current state drop_last is {self.drop_last}.\"\n            )\n\n        if self.seed != state[\"seed\"]:\n            raise ValueError(f\"The state dict seed is {state['seed']} while the current state seed is {self.seed}.\")\n\n        if self.input_dir.path != state[\"input_dir_path\"]:\n            raise ValueError(\n                f\"The state dict input_dir_path is {state['input_dir_path']} while the current state input_dir_path is {self.input_dir.path}.\"\n            )\n\n        if self.input_dir.url != state[\"input_dir_url\"]:\n            raise ValueError(\n                f\"The state dict input_dir_url is {state['input_dir_url']} while the current state input_dir_url is {self.input_dir.url}.\"\n            )\n\n        if self.item_loader is None and state[\"item_loader\"] is not None:\n            raise ValueError(\n                f\"The state dict item_loader is {state['item_loader']} while the current state item_loader is None.\"\n            )\n\n        if self.item_loader is not None and state[\"item_loader\"] is None:\n            raise Value"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        assert self._state_dict is not None\n\n        if self.worker_env is None:\n            self.worker_env = _WorkerEnv.detect()\n\n        if self.cache is None:\n            self.cache = self._create_cache(worker_env=self.worker_env)\n\n        if self.shuffler is None:\n            self.shuffler = self._create_shuffler(self.cache)\n\n        # Check if the shuffle parameter matches\n        if self.shuffle != state_dict[\"shuffle\"]:\n            raise ValueError(\n                f\"The shuffle parameter in the state dictionary ({state_dict['shuffle']}) does not match the current state of the StreamingDataset ({self.shuffle}).\"\n            )\n\n        # Check if the input directory path matches\n        if self.input_dir.path != state_dict[\"input_dir_path\"]:\n            raise ValueError(\n                f\"The input directory path in the state dictionary ({state_dict['input_dir_path']}) does not match the current state of the StreamingDataset ({self.input_dir.path}).\"\n            )\n\n        # Check if the input directory URL matches\n        if self.input_dir.url != state_dict[\"input_dir_url\"]:\n            raise ValueError(\n                f\"The input directory URL in the state dictionary ({state_dict['input_dir_url']}) does not match the current state of the StreamingDataset ({self.input_dir.url}).\"\n            )\n\n        # Check if the seed matches\n        if self.seed != state_dict[\"seed\"]:\n            raise ValueError(\n                f\"The seed in the state dictionary ({state_dict['seed']}) does not match the current state of the StreamingDataset ({self.seed}).\"\n            )\n\n        # Check if the item_loader state matches\n        if self.item_loader is not None and self.item_loader.state_dict() != state_dict[\"item_loader\"]:\n            raise ValueError(\n                f\"The item_loader state in the state dictionary"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            raise RuntimeError(\"The state dictionary is empty.\")\n\n        state = self._state_dict\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The input directory path in the state dictionary ({state['input_dir_path']}) does not match the current input directory path ({self.input_dir.path}).\"\n            )\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The input directory URL in the state dictionary ({state['input_dir_url']}) does not match the current input directory URL ({self.input_dir.url}).\"\n            )\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"The shuffle flag in the state dictionary ({state['shuffle']}) does not match the current shuffle flag ({self.shuffle}).\"\n            )\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(f\"The seed in the state dictionary ({state['seed']}) does not match the current seed ({self.seed}).\")\n\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                f\"The drop_last flag in the state dictionary ({state['drop_last']}) does not match the current drop_last flag ({self.drop_last}).\"\n            )\n\n        if self.item_loader is None:\n            if state[\"item_loader\"] is not None:\n                raise ValueError(\n                    f\"The item_loader in the state dictionary ({state['item_loader']}) does not match the current item_loader ({self.item_loader}).\"\n                )\n        else:\n            if state[\"item_loader\"] is None:\n                raise ValueError(\n                    f\"The item_loader in the state dictionary ({state['item_loader']}) does not match the current item_loader ({self.item_loader}).\"\n                )\n            else:\n                if state[\"item_"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            raise ValueError(\"The state dictionary is not set.\")\n\n        if self.worker_env is None:\n            raise ValueError(\"The worker environment is not set.\")\n\n        if self.cache is None:\n            raise ValueError(\"The cache is not set.\")\n\n        if self.shuffle != self._state_dict[\"shuffle\"]:\n            raise ValueError(\"The shuffle parameter in the state dictionary does not match the current state.\")\n\n        if self.seed != self._state_dict[\"seed\"]:\n            raise ValueError(\"The seed parameter in the state dictionary does not match the current state.\")\n\n        if self.distributed_env.world_size != self._state_dict[\"world_size\"]:\n            raise ValueError(\n                \"The world size parameter in the state dictionary does not match the current state.\"\n            )\n\n        if self.input_dir.path != self._state_dict[\"input_dir_path\"]:\n            raise ValueError(\"The input directory path in the state dictionary does not match the current state.\")\n\n        if self.input_dir.url != self._state_dict[\"input_dir_url\"]:\n            raise ValueError(\"The input directory URL in the state dictionary does not match the current state.\")\n\n        if self.item_loader is not None:\n            if self.item_loader.state_dict() != self._state_dict[\"item_loader\"]:\n                raise ValueError(\"The item loader state in the state dictionary does not match the current state.\")\n\n        if self.drop_last != self._state_dict[\"drop_last\"]:\n            raise ValueError(\"The drop_last parameter in the state dictionary does not match the current state.\")"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        assert self._state_dict\n        assert self.worker_env\n        assert self.cache\n        assert self.shuffler\n\n        state = self._state_dict\n\n        if self.shuffle != state[\"shuffle\"]:\n            raise ValueError(\n                f\"The shuffle parameter is different between the current state and the state dictionary. \"\n                f\"Current state: {self.shuffle}, State dictionary: {state['shuffle']}\"\n            )\n\n        if self.worker_env.world_size != state[\"world_size\"]:\n            raise ValueError(\n                f\"The world size is different between the current state and the state dictionary. \"\n                f\"Current state: {self.worker_env.world_size}, State dictionary: {state['world_size']}\"\n            )\n\n        if self.cache.input_dir.path != state[\"input_dir_path\"]:\n            raise ValueError(\n                f\"The input directory path is different between the current state and the state dictionary. \"\n                f\"Current state: {self.cache.input_dir.path}, State dictionary: {state['input_dir_path']}\"\n            )\n\n        if self.cache.input_dir.url != state[\"input_dir_url\"]:\n            raise ValueError(\n                f\"The input directory url is different between the current state and the state dictionary. \"\n                f\"Current state: {self.cache.input_dir.url}, State dictionary: {state['input_dir_url']}\"\n            )\n\n        if self.seed != state[\"seed\"]:\n            raise ValueError(\n                f\"The seed is different between the current state and the state dictionary. \"\n                f\"Current state: {self.seed}, State dictionary: {state['seed']}\"\n            )\n\n        if self.item_loader is not None and self.item_loader.state_dict() != state[\"item_loader\"]:\n            raise ValueError(\n                f\"The item_loader is different between the current state and the state dictionary. \"\n                f\"Current state: {self.item_loader.state_dict()}, State dictionary: {state['"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        state = self._state_dict\n\n        # Check if the shuffle parameter is consistent\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(f\"The state dictionary shuffle parameter ({state['shuffle']}) is not consistent with the current state ({self.shuffle}).\")\n\n        # Check if the num_workers parameter is consistent\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(f\"The state dictionary num_workers parameter ({state['num_workers']}) is not consistent with the current state ({self.worker_env.world_size}).\")\n\n        # Check if the input directory path is consistent\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(f\"The state dictionary input directory path ({state['input_dir_path']}) is not consistent with the current state ({self.input_dir.path}).\")\n\n        # Check if the input directory URL is consistent\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(f\"The state dictionary input directory URL ({state['input_dir_url']}) is not consistent with the current state ({self.input_dir.url}).\")\n\n        # Check if the seed is consistent\n        if state[\"seed\"] != self.seed:\n            raise ValueError(f\"The state dictionary seed ({state['seed']}) is not consistent with the current state ({self.seed}).\")\n\n        # Check if the item_loader state is consistent\n        if self.item_loader and state[\"item_loader\"] != self.item_loader.state_dict():\n            raise ValueError(f\"The state dictionary item_loader state is not consistent with the current state.\")\n\n        # Check if the drop_last flag is consistent\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(f\"The state dictionary drop_last flag ({state['drop_last']}) is not consistent with the current state ({self.drop_last}).\")"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        assert self._state_dict\n        assert self.worker_env\n        assert self.shuffler\n\n        state: Dict[str, Any] = self._state_dict\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"The state_dict has been loaded with a different shuffle value. \"\n                f\"The current shuffle is {self.shuffle} and the state_dict shuffle is {state['shuffle']}.\"\n            )\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(\n                f\"The state_dict has been loaded with a different seed value. \"\n                f\"The current seed is {self.seed} and the state_dict seed is {state['seed']}.\"\n            )\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The state_dict has been loaded with a different input_dir. \"\n                f\"The current input_dir is {self.input_dir.path} and the state_dict input_dir is {state['input_dir_path']}.\"\n            )\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The state_dict has been loaded with a different input_dir. \"\n                f\"The current input_dir is {self.input_dir.url} and the state_dict input_dir is {state['input_dir_url']}.\"\n            )\n\n        if state[\"item_loader\"] != self.item_loader.state_dict():\n            raise ValueError(\n                f\"The state_dict has been loaded with a different item_loader. \"\n                f\"The current item_loader is {self.item_loader.state_dict()} and the state_dict item_loader is {state['item_loader']}.\"\n            )\n\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                f\"The state_dict has been loaded with a different drop_last value. \"\n                f"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if not isinstance(self._state_dict, dict):\n            raise ValueError(\"The state_dict must be a dictionary.\")\n\n        if \"input_dir_path\" not in self._state_dict:\n            raise ValueError(\"The state_dict must contain the input_dir_path.\")\n\n        if \"input_dir_url\" not in self._state_dict:\n            raise ValueError(\"The state_dict must contain the input_dir_url.\")\n\n        if \"num_samples_yielded\" not in self._state_dict:\n            raise ValueError(\"The state_dict must contain the num_samples_yielded.\")\n\n        if \"num_workers\" not in self._state_dict:\n            raise ValueError(\"The state_dict must contain the num_workers.\")\n\n        if \"batch_size\" not in self._state_dict:\n            raise ValueError(\"The state_dict must contain the batch_size.\")\n\n        if \"current_epoch\" not in self._state_dict:\n            raise ValueError(\"The state_dict must contain the current_epoch.\")\n\n        if \"item_loader\" not in self._state_dict:\n            raise ValueError(\"The state_dict must contain the item_loader.\")\n\n        if \"drop_last\" not in self._state_dict:\n            raise ValueError(\"The state_dict must contain the drop_last.\")\n\n        if \"seed\" not in self._state_dict:\n            raise ValueError(\"The state_dict must contain the seed.\")\n\n        if \"world_size\" not in self._state_dict:\n            raise ValueError(\"The state_dict must contain the world_size.\")\n\n        if \"shuffle\" not in self._state_dict:\n            raise ValueError(\"The state_dict must contain the shuffle.\")\n\n        if self.input_dir.path != self._state_dict[\"input_dir_path\"]:\n            raise ValueError(\"The input_dir_path in the state_dict does not match the current input_dir.path.\")\n\n        if self.input_dir.url != self._state_dict[\"input_dir_url\"]:"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        assert self._state_dict is not None\n        state = self._state_dict\n\n        if self.shuffle != state[\"shuffle\"]:\n            raise ValueError(\n                f\"The shuffle parameter is not the same between the state dictionary and the current state. \"\n                f\"Current state: {self.shuffle}, State dictionary: {state['shuffle']}\"\n            )\n\n        if self.distributed_env.world_size != state[\"world_size\"]:\n            raise ValueError(\n                f\"The world size is not the same between the state dictionary and the current state. \"\n                f\"Current state: {self.distributed_env.world_size}, State dictionary: {state['world_size']}\"\n            )\n\n        if self.drop_last != state[\"drop_last\"]:\n            raise ValueError(\n                f\"The drop_last parameter is not the same between the state dictionary and the current state. \"\n                f\"Current state: {self.drop_last}, State dictionary: {state['drop_last']}\"\n            )\n\n        if self.seed != state[\"seed\"]:\n            raise ValueError(\n                f\"The seed is not the same between the state dictionary and the current state. \"\n                f\"Current state: {self.seed}, State dictionary: {state['seed']}\"\n            )\n\n        if self.input_dir.path != state[\"input_dir_path\"]:\n            raise ValueError(\n                f\"The input_dir.path is not the same between the state dictionary and the current state. \"\n                f\"Current state: {self.input_dir.path}, State dictionary: {state['input_dir_path']}\"\n            )\n\n        if self.input_dir.url != state[\"input_dir_url\"]:\n            raise ValueError(\n                f\"The input_dir.url is not the same between the state dictionary and the current state. \"\n                f\"Current state: {self.input_dir.url}, State dictionary: {state['input_dir_url']}\"\n            )\n\n        if self.item_loader is None and state[\"item_loader\"] is"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            raise RuntimeError(\"The state dict should be set before calling this method.\")\n\n        if self.worker_env is None:\n            self.worker_env = _WorkerEnv.detect()\n\n        if self.cache is None:\n            self.cache = self._create_cache(worker_env=self.worker_env)\n\n        if self.shuffler is None:\n            self.shuffler = self._create_shuffler(self.cache)\n\n        state = self._state_dict\n\n        if self.shuffle != state[\"shuffle\"]:\n            raise ValueError(\n                f\"The shuffle parameter is different between the state dict and the current state. \"\n                f\"Current: {self.shuffle}, State dict: {state['shuffle']}\"\n            )\n\n        if self.input_dir.path != state[\"input_dir_path\"] or self.input_dir.url != state[\"input_dir_url\"]:\n            raise ValueError(\n                f\"The input directory path is different between the state dict and the current state. \"\n                f\"Current: {self.input_dir.path}, State dict: {state['input_dir_path']}\"\n            )\n\n        if self.seed != state[\"seed\"]:\n            raise ValueError(\n                f\"The seed is different between the state dict and the current state. \"\n                f\"Current: {self.seed}, State dict: {state['seed']}\"\n            )\n\n        if self.distributed_env.world_size != state[\"world_size\"]:\n            raise ValueError(\n                f\"The world size is different between the state dict and the current state. \"\n                f\"Current: {self.distributed_env.world_size}, State dict: {state['world_size']}\"\n            )\n\n        if self.drop_last != state[\"drop_last\"]:\n            raise ValueError(\n                f\"The drop_last parameter is different between the state dict and the current state. \"\n                f\"Current: {self.drop_last}, State dict: {state['"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Generate a unique directory name based on the input directory\n    directory_name = hashlib.sha256(input_dir.encode()).hexdigest()\n\n    # Check if certain environment variables are set\n    if os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\") is not None:\n        cache_dir = os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\")\n    else:\n        cache_dir = _DEFAULT_CACHE_DIR\n\n    # Create the cache directory\n    cache_path = os.path.join(cache_dir, directory_name)\n    try:\n        os.makedirs(cache_path, exist_ok=True)\n    except Exception:\n        return None\n\n    return cache_path"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Generate a unique directory name based on the input directory\n    hashed_dir = hashlib.sha256(input_dir.encode(\"utf-8\")).hexdigest()\n    cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hashed_dir)\n\n    # Check if the cache directory already exists\n    if os.path.exists(cache_dir):\n        return cache_dir\n\n    # Create the cache directory\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n    except Exception as e:\n        logger.warning(f\"Failed to create cache directory: {e}\")\n        return None"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Generate a unique directory name based on the input directory\n    unique_dir_name = hashlib.sha256(input_dir.encode(\"utf-8\")).hexdigest()\n\n    # If certain environment variables are not set, create the cache directory in a default location\n    if not os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\") and not os.getenv(\"DATA_OPTIMIZER_CACHE_DIR_URL\"):\n        cache_dir = os.path.join(_DEFAULT_CACHE_DIR, unique_dir_name)\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n\n    # If environment variables are set, create the cache directory in a specified location\n    if os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"):\n        cache_dir = os.path.join(os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"), unique_dir_name)\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n\n    if os.getenv(\"DATA_OPTIMIZER_CACHE_DIR_URL\"):\n        cache_dir = os.path.join(os.getenv(\"DATA_OPTIMIZER_CACHE_DIR_URL\"), unique_dir_name)\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n\n    return None"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    hash_object = hashlib.sha256(input_dir.encode())\n    hash_string = hash_object.hexdigest()\n\n    cache_dir = os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\")\n    if cache_dir is None:\n        cache_dir = _DEFAULT_CACHE_DIR\n\n    cache_path = os.path.join(cache_dir, hash_string)\n    try:\n        os.makedirs(cache_path, exist_ok=True)\n    except Exception:\n        return None\n\n    return cache_path"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Generate a unique directory name based on the input directory\n    directory_name = hashlib.md5(input_dir.encode()).hexdigest()\n\n    # Check if certain environment variables are set\n    if os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"):\n        cache_dir = os.path.join(os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"), directory_name)\n    elif os.getenv(\"DATA_OPTIMIZER_GLOBAL_RANK\"):\n        cache_dir = os.path.join(os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"), directory_name)\n    else:\n        cache_dir = os.path.join(_DEFAULT_CACHE_DIR, directory_name)\n\n    # Create the cache directory\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n    except Exception:\n        return None"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Create a unique directory name based on the input directory\n    cache_dir_name = hashlib.sha256(input_dir.encode(\"utf-8\")).hexdigest()\n\n    # Check if the cache directory exists\n    cache_dir_path = os.path.join(_DEFAULT_CACHE_DIR, cache_dir_name)\n    if os.path.exists(cache_dir_path):\n        return cache_dir_path\n\n    # Create the cache directory\n    try:\n        os.makedirs(cache_dir_path, exist_ok=True)\n        return cache_dir_path\n    except Exception as e:\n        logger.warn(f\"Failed to create cache directory: {e}\")\n        return None"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Create a unique directory name based on the input directory\n    unique_dir_name = hashlib.sha256(input_dir.encode()).hexdigest()\n\n    # Check if the environment variables are set\n    if os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"):\n        cache_dir = os.path.join(os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"), unique_dir_name)\n    elif os.getenv(\"DATA_OPTIMIZER_GLOBAL_RANK\"):\n        cache_dir = os.path.join(_DEFAULT_CACHE_DIR, unique_dir_name)\n    else:\n        cache_dir = os.path.join(\"/tmp\", unique_dir_name)\n\n    # Create the cache directory if it doesn't exist\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n    except Exception:\n        return None\n\n    return cache_dir"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    hash_dir = hashlib.md5(input_dir.encode()).hexdigest()\n\n    cache_dir = os.environ.get(\"DATA_OPTIMIZER_CACHE_DIR\", _DEFAULT_CACHE_DIR)\n    cache_dir = os.path.join(cache_dir, hash_dir)\n\n    if os.path.exists(cache_dir):\n        return cache_dir\n\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n    except Exception:\n        return None"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Generate a unique directory name based on the input directory\n    hashed_dir_name = hashlib.sha256(input_dir.encode()).hexdigest()\n\n    # Check if the cache directory already exists\n    cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hashed_dir_name)\n    if os.path.exists(cache_dir):\n        return cache_dir\n\n    # If the cache directory does not exist, create it\n    if os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"):\n        cache_dir = os.path.join(os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"), hashed_dir_name)\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n\n    # If the environment variable is not set, create the cache directory in the default location\n    os.makedirs(cache_dir, exist_ok=True)\n    return cache_dir"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    cache_dir = os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\")\n    if cache_dir is None:\n        cache_dir = _DEFAULT_CACHE_DIR\n\n    cache_dir = os.path.join(cache_dir, hashlib.md5(input_dir.encode(\"utf-8\")).hexdigest())\n\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n    except Exception:\n        logger.error(f\"Failed to create cache directory: {cache_dir}\")\n        return None\n\n    return cache_dir"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Create a unique directory name based on the input directory\n    cache_dir_name = hashlib.sha256(input_dir.encode()).hexdigest()\n\n    # Check if the environment variables are set\n    if os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\") and os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\") != \"\":\n        cache_dir_path = os.path.join(os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"), cache_dir_name)\n    else:\n        cache_dir_path = os.path.join(_DEFAULT_CACHE_DIR, cache_dir_name)\n\n    # Create the cache directory if it doesn't exist\n    try:\n        os.makedirs(cache_dir_path, exist_ok=True)\n        return cache_dir_path\n    except Exception as e:\n        logger.warn(f\"Unable to create the cache directory at {cache_dir_path}. Error: {e}\")\n        return None"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Create a unique directory name based on the input directory\n    directory_name = hashlib.sha256(input_dir.encode(\"utf-8\")).hexdigest()\n\n    # Check if the environment variables are set\n    if os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\") is not None:\n        cache_dir = os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\")\n    else:\n        cache_dir = _DEFAULT_CACHE_DIR\n\n    # Create the cache directory\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n        cache_dir = os.path.join(cache_dir, directory_name)\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n    except Exception as e:\n        logger.warning(f\"Failed to create cache directory: {e}\")\n        return None"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Generate a unique directory name by hashing the input directory\n    unique_dir_name = hashlib.sha256(input_dir.encode()).hexdigest()\n\n    # Check if certain environment variables are set\n    if os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"):\n        cache_dir = os.path.join(os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"), unique_dir_name)\n    elif os.getenv(\"DATA_OPTIMIZER_CACHE_DIR_LOCAL\"):\n        cache_dir = os.path.join(os.getenv(\"DATA_OPTIMIZER_CACHE_DIR_LOCAL\"), unique_dir_name)\n    else:\n        cache_dir = os.path.join(_DEFAULT_CACHE_DIR, unique_dir_name)\n\n    # Create the cache directory\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n    except Exception as e:\n        logger.error(f\"Failed to create cache directory: {cache_dir}\")\n        logger.error(e)\n        return None"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Compute the hash of the input directory\n    input_dir_hash = hashlib.sha256(input_dir.encode(\"utf-8\")).hexdigest()\n\n    # Create the cache directory name\n    cache_dir_name = f\"{input_dir_hash}\"\n\n    # Check if the cache directory environment variable is set\n    cache_dir_env = os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\")\n\n    # If the cache directory environment variable is set, use it\n    if cache_dir_env:\n        cache_dir_path = os.path.join(cache_dir_env, cache_dir_name)\n    # Otherwise, use the default cache directory\n    else:\n        cache_dir_path = os.path.join(_DEFAULT_CACHE_DIR, cache_dir_name)\n\n    # Create the cache directory if it doesn't exist\n    if not os.path.exists(cache_dir_path):\n        os.makedirs(cache_dir_path, exist_ok=True)\n\n    return cache_dir_path"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Create a unique directory name using the input directory\n    directory_name = hashlib.sha256(input_dir.encode()).hexdigest()\n\n    # Check if the environment variables are set\n    if os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\") is not None:\n        cache_dir = os.path.join(os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"), directory_name)\n    elif os.getenv(\"DATA_OPTIMIZER_CACHE_DIR_S3\") is not None:\n        cache_dir = os.path.join(os.getenv(\"DATA_OPTIMIZER_CACHE_DIR_S3\"), directory_name)\n    else:\n        # Default cache directory\n        cache_dir = os.path.join(_DEFAULT_CACHE_DIR, directory_name)\n\n    # Create the cache directory if it doesn't exist\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n    except Exception as e:\n        logger.error(f\"Failed to create cache directory: {e}\")\n        return None"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    # If the input directory is not provided, use an empty string for hashing.\n    if input_dir is None:\n        input_dir = \"\"\n\n    # Generate a unique directory name by hashing the input directory.\n    dir_name = hashlib.md5(input_dir.encode(\"utf-8\")).hexdigest()\n\n    # If certain environment variables are not set, create the cache directory in a default location.\n    if not os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\") and not os.getenv(\"DATA_OPTIMIZER_LOCAL_CACHE_DIR\"):\n        cache_dir = os.path.join(_DEFAULT_CACHE_DIR, dir_name)\n\n    # If the environment variable DATA_OPTIMIZER_CACHE_DIR is set, create the cache directory in the specified location.\n    elif os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"):\n        cache_dir = os.path.join(os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"), dir_name)\n\n    # If the environment variable DATA_OPTIMIZER_LOCAL_CACHE_DIR is set, create the cache directory in the specified location.\n    elif os.getenv(\"DATA_OPTIMIZER_LOCAL_CACHE_DIR\"):\n        cache_dir = os.path.join(os.getenv(\"DATA_OPTIMIZER_LOCAL_CACHE_DIR\"), dir_name)\n\n    # Create the cache directory if it doesn't exist.\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    # Return the path of the created cache directory.\n    return cache_dir"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    # Generate a unique directory name based on the input directory\n    dir_hash = hashlib.sha256(input_dir.encode(\"utf-8\")).hexdigest()\n    cache_dir_name = f\"{dir_hash}\"\n\n    # Check if the cache directory already exists\n    if os.path.exists(cache_dir_name):\n        return cache_dir_name\n\n    # If not, create the cache directory in the specified location\n    if os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"):\n        cache_dir_path = os.path.join(os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"), cache_dir_name)\n    else:\n        cache_dir_path = os.path.join(_DEFAULT_CACHE_DIR, cache_dir_name)\n\n    try:\n        os.makedirs(cache_dir_path, exist_ok=True)\n        return cache_dir_path\n    except Exception:\n        return None"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    # if the input directory is not provided, use an empty string for hashing\n    if input_dir is None:\n        input_dir = \"\"\n\n    # generate a unique directory name based on the input directory\n    directory_name = hashlib.sha256(input_dir.encode(\"utf-8\")).hexdigest()\n\n    # check if certain environment variables are set\n    if os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\") is not None:\n        # if the environment variable is set, use it as the base directory\n        base_directory = os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\")\n    else:\n        # if the environment variable is not set, use the default location\n        base_directory = _DEFAULT_CACHE_DIR\n\n    # create the cache directory\n    try:\n        os.makedirs(os.path.join(base_directory, directory_name), exist_ok=True)\n        return os.path.join(base_directory, directory_name)\n    except Exception:\n        return None"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    # if the input directory is None, set it to an empty string\n    if input_dir is None:\n        input_dir = \"\"\n\n    # hash the input directory to create a unique directory name\n    hashed_input_dir = hashlib.sha256(input_dir.encode()).hexdigest()\n\n    # get the cache directory path from the environment variable\n    cache_dir = os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\")\n\n    # if the cache directory path is not set, use the default location\n    if cache_dir is None:\n        cache_dir = _DEFAULT_CACHE_DIR\n\n    # create the cache directory path\n    cache_dir_path = os.path.join(cache_dir, hashed_input_dir)\n\n    # create the cache directory if it doesn't exist\n    try:\n        os.makedirs(cache_dir_path, exist_ok=True)\n    except Exception as e:\n        print(f\"Error creating cache directory: {e}\")\n        return None\n\n    return cache_dir_path"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    # Create a unique directory name by hashing the input directory\n    input_dir = input_dir or \"\"\n    unique_dir_name = hashlib.sha256(input_dir.encode(\"utf-8\")).hexdigest()\n\n    # Check if certain environment variables are set\n    cache_dir_env = os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\")\n    s3_connection_env = os.getenv(\"DATA_OPTIMIZER_S3_CONNECTION\")\n    s3_bucket_env = os.getenv(\"DATA_OPTIMIZER_S3_BUCKET\")\n\n    # If not set, use the default location for cache directory\n    if not cache_dir_env and not s3_connection_env and not s3_bucket_env:\n        cache_dir = os.path.join(_DEFAULT_CACHE_DIR, unique_dir_name)\n\n    # If set, use the specified location for cache directory\n    elif cache_dir_env:\n        cache_dir = os.path.join(cache_dir_env, unique_dir_name)\n\n    # If set, use the specified location for cache directory\n    elif s3_connection_env and s3_bucket_env:\n        cache_dir = f\"s3://{s3_bucket_env}/{unique_dir_name}\"\n\n    # If none of the above conditions are met, raise an error\n    else:\n        raise ValueError(\"Either DATA_OPTIMIZER_CACHE_DIR or (DATA_OPTIMIZER_S3_CONNECTION and DATA_OPTIMIZER_S3_BUCKET) must be set.\")\n\n    # Create the cache directory if it doesn't exist\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n    except Exception as e:\n        logger.warning(f\"Unable to create cache directory: {e}\")\n        return None"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        parsed_remote_filepath = parse.urlparse(remote_filepath)\n        if parsed_remote_filepath.scheme != \"s3\":\n            raise ValueError(f\"The provided remote_path {remote_filepath} is not an S3 URL.\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        lock_filepath = f\"{local_filepath}.lock\"\n        with FileLock(lock_filepath, timeout=10):\n            if self._s5cmd_available:\n                subprocess.run(\n                    f\"s5cmd --no-sign-request cp {remote_filepath} {local_filepath}\",\n                    shell=True,\n                    check=True,\n                )\n            else:\n                bucket = parsed_remote_filepath.netloc\n                key = parsed_remote_filepath.path[1:]\n                self._client.download_file(bucket, key, local_filepath)"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        parsed_remote_filepath = parse.urlparse(remote_filepath)\n        if parsed_remote_filepath.scheme != \"s3\":\n            raise ValueError(f\"The provided remote_path is not an S3 URL: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        os.makedirs(os.path.dirname(local_filepath), exist_ok=True)\n\n        lock_path = local_filepath + \".lock\"\n        with FileLock(lock_path, timeout=10):\n            if self._s5cmd_available:\n                subprocess.run(\n                    f\"s5cmd --no-sign-request cp {remote_filepath} {local_filepath}\",\n                    shell=True,\n                    check=True,\n                )\n            else:\n                self._client.download_file(parsed_remote_filepath.netloc, parsed_remote_filepath.path[1:], local_filepath)"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        parsed_url = parse.urlparse(remote_filepath)\n        if parsed_url.scheme != \"s3\":\n            raise ValueError(f\"The provided remote_path is not an S3 URL: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        if self._s5cmd_available:\n            subprocess.run(\n                [\n                    \"s5cmd\",\n                    \"--no-sign-request\",\n                    \"cp\",\n                    remote_filepath,\n                    local_filepath,\n                ],\n                check=True,\n            )\n        else:\n            bucket = parsed_url.netloc\n            key = parsed_url.path.lstrip(\"/\")\n            self._client.download_file(bucket, key, local_filepath)\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        parsed_remote_filepath = parse.urlparse(remote_filepath)\n        if parsed_remote_filepath.scheme != \"s3\":\n            raise ValueError(f\"The provided remote_path doesn't use the 's3' scheme: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        with FileLock(f\"{local_filepath}.lock\", timeout=10):\n            if self._s5cmd_available:\n                cmd = f\"s5cmd --no-sign-request cp {remote_filepath} {local_filepath}\"\n                subprocess.run(cmd, shell=True, check=True)\n            else:\n                bucket, key = parsed_remote_filepath.netloc, parsed_remote_filepath.path.lstrip(\"/\")\n                self._client.download_file(bucket, key, local_filepath)"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        parsed_url = parse.urlparse(remote_filepath)\n        if parsed_url.scheme != \"s3\":\n            raise ValueError(f\"The provided remote_path doesn't use the 's3' scheme: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        lock_path = f\"{local_filepath}.lock\"\n        with FileLock(lock_path, timeout=120):\n            if self._s5cmd_available:\n                self._download_file_with_s5cmd(remote_filepath, local_filepath)\n            else:\n                self._download_file_with_boto3(parsed_url.netloc, parsed_url.path, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        parsed_remote_filepath = parse.urlparse(remote_filepath)\n        if parsed_remote_filepath.scheme != \"s3\":\n            raise ValueError(f\"The provided remote_path doesn't use the `s3` scheme: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        lock_path = f\"{local_filepath}.lock\"\n        with FileLock(lock_path, timeout=120):\n            if self._s5cmd_available:\n                cmd = f\"s5cmd --no-sign-request cp {remote_filepath} {local_filepath}\"\n                subprocess.run(cmd, shell=True, check=True)\n            else:\n                bucket, key = parsed_remote_filepath.netloc, parsed_remote_filepath.path.lstrip(\"/\")\n                self._client.download_file(bucket, key, local_filepath)"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        parsed_remote_filepath = parse.urlparse(remote_filepath)\n        if parsed_remote_filepath.scheme != \"s3\":\n            raise ValueError(\n                f\"The provided remote_path {remote_filepath} is not an S3 URL. It should start with 's3://'.\"\n            )\n\n        if os.path.exists(local_filepath):\n            return\n\n        lock_path = local_filepath + \".lock\"\n        with FileLock(lock_path, timeout=60 * 10):\n            if self._s5cmd_available:\n                cmd = f\"s5cmd --no-sign-request cp {remote_filepath} {local_filepath}\"\n                subprocess.run(cmd, shell=True, check=True)\n            else:\n                bucket_name = parsed_remote_filepath.netloc\n                key = parsed_remote_filepath.path.lstrip(\"/\")\n                self._client.download_file(bucket_name, key, local_filepath)"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        parsed_url = parse.urlparse(remote_filepath)\n        if parsed_url.scheme != \"s3\":\n            raise ValueError(f\"The provided remote_path {remote_filepath} is not an S3 URL\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        if self._s5cmd_available:\n            subprocess.run(\n                f\"s5cmd --no-sign-request cp {remote_filepath} {local_filepath}\".split(),\n                check=True,\n            )\n        else:\n            self._client.download_file(remote_filepath, local_filepath)\n\n        with FileLock(f\"{local_filepath}.lock\", timeout=10):\n            pass\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        parsed_remote_filepath = parse.urlparse(remote_filepath)\n        if parsed_remote_filepath.scheme != \"s3\":\n            raise ValueError(f\"The provided remote_path does not use the 's3' scheme: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        os.makedirs(os.path.dirname(local_filepath), exist_ok=True)\n\n        with FileLock(local_filepath + \".lock\", timeout=60):\n            if self._s5cmd_available:\n                subprocess.check_call(\n                    [\n                        \"s5cmd\",\n                        \"--no-sign-request\",\n                        \"cp\",\n                        remote_filepath,\n                        local_filepath,\n                    ]\n                )\n            else:\n                self._client.download_file(\n                    parsed_remote_filepath.netloc,\n                    parsed_remote_filepath.path.lstrip(\"/\"),\n                    local_filepath,\n                )"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        parsed_url = parse.urlparse(remote_filepath)\n        if parsed_url.scheme != \"s3\":\n            raise ValueError(f\"The provided remote_path is not an S3 URL: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        lock_file = os.path.join(self._cache_dir, f\".{local_filepath}.lock\")\n        try:\n            with FileLock(lock_file, timeout=10):\n                if self._s5cmd_available:\n                    subprocess.check_call([\"s5cmd\", \"cp\", remote_filepath, local_filepath])\n                else:\n                    self._client.download_file(parsed_url.netloc, parsed_url.path, local_filepath)\n        except Timeout:\n            print(f\"Could not acquire lock for {local_filepath}\")"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        parsed_remote_filepath = parse.urlparse(remote_filepath)\n        if parsed_remote_filepath.scheme != \"s3\":\n            raise ValueError(\n                f\"The provided remote_path {remote_filepath} is not an S3 URL. It should start with s3://\"\n            )\n        if os.path.exists(local_filepath):\n            return\n\n        lock_path = os.path.join(self._cache_dir, f\"{parsed_remote_filepath.path}.lock\")\n        with FileLock(lock_path, timeout=10):\n            if self._s5cmd_available:\n                cmd = f\"s5cmd --no-sign-request cp {remote_filepath} {local_filepath}\"\n                subprocess.run(cmd, shell=True, check=True)\n            else:\n                bucket = parsed_remote_filepath.netloc\n                key = parsed_remote_filepath.path.lstrip(\"/\")\n                self._client.download_file(bucket, key, local_filepath)"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        parsed_remote_filepath = parse.urlparse(remote_filepath)\n        if parsed_remote_filepath.scheme != \"s3\":\n            raise ValueError(f\"The provided remote_path is not an S3 URL: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        if self._s5cmd_available:\n            self._download_file_with_s5cmd(remote_filepath, local_filepath)\n        else:\n            self._download_file_with_boto3(parsed_remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        parsed_remote_filepath = parse.urlparse(remote_filepath)\n        if parsed_remote_filepath.scheme != \"s3\":\n            raise ValueError(f\"The provided remote_path is not an S3 URL: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        if self._s5cmd_available:\n            self._download_file_with_s5cmd(remote_filepath, local_filepath)\n        else:\n            self._download_file_with_boto3(parsed_remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        parsed_remote_filepath = parse.urlparse(remote_filepath)\n        if parsed_remote_filepath.scheme != \"s3\":\n            raise ValueError(f\"The provided remote_path doesn't use the 's3' scheme: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        lock_filepath = local_filepath + \".lock\"\n        try:\n            with FileLock(lock_filepath, timeout=10):\n                if not os.path.exists(local_filepath):\n                    if self._s5cmd_available:\n                        subprocess.run(\n                            [\"s5cmd\", \"--no-sign-request\", \"cp\", remote_filepath, local_filepath],\n                            check=True,\n                        )\n                    else:\n                        self._client.download_file(\n                            parsed_remote_filepath.netloc,\n                            parsed_remote_filepath.path[1:],\n                            local_filepath,\n                        )\n        except Timeout:\n            raise Timeout(f\"The file lock could not be acquired within the specified timeout: {lock_filepath}\")\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        parsed_remote_filepath = parse.urlparse(remote_filepath)\n        if parsed_remote_filepath.scheme != \"s3\":\n            raise ValueError(\n                f\"The provided remote_path {remote_filepath} is not an S3 URL. \"\n                \"Please provide a valid S3 URL.\"\n            )\n\n        if os.path.exists(local_filepath):\n            return\n\n        if self._s5cmd_available:\n            self._download_file_with_s5cmd(remote_filepath, local_filepath)\n        else:\n            self._download_file_with_boto3(parsed_remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        parsed_remote_filepath = parse.urlparse(remote_filepath)\n        if parsed_remote_filepath.scheme != \"s3\":\n            raise ValueError(f\"The provided remote_path doesn't use the s3 scheme: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        os.makedirs(os.path.dirname(local_filepath), exist_ok=True)\n        lock_filepath = local_filepath + \".lock\"\n        lock = FileLock(lock_filepath, timeout=60)\n        try:\n            with lock.acquire(timeout=60):\n                if os.path.exists(local_filepath):\n                    return\n                if self._s5cmd_available:\n                    self._download_file_s5cmd(remote_filepath, local_filepath)\n                else:\n                    self._download_file_boto3(remote_filepath, local_filepath)\n        except Timeout:\n            raise Timeout(f\"Could not acquire lock for {local_filepath}\")\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        parsed_remote_filepath = parse.urlparse(remote_filepath)\n        if parsed_remote_filepath.scheme != \"s3\":\n            raise ValueError(f\"The provided remote_path does not use the 's3' scheme: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        os.makedirs(os.path.dirname(local_filepath), exist_ok=True)\n\n        lock_file = local_filepath + \".lock\"\n        with FileLock(lock_file, timeout=10) as lock:\n            try:\n                lock.acquire()\n                if self._s5cmd_available:\n                    cmd = f\"s5cmd --no-sign-request cp {remote_filepath} {local_filepath}\"\n                    subprocess.run(cmd, shell=True, check=True)\n                else:\n                    self._client.download_file(\n                        parsed_remote_filepath.netloc,\n                        parsed_remote_filepath.path.lstrip(\"/\"),\n                        local_filepath,\n                    )\n            except Timeout:\n                raise Timeout(\n                    f\"Failed to acquire the lock for the file {local_filepath} within the specified timeout.\"\n                )\n            except Exception as e:\n                if os.path.exists(local_filepath):\n                    os.remove(local_filepath)\n                raise e\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not str(remote_filepath).startswith(\"s3://\"):\n            raise ValueError(\n                f\"The provided `remote_filepath` {remote_filepath} is not an S3 URL. Please use an S3 URL.\"\n            )\n\n        if os.path.exists(local_filepath):\n            return\n\n        if self._s5cmd_available:\n            self._download_file_with_s5cmd(remote_filepath, local_filepath)\n        else:\n            self._download_file_with_boto3(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        parsed_remote_filepath = parse.urlparse(remote_filepath)\n        if parsed_remote_filepath.scheme != \"s3\":\n            raise ValueError(f\"The provided remote_path is not an S3 URL: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        if self._s5cmd_available:\n            subprocess.run(\n                f\"s5cmd cp {remote_filepath} {local_filepath}\",\n                shell=True,\n                check=True,\n            )\n        else:\n            lock_file = os.path.join(self._cache_dir, f\"{os.path.basename(local_filepath)}.lock\")\n            try:\n                with FileLock(lock_file, timeout=120):\n                    self._client.download_file(\n                        bucket=parsed_remote_filepath.netloc,\n                        key=parsed_remote_filepath.path[1:],\n                        local_filepath=local_filepath,\n                    )\n            except Timeout:\n                raise Timeout(\n                    f\"Could not acquire the lock for the file {local_filepath} within the timeout. \"\n                    f\"Please delete the lock file {lock_file} and try again.\"\n                )\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not str(remote_filepath).startswith(\"s3://\"):\n            raise ValueError(f\"The provided remote_path doesn't start with s3://: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        os.makedirs(os.path.dirname(local_filepath), exist_ok=True)\n\n        if self._s5cmd_available:\n            self._download_file_with_s5cmd(remote_filepath, local_filepath)\n        else:\n            self._download_file_with_boto3(remote_filepath, local_filepath)\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {}\n    workers_intervals = {}\n\n    for worker_idx in range(num_workers):\n        workers_chunks[worker_idx] = []\n        workers_intervals[worker_idx] = []\n\n    if worker_env.world_size == 1:\n        for chunk_index, chunk_interval in zip(chunks_replica, intervals_replica):\n            workers_chunks[worker_idx].append(chunk_index)\n            workers_intervals[worker_idx].append(chunk_interval)\n    else:\n        for chunk_index, chunk_interval in zip(chunks_replica, intervals_replica):\n            worker_idx = chunk_index % worker_env.world_size\n            workers_chunks[worker_idx].append(chunk_index)\n            workers_intervals[worker_idx].append(chunk_interval)\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Initialize dictionaries to store the assigned chunks and intervals for each worker\n    workers_chunks = {i: [] for i in range(num_workers)}\n    workers_intervals = {i: [] for i in range(num_workers)}\n\n    # Distribute chunks and intervals among workers based on the worker's index and the total world size\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_idx = i % worker_env.world_size\n        workers_chunks[worker_idx].append(chunk_index)\n        workers_intervals[worker_idx].append(chunk_interval)\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Create dictionaries to store the chunks and intervals assigned to each worker\n    workers_chunks = {worker_idx: [] for worker_idx in range(num_workers)}\n    workers_intervals = {worker_idx: [] for worker_idx in range(num_workers)}\n\n    # Distribute the chunks and intervals based on the worker's index and the total world size\n    for chunk_idx, chunk in enumerate(chunks_replica):\n        worker_idx = chunk_idx % num_workers\n        workers_chunks[worker_idx].append(chunk)\n        workers_intervals[worker_idx].append(intervals_replica[chunk_idx])\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    chunks_per_worker = len(chunks_replica) // num_workers\n    remainder = len(chunks_replica) % num_workers\n\n    workers_chunks = {}\n    workers_intervals = {}\n\n    for worker_idx in range(num_workers):\n        start_idx = worker_idx * chunks_per_worker\n        end_idx = (worker_idx + 1) * chunks_per_worker\n        if worker_idx < remainder:\n            start_idx += worker_idx\n            end_idx += worker_idx + 1\n        else:\n            start_idx += remainder\n            end_idx += remainder\n\n        workers_chunks[worker_idx] = chunks_replica[start_idx:end_idx]\n        workers_intervals[worker_idx] = intervals_replica[start_idx:end_idx]\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Initialize dictionaries to store the assigned chunks and intervals for each worker\n    workers_chunks = {}\n    workers_intervals = {}\n\n    # Iterate over each worker index\n    for worker_idx in range(num_workers):\n        # Initialize lists to store the assigned chunks and intervals for the current worker\n        worker_chunks = []\n        worker_intervals = []\n\n        # Iterate over each chunk and its corresponding interval\n        for chunk_index, interval in zip(chunks_replica, intervals_replica):\n            # Check if the worker index is divisible by the total world size\n            if worker_idx % worker_env.world_size == 0:\n                # If the worker index is divisible, assign the chunk and its interval to the current worker\n                worker_chunks.append(chunk_index)\n                worker_intervals.append(interval)\n\n        # Assign the assigned chunks and intervals for the current worker to the dictionaries\n        workers_chunks[worker_idx] = worker_chunks\n        workers_intervals[worker_idx] = worker_intervals\n\n    # Return the dictionaries containing the assigned chunks and intervals for each worker\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    if worker_env.world_size == 1:\n        return {0: chunks_replica}, {0: intervals_replica}\n\n    if worker_env.world_size == num_workers:\n        return {worker_idx: [chunks_replica[worker_idx]] for worker_idx in range(num_workers)}, {\n            worker_idx: [intervals_replica[worker_idx]] for worker_idx in range(num_workers)\n        }\n\n    workers_chunks = {worker_idx: [] for worker_idx in range(num_workers)}\n    workers_intervals = {worker_idx: [] for worker_idx in range(num_workers)}\n\n    for chunk_idx, chunk in enumerate(chunks_replica):\n        worker_idx = chunk_idx % num_workers\n        workers_chunks[worker_idx].append(chunk)\n        workers_intervals[worker_idx].append(intervals_replica[chunk_idx])\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Initialize dictionaries to store the chunks and intervals assigned to each worker\n    workers_chunks = {}\n    workers_intervals = {}\n\n    # Distribute chunks and intervals among workers based on the worker environment\n    for worker_idx in range(num_workers):\n        worker_chunks = []\n        worker_intervals = []\n\n        for chunk_idx, interval in zip(chunks_replica, intervals_replica):\n            if chunk_idx % worker_env.world_size == worker_env.rank:\n                worker_chunks.append(chunk_idx)\n                worker_intervals.append(interval)\n\n        workers_chunks[worker_idx] = worker_chunks\n        workers_intervals[worker_idx] = worker_intervals\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    world_size = worker_env.world_size\n    rank = worker_env.rank\n\n    # Create empty lists to store the assigned chunks and intervals for each worker\n    workers_chunks = {i: [] for i in range(num_workers)}\n    workers_intervals = {i: [] for i in range(num_workers)}\n\n    # Distribute chunks and intervals among workers\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_idx = i % num_workers\n        workers_chunks[worker_idx].append(chunk_index)\n        workers_intervals[worker_idx].append(chunk_interval)\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {}\n    workers_intervals = {}\n\n    for worker_idx in range(num_workers):\n        workers_chunks[worker_idx] = []\n        workers_intervals[worker_idx] = []\n\n    if num_workers == 1:\n        workers_chunks[0] = chunks_replica\n        workers_intervals[0] = intervals_replica\n    else:\n        if worker_env.world_size == 1:\n            for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n                workers_chunks[i % num_workers].append(chunk_index)\n                workers_intervals[i % num_workers].append(chunk_interval)\n        else:\n            for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n                worker_idx = i % num_workers\n                workers_chunks[worker_idx].append(chunk_index)\n                workers_intervals[worker_idx].append(chunk_interval)\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Create two dictionaries to store the assigned chunks and intervals for each worker\n    workers_chunks = {worker_idx: [] for worker_idx in range(num_workers)}\n    workers_intervals = {worker_idx: [] for worker_idx in range(num_workers)}\n\n    # Determine the distribution strategy based on the worker's index and the total world size\n    if worker_env.world_size > 1:\n        # If there are multiple workers, distribute the chunks and intervals in a round-robin manner\n        for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n            worker_idx = i % num_workers\n            workers_chunks[worker_idx].append(chunk_index)\n            workers_intervals[worker_idx].append(chunk_interval)\n    else:\n        # If there is only one worker, assign all chunks and intervals to it\n        for chunk_index, chunk_interval in zip(chunks_replica, intervals_replica):\n            workers_chunks[0].append(chunk_index)\n            workers_intervals[0].append(chunk_interval)\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Initialize dictionaries to store the assigned chunks and intervals for each worker\n    workers_chunks = {}\n    workers_intervals = {}\n\n    # Distribute chunks and intervals among workers based on the distribution strategy\n    if num_workers > 1:\n        for worker_idx in range(num_workers):\n            workers_chunks[worker_idx] = []\n            workers_intervals[worker_idx] = []\n\n            for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n                if i % worker_env.world_size == worker_env.rank:\n                    workers_chunks[worker_idx].append(chunk_index)\n                    workers_intervals[worker_idx].append(chunk_interval)\n    else:\n        workers_chunks[0] = chunks_replica\n        workers_intervals[0] = intervals_replica\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {}\n    workers_intervals = {}\n\n    for worker_idx in range(num_workers):\n        workers_chunks[worker_idx] = []\n        workers_intervals[worker_idx] = []\n\n    # Distribution strategy\n    if worker_env.world_size % num_workers != 0:\n        raise ValueError(\n            f\"The number of workers ({num_workers}) must be a divisor of the world size ({worker_env.world_size})\"\n        )\n\n    # Distribute chunks and intervals\n    for chunk_idx, (chunk, interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_idx = chunk_idx % num_workers\n        workers_chunks[worker_idx].append(chunk)\n        workers_intervals[worker_idx].append(interval)\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # This function distributes chunks of data and their corresponding intervals across multiple workers based on the number of workers and a worker environment. It ensures that each worker is assigned a subset of chunks and intervals, following a distribution strategy that depends on the worker's index and the total world size defined in the worker environment.\n    #\n    # Input-Output Arguments\n    # :param num_workers: int, the total number of workers among which the chunks and intervals are to be distributed.\n    # :param worker_env: _WorkerEnv, an instance or object representing the worker environment, which includes details like world size that are used in the distribution logic.\n    # :param chunks_replica: List[int], a list of chunk indices that need to be distributed among the workers.\n    # :param intervals_replica: List[Any], a list of intervals corresponding to each chunk in chunks_replica. Each interval represents the range or scope of the chunk it corresponds to.\n    # :return: A tuple containing two dictionaries. The first dictionary maps worker indices to their assigned chunks, and the second dictionary maps worker indices to the intervals corresponding to their assigned chunks. There is no explicit data type mentioned for the return value, but it is implied to be a tuple of two dictionaries based on the function's implementation.\n\n    # Initialize dictionaries to store the assigned chunks and intervals for each worker\n    workers_chunks = {worker_idx: [] for worker_idx in range(num_workers)}\n    workers_intervals = {worker_idx: [] for worker_idx in range(num_workers)}\n\n    # Iterate over the chunks and intervals\n    for chunk_idx, (chunk, interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        # Determine the worker index based on the chunk index and the total number of workers\n        worker_idx = chunk_idx % num_workers\n\n        # Assign the chunk and interval to the corresponding worker\n        workers_chunks[worker_idx].append(chunk)\n        workers_intervals[worker_idx].append(interval)\n\n    # Return the dictionaries of assigned chunks and intervals for each worker\n    return"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # TODO: Implement elastic sampling where the number of workers, ranks can change.\n    if num_workers == 1:\n        return {0: chunks_replica}, {0: intervals_replica}\n\n    # TODO: Implement elastic sampling where the number of workers, ranks can change.\n    if worker_env.world_size == 1:\n        return {0: chunks_replica}, {0: intervals_replica}\n\n    # TODO: Implement elastic sampling where the number of workers, ranks can change.\n    if worker_env.world_size != num_workers:\n        raise ValueError(\n            \"The number of workers must be equal to the world size. \"\n            f\"Found {num_workers} workers and {worker_env.world_size} world size.\"\n        )\n\n    chunks_per_worker = len(chunks_replica) // num_workers\n    workers_chunks = {worker_idx: [] for worker_idx in range(num_workers)}\n    workers_intervals = {worker_idx: [] for worker_idx in range(num_workers)}\n\n    for worker_idx in range(num_workers):\n        start_index = worker_idx * chunks_per_worker\n        end_index = (worker_idx + 1) * chunks_per_worker\n\n        if worker_idx == num_workers - 1:\n            end_index = len(chunks_replica)\n\n        workers_chunks[worker_idx] = chunks_replica[start_index:end_index]\n        workers_intervals[worker_idx] = intervals_replica[start_index:end_index]\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # initialize the dictionaries to store the assigned chunks and intervals for each worker\n    workers_chunks = {worker_idx: [] for worker_idx in range(num_workers)}\n    workers_intervals = {worker_idx: [] for worker_idx in range(num_workers)}\n\n    # determine the distribution strategy based on the worker's index and the total world size\n    if worker_env.rank == 0:\n        # if the worker is the first one, assign all chunks and intervals to it\n        for chunk_idx, interval in zip(chunks_replica, intervals_replica):\n            workers_chunks[0].append(chunk_idx)\n            workers_intervals[0].append(interval)\n    elif worker_env.rank == 1:\n        # if the worker is the second one, assign the last chunk and interval to it\n        workers_chunks[1].append(chunks_replica[-1])\n        workers_intervals[1].append(intervals_replica[-1])\n    else:\n        # for all other workers, assign chunks and intervals based on the worker's index and the total world size\n        for chunk_idx, interval in zip(chunks_replica, intervals_replica):\n            worker_idx = chunk_idx % num_workers\n            workers_chunks[worker_idx].append(chunk_idx)\n            workers_intervals[worker_idx].append(interval)\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Create two empty dictionaries to store the worker-chunk and worker-interval mappings\n    workers_chunks = {}\n    workers_intervals = {}\n\n    # Iterate over the number of workers\n    for worker_idx in range(num_workers):\n        # Initialize empty lists for the chunks and intervals of the current worker\n        chunks = []\n        intervals = []\n\n        # Iterate over the chunks and intervals\n        for chunk_index, interval in zip(chunks_replica, intervals_replica):\n            # Check if the current worker index is divisible by the number of workers\n            if worker_idx % worker_env.world_size == 0:\n                # If so, add the chunk and interval to the worker's lists\n                chunks.append(chunk_index)\n                intervals.append(interval)\n\n        # Add the worker's chunks and intervals to the dictionaries\n        workers_chunks[worker_idx] = chunks\n        workers_intervals[worker_idx] = intervals\n\n    # Return the dictionaries containing the worker-chunk and worker-interval mappings\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Initialize dictionaries to store the assigned chunks and intervals for each worker\n    workers_chunks = {}\n    workers_intervals = {}\n\n    # Iterate over each worker index\n    for worker_idx in range(num_workers):\n        # Calculate the worker's rank based on the worker index and the world size\n        worker_rank = worker_idx % worker_env.world_size\n\n        # Initialize lists to store the assigned chunks and intervals for the current worker\n        worker_chunks = []\n        worker_intervals = []\n\n        # Iterate over each chunk and interval in the chunks_replica and intervals_replica lists\n        for chunk_index, interval in zip(chunks_replica, intervals_replica):\n            # Check if the chunk index is divisible by the number of workers\n            if chunk_index % num_workers == 0:\n                # If the chunk index is divisible by the number of workers, check if the worker rank is less than the number of workers\n                if worker_rank < num_workers:\n                    # If the worker rank is less than the number of workers, assign the chunk and interval to the current worker\n                    worker_chunks.append(chunk_index)\n                    worker_intervals.append(interval)\n            else:\n                # If the chunk index is not divisible by the number of workers, check if the worker rank is equal to the remainder of the chunk index divided by the number of workers\n                if worker_rank == chunk_index % num_workers:\n                    # If the worker rank is equal to the remainder, assign the chunk and interval to the current worker\n                    worker_chunks.append(chunk_index)\n                    worker_intervals.append(interval)\n\n        # Assign the assigned chunks and intervals to the dictionaries for the current worker\n        workers_chunks[worker_idx] = worker_chunks\n        workers_intervals[worker_idx] = worker_intervals\n\n    # Return the dictionaries containing the assigned chunks and intervals for each worker\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Initialize dictionaries to store the worker-chunk and worker-interval mappings\n    workers_chunks = {worker_idx: [] for worker_idx in range(num_workers)}\n    workers_intervals = {worker_idx: [] for worker_idx in range(num_workers)}\n\n    # Calculate the number of chunks per worker based on the number of workers and the total number of chunks\n    chunks_per_worker = len(chunks_replica) // num_workers\n\n    # Iterate through each worker and assign chunks and intervals to each worker\n    for worker_idx in range(num_workers):\n        # Calculate the start and end indices for the chunks assigned to the current worker\n        start_idx = worker_idx * chunks_per_worker\n        end_idx = start_idx + chunks_per_worker\n\n        # Check if the current worker is the last worker and has extra chunks\n        if worker_idx == num_workers - 1 and len(chunks_replica) % num_workers != 0:\n            end_idx = len(chunks_replica)\n\n        # Assign the chunks and intervals to the current worker\n        workers_chunks[worker_idx] = chunks_replica[start_idx:end_idx]\n        workers_intervals[worker_idx] = intervals_replica[start_idx:end_idx]\n\n    # Return the worker-chunk and worker-interval mappings as dictionaries\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # if the number of workers is equal to the world size, we can directly assign each worker a chunk and its corresponding interval\n    if num_workers == worker_env.world_size:\n        return {worker_idx: [chunks_replica[worker_idx]] for worker_idx in range(num_workers)}, {\n            worker_idx: [intervals_replica[worker_idx]] for worker_idx in range(num_workers)\n        }\n\n    # if the number of workers is greater than the world size, we need to distribute the chunks and intervals in a round-robin fashion\n    if num_workers > worker_env.world_size:\n        # create a dictionary to store the assigned chunks and intervals for each worker\n        workers_chunks = {worker_idx: [] for worker_idx in range(num_workers)}\n        workers_intervals = {worker_idx: [] for worker_idx in range(num_workers)}\n\n        # iterate through the chunks and intervals, assigning them to workers in a round-robin fashion\n        for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n            worker_idx = i % num_workers\n            workers_chunks[worker_idx].append(chunk_index)\n            workers_intervals[worker_idx].append(chunk_interval)\n\n        return workers_chunks, workers_intervals\n\n    # if the number of workers is less than the world size, we need to distribute the chunks and intervals in a round-robin fashion\n    # but only assign chunks and intervals to the first num_workers workers\n    workers_chunks = {worker_idx: [] for worker_idx in range(num_workers)}\n    workers_intervals = {worker_idx: [] for worker_idx in range(num_workers)}\n\n    # iterate through the chunks and intervals, assigning them to workers in a round-robin fashion\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    assert len(chunks_replica) == len(intervals_replica)\n    assert len(chunks_replica) > 0\n\n    if num_workers == 1:\n        return {0: chunks_replica}, {0: intervals_replica}\n\n    # TODO: Implement elastic sampling where the number of workers, ranks can change.\n    if num_workers == worker_env.world_size:\n        # Simple case where the number of workers is equal to the world size.\n        # Each worker is assigned a chunk and its corresponding interval.\n        workers_chunks = {worker_idx: [chunks_replica[worker_idx]] for worker_idx in range(num_workers)}\n        workers_intervals = {worker_idx: [intervals_replica[worker_idx]] for worker_idx in range(num_workers)}\n    else:\n        # TODO: Implement a more complex case where the number of workers is smaller than the world size.\n        # For now, we simply raise an error if the number of workers is not equal to the world size.\n        raise NotImplementedError(\"The number of workers is smaller than the world size.\")\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": ""}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[6:]\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if str(remote_filepath).startswith(\"local:\"):\n            remote_filepath = str(remote_filepath).replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[6:]\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if str(remote_filepath).startswith(\"local:\"):\n            remote_filepath = str(remote_filepath).replace(\"local:\", \"\")\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[6:]\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if str(remote_filepath).startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if str(remote_filepath).startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath.replace(\"local:\", \"\")\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if str(remote_filepath).startswith(\"local:\"):\n            remote_filepath = str(remote_filepath)[len(\"local:\") :]\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if str(remote_filepath).startswith(\"local:\"):\n            remote_filepath = str(remote_filepath).replace(\"local:\", \"\")\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        data = [\n            np.uint32(item.width).tobytes(),\n            np.uint32(item.height).tobytes(),\n            np.uint32(len(item.mode)).tobytes(),\n            item.mode.encode(\"utf-8\"),\n            item.tobytes(),\n        ]\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        mode = item.mode\n        size = item.width, item.height\n        raw = item.tobytes()\n        data = [np.uint32(len(mode)).tobytes()]\n        data.append(mode.encode(\"utf-8\"))\n        data.append(np.uint32(size[0]).tobytes())\n        data.append(np.uint32(size[1]).tobytes())\n        data.append(raw)\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        mode = item.mode\n        width, height = item.size\n        data = [\n            np.uint32(width).tobytes(),\n            np.uint32(height).tobytes(),\n            np.uint32(len(mode)).tobytes(),\n            mode.encode(\"utf-8\"),\n            item.tobytes(),\n        ]\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        data = [np.uint32(item.width).tobytes()]\n        data.append(np.uint32(item.height).tobytes())\n        mode = item.mode\n        data.append(np.uint32(len(mode)).tobytes())\n        data.append(mode.encode(\"utf-8\"))\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        data = [\n            np.uint32(item.width).tobytes(),\n            np.uint32(item.height).tobytes(),\n            np.uint32(len(item.mode)).tobytes(),\n            item.mode.encode(\"utf-8\"),\n            item.tobytes(),\n        ]\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        data = [\n            np.uint32(item.width).tobytes(),\n            np.uint32(item.height).tobytes(),\n            np.uint32(len(item.mode)).tobytes(),\n            item.mode.encode(\"utf-8\"),\n            item.tobytes(encoder_name=\"raw\"),\n        ]\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        mode = item.mode\n        width, height = item.size\n        data = [np.uint32(width).tobytes(), np.uint32(height).tobytes(), np.uint32(len(mode)).tobytes(), mode.encode(\"utf-8\")]\n        data.append(item.tobytes())\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        mode = item.mode\n        width, height = item.size\n        data = [np.uint32(width).tobytes(), np.uint32(height).tobytes(), np.uint32(len(mode)).tobytes(), mode.encode(\"utf-8\")]\n        data.append(item.tobytes())\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        mode = item.mode\n        size = item.size\n        width, height = size\n        mode_size = len(mode)\n        data = [\n            np.uint32(width).tobytes(),\n            np.uint32(height).tobytes(),\n            np.uint32(mode_size).tobytes(),\n            mode.encode(\"utf-8\"),\n            item.tobytes(),\n        ]\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        mode = item.mode\n        size = item.width, item.height\n        raw = item.tobytes()\n        return (\n            np.uint32(len(mode)).tobytes()\n            + mode.encode(\"utf-8\")\n            + np.uint32(size[0]).tobytes()\n            + np.uint32(size[1]).tobytes()\n            + raw,\n            None,\n        )"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        mode = item.mode\n        width, height = item.size\n        data = [\n            np.uint32(width).tobytes(),\n            np.uint32(height).tobytes(),\n            np.uint32(len(mode)).tobytes(),\n            mode.encode(\"utf-8\"),\n            item.tobytes(\"raw\"),\n        ]\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        mode = item.mode\n        size = item.width, item.height\n        raw = item.tobytes()\n        return b\"\".join(\n            [\n                np.uint32(item.width).tobytes(),\n                np.uint32(item.height).tobytes(),\n                np.uint32(len(mode)).tobytes(),\n                mode.encode(\"utf-8\"),\n                raw,\n            ]\n        ), None"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        mode = item.mode\n        mode_size = len(mode)\n        width, height = item.size\n        data = [\n            np.uint32(width).tobytes(),\n            np.uint32(height).tobytes(),\n            np.uint32(mode_size).tobytes(),\n            mode.encode(\"utf-8\"),\n            item.tobytes(),\n        ]\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        data = [np.uint32(item.width).tobytes(), np.uint32(item.height).tobytes(), np.uint32(len(item.mode)).tobytes()]\n        data.append(item.mode.encode(\"utf-8\"))\n        data.append(item.tobytes())\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        mode = item.mode\n        size = item.width, item.height\n        raw = item.tobytes()\n        data = [np.uint32(len(mode)).tobytes(), mode.encode(\"utf-8\"), np.uint32(size[0]).tobytes(),\n                np.uint32(size[1]).tobytes(), raw]\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Serialize the image's width, height, and mode length as bytes\n        width, height = item.size\n        mode = item.mode\n        mode_size = len(mode)\n        data = [\n            np.uint32(width).tobytes(),\n            np.uint32(height).tobytes(),\n            np.uint32(mode_size).tobytes(),\n            mode.encode(\"utf-8\"),\n            item.tobytes(order=\"C\"),\n        ]\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        mode = item.mode\n        size = item.width, item.height\n        raw = item.tobytes(\"raw\")\n        data = [np.uint32(len(mode)).tobytes()]\n        data.append(mode.encode(\"utf-8\"))\n        data.append(np.uint32(size[0]).tobytes())\n        data.append(np.uint32(size[1]).tobytes())\n        data.append(raw)\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # TODO: Add support for a better serialization mechanism for PIL images\n        # TODO: Investigate https://pytorch.org/vision/main/generated/torchvision.io.encode_jpeg.html\n        # TODO: Investigate https://pytorch.org/vision/main/generated/torchvision.io.encode_png.html\n        # TODO: Investigate https://pytorch.org/vision/main/generated/torchvision.io.encode_webp.html\n        # TODO: Investigate https://pytorch.org/vision/main/generated/torchvision.io.encode_gif.html\n        # TODO: Investigate https://pytorch.org/vision/main/generated/torchvision.io.encode_video.html\n        # TODO: Investigate https://pytorch.org/vision/main/generated/torchvision.io.encode_audio.html\n        # TODO: Investigate https://pytorch.org/vision/main/generated/torchvision.io.encode_image.html\n\n        # Add support for a better serialization mechanism for PIL images\n        # TODO: Investigate https://pytorch.org/vision/main/generated/torchvision.io.encode_jpeg.html\n        # TODO: Investigate https://pytorch.org/vision/main/generated/torchvision.io.encode_png.html\n        # TODO: Investigate https://pytorch.org/vision/main/generated/torchvision.io.encode_webp.html\n        # TODO: Investigate https://pytorch.org/vision/main/generated/torchvision.io.encode_gif.html\n        # TODO: Investigate https://pytorch.org/vision/main/generated/torchvision.io.encode_video.html\n        # TODO: Investigate https://pytorch.org/vision/main/generated/torchvision.io.encode_audio.html\n        # TODO: Investigate https://pytorch.org/vision/main/generated/torchvision.io.encode_image.html\n\n        # Add support for a better serialization mechanism for P"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        width, height = item.size\n        mode = item.mode\n        raw = item.tobytes()\n        data = [\n            np.uint32(width).tobytes(),\n            np.uint32(height).tobytes(),\n            np.uint32(len(mode)).tobytes(),\n            mode.encode(\"utf-8\"),\n            raw,\n        ]\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        if not isinstance(item, Image.Image):\n            raise TypeError(f\"The provided item should be of type {Image.Image}. Found {item}.\")\n\n        # Note: The first 4 bytes are the width, the second 4 bytes are the height, the third 4 bytes are the mode length\n        # The last bytes are the mode and the raw pixel data\n        mode = item.mode\n        mode_size = len(mode)\n        data = [\n            np.uint32(item.width).tobytes(),\n            np.uint32(item.height).tobytes(),\n            np.uint32(mode_size).tobytes(),\n            mode.encode(\"utf-8\"),\n            item.tobytes(\"raw\", \"RGB\"),\n        ]\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            array = torch.frombuffer(data, dtype=torch.uint8)\n            try:\n                return decode_jpeg(array)\n            except RuntimeError:\n                # Note: Some datasets like Imagenet contains some PNG images with JPEG extension, so we fallback to PIL\n                pass\n\n        img = PILSerializer.deserialize(data)\n        if _TORCH_VISION_AVAILABLE:\n            img = pil_to_tensor(img)\n        return img\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            if isinstance(item, JpegImageFile):\n                if item.filename and os.path.exists(item.filename):\n                    with open(item.filename, \"rb\") as f:\n                        return f.read(), None\n                else:\n                    array = torch.frombuffer(item.tobytes(), dtype=torch.uint8)\n                    return decode_jpeg(array).tobytes(), None\n            else:\n                array = torch.frombuffer(item.tobytes(), dtype=torch.uint8)\n                return decode_jpeg(array).tobytes(), None\n        else:\n            raise ModuleNotFoundError(\"torchvision is required. Run `pip install torchvision`\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            if isinstance(item, JpegImageFile):\n                if item.filename and os.path.exists(item.filename):\n                    with open(item.filename, \"rb\") as f:\n                        return f.read(), \"jpeg\"\n                else:\n                    return item.tobytes(), \"jpeg\"\n            elif isinstance(item, Image.Image):\n                with io.BytesIO() as f:\n                    item.save(f, format=\"JPEG\")\n                    return f.getvalue(), \"jpeg\"\n            else:\n                raise TypeError(f\"Expected JPEG image, got {type(item)}\")\n        else:\n            raise ModuleNotFoundError(\"torchvision is required. Run `pip install torchvision`\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            if isinstance(item, JpegImageFile):\n                if item.filename and os.path.isfile(item.filename):\n                    with open(item.filename, \"rb\") as f:\n                        return f.read(), None\n\n            array = pil_to_tensor(item)\n            return array.numpy().tobytes(order=\"C\"), None\n\n        with io.BytesIO() as buffer:\n            item.save(buffer, \"JPEG\")\n            return buffer.getvalue(), None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            if isinstance(item, JpegImageFile):\n                if item.filename and os.path.isfile(item.filename):\n                    with open(item.filename, \"rb\") as f:\n                        return f.read(), None\n                else:\n                    # Note: Some datasets like Imagenet contains some PNG images with JPEG extension, so we fallback to PIL\n                    pass\n\n            if isinstance(item, Image.Image):\n                buffer = io.BytesIO()\n                item.save(buffer, format=\"JPEG\")\n                return buffer.getvalue(), None\n\n        raise TypeError(f\"Only JPEG images are supported. Got {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            if isinstance(item, JpegImageFile):\n                return item.tobytes(), None\n            if isinstance(item, Image.Image):\n                with tempfile.TemporaryDirectory() as dirname:\n                    fname = os.path.join(dirname, \"file.jpeg\")\n                    item.save(fname)\n                    with open(fname, \"rb\") as stream:\n                        return stream.read(), None\n        raise TypeError(f\"{self.__class__.__name__} can only serialize JPEG images.\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            if hasattr(item, \"filename\") and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                return self.serialize(item.convert(\"RGB\"))\n        elif isinstance(item, Image.Image):\n            with io.BytesIO() as f:\n                item.save(f, format=\"JPEG\")\n                return f.getvalue(), None\n        else:\n            raise TypeError(f\"Expected PIL Image, but got {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            if isinstance(item, JpegImageFile):\n                if hasattr(item, \"_filename\") and os.path.isfile(item._filename):\n                    with open(item._filename, \"rb\") as f:\n                        return f.read(), None\n                else:\n                    return item.tobytes(), None\n            elif isinstance(item, Image.Image):\n                with io.BytesIO() as f:\n                    item.save(f, format=\"JPEG\")\n                    return f.getvalue(), None\n\n        raise TypeError(f\"Unsupported image type {type(item)}\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            if item.filename:\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                with io.BytesIO() as f:\n                    item.save(f, format=\"jpeg\")\n                    return f.getvalue(), None\n        elif isinstance(item, Image.Image):\n            with io.BytesIO() as f:\n                item.save(f, format=\"jpeg\")\n                return f.getvalue(), None\n        else:\n            raise TypeError(f\"Expected JPEG image, but got {type(item)}\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            if isinstance(item, JpegImageFile):\n                if item.filename and os.path.exists(item.filename):\n                    with open(item.filename, \"rb\") as f:\n                        return f.read(), None\n                else:\n                    return item.tobytes(), None\n            elif isinstance(item, PngImageFile):\n                if item.filename and os.path.exists(item.filename):\n                    with open(item.filename, \"rb\") as f:\n                        return f.read(), None\n                else:\n                    return item.tobytes(), None\n            elif isinstance(item, WebPImageFile):\n                if item.filename and os.path.exists(item.filename):\n                    with open(item.filename, \"rb\") as f:\n                        return f.read(), None\n                else:\n                    return item.tobytes(), None\n            elif isinstance(item, GifImageFile):\n                if item.filename and os.path.exists(item.filename):\n                    with open(item.filename, \"rb\") as f:\n                        return f.read(), None\n                else:\n                    return item.tobytes(), None\n            elif isinstance(item, Image.Image):\n                with io.BytesIO() as f:\n                    item.save(f, format=\"JPEG\")\n                    return f.getvalue(), None\n            else:\n                raise TypeError(f\"Expected a PIL Image, got {type(item)}\")\n        else:\n            raise ModuleNotFoundError(\"torchvision is required. Run `pip install torchvision`\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            if isinstance(item, JpegImageFile):\n                if item.filename and os.path.isfile(item.filename):\n                    with open(item.filename, \"rb\") as f:\n                        return f.read(), None\n                return item.tobytes(), None\n            if isinstance(item, PngImageFile):\n                with io.BytesIO() as f:\n                    item.save(f, format=\"JPEG\")\n                    return f.getvalue(), None\n            if isinstance(item, Image.Image):\n                with io.BytesIO() as f:\n                    item.save(f, format=\"JPEG\")\n                    return f.getvalue(), None\n        raise TypeError(f\"Cannot serialize {type(item)} to JPEG.\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            if isinstance(item, JpegImageFile):\n                if item.filename and os.path.exists(item.filename):\n                    with open(item.filename, \"rb\") as f:\n                        return f.read(), None\n                else:\n                    return item.tobytes(), None\n            else:\n                try:\n                    return item.tobytes(), None\n                except OSError:\n                    pass\n\n        if _PIL_AVAILABLE:\n            if isinstance(item, Image.Image):\n                if item.format == \"JPEG\":\n                    if item.filename and os.path.exists(item.filename):\n                        with open(item.filename, \"rb\") as f:\n                            return f.read(), None\n                    else:\n                        with io.BytesIO() as buffer:\n                            item.save(buffer, \"JPEG\")\n                        return buffer.getvalue(), None\n                else:\n                    with io.BytesIO() as buffer:\n                        item.save(buffer, \"JPEG\")\n                    return buffer.getvalue(), None\n\n        raise TypeError(f\"Unsupported image type: {type(item)}\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            if isinstance(item, JpegImageFile):\n                return item.tobytes(), None\n            if isinstance(item, GifImageFile):\n                return item.tobytes(), None\n            if isinstance(item, PngImageFile):\n                return item.tobytes(), None\n            if isinstance(item, WebPImageFile):\n                return item.tobytes(), None\n\n        if isinstance(item, Image.Image):\n            if hasattr(item, \"filename\") and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            with io.BytesIO() as f:\n                item.save(f, format=\"JPEG\")\n                return f.getvalue(), None\n        raise TypeError(\"Item is not a supported image type.\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            if isinstance(item, JpegImageFile):\n                if hasattr(item, \"filename\"):\n                    if os.path.exists(item.filename):\n                        with open(item.filename, \"rb\") as f:\n                            return f.read(), None\n                else:\n                    return item.tobytes(), None\n            elif isinstance(item, PngImageFile):\n                return self.serialize(item.convert(\"RGB\"))\n            elif isinstance(item, WebPImageFile):\n                return self.serialize(item.convert(\"RGB\"))\n            elif isinstance(item, GifImageFile):\n                return self.serialize(item.convert(\"RGB\"))\n\n        if _PIL_AVAILABLE:\n            if isinstance(item, Image.Image):\n                with io.BytesIO() as buffer:\n                    item.save(buffer, \"JPEG\")\n                    return buffer.getvalue(), None\n\n        raise TypeError(f\"{type(item)} is not supported by {self.__class__.__name__}\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            if isinstance(item, JpegImageFile):\n                if item.filename and os.path.isfile(item.filename):\n                    with open(item.filename, \"rb\") as f:\n                        return f.read(), None\n                else:\n                    with io.BytesIO() as buffer:\n                        item.save(buffer, \"JPEG\")\n                        return buffer.getvalue(), None\n            elif isinstance(item, torch.Tensor):\n                with io.BytesIO() as buffer:\n                    torchvision.io.write_jpeg(item, buffer)\n                    return buffer.getvalue(), None\n            else:\n                raise TypeError(\n                    \"JPEGSerializer can only serialize JPEG images. \"\n                    \"Please use PILSerializer if you want to serialize other image types.\"\n                )\n        else:\n            raise ModuleNotFoundError(\"torchvision is required. Run `pip install torchvision`\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            if isinstance(item, JpegImageFile):\n                if item.filename:\n                    with open(item.filename, \"rb\") as f:\n                        return f.read(), None\n                return item.tobytes(), None\n            if isinstance(item, torch.Tensor):\n                return item.numpy().tobytes(order=\"C\"), None\n            return item.tobytes(), None\n        if isinstance(item, JpegImageFile):\n            if item.filename:\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            with tempfile.TemporaryDirectory() as dirname:\n                fname = os.path.join(dirname, \"file.jpeg\")\n                item.save(fname)\n                with open(fname, \"rb\") as f:\n                    return f.read(), None\n        if isinstance(item, torch.Tensor):\n            with io.BytesIO() as output:\n                torchvision.io.write_jpeg(item, output)\n                return output.getvalue(), None\n        with io.BytesIO() as output:\n            item.save(output, format=\"JPEG\")\n            return output.getvalue(), None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            if isinstance(item, JpegImageFile):\n                if item.filename and os.path.isfile(item.filename):\n                    with open(item.filename, \"rb\") as f:\n                        return f.read(), None\n                else:\n                    item = item.convert(\"RGB\")\n                    array = torch.from_numpy(np.array(item))\n                    return array.tobytes(), None\n            elif isinstance(item, torch.Tensor):\n                item = item.permute(1, 2, 0).numpy()\n                item = Image.fromarray(item)\n                item = item.convert(\"RGB\")\n                array = torch.from_numpy(np.array(item))\n                return array.tobytes(), None\n        else:\n            if isinstance(item, JpegImageFile):\n                if item.filename and os.path.isfile(item.filename):\n                    with open(item.filename, \"rb\") as f:\n                        return f.read(), None\n                else:\n                    item = item.convert(\"RGB\")\n                    array = np.array(item)\n                    return array.tobytes(), None\n            elif isinstance(item, torch.Tensor):\n                item = item.permute(1, 2, 0).numpy()\n                item = Image.fromarray(item)\n                item = item.convert(\"RGB\")\n                array = np.array(item)\n                return array.tobytes(), None\n\n        if not isinstance(item, Image.Image):\n            raise TypeError(f\"Expected a PIL Image, got {type(item)}\")\n        if not isinstance(item, JpegImageFile):\n            item = item.convert(\"RGB\")\n        array = np.array(item)\n        return array.tobytes(), None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            if isinstance(item, JpegImageFile) and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                return self.serialize_tensor(item)\n        else:\n            raise ModuleNotFoundError(\"torchvision is required. Run `pip install torchvision`\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise ModuleNotFoundError(\"torchvision is required. Run `pip install torchvision`\")\n\n        if isinstance(item, JpegImageFile):\n            # Note: Some datasets like Imagenet contains some PNG images with JPEG extension, so we fallback to PIL\n            return PILSerializer.serialize(item)\n\n        if item.mode != \"RGB\":\n            item = item.convert(\"RGB\")\n        with io.BytesIO() as output:\n            item.save(output, format=\"JPEG\")\n            return output.getvalue(), None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if not _PIL_AVAILABLE:\n            raise ModuleNotFoundError(\"PIL is required. Run `pip install Pillow`\")\n\n        if isinstance(item, JpegImageFile):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n\n        with io.BytesIO() as f:\n            item.save(f, format=\"JPEG\")\n            return f.getvalue(), None\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[:12], np.uint32)\n        width, height, mode_size = ints[0], ints[1], ints[2]\n        mode = data[12:12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size:]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[0:12], np.uint32)\n        width, height, mode_size = ints[0], ints[1], ints[2]\n        mode = data[12:12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size:]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[0:12], np.uint32)\n        width, height, mode_size = ints[0], ints[1], ints[2]\n        mode = data[12:12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size:]\n        return Image.frombytes(mode, (width, height), raw)\n\n    "}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[:12], np.uint32)\n        width, height, mode_size = ints[0], ints[1], ints[2]\n        mode = data[12:12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size:]\n        return Image.frombytes(mode, (width, height), raw)\n\n    "}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[0:12], np.uint32)\n        width, height, mode_size = ints[0], ints[1], ints[2]\n        mode = data[12:12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size:]\n        return Image.frombytes(mode, (width, height), raw)\n\n    "}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[:12], np.uint32)\n        width, height, mode_size = ints[0], ints[1], ints[2]\n        mode = data[12:12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size:]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = np.frombuffer(data[0:12], np.uint32)\n        mode = data[12 : 12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size :]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = np.frombuffer(data[0:12], np.uint32)\n        mode = data[12 : 12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size :]\n        return Image.frombytes(mode, (width, height), raw)\n\n    "}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = np.frombuffer(data[0:12], np.uint32)\n        mode = data[12:12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size:]\n        return Image.frombytes(mode, (width, height), raw)\n\n    "}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[:12], np.uint32)\n        width, height, mode_size = ints[0], ints[1], ints[2]\n        mode = data[12:12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size:]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[0:12], np.uint32)\n        width, height, mode_size = ints\n        mode = data[12:12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size:]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = np.frombuffer(data[:12], np.uint32)\n        mode = data[12:12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size:]\n        return Image.frombytes(mode, (width, height), raw)\n\n    "}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = np.frombuffer(data[:12], np.uint32)\n        mode = data[12 : 12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size :]\n        return Image.frombytes(mode, (width, height), raw)\n\n    "}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[0:12], np.uint32)\n        width, height, mode_size = ints[0], ints[1], ints[2]\n        mode = data[12 : 12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size :]\n        return Image.frombytes(mode, (width, height), raw)\n\n    "}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[0:12], np.uint32)\n        mode = data[12 : 12 + ints[2]].decode(\"utf-8\")\n        width, height = ints[0], ints[1]\n        raw = data[12 + ints[2] :]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[0:12], np.uint32)\n        width, height, mode_len = ints[0], ints[1], ints[2]\n        mode = data[12:12 + mode_len].decode(\"utf-8\")\n        raw = data[12 + mode_len:]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[0:12], np.uint32)\n        mode_size = ints[2]\n        width, height = ints[0], ints[1]\n        mode = data[12:12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size:]\n        return Image.frombytes(mode, (width, height), raw)\n\n    "}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[:12], np.uint32)\n        width, height, mode_size = ints[0], ints[1], ints[2]\n        mode = data[12:12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size:]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[0:12], np.uint32)\n        mode_size = ints[2]\n        mode = data[12 : 12 + mode_size].decode(\"utf-8\")\n        width, height = ints[0], ints[1]\n        raw = data[12 + mode_size :]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[0:12], np.uint32)\n        width, height, mode_len = ints\n        mode = data[12:12 + mode_len].decode(\"utf-8\")\n        raw = data[12 + mode_len:]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the tensor bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return torch.from_numpy(tensor)\n        return torch.from_numpy(np.reshape(tensor, shape))"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the tensor bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return torch.from_numpy(tensor)\n        return torch.from_numpy(np.reshape(tensor, shape))"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the tensor bytes\n        tensor = torch.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return torch.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the tensor bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return torch.from_numpy(tensor)\n        return torch.from_numpy(np.reshape(tensor, shape))"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the tensor bytes\n        tensor = torch.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return torch.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the tensor bytes\n        tensor = torch.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return torch.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the tensor bytes\n        tensor = torch.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return torch.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the tensor bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return torch.from_numpy(tensor)\n        return torch.from_numpy(np.reshape(tensor, shape))"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the tensor bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return torch.from_numpy(tensor)\n        return torch.from_numpy(np.reshape(tensor, shape))"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the tensor bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return torch.from_numpy(tensor)\n        return torch.from_numpy(np.reshape(tensor, shape))"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return torch.from_numpy(tensor)\n        return torch.from_numpy(np.reshape(tensor, shape))"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the tensor bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return torch.from_numpy(tensor)\n        return torch.from_numpy(np.reshape(tensor, shape))"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the tensor bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return torch.from_numpy(tensor)\n        return torch.from_numpy(np.reshape(tensor, shape))"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the tensor bytes\n        tensor = torch.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return torch.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return torch.from_numpy(tensor)\n        return torch.from_numpy(np.reshape(tensor, shape))"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return torch.from_numpy(tensor)\n        return torch.from_numpy(np.reshape(tensor, shape))"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return torch.from_numpy(tensor)\n        return torch.from_numpy(np.reshape(tensor, shape))"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the tensor bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return torch.from_numpy(tensor)\n        return torch.from_numpy(np.reshape(tensor, shape))"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the tensor bytes\n        tensor = torch.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return torch.reshape(tensor, shape)\n\n    "}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the tensor bytes\n        tensor = torch.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return torch.reshape(tensor, shape)\n\n   "}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            if _TORCH_VISION_AVAILABLE:\n                return decode_jpeg(data)\n            else:\n                return Image.open(io.BytesIO(data))\n        except RuntimeError:\n            return Image.open(io.BytesIO(data))\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError as e:\n                if \"JPEG\" in str(e):\n                    return pil_to_tensor(PILSerializer().deserialize(data))\n        return PILSerializer().deserialize(data)"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                # fallback to PIL\n                pass\n\n        if _PIL_AVAILABLE:\n            return Image.open(io.BytesIO(data))\n\n        raise ModuleNotFoundError(\"PIL is required. Run `pip install pillow`\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n        if _PIL_AVAILABLE:\n            return Image.open(io.BytesIO(data))\n        raise ModuleNotFoundError(\"PIL is required. Run `pip install Pillow`\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError as e:\n                if \"Expected a JPEG image\" in str(e):\n                    return pil_to_tensor(Image.open(io.BytesIO(data)))\n                raise e\n        else:\n            return Image.open(io.BytesIO(data))"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                # fallback to PIL\n                pass\n        return PILSerializer().deserialize(data)"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise ModuleNotFoundError(\"torchvision is required. Run `pip install torchvision`\")\n\n        try:\n            return decode_jpeg(data)\n        except RuntimeError as e:\n            if \"JPEG\" in str(e):\n                return Image.open(io.BytesIO(data))\n            raise e\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        return self.deserialize_pil(data)\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError as e:\n                if \"Expected JPEG\" in str(e):\n                    return pil_to_tensor(PILSerializer().deserialize(data))\n                raise e\n        return pil_to_tensor(PILSerializer().deserialize(data))"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError as e:\n                if \"torchvision\" in str(e):\n                    raise e\n\n        item = Image.open(io.BytesIO(data))\n        if _TORCH_VISION_AVAILABLE:\n            return pil_to_tensor(item)\n        return item"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise ModuleNotFoundError(\"torchvision is required. Run `pip install torchvision`\")\n\n        try:\n            return decode_jpeg(data)\n        except RuntimeError as e:\n            if \"Expected a JPEG image\" in str(e):\n                return PILSerializer().deserialize(data)\n            raise e\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            if bool(_TORCH_VISION_AVAILABLE):\n                return decode_jpeg(data)\n        except RuntimeError:\n            return Image.open(io.BytesIO(data))\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                # try to decode the image using torchvision\n                return decode_jpeg(data)\n            except RuntimeError as e:\n                if \"Unable to decode JPEG\" in str(e):\n                    # fallback to PIL\n                    return Image.open(io.BytesIO(data))\n        return Image.open(io.BytesIO(data))"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError as e:\n                if \"Expected JPEG file\" in str(e):\n                    return PILSerializer().deserialize(data)\n\n        item = PILSerializer().deserialize(data)\n        if _TORCH_VISION_AVAILABLE:\n            return pil_to_tensor(item)\n        return item"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        try:\n            if _TORCH_VISION_AVAILABLE:\n                return decode_jpeg(data)\n        except RuntimeError:\n            pass\n\n        if _PIL_AVAILABLE:\n            item = Image.open(io.BytesIO(data))\n            if _TORCH_VISION_AVAILABLE:\n                return pil_to_tensor(item)\n            return item\n\n        raise TypeError(f\"The provided item should be of type {JpegImageFile}. Found {item}.\")\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError as e:\n                if \"JPEG\" in str(e):\n                    item = Image.open(io.BytesIO(data))\n                    return item.convert(\"RGB\")\n\n        if _PIL_AVAILABLE:\n            item = Image.open(io.BytesIO(data))\n            if _TORCH_VISION_AVAILABLE:\n                return pil_to_tensor(item.convert(\"RGB\"))\n            return item.convert(\"RGB\")\n\n        raise ModuleNotFoundError(\"PIL or torchvision is required. Run `pip install Pillow torchvision`\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError as e:\n                if \"torchvision\" in str(e) and \"JPEG\" in str(e):\n                    item = Image.open(io.BytesIO(data))\n                    item.convert(\"RGB\")\n                    return item\n        item = Image.open(io.BytesIO(data))\n        item.convert(\"RGB\")\n        if _TORCH_VISION_AVAILABLE:\n            return pil_to_tensor(item)\n        return item"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError as e:\n                if \"Not a JPEG file\" in str(e):\n                    return cls.deserialize_pil(data)\n                else:\n                    raise e\n        else:\n            return cls.deserialize_pil(data)\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise ModuleNotFoundError(\"torchvision is required. Run `pip install torchvision`\")\n\n        try:\n            tensor = decode_jpeg(data)\n        except RuntimeError as e:\n            if \"Unable to decode JPEG\" in str(e):\n                # fallback to PIL\n                return PILSerializer().deserialize(data)\n            raise e\n\n        if _TORCH_VISION_AVAILABLE:\n            return pil_to_tensor(tensor)\n\n        return tensor\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                # Try to decode the data as a JPEG image using torchvision\n                return decode_jpeg(data)\n            except RuntimeError as e:\n                if \"JPEG\" in str(e):\n                    # If the decoding fails due to a runtime error (e.g., the data is actually a PNG with a JPEG extension),\n                    # fall back to using PIL to deserialize the data.\n                    return cls.deserialize_with_pil(data)\n\n        # If torchvision is not available, fall back to using PIL to deserialize the data.\n        return cls.deserialize_with_pil(data)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\"\n\n   "}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.from_numpy(np.frombuffer(data, dtype=self._dtype))"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.from_numpy(np.frombuffer(data, dtype=self._dtype))"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.from_numpy(np.frombuffer(data, dtype=self._dtype))"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.from_numpy(np.frombuffer(data, dtype=self._dtype))"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        tensor = torch.frombuffer(data, dtype=self._dtype)\n        return tensor"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        tensor = torch.frombuffer(data, dtype=self._dtype)\n        return tensor"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        tensor = torch.frombuffer(data, dtype=self._dtype)\n        return tensor"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.from_numpy(np.frombuffer(data, dtype=self._dtype))"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.from_numpy(np.frombuffer(data, dtype=self._dtype))"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.from_numpy(np.frombuffer(data, self._dtype))"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.from_numpy(np.frombuffer(data, self._dtype))"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        array = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if array.shape == shape:\n            return array\n        return np.reshape(array, shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        array = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if array.shape == shape:\n            return array\n        return np.reshape(array, shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        array = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if array.shape == shape:\n            return array\n        return np.reshape(array, shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        array = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if array.shape == shape:\n            return array\n        return np.reshape(array, shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        array = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if array.shape == shape:\n            return array\n        return np.reshape(array, shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        array = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if array.shape == shape:\n            return array\n        return np.reshape(array, shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        array = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if array.shape == shape:\n            return array\n        return np.reshape(array, shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        array = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if array.shape == shape:\n            return array\n        return np.reshape(array, shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        array = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if array.shape == shape:\n            return array\n        return array.reshape(shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        array = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if array.shape == shape:\n            return array\n        return np.reshape(array, shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        array = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if array.shape == shape:\n            return array\n        return array.reshape(shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        array = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if array.shape == shape:\n            return array\n        return array.reshape(shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        array = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if array.shape == shape:\n            return array\n        return np.reshape(array, shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        array = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if array.shape == shape:\n            return array\n        return array.reshape(shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        array = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if array.shape == shape:\n            return array\n        return array.reshape(shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        array = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if array.shape == shape:\n            return array\n        return array.reshape(shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        array = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if array.shape == shape:\n            return array\n        return array.reshape(shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        array = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if array.shape == shape:\n            return array\n        return array.reshape(shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        array = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if array.shape == shape:\n            return array\n        return array.reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(order=\"C\"), f\"no_header_numpy:{self._dtype_to_indices[self._dtype]}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(order=\"C\"), f\"no_header_numpy:{self._dtype_to_indices[self._dtype]}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(order=\"C\"), f\"no_header_numpy:{self._dtype_to_indices[self._dtype]}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(order=\"C\"), f\"no_header_numpy:{self._dtype_to_indices[self._dtype]}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(), f\"no_header_numpy:{self._dtype_to_indices[self._dtype]}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(order=\"C\"), f\"no_header_numpy:{self._dtype_to_indices[self._dtype]}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(order=\"C\"), f\"no_header_numpy:{self._dtype_to_indices[self._dtype]}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(), f\"no_header_numpy:{self._dtype_to_indices[self._dtype]}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n\n   "}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(), f\"no_header_numpy:{self._dtype_to_indices[self._dtype]}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n\n   "}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[self._dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[self._dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(order=\"C\"), f\"no_header_numpy:{self._dtype_to_indices[self._dtype]}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(order=\"C\"), f\"no_header_numpy:{self._dtype_to_indices[self._dtype]}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        return item.tobytes(), f\"no_header_numpy:{self._dtype_to_indices[self._dtype]}\"\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        # serialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        # serialize the shape header\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        # serialize the shape header\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        # serialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": self.dataset.state_dict(),\n            \"current_epoch\": self.current_epoch,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n\n        if isinstance(self.dataset, StreamingDataset):\n            state[\"num_samples_yielded\"] = self._num_samples_yielded_streaming\n        else:\n            state[\"num_samples_yielded\"] = deepcopy(self._num_samples_yielded_combined)\n\n        return state\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": self.dataset.state_dict(),\n            \"current_epoch\": self.current_epoch,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n\n        if isinstance(self.dataset, StreamingDataset):\n            state[\"num_samples_yielded\"] = self._num_samples_yielded_streaming\n        else:\n            state[\"num_samples_yielded\"] = deepcopy(self._num_samples_yielded_combined)\n\n        return state\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            state = {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            state = {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n        return state"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": self.dataset.state_dict(),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_streaming,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n\n        if isinstance(self.dataset, CombinedStreamingDataset):\n            state[\"num_samples_yielded\"] = self._num_samples_yielded_combined\n\n        return state\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": self.dataset.state_dict(),\n            \"current_epoch\": self.current_epoch,\n            \"batch_size\": self.batch_size,\n            \"num_workers\": self.num_workers,\n        }\n\n        if isinstance(self.dataset, StreamingDataset):\n            state[\"num_samples_yielded\"] = self._num_samples_yielded_streaming\n        else:\n            state[\"num_samples_yielded\"] = self._num_samples_yielded_combined\n\n        state[\"latest_worker_idx\"] = self._latest_worker_idx\n        return state\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": self.dataset.state_dict(),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_streaming\n            if isinstance(self.dataset, StreamingDataset)\n            else self._num_samples_yielded_combined,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n\n        return state\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            dataset_state = self.dataset.state_dict()\n            state = {\n                \"dataset\": dataset_state,\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            dataset_state = self.dataset.state_dict()\n            state = {\n                \"dataset\": dataset_state,\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n        return state"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state_dict = {\n            \"dataset\": self.dataset.state_dict(),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_streaming,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n        if isinstance(self.dataset, CombinedStreamingDataset):\n            state_dict[\"num_samples_yielded\"] = deepcopy(self._num_samples_yielded_combined)\n        return state_dict\n\n    "}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            state = {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            state = {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": self.dataset.state_dict() if isinstance(self.dataset, StreamingDataset) else self.dataset,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_streaming\n            if isinstance(self.dataset, StreamingDataset)\n            else self._num_samples_yielded_combined,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n\n        return state\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            dataset_state = self.dataset.state_dict()\n        else:\n            dataset_state = self.dataset.state_dict()\n\n        state = {\n            \"dataset\": dataset_state,\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_streaming\n            if isinstance(self.dataset, StreamingDataset)\n            else self._num_samples_yielded_combined,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n\n        return state\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise ImportError(\"The av library is required for video deserialization. Please install it with `pip install av`.\")\n        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\n                \"The torchvision library is required for video deserialization. Please install it with `pip install torchvision`.\"\n            )\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(data)\n            f.flush()\n            return read_video(f.name)"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\"To use the video serializer, please install torchvision.\")\n        if not _AV_AVAILABLE:\n            raise ImportError(\"To use the video serializer, please install av.\")\n\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(data)\n            f.flush()\n            return read_video(f.name)"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise ImportError(\n                \"The video serializer requires the av library. Please install it with `pip install av`.\"\n            )\n\n        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\n                \"The video serializer requires the torchvision library. Please install it with `pip install torchvision`.\"\n            )\n\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(data)\n            f.flush()\n            return read_video(f.name)"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise ImportError(\"The av library is required to deserialize video files. Please install it with `pip install av`.\")\n        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\"The torchvision library is required to deserialize video files. Please install it with `pip install torchvision`.\")\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.flush()\n            return torchvision.io.read_video(f.name)"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise ImportError(\"The `av` library is required to deserialize videos. Please install it with `pip install av`.\")\n\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(data)\n            f.flush()\n            return torchvision.io.read_video(f.name)"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\"The torchvision library is required for video deserialization.\")\n        if not _AV_AVAILABLE:\n            raise ImportError(\"The av library is required for video deserialization.\")\n\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(data)\n            f.flush()\n            return torchvision.io.read_video(f.name)"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise ImportError(\n                \"The video serializer requires the av library. Please install it with `pip install av`.\"\n            )\n        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\n                \"The video serializer requires the torchvision library. Please install it with `pip install torchvision`.\"\n            )\n        with tempfile.TemporaryDirectory() as tmpdir:\n            filepath = os.path.join(tmpdir, \"video.mp4\")\n            with open(filepath, \"wb\") as f:\n                f.write(data)\n            return read_video(filepath)"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise ImportError(\"To use the VideoSerializer, please install the av library.\")\n\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(data)\n            f.flush()\n            return torchvision.io.read_video(f.name)"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise ImportError(\n                \"To use the VideoSerializer, you need to install the av library. Please install it with `pip install av`.\"\n            )\n        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\n                \"To use the VideoSerializer, you need to install the torchvision library. Please install it with `pip install torchvision`.\"\n            )\n\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(data)\n            f.flush()\n            return torchvision.io.read_video(f.name)"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise ImportError(\"The av library is required to deserialize video data. Please install it with `pip install av`.\")\n        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\"The torchvision library is required to deserialize video data. Please install it with `pip install torchvision`.\")\n\n        with tempfile.NamedTemporaryFile() as tmp:\n            tmp.write(data)\n            tmp.flush()\n            return torchvision.io.read_video(tmp.name)"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise ModuleNotFoundError(\"The av library is required to deserialize video files. Install it with `pip install av`.\")\n        if not _TORCH_VISION_AVAILABLE:\n            raise ModuleNotFoundError(\n                \"The torchvision library is required to deserialize video files. Install it with `pip install torchvision`.\"\n            )\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(data)\n            f.flush()\n            return torchvision.io.read_video(f.name)"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise ImportError(\"To use the VideoSerializer, you need to install the av library.\")\n        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\"To use the VideoSerializer, you need to install the torchvision library.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.flush()\n            return read_video(f.name)"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise ImportError(\"Please install the `av` library to use the VideoSerializer.\")\n\n        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\"Please install the `torchvision` library to use the VideoSerializer.\")\n\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(data)\n            f.flush()\n            return read_video(f.name)"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise ImportError(\n                \"The VideoSerializer requires the av library to be installed. Please install it with `pip install av`.\"\n            )\n        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\n                \"The VideoSerializer requires the torchvision library to be installed. Please install it with `pip install torchvision`.\"\n            )\n        with tempfile.TemporaryDirectory() as tmpdir:\n            filepath = os.path.join(tmpdir, \"video.mp4\")\n            with open(filepath, \"wb\") as f:\n                f.write(data)\n            return torchvision.io.read_video(filepath)"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\"The torchvision library is required to use the VideoSerializer. Please install it with `pip install torchvision`.\")\n        if not _AV_AVAILABLE:\n            raise ImportError(\"The av library is required to use the VideoSerializer. Please install it with `pip install av`.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.flush()\n            return read_video(f.name)"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise ModuleNotFoundError(\n                \"The torchvision.io.read_video function requires the av library. \"\n                \"Please install it with `pip install av`.\"\n            )\n        if not _TORCH_VISION_AVAILABLE:\n            raise ModuleNotFoundError(\n                \"The torchvision.io.read_video function requires the torchvision library. \"\n                \"Please install it with `pip install torchvision`.\"\n            )\n\n        with tempfile.NamedTemporaryFile() as tmp:\n            tmp.write(data)\n            tmp.flush()\n            return torchvision.io.read_video(tmp.name)"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise RuntimeError(\n                \"The VideoSerializer requires the av library to be installed. \"\n                \"You can install it with `pip install av`.\"\n            )\n        if not _TORCH_VISION_AVAILABLE:\n            raise RuntimeError(\n                \"The VideoSerializer requires the torchvision library to be installed. \"\n                \"You can install it with `pip install torchvision`.\"\n            )\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(data)\n            f.flush()\n            return read_video(f.name)"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise ImportError(\n                \"The av library is required for video deserialization. Please install it with `pip install av`.\"\n            )\n        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\n                \"The torchvision library is required for video deserialization. Please install it with `pip install torchvision`.\"\n            )\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(data)\n            return read_video(f.name)\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise ValueError(\"The torchvision library is required for deserializing video data. Please install it first.\")\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.flush()\n            return torchvision.io.read_video(f.name)"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\n                \"Torchvision is not installed. To install it, run `pip install torchvision`.\"\n            )\n        if not _AV_AVAILABLE:\n            raise ImportError(\n                \"PyAV is not installed. To install it, run `pip install av`.\"\n            )\n        with tempfile.TemporaryDirectory() as tmpdir:\n            filepath = os.path.join(tmpdir, \"tmp.mp4\")\n            with open(filepath, \"wb\") as f:\n                f.write(data)\n            return read_video(filepath)"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        # Write the last chunk\n        if self._serialized_items:\n            self.write_chunk(on_done=True)\n\n        # Write the index\n        self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        return [os.path.join(self._cache_dir, chunk[\"filename\"]) for chunk in self._chunks_info]"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        # Write the remaining chunks\n        while self._should_write():\n            self.write_chunk()\n\n        # Write the chunks index\n        self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        # Return the list of file paths to the written chunks\n        return [os.path.join(self._cache_dir, chunk[\"filename\"]) for chunk in self._chunks_info]"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        # Write the remaining chunks\n        while self._should_write():\n            self.write_chunk()\n\n        # Write the index\n        self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        # Return the list of file paths to the written chunks\n        return [os.path.join(self._cache_dir, chunk[\"filename\"]) for chunk in self._chunks_info]"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        # Write the last chunk\n        if self._serialized_items:\n            self.write_chunk(on_done=True)\n\n        # Write the index file\n        self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        # Return the list of written chunk file paths\n        return [os.path.join(self._cache_dir, chunk[\"filename\"]) for chunk in self._chunks_info]"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        # Write the remaining items\n        if self._serialized_items:\n            self.write_chunk(on_done=True)\n\n        # Write the index file\n        self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        # Return the list of file paths to the written chunks\n        return [os.path.join(self._cache_dir, chunk[\"filename\"]) for chunk in self._chunks_info]"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        # Write the remaining chunks\n        while self._should_write():\n            self.write_chunk()\n\n        # Write the index file\n        self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        # Return the list of file paths to the written chunks\n        return [os.path.join(self._cache_dir, chunk[\"filename\"]) for chunk in self._chunks_info]\n\n    "}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._should_write():\n            self.write_chunk()\n\n        self.write_chunks_index()\n        self._is_done = True\n        return [os.path.join(self._cache_dir, chunk[\"filename\"]) for chunk in self._chunks_info]"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return []\n        self._is_done = True\n        filepaths = []\n        while self._serialized_items:\n            filepaths.append(self.write_chunk(on_done=True))\n        self.write_chunks_index()\n        return filepaths"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return []\n\n        filepaths = []\n        while self._should_write():\n            filepaths.append(self.write_chunk())\n\n        if self._serialized_items:\n            filepaths.append(self.write_chunk(on_done=True))\n\n        self.write_chunks_index()\n        self._is_done = True\n        return filepaths"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        # Write the last chunk if it exists\n        if self._serialized_items:\n            self.write_chunk(on_done=True)\n\n        # Write the index\n        self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        # Return the list of file paths to the written chunks\n        return [os.path.join(self._cache_dir, chunk[\"filename\"]) for chunk in self._chunks_info]"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        # Write the last chunk\n        if self._serialized_items:\n            self.write_chunk(on_done=True)\n            self._min_index = None\n            self._max_index = None\n\n        # Write the chunks index\n        self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        # Return the list of file paths to the written chunks\n        return [os.path.join(self._cache_dir, chunk[\"filename\"]) for chunk in self._chunks_info]"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        # Write the last chunk\n        if self._serialized_items:\n            self.write_chunk(on_done=True)\n\n        # Write the chunks index\n        self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        # Return the list of chunk file paths\n        return [os.path.join(self._cache_dir, chunk_info[\"filename\"]) for chunk_info in self._chunks_info]"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return []\n\n        files = []\n        while self._should_write():\n            files.append(self.write_chunk())\n\n        # Write the last chunk\n        if self._serialized_items:\n            files.append(self.write_chunk(on_done=True))\n\n        # Write the index file\n        self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        return files"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return []\n\n        # Write the remaining chunks\n        while self._should_write():\n            self.write_chunk()\n            self._min_index = None\n            self._max_index = None\n\n        # Write the index\n        self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        # Return the list of chunk file paths\n        return [os.path.join(self._cache_dir, chunk[\"filename\"]) for chunk in self._chunks_info]"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        # Write the last chunk if there are any\n        if self._serialized_items:\n            self.write_chunk(on_done=True)\n\n        # Write the chunks index\n        self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        return [os.path.join(self._cache_dir, chunk[\"filename\"]) for chunk in self._chunks_info]"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return []\n\n        if self._should_write():\n            self.write_chunk()\n            self._min_index = None\n            self._max_index = None\n\n        # Write the last chunk\n        if self._serialized_items:\n            self.write_chunk(on_done=True)\n            self._serialized_items = {}\n\n        # Write the index file\n        filepath = self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        return [filepath]"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        # If there are remaining items, write them to the last chunk\n        if self._serialized_items:\n            self.write_chunk(on_done=True)\n\n        # Write the chunks index\n        self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        # Return the list of file paths to the chunks that have been written\n        return [os.path.join(self._cache_dir, chunk[\"filename\"]) for chunk in self._chunks_info]"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return []\n\n        # Write the remaining chunks\n        chunk_files = []\n        while self._should_write():\n            chunk_file = self.write_chunk()\n            chunk_files.append(chunk_file)\n\n        # Write the index\n        self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        return chunk_files\n\n    "}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        # If the writing process is already marked as complete, return an empty list.\n        if self._is_done:\n            return []\n\n        # If there are remaining items in `self._serialized_items`, write them to a chunk.\n        if self._serialized_items:\n            self.write_chunk(on_done=True)\n\n        # Write the index file for the written chunks.\n        self.write_chunks_index()\n\n        # Mark the writing process as complete.\n        self._is_done = True\n\n        # Return a list of file paths to the written chunks.\n        return [os.path.join(self._cache_dir, chunk[\"filename\"]) for chunk in self._chunks_info]\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if not self._is_done:\n            # If there are still items in the cache, write them to a chunk\n            if self._serialized_items:\n                self.write_chunk(on_done=True)\n\n            # Write the chunks index\n            self.write_chunks_index()\n\n            # Mark the writing process as complete\n            self._is_done = True\n\n        # Return the list of file paths to the written chunks\n        return [os.path.join(self._cache_dir, chunk[\"filename\"]) for chunk in self._chunks_info]\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        self.restore = True\n        if isinstance(self.dataset, StreamingDataset):\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        self.restore = True\n\n        if isinstance(self.dataset, StreamingDataset):\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        self.restore = True\n\n        if isinstance(self.dataset, StreamingDataset):\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n        self._worker_idx_iter = iter(self._worker_idx)\n        for _ in range(self._latest_worker_idx):\n            next(self._worker_idx_iter)"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self._latest_worker_idx = obj[\"latest_worker_idx\"]\n\n        if isinstance(self.dataset, StreamingDataset):\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.restore = True"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self.restore = True\n        self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        self._worker_idx = cycle(list(range(self._latest_worker_idx, self.num_workers)))\n        self._worker_idx_iter = iter(self._worker_idx)\n\n        if isinstance(self.dataset, StreamingDataset):\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\")"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n        self._worker_idx_iter = iter(self._worker_idx)\n        self.restore = True\n\n        if isinstance(self.dataset, StreamingDataset):\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        self.restore = True\n        self._worker_idx = cycle(list(range(self._latest_worker_idx, self.num_workers)))\n        self._worker_idx_iter = iter(self._worker_idx)\n        self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n\n        if isinstance(self.dataset, StreamingDataset):\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self.restore = True\n        self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        self._worker_idx = cycle(list(range(self._latest_worker_idx, self.num_workers)))\n        self._worker_idx_iter = iter(self._worker_idx)\n\n        if isinstance(self.dataset, StreamingDataset):\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n\n        self.dataset.set_epoch(self.current_epoch)"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self.restore = True\n        self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        self._worker_idx_iter = iter(cycle(list(range(self._latest_worker_idx, self.num_workers))))\n        self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n\n        if isinstance(self.dataset, StreamingDataset):\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        self.restore = True\n\n        if isinstance(self.dataset, StreamingDataset):\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\")\n\n        self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n        self._worker_idx_iter = iter(self._worker_idx)\n\n        for _ in range(self._latest_worker_idx):\n            self._latest_worker_idx = next(self._worker_idx_iter)  # type: ignore"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        self.restore = True\n        self._worker_idx_iter = cycle(list(range(self._latest_worker_idx, self.num_workers)))\n\n        if isinstance(self.dataset, StreamingDataset):\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.dataset.set_epoch(self.current_epoch)"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        self.restore = True\n        if isinstance(self.dataset, StreamingDataset):\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self._worker_idx_iter = cycle(list(range(self._latest_worker_idx, self.num_workers)))\n        self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self._latest_worker_idx = obj[\"latest_worker_idx\"]\n\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(\n                obj[\"dataset\"], self._num_samples_yielded_streaming, self.num_workers, self.batch_size\n            )\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(\n                obj[\"dataset\"], self.num_workers, self.batch_size, obj[\"num_samples_yielded\"]\n            )\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.restore = True"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        self.restore = True\n        if isinstance(self.dataset, StreamingDataset):\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\")\n\n        self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n        self._worker_idx_iter = iter(self._worker_idx)\n        for _ in range(self._latest_worker_idx):\n            next(self._worker_idx_iter)\n\n        self._worker_idx_iter = iter(self._worker_idx)\n        self.dataset.set_epoch(self.current_epoch)"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        self.restore = True\n\n        if isinstance(self.dataset, StreamingDataset):\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n\n        self._worker_idx = cycle(list(range(self._latest_worker_idx, self.num_workers)))\n        self._worker_idx_iter = iter(self._worker_idx)\n\n        if self._profile_batches and self.num_workers > 0:\n            from viztracer import VizTracer\n\n            tracer = VizTracer(output_file=os.path.join(self._profile_dir, \"result.json\"))\n            tracer.start()"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self.restore = True\n        self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        self._worker_idx_iter = cycle(list(range(self._latest_worker_idx, self.num_workers)))\n\n        if isinstance(self.dataset, StreamingDataset):\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.dataset.set_epoch(self.current_epoch)\n\n        return"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.restore = True\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.restore = True\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        if self.rng_state is not None:\n            self.dataset.set_rng_state(self.rng_state)\n\n        if self.num_workers > 0:\n            self._worker_idx = cycle(list(range(self.num_workers)))\n            self._worker_idx_iter = iter(self._worker_idx)\n            for _ in range(self._latest_worker_idx):\n                next(self._worker_idx_iter)"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self._latest_worker_idx = obj[\"latest_worker_idx\"]\n\n        if isinstance(self.dataset, StreamingDataset):\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.restore = True\n\n        if isinstance(self.dataset, StreamingDataset):\n            self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n            self._worker_idx_iter = iter(self._worker_idx)\n        else:\n            self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n            self._worker_idx_iter = iter(self._worker_idx)\n\n        self.current_epoch += 1"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            assert self.batch_size\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(\n                obj[\"dataset\"], self._num_samples_yielded_streaming, self.num_workers, self.batch_size\n            )\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            assert self.batch_size\n            self.current_epoch = obj[\"current_epoch\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(\n                obj[\"dataset\"], self.num_workers, self.batch_size, self._num_samples_yielded_combined\n            )\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.restore = True\n\n        self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n        self._worker_idx_iter = iter(self._worker_idx)\n        for _ in range(self._latest_worker_idx):\n            next(self._worker_idx_iter)\n\n        if self.num_workers > 0:\n            self._worker_idx_iter = None\n\n        self.dataset.set_epoch(self.current_epoch)"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self._latest_worker_idx = obj[\"latest_worker_idx\"]\n\n        if isinstance(self.dataset, StreamingDataset):\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.restore = True\n        self._worker_idx_iter = cycle(list(range(self._latest_worker_idx, self.num_workers)))\n        self._worker_idx = cycle(list(range(self._latest_worker_idx, self.num_workers)))\n\n        # TODO: Find a better way to handle the case where the user wants to resume the training\n        #       from a specific worker and not from the beginning.\n        #       This is a hack to make sure that the worker_idx is updated.\n        for _ in range(self._latest_worker_idx):\n            next(self._worker_idx_iter)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            return _state_dict(self._datasets, num_samples_yielded, num_workers, batch_size)\n\n        return self._iterator.state_dict(num_workers, batch_size)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            return _state_dict(self._datasets, num_samples_yielded, num_workers, batch_size)\n\n        return self._iterator.state_dict(num_workers, batch_size)\n\n    "}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is None:\n            return _state_dict(self._datasets, num_samples_yielded, num_workers, batch_size)\n\n        return self._iterator.state_dict(num_workers, batch_size)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is None:\n            return _state_dict(self._datasets, num_samples_yielded, num_workers, batch_size)\n\n        return self._iterator.state_dict(num_workers, batch_size)\n\n    "}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is None:\n            assert num_samples_yielded is not None\n            return {\n                \"dataset\": _state_dict(self._datasets, num_samples_yielded, num_workers, batch_size),\n                \"num_samples_yielded\": num_samples_yielded,\n            }\n\n        return self._iterator.state_dict(num_workers, batch_size)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is None:\n            assert num_samples_yielded is not None\n            return _state_dict(self._datasets, num_samples_yielded, num_workers, batch_size)\n\n        return self._iterator.state_dict(num_workers, batch_size)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is not None:\n            return self._iterator.state_dict(num_workers, batch_size)\n\n        return _state_dict(self._datasets, self._num_samples_yielded, num_workers, batch_size)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is not None:\n            return self._iterator.state_dict(num_workers=num_workers, batch_size=batch_size)\n\n        return _state_dict(self._datasets, self._num_samples_yielded, num_workers, batch_size)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            else:\n                return _state_dict(self._datasets, num_samples_yielded, num_workers, batch_size)\n\n        if self._use_streaming_dataloader:\n            return self._iterator.state_dict(num_workers, batch_size)\n\n        return {\n            \"dataset\": self._iterator.state_dict(num_workers, batch_size),\n            \"num_samples_yielded\": self._iterator._num_samples_yielded,\n        }"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            return _state_dict(self._datasets, num_samples_yielded, num_workers, batch_size)\n\n        return self._iterator.state_dict(num_workers=num_workers, batch_size=batch_size)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is not None:\n            return self._iterator.state_dict(num_workers=num_workers, batch_size=batch_size)\n\n        return _state_dict(self._datasets, num_samples_yielded, num_workers, batch_size)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is not None:\n            return self._iterator.state_dict(num_workers, batch_size)\n\n        return {\n            \"dataset\": _state_dict(self._datasets, self._num_samples_yielded, num_workers, batch_size),\n            \"num_samples_yielded\": self._num_samples_yielded,\n        }\n\n    "}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            return {}\n\n        if num_samples_yielded is None:\n            num_samples_yielded = self._iterator._num_samples_yielded\n\n        if self._iterator._use_streaming_dataloader:\n            return {\n                \"dataset\": _state_dict(self._datasets, num_samples_yielded, num_workers, batch_size),\n                \"num_samples_yielded\": num_samples_yielded,\n            }\n\n        return self._iterator.state_dict(num_workers, batch_size)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            return {}\n\n        if num_samples_yielded is not None:\n            return {\n                \"dataset\": _state_dict(self._datasets, num_samples_yielded, num_workers, batch_size),\n                \"num_samples_yielded\": num_samples_yielded,\n            }\n\n        return self._iterator.state_dict(num_workers, batch_size)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            else:\n                return _state_dict(self._datasets, num_samples_yielded, num_workers, batch_size)\n\n        if self._use_streaming_dataloader:\n            return self._iterator.state_dict()\n        else:\n            return _state_dict(self._datasets, self._iterator._num_samples_yielded, num_workers, batch_size)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is not None:\n            return self._iterator.state_dict(num_workers, batch_size)\n\n        assert num_samples_yielded is not None\n\n        return {\n            \"dataset\": _state_dict(self._datasets, num_samples_yielded, num_workers, batch_size),\n            \"num_samples_yielded\": num_samples_yielded,\n        }"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            return {}\n\n        if self._use_streaming_dataloader:\n            return self._iterator.state_dict(num_workers=num_workers, batch_size=batch_size)\n\n        return {\n            \"dataset\": _state_dict(self._datasets, self._num_samples_yielded, num_workers, batch_size),\n            \"num_samples_yielded\": self._num_samples_yielded,\n        }\n\n    "}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is not None:\n            return self._iterator.state_dict(num_workers=num_workers, batch_size=batch_size)\n\n        assert num_samples_yielded is not None\n        return _state_dict(self._datasets, num_samples_yielded, num_workers, batch_size)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            else:\n                return _state_dict(self._datasets, num_samples_yielded, num_workers, batch_size)\n\n        if self._use_streaming_dataloader:\n            return self._iterator.state_dict(num_workers, batch_size)\n        else:\n            return self._iterator.state_dict()"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            return {}\n\n        if num_samples_yielded is None:\n            num_samples_yielded = self._iterator._num_samples_yielded\n\n        if self._use_streaming_dataloader:\n            return {\n                \"dataset\": self._iterator.state_dict(num_workers=num_workers, batch_size=batch_size),\n                \"num_samples_yielded\": num_samples_yielded,\n            }\n\n        return {\n            \"dataset\": _state_dict(self._datasets, self._iterator._num_samples_yielded, num_workers, batch_size),\n            \"num_samples_yielded\": num_samples_yielded,\n        }\n\n    "}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self._num_samples_yielded = state_dict.pop(__NUM_SAMPLES_YIELDED_KEY__, None)\n        for dataset_idx, dataset in enumerate(self._datasets):\n            dataset.load_state_dict(state_dict[str(dataset_idx)])"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self._num_samples_yielded = state_dict.pop(__NUM_SAMPLES_YIELDED_KEY__, None)\n        for dataset_idx, dataset in enumerate(self._datasets):\n            dataset.load_state_dict(state_dict[str(dataset_idx)])"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if self._iterator is not None:\n            self._iterator.load_state_dict(state_dict)\n        else:\n            self._num_samples_yielded = num_samples_yielded"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if self._iterator is None:\n            self._num_samples_yielded = num_samples_yielded\n            return\n        self._iterator.load_state_dict(state_dict)\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self._num_samples_yielded = state_dict.pop(__NUM_SAMPLES_YIELDED_KEY__, None)\n\n        for dataset_idx, dataset in enumerate(self._datasets):\n            dataset.load_state_dict(state_dict[str(dataset_idx)])\n\n        self._iterator = None"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self._num_samples_yielded = state_dict.pop(__NUM_SAMPLES_YIELDED_KEY__, None)\n        for dataset_idx, dataset in enumerate(self._datasets):\n            dataset.load_state_dict(state_dict[str(dataset_idx)])\n\n        if self._iterator is not None:\n            self._iterator.load_state_dict(state_dict)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        assert self._iterator is not None\n        self._iterator.load_state_dict(state_dict)\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self._num_samples_yielded = num_samples_yielded\n        for dataset, dataset_state_dict in state_dict.items():\n            self._datasets[int(dataset)].load_state_dict(dataset_state_dict)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if self._iterator is None:\n            self._num_samples_yielded = num_samples_yielded\n            return\n\n        self._iterator.load_state_dict(state_dict)\n        self._num_samples_yielded = self._iterator._num_samples_yielded"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self._num_samples_yielded = state_dict\n\n        for dataset_idx, dataset in enumerate(self._datasets):\n            if dataset_idx in state_dict:\n                dataset.load_state_dict(state_dict[dataset_idx])\n\n        if self._iterator is not None:\n            self._iterator.load_state_dict(state_dict)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if self._iterator is None:\n            self._iterator = _CombinedDatasetIterator(\n                self._datasets, self._seed, self._weights, self._use_streaming_dataloader, num_samples_yielded\n            )\n        self._iterator.load_state_dict(state_dict)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self._num_samples_yielded = _state_dict(self._datasets, num_samples_yielded)\n        for dataset in self._datasets:\n            dataset.load_state_dict(state_dict[str(dataset)])\n\n        if self._iterator is not None:\n            self._iterator.load_state_dict(state_dict)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self._num_samples_yielded = state_dict.get(__NUM_SAMPLES_YIELDED_KEY__)\n        if self._num_samples_yielded is None:\n            return\n\n        for dataset_idx, dataset in enumerate(self._datasets):\n            dataset.load_state_dict(state_dict[str(dataset_idx)])\n\n        if self._iterator is not None:\n            self._iterator._num_samples_yielded = self._num_samples_yielded"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if self._iterator is not None:\n            self._iterator.load_state_dict(state_dict)\n        else:\n            self._num_samples_yielded = num_samples_yielded\n\n        if num_samples_yielded is not None:\n            for dataset, num_samples in zip(self._datasets, num_samples_yielded):\n                dataset.set_num_samples_yielded(num_samples)\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if self._iterator is not None:\n            self._iterator.load_state_dict(state_dict)\n            return\n\n        # update the state of each dataset\n        for dataset_idx, dataset in enumerate(self._datasets):\n            dataset.load_state_dict(state_dict[str(dataset_idx)])\n\n        # update the number of samples yielded\n        if num_samples_yielded is not None:\n            self._num_samples_yielded = num_samples_yielded\n\n        # update the iterator\n        self._iterator = _CombinedDatasetIterator(\n            self._datasets,\n            self._seed,\n            self._weights,\n            self._use_streaming_dataloader,\n            self._num_samples_yielded,\n        )"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if __NUM_SAMPLES_YIELDED_KEY__ in state_dict:\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n        else:\n            self._num_samples_yielded = None\n\n        if __SAMPLES_KEY__ in state_dict:\n            for dataset_idx, dataset in enumerate(self._datasets):\n                dataset.load_state_dict(state_dict[__SAMPLES_KEY__][str(dataset_idx)])\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        self._num_samples_yielded = state_dict\n\n        # Load the state of each dataset in the CombinedStreamingDataset\n        for dataset_idx, dataset in enumerate(self._datasets):\n            dataset.load_state_dict(state_dict[str(dataset_idx)])\n\n        # Update the number of samples yielded by the streaming dataloader to avoid repeating samples\n        for dataset_idx, dataset in enumerate(self._datasets):\n            dataset.update_num_samples_yielded(state_dict[str(dataset_idx)][__NUM_SAMPLES_YIELDED_KEY__])"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if self._iterator is not None:\n            self._iterator.load_state_dict(state_dict)\n            self._num_samples_yielded = self._iterator._num_samples_yielded\n        else:\n            self._num_samples_yielded = num_samples_yielded\n\n        for dataset_idx, dataset in enumerate(self._datasets):\n            if str(dataset_idx) in state_dict:\n                dataset.load_state_dict(state_dict[str(dataset_idx)])\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        worker_env = _WorkerEnv.detect()\n\n        if not self._iterator:\n            self._iterator = _CombinedDatasetIterator(\n                self._datasets, self._seed, self._weights, self._use_streaming_dataloader\n            )\n\n        self._iterator.load_state_dict(state_dict[str(worker_env.rank)])\n\n        if self._use_streaming_dataloader:\n            self._num_samples_yielded = state_dict[str(worker_env.rank)][__NUM_SAMPLES_YIELDED_KEY__]"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if self._iterator is None:\n            self._num_samples_yielded = num_samples_yielded\n            return\n\n        self._iterator.load_state_dict(state_dict)\n        self._num_samples_yielded = self._iterator._num_samples_yielded\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if isinstance(dir_path, str):\n        if dir_path.startswith(\"s3://\"):\n            return Dir(path=dir_path, url=dir_path)\n\n        if dir_path.startswith(\"data-connections://\"):\n            return _resolve_s3_connections(dir_path)\n\n        if dir_path.startswith(\"datasets://\"):\n            return _resolve_datasets(dir_path)\n\n        if dir_path.startswith(\"studio://\"):\n            return _resolve_studio(dir_path, None, None)\n\n        if dir_path.startswith(\"studio://\"):\n            return _resolve_studio(dir_path, None, None)\n\n        if dir_path.startswith(\"studio://\"):\n            return _resolve_studio(dir_path, None, None)\n\n        if dir_path.startswith(\"studio://\"):\n            return _resolve_studio(dir_path, None, None)\n\n        if dir_path.startswith(\"studio://\"):\n            return _resolve_studio(dir_path, None, None)\n\n        if dir_path.startswith(\"studio://\"):\n            return _resolve_studio(dir_path, None, None)\n\n        if dir_path.startswith(\"studio://\"):\n            return _resolve_studio(dir_path, None, None)\n\n        if dir_path.startswith(\"studio://\"):\n            return _resolve_studio(dir_path, None, None)\n\n        if dir_path.startswith(\"studio://\"):\n            return _resolve_studio(dir_path, None, None)\n\n        if dir_path.startswith(\"studio://\"):\n            return _resolve_studio(dir_path, None, None)\n\n        if dir_path.startswith(\"st"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir()\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return _resolve_lightning(dir_path)\n\n    if dir_path.startswith(\"datasets://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"connections://\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"projects://\"):\n        return _resolve_projects(dir_path)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"s3a://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"s3n://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"s3n://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"s3a://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"gs://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"gcs://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"wasb://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"wasbs://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"wasb://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"wasbs://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"azure://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"abfs://\"):\n        return Dir"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if not isinstance(dir_path, str):\n        raise TypeError(\"The provided dir_path should be a string.\")\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return _resolve_lightning(dir_path)\n\n    if dir_path.startswith(\"s3-connections://\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"datasets://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"gs://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"file://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"~\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.start"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return _resolve_lightning_cloud(dir_path)\n\n    if dir_path.startswith(\"datasets://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"data_connections://\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"gs://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"https://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"http://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"~\"):\n        return Dir(path=os.path.expanduser(dir_path))\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.abspath(dir_path))\n\n    if dir_path.startswith(\"../\"):\n        return Dir(path=os.path.abspath(dir_path))\n\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n\n    if dir_"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if isinstance(dir_path, Path):\n        dir_path = str(dir_path)\n\n    if not isinstance(dir_path, str):\n        raise ValueError(\"The provided dir_path should be either a string or a Dir object.\")\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"s3-connections://\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"datasets://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"lightning://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"lightning://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"lightning://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"lightning://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"lightning://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"lightning://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"lightning://\"):\n        return _resolve_studio(dir_path, None, None"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    dir_path = _resolve_time_template(dir_path)\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"datasets://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"data-connections://\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"studio://\"):\n        target_name, *target_id = dir_path.split(\"/\")[3:]\n        target_id = \"/\".join(target_id)\n        return _resolve_studio(dir_path, target_name, target_id)\n\n    return Dir(path=dir_path)"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if not isinstance(dir_path, str):\n        raise ValueError(f\"The provided dir_path should be a string or a Dir object. Found {dir_path}.\")\n\n    dir_path = _resolve_time_template(dir_path)\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"datasets://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"connections://\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, target_name=None, target_id=None)\n\n    if dir_path.startswith(\"studio://\"):\n        target_name = dir_path.split(\"/\")[2]\n        return _resolve_studio(dir_path, target_name=target_name, target_id=None)\n\n    if dir_path.startswith(\"studio://\"):\n        target_id = dir_path.split(\"/\")[2]\n        return _resolve_studio(dir_path, target_name=None, target_id=target_id)\n\n    if dir_path.startswith(\"studio://\"):\n        target_name = dir_path.split(\"/\")[2]\n        target_id = dir_path.split(\"/\")[2]\n        return _resolve_studio(dir_path, target_name=target_name, target_id=target_id)\n\n    return Dir(path=dir_path, url=dir_path)"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"s3-connections://\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"datasets://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"datasets-connections://\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"datasets-datasets://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"datasets-models://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"datasets-predictions://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"datasets-predictions-predictions://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"datasets-predictions-predictions-predictions://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"datasets-predictions-predictions-predictions-predictions://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"datasets-predictions-predictions-predictions-predictions-predictions://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"datasets-predictions"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"lightning://datasets/\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"lightning://studio/\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"lightning://dataconnections/\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        raise ValueError(\n            f\"The provided dir_path `{dir_path}` is not supported. Supported paths are `s3://`, `lightning://datasets/`, `lightning://studio/`, `lightning://dataconnections/`.\"\n        )\n\n    if Path(dir_path).is_absolute():\n        return Dir(path=dir_path)\n\n    return Dir(path=os.path.join(os.getcwd(), dir_path))"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    dir_path = str(dir_path)\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return _resolve_lightning(dir_path)\n\n    if dir_path.startswith(\"s3-connections://\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"datasets://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"studio://name=\"):\n        target_name = dir_path.split(\"studio://name=\")[1].split(\"/\")[0]\n        return _resolve_studio(dir_path, target_name, None)\n\n    if dir_path.startswith(\"studio://id=\"):\n        target_id = dir_path.split(\"studio://id=\")[1].split(\"/\")[0]\n        return _resolve_studio(dir_path, None, target_id)\n\n    if dir_path.startswith(\"lightning-project://\"):\n        return _resolve_project(dir_path)\n\n    if dir_path.startswith(\"lightning-project://name=\"):\n        target_name = dir_path.split(\"lightning-project://name=\")[1].split(\"/\")[0]\n        return _resolve_project(dir_path, target_name, None)\n\n    if dir_path.startswith(\"lightning-project://id=\"):\n        target_id = dir_path.split(\"lightning-project://id=\")[1].split(\"/\")[0]\n        return _resolve_project("}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"s3-access-key://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"s3-secret-key://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"s3-session-token://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"s3-region://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"s3-endpoint-url://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"s3-profile://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"s3-extra-args://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"s3-anonymous://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"s3-expiration://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"s3-skip-check-bucket://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"s3-requester-pays://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"s3-"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir(path=dir_path, url=dir_path)\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    dir_path = _resolve_time_template(dir_path)\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"data://\"):\n        return _resolve_data_connections(dir_path)\n\n    if dir_path.startswith(\"dataset://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"projects/\"):\n        return _resolve_studio(dir_path, target_name=None, target_id=None)\n\n    if dir_path.startswith(\"projects/\"):\n        return _resolve_studio(dir_path, target_name=None, target_id=None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, target_name=None, target_id=None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, target_name=None, target_id=None)\n\n    if dir_path.startswith(\"lightning://\"):\n        return _resolve_studio(dir_path, target_name=None, target_id=None)\n\n    if dir_path.startswith(\"lightning://\"):\n        return _resolve_studio(dir_path, target_name=None, target_id=None)\n\n    if dir_path.startswith(\"lightning://\"):\n        return _resolve_studio(dir_path, target_name=None, target_id=None)\n\n    if dir_path.startswith(\"lightning://\"):\n        return _resolve_studio(dir_path, target_name=None, target_id=None)\n\n    if dir_path"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"datasets://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"connections://\"):\n        return _resolve_s3_connections(dir_path)\n\n    # Check if the path is a local path\n    if os.path.exists(dir_path):\n        return Dir(path=dir_path)\n\n    # Check if the path is a local path\n    if Path(dir_path).is_absolute():\n        return Dir(path=dir_path)\n\n    # Check if the path is a local path\n    if Path(dir_path).is_relative_to(Path.cwd()):\n        return Dir(path=dir_path)\n\n    # Check if the path is a local path\n    if Path(dir_path).is_relative_to(Path.home()):\n        return Dir(path=dir_path)\n\n    # Check if the path is a local path\n    if Path(dir_path).is_relative_to(Path.home() / \".lightning\"):\n        return Dir(path=dir_path)\n\n    # Check if the path is a local path\n    if Path(dir_path).is_relative_to(Path.home() / \".lightning-datasets\"):\n        return Dir(path=dir_path)\n\n    # Check if the path is a local path\n    if Path(dir_path).is_relative_to(Path.home() / \".lightning-connections\"):\n        return Dir(path=dir_path)\n\n    # Check if the path is a local path\n    if Path(dir_path).is"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if not isinstance(dir_path, str):\n        raise ValueError(f\"The provided dir_path should be a string or a Dir object. Found {dir_path}.\")\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"datasets://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"s3-connections://\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"projects://\"):\n        return _resolve_projects(dir_path)\n\n    if dir_path.startswith(\"~\"):\n        return Dir(path=os.path.expanduser(dir_path))\n\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=os.path.abspath(dir_path))\n\n    if dir_path.startswith(\"../\"):\n        return Dir(path=os.path.abspath(dir_path))\n\n    if dir_path.startswith(\".\"):\n        return Dir(path=os.path.abspath(dir_path))\n\n    if dir_path.startswith(\"lightning://\"):\n        return Dir(path=os.path.abspath(dir_path.replace(\"lightning://\", \"\")))\n\n    if dir_path.startswith(\"https://\"):\n        return Dir(path=os.path.abspath(dir_path))\n\n    if dir_path.startswith(\"http://\"):\n       "}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    # If the path is a local path\n    if os.path.exists(dir_path):\n        return Dir(path=dir_path)\n\n    # If the path is a S3 URL\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    # If the path is a local path with a prefix\n    if dir_path.startswith(\"datasets/\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"studio/\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"connections/\"):\n        return _resolve_s3_connections(dir_path)\n\n    # If the path is a local path with a prefix\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    # If the path is a local path with a prefix\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    # If the path is a local path with a prefix\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    # If the path is a local path with a prefix\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    # If the path is a local path with a prefix\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    # If the path is a local path with a prefix\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path, url="}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir()\n\n    dir_path = str(dir_path)\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"datasets://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"s3-connections://\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"projects://\"):\n        return _resolve_project(dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return _resolve_lightning(dir_path)\n\n    if dir_path.startswith(\"lightning-datasets://\"):\n        return _resolve_lightning_datasets(dir_path)\n\n    if dir_path.startswith(\"lightning-studio://\"):\n        return _resolve_lightning_studio(dir_path)\n\n    if dir_path.startswith(\"lightning-connections://\"):\n        return _resolve_lightning_connections(dir_path)\n\n    if dir_path.startswith(\"lightning-projects://\"):\n        return _resolve_lightning_projects(dir_path)\n\n    if dir_path.startswith(\"~/\"):\n        return Dir(path=Path.home().joinpath(dir_path[2:]))\n\n    return Dir(path=dir_path)\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    # Check if the path starts with a prefix\n    prefixes = [\"s3://\", \"lightning://\", \"data://\"]\n    for prefix in prefixes:\n        if dir_path.startswith(prefix):\n            return _resolve_dir(dir_path[len(prefix) :])\n\n    # Check if the path is a local path\n    if Path(dir_path).exists():\n        return Dir(path=dir_path)\n\n    # Check if the path is a S3 URL\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    # Check if the path is a project path\n    if dir_path.startswith(\"lightning://\"):\n        return _resolve_project(dir_path)\n\n    # Check if the path is a data connection path\n    if dir_path.startswith(\"data://\"):\n        return _resolve_s3_connections(dir_path)\n\n    # Check if the path is a dataset path\n    if dir_path.startswith(\"datasets://\"):\n        return _resolve_datasets(dir_path)\n\n    raise ValueError(f\"The provided directory path `{dir_path}` is not supported.\")\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path, url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return _resolve_lightning_cloud(dir_path)\n\n    if dir_path.startswith(\"s3-dataset://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"s3-connection://\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path)\n\n    if dir_path.startswith(\"{%\"):\n        dir_path = _resolve_time_template(dir_path)\n\n    if Path(dir_path).exists():\n        return Dir(path=dir_path)\n\n    raise ValueError(\n        f\"The provided directory path `{dir_path}` is not a valid local path, s3, s3-dataset, s3-connection, studio, or lightning path.\"\n    )\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    dir_path = _resolve_time_template(dir_path)\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"s3-connections://\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"datasets://\"):\n        return _resolve_datasets(dir_path)\n\n    # If the dir_path is a local path, we can't resolve it to a URL.\n    # We'll just return the Dir object with the path set.\n    return Dir(path=dir_path)\n\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    # No files are found in this folder\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    # Check the index file exists\n    try:\n        s3.head_object(Bucket=obj.netloc, Key=os.path.join(prefix, \"index.json\"))\n        has_index_file = True\n    except botocore.exceptions.ClientError:\n        has_index_file = False\n\n    if has_index_file:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an optimized immutable datasets.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    bucket_name = obj.netloc\n    s3 = boto3.resource(\"s3\")\n    for obj in s3.Bucket(bucket_name).objects.filter(Prefix=prefix):\n        s3.Object(bucket_name, obj.key).delete()"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    # No files are found in this folder\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    # Check the index file exists\n    try:\n        s3.head_object(Bucket=obj.netloc, Key=os.path.join(prefix, \"index.json\"))\n        has_index_file = True\n    except botocore.exceptions.ClientError:\n        has_index_file = False\n\n    if has_index_file:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an optimized immutable datasets.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    bucket_name = obj.netloc\n    s3 = boto3.resource(\"s3\")\n    for obj in s3.Bucket(bucket_name).objects.filter(Prefix=prefix):\n        s3.Object(bucket_name, obj.key).delete()"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data. HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data. \"\n            \"HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` is not empty.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    if objects[\"KeyCount\"] != 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    if append:\n        raise NotImplementedError(\"Appending data is not supported yet.\")\n\n    if overwrite:\n        raise NotImplementedError(\"Overwriting data is not supported yet.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` is not empty. HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    # No files are found in this folder\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    if append:\n        raise NotImplementedError(\"Append is not supported yet.\")\n\n    if overwrite:\n        raise NotImplementedError(\"Overwrite is not supported yet.\")\n\n    raise RuntimeError(\n        f\"The provided output_dir `{output_dir.path}` is not empty. HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n    )"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    # No files are found in this folder\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    if append:\n        raise NotImplementedError(\"Appending to a folder is not supported yet.\")\n\n    if overwrite:\n        raise NotImplementedError(\"Overwriting a folder is not supported yet.\")\n\n    raise RuntimeError(\n        f\"The provided output_dir `{output_dir.path}` already contains data. HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n    )"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    # No files are found in this folder\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    if append or overwrite:\n        raise NotImplementedError(\"`append` and `overwrite` are not implemented yet.\")\n\n    raise RuntimeError(\n        f\"The provided output_dir `{output_dir.path}` is not empty. HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n    )"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    if append:\n        raise NotImplementedError(\"Appending to an S3 folder is not supported yet.\")\n\n    if overwrite:\n        raise NotImplementedError(\"Overwriting an S3 folder is not supported yet.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    # No files are found in this folder\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    if append or overwrite:\n        raise NotImplementedError(\"Currently, appending or overwriting data in the directory is not supported.\")\n\n    raise RuntimeError(\n        f\"The provided output_dir `{output_dir.path}` is not empty. HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n    )"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    if append or overwrite:\n        raise NotImplementedError(\"Overwriting and appending are not supported yet.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` is not empty. HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    if append:\n        raise NotImplementedError(\"Appending data is not yet supported.\")\n\n    if overwrite:\n        raise NotImplementedError(\"Overwriting data is not yet supported.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    # No files are found in this folder\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    # Check the index file exists\n    try:\n        s3.head_object(Bucket=obj.netloc, Key=os.path.join(prefix, \"index.json\"))\n        has_index_file = True\n    except botocore.exceptions.ClientError:\n        has_index_file = False\n\n    if has_index_file:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an optimized immutable datasets.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    bucket_name = obj.netloc\n    s3 = boto3.resource(\"s3\")\n    for obj in s3.Bucket(bucket_name).objects.filter(Prefix=prefix):\n        s3.Object(bucket_name, obj.key).delete()"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    # No files are found in this folder\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    if append:\n        raise NotImplementedError(\"Append mode is not yet supported.\")\n\n    if overwrite:\n        raise NotImplementedError(\"Overwrite mode is not yet supported.\")\n\n    raise RuntimeError(\n        f\"The provided output_dir `{output_dir.path}` already contains data.\"\n        \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n    )"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    if append:\n        raise NotImplementedError(\"Appending data to the output_dir is not supported yet.\")\n\n    if overwrite:\n        raise NotImplementedError(\"Overwriting data in the output_dir is not supported yet.\")"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    try:\n        s3.get_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/index.json\")\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n            return\n\n    raise RuntimeError(\n        f\"The provided output_dir `{output_dir.path}` already contains an index file and datasets are meant to be immutable.\"\n        \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n    )"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    try:\n        s3.get_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/index.json\")\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n            return\n        raise e\n\n    # We aren't alloweing to add more data\n    # TODO: Add support for `append` and `overwrite`.\n    raise RuntimeError(\n        f\"The provided output_dir `{output_dir.path}` already contains an index file and datasets are meant to be immutable.\"\n        \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n    )"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    try:\n        s3.get_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/index.json\")\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n            return\n        raise e\n\n    raise RuntimeError(\n        f\"The provided output_dir `{output_dir.path}` already contains an index.json file.\"\n        \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n    )"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # We aren't alloweing to add more data\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    # We aren't alloweing to add more data\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    # We aren't allowing to add more data\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    # We aren't allowing to add more data\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be imm"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    try:\n        s3.get_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/index.json\")\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n            return\n        raise e\n\n    # Delete all objects in the bucket\n    s3.delete_objects(\n        Bucket=obj.netloc,\n        Delete={\n            \"Objects\": [{\"Key\": obj[\"Key\"]} for obj in s3.list_objects_v2(Bucket=obj.netloc, Prefix=obj.path.lstrip(\"/\").rstrip(\"/\"))[\"Contents\"]]\n        },\n    )"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # We aren't alloweing to add more data\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    try:\n        s3.get_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\") + \"/index.json\")\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n            # Delete all objects in the bucket\n            s3.delete_objects(\n                Bucket=obj.netloc,\n                Delete={\n                    \"Objects\": [{\"Key\": obj[\"Key\"]} for obj in s3.list_objects_v2(Bucket=obj.netloc)[\"Contents\"]],\n                },\n            )\n        else:\n            raise e"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # We aren't alloweing to add more data\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    try:\n        s3.get_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\") + \"/index.json\")\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an index file. Datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n            return\n        raise e"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # We aren't alloweing to add more data\n    # TODO: Add support for `append` and `overwrite`.\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    # Check if the index file exists\n    try:\n        s3.get_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/index.json\")\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n            # The index file does not exist, so we can continue\n            return\n        else:\n            # An error occurred while trying to get the index file\n            raise e\n\n    # The index file exists, so we need to delete all objects within the specified prefix\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n    for obj in objects[\"Contents\"]:\n        s3.delete_object(Bucket="}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # We aren't alloweing to add more data\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    try:\n        s3.get_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/index.json\")\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n            # Delete all objects\n            s3.delete_objects(\n                Bucket=obj.netloc,\n                Delete={\"Objects\": [{\"Key\": obj[\"Key\"]} for obj in objects[\"Contents\"]]},\n            )\n            return\n        else:\n            raise e\n\n    raise RuntimeError(\n        f\"The provided output_dir `{output_dir.path}` already contains an index file and datasets are meant to be immutable.\"\n        \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n    )"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    try:\n        s3.head_object(Bucket=obj.netloc, Key=os.path.join(obj.path.lstrip(\"/\"), \"index.json\"))\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"404\":\n            return\n        raise\n\n    # TODO: Add support for `append` and `overwrite`.\n    raise RuntimeError(\n        f\"The provided output_dir `{output_dir.path}` already contains an index file and datasets are meant to be immutable.\"\n        \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n    )"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # We aren't alloweing to add more data\n    # TODO: Add support for `append` and `overwrite`.\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    # If an index file is not found, delete all objects within the specified prefix in the bucket.\n    try:\n        s3.get_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\") + \"index.json\")\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n            s3.delete_objects(\n                Bucket=obj.netloc,\n                Delete={\n                    \"Objects\": [{\"Key\": obj[\"Key\"]} for obj in objects[\"Contents\"]],\n                    \"Quiet\": True,\n                },\n            )\n        else:\n            raise e"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    # Check if the index file exists\n    try:\n        s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\") + \"index.json\")\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an index file and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"404\":\n            # The index file does not exist, so we can continue\n            pass\n        else:\n            # Some other error occurred\n            raise\n\n    # Delete all objects within the specified prefix\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if objects[\"KeyCount\"] > 0:\n        s3.delete_objects(\n            Bucket=obj.netloc,\n            Delete={\n                \"Objects\": [{\"Key\": obj[\"Key\"]} for obj in objects[\"Contents\"]],\n                \"Quiet\": True,\n            },\n        )"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    try:\n        s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/index.json\")\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an index.json file and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] != \"404\":\n            raise e\n\n    # Delete all objects in the bucket\n    response = s3.list_objects_v2(Bucket=obj.netloc, Prefix=obj.path.lstrip(\"/\").rstrip(\"/\"))\n    if \"Contents\" in response:\n        for obj in response[\"Contents\"]:\n            s3.delete_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\").rstrip(\"/\"))"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    try:\n        s3.get_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/index.json\")\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n            # If the index file does not exist, delete all objects within the specified prefix in the bucket.\n            s3.delete_objects(\n                Bucket=obj.netloc,\n                Delete={\n                    \"Objects\": [{\"Key\": obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\" + obj[\"Key\"]} for obj in s3.list_objects(Bucket=obj.netloc, Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\")[\"Contents\"]]\n                },\n            )\n        else:\n            raise e"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    try:\n        s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/index.json\")\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"404\":\n            return\n\n        raise e\n\n    raise RuntimeError(\n        f\"The provided output_dir `{output_dir.path}` already contains an index file. Please provide an empty directory.\"\n    )"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    # Check if the index file exists\n    try:\n        s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/index.json\")\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an index file and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"404\":\n            # The index file does not exist, so we can continue\n            return\n        else:\n            # An error occurred while checking for the index file\n            raise e\n\n    # Delete all objects within the specified prefix\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    for obj in objects[\"Contents\"]:\n        s3.delete_object(Bucket=obj.netloc, Key=obj.key)"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # We aren't alloweing to add more data\n    # TODO: Add support for `append` and `overwrite`.\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    if \"Contents\" not in objects:\n        return\n\n    for obj in objects[\"Contents\"]:\n        if obj[\"Key\"].endswith(\"index.json\"):\n            raise RuntimeError(\n                f\"The provided output_dir `{output_dir.path}` already contains an index file and datasets are meant to be immutable.\"\n                \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n            )\n\n    # We need to delete the objects in the bucket\n    # TODO: Add support for `append` and `overwrite`.\n    s3.delete_objects(\n        Bucket=obj.netloc,\n        Delete={\"Objects\": [{\"Key\": obj[\"Key\"]} for obj in objects[\"Contents\"]]},\n    )"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    # Check if the output_dir is an S3 bucket\n    if not output_dir.url.startswith(\"s3://\"):\n        return\n\n    # Check if the index file exists\n    index_file_path = os.path.join(output_dir.url, \"index.json\")\n    try:\n        s3 = boto3.client(\"s3\")\n        s3.head_object(Bucket=output_dir.url.split(\"/\")[2], Key=index_file_path.lstrip(\"/\"))\n        raise ValueError(f\"The provided output_dir `{output_dir.path}` already contains an index file.\")\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"404\":\n            # The index file does not exist, so we can continue\n            return\n        else:\n            # An error occurred while checking for the index file\n            raise e\n\n    # Delete all objects within the specified prefix in the bucket\n    s3.delete_objects(\n        Bucket=output_dir.url.split(\"/\")[2],\n        Delete={\n            \"Objects\": [\n                {\"Key\": obj[\"Key\"]}\n                for obj in s3.list_objects_v2(Bucket=output_dir.url.split(\"/\")[2], Prefix=output_dir.url.split(\"/\")[3])[\n                    \"Contents\"\n                ]\n            ]\n        },\n    )"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    # Check if the output_dir is an S3 bucket directory\n    if not output_dir.url.startswith(\"s3://\"):\n        raise ValueError(f\"The provided output_dir {output_dir.path} is not an S3 bucket directory.\")\n\n    # Check if the output_dir already contains an index file\n    if \"index.json\" in output_dir.url:\n        raise ValueError(\n            f\"The provided output_dir {output_dir.path} already contains an index file. Please provide a different output_dir.\"\n        )\n\n    # Check if the index file already exists in the S3 bucket\n    s3 = boto3.resource(\"s3\")\n    bucket_name, prefix = output_dir.url.replace(\"s3://\", \"\").split(\"/\", 1)\n    index_file_key = f\"{prefix}/index.json\"\n    try:\n        s3.Object(bucket_name, index_file_key).load()\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"404\":\n            # The object does not exist.\n            return\n        else:\n            # Something else has gone wrong.\n            raise e\n\n    # Delete all objects within the specified prefix in the bucket\n    bucket = s3.Bucket(bucket_name)\n    bucket.objects.filter(Prefix=prefix).delete()"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    # Check if the output_dir is an S3 bucket\n    if output_dir.url is None:\n        raise ValueError(\"The provided output_dir is not an S3 bucket.\")\n\n    # Parse the output_dir URL\n    obj = parse.urlparse(output_dir.url)\n\n    # Check if the scheme is \"s3\"\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    # Create an S3 client\n    s3 = boto3.client(\"s3\")\n\n    # Check if the index file already exists\n    try:\n        s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\") + \"index.json\")\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"404\":\n            # Index file does not exist, proceed with the rest of the code\n            pass\n        else:\n            # Some other error occurred, raise an exception\n            raise e\n    else:\n        # Index file exists, raise an error\n        raise ValueError(\n            f\"The provided output_dir `{output_dir.path}` already contains an index.json file and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    # Get all objects within the specified prefix in the bucket\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # Delete all objects within the specified prefix in the bucket\n    for obj in objects.get(\"Contents\", []):\n        s3.delete_object(Bucket=obj.netloc, Key=obj.get(\"Key\"))"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.rank\n\n        if node_rank != 0:\n            while not os.path.exists(os.path.join(self._cache_dir, _INDEX_FILENAME)):\n                sleep(1)\n            return\n\n        while len(os.listdir(self._cache_dir)) < num_workers:\n            sleep(1)\n\n        self._merge_no_wait(node_rank)\n\n    "}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.rank\n\n        if node_rank != 0:\n            while not os.path.exists(os.path.join(self._cache_dir, _INDEX_FILENAME)):\n                sleep(0.1)\n            return\n\n        while True:\n            files = os.listdir(self._cache_dir)\n            index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n            if len(index_files) == num_workers:\n                break\n            sleep(0.1)\n\n        self._merge_no_wait(node_rank)"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = get_worker_rank()\n\n        if node_rank is not None and node_rank != 0:\n            while not os.path.exists(os.path.join(self._cache_dir, f\"{node_rank}-{_INDEX_FILENAME}\")):\n                sleep(1)\n            return\n\n        while len(os.listdir(self._cache_dir)) < num_workers:\n            sleep(1)\n\n        self._merge_no_wait(node_rank)"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            worker_env = _WorkerEnv.detect()\n            node_rank = worker_env.rank\n\n        if node_rank == 0:\n            self._merge_no_wait(node_rank)\n            return\n\n        while True:\n            files = os.listdir(self._cache_dir)\n            index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n            if len(index_files) == num_workers:\n                self._merge_no_wait(node_rank)\n                return\n            sleep(1)"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = os.getenv(\"DATA_OPTIMIZER_GLOBAL_RANK\", None)\n            if node_rank is not None:\n                node_rank = int(node_rank)\n\n        if node_rank is not None and node_rank != 0:\n            while not os.path.exists(os.path.join(self._cache_dir, f\"{node_rank}-{_INDEX_FILENAME}\")):\n                sleep(1)\n            return\n\n        while True:\n            files = os.listdir(self._cache_dir)\n            index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n            if len(index_files) == num_workers:\n                break\n            sleep(1)\n\n        self._merge_no_wait(node_rank)"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.rank\n\n        while True:\n            files = os.listdir(self._cache_dir)\n            index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n\n            if len(index_files) == num_workers:\n                if node_rank == 0:\n                    self._merge_no_wait(node_rank)\n                else:\n                    while not os.path.exists(os.path.join(self._cache_dir, f\"{node_rank}-{_INDEX_FILENAME}\")):\n                        sleep(0.1)\n                break\n            sleep(0.1)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = get_worker_rank()\n\n        if node_rank is None:\n            self._merge_no_wait()\n            return\n\n        if node_rank != 0:\n            while not os.path.exists(os.path.join(self._cache_dir, _INDEX_FILENAME)):\n                sleep(0.1)\n            return\n\n        while True:\n            files = os.listdir(self._cache_dir)\n            index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n            if len(index_files) < num_workers:\n                sleep(0.1)\n                continue\n            break\n\n        self._merge_no_wait(node_rank)"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = get_worker_rank()\n\n        if node_rank is not None and node_rank != 0:\n            while not os.path.exists(os.path.join(self._cache_dir, f\"{node_rank}-{_INDEX_FILENAME}\")):\n                sleep(0.1)\n            return\n\n        # Wait until all parts are available\n        while True:\n            files = os.listdir(self._cache_dir)\n            index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n            if len(index_files) == num_workers:\n                break\n            sleep(0.1)\n\n        self._merge_no_wait(node_rank)"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.rank\n\n        if node_rank == 0:\n            while not self.filled:\n                sleep(1)\n\n            self._merge_no_wait(node_rank)\n        else:\n            while not self.filled:\n                sleep(1)\n\n            while not os.path.exists(os.path.join(self._cache_dir, f\"{node_rank}-{_INDEX_FILENAME}\")):\n                sleep(1)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = get_worker_rank()\n\n        if node_rank is not None and node_rank != 0:\n            while not os.path.exists(os.path.join(self._cache_dir, _INDEX_FILENAME)):\n                sleep(1)\n            return\n\n        self._merge_no_wait(node_rank)\n\n        # Wait for all the workers to write their index\n        while len(os.listdir(self._cache_dir)) != num_workers:\n            sleep(1)\n\n        self._merge_no_wait(node_rank)"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            self._merge_no_wait()\n            return\n\n        while True:\n            files = os.listdir(self._cache_dir)\n            index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n\n            if len(index_files) == num_workers:\n                break\n\n            sleep(1)\n\n        if node_rank != 0:\n            while True:\n                if os.path.exists(os.path.join(self._cache_dir, _INDEX_FILENAME)):\n                    break\n                sleep(1)\n\n            return\n\n        self._merge_no_wait(node_rank)\n\n    "}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.rank\n\n        # If the current node is not the master node (rank 0), wait until the merged index file is available\n        if node_rank != 0:\n            while not os.path.exists(os.path.join(self._cache_dir, _INDEX_FILENAME)):\n                sleep(1)\n            return\n\n        # Wait until all index parts are available\n        while len(os.listdir(self._cache_dir)) < num_workers:\n            sleep(1)\n\n        # Merge the index parts into a single index file\n        self._merge_no_wait(node_rank)"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait until all parts are available\n        while len(os.listdir(self._cache_dir)) < num_workers:\n            sleep(0.1)\n\n        # Only the master node (rank 0) is responsible for merging\n        if node_rank is None:\n            self._merge_no_wait()\n        elif node_rank == 0:\n            self._merge_no_wait()\n        else:\n            # Wait until the merged index file is available\n            while not os.path.exists(os.path.join(self._cache_dir, _INDEX_FILENAME)):\n                sleep(0.1)"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = os.getenv(\"DATA_OPTIMIZER_GLOBAL_RANK\", None)\n            if node_rank is not None:\n                node_rank = int(node_rank)\n            else:\n                worker_env = _WorkerEnv.detect()\n                node_rank = worker_env.rank\n\n        if node_rank is not None and node_rank != 0:\n            while not os.path.exists(os.path.join(self._cache_dir, f\"{node_rank}-{_INDEX_FILENAME}\")):\n                sleep(1)\n            return\n\n        while len(os.listdir(self._cache_dir)) < num_workers:\n            sleep(1)\n\n        self._merge_no_wait(node_rank)\n\n    "}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.rank\n\n        if node_rank != 0:\n            while not os.path.exists(os.path.join(self._cache_dir, _INDEX_FILENAME)):\n                sleep(1)\n            return\n\n        while True:\n            files = os.listdir(self._cache_dir)\n            index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n            if len(index_files) < num_workers:\n                sleep(1)\n                continue\n\n            # Once all the workers have written their own index, the merge function is responsible to read and merge them\n            # into a single index.\n            self._merge_no_wait(node_rank=node_rank)\n            break"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.rank\n\n        if node_rank == 0:\n            self._merge_no_wait(node_rank)\n        else:\n            while not self.filled:\n                sleep(0.1)\n            self._merge_no_wait(node_rank)\n\n        if node_rank == 0:\n            os.rename(\n                os.path.join(self._cache_dir, f\"{node_rank}-{_INDEX_FILENAME}\"),\n                os.path.join(self._cache_dir, _INDEX_FILENAME),\n            )\n\n        return\n\n    "}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = get_worker_rank()\n\n        while True:\n            files = os.listdir(self._cache_dir)\n            index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n            if len(index_files) == num_workers:\n                break\n            sleep(0.1)\n\n        if node_rank is not None and node_rank != 0:\n            while not os.path.exists(os.path.join(self._cache_dir, f\"{node_rank}-{_INDEX_FILENAME}\")):\n                sleep(0.1)\n            return\n\n        self._merge_no_wait(node_rank)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # If the node rank is None, we assume that the rank determination is handled externally.\n        if node_rank is None:\n            node_rank = self.rank\n\n        # If the node rank is not 0, we wait until the merged index file is available.\n        if node_rank != 0:\n            while not os.path.exists(os.path.join(self._cache_dir, _INDEX_FILENAME)):\n                sleep(1)\n            return\n\n        # Wait until all parts are available.\n        while len(os.listdir(self._cache_dir)) < num_workers:\n            sleep(1)\n\n        # If the node rank is 0, we proceed with the merge operation.\n        self._merge_no_wait(node_rank)"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait until all the workers have written their own index\n        while True:\n            files = os.listdir(self._cache_dir)\n            index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n\n            if len(index_files) == num_workers:\n                break\n            sleep(1)\n\n        if node_rank is None:\n            self._merge_no_wait(node_rank)\n        else:\n            # Wait until the merged index file is available\n            while True:\n                files = os.listdir(self._cache_dir)\n                index_files = [f for f in files if f.endswith(f\"{node_rank}-{_INDEX_FILENAME}\")]\n\n                if len(index_files) == 1:\n                    break\n                sleep(1)\n\n            # Once the merged index file is available, rename it to the default index filename\n            os.rename(\n                os.path.join(self._cache_dir, f\"{node_rank}-{_INDEX_FILENAME}\"),\n                os.path.join(self._cache_dir, _INDEX_FILENAME),\n            )\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # TODO: The merge function is currently only supported for non-distributed environments.\n        #       It's not clear how to handle the merging process in a distributed environment.\n        #       It's recommended to investigate and implement a suitable merge strategy for distributed environments.\n        if node_rank is None:\n            node_rank = self.rank\n\n        while True:\n            files = os.listdir(self._cache_dir)\n            index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n            if len(index_files) == num_workers:\n                break\n            sleep(1)\n\n        if node_rank == 0:\n            self._merge_no_wait(node_rank)\n        else:\n            while True:\n                if os.path.exists(os.path.join(self._cache_dir, f\"{node_rank}-{_INDEX_FILENAME}\")):\n                    break\n                sleep(1)"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\n            \"The `lightning_sdk` is required to execute the operator remotely. Please install it with `pip install lightning_sdk`.\"\n        )\n\n    if not _BOTO3_AVAILABLE:\n        raise RuntimeError(\n            \"The `boto3` is required to execute the operator remotely. Please install it with `pip install boto3`.\"\n        )\n\n    if machine is None:\n        machine = Machine(\n            name=\"default\",\n            accelerators=\"auto\",\n            machine_type=\"default\",\n            num_cpus=num_nodes,\n            num_gpus=0,\n            disk_size=10,\n        )\n\n    if command is None:\n        command = \" \".join(sys.argv)\n\n    client = LightningClient(max_tries=2)\n\n    # Get the ids from env variables\n    cluster_id = os.getenv(\"LIGHTNING_CLUSTER_ID\", None)\n    project_id = os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", None)\n    cloud_space_id = os.getenv(\"LIGHTNING_CLOUD_SPACE_ID\", None)\n\n    if cluster_id is None:\n        raise RuntimeError(\"The `cluster_id` couldn't be found from the environement variables.\")\n\n    if project_id is None:\n        raise RuntimeError(\"The `project_id` couldn't be found from the environement variables.\")\n\n    if cloud_space_id is None:\n        raise RuntimeError(\"The `cloud_space_id` couldn't be found from the environement variables.\")\n\n    clusters = client.cluster_service_list_project_clusters(project_id).clusters\n\n    target_cloud_space = [\n        cloudspace\n        for cloudspace in client.cloud_space_service_list_cloud_spaces(\n            project_id=project_id, cluster_id=cluster_id\n        ).cloudsp"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\n            \"You need to install the lightning-sdk to execute this operator. \"\n            \"You can do so by running `pip install lightning-sdk`.\"\n        )\n\n    if not _BOTO3_AVAILABLE:\n        raise RuntimeError(\n            \"You need to install the boto3 to execute this operator. \"\n            \"You can do so by running `pip install boto3`.\"\n        )\n\n    if machine is None:\n        machine = Machine(\n            name=\"default\",\n            accelerators=\"auto\",\n            cpu_count=1,\n            memory=10,\n            disk_size=10,\n            image=\"lightning/base:latest\",\n        )\n\n    if command is None:\n        command = \" \".join(sys.argv)\n\n    client = LightningClient(max_tries=2)\n\n    # Get the ids from env variables\n    cluster_id = os.getenv(\"LIGHTNING_CLUSTER_ID\", None)\n    project_id = os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", None)\n    cloud_space_id = os.getenv(\"LIGHTNING_CLOUD_SPACE_ID\", None)\n\n    if cluster_id is None:\n        raise RuntimeError(\"The `cluster_id` couldn't be found from the environement variables.\")\n\n    if project_id is None:\n        raise RuntimeError(\"The `project_id` couldn't be found from the environement variables.\")\n\n    if cloud_space_id is None:\n        raise RuntimeError(\"The `cloud_space_id` couldn't be found from the environement variables.\")\n\n    clusters = client.cluster_service_list_project_clusters(project_id).clusters\n\n    target_cloud_space = [\n        cloudspace\n        for cloudspace in client.cloud_space_service_list_cloud_spaces(\n            project_id=project_id, cluster_id=cluster_"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\n            \"The `lightning_sdk` package is required to execute this operator remotely. \"\n            \"Please install the package with `pip install lightning-sdk`.\"\n        )\n\n    if not _BOTO3_AVAILABLE:\n        raise RuntimeError(\n            \"The `boto3` package is required to execute this operator remotely. \"\n            \"Please install the package with `pip install boto3`.\"\n        )\n\n    if machine is None:\n        machine = Machine(\n            name=\"default\",\n            accelerators=\"auto\",\n            instance_type=\"default\",\n            instance_size=\"default\",\n        )\n\n    if command is None:\n        command = f\"cd {os.getcwd()} && {' '.join(sys.argv)}\"\n\n    # Get the ids from env variables\n    cluster_id = os.getenv(\"LIGHTNING_CLUSTER_ID\", None)\n    project_id = os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", None)\n    cloud_space_id = os.getenv(\"LIGHTNING_CLOUD_SPACE_ID\", None)\n\n    if cluster_id is None:\n        raise RuntimeError(\"The `cluster_id` couldn't be found from the environement variables.\")\n\n    if project_id is None:\n        raise RuntimeError(\"The `project_id` couldn't be found from the environement variables.\")\n\n    if cloud_space_id is None:\n        raise RuntimeError(\"The `cloud_space_id` couldn't be found from the environement variables.\")\n\n    client = LightningClient(max_tries=2)\n\n    # Create the job\n    job = client.machine_service_create_machine(\n        project_id=project_id,\n        cluster_id=cluster_id,\n        cloud_space_id=cloud_space_id,\n        name=name,\n        machine_type=\"job\",\n        machine_configuration={\n           "}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\n            \"The Lightning SDK is required to execute the operator remotely. \"\n            \"Please install the Lightning SDK by running `pip install lightning`.\"\n        )\n\n    if not _BOTO3_AVAILABLE:\n        raise RuntimeError(\n            \"The boto3 library is required to execute the operator remotely. \"\n            \"Please install the boto3 library by running `pip install boto3`.\"\n        )\n\n    if machine is None:\n        machine = Machine(\n            name=\"default\",\n            accelerators=\"auto\",\n            cpu_count=1,\n            memory=2,\n            disk_size=10,\n            shutdown_timeout=60,\n        )\n\n    if command is None:\n        command = f\"cd {os.getcwd()}; {sys.executable} -m lightning run operators {name}\"\n\n    # Get the ids from env variables\n    cluster_id = os.getenv(\"LIGHTNING_CLUSTER_ID\", None)\n    project_id = os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", None)\n    cloud_space_id = os.getenv(\"LIGHTNING_CLOUD_SPACE_ID\", None)\n\n    if cluster_id is None:\n        raise RuntimeError(\"The `cluster_id` couldn't be found from the environement variables.\")\n\n    if project_id is None:\n        raise RuntimeError(\"The `project_id` couldn't be found from the environement variables.\")\n\n    if cloud_space_id is None:\n        raise RuntimeError(\"The `cloud_space_id` couldn't be found from the environement variables.\")\n\n    client = LightningClient(max_tries=2)\n\n    clusters = client.cluster_service_list_project_clusters(project_id).clusters\n\n    target_cloud_space = [\n        cloudspace\n        for cloudspace in client.cloud_space_service_list_cloud_sp"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\"The Lightning SDK is not available.\")\n\n    if not _BOTO3_AVAILABLE:\n        raise RuntimeError(\"The boto3 library is not available.\")\n\n    if machine is None:\n        machine = Machine(\n            machine_type=\"CPU\",\n            accelerators=\"V100\",\n            disk_size=100,\n            shutdown_timeout_minutes=30,\n            preemptible=False,\n        )\n\n    if command is None:\n        command = \" \".join(\n            [\n                f\"cd {os.getcwd()}\",\n                \"&&\",\n                \" \".join([f\"export {k}={v}\" for k, v in os.environ.items()]),\n                \"&&\",\n                \" \".join(sys.argv),\n            ]\n        )\n\n    # Get the ids from env variables\n    cluster_id = os.getenv(\"LIGHTNING_CLUSTER_ID\", None)\n    project_id = os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", None)\n    cloud_space_id = os.getenv(\"LIGHTNING_CLOUD_SPACE_ID\", None)\n\n    if cluster_id is None:\n        raise RuntimeError(\"The `cluster_id` couldn't be found from the environement variables.\")\n\n    if project_id is None:\n        raise RuntimeError(\"The `project_id` couldn't be found from the environement variables.\")\n\n    if cloud_space_id is None:\n        raise RuntimeError(\"The `cloud_space_id` couldn't be found from the environement variables.\")\n\n    studio = Studio(\n        cluster_id=cluster_id,\n        project_id=project_id,\n        cloud_space_id=cloud_space_id,\n    )\n\n    job = studio.create_job(\n        name=name,\n        machine=machine,\n        command=command,\n        num_nodes=num_nodes"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\"The `lightning_sdk` is not installed. Please install it with `pip install lightning_sdk`.\")\n\n    if not _BOTO3_AVAILABLE:\n        raise RuntimeError(\"The `boto3` is not installed. Please install it with `pip install boto3`.\")\n\n    # Get the ids from env variables\n    cluster_id = os.getenv(\"LIGHTNING_CLUSTER_ID\", None)\n    project_id = os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", None)\n    cloud_space_id = os.getenv(\"LIGHTNING_CLOUD_SPACE_ID\", None)\n\n    if cluster_id is None:\n        raise RuntimeError(\"The `cluster_id` couldn't be found from the environement variables.\")\n\n    if project_id is None:\n        raise RuntimeError(\"The `project_id` couldn't be found from the environement variables.\")\n\n    if cloud_space_id is None:\n        raise RuntimeError(\"The `cloud_space_id` couldn't be found from the environement variables.\")\n\n    # Get the machine\n    if machine is None:\n        machine = Machine(\n            name=\"Data Preparation Machine\",\n            machine_type=\"GPU\",\n            accelerators=\"V100\",\n            num_cpus=4,\n            num_gpus=1,\n            memory_mb=16384,\n            image=\"lightning/data-prep\",\n        )\n\n    # Get the command\n    if command is None:\n        command = \" \".join(sys.argv)\n\n    # Get the current working directory\n    cwd = os.getcwd()\n\n    # Get the environment variables\n    env_vars = os.environ.copy()\n\n    # Create the job\n    job = Studio(\n        name=name,\n        machine=machine,\n        command=command,\n        working_dir=cwd,\n        env_vars=env"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\"The Lightning SDK is required to use this function.\")\n\n    if not _BOTO3_AVAILABLE:\n        raise RuntimeError(\"The boto3 package is required to use this function.\")\n\n    if machine is None:\n        machine = Machine(\n            name=\"default\",\n            accelerators=\"auto\",\n            instance_type=\"default\",\n            disk_size=10,\n            image=\"default\",\n        )\n\n    if command is None:\n        command = \" \".join(\n            [\n                \"python\",\n                \"-m\",\n                \"lightning_datasets\",\n                \"--workdir\",\n                os.getcwd(),\n                \"--name\",\n                name,\n                \"--num_nodes\",\n                str(num_nodes),\n                \"--cloud_compute\",\n                \"lightning_datasets\",\n                \"--cluster_id\",\n                os.getenv(\"LIGHTNING_CLUSTER_ID\", \"\"),\n                \"--project_id\",\n                os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", \"\"),\n                \"--cloud_space_id\",\n                os.getenv(\"LIGHTNING_CLOUD_SPACE_ID\", \"\"),\n                \"--cluster_environment_id\",\n                os.getenv(\"LIGHTNING_CLUSTER_ENVIRONMENT_ID\", \"\"),\n                \"--cluster_id\",\n                os.getenv(\"LIGHTNING_CLUSTER_ID\", \"\"),\n                \"--cluster_name\",\n                os.getenv(\"LIGHTNING_CLUSTER_NAME\", \"\"),\n                \"--cluster_region\",\n                os.getenv(\"LIGHTNING_CLUSTER_REGION\", \"\"),\n                \"--cluster_type\",\n                os.getenv(\"LIGHTNING_CLUSTER_TYPE\", \"\"),\n                \"--cluster_provider\",\n                os.getenv(\"LIGHTNING_CLUSTER_PROVIDER\", \"\"),\n                \"--cluster"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\"The Lightning SDK is required to execute this operator remotely. Please install it with `pip install lightning`.\")\n\n    if not _BOTO3_AVAILABLE:\n        raise RuntimeError(\n            \"The boto3 library is required to execute this operator remotely. Please install it with `pip install boto3`.\"\n        )\n\n    if machine is None:\n        machine = Machine(\n            name=\"default\",\n            accelerator=\"cpu\",\n            accelerator_count=1,\n            cpu_count=2,\n            memory=2048,\n            image=\"lightning/pytorch\",\n            shared_memory_size=1024,\n        )\n\n    if command is None:\n        command = \" \".join(sys.argv)\n\n    # Get the ids from env variables\n    cluster_id = os.getenv(\"LIGHTNING_CLUSTER_ID\", None)\n    project_id = os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", None)\n    cloud_space_id = os.getenv(\"LIGHTNING_CLOUD_SPACE_ID\", None)\n\n    if cluster_id is None:\n        raise RuntimeError(\"The `cluster_id` couldn't be found from the environement variables.\")\n\n    if project_id is None:\n        raise RuntimeError(\"The `project_id` couldn't be found from the environement variables.\")\n\n    if cloud_space_id is None:\n        raise RuntimeError(\"The `cloud_space_id` couldn't be found from the environement variables.\")\n\n    # Get the cluster information\n    client = LightningClient(max_tries=2)\n    clusters = client.cluster_service_list_project_clusters(project_id).clusters\n    target_cluster = [cluster for cluster in clusters if cluster.id == cluster_id]\n\n    if not target_cluster:\n        raise ValueError(f\"We didn't find a matching cluster associated with"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ImportError(\n            \"The `lightning_sdk` package is required to execute this operator remotely. Please install it with `pip install lightning-sdk`.\"\n        )\n\n    if not _BOTO3_AVAILABLE:\n        raise ImportError(\n            \"The `boto3` package is required to execute this operator remotely. Please install it with `pip install boto3`.\"\n        )\n\n    if machine is None:\n        machine = Machine(\n            name=\"default\",\n            accelerators=\"auto\",\n            instance_type=\"g4dn.xlarge\",\n            instance_type_spot=True,\n            cloud_compute=CloudCompute(\"gpu-fast\"),\n        )\n\n    if command is None:\n        command = \" \".join(sys.argv)\n\n    # Get the ids from env variables\n    cluster_id = os.getenv(\"LIGHTNING_CLUSTER_ID\", None)\n    project_id = os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", None)\n    cloud_space_id = os.getenv(\"LIGHTNING_CLOUD_SPACE_ID\", None)\n\n    if cluster_id is None:\n        raise RuntimeError(\"The `cluster_id` couldn't be found from the environement variables.\")\n\n    if project_id is None:\n        raise RuntimeError(\"The `project_id` couldn't be found from the environement variables.\")\n\n    if cloud_space_id is None:\n        raise RuntimeError(\"The `cloud_space_id` couldn't be found from the environement variables.\")\n\n    client = LightningClient(max_tries=2)\n\n    # Create the job\n    job = client.studio_service_create_machine_job(\n        project_id=project_id,\n        cluster_id=cluster_id,\n        cloudspace_id=cloud_space_id,\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n       "}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\"The lightning_sdk package is required to execute this operator remotely.\")\n\n    if not _BOTO3_AVAILABLE:\n        raise RuntimeError(\"The boto3 package is required to execute this operator remotely.\")\n\n    if machine is None:\n        machine = Machine(num_nodes=num_nodes)\n\n    if command is None:\n        command = f\"cd {os.getcwd()} && {' '.join(sys.argv)}\"\n\n    try:\n        studio = Studio(\n            cloud_compute=machine,\n            name=name,\n            command=command,\n            environment=dict(os.environ),\n        )\n    except Exception as e:\n        raise RuntimeError(f\"Failed to create the job: {e}\")\n\n    try:\n        studio.run()\n    except Exception as e:\n        raise RuntimeError(f\"Failed to run the job: {e}\")\n\n    print(f\"Job started: {studio.url}\")\n\n    while True:\n        try:\n            status = studio.status()\n        except Exception as e:\n            raise RuntimeError(f\"Failed to get the job status: {e}\")\n\n        if status == \"failed\":\n            raise RuntimeError(\"Job failed.\")\n\n        if status == \"succeeded\":\n            return\n\n        sleep(1)"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise Exception(\"You need to install the lightning-sdk to use this function.\")\n\n    if not _BOTO3_AVAILABLE:\n        raise Exception(\"You need to install the boto3 to use this function.\")\n\n    if machine is None:\n        machine = Machine(num_nodes=num_nodes)\n\n    if command is None:\n        command = f\"cd {os.getcwd()}; export {os.environ.get('PYTHONPATH')}; {sys.executable} -m {__name__} --name {name}\"\n\n    job = Studio(name=name, machine=machine, command=command, cloud_compute=\"default\")\n\n    print(f\"Job URL: {_get_lightning_cloud_url()}/{job.id}\")\n\n    while True:\n        try:\n            job.status()\n            sleep(1)\n        except Exception as e:\n            raise e\n        if job.status().state == \"FINISHED\":\n            break\n\n    if job.status().state == \"FAILED\":\n        raise Exception(\"Job failed.\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\n            \"The Lightning SDK is not available. Please install it with `pip install lightning-sdk`.\"\n        )\n\n    if not _BOTO3_AVAILABLE:\n        raise RuntimeError(\n            \"The Boto3 library is not available. Please install it with `pip install boto3`.\"\n        )\n\n    if machine is None:\n        machine = Machine(\n            resource=\"gpu\",\n            accelerators=\"V100\",\n            instance_type=\"g4dn.xlarge\",\n            num_cpus=1,\n            num_gpus=1,\n        )\n\n    if command is None:\n        command = \" \".join(\n            [\n                f\"cd {os.getcwd()}\",\n                \"&&\",\n                \" \".join([f\"{k}={v}\" for k, v in os.environ.items()]),\n                \"&&\",\n                \" \".join(sys.argv),\n            ]\n        )\n\n    studio = Studio(\n        name=name,\n        machine=machine,\n        command=command,\n        num_nodes=num_nodes,\n    )\n\n    print(f\"Executing {name}...\")\n    print(f\"Job URL: {_get_lightning_cloud_url()}/{studio.id}\")\n\n    while True:\n        sleep(1)\n        if studio.status == \"succeeded\":\n            break\n        if studio.status == \"failed\":\n            raise RuntimeError(f\"Job {name} failed\")\n        if studio.status == \"stopped\":\n            raise RuntimeError(f\"Job {name} stopped\")\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\n            \"The `lightning_sdk` is required to execute the current operator. \"\n            \"HINT: You can install it with `pip install lightning_sdk`.\"\n        )\n\n    if not _BOTO3_AVAILABLE:\n        raise RuntimeError(\n            \"The `boto3` is required to execute the current operator. \"\n            \"HINT: You can install it with `pip install boto3`.\"\n        )\n\n    if machine is None:\n        machine = Machine(\n            name=\"default\",\n            accelerators=\"auto\",\n            cpu_count=1,\n            memory=1000,\n            disk_size=1000,\n            shutdown_timeout=60,\n        )\n\n    if command is None:\n        command = \" \".join(\n            [\n                f\"cd {os.getcwd()}\",\n                \" && \".join(f\"export {key}={value}\" for key, value in os.environ.items()),\n                \" && \".join(sys.argv),\n            ]\n        )\n\n    client = Studio(\n        project_id=os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", \"\"),\n        cloud_compute_id=os.getenv(\"LIGHTNING_CLOUD_COMPUTE_ID\", \"\"),\n        cloud_space_id=os.getenv(\"LIGHTNING_CLOUD_SPACE_ID\", \"\"),\n        cloud_space_region=os.getenv(\"LIGHTNING_CLOUD_SPACE_REGION\", \"\"),\n        cloud_space_name=os.getenv(\"LIGHTNING_CLOUD_SPACE_NAME\", \"\"),\n        cloud_space_url=_get_lightning_cloud_url(),\n    )\n\n    job = client.create_machine(\n        name=name,\n        machine=machine,\n        command=command,\n        num_nodes=num_nodes,\n        wait_for_terminal_state=False,"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\"The lightning-sdk is required to run this operator. Please install it with `pip install lightning-sdk`.\")\n\n    if not _BOTO3_AVAILABLE:\n        raise RuntimeError(\"The boto3 is required to run this operator. Please install it with `pip install boto3`.\")\n\n    if machine is None:\n        machine = Machine(\n            name=\"default\",\n            accelerators=\"auto\",\n            cpu_count=4,\n            mem_size=\"16G\",\n            disk_size=100,\n            disk_type=\"SSD\",\n            autoscale_enabled=False,\n        )\n\n    if command is None:\n        command = \" \".join(sys.argv)\n\n    # Get the ids from env variables\n    cluster_id = os.getenv(\"LIGHTNING_CLUSTER_ID\", None)\n    project_id = os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", None)\n    cloud_space_id = os.getenv(\"LIGHTNING_CLOUD_SPACE_ID\", None)\n\n    if cluster_id is None:\n        raise RuntimeError(\"The `cluster_id` couldn't be found from the environement variables.\")\n\n    if project_id is None:\n        raise RuntimeError(\"The `project_id` couldn't be found from the environement variables.\")\n\n    if cloud_space_id is None:\n        raise RuntimeError(\"The `cloud_space_id` couldn't be found from the environement variables.\")\n\n    # TODO: Add a way to specify the cluster_id and project_id\n    studio = Studio(\n        name=name,\n        cluster_id=cluster_id,\n        project_id=project_id,\n        cloud_space_id=cloud_space_id,\n        machine_type=machine.name,\n        num_nodes=num_nodes,\n        command=command,\n        machine_config={\n            \"accelerators\": machine."}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\n            \"The Lightning SDK is not installed. Please install the Lightning SDK to use this function.\"\n        )\n\n    if not _BOTO3_AVAILABLE:\n        raise RuntimeError(\n            \"The boto3 library is not installed. Please install the boto3 library to use this function.\"\n        )\n\n    if machine is None:\n        machine = Machine(\n            machine_type=\"CPU\",\n            accelerators=None,\n            enable_ssh=False,\n            enable_internet=False,\n            enable_gpus=False,\n            enable_spot=False,\n            enable_hub=False,\n            enable_studio=True,\n        )\n\n    if command is None:\n        command = \" \".join(\n            [\n                f\"cd {os.getcwd()}\",\n                \"&&\",\n                \" \".join(f\"{key}={value}\" for key, value in os.environ.items()),\n                \"&&\",\n                \" \".join(sys.argv),\n            ]\n        )\n\n    studio = Studio()\n\n    job = studio.create_job(\n        name=name,\n        machine=machine,\n        command=command,\n        num_nodes=num_nodes,\n    )\n\n    print(f\"Job {name} has been created. You can view the job at {job.url}\")\n\n    while True:\n        job.reload()\n        if job.status.value == \"STARTED\":\n            break\n        sleep(1)\n\n    print(f\"Job {name} has started. You can view the job at {job.url}\")\n\n    while True:\n        job.reload()\n        if job.status.value == \"FAILED\":\n            raise RuntimeError(f\"Job {name} has failed. You can view the job at {job.url}\")\n        if job.status.value == \"SUCCEEDED\":\n            break\n        sleep(1)\n\n    print(f\"Job {name}"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\"The lightning-sdk is not installed. Please install it with `pip install lightning-sdk`\")\n\n    if not _BOTO3_AVAILABLE:\n        raise RuntimeError(\"The boto3 is not installed. Please install it with `pip install boto3`\")\n\n    if machine is None:\n        machine = Machine(\n            name=\"default\",\n            accelerators=\"auto\",\n            cpu_count=num_nodes,\n            memory=\"auto\",\n            disk_size=\"auto\",\n            image=\"lightning/base:latest\",\n        )\n\n    if command is None:\n        command = f\"cd {os.getcwd()} && export {os.environ} && {sys.executable} -m {sys.modules[__name__].__name__}\"\n\n    # TODO: Add support for `append` and `overwrite`.\n    job = Studio.run(\n        name=name,\n        machine=machine,\n        command=command,\n        wait=False,\n    )\n\n    print(f\"Job URL: {_get_lightning_cloud_url()}/{job.id}\")\n\n    while job.status.value != \"SUCCEEDED\":\n        if job.status.value == \"FAILED\":\n            raise RuntimeError(f\"Job failed with status {job.status.value}\")\n        sleep(1)\n        job = Studio.get(job_id=job.id)\n\n    print(\"Job succeeded\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\n            \"The lightning-sdk is not installed. Please install it with `pip install lightning-sdk`.\"\n        )\n\n    if not _BOTO3_AVAILABLE:\n        raise RuntimeError(\n            \"The boto3 is not installed. Please install it with `pip install boto3`.\"\n        )\n\n    if machine is None:\n        machine = Machine(\n            accelerators=\"auto\",\n            cpu_count=4,\n            memory=16,\n            disk_size=20,\n            image=\"lightning/build-system:latest\",\n            interruptible=True,\n        )\n\n    if command is None:\n        command = f\"cd {os.getcwd()} && {' '.join(sys.argv)}\"\n\n    client = Studio(\n        api_key=os.getenv(\"LIGHTNING_API_KEY\", None),\n        cloud_compute=machine,\n        cloud_build_config={\n            \"requirements\": [\"lightning-datasets\"],\n            \"pip_packages\": [],\n            \"environment\": {\n                **os.environ,\n                \"LIGHTNING_CLOUD_URL\": _get_lightning_cloud_url(),\n            },\n        },\n    )\n\n    job = client.create_job(name=name, command=command, num_nodes=num_nodes)\n\n    print(f\"Job {name} has started. You can access it at: {job.url}\")\n\n    while True:\n        job.reload()\n        if job.status == \"failed\":\n            raise RuntimeError(f\"Job {name} failed with the following error: {job.error}\")\n        if job.status == \"succeeded\":\n            break\n        sleep(1)"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\n            \"The Lightning SDK is required to run this operator remotely. Please install the SDK by running `pip install lightning`.\"\n        )\n\n    if not _BOTO3_AVAILABLE:\n        raise RuntimeError(\n            \"The boto3 library is required to run this operator remotely. Please install the library by running `pip install boto3`.\"\n        )\n\n    if machine is None:\n        machine = Machine(\n            name=\"default\",\n            accelerators=\"cpu\",\n            instance_type=\"g4dn.xlarge\",\n            num_cpus=1,\n            num_gpus=0,\n            num_memory_gb=1,\n        )\n\n    if command is None:\n        command = f\"cd {os.getcwd()} && {' '.join([f'{k}={v}' for k, v in os.environ.items()])} && {sys.executable} -m lightning run data {name}\"\n\n    studio = Studio(\n        name=\"default\",\n        cloud_compute=machine,\n        cloud_build_config={\n            \"requirements\": [\"lightning\"],\n            \"dockerfile\": \"FROM lightningai/lightning-python-base:latest\",\n        },\n    )\n\n    job = studio.run(\n        name,\n        command,\n        num_nodes=num_nodes,\n        wait_until_running=True,\n    )\n\n    print(f\"Job URL: {_get_lightning_cloud_url()}/apps/{job.app_id}/{job.id}\")\n\n    while job.status != \"succeeded\" and job.status != \"failed\":\n        sleep(1)\n        job = studio.get_job(job.id)\n\n    if job.status == \"failed\":\n        raise RuntimeError(f\"Job {job.name} failed. Check the logs for more information.\")\n\n    print(\"Job succeeded.\")"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ImportError(\n            \"The lightning_sdk package is required to execute this operator. Please install it with `pip install lightning_sdk`.\"\n        )\n\n    if not _BOTO3_AVAILABLE:\n        raise ImportError(\n            \"The boto3 package is required to execute this operator. Please install it with `pip install boto3`.\"\n        )\n\n    if machine is None:\n        machine = Machine(\n            name=\"DataPreparationMachine\",\n            accelerators=\"auto\",\n            cpu_count=1,\n            memory=1000,\n            disk_size=20000,\n            image=\"lightning/data-preparation-machine:latest\",\n            cloud_compute=LightningClient().compute_service_get_default_cloud_compute(),\n        )\n\n    if command is None:\n        command = \" \".join(\n            [\n                f\"cd {os.getcwd()}\",\n                \"&&\",\n                \" \".join([f\"{k}={v}\" for k, v in os.environ.items()]),\n                \"&&\",\n                f\"python {sys.argv[0]}\",\n                *sys.argv[1:],\n            ]\n        )\n\n    job = Studio().machine(machine).run(\n        name=name,\n        command=command,\n        num_nodes=num_nodes,\n    )\n\n    print(f\"Job URL: {_get_lightning_cloud_url()}/view/{job.id}\")\n\n    while True:\n        job = Studio().get_job(job.id)\n        if job.status == \"failed\":\n            raise RuntimeError(f\"Job failed with message: {job.message}\")\n        if job.status == \"succeeded\":\n            break\n        sleep(1)\n\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ImportError(\n            \"To use the `_execute` function, you need to install the Lightning SDK. \"\n            \"You can do this by running `pip install lightning`.\"\n        )\n\n    if not _BOTO3_AVAILABLE:\n        raise ImportError(\n            \"To use the `_execute` function, you need to install the `boto3` library. \"\n            \"You can do this by running `pip install boto3`.\"\n        )\n\n    if machine is None:\n        machine = Machine(num_cpus=1, num_gpus=1)\n\n    if command is None:\n        command = \" \".join(\n            [\n                f\"cd {os.getcwd()}\",\n                \"&&\",\n                \" \".join(\n                    [\n                        f'{key}=\"{value}\"'\n                        for key, value in os.environ.items()\n                        if key not in [\"PYTHONPATH\", \"PYTHONHOME\", \"LD_LIBRARY_PATH\"]\n                    ]\n                ),\n                \"&&\",\n                \" \".join(sys.argv),\n            ]\n        )\n\n    # TODO: Remove the `wait_for_status` call when the SDK is updated.\n    job = Studio().run(\n        name=name,\n        machine=machine,\n        command=command,\n        num_nodes=num_nodes,\n        wait_for_status=False,\n    )\n\n    print(f\"Job started at: {_get_lightning_cloud_url()}/{job.id}\")\n\n    while job.status != \"Succeeded\":\n        if job.status == \"Failed\":\n            raise RuntimeError(f\"Job failed with the following error: {job.error}\")\n\n        sleep(5)\n        job = Studio().get_job(job.id)\n\n    print(f\"Job succeeded at: {_get_lightning_cloud_url()}/{job.id}\")"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config is None:\n            self._config = ChunksConfig.from_cache_dir(\n                self._cache_dir,\n                self._serializers,\n                self._remote_input_dir,\n                self._compression,\n                self._item_loader,\n            )\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config is None:\n            try:\n                self._config = ChunksConfig.from_cache_dir(\n                    self._cache_dir,\n                    self._serializers,\n                    remote_input_dir=self._remote_input_dir,\n                    compression=self._compression,\n                    item_loader=self._item_loader,\n                )\n            except FileNotFoundError:\n                pass\n        return self._config\n\n    "}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Load the config containing the index\n        if self._config is None:\n            try:\n                self._config = ChunksConfig(\n                    self._cache_dir,\n                    self._serializers,\n                    self._remote_input_dir,\n                    self._compression,\n                    self._item_loader,\n                )\n            except FileNotFoundError:\n                return None\n        return self._config\n\n    "}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config is not None:\n            return self._config\n\n        # Try to load the index files\n        try:\n            self._config = ChunksConfig.from_cache_dir(\n                self._cache_dir,\n                self._serializers,\n                self._remote_input_dir,\n                self._compression,\n                self._item_loader,\n            )\n        except FileNotFoundError:\n            return None\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config is None:\n            # Load the index files\n            try:\n                self._config = ChunksConfig.from_cache_dir(\n                    self._cache_dir,\n                    self._serializers,\n                    self._remote_input_dir,\n                    self._compression,\n                    self._item_loader,\n                )\n            except FileNotFoundError:\n                return None\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Check if the index files are available\n        index_filepath = os.path.join(self._cache_dir, \"index.json\")\n        if not os.path.exists(index_filepath):\n            return None\n\n        # Load the config\n        self._config = ChunksConfig.from_json(\n            index_filepath,\n            self._cache_dir,\n            self._remote_input_dir,\n            self._compression,\n            self._serializers,\n            self._item_loader,\n        )\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config is not None:\n            return self._config\n\n        # Load the index file\n        index_filepath = os.path.join(self._cache_dir, \"index.json\")\n        if not os.path.exists(index_filepath):\n            return None\n\n        # Load the config\n        self._config = ChunksConfig.load_from_file(\n            index_filepath,\n            self._cache_dir,\n            self._serializers,\n            self._remote_input_dir,\n            self._compression,\n            self._item_loader,\n        )\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config is not None:\n            return self._config\n\n        # Load the index files\n        index_filepath = os.path.join(self._cache_dir, \"index.json\")\n        if not os.path.exists(index_filepath):\n            return None\n\n        # Load the config\n        self._config = ChunksConfig.from_json_file(\n            index_filepath,\n            cache_dir=self._cache_dir,\n            remote_dir=self._remote_input_dir,\n            compressor=self._compression,\n            serializers=self._serializers,\n            item_loader=self._item_loader,\n        )\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Load the config containing the index\n        if self._config is None:\n            if os.path.exists(self._cache_dir):\n                self._config = ChunksConfig.from_cache_dir(\n                    self._cache_dir, self._serializers, self._remote_input_dir, self._compression, self._item_loader\n                )\n            else:\n                logger.warning(f\"The provided cache_dir `{self._cache_dir}` doesn't exist.\")\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config is None:\n            self._config = ChunksConfig.load(\n                self._cache_dir,\n                self._serializers,\n                self._remote_input_dir,\n                self._compression,\n                self._item_loader,\n            )\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        return self._config._get_chunk_index_from_index(index)  # type: ignore\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config is not None:\n            return self._config\n\n        # Check whether the index files exist\n        index_filepath = os.path.join(self._cache_dir, \"index.json\")\n        if not os.path.exists(index_filepath):\n            return None\n\n        # Load the index file\n        self._config = ChunksConfig(\n            index_filepath,\n            self._serializers,\n            self._remote_input_dir,\n            self._cache_dir,\n            self._compression,\n            self._item_loader,\n        )\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Check if the index files exist\n        if not os.path.exists(os.path.join(self._cache_dir, \"index.json\")):\n            return None\n\n        # Load the config\n        self._config = ChunksConfig.load(\n            self._cache_dir,\n            remote_input_dir=self._remote_input_dir,\n            serializers=self._serializers,\n            compression=self._compression,\n            item_loader=self._item_loader,\n        )\n\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config is None:\n            try:\n                self._config = ChunksConfig.from_dir(\n                    self._cache_dir,\n                    self._serializers,\n                    remote_input_dir=self._remote_input_dir,\n                    compression=self._compression,\n                    item_loader=self._item_loader,\n                )\n            except FileNotFoundError:\n                return None\n\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config is not None:\n            return self._config\n\n        # Load the config containing the index\n        try:\n            self._config = ChunksConfig.load(\n                self._cache_dir,\n                self._serializers,\n                self._remote_input_dir,\n                self._compression,\n                self._item_loader,\n            )\n            return self._config\n        except Exception as e:\n            logger.debug(f\"Error while loading the chunks config: {e}\")\n            return None\n\n    "}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # If the index file doesn't exist, return None\n        if not os.path.exists(self._config_path):\n            return None\n\n        # Load the config from the index file\n        self._config = ChunksConfig.load(\n            self._config_path,\n            self._serializers,\n            self._item_loader,\n            self._remote_input_dir,\n            self._compression,\n        )\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config is None:\n            self._config = ChunksConfig.from_cache_dir(\n                self._cache_dir, self._serializers, self._remote_input_dir, self._compression, self._item_loader\n            )\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config is not None:\n            return self._config\n\n        # Try to load the config\n        if not os.path.exists(self._cache_dir):\n            return None\n\n        # Check if the index files are available\n        if not os.path.exists(os.path.join(self._cache_dir, \"index.json\")):\n            return None\n\n        # Load the config\n        self._config = ChunksConfig.from_dir(\n            self._cache_dir,\n            remote_dir=self._remote_input_dir,\n            compressor=self._compression,\n            item_loader=self._item_loader,\n            serializers=self._serializers,\n        )\n\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config is not None:\n            return self._config\n\n        # Load the index files\n        index_files = [\n            os.path.join(self._cache_dir, file)\n            for file in os.listdir(self._cache_dir)\n            if file.startswith(\"index\") and file.endswith(\".json\")\n        ]\n\n        # If no index file is found, return None\n        if not index_files:\n            return None\n\n        # Load the chunks configuration\n        self._config = ChunksConfig.load(\n            index_files=index_files,\n            cache_dir=self._cache_dir,\n            serializers=self._serializers,\n            remote_dir=self._remote_input_dir,\n            compression=self._compression,\n            item_loader=self._item_loader,\n        )\n\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config is None:\n            # If the config isn't defined, try to load it\n            try:\n                self._config = ChunksConfig.from_dir(\n                    self._cache_dir,\n                    serializers=self._serializers,\n                    remote_dir=self._remote_input_dir,\n                    compression=self._compression,\n                    item_loader=self._item_loader,\n                )\n            except FileNotFoundError:\n                # The index file isn't available\n                return None\n\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n\n    "}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n\n    "}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The config should be defined before accessing it.\")\n        return self._config\n\n    "}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The config should be defined before accessing it.\")\n        return self._config\n\n    "}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The config should be defined before accessing it.\")\n        return self._config\n\n    "}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The config should be defined before accessing it.\")\n        return self._config\n\n    "}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The config should be defined before accessing it.\")\n        return self._config\n\n    "}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The config should be defined before accessing it.\")\n        return self._config\n\n    "}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The config should be defined before accessing it.\")\n        return self._config\n\n    "}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The config should be defined before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration should be defined before accessing it.\")\n        return self._config\n\n    "}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration should be defined before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration should be defined before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The config should be defined before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The config isn't defined. Please call the `set_index` method.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\n                \"The config should be defined before accessing it. \"\n                \"You can define it by calling the `set_config(...)` method.\"\n            )\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The config should be defined before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The BinaryReader config should be defined before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The config should be defined before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The config should be defined before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The BinaryReader.config property should be defined before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration is not set. Please define the index.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index needs to be an instance of ChunkedIndex.\")\n\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Download the chunk if it's not available\n        if chunk_index != self._last_chunk_index:\n            assert self._prepare_thread is not None\n            self._prepare_thread.download([chunk_index])\n            self._last_chunk_index = chunk_index\n\n        # Load the chunk if it's not available\n        chunk_filepath, _, _ = self.config[index]\n        self._item_loader.load_chunk(chunk_index, chunk_filepath)\n\n        # Get the item\n        item = self._item_loader.get_item(index)\n\n        # Delete the chunk if it's fully consumed\n        if self._prepare_thread is not None and self._prepare_thread._delete_chunks_when_processed:\n            self._prepare_thread.delete([chunk_index])\n\n        return item"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index should be a ChunkedIndex.\")\n\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Download the chunk if it's not available\n        if self._prepare_thread is None:\n            raise Exception(\"The prepare thread should be defined.\")\n\n        if chunk_index != self._last_chunk_index:\n            self._prepare_thread.download([chunk_index])\n            self._last_chunk_index = chunk_index\n\n        # Read the item from the chunk\n        chunk_filepath, _, _ = self._config[index]\n        return self._item_loader.read_item_from_chunk(chunk_index, chunk_filepath, index.index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The index should be an instance of ChunkedIndex. Got {type(index)}.\")\n\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Load the chunk if it isn't already loaded\n        if self._last_chunk_index is None or self._last_chunk_index != chunk_index:\n            self._last_chunk_index = chunk_index\n\n            # Download the chunk if it isn't already downloaded\n            if not self._config.chunk_is_downloaded(chunk_index):\n                self._config.download_chunk_from_index(chunk_index)\n\n            # Preload the chunk if it isn't already preloaded\n            if not self._config.chunk_is_preloaded(chunk_index):\n                self._config.preload_chunk_from_index(chunk_index)\n\n        assert self._prepare_thread is not None\n        self._prepare_thread.download([chunk_index])\n        return self._config.read_item(index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index should be an instance of ChunkedIndex.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Download the chunk if necessary\n        chunk_index = self._get_chunk_index_from_index(index.index)\n        if chunk_index != self._last_chunk_index:\n            self._last_chunk_index = chunk_index\n            self._prepare_thread.download([chunk_index])\n\n        # Wait until the chunk is available\n        chunk_filepath, _, _ = self._config[index]\n        self._item_loader.wait_until_chunk_is_available(chunk_index, chunk_filepath)\n\n        # Load the item from the chunk\n        item = self._item_loader.load_item_from_chunk(chunk_index, chunk_filepath, index.index)\n\n        # Delete the chunk if necessary\n        if self._prepare_thread is not None:\n            self._prepare_thread.delete([chunk_index])\n\n        return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The index should be of type `ChunkedIndex`. Got {type(index)}.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index from the index\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Check whether the chunk is available locally or in memory\n        chunk_filepath, _, _ = self._config[ChunkedIndex(index=-1, chunk_index=chunk_index)]\n        if not self._item_loader.is_chunk_available(chunk_index, chunk_filepath):\n            # Download the chunk if it's not available\n            self._download_chunk(chunk_index)\n\n        # Load the item from the chunk\n        item = self._item_loader.load_item_from_chunk(chunk_index, chunk_filepath, index.index)\n\n        # Prefetch the next chunk if the current chunk is fully consumed\n        if self._prepare_thread is not None:\n            self._prefetch_chunk(chunk_index)\n\n        # Delete the chunk if it's fully consumed\n        if self._prepare_thread is not None:\n            self._delete_chunk(chunk_index)\n\n        return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The index should be an instance of ChunkedIndex, got {type(index)}\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index from the index\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Download the chunk if it doesn't exist\n        if self._prepare_thread is None:\n            raise Exception(\"The prepare thread should be defined.\")\n\n        if self._last_chunk_index != chunk_index:\n            self._prepare_thread.download([chunk_index])\n            self._last_chunk_index = chunk_index\n\n        # Read the item from the chunk\n        item = self._config.read_item(index)\n\n        # Preload the next chunk if possible to gain some time\n        next_chunk_index = self._get_chunk_index_from_index(index.index + 1)\n        if self._prepare_thread is None:\n            raise Exception(\"The prepare thread should be defined.\")\n        if next_chunk_index != self._last_chunk_index:\n            self._prepare_thread.pre_load_chunk(next_chunk_index)\n            self._last_chunk_index = next_chunk_index\n\n        return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The provided index `{index}` isn't an instance of `ChunkedIndex`.\")\n\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        assert self._prepare_thread is not None, \"The prepare thread should be defined.\"\n\n        # Get the chunk index from the global index\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Check if the chunk is already loaded in memory\n        if self._item_loader.is_loaded(chunk_index):\n            return self._item_loader.get_item(chunk_index, index.chunk_index)\n\n        # Download the chunk if it's not already downloaded\n        if not self._config.is_chunk_available(chunk_index):\n            self._prepare_thread.download([chunk_index])\n\n        # Preload the chunk\n        self._prepare_thread.download([chunk_index])\n\n        # Load the chunk\n        self._config.load_chunk_from_index(chunk_index)\n\n        # Delete the chunk if it's not needed anymore\n        if self._prepare_thread._delete_chunks_when_processed:\n            self._prepare_thread.delete([self._last_chunk_index])\n            self._last_chunk_index = chunk_index\n\n        # Get the item from the chunk\n        return self._item_loader.get_item(chunk_index, index.chunk_index)\n\n    "}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index needs to be a ChunkedIndex.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Check if the chunk is available in the local cache\n        chunk_filepath, _, _ = self._config[index]\n        if not os.path.exists(chunk_filepath):\n            # If the chunk is not available, download it\n            if self._prepare_thread is None:\n                raise Exception(\"The prepare thread should be defined.\")\n            self._prepare_thread.download([chunk_index])\n\n        # Check if the chunk is available in memory\n        if not self._item_loader.is_chunk_loaded(chunk_index):\n            self._item_loader.load_chunk(chunk_index, chunk_filepath)\n\n        # Read the item from the chunk\n        item = self._item_loader.read_item(chunk_index, index.item_index)\n\n        # Check if the chunk can be deleted\n        if self._prepare_thread is None:\n            raise Exception(\"The prepare thread should be defined.\")\n        if self._prepare_thread._delete_chunks_when_processed and chunk_index == self._last_chunk_index:\n            self._prepare_thread.delete([chunk_index])\n\n        # Update the last chunk index\n        self._last_chunk_index = chunk_index\n\n        return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The provided index should be an instance of ChunkedIndex. Got {type(index)}.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Download the chunk if it isn't available\n        chunk_index = self._get_chunk_index_from_index(index.index)\n        if chunk_index != self._last_chunk_index:\n            self._last_chunk_index = chunk_index\n            self._config.download_chunk_from_index(chunk_index)\n\n        # Ensure the prepare thread is running\n        assert self._prepare_thread is not None, \"The prepare thread should be running.\"\n\n        # Preload item if possible to gain some time but only\n        # if this is one of the pre-downloaded chunk\n        if self._prepare_thread._pre_download_counter > 0:\n            self._prepare_thread._pre_load_chunk(chunk_index)\n\n        # Download the next chunk\n        if self._prepare_thread._pre_download_counter < self._prepare_thread._max_pre_download:\n            self._prepare_thread.download([chunk_index + 1])\n\n        # Load the item from the chunk\n        return self._config.read_item_from_chunk(index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The index `{index}` should be an instance of ChunkedIndex.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index from the index\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Get the chunk filepath\n        chunk_filepath, _, _ = self.config[index]\n\n        # Download the chunk if necessary\n        if not os.path.exists(chunk_filepath):\n            self.config.download_chunk_from_index(chunk_index)\n\n        # Load the chunk\n        chunk = self._item_loader.load_chunk(chunk_index, chunk_filepath)\n\n        # Prefetch the next chunk\n        if self._prepare_thread is not None:\n            self._prepare_thread.download([chunk_index + 1])\n\n        # Load the item\n        item = self._item_loader.load_item(chunk, index.chunk_index)\n\n        # Delete the chunk if necessary\n        if self._prepare_thread is not None:\n            self._prepare_thread.delete([chunk_index])\n\n        return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The index should be an instance of ChunkedIndex but got {type(index)}.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Load the chunk from the cache if it's not already loaded\n        if index.chunk_index != self._last_chunk_index:\n            self._last_chunk_index = index.chunk_index\n            self._config.load_chunk(index.chunk_index)\n\n        # Download the chunk if it's not already downloaded\n        if not self._config.is_chunk_downloaded(index.chunk_index):\n            assert self._prepare_thread is not None, \"The prepare thread should be defined.\"\n            self._prepare_thread.download([index.chunk_index])\n\n        # Read the item from the chunk\n        item = self._config.read_item(index)\n\n        # Delete the chunk if it's fully consumed\n        if self._prepare_thread is not None and self._config.is_chunk_consumed(index.chunk_index):\n            self._prepare_thread.delete([index.chunk_index])\n\n        return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The index should be an instance of `ChunkedIndex`. Received {type(index)}\")\n\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Load the chunk index\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Load the chunk\n        chunk_filepath, chunk_offset, chunk_length = self._config[index]\n\n        # Download the chunk if it's not available\n        if not os.path.exists(chunk_filepath):\n            assert self._prepare_thread is not None\n            self._prepare_thread.download([chunk_index])\n\n        # Load the chunk\n        item = self._item_loader.load_item_from_chunk(chunk_filepath, chunk_offset, chunk_length)\n\n        # Prefetch the next chunk\n        if self._prepare_thread is not None:\n            self._prepare_thread.download([chunk_index + 1])\n\n        # Delete the previous chunk\n        if self._last_chunk_index is not None:\n            assert self._prepare_thread is not None\n            self._prepare_thread.delete([self._last_chunk_index])\n\n        self._last_chunk_index = chunk_index\n\n        return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The index needs to be a `ChunkedIndex`, got {type(index)} instead.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        chunk_index = self._get_chunk_index_from_index(index.index)\n        # Check if the chunk is available locally\n        if chunk_index != self._last_chunk_index:\n            self._last_chunk_index = chunk_index\n            # Check if the chunk is available locally\n            if self._config.is_chunk_available(chunk_index):\n                self._config.load_chunk_from_index(chunk_index)\n            else:\n                # Initiate the download of the chunk\n                assert self._prepare_thread is not None, \"The prepare thread should be defined.\"\n                self._prepare_thread.download([chunk_index])\n\n        return self._config.read_item(index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The index should be an instance of ChunkedIndex. Received: {index}.\")\n\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Load the config containing the index\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Ensure the prepare thread is running\n        assert self._prepare_thread is not None, \"The prepare thread should be running.\"\n\n        # Download the chunk if it's not available\n        chunk_filepath, _, _ = self._config[chunk_index]\n        self._item_loader.download_chunk(chunk_index, chunk_filepath)\n\n        # Load the item from the chunk\n        item = self._item_loader.load_item(chunk_index, chunk_filepath, index.chunk_index)\n\n        # Delete the chunk if it's the last one\n        if chunk_index != self._last_chunk_index:\n            self._prepare_thread.delete([chunk_index])\n\n        # Update the last chunk index\n        self._last_chunk_index = chunk_index\n\n        return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The index should be of type {ChunkedIndex.__name__}.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index from the global index\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Check if the chunk is available locally\n        if not self.config.is_chunk_available(chunk_index):\n            # Download the chunk if it's not available locally\n            self.config.download_chunk_from_index(chunk_index)\n\n        # Load the chunk\n        chunk = self.config.load_chunk_from_index(chunk_index)\n\n        # Prefetch the next chunk if it's not already available locally\n        if self._prepare_thread is not None:\n            self._prepare_thread.download([chunk_index + 1])\n\n        # Read the item from the chunk\n        item = self._item_loader.load_item(chunk, index.chunk_index)\n\n        # Delete the chunk if it's the last one to be consumed\n        if self._last_chunk_index is not None and self._last_chunk_index != chunk_index:\n            assert self._prepare_thread is not None\n            self._prepare_thread.delete([self._last_chunk_index])\n\n        self._last_chunk_index = chunk_index\n\n        return item\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The provided index should be an instance of ChunkedIndex but got {type(index)}\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Ensure the prepare thread is running\n        assert self._prepare_thread is not None\n\n        # Get the chunk index from the index\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Download the chunk if it's not available\n        if self._prepare_thread._has_exited:\n            raise RuntimeError(\"The prepare thread has exited.\")\n\n        if chunk_index != self._last_chunk_index:\n            self._prepare_thread.download([chunk_index])\n            self._last_chunk_index = chunk_index\n\n        # Load the chunk\n        chunk_filepath, _, _ = self._config[index]\n        return self._item_loader.load_chunk(index.chunk_index, chunk_filepath)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The provided index should be an instance of ChunkedIndex but got {type(index)}.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Check whether the chunk has been downloaded\n        chunk_filepath, _, _ = self._config[ChunkedIndex(index=-1, chunk_index=chunk_index)]\n        self._item_loader.download_chunk(chunk_index, chunk_filepath)\n\n        # Check whether the chunk has been loaded\n        if self._last_chunk_index != chunk_index:\n            self._item_loader.load_chunk(chunk_index, chunk_filepath)\n            self._last_chunk_index = chunk_index\n\n        # Check whether the prepare thread is defined\n        assert self._prepare_thread is not None\n\n        # Download the chunk if necessary\n        if not self._item_loader.is_chunk_available(chunk_index):\n            self._prepare_thread.download([chunk_index])\n\n        # Delete the chunk if necessary\n        if self._prepare_thread._has_exited:\n            self._prepare_thread.delete([chunk_index])\n\n        # Load the chunk if necessary\n        if not self._item_loader.is_chunk_available(chunk_index):\n            self._item_loader.load_chunk(chunk_index, chunk_filepath)\n\n        # Read the item\n        return self._item_loader.read_item(index.index, chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index should be an instance of ChunkedIndex.\")\n\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        assert self._prepare_thread is not None\n\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # If the chunk is already loaded, we can return the item\n        if chunk_index == self._last_chunk_index:\n            return self._item_loader.get_item(index.index)\n\n        # If the chunk isn't loaded, we need to load it\n        self._prepare_thread.download([chunk_index])\n\n        # Wait until the chunk is available\n        self._item_loader.wait_until_chunk_is_available(chunk_index)\n\n        # Update the last chunk index\n        self._last_chunk_index = chunk_index\n\n        # Return the item\n        return self._item_loader.get_item(index.index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The provided index `{index}` isn't an instance of `ChunkedIndex`.\")\n\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Check if the chunk is already loaded\n        chunk_index = self._get_chunk_index_from_index(index.index)\n        if chunk_index != self._last_chunk_index:\n            # If the chunk is not loaded, download it\n            assert self._prepare_thread is not None\n            self._prepare_thread.download([chunk_index])\n            self._last_chunk_index = chunk_index\n\n        # Check if the chunk is available\n        if not self._config.is_chunk_available(chunk_index):\n            assert self._prepare_thread is not None\n            self._prepare_thread.download([chunk_index])\n\n        # Load the chunk\n        chunk_filepath, _, _ = self._config[index]\n        return self._item_loader.load_chunk(chunk_index, chunk_filepath)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The index should be of type {ChunkedIndex}\")\n\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index from the index\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Check whether the chunk is already loaded\n        if self._last_chunk_index is None or self._last_chunk_index != chunk_index:\n            self._last_chunk_index = chunk_index\n\n            # Download the chunk if not already present\n            if self._config.is_chunk_missing(chunk_index):\n                assert self._prepare_thread is not None\n                self._prepare_thread.download([chunk_index])\n\n        # Load the chunk\n        chunk_filepath, _, _ = self._config[index]\n        item = self._item_loader.load_item_from_chunk(chunk_index, chunk_filepath, index.index)\n\n        # Check whether the chunk should be deleted\n        if self._prepare_thread is not None and self._prepare_thread._delete_chunks_when_processed:\n            self._prepare_thread.delete([chunk_index])\n\n        return item\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n\n    return _ImmutableDistributedMap().set_and_get(key, obj)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n\n    return _ImmutableDistributedMap().set_and_get(key, obj)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n    return _ImmutableDistributedMap().set_and_get(key, obj)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n    return _ImmutableDistributedMap().set_and_get(key, obj)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n    return _ImmutableDistributedMap().set_and_get(key, obj)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n    return _ImmutableDistributedMap().set_and_get(key, obj)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_STATE_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n\n    map = _ImmutableDistributedMap()\n    return map.set_and_get(key, obj)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_STATE_URL\") is None:\n        return obj\n\n    return _ImmutableDistributedMap().set_and_get(key, obj)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    else:\n        return obj"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n\n    immutable_distributed_map = _ImmutableDistributedMap()\n    return immutable_distributed_map.set_and_get(key, obj)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n    else:\n        return _ImmutableDistributedMap().set_and_get(key, obj)"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_STATE_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    try:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    except Exception as e:\n        logger.warning(f\"Failed to broadcast the following {key=} {obj=}.\", exc_info=e)\n        return obj"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Flatten the chunks_per_ranks list\n    chunks_per_ranks = [chunk for sublist in chunks_per_ranks for chunk in sublist]\n\n    # 2. Shuffle the flattened list based on the seed and current epoch\n    np.random.seed(seed + current_epoch)\n    np.random.shuffle(chunks_per_ranks)\n\n    # 3. Return the shuffled list\n    return chunks_per_ranks"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Shuffle the chunks\n    np.random.seed(seed + current_epoch)\n    np.random.shuffle(chunks_per_ranks)\n\n    # 2. Flatten the list of chunks\n    chunks_per_ranks = [chunk for sublist in chunks_per_ranks for chunk in sublist]\n\n    # 3. Shuffle the flattened list of chunks\n    np.random.seed(seed + current_epoch)\n    np.random.shuffle(chunks_per_ranks)\n\n    return chunks_per_ranks"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Flatten the chunks_per_ranks list\n    chunks_per_ranks = [chunk for sublist in chunks_per_ranks for chunk in sublist]\n\n    # 2. Shuffle the chunks_per_ranks list based on the seed and current epoch\n    np.random.seed(seed + current_epoch)\n    np.random.shuffle(chunks_per_ranks)\n\n    # 3. Split the shuffled chunks_per_ranks list into chunks_per_ranks_per_node lists\n    chunks_per_ranks_per_node = np.array_split(chunks_per_ranks, distributed_env.world_size)\n\n    # 4. Flatten the chunks_per_ranks_per_node lists\n    chunks_per_ranks_per_node = [\n        chunk for sublist in chunks_per_ranks_per_node for chunk in sublist\n    ]\n\n    return chunks_per_ranks_per_node"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Flatten the chunks_per_ranks list\n    chunks_per_ranks_flattened = [chunk for chunks in chunks_per_ranks for chunk in chunks]\n\n    # 2. Shuffle the flattened list based on the seed and the current epoch\n    np.random.seed(seed + current_epoch)\n    np.random.shuffle(chunks_per_ranks_flattened)\n\n    # 3. Return the shuffled list\n    return chunks_per_ranks_flattened"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Shuffle the chunks per ranks\n    np.random.seed(seed + current_epoch)\n    np.random.shuffle(chunks_per_ranks)\n\n    # 2. Flatten the list of chunks per ranks\n    chunks_per_ranks = [chunk for sublist in chunks_per_ranks for chunk in sublist]\n\n    # 3. Shuffle the flattened list of chunks per ranks\n    np.random.seed(seed + current_epoch)\n    np.random.shuffle(chunks_per_ranks)\n\n    return chunks_per_ranks"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Flatten the chunks_per_ranks list\n    chunks_per_ranks = [chunk for sublist in chunks_per_ranks for chunk in sublist]\n\n    # 2. Shuffle the chunks_per_ranks list based on the seed and current epoch\n    np.random.seed(seed + current_epoch)\n    np.random.shuffle(chunks_per_ranks)\n\n    # 3. Split the shuffled chunks_per_ranks list into chunks_per_ranks_per_node lists\n    chunks_per_ranks_per_node = np.array_split(chunks_per_ranks, distributed_env.world_size)\n\n    # 4. Flatten the chunks_per_ranks_per_node lists\n    chunks_per_ranks_per_node = [\n        chunk for sublist in chunks_per_ranks_per_node for chunk in sublist\n    ]\n\n    return chunks_per_ranks_per_node"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    np.random.seed(seed + current_epoch)\n\n    # 1. Flatten the chunks_per_ranks list\n    chunks_per_ranks = [chunk for chunks in chunks_per_ranks for chunk in chunks]\n\n    # 2. Shuffle the flattened list\n    np.random.shuffle(chunks_per_ranks)\n\n    # 3. Split the shuffled list into chunks_per_ranks_per_node\n    chunks_per_ranks_per_node = np.array_split(chunks_per_ranks, distributed_env.world_size)\n\n    # 4. Flatten the chunks_per_ranks_per_node list\n    chunks_per_ranks_per_node = [\n        chunk for chunks in chunks_per_ranks_per_node for chunk in chunks\n    ]\n\n    return chunks_per_ranks_per_node"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Flatten the chunk indexes assigned to each rank\n    flattened_chunks = [chunk for chunks in chunks_per_ranks for chunk in chunks]\n\n    # 2. Shuffle the flattened chunk indexes based on the seed and current epoch\n    np.random.seed(seed + current_epoch)\n    np.random.shuffle(flattened_chunks)\n\n    # 3. Assign the shuffled chunk indexes back to each rank\n    shuffled_chunks_per_ranks = [\n        flattened_chunks[i::distributed_env.world_size] for i in range(distributed_env.world_size)\n    ]\n\n    return shuffled_chunks_per_ranks"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    np.random.seed(seed + current_epoch)\n\n    # 1. Shuffle the chunks of data assigned to each node\n    np.random.shuffle(chunks_per_ranks)\n\n    # 2. Flatten the list of shuffled chunks\n    chunks_per_ranks = [chunk for chunks in chunks_per_ranks for chunk in chunks]\n\n    return chunks_per_ranks"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Flatten the chunks_per_ranks list\n    flattened_chunks_per_ranks = [\n        chunk_index for chunks in chunks_per_ranks for chunk_index in chunks\n    ]\n\n    # 2. Shuffle the flattened chunks_per_ranks list using the provided seed and current epoch\n    np.random.seed(seed + current_epoch)\n    np.random.shuffle(flattened_chunks_per_ranks)\n\n    # 3. Distribute the shuffled chunks_per_ranks list across nodes\n    shuffled_chunks_per_ranks = [\n        flattened_chunks_per_ranks[i::distributed_env.world_size]\n        for i in range(distributed_env.world_size)\n    ]\n\n    # 4. Flatten the shuffled chunks_per_ranks list\n    shuffled_chunks_per_ranks = [\n        chunk_index for chunks in shuffled_chunks_per_ranks for chunk_index in chunks\n    ]\n\n    return shuffled_chunks_per_ranks"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Create a flattened list of chunk indexes\n    flattened_chunks = [chunk for chunks in chunks_per_ranks for chunk in chunks]\n\n    # 2. Shuffle the chunk indexes based on the seed and current epoch\n    np.random.seed(seed + current_epoch)\n    np.random.shuffle(flattened_chunks)\n\n    # 3. Assign the shuffled chunk indexes back to each rank\n    chunks_per_ranks = [\n        flattened_chunks[i::distributed_env.world_size] for i in range(distributed_env.world_size)\n    ]\n\n    # 4. Flatten the chunk indexes again\n    flattened_chunks = [chunk for chunks in chunks_per_ranks for chunk in chunks]\n\n    return flattened_chunks"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Flatten the chunks_per_ranks list\n    chunks_per_ranks = [chunk for sublist in chunks_per_ranks for chunk in sublist]\n\n    # 2. Shuffle the flattened list using the provided seed and current epoch\n    rng = np.random.default_rng(seed + current_epoch)\n    rng.shuffle(chunks_per_ranks)\n\n    # 3. Distribute the shuffled chunks across nodes\n    num_chunks_per_node = len(chunks_per_ranks) // distributed_env.world_size\n    chunks_per_ranks = [\n        chunks_per_ranks[i : i + num_chunks_per_node]\n        for i in range(0, len(chunks_per_ranks), num_chunks_per_node)\n    ]\n\n    return chunks_per_ranks"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Get the shuffled chunk indexes for each node\n    shuffled_chunk_indexes = [\n        np.random.RandomState(seed + current_epoch + rank).permutation(chunks_per_rank).tolist()\n        for rank, chunks_per_rank in enumerate(chunks_per_ranks)\n    ]\n\n    # 2. Flatten the shuffled chunk indexes\n    flattened_chunk_indexes = [\n        chunk_index for sublist in shuffled_chunk_indexes for chunk_index in sublist\n    ]\n\n    return flattened_chunk_indexes"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Get the number of chunks per rank\n    num_chunks_per_rank = [len(chunks) for chunks in chunks_per_ranks]\n\n    # 2. Get the number of chunks per node\n    num_chunks_per_node = distributed_env.all_gather(num_chunks_per_rank)\n\n    # 3. Shuffle the chunks per node\n    np.random.seed(seed + current_epoch)\n    np.random.shuffle(num_chunks_per_node)\n\n    # 4. Get the shuffled chunks per rank\n    shuffled_chunks_per_rank = distributed_env.scatter(num_chunks_per_node)\n\n    # 5. Shuffle the chunks per rank\n    shuffled_chunks_per_rank = [\n        np.random.permutation(chunks).tolist() for chunks in shuffled_chunks_per_rank\n    ]\n\n    # 6. Flatten the shuffled chunks per rank\n    shuffled_chunks_per_rank = [\n        chunk for chunks in shuffled_chunks_per_rank for chunk in chunks\n    ]\n\n    return shuffled_chunks_per_rank"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Flatten the list of chunk indexes\n    flattened_chunks = [chunk for sublist in chunks_per_ranks for chunk in sublist]\n\n    # 2. Shuffle the flattened list based on the seed and current epoch\n    np.random.seed(seed + current_epoch)\n    np.random.shuffle(flattened_chunks)\n\n    # 3. Distribute the shuffled chunks across nodes\n    chunk_per_ranks = [\n        flattened_chunks[rank::distributed_env.world_size] for rank in range(distributed_env.world_size)\n    ]\n\n    return chunk_per_ranks"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Create a flattened list of chunk indexes\n    flattened_chunks = [chunk for chunks in chunks_per_ranks for chunk in chunks]\n\n    # 2. Shuffle the flattened list using the provided seed and current epoch\n    np.random.seed(seed + current_epoch)\n    np.random.shuffle(flattened_chunks)\n\n    # 3. Distribute the shuffled chunk indexes across nodes\n    num_chunks_per_node = len(flattened_chunks) // distributed_env.world_size\n    num_chunks_in_last_node = len(flattened_chunks) % distributed_env.world_size\n    chunks_per_ranks = [\n        flattened_chunks[i * num_chunks_per_node : (i + 1) * num_chunks_per_node]\n        for i in range(distributed_env.world_size - 1)\n    ]\n    chunks_per_ranks.append(\n        flattened_chunks[\n            (distributed_env.world_size - 1) * num_chunks_per_node : (distributed_env.world_size - 1) * num_chunks_per_node\n            + num_chunks_in_last_node\n        ]\n    )\n\n    return chunks_per_ranks"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Create a random number generator based on the provided seed and current epoch\n    rng = np.random.default_rng(seed + current_epoch)\n\n    # 2. Flatten the chunks_per_ranks list to get a 1D list of chunk indexes\n    flat_chunks_per_ranks = [chunk for sublist in chunks_per_ranks for chunk in sublist]\n\n    # 3. Shuffle the flat_chunks_per_ranks list using the random number generator\n    rng.shuffle(flat_chunks_per_ranks)\n\n    # 4. Return the shuffled flat_chunks_per_ranks list\n    return flat_chunks_per_ranks"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Get the number of chunks per rank\n    num_chunks_per_ranks = [len(chunks) for chunks in chunks_per_ranks]\n\n    # 2. Get the number of chunks per node\n    num_chunks_per_node = distributed_env.num_chunks_per_node\n\n    # 3. Get the number of nodes per rank\n    num_nodes_per_ranks = [\n        num_chunks // num_chunks_per_node + (num_chunks % num_chunks_per_node != 0)\n        for num_chunks in num_chunks_per_ranks\n    ]\n\n    # 4. Get the number of nodes\n    num_nodes = distributed_env.num_nodes\n\n    # 5. Get the number of chunks per rank per node\n    num_chunks_per_ranks_per_node = [\n        [\n            num_chunks_per_rank // num_nodes_per_rank + (num_chunks_per_rank % num_nodes_per_rank != 0)\n            for _ in range(num_nodes_per_rank)\n        ]\n        for num_chunks_per_rank, num_nodes_per_rank in zip(num_chunks_per_ranks, num_nodes_per_ranks)\n    ]\n\n    # 6. Get the number of chunks per node per rank\n    num_chunks_per_node_per_rank = [\n        [\n            num_chunks_per_rank_per_node[rank]\n            for num_chunks_per_rank_per_node in num_chunks_per_ranks_per_node\n        ]\n        for rank in range(num_nodes)\n    ]\n\n    # 7. Get the number of chunks per node per rank\n    num_chunks_per_node_per_rank = [\n        [\n            num_chunks_per_rank_per_node[rank]\n            for num_chunks_per_rank_per_node in num_chunks_"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    if distributed_env.world_size == 1:\n        return [chunk_index for chunk_indexes in chunks_per_ranks for chunk_index in chunk_indexes]\n\n    # 1. Create a random number generator\n    rng = np.random.default_rng(seed + current_epoch)\n\n    # 2. Shuffle the chunks of data\n    chunks_per_ranks_shuffled = [\n        rng.permutation(chunk_indexes).tolist()\n        for chunk_indexes in chunks_per_ranks\n    ]\n\n    # 3. Flatten the shuffled chunks of data\n    chunks_per_ranks_shuffled_flattened = [\n        chunk_index for chunk_indexes in chunks_per_ranks_shuffled for chunk_index in chunk_indexes\n    ]\n\n    return chunks_per_ranks_shuffled_flattened"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Flatten the chunks_per_ranks list\n    flattened_chunks_per_ranks = [\n        chunk_index for chunks_per_rank in chunks_per_ranks for chunk_index in chunks_per_rank\n    ]\n\n    # 2. Create a numpy array from the flattened list\n    chunks_array = np.array(flattened_chunks_per_ranks)\n\n    # 3. Shuffle the numpy array using a deterministic random number generator\n    rng = np.random.default_rng(seed + current_epoch)\n    rng.shuffle(chunks_array)\n\n    # 4. Convert the shuffled numpy array back to a list\n    shuffled_chunks_per_ranks = chunks_array.tolist()\n\n    # 5. Return the shuffled chunks_per_ranks list\n    return shuffled_chunks_per_ranks"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if len(inputs) > 1:\n        first_path = inputs[0]\n        second_path = inputs[1]\n\n        if isinstance(first_path, str) and os.path.exists(first_path):\n            if isinstance(second_path, str) and os.path.exists(second_path):\n                if os.path.dirname(first_path) != os.path.dirname(second_path):\n                    raise ValueError(\n                        f\"The provided inputs should be in the same directory. Found {first_path} and {second_path}.\"\n                    )\n                return os.path.dirname(first_path)\n            elif isinstance(second_path, list) and isinstance(second_path[0], str) and os.path.exists(second_path[0]):\n                if os.path.dirname(first_path) != os.path.dirname(second_path[0]):\n                    raise ValueError(\n                        f\"The provided inputs should be in the same directory. Found {first_path} and {second_path[0]}.\"\n                    )\n                return os.path.dirname(first_path)\n\n        elif isinstance(first_path, list) and isinstance(first_path[0], str) and os.path.exists(first_path[0]):\n            if isinstance(second_path, str) and os.path.exists(second_path):\n                if os.path.dirname(first_path[0]) != os.path.dirname(second_path):\n                    raise ValueError(\n                        f\"The provided inputs should be in the same directory. Found {first_path[0]} and {second_path}.\"\n                    )\n                return os.path.dirname(first_path[0])\n            elif isinstance(second_path, list) and isinstance(second_path[0], str) and os.path.exists(second_path[0]):\n                if os.path.dirname(first_path[0]) != os.path.dirname(second_path[0]):\n                    raise ValueError(\n                        f"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    indexed_paths = _get_indexed_paths(inputs[0])\n\n    if len(inputs) > 1:\n        indexed_paths_2 = _get_indexed_paths(inputs[1])\n\n        if len(indexed_paths) != len(indexed_paths_2):\n            raise ValueError(\n                f\"The provided inputs should have the same number of indexed paths. Found {len(indexed_paths)} and {len(indexed_paths_2)}.\"\n            )\n\n        for index, path in indexed_paths.items():\n            if indexed_paths_2[index] != path:\n                raise ValueError(\n                    f\"The provided inputs should have the same indexed paths. Found {indexed_paths} and {indexed_paths_2}.\"\n                )\n\n    if len(indexed_paths) == 0:\n        raise ValueError(f\"The provided inputs should have at least one indexed path. Found {inputs}.\")\n\n    return str(Path(next(iter(indexed_paths.values()))).resolve())"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if len(inputs) == 0:\n        return None\n\n    if len(inputs) == 1:\n        if isinstance(inputs[0], str) and os.path.exists(inputs[0]):\n            return os.path.abspath(inputs[0])\n        return None\n\n    if len(inputs) == 2:\n        if isinstance(inputs[0], str) and os.path.exists(inputs[0]):\n            return os.path.abspath(inputs[0])\n\n        if isinstance(inputs[1], str) and os.path.exists(inputs[1]):\n            return os.path.abspath(inputs[1])\n\n        if isinstance(inputs[0], str) and isinstance(inputs[1], str) and inputs[0] != inputs[1]:\n            raise ValueError(\n                f\"The provided inputs are inconsistent. Found {inputs[0]} and {inputs[1]}.\"\n            )\n\n        return None\n\n    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) == 1:\n        return os.path.abspath(list(indexed_paths.values())[0])\n\n    if len(indexed_paths) == 2:\n        if list(indexed_paths.values())[0] != list(indexed_paths.values())[1]:\n            raise ValueError(\n                f\"The provided inputs are inconsistent. Found {list(indexed_paths.values())[0]} and {list(indexed_paths.values())[1]}.\"\n            )\n\n        return os.path.abspath(list(indexed_paths.values())[0])\n\n    return os.path.abspath(list(indexed_paths.values())[0])"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if len(inputs) == 0:\n        return None\n\n    if len(inputs) == 1:\n        if isinstance(inputs[0], str):\n            if os.path.exists(inputs[0]):\n                return os.path.dirname(inputs[0])\n            return None\n        if isinstance(inputs[0], (list, tuple)):\n            if len(inputs[0]) == 0:\n                return None\n            if os.path.exists(inputs[0][0]):\n                return os.path.dirname(inputs[0][0])\n            return None\n        if isinstance(inputs[0], dict):\n            if len(inputs[0]) == 0:\n                return None\n            if os.path.exists(list(inputs[0].values())[0]):\n                return os.path.dirname(list(inputs[0].values())[0])\n            return None\n\n    if len(inputs) > 1:\n        if isinstance(inputs[0], str) and isinstance(inputs[1], str):\n            if os.path.exists(inputs[0]) and os.path.exists(inputs[1]):\n                if os.path.dirname(inputs[0]) == os.path.dirname(inputs[1]):\n                    return os.path.dirname(inputs[0])\n                raise ValueError(\n                    f\"The provided inputs should have the same parent directory. Found {inputs[0]} and {inputs[1]}.\"\n                )\n            return None\n        if isinstance(inputs[0], (list, tuple)) and isinstance(inputs[1], (list, tuple)):\n            if len(inputs[0]) == 0 or len(inputs[1]) == 0:\n                return None\n            if os.path.exists(inputs[0][0]) and os.path.exists(inputs[1][0]):\n                if os.path.dirname(inputs[0][0]) == os.path.dirname(inputs[1][0]):\n                    return os."}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if len(inputs) == 0:\n        return None\n\n    if len(inputs) == 1:\n        if isinstance(inputs[0], str):\n            return _resolve_dir(inputs[0]).path\n\n        if isinstance(inputs[0], Path):\n            return _resolve_dir(str(inputs[0])).path\n\n        if isinstance(inputs[0], Sequence):\n            return _get_input_dir(inputs[0])\n\n        return None\n\n    first_path = _get_input_dir(inputs[:1])\n    second_path = _get_input_dir(inputs[1:2])\n\n    if first_path is None and second_path is None:\n        return None\n\n    if first_path is None:\n        return second_path\n\n    if second_path is None:\n        return first_path\n\n    if first_path != second_path:\n        raise ValueError(\n            f\"The provided inputs should contain the same filepath. Found {first_path} and {second_path}.\"\n        )\n\n    return first_path"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if len(inputs) == 0:\n        return None\n\n    if len(inputs) == 1:\n        if isinstance(inputs[0], str):\n            return os.path.dirname(inputs[0])\n        return None\n\n    indexed_paths = _get_indexed_paths(inputs[0])\n\n    if len(inputs) > 1:\n        indexed_paths_2 = _get_indexed_paths(inputs[1])\n\n        if len(indexed_paths) != len(indexed_paths_2):\n            raise ValueError(\n                f\"The provided inputs should contain the same number of indexed paths. Found {len(indexed_paths)} and {len(indexed_paths_2)}.\"\n            )\n\n        for index, path in indexed_paths.items():\n            if path != indexed_paths_2[index]:\n                raise ValueError(\n                    f\"The provided inputs should contain the same indexed paths. Found {path} and {indexed_paths_2[index]}.\"\n                )\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) > 1:\n        raise ValueError(\n            f\"The provided inputs should contain only one indexed path. Found {len(indexed_paths)}.\"\n        )\n\n    return os.path.dirname(indexed_paths[0])"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Extract the first two elements of the input sequence\n    first_element = inputs[0]\n    second_element = inputs[1] if len(inputs) > 1 else None\n\n    # Check if the first element is a valid file path\n    if isinstance(first_element, str) and os.path.exists(first_element):\n        first_path = Path(first_element)\n        if first_path.is_file():\n            return str(first_path.parent)\n\n    # Check if the second element is a valid file path\n    if isinstance(second_element, str) and os.path.exists(second_element):\n        second_path = Path(second_element)\n        if second_path.is_file():\n            return str(second_path.parent)\n\n    # Check if the first and second elements have consistent file paths\n    if isinstance(first_element, str) and isinstance(second_element, str):\n        first_path = Path(first_element)\n        second_path = Path(second_element)\n        if first_path.is_file() and second_path.is_file():\n            if first_path.parent != second_path.parent:\n                raise ValueError(\n                    f\"The provided inputs have inconsistent file paths. Found {first_path.parent} and {second_path.parent}.\"\n                )\n            return str(first_path.parent)\n\n    return None"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if len(inputs) == 0:\n        return None\n\n    if len(inputs) == 1:\n        if isinstance(inputs[0], (str, Path)):\n            return os.path.dirname(os.path.abspath(inputs[0]))\n        else:\n            raise ValueError(f\"The provided inputs should be non empty sequence of file paths. Found {inputs}.\")\n\n    if len(inputs) > 2:\n        raise ValueError(f\"The provided inputs should be non empty sequence of file paths. Found {inputs}.\")\n\n    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        raise ValueError(f\"The provided inputs should be non empty sequence of file paths. Found {inputs}.\")\n\n    if len(indexed_paths) > 1:\n        raise ValueError(f\"The provided inputs should be non empty sequence of file paths. Found {inputs}.\")\n\n    indexed_paths = _get_indexed_paths(inputs[0])\n\n    if len(indexed_paths) == 0:\n        indexed_paths = _get_indexed_paths(inputs[1])\n\n    if len(indexed_paths) == 0:\n        raise ValueError(f\"The provided inputs should be non empty sequence of file paths. Found {inputs}.\")\n\n    if len(indexed_paths) > 1:\n        raise ValueError(f\"The provided inputs should be non empty sequence of file paths. Found {inputs}.\")\n\n    indexed_paths = _get_indexed_paths(inputs[0])\n\n    if len(indexed_paths) == 0:\n        indexed_paths = _get_indexed_paths(inputs[1])\n\n    if len(indexed_paths) == 0:\n        raise ValueError(f\"The provided inputs should be non empty sequence of file paths. Found {inputs}.\")\n\n    if len(indexed_paths) > 1:\n        raise ValueError(f\"The provided inputs should be non empty sequence of file paths. Found {"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if len(inputs) == 0:\n        return None\n\n    if len(inputs) == 1:\n        return _resolve_dir(inputs[0]).path\n\n    if isinstance(inputs[0], str) and os.path.exists(inputs[0]):\n        return _resolve_dir(inputs[0]).path\n\n    if isinstance(inputs[0], Path) and inputs[0].exists():\n        return _resolve_dir(inputs[0]).path\n\n    if isinstance(inputs[0], (list, tuple)) and len(inputs[0]) > 0 and isinstance(inputs[0][0], str):\n        return _resolve_dir(inputs[0][0]).path\n\n    if isinstance(inputs[0], (list, tuple)) and len(inputs[0]) > 0 and isinstance(inputs[0][0], Path):\n        return _resolve_dir(inputs[0][0]).path\n\n    if isinstance(inputs[0], dict) and len(inputs[0]) > 0:\n        return _resolve_dir(list(inputs[0].values())[0]).path\n\n    if isinstance(inputs[0], (list, tuple)) and len(inputs[0]) > 0 and isinstance(inputs[0][0], dict):\n        return _resolve_dir(list(inputs[0][0].values())[0]).path\n\n    if isinstance(inputs[0], (list, tuple)) and len(inputs[0]) > 0 and isinstance(inputs[0][0], (list, tuple)):\n        return _resolve_dir(inputs[0][0][0]).path\n\n    if isinstance(inputs[0], dict) and len(inputs[0]) > 0 and isinstance(list(inputs[0].values())[0], dict):\n        return _resolve_dir(list(list(inputs[0].values())[0].values())[0]).path\n\n    if isinstance(inputs[0], (list, tuple)) and len(input"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if len(inputs) == 0:\n        raise ValueError(\"The provided inputs should be non empty.\")\n\n    if len(inputs) == 1:\n        if isinstance(inputs[0], str) and os.path.exists(inputs[0]):\n            return os.path.dirname(inputs[0])\n\n        if isinstance(inputs[0], Sequence) and len(inputs[0]) > 0 and os.path.exists(inputs[0][0]):\n            return os.path.dirname(inputs[0][0])\n\n        raise ValueError(f\"The provided inputs should contain a valid filepath. Found {inputs}.\")\n\n    if len(inputs) > 1:\n        if isinstance(inputs[0], str) and os.path.exists(inputs[0]):\n            if isinstance(inputs[1], str) and os.path.exists(inputs[1]):\n                if os.path.dirname(inputs[0]) != os.path.dirname(inputs[1]):\n                    raise ValueError(\n                        f\"The provided inputs should contain files from the same directory. Found {inputs}.\"\n                    )\n                return os.path.dirname(inputs[0])\n\n            if isinstance(inputs[1], Sequence) and len(inputs[1]) > 0 and os.path.exists(inputs[1][0]):\n                if os.path.dirname(inputs[0]) != os.path.dirname(inputs[1][0]):\n                    raise ValueError(\n                        f\"The provided inputs should contain files from the same directory. Found {inputs}.\"\n                    )\n                return os.path.dirname(inputs[0])\n\n        if isinstance(inputs[0], Sequence) and len(inputs[0]) > 0 and os.path.exists(inputs[0][0]):\n            if isinstance(inputs[1], str) and os.path.exists(inputs[1]):\n                if os.path.dirname(inputs[0][0]) !"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if len(inputs) == 0:\n        return None\n\n    if len(inputs) > 2:\n        inputs = inputs[:2]\n\n    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) > 1:\n        raise ValueError(\n            f\"The provided inputs should contain at most one valid file path. Found {inputs}.\"\n        )\n\n    input_dir = Path(next(iter(indexed_paths.values()))).parent.as_posix()\n\n    if _IS_IN_STUDIO:\n        input_dir = os.path.join(\"/teamspace\", input_dir)\n    else:\n        input_dir = os.path.abspath(input_dir)\n\n    return input_dir"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if len(inputs) == 0:\n        return None\n\n    if len(inputs) == 1:\n        if isinstance(inputs[0], str):\n            return inputs[0]\n        elif isinstance(inputs[0], Path):\n            return str(inputs[0])\n        else:\n            return _get_input_dir(inputs[0])\n\n    indexed_paths = _get_indexed_paths(inputs[0])\n\n    if len(indexed_paths) > 0:\n        if len(inputs) > 1:\n            for index, element in _get_indexed_paths(inputs[1]).items():\n                if index not in indexed_paths:\n                    raise ValueError(\n                        f\"Inconsistent file paths found. Found {indexed_paths[index]} and {element}.\"\n                    )\n        return indexed_paths[0]\n\n    return _get_input_dir(inputs[1:])"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if len(inputs) == 0:\n        return None\n\n    indexed_paths = _get_indexed_paths(inputs[0])\n\n    if len(inputs) > 1:\n        indexed_paths_2 = _get_indexed_paths(inputs[1])\n\n        if len(indexed_paths) != len(indexed_paths_2):\n            raise ValueError(\n                f\"The provided inputs should have the same number of indexed paths. Found {len(indexed_paths)} and {len(indexed_paths_2)}.\"\n            )\n\n        for index, path in indexed_paths.items():\n            path_2 = indexed_paths_2.get(index)\n\n            if path_2 is None:\n                raise ValueError(\n                    f\"The provided inputs should have the same indexed paths. Found {path} and None.\"\n                )\n\n            if path != path_2:\n                raise ValueError(\n                    f\"The provided inputs should have the same indexed paths. Found {path} and {path_2}.\"\n                )\n\n    if len(indexed_paths) == 0:\n        return None\n\n    indexed_path = list(indexed_paths.values())[0]\n    indexed_path = Path(indexed_path)\n\n    if indexed_path.is_absolute():\n        return str(indexed_path.parent)\n\n    if _IS_IN_STUDIO:\n        return os.path.join(\"/teamspace\", \"datasets\", indexed_path.parts[0])\n    return os.path.join(\"/teamspace\", \"s3_connections\", indexed_path.parts[0])"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths: Dict[int, str] = {}\n\n    if isinstance(inputs[0], str):\n        indexed_paths = _get_indexed_paths(inputs[0])\n\n    if isinstance(inputs[1], str):\n        indexed_paths.update(_get_indexed_paths(inputs[1]))\n\n    if len(indexed_paths) == 0:\n        raise ValueError(\n            f\"The provided inputs should contain at least a valid filepath. Found {inputs}.\"\n        )\n\n    if len(indexed_paths) > 1:\n        raise ValueError(\n            f\"The provided inputs should contain at most one valid filepath. Found {inputs}.\"\n        )\n\n    input_dir = list(indexed_paths.values())[0]\n    input_dir = Path(input_dir).parent.absolute()\n\n    if \"cloudspaces\" in input_dir.as_posix():\n        input_dir = input_dir.relative_to(Path(input_dir.as_posix().split(\"cloudspaces\")[0]))\n\n    return input_dir.as_posix()"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if len(inputs) == 0:\n        return None\n\n    indexed_paths = _get_indexed_paths(inputs[0])\n\n    if len(inputs) > 1:\n        for index, path in _get_indexed_paths(inputs[1]).items():\n            if index not in indexed_paths:\n                indexed_paths[index] = path\n            elif indexed_paths[index] != path:\n                raise ValueError(\n                    f\"The provided inputs are inconsistent. Found {indexed_paths[index]} and {path}.\"\n                )\n\n    if len(indexed_paths) == 0:\n        return None\n\n    path = indexed_paths[0]\n\n    if not os.path.isabs(path):\n        path = os.path.abspath(path)\n\n    if \"cloudspaces\" in path:\n        return path\n\n    if \"datasets\" in path:\n        return path\n\n    if os.path.isfile(path):\n        return os.path.dirname(path)\n\n    return path"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    input_dir = None\n    for index, element in enumerate(inputs[:2]):\n        if isinstance(element, str) and os.path.exists(element):\n            input_dir = element\n            break\n\n    if input_dir is None:\n        raise ValueError(f\"The provided inputs {inputs} should contain at least a valid filepath.\")\n\n    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) > 1:\n        raise ValueError(f\"The provided inputs {inputs} should contain at most one valid filepath.\")\n\n    if len(indexed_paths) == 1:\n        index, input_dir = next(iter(indexed_paths.items()))\n        input_dir = os.path.dirname(input_dir)\n\n    input_dir = Path(input_dir).resolve().as_posix()\n\n    if not input_dir.startswith(\"/\"):\n        input_dir = \"/\" + input_dir\n\n    if _IS_IN_STUDIO:\n        input_dir = f\"/teamspace{input_dir}\"\n\n    return input_dir"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs[0])\n    indexed_paths.update(_get_indexed_paths(inputs[1]))\n\n    if len(indexed_paths) == 0:\n        raise ValueError(\n            \"The provided inputs should contain at least two valid file paths. \"\n            f\"Found {inputs[0]} and {inputs[1]}.\"\n        )\n\n    indexed_paths_values = list(indexed_paths.values())\n    if len(indexed_paths_values) != 1:\n        raise ValueError(\n            f\"The provided inputs should contain at least two valid file paths. \"\n            f\"Found {indexed_paths_values}.\"\n        )\n\n    input_dir = Path(indexed_paths_values[0]).parent.absolute()\n    if input_dir.is_relative_to(Path.home()):\n        input_dir = f\"~{str(input_dir).split('~')[-1]}\"\n\n    return input_dir"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if len(inputs) == 0:\n        return None\n\n    if len(inputs) == 1:\n        if isinstance(inputs[0], str):\n            if os.path.exists(inputs[0]):\n                return inputs[0]\n            else:\n                raise ValueError(f\"The provided path isn't valid. Found {inputs[0]}.\")\n        elif isinstance(inputs[0], Path):\n            if inputs[0].exists():\n                return str(inputs[0])\n            else:\n                raise ValueError(f\"The provided path isn't valid. Found {inputs[0]}.\")\n        else:\n            raise ValueError(f\"The provided input isn't valid. Found {inputs[0]}.\")\n\n    if len(inputs) > 2:\n        raise ValueError(f\"The provided inputs should be at most 2. Found {len(inputs)}.\")\n\n    if isinstance(inputs[0], str) and isinstance(inputs[1], str):\n        if os.path.exists(inputs[0]) and os.path.exists(inputs[1]):\n            if os.path.dirname(inputs[0]) != os.path.dirname(inputs[1]):\n                raise ValueError(f\"The provided inputs should have the same parent directory. Found {inputs}.\")\n            return os.path.dirname(inputs[0])\n        else:\n            raise ValueError(f\"The provided paths aren't valid. Found {inputs}.\")\n    elif isinstance(inputs[0], Path) and isinstance(inputs[1], Path):\n        if inputs[0].exists() and inputs[1].exists():\n            if inputs[0].parent != inputs[1].parent:\n                raise ValueError(f\"The provided inputs should have the same parent directory. Found {inputs}.\")\n            return str(inputs[0].parent)\n        else:\n            raise ValueError(f\"The provided paths aren't valid. Found {inputs}.\")\n    else:\n        raise ValueError(f\"The provided inputs aren't"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if len(inputs) == 0:\n        raise ValueError(\"The provided inputs should be non empty.\")\n\n    if isinstance(inputs[0], str):\n        first_path = inputs[0]\n    elif isinstance(inputs[0], Path):\n        first_path = str(inputs[0])\n    else:\n        raise ValueError(\n            f\"The provided inputs should contain at least a valid filepath. Found {inputs[0]}.\"\n        )\n\n    if len(inputs) == 1:\n        if os.path.exists(first_path):\n            return os.path.dirname(first_path)\n        raise ValueError(f\"The provided inputs should contain at least a valid filepath. Found {inputs[0]}.\")\n\n    if isinstance(inputs[1], str):\n        second_path = inputs[1]\n    elif isinstance(inputs[1], Path):\n        second_path = str(inputs[1])\n    else:\n        raise ValueError(\n            f\"The provided inputs should contain at least a valid filepath. Found {inputs[1]}.\"\n        )\n\n    if not os.path.exists(first_path) or not os.path.exists(second_path):\n        raise ValueError(\n            f\"The provided inputs should contain at least a valid filepath. Found {inputs[0]} and {inputs[1]}.\"\n        )\n\n    if os.path.dirname(first_path) != os.path.dirname(second_path):\n        raise ValueError(\n            f\"The provided inputs should contain files from the same directory. Found {inputs[0]} and {inputs[1]}.\"\n        )\n\n    input_dir = os.path.dirname(first_path)\n\n    if _IS_IN_STUDIO:\n        input_dir = f\"/teamspace/datasets/{input_dir}\"\n    else:\n        input_dir = os.path.abspath(input_dir)\n\n    return input_dir"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths: Dict[int, str] = {}\n\n    if isinstance(inputs[0], str) and os.path.exists(inputs[0]):\n        indexed_paths = {0: inputs[0]}\n    elif isinstance(inputs[0], (list, tuple)) and isinstance(inputs[0][0], str) and os.path.exists(inputs[0][0]):\n        indexed_paths = {0: inputs[0][0]}\n    elif isinstance(inputs[1], str) and os.path.exists(inputs[1]):\n        indexed_paths = {1: inputs[1]}\n    elif isinstance(inputs[1], (list, tuple)) and isinstance(inputs[1][0], str) and os.path.exists(inputs[1][0]):\n        indexed_paths = {1: inputs[1][0]}\n\n    if len(indexed_paths) == 0:\n        raise ValueError(f\"The provided inputs should contain at least one valid file path. Found {inputs}.\")\n\n    if len(indexed_paths) == 2:\n        if indexed_paths[0] != indexed_paths[1]:\n            raise ValueError(\n                f\"The provided inputs should contain at least two valid file paths. Found {inputs}.\"\n                f\" HINT: The inputs should have the same file path.\"\n            )\n        indexed_paths = {0: indexed_paths[0]}\n\n    input_dir = Path(list(indexed_paths.values())[0]).parent.absolute()\n\n    if \"cloudspaces\" in input_dir.as_uri():\n        input_dir = input_dir.relative_to(input_dir.root)\n\n    return input_dir.as_posix()"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        optimize_dns(enable)\n        yield\n    finally:\n        optimize_dns(False)"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        optimize_dns(enable)\n        yield\n    finally:\n        optimize_dns(False)"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        optimize_dns(enable)\n        yield\n    finally:\n        optimize_dns(False)"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        optimize_dns(enable)\n        yield\n    finally:\n        optimize_dns(False)"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        optimize_dns(enable)\n        yield\n    finally:\n        optimize_dns(False)"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        optimize_dns(enable)\n        yield\n    finally:\n        optimize_dns(False)"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if not _IS_IN_STUDIO:\n        yield\n        return\n\n    try:\n        optimize_dns(enable)\n        yield\n    finally:\n        optimize_dns(False)"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    optimize_dns(enable)\n    try:\n        yield\n    finally:\n        optimize_dns(False)"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    optimize_dns(enable)\n    try:\n        yield\n    finally:\n        optimize_dns(False)"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        optimize_dns(enable)\n        yield\n    finally:\n        optimize_dns(False)\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        optimize_dns(enable)\n        yield\n    finally:\n        optimize_dns(False)\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        optimize_dns(enable=enable)\n        yield\n    finally:\n        optimize_dns(enable=False)"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        optimize_dns(enable)\n        yield\n    finally:\n        optimize_dns(False)\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if not _IS_IN_STUDIO:\n        yield\n        return\n\n    optimize_dns(enable=enable)\n    try:\n        yield\n    finally:\n        optimize_dns(enable=False)"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        optimize_dns(enable)\n        yield\n    finally:\n        optimize_dns(False)\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    try:\n        optimize_dns(enable=enable)\n        yield\n    finally:\n        optimize_dns(enable=False)"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    with optimize_dns_context(enable):\n        yield\n\n\n\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if not _IS_IN_STUDIO:\n        yield\n        return\n\n    with optimize_dns_context(enable=enable):\n        yield\n\n\n\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if not _IS_IN_STUDIO:\n        yield\n        return\n\n    optimize_dns(enable)\n    try:\n        yield\n    finally:\n        optimize_dns(False)\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    with optimize_dns_context(enable):\n        yield\n\n\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    if drop_last:\n        num_items_per_rank = num_items_per_rank * distributed_env.world_size\n    else:\n        num_items_per_rank = num_items_per_rank + len(indexes) % distributed_env.world_size\n\n    # assign chunks and their intervals to each rank\n    chunks_per_ranks = []\n    intervals_per_ranks = []\n    for rank in range(distributed_env.world_size):\n        start = rank * num_items_per_rank\n        end = start + num_items_per_rank\n        chunks_per_ranks.append(indexes[start:end])\n        intervals_per_ranks.append(chunk_intervals[start:end])\n\n    return chunks_per_ranks, intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    if drop_last:\n        num_items_per_rank = num_items_per_rank * distributed_env.world_size\n    else:\n        num_items_per_rank = num_items_per_rank + len(indexes) % distributed_env.world_size\n\n    # assign chunks and their intervals to each rank\n    chunks_per_ranks = []\n    intervals_per_ranks = []\n    for rank in range(distributed_env.world_size):\n        start = rank * num_items_per_rank\n        end = start + num_items_per_rank\n        chunks_per_ranks.append(indexes[start:end])\n        intervals_per_ranks.append(chunk_intervals[start:end])\n\n    return chunks_per_ranks, intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    if not drop_last:\n        num_items_per_rank += len(indexes) % distributed_env.world_size\n\n    # assign chunks and their intervals to each rank\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    for i, (chunk_index, chunk_interval) in enumerate(zip(indexes, chunk_intervals)):\n        rank = i % distributed_env.world_size\n        chunks_per_ranks[rank].append(chunk_index)\n        intervals_per_ranks[rank].append(chunk_interval)\n\n    return chunks_per_ranks, intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    if not drop_last:\n        num_items_per_rank += len(indexes) % distributed_env.world_size\n\n    # assign chunks and their intervals to each rank\n    chunks_per_ranks = []\n    intervals_per_ranks = []\n    for rank in range(distributed_env.world_size):\n        start_index = rank * num_items_per_rank\n        end_index = start_index + num_items_per_rank\n        if rank == distributed_env.world_size - 1 and not drop_last:\n            end_index = len(indexes)\n        chunks_per_ranks.append(indexes[start_index:end_index])\n        intervals_per_ranks.append(chunk_intervals[start_index:end_index])\n\n    return chunks_per_ranks, intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    if drop_last:\n        num_items_per_rank = num_items_per_rank * distributed_env.world_size\n    else:\n        num_items_per_rank = num_items_per_rank + len(indexes) % distributed_env.world_size\n\n    # assign chunks and their intervals to each rank\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    for rank in range(distributed_env.world_size):\n        start_index = rank * num_items_per_rank\n        end_index = (rank + 1) * num_items_per_rank\n        chunks_per_ranks[rank] = indexes[start_index:end_index]\n        intervals_per_ranks[rank] = chunk_intervals[start_index:end_index]\n\n    return chunks_per_ranks, intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    if not drop_last:\n        num_items_per_rank += len(indexes) % distributed_env.world_size\n\n    # Assign chunks and their intervals to each rank\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    for rank in range(distributed_env.world_size):\n        start_index = rank * num_items_per_rank\n        end_index = (rank + 1) * num_items_per_rank\n        if rank == distributed_env.world_size - 1 and not drop_last:\n            end_index = len(indexes)\n        chunks_per_ranks[rank] = indexes[start_index:end_index]\n        intervals_per_ranks[rank] = chunk_intervals[start_index:end_index]\n\n    return chunks_per_ranks, intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    total_items = len(indexes)\n    items_per_rank = total_items // distributed_env.world_size\n    if drop_last:\n        indexes = indexes[: items_per_rank * distributed_env.world_size]\n    else:\n        indexes = indexes[: (items_per_rank + 1) * distributed_env.world_size]\n\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    for i in range(distributed_env.world_size):\n        chunks_per_ranks[i] = indexes[i * items_per_rank : (i + 1) * items_per_rank]\n        intervals_per_ranks[i] = chunk_intervals[i * items_per_rank : (i + 1) * items_per_rank]\n\n    return chunks_per_ranks, intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    total_items = len(indexes)\n    items_per_rank = total_items // distributed_env.world_size\n    if drop_last:\n        indexes = indexes[: items_per_rank * distributed_env.world_size]\n        chunk_intervals = chunk_intervals[: items_per_rank * distributed_env.world_size]\n    else:\n        indexes = indexes[: (items_per_rank + 1) * distributed_env.world_size]\n        chunk_intervals = chunk_intervals[: (items_per_rank + 1) * distributed_env.world_size]\n\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    for rank in range(distributed_env.world_size):\n        chunks_per_ranks[rank] = indexes[rank * items_per_rank : (rank + 1) * items_per_rank]\n        intervals_per_ranks[rank] = chunk_intervals[rank * items_per_rank : (rank + 1) * items_per_rank]\n\n    return chunks_per_ranks, intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    num_items = len(indexes)\n    items_per_rank = num_items // distributed_env.world_size\n    if drop_last:\n        indexes = indexes[: items_per_rank * distributed_env.world_size]\n        chunk_intervals = chunk_intervals[: items_per_rank * distributed_env.world_size]\n    else:\n        indexes = indexes[: (items_per_rank + 1) * distributed_env.world_size]\n        chunk_intervals = chunk_intervals[: (items_per_rank + 1) * distributed_env.world_size]\n\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    for rank in range(distributed_env.world_size):\n        chunks_per_ranks[rank] = indexes[rank * items_per_rank : (rank + 1) * items_per_rank]\n        intervals_per_ranks[rank] = chunk_intervals[\n            rank * items_per_rank : (rank + 1) * items_per_rank\n        ]\n\n    return chunks_per_ranks, intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    num_items = len(indexes)\n    items_per_rank = num_items // distributed_env.world_size\n    if drop_last:\n        num_items = items_per_rank * distributed_env.world_size\n\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    chunk_intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n\n    for i in range(distributed_env.world_size):\n        chunks_per_ranks[i] = indexes[i * items_per_rank : (i + 1) * items_per_rank]\n        chunk_intervals_per_ranks[i] = chunk_intervals[i * items_per_rank : (i + 1) * items_per_rank]\n\n    return chunks_per_ranks, chunk_intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    num_items = len(indexes)\n    items_per_rank = num_items // distributed_env.world_size\n    if drop_last:\n        indexes = indexes[: items_per_rank * distributed_env.world_size]\n        chunk_intervals = chunk_intervals[: items_per_rank * distributed_env.world_size]\n    else:\n        indexes = indexes[: (items_per_rank + 1) * distributed_env.world_size]\n        chunk_intervals = chunk_intervals[: (items_per_rank + 1) * distributed_env.world_size]\n\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    for i, index in enumerate(indexes):\n        chunks_per_ranks[i % distributed_env.world_size].append(index)\n        intervals_per_ranks[i % distributed_env.world_size].append(chunk_intervals[i])\n\n    return chunks_per_ranks, intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    if not drop_last:\n        num_items_per_rank = (len(indexes) + distributed_env.world_size - 1) // distributed_env.world_size\n\n    # assign chunks and their intervals to each rank\n    chunks_per_ranks = []\n    intervals_per_ranks = []\n    for rank in range(distributed_env.world_size):\n        start = rank * num_items_per_rank\n        end = (rank + 1) * num_items_per_rank\n        if rank == distributed_env.world_size - 1 and not drop_last:\n            end = len(indexes)\n        chunks_per_ranks.append(indexes[start:end])\n        intervals_per_ranks.append(chunk_intervals[start:end])\n\n    return chunks_per_ranks, intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    chunks_per_ranks: List[List[int]] = [[] for _ in range(distributed_env.world_size)]\n    chunk_intervals_per_ranks: List[List[Any]] = [[] for _ in range(distributed_env.world_size)]\n\n    items_per_rank = len(indexes) // distributed_env.world_size\n    if drop_last:\n        indexes = indexes[: items_per_rank * distributed_env.world_size]\n        chunk_intervals = chunk_intervals[: items_per_rank * distributed_env.world_size]\n\n    for i in range(distributed_env.world_size):\n        chunks_per_ranks[i] = indexes[i * items_per_rank : (i + 1) * items_per_rank]\n        chunk_intervals_per_ranks[i] = chunk_intervals[i * items_per_rank : (i + 1) * items_per_rank]\n\n    return chunks_per_ranks, chunk_intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    total_items = len(indexes)\n    items_per_rank = total_items // distributed_env.world_size\n    if drop_last:\n        total_items = items_per_rank * distributed_env.world_size\n\n    # distribute the items across the ranks\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    for i, index in enumerate(indexes):\n        rank = i % distributed_env.world_size\n        chunks_per_ranks[rank].append(index)\n        intervals_per_ranks[rank].append(chunk_intervals[index])\n\n    return chunks_per_ranks, intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    if distributed_env.world_size == 1:\n        return [indexes], [chunk_intervals]\n\n    # calculate the number of items each rank should process\n    total_items = len(indexes)\n    items_per_rank = total_items // distributed_env.world_size\n    if drop_last:\n        items_per_rank = total_items // distributed_env.world_size\n    else:\n        items_per_rank = (total_items + distributed_env.world_size - 1) // distributed_env.world_size\n\n    # assign chunks and their intervals to each rank\n    chunks_per_ranks = []\n    intervals_per_ranks = []\n    for rank in range(distributed_env.world_size):\n        start = rank * items_per_rank\n        end = min(start + items_per_rank, total_items)\n        chunks_per_ranks.append(indexes[start:end])\n        intervals_per_ranks.append(chunk_intervals[start:end])\n\n    return chunks_per_ranks, intervals_per_ranks\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    items_per_rank = len(indexes) // distributed_env.world_size\n    if not drop_last:\n        items_per_rank = items_per_rank + len(indexes) % distributed_env.world_size\n\n    chunks_per_ranks: List[List[int]] = []\n    chunk_intervals_per_ranks: List[Any] = []\n    for rank in range(distributed_env.world_size):\n        start = rank * items_per_rank\n        end = (rank + 1) * items_per_rank\n        chunks_per_ranks.append(indexes[start:end])\n        chunk_intervals_per_ranks.append(chunk_intervals[start:end])\n\n    return chunks_per_ranks, chunk_intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    total_items = len(indexes)\n    items_per_rank = total_items // distributed_env.world_size\n    if drop_last:\n        total_items = items_per_rank * distributed_env.world_size\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    chunk_intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    for i, rank in enumerate(indexes):\n        chunks_per_ranks[i % distributed_env.world_size].append(rank)\n        chunk_intervals_per_ranks[i % distributed_env.world_size].append(chunk_intervals[rank])\n\n    return chunks_per_ranks, chunk_intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    items_per_rank = len(indexes) // distributed_env.world_size\n    if not drop_last:\n        items_per_rank = items_per_rank + len(indexes) % distributed_env.world_size\n    chunks_per_ranks: List[List[int]] = [[] for _ in range(distributed_env.world_size)]\n    intervals_per_ranks: List[List[Any]] = [[] for _ in range(distributed_env.world_size)]\n\n    for rank in range(distributed_env.world_size):\n        start = rank * items_per_rank\n        end = start + items_per_rank\n        if rank == distributed_env.world_size - 1 and not drop_last:\n            end = len(indexes)\n\n        chunks_per_ranks[rank] = indexes[start:end]\n        intervals_per_ranks[rank] = chunk_intervals[start:end]\n\n    return chunks_per_ranks, intervals_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    num_items = len(indexes)\n    num_items_per_rank = num_items // distributed_env.world_size\n    num_items_per_rank += int(distributed_env.world_size * num_items_per_rank < num_items)\n\n    if drop_last:\n        num_items = num_items_per_rank * distributed_env.world_size\n\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    internal_indexes_per_ranks = [[] for _ in range(distributed_env.world_size)]\n\n    for rank in range(distributed_env.world_size):\n        start_index = rank * num_items_per_rank\n        end_index = start_index + num_items_per_rank\n        if rank == distributed_env.world_size - 1:\n            end_index = num_items\n        chunks_per_ranks[rank] = indexes[start_index:end_index]\n        internal_indexes_per_ranks[rank] = chunk_intervals[start_index:end_index]\n\n    return chunks_per_ranks, internal_indexes_per_ranks"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # distribute the chunks across the ranks\n    num_chunks = len(indexes)\n    num_items_per_rank = num_chunks // distributed_env.world_size\n    num_items_last_rank = num_chunks % distributed_env.world_size\n\n    # calculate the number of items for the last rank\n    if drop_last:\n        num_items_per_rank = num_items_per_rank if distributed_env.world_size == 1 else max(1, num_items_per_rank)\n        num_items_last_rank = 0\n    else:\n        num_items_last_rank = num_items_last_rank if distributed_env.world_size == 1 else num_items_last_rank\n\n    # assign the chunks to each rank\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    for rank in range(distributed_env.world_size):\n        if rank == distributed_env.world_size - 1:\n            chunks_per_ranks[rank] = indexes[rank * num_items_per_rank : rank * num_items_per_rank + num_items_last_rank]\n            intervals_per_ranks[rank] = chunk_intervals[\n                rank * num_items_per_rank : rank * num_items_per_rank + num_items_last_rank\n            ]\n        else:\n            chunks_per_ranks[rank] = indexes[rank * num_items_per_rank : (rank + 1) * num_items_per_rank]\n            intervals_per_ranks[rank] = chunk_intervals[\n                rank * num_items_per_rank : (rank + 1) * num_items_per_rank\n            ]\n\n    return chunks_per_ranks, intervals_per_ranks"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            kwargs = {\"device\": self._device}\n        else:\n            kwargs = {}\n\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._device is None:\n            self._find_device()\n\n        kwargs = {}\n        if self._contains_device:\n            kwargs[\"device\"] = self._device\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n\n        if self._contains_device:\n            if self._device is None:\n                self._find_device()\n            kwargs[\"device\"] = self._device\n\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_device:\n            kwargs[\"device\"] = self._device\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)\n\n    "}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_device:\n            kwargs[\"device\"] = self._device\n\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_device and self._device is not None:\n            kwargs[\"device\"] = self._device\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_device:\n            if self._device is None:\n                self._find_device()\n            kwargs[\"device\"] = self._device\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_device:\n            if self._device is None:\n                self._find_device()\n\n            kwargs[\"device\"] = self._device\n\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_device:\n            if self._device is None:\n                self._find_device()\n\n            kwargs[\"device\"] = self._device\n\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_device:\n            if self._device is None:\n                self._find_device()\n\n            kwargs[\"device\"] = self._device\n\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n\n        if self._contains_device:\n            self._find_device()\n            kwargs[\"device\"] = self._device\n\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_device:\n            if self._device is None:\n                self._find_device()\n            kwargs[\"device\"] = self._device\n\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_device:\n            if self._device is None:\n                self._find_device()\n            kwargs[\"device\"] = self._device\n\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n\n        if self._contains_device:\n            self._find_device()\n            kwargs[\"device\"] = self._device\n\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)  # type: ignore\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_device:\n            kwargs[\"device\"] = self._device\n\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        if isinstance(self._fn, partial):\n            self._fn(item_metadata, output_dir, **kwargs)\n        elif isinstance(self._fn, FunctionType):\n            self._fn(item_metadata, output_dir, **kwargs)\n        elif callable(self._fn):\n            self._fn.__call__(item_metadata, output_dir, **kwargs)  # type: ignore\n        else:\n            raise ValueError(f\"The provided {self._fn} isn't supported.\")\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_device:\n            self._find_device()\n            kwargs[\"device\"] = self._device\n\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_device:\n            if self._device is None:\n                self._find_device()\n            kwargs[\"device\"] = self._device\n\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_device:\n            if self._device is None:\n                self._find_device()\n            kwargs[\"device\"] = self._device\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_device:\n            kwargs[\"device\"] = self._device\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)  # type: ignore\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_device and self._device is not None:\n            kwargs[\"device\"] = self._device\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            break\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e\n\n    return s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            break\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e\n    return"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return True\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return True\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if isinstance(inputs, StreamingDataLoader):\n            input_dir = Dir()\n        else:\n            input_dir ="}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if isinstance(inputs, StreamingDataLoader):\n            input_dir = Dir()\n        else:\n            input_dir ="}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default_num_workers(),\n            fast_dev_run=fast_dev_run,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_files=reorder_files,\n            weights=weights,\n            reader=reader,\n        )\n        with optimize_dns_"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default_num_workers(),\n            fast_dev_run=fast_dev_run,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_files=reorder_files,\n            weights=weights,\n            reader=reader,\n        )\n        with optimize_dns_context"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default_num_workers(),\n            fast_dev_run=fast_dev_run,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_files=reorder_files,\n            weights=weights,\n            reader=reader,\n        )\n        with optimize_dns_context"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default_num_workers(),\n            fast_dev_run=fast_dev_run,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_files=reorder_files,\n            weights=weights"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default_num_workers(),\n            fast_dev_run=fast_dev_run,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_files=reorder_files,\n            weights=weights"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default_num_workers(),\n            fast_dev_run=fast_dev_run,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_files=reorder_files,\n            weights=weights,\n            reader=reader,\n        )\n        with optimize_dns_context"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if isinstance(inputs, StreamingDataLoader):\n            input_dir = Dir()\n        else:\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default_num_workers(),\n            fast_dev_run=fast_dev_run,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_files=reorder_files,\n            weights=weights,"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if isinstance(inputs, StreamingDataLoader):\n            input_dir = Dir()\n        else:\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if isinstance(inputs, StreamingDataLoader):\n            input_dir = Dir()\n        else:\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default_num_workers(),\n            fast_dev_run=fast_dev_run,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_files=reorder_files,\n            weights=weights,"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default_num_workers(),\n            fast_dev_run=fast_dev_run,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_files=reorder_files,\n            weights=weights,\n            reader=reader,\n        )\n        with optimize_dns_context(True"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir)\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default_num_workers(),\n            fast_dev_run=fast_dev_run,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if isinstance(inputs, StreamingDataLoader):\n            input_dir = Dir()\n        else:\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default_num_workers(),\n            fast_dev_run=fast_dev_run,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir = _resolve_dir(output_dir)\n        _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default_num_workers(),\n            fast_dev_run=fast_dev_run,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_files=reorder_files,\n            weights=weights,\n            reader=reader,\n        )\n        with optimize_dns_context(True):\n            return data_processor.run(\n                LambdaDataChunkRecipe(\n                    fn,\n                    inputs,\n                    chunk_size=chunk_size,\n                    chunk_bytes=chunk_bytes,\n                    compression=compression,\n                )\n            )\n    return _execute(\n        f\"data-prep-optimize-{datetime.now().strft"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if error_when_not_empty:\n            _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if error_when_not_empty:\n            _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir, error_when_not_empty)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir, error_when_not_empty)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if error_when_not_empty:\n            _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if error_when_not_empty:\n            _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if error_when_not_empty:\n            _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if error_when_not_empty:\n            _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir, error_when_not_empty)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if error_when_not_empty:\n            _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if error_when_not_empty:\n            _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if error_when_not_empty:\n            _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir, error_when_not_empty)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir, error_when_not_empty)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir, error_when_not_empty)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if not error_when_not_empty:\n            _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir, error_when_not_empty=error_when_not_empty)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve"}
{"namespace": "litdata.processing.functions.map", "completion": "    if not _IS_IN_STUDIO:\n        print(\"This method is optimized to run on https://lightning.ai/. Don't use it otherwise.\")\n\n    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir, error"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:"}
{"namespace": "litdata.processing.functions.map", "completion": "    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to map your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir, error_when_not_empty)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    obj = parse.urlparse(input_dir.url if input_dir.url else input_dir.path)\n\n    if obj.scheme == \"s3\":\n        s3 = S3Client()\n\n    while True:\n        # 1. Collect paths\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if index is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(cache_dir) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if not os.path.exists(path):\n                    if obj.scheme == \"s3\":\n                        obj = parse.urlparse(path)\n                        _wait_for_file_to_exist(s3, obj)\n                        with open(path, \"wb\") as f:\n                            s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n                    else:\n                        shutil.copyfile(path, path.replace(input_dir.path, cache_dir))\n\n            elif os.path.exists(path) and \"s3_connections\" not in path:\n                shutil.copyfile(path, os.path.join(cache_dir, os.path.basename(path)))\n\n        # 4. Signal that the download is finished\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    obj = parse.urlparse(input_dir.url if input_dir.url else input_dir.path)\n\n    if obj.scheme == \"s3\":\n        s3 = S3Client()\n\n    while True:\n        data: Optional[Tuple[int, List[str]]] = queue_in.get()\n\n        # Terminate the process if we received a termination signal\n        if data is None:\n            return\n\n        index, paths = data\n\n        for path in paths:\n            if input_dir:\n                if not path.startswith(cache_dir) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if not os.path.exists(path):\n                    if obj.scheme == \"s3\":\n                        obj = parse.urlparse(path)\n                        _wait_for_file_to_exist(s3, obj)\n                        with open(path, \"wb\") as f:\n                            s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n                    else:\n                        raise ValueError(f\"The provided {input_dir.path} isn't supported.\")\n\n            elif not os.path.exists(path):\n                raise ValueError(f\"The provided {path} doesn't exist.\")\n\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Collect paths\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(cache_dir) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if not os.path.exists(path):\n                    obj = parse.urlparse(path)\n                    if obj.scheme == \"s3\":\n                        _wait_for_file_to_exist(s3, obj)\n                        s3.client.download_file(obj.netloc, obj.path.lstrip(\"/\"), path)\n                    else:\n                        os.makedirs(os.path.dirname(path), exist_ok=True)\n                        shutil.copyfile(path, path)\n            elif os.path.exists(path):\n                shutil.copyfile(path, path)\n\n        # 4. Signal that the download is done\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    obj = parse.urlparse(input_dir.url if input_dir.url else input_dir.path)\n    if obj.scheme == \"s3\":\n        s3 = S3Client()\n\n    while True:\n        # 1. Collect paths\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if index is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(cache_dir) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if not os.path.exists(path):\n                    if obj.scheme == \"s3\":\n                        obj = parse.urlparse(path)\n                        _wait_for_file_to_exist(s3, obj)\n                        with open(path, \"wb\") as f:\n                            s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n                    else:\n                        raise ValueError(f\"The provided {input_dir.path} isn't supported.\")\n\n        # 4. Signal the completion of the download\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    obj = parse.urlparse(input_dir.url if input_dir.url else input_dir.path)\n\n    if obj.scheme == \"s3\":\n        s3 = S3Client()\n\n    while True:\n        # 1. Collect paths\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(cache_dir) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if not os.path.exists(path):\n                    if obj.scheme == \"s3\":\n                        obj = parse.urlparse(path)\n                        _wait_for_file_to_exist(s3, obj)\n                        s3.client.download_file(obj.netloc, obj.path.lstrip(\"/\"), path)\n                    else:\n                        raise ValueError(f\"The provided {input_dir.path} isn't supported.\")\n\n            elif obj.scheme == \"s3\":\n                obj = parse.urlparse(path)\n                _wait_for_file_to_exist(s3, obj)\n                s3.client.download_file(obj.netloc, obj.path.lstrip(\"/\"), path)\n            else:\n                raise ValueError(f\"The provided {input_dir.path} isn't supported.\")\n\n        # 4. Signal that the files are downloaded\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Collect paths\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if index is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(cache_dir) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if not os.path.exists(path):\n                    obj = parse.urlparse(path)\n                    if obj.scheme == \"s3\":\n                        _wait_for_file_to_exist(s3, obj)\n                        s3.client.download_file(obj.netloc, obj.path.lstrip(\"/\"), path)\n                    else:\n                        raise ValueError(f\"The provided {path} isn't supported.\")\n\n            elif os.path.exists(path):\n                pass\n            else:\n                raise ValueError(f\"The provided {path} isn't supported.\")\n\n        # 4. Signal the completion\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    obj = parse.urlparse(input_dir.url if input_dir.url else input_dir.path)\n\n    if obj.scheme == \"s3\":\n        s3 = S3Client()\n\n    while True:\n        index, paths = queue_in.get()\n\n        # Terminate the process if we received a termination signal\n        if index is None:\n            return\n\n        # Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(cache_dir) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if not os.path.exists(path):\n                    if obj.scheme == \"s3\":\n                        obj = parse.urlparse(path)\n                        _wait_for_file_to_exist(s3, obj)\n                        with open(path, \"wb\") as f:\n                            s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n                    else:\n                        os.makedirs(os.path.dirname(path), exist_ok=True)\n                        shutil.copyfile(path, path)\n\n            elif os.path.exists(path) and \"s3_connections\" not in path:\n                os.makedirs(os.path.dirname(path), exist_ok=True)\n                shutil.copyfile(path, path)\n\n        # Signal that the download is finished\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Collect paths\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if index is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(cache_dir) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if not os.path.exists(path):\n                    obj = parse.urlparse(path)\n\n                    if obj.scheme == \"s3\":\n                        _wait_for_file_to_exist(s3, obj)\n                        with open(path, \"wb\") as f:\n                            s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n                    else:\n                        shutil.copyfile(path, path)\n\n            elif os.path.exists(path) and \"s3_connections\" not in path:\n                shutil.copyfile(path, path)\n\n    # 4. Signal that the download is complete\n    queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    obj = parse.urlparse(input_dir.url if input_dir.url else input_dir.path)\n\n    if obj.scheme == \"s3\":\n        s3 = S3Client()\n\n    while True:\n        data: Optional[Tuple[int, List[str]]] = queue_in.get()\n\n        # 1. Terminate the process if we received a termination signal\n        if data is None:\n            return\n\n        # 2. Unpack the data\n        index, paths = data\n\n        # 3. Download the files\n        for path in paths:\n            if not path.startswith(cache_dir) and input_dir.path is not None:\n                path = path.replace(input_dir.path, cache_dir)\n\n            if not os.path.exists(path):\n                if obj.scheme == \"s3\":\n                    obj = parse.urlparse(path)\n                    _wait_for_file_to_exist(s3, obj)\n                    with open(path, \"wb\") as f:\n                        s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n                else:\n                    raise ValueError(f\"The provided {input_dir.path} isn't supported.\")\n\n        # 4. Signal that the files are downloaded\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Collect paths\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(cache_dir) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if not os.path.exists(path):\n                    obj = parse.urlparse(path)\n                    _wait_for_file_to_exist(s3, obj)\n                    if obj.scheme == \"s3\":\n                        with open(path, \"wb\") as f:\n                            s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n                    else:\n                        shutil.copyfile(path, path)\n\n            elif os.path.exists(path) and \"s3_connections\" not in path:\n                shutil.copyfile(path, path)\n\n        # 4. Signal completion by putting the index into the output queue\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Collect paths\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if index is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(cache_dir) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if not os.path.exists(path):\n                    obj = parse.urlparse(path)\n\n                    if obj.scheme == \"s3\":\n                        _wait_for_file_to_exist(s3, obj)\n                        with open(path, \"wb\") as f:\n                            s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n                    else:\n                        os.makedirs(os.path.dirname(path), exist_ok=True)\n                        shutil.copyfile(path, path)\n            else:\n                os.makedirs(os.path.dirname(path), exist_ok=True)\n                shutil.copyfile(path, path)\n\n        # 4. Signal that we are done\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Collect paths\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(cache_dir) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if not os.path.exists(path):\n                    obj = parse.urlparse(path)\n                    if obj.scheme == \"s3\":\n                        _wait_for_file_to_exist(s3, obj)\n                        s3.client.download_file(obj.netloc, obj.path.lstrip(\"/\"), path)\n                    else:\n                        raise ValueError(f\"The provided {path} isn't supported.\")\n\n            elif os.path.exists(path):\n                pass\n\n            else:\n                raise ValueError(f\"The provided {path} isn't supported.\")\n\n        # 4. Inform the main thread that the files are ready to be processed\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    obj = parse.urlparse(input_dir.url if input_dir.url else input_dir.path)\n\n    if obj.scheme == \"s3\":\n        s3 = S3Client()\n\n    while True:\n        # 1. Collect paths\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if index is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if not path.startswith(cache_dir) and input_dir.path is not None:\n                path = path.replace(input_dir.path, cache_dir)\n\n            if not os.path.exists(path):\n                if obj.scheme == \"s3\":\n                    obj = parse.urlparse(path)\n                    _wait_for_file_to_exist(s3, obj)\n                    with open(path, \"wb\") as f:\n                        s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n                elif input_dir.path and os.path.isdir(input_dir.path):\n                    shutil.copyfile(path, path.replace(cache_dir, input_dir.path))\n                else:\n                    raise ValueError(f\"The provided {input_dir.path} isn't supported.\")\n\n        # 4. Signal the worker that the download is completed\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client()\n    while True:\n        # 1. Collect paths\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if not os.path.exists(path):\n                if input_dir:\n                    if not path.startswith(cache_dir) and input_dir.path is not None:\n                        path = path.replace(input_dir.path, cache_dir)\n\n                    obj = parse.urlparse(path)\n                    if obj.scheme == \"s3\":\n                        _wait_for_file_to_exist(s3, obj)\n                        with open(path, \"wb\") as f:\n                            s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n                    elif input_dir.path:\n                        shutil.copyfile(path, path)\n\n        # 4. Signal that the files are available\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    obj = parse.urlparse(input_dir.url if input_dir.url else input_dir.path)\n\n    if obj.scheme == \"s3\":\n        s3 = S3Client()\n\n    while True:\n        data: Optional[Tuple[int, List[str]]] = queue_in.get()\n\n        # 1. Terminate the process if we received a termination signal\n        if data is None:\n            return\n\n        # 2. Iterate through the paths and download them sequentially.\n        index, paths = data\n\n        for path in paths:\n            if input_dir:\n                if not path.startswith(cache_dir) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if not os.path.exists(path):\n                    if obj.scheme == \"s3\":\n                        obj = parse.urlparse(path)\n                        _wait_for_file_to_exist(s3, obj)\n                        with open(path, \"wb\") as f:\n                            s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n                    else:\n                        raise ValueError(f\"The provided {input_dir.path} isn't supported.\")\n\n            elif os.path.exists(path) and \"s3_connections\" not in path:\n                shutil.copyfile(path, path.replace(input_dir.path, cache_dir))\n\n        # 3. Put the index into the output queue to signal that the files for that index are available.\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    if input_dir.path is None:\n        raise ValueError(\"The provided input directory is invalid.\")\n\n    obj = parse.urlparse(input_dir.path)\n\n    if obj.scheme == \"s3\":\n        s3 = S3Client()\n\n    while True:\n        # 1. Collect paths\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(cache_dir) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if not os.path.exists(path):\n                    if obj.scheme == \"s3\":\n                        obj = parse.urlparse(path)\n                        _wait_for_file_to_exist(s3, obj)\n                        with open(path, \"wb\") as f:\n                            s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n                    else:\n                        raise ValueError(f\"The provided {input_dir.path} isn't supported.\")\n            elif os.path.exists(path) and \"s3_connections\" not in path:\n                pass\n\n        # 4. Signal that the files are downloaded\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    # 1. Collect the paths\n    index, paths = queue_in.get()\n\n    # 2. Terminate the process if we received a termination signal\n    if index is None:\n        return\n\n    # 3. Download the files sequentially.\n    for path in paths:\n        if input_dir:\n            if not path.startswith(cache_dir) and input_dir.path is not None:\n                path = path.replace(input_dir.path, cache_dir)\n\n            if not os.path.exists(path):\n                obj = parse.urlparse(path)\n                s3 = S3Client()\n                _wait_for_file_to_exist(s3, obj)\n                with open(path, \"wb\") as f:\n                    s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n\n        elif os.path.exists(path) and \"s3_connections\" not in path:\n            obj = parse.urlparse(path)\n            s3 = S3Client()\n            _wait_for_file_to_exist(s3, obj)\n            with open(path, \"wb\") as f:\n                s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n\n        else:\n            raise ValueError(f\"The provided {path} isn't supported.\")\n\n    # 4. Signal that we are done\n    queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    if input_dir.url:\n        obj = parse.urlparse(input_dir.url)\n        s3 = S3Client()\n    else:\n        obj = None\n\n    while True:\n        # 1. Collect paths\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if index is None:\n            return\n\n        # 3. Download the files\n        for path in paths:\n            if input_dir:\n                if not path.startswith(cache_dir) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if not os.path.exists(path):\n                    if obj:\n                        if obj.scheme == \"s3\":\n                            obj = parse.urlparse(path)\n                            _wait_for_file_to_exist(s3, obj)\n                            with open(path, \"wb\") as f:\n                                s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n                        else:\n                            raise ValueError(f\"The provided {input_dir.url} isn't supported.\")\n                    else:\n                        raise ValueError(f\"The provided {input_dir.path} isn't supported.\")\n            else:\n                if not os.path.exists(path):\n                    raise ValueError(f\"The provided {path} doesn't exist.\")\n\n        # 4. Signal the download is done\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    if input_dir.path is None:\n        raise ValueError(\"The provided input_dir.path is None.\")\n\n    obj = parse.urlparse(input_dir.path)\n    if obj.scheme == \"s3\":\n        s3 = S3Client()\n\n    while True:\n        # 1. Collect paths\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if not path.startswith(cache_dir) and input_dir.path is not None:\n                path = path.replace(input_dir.path, cache_dir)\n\n            if os.path.exists(path):\n                continue\n\n            if obj.scheme == \"s3\":\n                try:\n                    output_filepath = os.path.join(str(obj.path).lstrip(\"/\"), os.path.basename(path))\n                    s3.client.download_file(obj.netloc, output_filepath, path)\n                except botocore.exceptions.ClientError as e:\n                    if \"the DownloadFile operation: Not Found\" in str(e):\n                        _wait_for_file_to_exist(s3, obj)\n                        s3.client.download_file(obj.netloc, output_filepath, path)\n                    else:\n                        raise e\n            elif input_dir.path:\n                os.makedirs(os.path.dirname(path), exist_ok=True)\n                shutil.copyfile(os.path.join(input_dir.path, os.path.basename(path)), path)\n            else:\n                raise ValueError(f\"The provided {input_dir.path} isn't supported.\")\n\n        # 4. Signal that the download is done\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    if input_dir.path is None:\n        while True:\n            data = queue_in.get()\n            if data is None:\n                return\n\n            queue_out.put(data[0])\n            continue\n\n    s3 = S3Client()\n    obj = parse.urlparse(input_dir.url if input_dir.url else input_dir.path)\n\n    while True:\n        data = queue_in.get()\n\n        if data is None:\n            return\n\n        index, paths = data\n\n        for path in paths:\n            if not path.startswith(cache_dir):\n                path = os.path.join(cache_dir, path)\n\n            if os.path.exists(path):\n                continue\n\n            if obj.scheme == \"s3\":\n                if not path.startswith(input_dir.path):\n                    path = path.replace(input_dir.path, \"\")\n\n                output_filepath = os.path.join(cache_dir, path)\n\n                # Make sure the directory exists\n                os.makedirs(os.path.dirname(output_filepath), exist_ok=True)\n\n                # Download the file from the cloud storage\n                _wait_for_disk_usage_higher_than_threshold(cache_dir)\n                s3.client.download_file(obj.netloc, path.lstrip(\"/\"), output_filepath)\n            else:\n                shutil.copyfile(path, os.path.join(cache_dir, path))\n\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 2. Fetch from the queue\n        data = upload_queue.get()\n\n        # 3. Terminate the process if we received a termination signal\n        if data is None:\n            return\n\n        # 4. Unpack\n        if isinstance(data, str):\n            local_filepath = data\n            remote_filepath = data.replace(cache_dir, \"\")\n        else:\n            local_filepath = data[-1]\n            remote_filepath = data[-1].replace(cache_dir, \"\")\n\n        # 5. Upload the file to the remote directory\n        obj = parse.urlparse(output_dir.url if output_dir.url else output_dir.path)\n\n        if obj.scheme == \"s3\":\n            s3.client.upload_file(\n                local_filepath, obj.netloc, os.path.join(str(obj.path).lstrip(\"/\"), os.path.basename(remote_filepath))\n            )\n        elif output_dir.path and os.path.isdir(output_dir.path):\n            shutil.copyfile(local_filepath, os.path.join(output_dir.path, os.path.basename(remote_filepath)))\n\n        # 6. Inform the worker the current file is uploaded\n        remove_queue.put(data)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 2. Fetch from the queue\n        data: Optional[Union[str, Tuple[str, str]]] = upload_queue.get()\n\n        # 3. Terminate the process if we received a termination signal\n        if data is None:\n            return\n\n        # 4. Upload the file\n        if isinstance(data, str):\n            local_filepath = data\n        else:\n            local_filepath = os.path.join(data[0], data[1])\n\n        if output_dir.url and output_dir.url.startswith(\"s3://\"):\n            obj = parse.urlparse(output_dir.url)\n            s3.client.upload_file(local_filepath, obj.netloc, os.path.join(obj.path.lstrip(\"/\"), os.path.basename(local_filepath)))\n        elif output_dir.path and os.path.isdir(output_dir.path):\n            shutil.copyfile(local_filepath, os.path.join(output_dir.path, os.path.basename(local_filepath)))\n\n        # 5. Remove the file\n        remove_queue.put(local_filepath)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 2. Fetch from the queue\n        data = upload_queue.get()\n\n        # 3. Terminate the process if we received a termination signal\n        if data is None:\n            return\n\n        # 4. Unpack\n        if isinstance(data, str):\n            local_path = data\n        else:\n            local_path = data[-1]\n\n        # 5. Upload the file\n        if output_dir.url:\n            obj = parse.urlparse(local_path.replace(cache_dir, output_dir.url))\n\n            if obj.scheme == \"s3\":\n                with open(local_path, \"rb\") as f:\n                    s3.client.upload_fileobj(f, obj.netloc, obj.path.lstrip(\"/\"))\n            else:\n                raise ValueError(f\"The provided {output_dir.url} isn't supported.\")\n\n        elif output_dir.path:\n            shutil.copyfile(local_path, local_path.replace(cache_dir, output_dir.path))\n\n        # 6. Inform the worker the current files are uploaded\n        remove_queue.put(data)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Fetch from the queue\n        data = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if data is None:\n            return\n\n        # 3. Unpack\n        if isinstance(data, str):\n            local_filepath = data\n        else:\n            local_filepath = data[-1]\n\n        # 4. Check whether the file exists\n        if not os.path.exists(local_filepath):\n            raise ValueError(f\"The file {local_filepath} doesn't exist.\")\n\n        # 5. Upload the file to the remote directory\n        obj = parse.urlparse(output_dir.url if output_dir.url else output_dir.path)\n\n        if obj.scheme == \"s3\":\n            s3.client.upload_file(\n                local_filepath, obj.netloc, os.path.join(str(obj.path).lstrip(\"/\"), os.path.basename(local_filepath))\n            )\n        elif output_dir.path and os.path.isdir(output_dir.path):\n            shutil.copyfile(local_filepath, os.path.join(output_dir.path, os.path.basename(local_filepath)))\n\n        # 6. Remove the file from the cache directory\n        remove_queue.put(local_filepath)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Collect paths\n        data = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if data is None:\n            return\n\n        # 3. Upload the file to the target directory\n        if isinstance(data, str):\n            file_path = data\n        else:\n            temp_dir, file_path = data\n            file_path = os.path.join(temp_dir, file_path)\n\n        if os.path.exists(file_path):\n            obj = parse.urlparse(output_dir.url if output_dir.url else output_dir.path)\n            if obj.scheme == \"s3\":\n                with open(file_path, \"rb\") as f:\n                    s3.client.upload_fileobj(f, obj.netloc, os.path.join(obj.path.lstrip(\"/\"), os.path.basename(file_path)))\n            elif output_dir.path and os.path.isdir(output_dir.path):\n                shutil.copyfile(file_path, os.path.join(output_dir.path, os.path.basename(file_path)))\n\n        # 4. Inform the remover the file is uploaded\n        remove_queue.put(file_path)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Collect paths\n        data = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if data is None:\n            return\n\n        if isinstance(data, str):\n            filepath = data\n        else:\n            temp_dir, filepath = data\n\n        # 3. Upload the file\n        if output_dir.url:\n            obj = parse.urlparse(output_dir.url)\n            if obj.scheme == \"s3\":\n                with open(filepath, \"rb\") as f:\n                    s3.client.upload_fileobj(f, obj.netloc, os.path.join(obj.path.lstrip(\"/\"), os.path.basename(filepath)))\n            else:\n                raise ValueError(f\"The provided {output_dir.url} isn't supported.\")\n        else:\n            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n            shutil.move(filepath, output_dir.path)\n\n        # 4. Remove the file\n        remove_queue.put(filepath)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 2. Fetch from the queue\n        data = upload_queue.get()\n\n        # 3. Terminate the process if we received a termination signal\n        if data is None:\n            return\n\n        # 4. Unpack\n        if isinstance(data, str):\n            filepath = data\n            tmp_dir = None\n        else:\n            tmp_dir, filepath = data\n\n        # 5. Upload the file to the remote directory\n        if output_dir.url:\n            obj = parse.urlparse(output_dir.url)\n\n            if obj.scheme == \"s3\":\n                if tmp_dir:\n                    tmp_filepath = os.path.join(tmp_dir, os.path.basename(filepath))\n                    with open(tmp_filepath, \"rb\") as f:\n                        s3.client.upload_fileobj(f, obj.netloc, os.path.join(obj.path.lstrip(\"/\"), os.path.basename(filepath)))\n                else:\n                    with open(filepath, \"rb\") as f:\n                        s3.client.upload_fileobj(f, obj.netloc, os.path.join(obj.path.lstrip(\"/\"), os.path.basename(filepath)))\n\n        elif output_dir.path:\n            if tmp_dir:\n                tmp_filepath = os.path.join(tmp_dir, os.path.basename(filepath))\n                shutil.copyfile(tmp_filepath, filepath)\n\n        # 6. Inform the worker the current file is uploaded\n        remove_queue.put(filepath)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Collect paths\n        data = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if data is None:\n            return\n\n        # 3. Upload the file to the target directory\n        if isinstance(data, str):\n            local_path = data\n        else:\n            temp_dir, local_path = data\n            local_path = os.path.join(temp_dir, local_path)\n\n        if output_dir.url:\n            obj = parse.urlparse(output_dir.url)\n            if obj.scheme == \"s3\":\n                # 4. Upload the file to the target directory\n                with open(local_path, \"rb\") as f:\n                    s3.client.upload_fileobj(f, obj.netloc, os.path.join(obj.path.lstrip(\"/\"), os.path.basename(local_path)))\n            elif output_dir.path:\n                os.makedirs(os.path.dirname(local_path), exist_ok=True)\n                shutil.copyfile(local_path, os.path.join(output_dir.path, os.path.basename(local_path)))\n            else:\n                raise ValueError(f\"The provided {output_dir.url} isn't supported.\")\n\n        # 5. Inform the worker the current files are available\n        remove_queue.put(local_path)\n\n        # 6. Remove the file from the cache directory\n        if cache_dir:\n            os.remove(local_path)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 2. Fetch from the queue\n        r: Optional[Union[str, Tuple[str, str]]] = upload_queue.get()\n\n        # 3. Terminate the process if we received a termination signal\n        if r is None:\n            return\n\n        # 4. Unpack\n        if isinstance(r, str):\n            filepath = r\n        else:\n            temp_dir, filepath = r\n\n        if not os.path.exists(filepath):\n            raise ValueError(f\"The provided file {filepath} doesn't exist.\")\n\n        # 5. Check whether all the files are already downloaded\n        if output_dir.path and all(\n            os.path.exists(p.replace(cache_dir, output_dir.path) if output_dir else p) for p in filepath\n        ):\n            continue\n\n        if output_dir.url is not None or output_dir.path is not None:\n            if output_dir.url:\n                # 6. Wait for the removers to catch up when we are downloading data.\n                _wait_for_disk_usage_higher_than_threshold(\"/\", 25)\n\n            # 7. Download all the required paths to unblock the current index\n            if output_dir.url and output_dir.path:\n                filepath = filepath.replace(cache_dir, output_dir.path)\n\n            obj = parse.urlparse(filepath)\n\n            if obj.scheme == \"s3\":\n                dirpath = os.path.dirname(filepath)\n\n                os.makedirs(dirpath, exist_ok=True)\n\n                with open(filepath, \"wb\") as f:\n                    s3.client.upload_fileobj(f, obj.netloc, obj.path.lstrip(\"/\"))\n\n            elif os.path.isfile(filepath):\n                if not filepath.startswith(\"/teamspace/studios/this_studio\"):\n                    os.makedirs(os.path.dirname"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 2. Fetch from the queue\n        data = upload_queue.get()\n\n        # 3. Terminate the process if we received a termination signal\n        if data is None:\n            return\n\n        if isinstance(data, str):\n            filepath = data\n            if not filepath.startswith(cache_dir):\n                filepath = os.path.join(cache_dir, filepath)\n            if not os.path.exists(filepath):\n                continue\n        else:\n            tmp_dir, filepath = data\n            if not filepath.startswith(cache_dir):\n                filepath = os.path.join(cache_dir, filepath)\n            if not os.path.exists(filepath):\n                continue\n\n        # 4. Upload the file\n        obj = parse.urlparse(filepath)\n        if output_dir.url:\n            if not obj.path.startswith(output_dir.url):\n                obj = obj._replace(path=os.path.join(output_dir.url, obj.path.lstrip(\"/\")))\n\n        if obj.scheme == \"s3\":\n            s3.client.upload_file(filepath, obj.netloc, obj.path.lstrip(\"/\"))\n        elif output_dir.path and os.path.isdir(output_dir.path):\n            shutil.copyfile(filepath, os.path.join(output_dir.path, os.path.basename(filepath)))\n\n        # 5. Inform the worker the current files are available\n        remove_queue.put(filepath)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n    while True:\n        # 2. Fetch from the queue\n        data = upload_queue.get()\n\n        # 3. Terminate the process if we received a termination signal\n        if data is None:\n            return\n\n        # 4. Unpack\n        if isinstance(data, str):\n            filepath = data\n        else:\n            tmp_dir, filepath = data\n\n        if output_dir.url and output_dir.url.startswith(\"s3://\"):\n            obj = parse.urlparse(output_dir.url)\n\n            with open(filepath, \"rb\") as f:\n                s3.client.upload_fileobj(f, obj.netloc, os.path.join(obj.path.lstrip(\"/\"), os.path.basename(filepath)))\n\n        elif output_dir.path:\n            dirpath = os.path.dirname(filepath)\n            if not dirpath.startswith(output_dir.path):\n                dirpath = dirpath.replace(cache_dir, output_dir.path)\n\n            os.makedirs(dirpath, exist_ok=True)\n            shutil.move(filepath, dirpath)\n\n        # 5. Inform the worker the current files are uploaded\n        remove_queue.put(filepath)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 2. Fetch from the queue\n        data = upload_queue.get()\n\n        # 3. Terminate the process if we received a termination signal\n        if data is None:\n            return\n\n        # 4. Unpack\n        if isinstance(data, str):\n            local_path = data\n        else:\n            local_path = data[-1]\n\n        if output_dir.url is not None or output_dir.path is not None:\n            if output_dir.url:\n                obj = parse.urlparse(local_path.replace(cache_dir, output_dir.url))\n\n                if obj.scheme == \"s3\":\n                    with open(local_path, \"rb\") as f:\n                        s3.client.upload_fileobj(f, obj.netloc, obj.path.lstrip(\"/\"))\n\n                elif os.path.isfile(local_path):\n                    # 5. Wait for the removers to catch up when we are uploading data.\n                    _wait_for_disk_usage_higher_than_threshold(\"/\", 25)\n\n                    with open(local_path, \"rb\") as f:\n                        s3.client.upload_fileobj(f, obj.netloc, obj.path.lstrip(\"/\"))\n\n                else:\n                    raise ValueError(f\"The provided {output_dir.url} isn't supported.\")\n\n            elif output_dir.path and os.path.isfile(local_path):\n                # 5. Wait for the removers to catch up when we are uploading data.\n                _wait_for_disk_usage_higher_than_threshold(\"/\", 25)\n\n                shutil.copyfile(local_path, local_path.replace(cache_dir, output_dir.path))\n\n        # 6. Inform the worker the current files are available\n        remove_queue.put(data)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Collect paths\n        data = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if data is None:\n            return\n\n        # 3. Upload the data to the output directory\n        if isinstance(data, str):\n            if not data.startswith(cache_dir):\n                data = data.replace(cache_dir, output_dir.path)\n\n            if os.path.exists(data):\n                obj = parse.urlparse(data)\n\n                if obj.scheme == \"s3\":\n                    with open(data, \"rb\") as f:\n                        s3.client.upload_fileobj(f, obj.netloc, obj.path.lstrip(\"/\"))\n                else:\n                    shutil.copyfile(data, data)\n\n                # 4. Inform the remover the files are available\n                remove_queue.put(data)\n\n        elif isinstance(data, tuple):\n            temp_dir, data = data\n            if not data.startswith(cache_dir):\n                data = data.replace(cache_dir, output_dir.path)\n\n            if os.path.exists(data):\n                obj = parse.urlparse(data)\n\n                if obj.scheme == \"s3\":\n                    with open(data, \"rb\") as f:\n                        s3.client.upload_fileobj(f, obj.netloc, obj.path.lstrip(\"/\"))\n                else:\n                    shutil.copyfile(data, data)\n\n                # 4. Inform the remover the files are available\n                remove_queue.put(data)\n\n            # 5. Delete the temporary directory\n            shutil.rmtree(temp_dir)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Fetch from the queue\n        data = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if data is None:\n            return\n\n        # 3. Upload the file\n        if isinstance(data, str):\n            local_path = data\n            if local_path.startswith(cache_dir):\n                local_path = local_path.replace(cache_dir, \"\")\n\n            obj = parse.urlparse(output_dir.url)\n            s3.client.upload_file(local_path, obj.netloc, os.path.join(obj.path.lstrip(\"/\"), os.path.basename(local_path)))\n\n            # 4. Inform the worker the file was uploaded\n            remove_queue.put(local_path)\n        else:\n            temp_dir, local_path = data\n            if local_path.startswith(cache_dir):\n                local_path = local_path.replace(cache_dir, \"\")\n\n            obj = parse.urlparse(output_dir.url)\n            s3.client.upload_file(local_path, obj.netloc, os.path.join(obj.path.lstrip(\"/\"), os.path.basename(local_path)))\n\n            # 4. Inform the worker the file was uploaded\n            remove_queue.put(local_path)\n\n            # 5. Remove the temporary directory\n            shutil.rmtree(temp_dir)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    # Setup the S3 client if the output directory is an S3 bucket\n    s3 = S3Client() if output_dir.url and output_dir.url.startswith(\"s3://\") else None\n\n    while True:\n        # 1. Fetch from the queue\n        data = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if data is None:\n            return\n\n        # 3. Unpack\n        if isinstance(data, str):\n            local_filepath = data\n            remote_filepath = data.replace(cache_dir, \"\")\n        else:\n            local_filepath, remote_filepath = data\n\n        # 4. Upload the file to the remote directory\n        if output_dir.url and output_dir.url.startswith(\"s3://\"):\n            assert s3\n            obj = parse.urlparse(output_dir.url)\n            s3.client.upload_file(local_filepath, obj.netloc, os.path.join(str(obj.path).lstrip(\"/\"), remote_filepath))\n        elif output_dir.path:\n            os.makedirs(os.path.dirname(os.path.join(output_dir.path, remote_filepath)), exist_ok=True)\n            shutil.copyfile(local_filepath, os.path.join(output_dir.path, remote_filepath))\n        else:\n            raise ValueError(\"The provided output directory is not supported.\")\n\n        # 5. Inform the worker the current file is uploaded\n        remove_queue.put(local_filepath)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Collect data\n        data = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if data is None:\n            return\n\n        # 3. Upload the data\n        if isinstance(data, str):\n            if not data.startswith(cache_dir):\n                data = os.path.join(cache_dir, data)\n\n            if not os.path.exists(data):\n                raise FileNotFoundError(f\"The file {data} doesn't exist.\")\n\n            with open(data, \"rb\") as f:\n                file_bytes = f.read()\n\n            file_name = os.path.basename(data)\n            obj = parse.urlparse(output_dir.url if output_dir.url else output_dir.path)\n\n            if obj.scheme == \"s3\":\n                s3.client.upload_fileobj(f, obj.netloc, os.path.join(obj.path.lstrip(\"/\"), file_name))\n            elif output_dir.path and os.path.isdir(output_dir.path):\n                shutil.copyfile(data, os.path.join(output_dir.path, file_name))\n            else:\n                raise ValueError(f\"The provided {output_dir.url} isn't supported.\")\n        else:\n            temp_dir, file_path = data\n            if not file_path.startswith(temp_dir):\n                file_path = os.path.join(temp_dir, file_path)\n\n            if not os.path.exists(file_path):\n                raise FileNotFoundError(f\"The file {file_path} doesn't exist.\")\n\n            with open(file_path, \"rb\") as f:\n                file_bytes = f.read()\n\n            file_name = os.path.basename(file_path)\n            obj = parse.urlparse(output_dir.url if output_dir.url else output_dir.path)\n\n            if obj.scheme =="}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Collect paths\n        data = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if data is None:\n            return\n\n        # 3. Upload the files\n        if isinstance(data, str):\n            filepath = data\n            if output_dir.path is not None:\n                filepath = filepath.replace(cache_dir, output_dir.path)\n            elif output_dir.url is not None:\n                obj = parse.urlparse(output_dir.url)\n                s3.client.upload_file(filepath, obj.netloc, os.path.join(str(obj.path).lstrip(\"/\"), os.path.basename(filepath)))\n        elif isinstance(data, tuple):\n            temp_dir, filepath = data\n            if output_dir.path is not None:\n                filepath = filepath.replace(cache_dir, output_dir.path)\n                shutil.move(filepath, filepath.replace(temp_dir, output_dir.path))\n            elif output_dir.url is not None:\n                obj = parse.urlparse(output_dir.url)\n                s3.client.upload_file(\n                    filepath, obj.netloc, os.path.join(str(obj.path).lstrip(\"/\"), os.path.basename(filepath))\n                )\n                shutil.rmtree(temp_dir)\n\n        # 4. Inform the remover that the file has been uploaded.\n        remove_queue.put(data)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n    while True:\n        # 1. Fetch from the queue\n        r: Optional[str] = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if r is None:\n            return\n\n        # 3. Unpack\n        if isinstance(r, str):\n            filepath = r\n        else:\n            temp_dir, filepath = r\n\n        # 4. Check whether the file exists\n        if not os.path.exists(filepath):\n            raise ValueError(f\"The file {filepath} doesn't exist.\")\n\n        # 5. Upload the file\n        obj = parse.urlparse(filepath)\n\n        if obj.scheme == \"s3\":\n            if not obj.netloc:\n                raise ValueError(f\"The provided {output_dir.url} is not valid.\")\n\n            s3.client.upload_file(filepath, obj.netloc, obj.path.lstrip(\"/\"))\n\n        elif output_dir.path and os.path.isdir(output_dir.path):\n            shutil.copyfile(filepath, os.path.join(output_dir.path, os.path.basename(filepath)))\n\n        elif output_dir.url and obj.scheme == \"\":\n            shutil.copyfile(filepath, os.path.join(output_dir.url, os.path.basename(filepath)))\n\n        else:\n            raise ValueError(f\"The provided {output_dir.url} isn't supported.\")\n\n        # 6. Remove the file\n        remove_queue.put(filepath)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n    while True:\n        # 1. Collect paths\n        data = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if data is None:\n            return\n\n        # 3. Upload the data\n        if isinstance(data, str):\n            src_filepath = data\n            if src_filepath.startswith(cache_dir):\n                src_filepath = src_filepath.replace(cache_dir, \"\")\n\n            if output_dir.url:\n                obj = parse.urlparse(output_dir.url)\n                s3.client.upload_file(\n                    src_filepath, obj.netloc, os.path.join(str(obj.path).lstrip(\"/\"), os.path.basename(src_filepath))\n                )\n            else:\n                assert output_dir.path\n                shutil.copyfile(src_filepath, os.path.join(output_dir.path, os.path.basename(src_filepath)))\n\n        else:\n            src_dirpath, src_filepath = data\n            if src_filepath.startswith(cache_dir):\n                src_filepath = src_filepath.replace(cache_dir, \"\")\n\n            if output_dir.url:\n                obj = parse.urlparse(output_dir.url)\n                s3.client.upload_file(\n                    src_filepath,\n                    obj.netloc,\n                    os.path.join(str(obj.path).lstrip(\"/\"), os.path.relpath(src_filepath, src_dirpath)),\n                )\n            else:\n                assert output_dir.path\n                shutil.copyfile(src_filepath, os.path.join(output_dir.path, os.path.relpath(src_filepath, src_dirpath)))\n\n        # 4. Inform the remover that the file is uploaded.\n        if remove_queue is not None:\n            remove_queue.put(data)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Collect paths\n        data = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if data is None:\n            return\n\n        # 3. Upload the file\n        if isinstance(data, str):\n            filepath = data\n            local_filepath = filepath.replace(cache_dir, \"\")\n            if local_filepath.startswith(\"/\"):\n                local_filepath = local_filepath[1:]\n\n            if not os.path.exists(filepath):\n                raise ValueError(f\"The file {filepath} doesn't exist.\")\n\n            if output_dir.url:\n                obj = parse.urlparse(os.path.join(output_dir.url, local_filepath))\n                s3.client.upload_file(filepath, obj.netloc, obj.path.lstrip(\"/\"))\n            elif output_dir.path:\n                output_filepath = os.path.join(output_dir.path, local_filepath)\n                os.makedirs(os.path.dirname(output_filepath), exist_ok=True)\n                shutil.copyfile(filepath, output_filepath)\n            else:\n                raise ValueError(\"The output directory should be either a path or a url.\")\n\n        elif isinstance(data, tuple):\n            temp_dir, filepath = data\n            local_filepath = filepath.replace(cache_dir, \"\")\n            if local_filepath.startswith(\"/\"):\n                local_filepath = local_filepath[1:]\n\n            if not os.path.exists(filepath):\n                raise ValueError(f\"The file {filepath} doesn't exist.\")\n\n            if output_dir.url:\n                obj = parse.urlparse(os.path.join(output_dir.url, local_filepath))\n                s3.client.upload_file(filepath, obj.netloc, obj.path.lstrip(\"/\"))\n            elif output_dir.path:"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if weights is None:\n        weights = [1] * len(user_items)\n\n    total_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = total_nodes * num_workers\n\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n    worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n    # Print the distribution details for workers on the current node\n    for worker_id, worker_item, worker_weight in zip(worker_ids_this_node, worker_items, worker_weights):\n        if file_size:\n            print(f\"Worker {worker_id} will process {len(worker_item)} items with a total size of {worker_weight / 1024 / 1024:.2f} MB\")\n        else:\n            print(f\"Worker {worker_id} will process {len(worker_item)} items with a total weight of {worker_weight}\")\n\n    # Return the list of items for each worker, with the items shuffled\n    return [random.sample(worker_item, len(worker_item)) for worker_item in worker_items]\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if file_size:\n        print(\"Distributing items based on file size:\")\n    else:\n        print(\"Distributing items based on weights:\")\n\n    for worker_id in worker_ids_this_node:\n        worker_items = worker_items[worker_id]\n        worker_weights = worker_weights[worker_id]\n        print(f\"Worker {worker_id} has {len(worker_items)} items, with total weight {sum(worker_weights)}\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Calculate the total number of workers across all nodes\n    total_workers = num_nodes * num_workers\n\n    # Calculate the number of items for each worker\n    items_per_worker = len(user_items) // total_workers\n    extra_items = len(user_items) % total_workers\n\n    # Assign items to workers based on weights\n    worker_items = []\n    worker_weights = []\n    start = 0\n    for i in range(total_workers):\n        num_items = items_per_worker + 1 if i < extra_items else items_per_worker\n        end = start + num_items\n        worker_items.append(user_items[start:end])\n        worker_weights.append(sum(weights[start:end]))\n        start = end\n\n    # Print the distribution details for workers on the current node\n    for worker_id in worker_ids_this_node:\n        worker_item = worker_items[worker_id]\n        worker_weight = worker_weights[worker_id]\n        print(f\"Worker {worker_id} will process {len(worker_item)} items with a total weight of {worker_weight}\")\n\n    # Shuffle the items for each worker\n    for i in range(len(worker_items)):\n        random.shuffle(worker_items[i])\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    total_nodes = os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 1)\n    node_rank = os.getenv(\"DATA_OPTIMIZER_NODE_RANK\", 0)\n    total_workers = total_nodes * num_workers\n\n    # Calculate the total number of workers across all nodes\n    total_workers = total_nodes * num_workers\n\n    # Calculate the number of items per worker\n    items_per_worker = len(user_items) // total_workers\n    extra_items = len(user_items) % total_workers\n\n    # Calculate the number of items for each worker\n    worker_items = [items_per_worker + 1 if i < extra_items else items_per_worker for i in range(total_workers)]\n\n    # Calculate the total weight of items for each worker\n    worker_weights = [sum([weights[j] for j in range(i * items_per_worker, (i + 1) * items_per_worker)]) for i in range(total_workers)]\n\n    # Calculate the total weight of items for each node\n    node_weights = [sum(worker_weights[i * num_workers:(i + 1) * num_workers]) for i in range(total_nodes)]\n\n    # Calculate the number of items for each node\n    node_items = [sum(worker_items[i * num_workers:(i + 1) * num_workers]) for i in range(total_nodes)]\n\n    # Calculate the start index for each node\n    node_starts = [sum(node_items[:i]) for i in range(total_nodes)]\n\n    # Calculate the start index for each worker\n    worker_starts = [sum(worker_items[:i]) for i in range(total_workers)]\n\n    # Calculate the end index for each worker\n    worker_ends = [sum(worker_items[:i + 1]) for i in range(total_workers)]\n\n    # Calculate the total weight of items for each worker\n    worker_total_"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if num_workers == 1:\n        return [user_items]\n\n    # Calculate the total number of workers across all nodes\n    total_workers = num_workers * num_nodes\n\n    # Calculate the number of items per worker\n    items_per_worker = len(user_items) // total_workers\n\n    # Calculate the number of extra items that need to be distributed among the workers\n    extra_items = len(user_items) % total_workers\n\n    # Initialize the start index for the current worker\n    start = 0\n\n    # Initialize the result list\n    result = []\n\n    # Iterate through the workers\n    for i in range(total_workers):\n        # Calculate the number of items for the current worker\n        worker_items = items_per_worker + 1 if i < extra_items else items_per_worker\n\n        # Calculate the end index for the current worker\n        end = start + worker_items\n\n        # Append the items for the current worker to the result list\n        result.append(user_items[start:end])\n\n        # Update the start index for the next worker\n        start = end\n\n    # Check if the number of workers in the result list matches the expected number of workers\n    if len(result) != num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers\")\n\n    # Shuffle the items for each worker\n    for worker_items in result:\n        random.shuffle(worker_items)\n\n    # Print the distribution details for workers on the current node\n    for i, worker_items in enumerate(result):\n        if i in worker_ids_this_node:\n            if file_size:\n                worker_size = sum([os.path.getsize(item) for item in worker_items]) / (1024 * 1024)\n                print(f\"Worker {i} has {len(worker_items)} items, total size {worker_size:.2f} MB\")\n            else:\n                print(f\"Worker {i} has {len(worker_items)} items, total weight"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    total_workers = num_nodes * num_workers\n\n    # Calculate the total weight of items\n    total_weight = sum(weights)\n\n    # Calculate the target weight per worker\n    target_weight = total_weight / total_workers\n\n    # Calculate the number of items per worker\n    items_per_worker = int(total_weight / total_workers)\n\n    # Calculate the number of items per worker with the remaining items\n    items_per_worker_with_remainder = int(total_weight / total_workers) + 1\n\n    # Calculate the number of items that will be assigned to each worker\n    num_items_per_worker = [items_per_worker] * total_workers\n    num_items_per_worker[-1] = total_weight - (items_per_worker * (total_workers - 1))\n\n    # Calculate the number of items that will be assigned to each worker with the remaining items\n    num_items_per_worker_with_remainder = [items_per_worker_with_remainder] * total_workers\n    num_items_per_worker_with_remainder[-1] = total_weight - (items_per_worker_with_remainder * (total_workers - 1))\n\n    # Calculate the number of items that will be assigned to each worker with the remaining items\n    num_items_per_worker_with_remainder = [items_per_worker_with_remainder] * total_workers\n    num_items_per_worker_with_remainder[-1] = total_weight - (items_per_worker_with_remainder * (total_workers - 1))\n\n    # Calculate the number of items that will be assigned to each worker with the remaining items\n    num_items_per_worker_with_remainder = [items_per_worker_with_remainder] * total_workers\n    num_items_per_worker_with_remainder[-1] = total_weight - (items_per_worker_with_remainder * ("}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Calculate the total number of workers across all nodes\n    total_workers = num_nodes * num_workers\n\n    # Calculate the number of items for each worker\n    items_per_worker = len(user_items) // total_workers\n    extra_items = len(user_items) % total_workers\n\n    # Calculate the start index for each worker\n    start = 0\n    result = []\n    for i in range(total_workers):\n        worker_items = items_per_worker + 1 if i < extra_items else items_per_worker\n        end = start + worker_items\n        result.append(user_items[start:end])\n        start = end\n\n    # Check if the number of workers matches the expected number\n    if len(result) != num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers\")\n\n    # Print the distribution details for workers on the current node\n    for worker_id in worker_ids_this_node:\n        worker_index = worker_id % num_workers\n        worker_items = result[worker_index]\n        worker_weights = worker_weights[worker_index * len(user_items) : (worker_index + 1) * len(user_items)]\n        if file_size:\n            worker_size = sum([os.path.getsize(item) for item in worker_items])\n            print(f\"Worker {worker_id} will process {len(worker_items)} files with a total size of {worker_size / (1024 ** 2)} MB.\")\n        else:\n            worker_weight = sum(worker_weights)\n            print(f\"Worker {worker_id} will process {len(worker_items)} files with a total weight of {worker_weight}.\")\n\n    # Shuffle the items for each worker\n    for worker_items in result:\n        random.shuffle(worker_items)\n\n    return result\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Calculate the number of items per worker\n    total_items = len(user_items)\n    items_per_worker = total_items // world_size\n    extra_items = total_items % world_size\n\n    # Distribute items to workers based on provided weights\n    worker_items = []\n    worker_weights = []\n    for i in range(world_size):\n        worker_weight = weights[i] if weights else 1\n        worker_items.append(items_per_worker + 1 if i < extra_items else items_per_worker)\n        worker_weights.append(worker_weight)\n\n    # Print the distribution details for workers on the current node\n    print(f\"Node {node_rank}:\")\n    for i, worker_id in enumerate(worker_ids_this_node):\n        print(f\"Worker {worker_id}: {worker_items[worker_id]} items\")\n\n    # Shuffle the items for each worker\n    for i, worker_id in enumerate(worker_ids_this_node):\n        start = sum(worker_items[:i])\n        end = start + worker_items[i]\n        worker_items[i] = user_items[start:end]\n        random.shuffle(worker_items[i])\n\n    # Print the distribution details for workers on the current node\n    for i, worker_id in enumerate(worker_ids_this_node):\n        if file_size:\n            total_size = sum([os.path.getsize(item) for item in worker_items[i]]) / (1024 * 1024)\n            print(f\"Worker {worker_id}: {len(worker_items[i])} items, {total_size:.2f} MB\")\n        else:\n            print(f\"Worker {worker_id}: {len(worker_items[i])} items, {sum(worker_weights[i])} total weight\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Check if the number of items and weights match\n    if len(user_items) != len(weights):\n        raise ValueError(\"The number of items and weights should match\")\n\n    # Calculate the total number of workers across all nodes\n    total_workers = num_nodes * num_workers\n\n    # Calculate the number of items and weights for each worker\n    items_per_worker = len(user_items) // total_workers\n    extra_items = len(user_items) % total_workers\n\n    # Calculate the total weight for each worker\n    total_weight_per_worker = sum(weights) // total_workers\n    extra_weight = sum(weights) % total_workers\n\n    # Calculate the number of items and weights for each worker on the current node\n    worker_items = [items_per_worker + 1 if i < extra_items else items_per_worker for i in range(total_workers)]\n    worker_weights = [total_weight_per_worker + 1 if i < extra_weight else total_weight_per_worker for i in range(total_workers)]\n\n    # Calculate the start and end indices for each worker on the current node\n    start_indices = [sum(worker_items[:i]) for i in range(total_workers)]\n    end_indices = [start_indices[i] + worker_items[i] for i in range(total_workers)]\n\n    # Assign items to each worker based on the weights\n    worker_assignments = []\n    for i in range(total_workers):\n        worker_assignments.append(user_items[start_indices[i]:end_indices[i]])\n\n    # Calculate the total weight for each worker on the current node\n    worker_total_weights = [sum(weights[start_indices[i]:end_indices[i]]) for i in range(total_workers)]\n\n    # Calculate the number of items and weights for each worker on the current node\n    worker_items = [len(worker_assignments[i]) for i in"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Calculate the total number of workers across all nodes\n    total_workers = num_workers * num_nodes\n\n    # Calculate the number of items to be assigned to each worker\n    items_per_worker = len(user_items) // total_workers\n    extra_items = len(user_items) % total_workers\n\n    # Initialize the start index for each worker\n    start = 0\n\n    # Initialize the result list\n    result = []\n\n    # Assign items to each worker\n    for i in range(total_workers):\n        # Determine the number of items for this worker\n        worker_items = items_per_worker + 1 if i < extra_items else items_per_worker\n\n        # Determine the end index for this worker\n        end = start + worker_items\n\n        # Append the items assigned to this worker to the result list\n        result.append(user_items[start:end])\n\n        # Update the start index for the next worker\n        start = end\n\n    # Check if the number of workers matches the expected number\n    if len(result) != num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers\")\n\n    # Print the distribution details for workers on the current node\n    for worker_id in worker_ids_this_node:\n        worker_items = result[worker_id]\n        worker_weight = sum(weights[user_items.index(item)] for item in worker_items)\n        print(f\"Worker {worker_id} has {len(worker_items)} items with {'megabytes' if file_size else 'weight'} of {worker_weight}.\")\n\n    # Shuffle the items assigned to each worker\n    for worker_items in result:\n        random.shuffle(worker_items)\n\n    return result\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    total_nodes = os.environ.get(\"DATA_OPTIMIZER_NUM_NODES\", 1)\n    node_rank = os.environ.get(\"DATA_OPTIMIZER_NODE_RANK\", 0)\n    total_workers = int(total_nodes) * num_workers\n\n    # Ensure the weights are integers\n    weights = [int(w) for w in weights]\n\n    # Calculate the total weight of all items\n    total_weight = sum(weights)\n\n    # Calculate the target weight for each worker\n    target_weight = total_weight // total_workers\n\n    # Calculate the remaining weight after distributing the target weight to all workers\n    remaining_weight = total_weight % total_workers\n\n    # Initialize a list to store the items assigned to each worker\n    worker_items = [[] for _ in range(total_workers)]\n\n    # Initialize a list to store the weights assigned to each worker\n    worker_weights = [0] * total_workers\n\n    # Distribute the items to the workers based on the calculated target weight\n    for i, (item, weight) in enumerate(zip(items, weights)):\n        # Calculate the index of the worker to assign the item to\n        worker_index = i % total_workers\n\n        # Assign the item to the worker\n        worker_items[worker_index].append(item)\n        worker_weights[worker_index] += weight\n\n        # If the weight assigned to the worker exceeds the target weight, move to the next worker\n        if worker_weights[worker_index] > target_weight:\n            worker_index = (worker_index + 1) % total_workers\n            worker_items[worker_index].append(item)\n            worker_weights[worker_index] += weight\n\n    # Distribute the remaining weight to the workers\n    for i in range(remaining_weight):\n        worker_index = i % total_workers\n        worker_weights[worker_index] += 1\n\n    # Print the distribution details for workers on the current node\n    for worker_index"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Calculate the total number of workers across all nodes\n    total_workers = num_nodes * num_workers\n\n    # Calculate the number of items to assign to each worker\n    items_per_worker = len(user_items) // total_workers\n\n    # Calculate the number of extra items to distribute among the first few workers\n    extra_items = len(user_items) % total_workers\n\n    # Initialize the start index for the current node\n    start = 0\n\n    # Initialize the result list\n    result = []\n\n    # Iterate through each worker in the current node\n    for i in range(num_workers):\n        # Calculate the number of items for the current worker\n        worker_items = items_per_worker + 1 if i < extra_items else items_per_worker\n\n        # Calculate the end index for the current worker\n        end = start + worker_items\n\n        # Get the items for the current worker\n        worker_items = user_items[start:end]\n\n        # Get the weights for the current worker\n        worker_weights = weights[start:end]\n\n        # Shuffle the items for the current worker\n        worker_items, worker_weights = zip(*sorted(zip(worker_items, worker_weights), key=lambda x: random.random()))\n\n        # Append the items for the current worker to the result list\n        result.append(list(worker_items))\n\n        # Update the start index for the next worker\n        start = end\n\n    # Check if the number of workers in the current node matches the expected number\n    if len(result) != num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers\")\n\n    # Print the distribution details for workers on the current node\n    for i, worker_items in enumerate(result):\n        worker_id = worker_ids_this_node[i]\n        worker_weight = sum(worker_weights[start:end])\n        if file_size:\n            worker_weight = round(worker_weight / (1024 * 1024), 2)  #"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Calculate the total number of workers across all nodes\n    total_workers = num_nodes * num_workers\n\n    # Calculate the number of items each worker should process\n    items_per_worker = len(user_items) // total_workers\n\n    # Calculate the number of extra items that need to be distributed among the workers\n    extra_items = len(user_items) % total_workers\n\n    # Initialize a list to store the items assigned to each worker\n    result = [[] for _ in range(total_workers)]\n\n    # Initialize a counter to keep track of the current worker\n    current_worker = 0\n\n    # Iterate through the items and assign them to the workers\n    for i, item in enumerate(user_items):\n        # Add the item to the current worker's list\n        result[current_worker].append(item)\n\n        # Check if the current worker has processed all the items it should process\n        if (i + 1) % items_per_worker == 0:\n            # If the current worker has processed all the items it should process, move to the next worker\n            current_worker += 1\n\n            # If there are extra items to be distributed, add one to the current worker's list\n            if extra_items > 0:\n                result[current_worker].append(user_items[i + 1])\n                extra_items -= 1\n\n    # Print the distribution details for workers on the current node\n    for i, worker_items in enumerate(result):\n        if i in worker_ids_this_node:\n            if file_size:\n                print(f\"Worker {i} will process {len(worker_items)} items with a total size of {sum(worker_items)} bytes.\")\n            else:\n                print(f\"Worker {i} will process {len(worker_items)} items with a total weight of {sum(worker_items)}.\")\n\n    # Shuffle the items assigned to each worker to avoid any bias in the distribution\n    for worker_items in result:\n        random.shuffle(worker_items)\n\n    return result\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Compute the total number of items and the number of items per worker\n    total_items = len(user_items)\n    items_per_worker = total_items // total_workers\n    extra_items = total_items % total_workers\n\n    # Initialize the worker_items and worker_weights lists\n    worker_items = [[] for _ in range(total_workers)]\n    worker_weights = [[] for _ in range(total_workers)]\n\n    # Distribute the items to the workers based on weights\n    for i, (item, weight) in enumerate(zip(user_items, weights)):\n        worker_id = i % total_workers\n        worker_items[worker_id].append(item)\n        worker_weights[worker_id].append(weight)\n\n    # Distribute the extra items to the first few workers\n    for i in range(extra_items):\n        worker_id = i % total_workers\n        worker_items[worker_id].append(user_items[i + total_items - extra_items])\n        worker_weights[worker_id].append(weights[i + total_items - extra_items])\n\n    # Print the distribution details for workers on the current node\n    for worker_id in worker_ids_this_node:\n        worker_items_str = \", \".join(\n            [f\"{item} ({weight})\" if file_size else f\"{item} ({weight} bytes)\" for item, weight in zip(worker_items[worker_id], worker_weights[worker_id])]\n        )\n        print(f\"Worker {worker_id} has {len(worker_items[worker_id])} items: {worker_items_str}\")\n\n    # Shuffle the items within each worker\n    for i in range(total_workers):\n        worker_items[i] = [item for _, item in sorted(zip(worker_weights[i], worker_items[i]))]\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if file_size:\n        def print_distribution(worker_items, worker_weights):\n            for worker_id, items in enumerate(worker_items):\n                print(f\"Worker {worker_id}:\")\n                for item, weight in zip(items, worker_weights):\n                    print(f\"  Item: {item} ({weight / 1000 / 1000:.2f} MB)\")\n    else:\n        def print_distribution(worker_items, worker_weights):\n            for worker_id, items in enumerate(worker_items):\n                print(f\"Worker {worker_id}:\")\n                for item, weight in zip(items, worker_weights):\n                    print(f\"  Item: {item} ({weight} weight)\")\n\n    print_distribution(worker_items, worker_weights)\n\n    # Shuffle the items for each worker\n    for i, worker_id in enumerate(worker_ids_this_node):\n        np.random.shuffle(worker_items[i])\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Calculate the total number of workers across all nodes\n    total_workers = num_workers * num_nodes\n\n    # Distribute items to workers based on weights\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=total_workers)\n\n    # Get the worker IDs for the current node\n    worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n    # Print the distribution details for workers on the current node\n    for worker_id in worker_ids_this_node:\n        worker_item_indices = [i for i, worker_id_item in enumerate(worker_items) if worker_id_item == worker_id]\n        worker_item_weights = [weights[i] for i in worker_item_indices]\n        worker_item_count = len(worker_item_indices)\n        worker_item_weight_sum = sum(worker_item_weights)\n\n        if file_size:\n            worker_item_size_sum = sum([os.path.getsize(user_items[i]) for i in worker_item_indices])\n            print(\n                f\"Worker {worker_id} will process {worker_item_count} items ({worker_item_size_sum / (1024 * 1024):.2f} MB) \"\n                f\"with total weight {worker_item_weight_sum}.\"\n            )\n        else:\n            print(\n                f\"Worker {worker_id} will process {worker_item_count} items with total weight {worker_item_weight_sum}.\"\n            )\n\n    # Return the items assigned to each worker, shuffled randomly\n    return [\n        [user_items[i] for i in worker_item_indices] for worker_item_indices in worker_items if worker_item_indices\n    ]\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if num_workers > num_nodes:\n        raise ValueError(\"The number of workers per node must be less than or equal to the number of nodes.\")\n\n    if num_nodes == 1:\n        return [worker_items]\n\n    # Compute the total number of workers across all nodes\n    total_workers = num_nodes * num_workers\n\n    # Compute the number of items per worker\n    items_per_worker = len(user_items) // total_workers\n    extra_items = len(user_items) % total_workers\n\n    # Compute the start index for each worker\n    start = 0\n    result = []\n    for i in range(total_workers):\n        worker_items = items_per_worker + 1 if i < extra_items else items_per_worker\n        end = start + worker_items\n        result.append(user_items[start:end])\n        start = end\n\n    # Print the distribution details for workers on the current node\n    for i, worker_items in enumerate(result):\n        if i % num_workers == node_rank:\n            worker_weight = sum(worker_weights[i * num_workers : (i + 1) * num_workers])\n            print(f\"Worker {i} on node {node_rank} will process {len(worker_items)} items with a total weight of {worker_weight}.\")\n            if file_size:\n                print(f\"Total size of files: {sum(worker_weight) / 1000 / 1000} MB\")\n\n    # Shuffle the items for each worker\n    for worker_items in result:\n        random.shuffle(worker_items)\n\n    return result\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if not weights:\n        weights = [1] * len(user_items)\n    if len(weights) != len(user_items):\n        raise ValueError(\"The length of weights should match the length of user_items.\")\n\n    # Calculate the total number of items and the number of items per worker\n    total_items = sum(weights)\n    items_per_worker = total_items // world_size\n    extra_items = total_items % world_size\n\n    # Calculate the start and end indices for each worker\n    start_indices = [0] * world_size\n    end_indices = [0] * world_size\n    for i in range(world_size):\n        start_indices[i] = sum(weights[:i])\n        end_indices[i] = start_indices[i] + items_per_worker + (1 if i < extra_items else 0)\n\n    # Assign items to workers based on the start and end indices\n    worker_items = [user_items[start:end] for start, end in zip(start_indices, end_indices)]\n    worker_weights = [sum(weights[start:end]) for start, end in zip(start_indices, end_indices)]\n\n    # Print the distribution details for workers on the current node\n    print(f\"Node {node_rank} distribution:\")\n    for i in worker_ids_this_node:\n        print(f\"Worker {i}: {worker_weights[i]} items ({worker_weights[i] / total_items * 100:.2f}%)\")\n\n    # Shuffle the items assigned to each worker\n    for i in range(len(worker_items)):\n        random.shuffle(worker_items[i])\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    total_workers = num_nodes * num_workers\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=total_workers)\n\n    # Print the distribution details for workers on the current node\n    print(f\"Worker distribution for node {node_rank}:\")\n    for i in worker_ids_this_node:\n        worker_item_indices = [j for j, w in enumerate(worker_items) if w == i]\n        worker_item_weights = [weights[j] for j in worker_item_indices]\n        worker_item_sizes = [os.path.getsize(user_items[j]) for j in worker_item_indices]\n        worker_item_sizes_str = [f\"{s / (1024 * 1024):.2f} MB\" for s in worker_item_sizes]\n        worker_item_weights_str = [f\"{w} items\" for w in worker_item_weights]\n        worker_item_str = (\n            \", \".join(worker_item_weights_str)\n            if file_size\n            else \", \".join(worker_item_sizes_str)\n            if worker_item_sizes\n            else \", \".join(worker_item_weights_str)\n        )\n        print(f\"Worker {i}: {worker_item_str}\")\n\n    # Return a list of items for each worker, with the items shuffled randomly\n    result = [\n        [user_items[j] for j, w in enumerate(worker_items) if w == i]\n        for i in worker_ids_this_node\n    ]\n\n    return result\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if file_size:\n        print(f\"Total workers: {total_workers}\")\n        print(f\"Total nodes: {num_nodes}\")\n        print(f\"Current node: {node_rank}\")\n\n        print(f\"Total items: {len(user_items)}\")\n        print(f\"Total weights: {sum(weights)}\")\n        print(f\"Total file sizes: {sum([os.path.getsize(item) for item in user_items])} bytes\")\n\n        print(\"Distributing items to workers...\")\n        for i, worker_items in enumerate(worker_items):\n            print(f\"Worker {i}: {len(worker_items)} items, {sum([os.path.getsize(item) for item in worker_items])} bytes\")\n\n        print(\"Distributing weights to workers...\")\n        for i, worker_weight in enumerate(worker_weights):\n            print(f\"Worker {i}: {worker_weight}\")\n\n        print(\"Distributing items to workers on current node...\")\n        for i, worker_items in enumerate(worker_items):\n            if i in worker_ids_this_node:\n                print(f\"Worker {i}: {len(worker_items)} items, {sum([os.path.getsize(item) for item in worker_items])} bytes\")\n\n    # Shuffle the items for each worker\n    for i in range(len(worker_items)):\n        random.shuffle(worker_items[i])\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Calculate the number of items each worker should process based on the total number of workers across all nodes and the total number of items.\n    num_nodes = _get_num_nodes()\n    total_workers = num_workers * num_nodes\n    num_items = len(user_items)\n    items_per_worker = num_items // total_workers\n    remainder = num_items % total_workers\n\n    # Adjust for any remainder by distributing extra items to the last workers in the list.\n    start_indices = np.cumsum([items_per_worker] * (total_workers - remainder) + [items_per_worker + 1] * remainder)\n    start_indices = np.insert(start_indices, 0, 0)\n    end_indices = np.cumsum([items_per_worker + 1] * (total_workers - remainder) + [items_per_worker] * remainder)\n    end_indices = np.append(end_indices, num_items)\n\n    # Ensure the output list has a length equal to the number of workers; otherwise, raise a RuntimeError.\n    if len(start_indices) != total_workers:\n        raise RuntimeError(\"Improper assignment of items to workers.\")\n\n    # Return the list of lists, where each sublist contains the items assigned to a worker.\n    return [user_items[start:end] for start, end in zip(start_indices, end_indices)]\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Calculate the number of items each worker should process\n    num_nodes = _get_num_nodes()\n    num_items = len(user_items)\n    num_workers_total = num_nodes * num_workers\n    num_items_per_worker = num_items // num_workers_total\n    num_items_remainder = num_items % num_workers_total\n\n    # Adjust for the remainder by adding extra items to the workers starting from the end of the list\n    for i in range(num_items_remainder):\n        user_items[num_items_per_worker * (num_workers_total - i - 1) : num_items_per_worker * (num_workers_total - i)] += [\n            user_items[num_items_per_worker * (num_workers_total - i - 1)]\n        ]\n\n    # Calculate the start and end indices for each worker's items\n    worker_start_indices = np.cumsum([0] + [num_items_per_worker] * (num_workers_total - 1))\n    worker_end_indices = np.cumsum([num_items_per_worker] * num_workers_total)\n\n    # Assign the items to the workers based on the start and end indices\n    worker_items = [user_items[start:end] for start, end in zip(worker_start_indices, worker_end_indices)]\n\n    # Ensure the output list has a length equal to the number of workers\n    if len(worker_items) != num_workers_total:\n        raise RuntimeError(\"Improper assignment of items to workers.\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    num_workers_total = num_workers * num_nodes\n    num_items = len(user_items)\n    items_per_worker = num_items // num_workers_total\n    remainder = num_items % num_workers_total\n\n    # Calculate the number of items each worker should process\n    worker_items = [items_per_worker] * num_workers_total\n\n    # Distribute the remainder among the last workers\n    for i in range(remainder):\n        worker_items[-i - 1] += 1\n\n    # Calculate the start and end indices for each worker's items\n    cumulative_sum = np.cumsum(worker_items)\n    start_indices = np.insert(cumulative_sum[:-1], 0, 0)\n    end_indices = cumulative_sum\n\n    # Split the items among the workers\n    worker_items = [user_items[start:end] for start, end in zip(start_indices, end_indices)]\n\n    # Check if the output list has the correct length\n    if len(worker_items) != num_workers_total:\n        raise RuntimeError(\n            f\"The output list has length {len(worker_items)}, but it should have length {num_workers_total}.\"\n        )\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Calculate the number of items each worker should process\n    num_nodes = _get_num_nodes()\n    world_size = num_nodes * num_workers\n    num_items = len(user_items)\n    items_per_worker = num_items // world_size\n    remainder = num_items % world_size\n\n    # Distribute items to workers\n    worker_items = []\n    start_index = 0\n    for worker_id in range(world_size):\n        end_index = start_index + items_per_worker\n        if remainder > 0:\n            end_index += 1\n            remainder -= 1\n        worker_items.append(user_items[start_index:end_index])\n        start_index = end_index\n\n    # Check if the output list has the correct number of workers\n    if len(worker_items) != world_size:\n        raise RuntimeError(f\"The number of workers ({len(worker_items)}) does not match the expected number ({world_size}).\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    total_workers = num_workers * num_nodes\n    num_items = len(user_items)\n    items_per_worker = num_items // total_workers\n    remainder = num_items % total_workers\n\n    worker_items = []\n    start_index = 0\n    for worker_id in range(total_workers):\n        end_index = start_index + items_per_worker\n        if remainder > 0:\n            end_index += 1\n            remainder -= 1\n        worker_items.append(user_items[start_index:end_index])\n        start_index = end_index\n\n    if len(worker_items) != total_workers:\n        raise RuntimeError(f\"The number of workers ({len(worker_items)}) doesn't match the total number of workers ({total_workers}).\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    if num_workers < 1:\n        raise ValueError(\"The number of workers must be greater than 0.\")\n\n    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // world_size\n    remainder = len(user_items) % world_size\n\n    # Adjust for any remainder by distributing extra items to the last workers in the list\n    start_index = num_items_per_worker * node_rank * num_workers\n    end_index = start_index + num_items_per_worker\n    if node_rank == num_nodes - 1:\n        end_index += remainder\n\n    # Ensure the output list has a length equal to the number of workers\n    if end_index - start_index != num_items_per_worker:\n        raise RuntimeError(\"Improper assignment of items to workers.\")\n\n    return [user_items[start_index:end_index]]\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    world_size = num_nodes * num_workers\n    num_items = len(user_items)\n    items_per_worker = num_items // world_size\n    remainder = num_items % world_size\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = [items_per_worker] * world_size\n    for i in range(remainder):\n        num_items_per_worker[-(i + 1)] += 1\n\n    # Calculate the start and end indices for each worker's items\n    cumulative_sum = np.cumsum(num_items_per_worker)\n    start_indices = np.insert(cumulative_sum[:-1], 0, 0)\n    end_indices = cumulative_sum\n\n    # Assign the items to each worker\n    worker_items = [user_items[start:end] for start, end in zip(start_indices, end_indices)]\n\n    # Check if the output list has a length equal to the number of workers\n    if len(worker_items) != world_size:\n        raise RuntimeError(\"Improper assignment of items to workers.\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Calculate the total number of workers across all nodes\n    num_nodes = _get_num_nodes()\n    total_workers = num_workers * num_nodes\n\n    # Calculate the number of items each worker should process\n    num_items = len(user_items)\n    items_per_worker = num_items // total_workers\n    remainder = num_items % total_workers\n\n    # Create a list of lists to store the items assigned to each worker\n    worker_items = [[] for _ in range(total_workers)]\n\n    # Assign items to each worker sequentially\n    for i, item in enumerate(user_items):\n        worker_index = i // items_per_worker\n        worker_items[worker_index].append(item)\n\n    # Distribute the remaining items to the last workers in the list\n    for i in range(remainder):\n        worker_items[-(i + 1)].append(user_items[num_items - (i + 1)])\n\n    # Verify that the output list has a length equal to the number of workers\n    if len(worker_items) != total_workers:\n        raise RuntimeError(f\"The number of items assigned to each worker is not equal to the total number of workers. Expected {total_workers} but got {len(worker_items)}.\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Calculate the total number of workers across all nodes\n    num_nodes = _get_num_nodes()\n    total_workers = num_nodes * num_workers\n\n    # Calculate the number of items each worker should process\n    items_per_worker = len(user_items) // total_workers\n\n    # Calculate the remaining items that need to be distributed among the workers\n    remaining_items = len(user_items) % total_workers\n\n    # Create a list of lists, where each sublist contains the items assigned to a worker\n    worker_items = []\n\n    # Assign items to each worker sequentially\n    for i in range(total_workers):\n        start_index = i * items_per_worker + min(i, remaining_items)\n        end_index = start_index + items_per_worker + (1 if i < remaining_items else 0)\n        worker_items.append(user_items[start_index:end_index])\n\n    # Check if the number of items in the worker_items list matches the total number of workers\n    if len(worker_items) != total_workers:\n        raise RuntimeError(\"The number of items assigned to each worker does not match the total number of workers.\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Calculate the total number of workers across all nodes\n    num_nodes = _get_num_nodes()\n    total_workers = num_workers * num_nodes\n\n    # Calculate the number of items each worker should process\n    items_per_worker = len(user_items) // total_workers\n\n    # Calculate the remaining items after distributing the items evenly among all workers\n    remainder = len(user_items) % total_workers\n\n    # Create a list of lists to store the items assigned to each worker\n    worker_items = [[] for _ in range(total_workers)]\n\n    # Assign the items to the workers sequentially\n    for i, item in enumerate(user_items):\n        worker_index = i // items_per_worker\n        worker_items[worker_index].append(item)\n\n    # Distribute the remaining items among the last workers\n    for i in range(remainder):\n        worker_index = total_workers - remainder + i\n        worker_items[worker_index].append(user_items[i * items_per_worker + items_per_worker])\n\n    # Ensure the output list has a length equal to the number of workers\n    if len(worker_items) != total_workers:\n        raise RuntimeError(\"Improper assignment of items to workers.\")\n\n    # Return the list of lists of items assigned to each worker\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Calculate the total number of workers across all nodes\n    num_nodes = _get_num_nodes()\n    total_num_workers = num_nodes * num_workers\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // total_num_workers\n    remainder = len(user_items) % total_num_workers\n\n    # Adjust the number of items for the last workers in the list\n    num_items_per_worker += remainder // num_workers\n\n    # Calculate the start and end indices for each worker's items\n    cumulative_sum = np.cumsum([0] + [num_items_per_worker] * total_num_workers)\n    start_indices = cumulative_sum[:-1]\n    end_indices = cumulative_sum[1:]\n\n    # Create a list of lists, where each sublist contains the items assigned to a worker\n    worker_items = [user_items[start:end] for start, end in zip(start_indices, end_indices)]\n\n    # Ensure the output list has a length equal to the number of workers\n    if len(worker_items) != total_num_workers:\n        raise RuntimeError(f\"The number of workers ({total_num_workers}) does not match the number of items ({len(user_items)})\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # 1. Calculate the number of items each worker should process based on the total number of workers across all nodes and the total number of items.\n    num_nodes = _get_num_nodes()\n    world_size = num_nodes * num_workers\n    num_items = len(user_items)\n    items_per_worker = num_items // world_size\n    remainder = num_items % world_size\n\n    # 2. Adjust for any remainder by adding extra items to the workers starting from the end of the list, ensuring an even distribution as much as possible.\n    extra_items = remainder\n    for i in range(world_size - 1, -1, -1):\n        if extra_items > 0:\n            items_per_worker += 1\n            extra_items -= 1\n\n    # 3. Use cumulative sum to efficiently calculate the start and end indices for each worker's items.\n    start_indices = np.cumsum(np.array([0] + [items_per_worker] * (world_size - 1)))\n    end_indices = start_indices + np.array([items_per_worker] * world_size)\n\n    # 4. Ensure the output list has a length equal to the number of workers; otherwise, it raises a RuntimeError indicating improper assignment.\n    if len(start_indices) != len(end_indices) or len(start_indices) != world_size:\n        raise RuntimeError(\"Improper assignment of items to workers.\")\n\n    # 5. Return the list of lists, where each sublist contains the items assigned to a worker.\n    return [user_items[start:end] for start, end in zip(start_indices, end_indices)]\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # 1. Calculate the total number of workers across all nodes.\n    world_size = num_workers * _get_num_nodes()\n\n    # 2. Calculate the number of items each worker should process.\n    num_items_per_worker = len(user_items) // world_size\n\n    # 3. Calculate the remainder of items that need to be distributed.\n    remainder = len(user_items) % world_size\n\n    # 4. Initialize a list of lists to store the items assigned to each worker.\n    worker_items = [[] for _ in range(world_size)]\n\n    # 5. Assign items to each worker sequentially.\n    for i, item in enumerate(user_items):\n        worker_index = i // num_items_per_worker\n        worker_items[worker_index].append(item)\n\n    # 6. Distribute the remainder of items to the last workers.\n    for i in range(remainder):\n        worker_index = world_size - remainder + i\n        worker_items[worker_index].append(user_items[num_items_per_worker * worker_index + i])\n\n    # 7. Ensure the output list has a length equal to the number of workers.\n    assert len(worker_items) == world_size, \"The assignment of items to workers is incorrect.\"\n\n    # 8. Return the list of lists containing the items assigned to each worker.\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Calculate the total number of workers across all nodes\n    num_nodes = _get_num_nodes()\n    total_workers = num_nodes * num_workers\n\n    # Calculate the number of items each worker should process\n    items_per_worker = len(user_items) // total_workers\n\n    # Calculate the remainder of items that couldn't be evenly distributed\n    remainder = len(user_items) % total_workers\n\n    # Create a list of lists to store the items assigned to each worker\n    worker_items = [[] for _ in range(total_workers)]\n\n    # Distribute the items to the workers sequentially\n    start_index = 0\n    for i in range(total_workers):\n        end_index = start_index + items_per_worker\n        if i < remainder:\n            end_index += 1\n        worker_items[i] = user_items[start_index:end_index]\n        start_index = end_index\n\n    # Check if the number of items assigned to each worker is correct\n    assert sum([len(items) for items in worker_items]) == len(user_items)\n\n    # Return the list of worker items\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Calculate the number of items each worker should process\n    total_workers = _get_num_nodes() * num_workers\n    items_per_worker = len(user_items) // total_workers\n    remainder = len(user_items) % total_workers\n\n    # Create a list of lists to store the items assigned to each worker\n    worker_items = [[] for _ in range(total_workers)]\n\n    # Assign items to each worker sequentially\n    for i, item in enumerate(user_items):\n        worker_index = i // items_per_worker\n        if worker_index < remainder:\n            worker_index = i % (total_workers - remainder)\n        worker_items[worker_index].append(item)\n\n    # Check if the number of items assigned to each worker is correct\n    for worker_index, items in enumerate(worker_items):\n        expected_items = items_per_worker + int(worker_index < remainder)\n        if len(items) != expected_items:\n            raise RuntimeError(f\"Worker {worker_index} has {len(items)} items instead of {expected_items}\")\n\n    # Return the list of lists\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Calculate the number of workers across all nodes\n    num_nodes = _get_num_nodes()\n    total_num_workers = num_workers * num_nodes\n\n    # Determine the number of items each worker should process\n    items_per_worker = len(user_items) // total_num_workers\n    remainder = len(user_items) % total_num_workers\n\n    # Calculate the start and end indices for each worker's items\n    worker_start_indices = np.cumsum([0] + [items_per_worker] * (total_num_workers - 1))\n    worker_end_indices = np.cumsum([items_per_worker] * total_num_workers)\n\n    # Adjust for any remainder by distributing extra items to the last workers\n    if remainder > 0:\n        extra_items_per_worker = remainder // num_workers\n        extra_items_remainder = remainder % num_workers\n        worker_end_indices[-num_workers:] += extra_items_per_worker\n        worker_end_indices[-num_workers + extra_items_remainder :] += 1\n\n    # Assign items to workers based on start and end indices\n    worker_items = [user_items[start:end] for start, end in zip(worker_start_indices, worker_end_indices)]\n\n    # Ensure the output list has a length equal to the number of workers\n    if len(worker_items) != total_num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers.\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    items_per_worker = (num_workers * _get_num_nodes()) // len(user_items)\n    remainder = (num_workers * _get_num_nodes()) % len(user_items)\n\n    # Create a list of cumulative sums of items_per_worker\n    cumulative_sums = [0] + [items_per_worker * i for i in range(1, len(user_items) + 1)]\n\n    # Adjust for any remainder by adding extra items to the last workers\n    for i in range(remainder):\n        cumulative_sums[i + 1] += 1\n\n    # Use cumulative sums to determine start and end indices for each worker\n    worker_items = [user_items[cumulative_sums[i]:cumulative_sums[i + 1]] for i in range(len(user_items))]\n\n    # Ensure the output list has the correct length\n    if len(worker_items) != num_workers:\n        raise RuntimeError(\n            f\"The number of items assigned to each worker ({len(worker_items)}) does not match the number of workers ({num_workers}).\"\n        )\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    weights = [1] * len(user_items)\n    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    # Calculate the number of items each worker should process\n    items_per_worker = len(user_items) // world_size\n    remainder = len(user_items) % world_size\n\n    # Adjust the number of items for the last workers if there's a remainder\n    if remainder > 0:\n        items_per_worker += 1\n        for i in range(remainder):\n            weights[-i - 1] += 1\n\n    # Calculate the start and end indices for each worker's items\n    cumulative_sum = np.cumsum(weights)\n    start_indices = np.insert(cumulative_sum[:-1], 0, 0)\n    end_indices = cumulative_sum\n\n    # Assign the items to the workers\n    worker_items = [user_items[start:end] for start, end in zip(start_indices, end_indices)]\n\n    # Ensure the output list has the same length as the number of workers\n    if len(worker_items) != world_size:\n        raise RuntimeError(\"The number of workers and the number of assigned items do not match.\")\n\n    # Return the assigned items for each worker\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Calculate the total number of workers across all nodes\n    num_nodes = _get_num_nodes()\n    total_workers = num_nodes * num_workers\n\n    # Calculate the number of items each worker should process\n    items_per_worker = len(user_items) // total_workers\n    remainder = len(user_items) % total_workers\n\n    # Distribute items to workers\n    worker_items = []\n    start_index = 0\n    for worker_id in range(total_workers):\n        # Determine the number of items for the current worker\n        if worker_id < remainder:\n            num_items = items_per_worker + 1\n        else:\n            num_items = items_per_worker\n\n        # Get the items for the current worker\n        end_index = start_index + num_items\n        worker_items.append(user_items[start_index:end_index])\n\n        # Update the start index for the next worker\n        start_index = end_index\n\n    # Check if the number of items in the last worker is 0\n    if len(worker_items[-1]) == 0:\n        # Remove the last worker's items\n        worker_items.pop()\n\n        # Distribute the remainder items to the remaining workers\n        for i in range(remainder):\n            worker_items[i % (total_workers - 1)].append(user_items[start_index + i])\n\n    # Check if the number of items in the first worker is 0\n    if len(worker_items[0]) == 0:\n        # Remove the first worker's items\n        worker_items.pop(0)\n\n        # Distribute the remainder items to the remaining workers\n        for i in range(remainder):\n            worker_items[i % (total_workers - 1)].append(user_items[i])\n\n    # Check if the number of items in the first worker is 0\n    if len(worker_items[0]) == 0:\n        # Remove the first worker's items\n        worker_items.pop(0)\n\n    #"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Calculate the number of workers across all nodes\n    num_nodes = _get_num_nodes()\n    total_num_workers = num_workers * num_nodes\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // total_num_workers\n    remainder = len(user_items) % total_num_workers\n\n    # Initialize a list to store the items assigned to each worker\n    worker_items = [[] for _ in range(total_num_workers)]\n\n    # Distribute the items to the workers\n    for i, item in enumerate(user_items):\n        worker_idx = i // num_items_per_worker\n        if i < remainder:\n            # Add an extra item to the last workers\n            worker_idx += i\n        worker_items[worker_idx].append(item)\n\n    # Check if all workers have been assigned items\n    assert all(len(items) > 0 for items in worker_items), \"Not all workers have been assigned items.\"\n\n    # Return the list of items assigned to each worker\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": ""}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": ""}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": ""}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": ""}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "    def _cleanup_cache(self) -> None:\n        cache_dir = _get_cache_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "    def _cleanup_cache(self) -> None:\n        cache_dir = _get_cache_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "    def _cleanup_cache(self) -> None:\n        cache_dir = _get_cache_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        cache_data_dir = _get_cache_data_dir()\n\n        # Cleanup the cache data dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_data_dir):\n            shutil.rmtree(cache_data_dir, ignore_errors=True)\n\n        os.makedirs(cache_data_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        cache_data_dir = _get_cache_data_dir()\n\n        # Cleanup the cache data dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_data_dir):\n            shutil.rmtree(cache_data_dir, ignore_errors=True)\n\n        os.makedirs(cache_data_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        cache_data_dir = _get_cache_data_dir()\n\n        # Cleanup the cache data dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_data_dir):\n            shutil.rmtree(cache_data_dir, ignore_errors=True)\n\n        os.makedirs(cache_data_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        cache_data_dir = _get_cache_data_dir()\n\n        # Cleanup the cache data dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_data_dir):\n            shutil.rmtree(cache_data_dir, ignore_errors=True)\n\n        os.makedirs(cache_data_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "    def _cleanup_cache(self) -> None:\n        cache_dir = _get_cache_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "    def _cleanup_cache(self) -> None:\n        cache_dir = _get_cache_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "    def _cleanup_cache(self) -> None:\n        cache_dir = _get_cache_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)\n\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n    def _cleanup_cache(self) -> None:\n        cache_dir = _get_cache_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)\n\n\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n\n    def _cleanup_cache(self) -> None:\n        cache_dir = _get_cache_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "class DataChunkRecipe(DataRecipe):\n    def __init__(\n        self,\n        chunk_size: Optional[int] = None,\n        chunk_bytes: Optional[Union[int, str]] = None,\n        compression: Optional[str] = None,\n    ):\n        super().__init__()\n        if chunk_size is not None and chunk_bytes is not None:\n            raise ValueError(\"Either one of the `chunk_size` or the `chunk_bytes` need to be provided.\")\n\n        self.chunk_size = chunk_size\n        self.chunk_bytes = 1 << 26 if chunk_size is None else chunk_bytes\n        self.compression = compression\n\n    @abstractmethod\n    def prepare_structure(self, input_dir: Optional[str]) -> List[T]:\n        \"\"\"Return the structure of your data.\n\n        Each element should contain at least a filepath.\n\n        \"\"\"\n\n    @abstractmethod\n    def prepare_item(self, item_metadata: T) -> Any:\n        \"\"\"The return of this `prepare_item` method is persisted in chunked binary files.\"\"\"\n\n    def _done(self, size: int, delete_cached_files: bool, output_dir: Dir) -> _Result:\n        num_nodes = _get_num_nodes()\n        cache_dir = _get_cache_dir()\n\n        chunks = [file for file in os.listdir(cache_dir) if file.endswith(\".bin\")]\n        if chunks and delete_cached_files and output_dir.path is not None:\n            raise RuntimeError(f\"All the chunks should have been deleted. Found {chunks}\")\n\n        merge_cache = Cache(cache_dir, chunk_bytes=1)\n        node_rank = _get_node_rank()\n        merge_cache._merge_no_wait(node_rank if num_nodes > 1 else None)\n        self._upload_index(output_dir, cache_dir, num_nodes, node_rank)\n\n        if num_nodes == node_rank + 1"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\"\n        \"\"\"\n        The function cleans up cache directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "    def _cleanup_cache_data(self) -> None:\n        cache_data_dir = _get_cache_data_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_data_dir):\n            shutil.rmtree(cache_data_dir, ignore_errors=True)\n\n        os.makedirs(cache_data_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        cache_data_dir = _get_cache_data_dir()\n\n        # Cleanup the cache data dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_data_dir):\n            shutil.rmtree(cache_data_dir, ignore_errors=True)\n\n        os.makedirs(cache_data_dir, exist_ok=True)\n\n\n        \"\"\"\n        The function cleans up cache data directories by removing them if they exist to prevent issues from corrupted files from previous runs, and then recreates these directories to ensure they are available for use.\n\n        Input-Output Arguments\n        :param self: DataProcessor. An instance of the DataProcessor class. It uses this instance to access the method.\n        :return: No return values. This method performs operations on the filesystem but does not return any value.\n        \"\"\""}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    item_sizes = []\n\n    cpu_count = os.cpu_count() or 1\n\n    # Parallelize to accelerate retrieving the number of file bytes to read for each item\n    with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count * 2 if cpu_count > 4 else cpu_count) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    \"\"\"\n    Computes and returns a list of file sizes for each item in the given list by parallelizing the file size retrieval process to improve performance. It uses a ThreadPoolExecutor to execute the file size retrieval in parallel, adjusting the number of workers based on the CPU count.\n\n    Input-Output Arguments\n    :param items: List[Any]. A list of items for which to compute file sizes. The nature of these items is not specified, but they are used to determine file sizes in some way.\n    :param base_path: str, optional. A base path to be prepended to each item's path before computing its file size. Defaults to an empty string, indicating no base path is used.\n    :return: List[int]. A list of file sizes corresponding to each item in the input list. Each size is an integer representing the number of bytes.\n    \"\"\"\n    item_sizes = []\n\n    cpu_count = os.cpu_count() or 1\n\n    # Parallelize to accelerate retrieving the number of file bytes to read for each item\n    with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count * 2 if cpu_count > 4 else cpu_count) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    \"\"\"\n    Computes and returns a list of file sizes for each item in the given list by parallelizing the file size retrieval process to improve performance. It uses a ThreadPoolExecutor to execute the file size retrieval in parallel, adjusting the number of workers based on the CPU count.\n\n    Input-Output Arg"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    item_sizes = []\n\n    cpu_count = os.cpu_count() or 1\n\n    # Parallelize to accelerate retrieving the number of file bytes to read for each item\n    with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count * 2 if cpu_count > 4 else cpu_count) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    \"\"\"\n    Computes and returns a list of file sizes for each item in the given list by parallelizing the file size retrieval process to improve performance. It uses a ThreadPoolExecutor to execute the file size retrieval in parallel, adjusting the number of workers based on the CPU count.\n\n    Input-Output Arguments\n    :param items: List[Any]. A list of items for which to compute file sizes. The nature of these items is not specified, but they are used to determine file sizes in some way.\n    :param base_path: str, optional. A base path to be prepended to each item's path before computing its file size. Defaults to an empty string, indicating no base path is used.\n    :return: List[int]. A list of file sizes corresponding to each item in the input list. Each size is an integer representing the number of bytes.\n    \"\"\"\n    item_sizes = []\n\n    cpu_count = os.cpu_count() or 1\n\n    # Parallelize to accelerate retrieving the number of file bytes to read for each item\n    with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count * 2 if cpu_count > 4 else cpu_count) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    for future in futures:\n        item_sizes.append(future.result())\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [f.result() for f in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    for future in concurrent.futures.as_completed(futures):\n        item_sizes.append(future.result())\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [f.result() for f in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    for future in concurrent.futures.as_completed(futures):\n        item_sizes.append(future.result())\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [f.result() for f in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [f.result() for f in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [f.result() for f in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    for future in concurrent.futures.as_completed(futures):\n        item_sizes.append(future.result())\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [f.result() for f in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    for future in concurrent.futures.as_completed(futures):\n        item_sizes.append(future.result())\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    for future in concurrent.futures.as_completed(futures):\n        item_sizes.append(future.result())\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    # Parallelize to accelerate retrieving the number of file bytes to read for each item\n    with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count * 2 if cpu_count > 4 else cpu_count) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    # Retrieve the results from the futures\n    item_sizes = [future.result() for future in futures]\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [f.result() for f in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [f.result() for f in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    for future in concurrent.futures.as_completed(futures):\n        item_sizes.append(future.result())\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [f.result() for f in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is not None and element.startswith(input_dir):\n        return True\n\n    if _IS_IN_STUDIO and element.startswith(\"/teamspace/studios/this_studio\"):\n        return True\n\n    if os.path.exists(element):\n        return True\n\n    return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is not None and element.startswith(input_dir):\n        return True\n\n    if _IS_IN_STUDIO and element.startswith(\"/teamspace/studios/this_studio\"):\n        return True\n\n    if os.path.exists(element):\n        return True\n\n    return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is not None and element.startswith(input_dir):\n        return True\n\n    if _IS_IN_STUDIO and element.startswith(\"/teamspace/studios/this_studio\"):\n        return True\n\n    if os.path.exists(element):\n        return True\n\n    return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if element.startswith(input_dir):\n        return True\n\n    element = _to_path(element)\n\n    return os.path.exists(element)"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if element.startswith(input_dir):\n        return True\n\n    element = _to_path(element)\n\n    if os.path.exists(element):\n        return True\n\n    return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if element.startswith(input_dir):\n        return True\n\n    if _IS_IN_STUDIO and element.startswith(\"/teamspace/studios/this_studio\"):\n        return True\n\n    if os.path.exists(element):\n        return True\n\n    return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is not None and element.startswith(input_dir):\n        return True\n\n    if _IS_IN_STUDIO and element.startswith(\"/teamspace/studios/this_studio\"):\n        return True\n\n    return os.path.exists(element)"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is not None:\n        return element.startswith(input_dir)\n\n    return os.path.exists(element)\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if os.path.exists(element):\n        return True\n\n    if input_dir is None:\n        return False\n\n    return element.startswith(input_dir)\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir and element.startswith(input_dir):\n        return True\n\n    if _IS_IN_STUDIO and element.startswith(\"/teamspace\"):\n        return True\n\n    return os.path.exists(element)\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir and element.startswith(input_dir):\n        return True\n\n    element = _to_path(element)\n\n    if os.path.exists(element):\n        return True\n\n    return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if element.startswith(input_dir):\n        return True\n\n    if _IS_IN_STUDIO and element.startswith(\"/teamspace\"):\n        return True\n\n    if os.path.exists(element):\n        return True\n\n    return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is not None and element.startswith(input_dir):\n        return True\n\n    if input_dir is not None:\n        element = _to_path(element)\n\n    return os.path.exists(element)\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if element.startswith(input_dir):\n        return True\n\n    if input_dir is not None:\n        element = str(Path(element).absolute())\n\n    return os.path.exists(element)\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if element.startswith(input_dir):\n        return True\n    if not os.path.exists(element):\n        return False\n    return True"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir and element.startswith(input_dir):\n        return True\n\n    if os.path.exists(element):\n        return True\n\n    if os.path.exists(os.path.abspath(element)):\n        return True\n\n    return False"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is not None and element.startswith(input_dir):\n        return True\n\n    if input_dir is not None and element.startswith(\"/teamspace/studios/this_studio\"):\n        return True\n\n    if os.path.exists(element):\n        return True\n\n    if os.path.exists(os.path.join(input_dir, element)):\n        return True\n\n    return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is not None and element.startswith(input_dir):\n        return True\n\n    if _IS_IN_STUDIO:\n        if element.startswith(\"/teamspace\"):\n            return True\n\n        element = str(Path(element).absolute())\n\n    return os.path.exists(element)\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if element.startswith(input_dir):\n        return True\n\n    if _IS_IN_STUDIO:\n        if element.startswith(\"/teamspace\"):\n            return True\n\n    if os.path.exists(element):\n        return True\n\n    return False\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if input_dir is not None and element.startswith(input_dir):\n        return True\n\n    if input_dir is not None and element.startswith(\"/teamspace/studios/this_studio\"):\n        return True\n\n    if _IS_IN_STUDIO and input_dir is not None:\n        element = str(Path(element).absolute())\n\n    return os.path.exists(element)\n\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons < 16:\n                return self.get_tinycudann_network(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    n_layers=n_layers,\n                    n_neurons=n_neurons,\n                    activation=activation,\n                    output_activation=output_activation,\n                )\n            else:\n                return self.get_tinycudann_network(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    n_layers=n_layers,\n                    n_neurons=n_neurons,\n                    activation=activation,\n                    output_activation=output_activation,\n                    n_hidden_layers=8,\n                )\n        else:\n            return self.get_torch_network(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                n_layers=n_layers,\n                n_neurons=n_neurons,\n                activation=activation,\n                output_activation=output_activation,\n            )\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            import tinycudann as tcnn\n\n            if n_neurons < 16:\n                network_type = tcnn.NetworkWithInputEncoding\n            elif n_neurons < 64:\n                network_type = tcnn.NetworkWithInputSkips\n            else:\n                network_type = tcnn.Network\n\n            return network_type(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                encoding_config={\n                    \"otype\": \"Frequency\",\n                    \"n_frequencies\": 10,\n                },\n                n_layers=n_layers,\n                n_neurons=n_neurons,\n                activation=activation,\n                output_activation=output_activation,\n                seed=self._get_seed(),\n            )\n        else:\n            model_list = []\n            for i in range(n_layers):\n                model_list += self._get_torch_layer(\n                    n_input_dims if i == 0 else n_neurons,\n                    n_output_dims if i == n_layers - 1 else n_neurons,\n                    activation if i < n_layers - 1 else output_activation,\n                )\n            return nn.Sequential(*model_list)\n\n    "}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons < 16:\n                return self._get_tcnn_network_tiny(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    n_layers=n_layers,\n                    n_neurons=n_neurons,\n                    activation=activation,\n                    output_activation=output_activation,\n                )\n            else:\n                return self._get_tcnn_network_large(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    n_layers=n_layers,\n                    n_neurons=n_neurons,\n                    activation=activation,\n                    output_activation=output_activation,\n                )\n        else:\n            return self._get_torch_network(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                n_layers=n_layers,\n                n_neurons=n_neurons,\n                activation=activation,\n                output_activation=output_activation,\n            )\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons < 16:\n                network = tcnn.Network(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    network_config={\n                        \"otype\": \"FullyFusedMLP\",\n                        \"activation\": activation,\n                        \"output_activation\": output_activation,\n                        \"n_neurons\": n_neurons,\n                        \"n_hidden_layers\": n_layers - 1,\n                    },\n                )\n            else:\n                network = tcnn.Network(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    network_config={\n                        \"otype\": \"CutlassMLP\",\n                        \"activation\": activation,\n                        \"output_activation\": output_activation,\n                        \"n_neurons\": n_neurons,\n                        \"n_hidden_layers\": n_layers - 1,\n                    },\n                )\n            return network\n\n        layers = []\n        for i in range(n_layers - 1):\n            layers += self._get_torch_layer(\n                in_features=n_input_dims,\n                out_features=n_neurons,\n                activation_name=activation,\n            )\n            n_input_dims = n_neurons\n        layers += self._get_torch_layer(\n            in_features=n_input_dims,\n            out_features=n_output_dims,\n            activation_name=output_activation,\n        )\n        return nn.Sequential(*layers)"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"n_layers must be greater than 0\"\n        assert n_neurons > 0, \"n_neurons must be greater than 0\"\n\n        if self.tcnn:\n            import tinycudann as tcnn\n\n            network_fn = tcnn.Network(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                encoding=tcnn.Encoding(\n                    n_levels=16,\n                    n_features_per_level=2,\n                    base_resolution=16,\n                    log2_hashmap_size=19,\n                ),\n                network_config={\n                    \"otype\": \"FullyFusedMLP\",\n                    \"activation\": activation,\n                    \"output_activation\": output_activation,\n                    \"n_neurons\": n_neurons,\n                    \"n_hidden_layers\": n_layers - 1,\n                },\n            )\n        else:\n            model_list = []\n            for i in range(n_layers - 1):\n                model_list += self._get_torch_layer(\n                    in_features=n_input_dims if i == 0 else n_neurons,\n                    out_features=n_neurons,\n                    activation_name=activation,\n                )\n            model_list += self._get_torch_layer(\n                in_features=n_neurons if n_layers > 1 else n_input_dims,\n                out_features=n_output_dims,\n                activation_name=output_activation,\n            )\n            network_fn = nn.Sequential(*model_list)\n\n        return network_fn"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            import tinycudann as tcnn\n\n            if n_neurons < 16:\n                network_type = \"fully_fused_mlp\"\n            else:\n                network_type = \"cutlass\"\n\n            return tcnn.Network(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                network_config={\n                    \"otype\": network_type,\n                    \"activation\": activation,\n                    \"output_activation\": output_activation,\n                    \"n_neurons\": n_neurons,\n                    \"n_hidden_layers\": n_layers - 1,\n                },\n                seed=self._get_seed(),\n            )\n\n        # pytorch\n        layers = []\n        for i in range(n_layers - 1):\n            layers.extend(\n                self._get_torch_layer(\n                    in_features=n_input_dims if i == 0 else n_neurons,\n                    out_features=n_neurons,\n                    activation_name=activation,\n                )\n            )\n\n        layers.extend(\n            self._get_torch_layer(\n                in_features=n_input_dims if n_layers == 1 else n_neurons,\n                out_features=n_output_dims,\n                activation_name=output_activation,\n            )\n        )\n\n        return nn.Sequential(*layers)\n\n    "}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            import tinycudann as tcnn\n\n            network_fn = tcnn.NetworkWithInputEncoding(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                encoding_config={\n                    \"otype\": \"HashGrid\",\n                    \"n_levels\": 16,\n                    \"n_features_per_level\": 2,\n                    \"log2_hashmap_size\": 19,\n                    \"base_resolution\": 16,\n                    \"per_level_scale\": 1.3819,\n                },\n                network_config={\n                    \"otype\": \"FullyFusedMLP\",\n                    \"activation\": activation,\n                    \"output_activation\": output_activation,\n                    \"n_neurons\": n_neurons,\n                    \"n_hidden_layers\": n_layers - 1,\n                },\n            )\n            return network_fn\n        else:\n            model = []\n            for i in range(n_layers - 1):\n                model.extend(\n                    self._get_torch_layer(\n                        n_input_dims if i == 0 else n_neurons,\n                        n_neurons,\n                        activation,\n                    )\n                )\n            model.extend(\n                self._get_torch_layer(\n                    n_input_dims if n_layers == 1 else n_neurons,\n                    n_output_dims,\n                    output_activation,\n                )\n            )\n            return nn.Sequential(*model)\n\n    "}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            import tinycudann as tcnn\n\n            network = tcnn.Network(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                encoding=tcnn.Encoding(\n                    n_levels=12,\n                    n_features_per_level=2,\n                    base_resolution=16,\n                    log2_hashmap_size=19,\n                ),\n                network_config={\n                    \"otype\": \"FullyFusedMLP\",\n                    \"activation\": activation,\n                    \"output_activation\": output_activation,\n                    \"n_neurons\": n_neurons,\n                    \"n_hidden_layers\": n_layers,\n                },\n            )\n            return network\n\n        # pytorch\n        layers = []\n        for i in range(n_layers):\n            if i == 0:\n                layers.extend(self._get_torch_layer(n_input_dims, n_neurons, activation))\n            elif i == n_layers - 1:\n                layers.extend(\n                    self._get_torch_layer(n_neurons, n_output_dims, output_activation)\n                )\n            else:\n                layers.extend(self._get_torch_layer(n_neurons, n_neurons, activation))\n\n        return nn.Sequential(*layers)"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            import tinycudann as tcnn\n            network = tcnn.Network(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                encoding=tcnn.Encoding(\n                    n_levels=12,\n                    n_features_per_level=2,\n                    base_resolution=16,\n                    log2_hashmap_size=19,\n                    desired_resolution=2048,\n                    per_level_scale=1.4142135623730951,\n                ),\n                network_config={\n                    \"otype\": \"FullyFusedMLP\",\n                    \"activation\": activation,\n                    \"output_activation\": output_activation,\n                    \"n_neurons\": n_neurons,\n                    \"n_hidden_layers\": n_layers - 1,\n                },\n            )\n            return network\n\n        model = []\n        for i in range(n_layers - 1):\n            model += self._get_torch_layer(n_input_dims, n_neurons, activation)\n            n_input_dims = n_neurons\n        model += self._get_torch_layer(n_input_dims, n_output_dims, output_activation)\n        return nn.Sequential(*model)\n\n    "}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            import tinycudann as tcnn\n\n            if n_neurons <= 128:\n                network_type = tcnn.NetworkWithInputEncoding\n            elif n_neurons <= 256:\n                network_type = tcnn.NetworkWithInputSkips\n            elif n_neurons <= 512:\n                network_type = tcnn.NetworkWithInputEncoding\n            else:\n                network_type = tcnn.Network\n\n            return network_type(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                encoding_config={\n                    \"otype\": \"HashGrid\",\n                    \"n_levels\": 16,\n                    \"n_features_per_level\": 2,\n                    \"log2_hashmap_size\": 19,\n                    \"base_resolution\": 16,\n                    \"per_level_scale\": 1.3819,\n                },\n                network_config={\n                    \"otype\": \"FullyFusedMLP\",\n                    \"activation\": activation,\n                    \"output_activation\": output_activation,\n                    \"n_neurons\": n_neurons,\n                    \"n_hidden_layers\": n_layers - 1,\n                },\n                seed=self._get_seed(),\n            )\n\n        # PyTorch\n        model_list = []\n        model_list.extend(self._get_torch_layer(\n            n_input_dims, n_neurons, activation))\n        for i in range(n_layers - 2):\n            model_list.extend(self._get_torch_layer(\n                n_neurons, n_neurons, activation))\n        model_list.extend(self._get_torch_layer(\n            n_neurons, n_output_dims, output_activation))\n        return nn.Sequential(*model_"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"need at least one layer\"\n        assert n_neurons > 0, \"need at least one neuron\"\n\n        if self.tcnn:\n            return self._get_tcnn_network(\n                n_input_dims,\n                n_output_dims,\n                n_layers,\n                n_neurons,\n                activation,\n                output_activation,\n            )\n        else:\n            return self._get_torch_network(\n                n_input_dims,\n                n_output_dims,\n                n_layers,\n                n_neurons,\n                activation,\n                output_activation,\n            )\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons < 16:\n                return self.get_network_with_skip_layers(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    n_layers=n_layers,\n                    n_neurons=n_neurons,\n                    activation=activation,\n                    output_activation=output_activation,\n                    skips=[n_layers],\n                )\n            else:\n                return self.get_network_with_skip_layers(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    n_layers=n_layers,\n                    n_neurons=n_neurons,\n                    activation=activation,\n                    output_activation=output_activation,\n                    skips=[n_layers // 2, n_layers],\n                )\n\n        model = nn.Sequential()\n        for i in range(n_layers):\n            if i == 0:\n                layer_list = self._get_torch_layer(\n                    in_features=n_input_dims,\n                    out_features=n_neurons,\n                    activation_name=activation,\n                )\n            elif i == n_layers - 1:\n                layer_list = self._get_torch_layer(\n                    in_features=n_neurons,\n                    out_features=n_output_dims,\n                    activation_name=output_activation,\n                )\n            else:\n                layer_list = self._get_torch_layer(\n                    in_features=n_neurons,\n                    out_features=n_neurons,\n                    activation_name=activation,\n                )\n            for layer in layer_list:\n                model.add_module(str(self._get_seed()), layer)\n        return model"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons < 16:\n                network_type = \"fully_fused_mlp\"\n            else:\n                network_type = \"cutlass\"\n            return nn.CompiledSubnet(\n                n_input_dims,\n                n_output_dims,\n                n_layers,\n                n_neurons,\n                network_type,\n                seed=self._get_seed(),\n            )\n\n        model_list = []\n        for i in range(n_layers):\n            is_last_layer = i == n_layers - 1\n            activation_name = output_activation if is_last_layer else activation\n            model_list += self._get_torch_layer(\n                in_features=n_input_dims if i == 0 else n_neurons,\n                out_features=n_output_dims if is_last_layer else n_neurons,\n                activation_name=activation_name,\n            )\n        return nn.Sequential(*model_list)"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons <= 16:\n                network_type = \"FullyFusedMLP\"\n            elif n_neurons <= 64:\n                network_type = \"CutlassMLP\"\n            elif n_neurons <= 256:\n                network_type = \"TensorCoresMLP\"\n            else:\n                network_type = \"FullyFusedMLP\"\n\n            network = getattr(tcnn, network_type)(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                n_layers=n_layers,\n                n_neurons=n_neurons,\n                activation=activation,\n                output_activation=output_activation,\n                seed=self._get_seed(),\n            )\n        else:\n            layers = []\n            for i in range(n_layers):\n                if i == 0:\n                    layers += self._get_torch_layer(\n                        n_input_dims, n_neurons, activation\n                    )\n                elif i == n_layers - 1:\n                    layers += self._get_torch_layer(\n                        n_neurons, n_output_dims, output_activation\n                    )\n                else:\n                    layers += self._get_torch_layer(\n                        n_neurons, n_neurons, activation\n                    )\n\n            network = nn.Sequential(*layers)\n\n        return network\n\n    "}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            import tinycudann as tcnn\n\n            if n_neurons <= 128:\n                network_type = tcnn.NetworkWithInputEncoding\n            elif n_neurons <= 256:\n                network_type = tcnn.NetworkWithInputSkips\n            elif n_neurons <= 512:\n                network_type = tcnn.NetworkWithInputEncoding\n            elif n_neurons <= 1024:\n                network_type = tcnn.NetworkWithInputSkips\n            else:\n                raise ValueError(\"unsupported number of neurons\")\n\n            return network_type(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                encoding_config={\n                    \"otype\": \"Frequency\",\n                    \"n_frequencies\": 10,\n                },\n                network_config={\n                    \"otype\": \"FullyFusedMLP\",\n                    \"activation\": activation,\n                    \"output_activation\": output_activation,\n                    \"n_neurons\": n_neurons,\n                    \"n_hidden_layers\": n_layers - 1,\n                },\n            ).to(self._get_seed())\n\n        # pytorch\n        model_list = []\n        for i in range(n_layers - 1):\n            model_list += self._get_torch_layer(n_input_dims, n_neurons, activation)\n            n_input_dims = n_neurons\n        model_list += self._get_torch_layer(n_input_dims, n_output_dims, output_activation)\n        return nn.Sequential(*model_list)\n\n    "}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            import tinycudann as tcnn\n\n            network_fn = tcnn.NetworkWithInputEncoding(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                encoding_config={\n                    \"otype\": \"HashGrid\" if n_neurons > 16384 else \"FrequencyGrid\",\n                    \"n_levels\": 16 if n_neurons > 16384 else 12,\n                    \"n_features_per_level\": 2,\n                    \"log2_hashmap_size\": 19 if n_neurons > 16384 else 15,\n                    \"base_resolution\": 16 if n_neurons > 16384 else 8,\n                    \"per_level_scale\": 1.3819,\n                },\n                network_config={\n                    \"otype\": \"CutlassMLP\"\n                    if n_neurons > 16384\n                    else \"FullyFusedMLP\",\n                    \"activation\": activation,\n                    \"output_activation\": output_activation,\n                    \"n_neurons\": n_neurons,\n                    \"n_hidden_layers\": n_layers - 1,\n                },\n            )\n            return network_fn\n\n        model_list = []\n        for i in range(n_layers - 1):\n            model_list.extend(\n                self._get_torch_layer(\n                    in_features=n_input_dims if i == 0 else n_neurons,\n                    out_features=n_neurons,\n                    activation_name=activation,\n                )\n            )\n        model_list.extend(\n            self._get_torch_layer(\n                in_features=n_input_dims if n_layers == 1 else n_neurons,\n                out_features=n_output_dims,\n                activation"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            import tinycudann as tcnn\n\n            # create the network\n            network = tcnn.Network(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                encoding=tcnn.Encoding(\n                    n_levels=16,\n                    n_features_per_level=2,\n                    base_resolution=16,\n                    log2_hashmap_size=19,\n                ),\n                network_config={\n                    \"otype\": \"FullyFusedMLP\",\n                    \"activation\": activation,\n                    \"output_activation\": output_activation,\n                    \"n_neurons\": n_neurons,\n                    \"n_hidden_layers\": n_layers - 1,\n                },\n            )\n\n            # set the seed for the network\n            torch.manual_seed(self._get_seed())\n            return network\n\n        # create the network\n        model_list = []\n        model_list.extend(self._get_torch_layer(n_input_dims, n_neurons, activation))\n        for i in range(1, n_layers - 1):\n            model_list.extend(\n                self._get_torch_layer(n_neurons, n_neurons, activation)\n            )\n        model_list.extend(\n            self._get_torch_layer(n_neurons, n_output_dims, output_activation)\n        )\n        model = nn.Sequential(*model_list)\n\n        # set the seed for the network\n        torch.manual_seed(self._get_seed())\n        return model"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            import tinycudann as tcnn\n\n            network_type = \"tcnn\"\n            if n_neurons < 16:\n                network_type = \"fully_fused_mlp\"\n            elif n_neurons < 64:\n                network_type = \"mlp\"\n\n            return tcnn.Network(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                network_config={\n                    \"otype\": network_type,\n                    \"activation\": activation,\n                    \"output_activation\": output_activation,\n                    \"n_neurons\": n_neurons,\n                    \"n_hidden_layers\": n_layers - 1,\n                },\n                seed=self._get_seed(),\n            )\n\n        model = []\n        model += self._get_torch_layer(n_input_dims, n_neurons, activation)\n        for i in range(n_layers - 2):\n            model += self._get_torch_layer(n_neurons, n_neurons, activation)\n        model += self._get_torch_layer(n_neurons, n_output_dims, output_activation)\n        model = nn.Sequential(*model)\n        return model\n\n    "}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            # TODO: fix this\n            # import tinycudann as tcnn\n            # network_fn = tcnn.NetworkWithInputEncoding(\n            #     n_input_dims=n_input_dims,\n            #     n_output_dims=n_output_dims,\n            #     encoding_config={\n            #         \"otype\": \"Frequency\",\n            #         \"n_frequencies\": 10,\n            #         \"n_levels\": 16,\n            #         \"base_resolution\": 16,\n            #         \"log2_hashmap_size\": 19,\n            #     },\n            #     network_config={\n            #         \"otype\": \"FullyFusedMLP\",\n            #         \"activation\": activation,\n            #         \"output_activation\": output_activation,\n            #         \"n_neurons\": n_neurons,\n            #         \"n_hidden_layers\": n_layers - 1,\n            #     },\n            # )\n            # return network_fn\n            raise NotImplementedError\n        else:\n            model_list = []\n            for i in range(n_layers - 1):\n                model_list += self._get_torch_layer(n_input_dims, n_neurons, activation)\n                n_input_dims = n_neurons\n            model_list += self._get_torch_layer(n_input_dims, n_output_dims, output_activation)\n            return nn.Sequential(*model_list)"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            import tinycudann as tcnn\n\n            if n_neurons <= 128:\n                network_type = tcnn.NetworkWithInputEncoding\n            elif n_neurons <= 256:\n                network_type = tcnn.NetworkWithInputSkips\n            elif n_neurons <= 512:\n                network_type = tcnn.NetworkWithInputEncodingAndSkips\n            else:\n                raise ValueError(f\"Unsupported number of neurons: {n_neurons}\")\n\n            return network_type(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                encoding_config={\n                    \"otype\": \"Frequency\",\n                    \"n_frequencies\": 10,\n                },\n                n_hidden_dims=n_neurons,\n                n_hidden_layers=n_layers - 1,\n                seed=self._get_seed(),\n            )\n        else:\n            model = nn.Sequential()\n            model_list = []\n\n            model_list.extend(self._get_torch_layer(n_input_dims, n_neurons, activation))\n            for i in range(n_layers - 2):\n                model_list.extend(self._get_torch_layer(n_neurons, n_neurons, activation))\n            model_list.extend(self._get_torch_layer(n_neurons, n_output_dims, output_activation))\n\n            model = nn.Sequential(*model_list)\n            return model"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        shifted_signals = [np.roll(signal, i) for i in range(-kernel_offset, kernel_offset + 1)]\n        median_array = np.median(shifted_signals, axis=0)\n        return median_array[kernel_offset:-kernel_offset]"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        shifted_signals = [np.roll(signal, i) for i in range(-kernel_offset, kernel_offset + 1)]\n        median_signals = np.median(shifted_signals, axis=0)\n        return median_signals[kernel_offset:-kernel_offset]"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Generate shifted versions of the signal\n        shifted_signals = [np.roll(signal, i) for i in range(-kernel_offset, kernel_offset + 1)]\n\n        # Compute the median of the shifted signals\n        median_signal = np.median(shifted_signals, axis=0)\n\n        # Trim the median signal to remove edge effects\n        trimmed_median_signal = median_signal[kernel_offset:-kernel_offset]\n\n        return trimmed_median_signal"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Shift the signal by a range defined by the kernel offset\n        shifted_signals = np.array([np.roll(signal, i) for i in range(-kernel_offset, kernel_offset + 1)])\n\n        # Compute the median of the shifted signals\n        median_signal = np.median(shifted_signals, axis=0)\n\n        # Trim the median array to remove edge effects\n        trimmed_median = median_signal[kernel_offset:-kernel_offset]\n\n        return trimmed_median"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        shifted_signals = [np.roll(signal, shift) for shift in range(-kernel_offset, kernel_offset + 1)]\n        median_signal = np.median(shifted_signals, axis=0)\n        return median_signal[kernel_offset:-kernel_offset]"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        shifted_signals = [np.roll(signal, i) for i in range(-kernel_offset, kernel_offset + 1)]\n        median_signal = np.median(shifted_signals, axis=0)\n        return median_signal[kernel_offset:-kernel_offset]"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        kernel_size = 2 * kernel_offset + 1\n        shifted_signals = [np.roll(signal, i) for i in range(-kernel_offset, kernel_offset + 1)]\n        median_array = np.median(shifted_signals, axis=0)\n        return median_array[kernel_offset:-kernel_offset]"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        shifted_signals = [np.roll(signal, shift) for shift in range(-kernel_offset, kernel_offset + 1)]\n        median_array = np.median(shifted_signals, axis=0)\n        trimmed_median_array = median_array[kernel_offset:-kernel_offset]\n        return trimmed_median_array"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        offset_signal = np.array([np.roll(signal, i) for i in range(-kernel_offset, kernel_offset + 1)])\n        median_signal = np.median(offset_signal, axis=0)\n        return median_signal[kernel_offset:-kernel_offset]"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        shifted_signals = np.vstack([np.roll(signal, offset) for offset in range(-kernel_offset, kernel_offset + 1)])\n        median_array = np.median(shifted_signals, axis=0)\n\n        return median_array[kernel_offset:-kernel_offset]"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        shifted_signals = [np.roll(signal, offset) for offset in range(-kernel_offset, kernel_offset + 1)]\n        median_signals = np.median(shifted_signals, axis=0)\n\n        return median_signals[kernel_offset:-kernel_offset]"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Shift the signal by a range defined by the kernel offset\n        shifted_signals = np.array([np.roll(signal, i) for i in range(-kernel_offset, kernel_offset + 1)])\n\n        # Compute the median of the shifted signals\n        median_signal = np.median(shifted_signals, axis=0)\n\n        # Trim the median signal to remove edge effects introduced by the shifting process\n        trimmed_median_signal = median_signal[kernel_offset:-kernel_offset]\n\n        return trimmed_median_signal"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Generate shifted versions of the signal\n        shifted_signals = np.array([np.roll(signal, i) for i in range(-kernel_offset, kernel_offset + 1)])\n\n        # Compute the median of the shifted signals\n        median_signal = np.median(shifted_signals, axis=0)\n\n        # Trim the median array to remove edge effects\n        trimmed_median = median_signal[kernel_offset:-kernel_offset]\n\n        return trimmed_median"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Create a 2D array of shifted signals\n        shifted_signals = np.vstack(\n            [np.roll(signal, i) for i in range(-kernel_offset, kernel_offset + 1)]\n        )\n\n        # Calculate the median of the shifted signals\n        median_signal = np.median(shifted_signals, axis=0)\n\n        # Trim the median signal to remove edge effects\n        trimmed_median_signal = median_signal[kernel_offset:-kernel_offset]\n\n        return trimmed_median_signal"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Shift the signal by a range defined by the kernel offset\n        shifted_signals = [np.roll(signal, shift) for shift in range(-kernel_offset, kernel_offset + 1)]\n\n        # Calculate the median of the shifted signals\n        median_signal = np.median(shifted_signals, axis=0)\n\n        # Trim the median signal to remove edge effects introduced by the shifting process\n        trimmed_median_signal = median_signal[kernel_offset:-kernel_offset]\n\n        return trimmed_median_signal"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Compute the shifted signals\n        shifted_signals = np.array([np.roll(signal, i) for i in range(-kernel_offset, kernel_offset + 1)])\n\n        # Compute the rolling median\n        median = np.median(shifted_signals, axis=0)\n\n        # Trim the median array to remove edge effects\n        trimmed_median = median[kernel_offset:-kernel_offset]\n\n        return trimmed_median"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        signal_length = len(signal)\n        shifted_signals = [np.roll(signal, offset) for offset in range(-kernel_offset, kernel_offset + 1)]\n        median_array = np.median(shifted_signals, axis=0)\n        trimmed_median = median_array[kernel_offset : signal_length - kernel_offset]\n\n        return trimmed_median"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        kernel_size = 2 * kernel_offset + 1\n        median_signal = np.zeros_like(signal)\n\n        for i in range(kernel_size):\n            shifted_signal = np.roll(signal, i - kernel_offset)\n            median_signal += np.median(shifted_signal.reshape(-1, kernel_size), axis=1)\n\n        median_signal /= kernel_size\n\n        return median_signal[kernel_offset:-kernel_offset]"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        if kernel_offset == 0:\n            return signal\n\n        kernel_size = 2 * kernel_offset + 1\n        signal_padded = np.pad(signal, kernel_offset, mode=\"wrap\")\n\n        shifted_signals = [\n            np.roll(signal_padded, i - kernel_offset)[kernel_offset:-kernel_offset] for i in range(kernel_size)\n        ]\n\n        median_array = np.median(shifted_signals, axis=0)\n\n        return median_array"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Create a shifted version of the signal for each offset in the range [-kernel_offset, kernel_offset]\n        # This is done by concatenating the signal with itself, shifted by different amounts\n        # The resulting array will have shape (2 * kernel_offset + 1, len(signal))\n        shifted_signals = np.array([np.roll(signal, i) for i in range(-kernel_offset, kernel_offset + 1)])\n\n        # Compute the median of the shifted signals along the first axis (the offset axis)\n        # This will give us the median value for each offset\n        median_signals = np.median(shifted_signals, axis=0)\n\n        # Trim the median array to remove edge effects introduced by the shifting process\n        # We do this by slicing the array to remove the first and last kernel_offset elements\n        # This effectively removes the edge effects caused by the shifting process\n        trimmed_median = median_signals[kernel_offset:-kernel_offset]\n\n        return trimmed_median"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Calculate Hamming distance\n    if weights:\n        # Calculate weighted Hamming distance\n        irisbits = [\n            np.unpackbits(x, axis=1, bitorder=\"little\", count=template_probe.code_size) for x in template_probe.iriscode\n        ]\n        maskbits = [\n            np.unpackbits(x, axis=1, bitorder=\"little\", count=template_probe.code_size) for x in template_gallery.iriscode\n        ]\n        irisbits = [np.roll(x, rotation_shift, axis=1) for x in irisbits]\n        irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n            irisbits, maskbits, template_probe.half_width, weights\n        )\n    else:\n        # Calculate unweighted Hamming distance\n        irisbits = np.unpackbits(template_probe.iriscode, axis=1, bitorder=\"little\", count=template_probe.code_size)\n        maskbits = np.unpackbits(template_gallery.iriscode, axis=1, bitorder=\"little\", count=template_gallery.code_size)\n        irisbits = np.roll(irisbits, rotation_shift, axis=1)\n        irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n            irisbits, maskbits, template_probe.half_width\n        )\n\n    # Calculate normalized Hamming distance\n    if nm_dist:\n        sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n            template_probe.code_size, template_probe.half_width, weights\n        )\n        norm_HD_top = normalized_HD(irisbitcount_top, maskbitcount_top, sqrt_totalbitcount_top, nm_dist)\n        norm_HD_bot = normalized"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if template_probe.code_size != template_gallery.code_size:\n        raise MatcherError(\"Both templates must have the same code size\")\n    if template_probe.code_size % 2 != 0:\n        raise MatcherError(\"Both templates must have even code size\")\n\n    half_width = [template_probe.code_size // 2] * template_probe.iris_count\n    toal_codesize = template_probe.code_size * template_probe.iris_count\n\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        toal_codesize, half_width, weights\n    )\n\n    irisbits = template_probe.irisbits\n    maskbits = template_gallery.irisbits\n\n    irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n        irisbits, maskbits, half_width, weights\n    )\n\n    if nm_dist:\n        norm_HD_top = normalized_HD(irisbitcount_top, maskbitcount_top, sqrt_totalbitcount_top, nm_dist)\n        norm_HD_bot = normalized_HD(irisbitcount_bot, maskbitcount_bot, sqrt_totalbitcount_bot, nm_dist)\n        norm_HD = norm_HD_top + norm_HD_bot\n    else:\n        norm_HD = None\n\n    if rotation_shift == 0:\n        return norm_HD, 0\n\n    irisbits_rotated = np.roll(irisbits, rotation_shift, axis=2)\n    irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n        irisbits_rotated, maskbits, half_width, weights\n    )\n\n    if nm_dist:\n        norm_HD_top = normalized_HD(irisbitcount_top, maskbitcount_top, sqrt_"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if len(template_probe.iriscode) != len(template_gallery.iriscode):\n        raise MatcherError(\"Iris codes must be of the same length\")\n\n    toal_codesize = len(template_probe.iriscode[0])\n    half_width = [int(toal_codesize / 2)] * len(template_probe.iriscode)\n\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        toal_codesize, half_width, weights\n    )\n\n    min_hd = np.inf\n    min_hd_shift = 0\n    for shift in range(-rotation_shift, rotation_shift + 1):\n        irisbits = np.roll(template_probe.iriscode, shift, axis=1)\n        maskbits = template_gallery.iriscode\n\n        irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n            irisbits, maskbits, half_width, weights\n        )\n\n        if nm_dist:\n            norm_HD_top = normalized_HD(irisbitcount_top, maskbitcount_top, sqrt_totalbitcount_top, nm_dist)\n            norm_HD_bot = normalized_HD(irisbitcount_bot, maskbitcount_bot, sqrt_totalbitcount_bot, nm_dist)\n\n            if norm_HD_top + norm_HD_bot < min_hd:\n                min_hd = norm_HD_top + norm_HD_bot\n                min_hd_shift = shift\n        else:\n            HD_top = irisbitcount_top / maskbitcount_top\n            HD_bot = irisbitcount_bot / maskbitcount_bot\n\n            if HD_top + HD_bot < min_hd:\n                min_hd = HD_top + HD_bot\n                min_hd_shift = shift\n\n    return min_hd, min_hd_shift"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    half_width = [int(template_probe.code.shape[1] / 2), int(template_gallery.code.shape[1] / 2)]\n    toal_codesize = np.sum(template_probe.code.shape)\n\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        toal_codesize, half_width, weights\n    )\n\n    if rotation_shift == 0:\n        irisbits = np.unpackbits(template_probe.code, axis=1, bitorder=\"little\", count=template_probe.code.shape[1])\n        maskbits = np.unpackbits(template_gallery.code, axis=1, bitorder=\"little\", count=template_gallery.code.shape[1])\n\n        irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n            irisbits, maskbits, half_width, weights\n        )\n\n        if weights:\n            irisbitcount = irisbitcount_top + irisbitcount_bot\n            maskbitcount = maskbitcount_top + maskbitcount_bot\n        else:\n            irisbitcount = irisbitcount_top + irisbitcount_bot\n            maskbitcount = maskbitcount_top + maskbitcount_bot\n\n        if nm_dist:\n            hd = normalized_HD(irisbitcount, maskbitcount, sqrt_totalbitcount, nm_dist)\n        else:\n            hd = (irisbitcount - maskbitcount) / sqrt_totalbitcount\n\n        return hd, 0\n\n    else:\n        irisbits = np.unpackbits(template_probe.code, axis=1, bitorder=\"little\", count=template_probe.code.shape[1])\n        maskbits = np.unpackbits(template_gallery.code, axis=1, bitorder=\"little\", count=template_gallery.code.shape"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if template_probe.code_size != template_gallery.code_size:\n        raise MatcherError(\"Template sizes do not match.\")\n\n    if template_probe.code_size == 0:\n        raise MatcherError(\"Template size is zero.\")\n\n    if rotation_shift < 0:\n        raise MatcherError(\"Rotation shift is negative.\")\n\n    if rotation_shift > template_probe.code_size:\n        raise MatcherError(\"Rotation shift is larger than template size.\")\n\n    toal_codesize = template_probe.code_size * 3 / 4\n    half_width = [int(template_probe.code_size / 2), int(template_probe.code_size / 2)]\n    irisbits = [\n        np.unpackbits(template_probe.iriscode, axis=1, bitorder=\"little\", count=template_probe.code_size),\n        np.unpackbits(template_gallery.iriscode, axis=1, bitorder=\"little\", count=template_gallery.code_size),\n    ]\n    maskbits = [\n        np.unpackbits(template_probe.maskcode, axis=1, bitorder=\"little\", count=template_probe.code_size),\n        np.unpackbits(template_gallery.maskcode, axis=1, bitorder=\"little\", count=template_gallery.code_size),\n    ]\n\n    irisbits[0] = np.roll(irisbits[0], rotation_shift, axis=1)\n    maskbits[0] = np.roll(maskbits[0], rotation_shift, axis=1)\n\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        toal_codesize, half_width, weights\n    )\n\n    irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n        irisbits, maskbits, half_width, weights\n    )\n\n   "}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Check if the input templates are of the correct type\n    if not isinstance(template_probe, IrisTemplate) or not isinstance(template_gallery, IrisTemplate):\n        raise MatcherError(\"The input templates must be of type IrisTemplate\")\n\n    # Check if the rotation shift is an integer\n    if not isinstance(rotation_shift, int):\n        raise MatcherError(\"The rotation shift must be an integer\")\n\n    # Check if the nm_dist and weights parameters are of the correct type\n    if nm_dist is not None and not isinstance(nm_dist, float):\n        raise MatcherError(\"The nm_dist parameter must be a float\")\n    if weights is not None and not isinstance(weights, list):\n        raise MatcherError(\"The weights parameter must be a list\")\n\n    # Check if the weights parameter is a list of numpy arrays\n    if weights is not None:\n        for weight in weights:\n            if not isinstance(weight, np.ndarray):\n                raise MatcherError(\"The weights parameter must be a list of numpy arrays\")\n\n    # Check if the nm_dist and weights parameters are compatible with the Hamming distance calculation\n    if nm_dist is not None and weights is not None:\n        raise MatcherError(\"The nm_dist and weights parameters are not compatible with the Hamming distance calculation\")\n\n    # Check if the rotation shift is within the allowed range\n    if rotation_shift < 0 or rotation_shift > 1:\n        raise MatcherError(\"The rotation shift must be between 0 and 1\")\n\n    # Check if the input templates have the same number of iris codes\n    if len(template_probe.iris_codes) != len(template_gallery.iris_codes):\n        raise MatcherError(\"The input templates must have the same number of iris codes\")\n\n    # Check if the input templates have the same number of mask codes\n    if len(template_probe.mask_codes) != len(template_gallery.mask_codes):\n        raise MatcherError(\"The input templates must have the same number of mask codes\")\n\n    # Check if the input templates have"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if len(template_probe.iriscode) != len(template_gallery.iriscode):\n        raise MatcherError(\"The number of iriscodes in the probe and gallery do not match.\")\n\n    if weights:\n        if len(weights) != len(template_probe.iriscode):\n            raise MatcherError(\"The number of weights and iriscodes do not match.\")\n\n    toal_codesize = sum([iriscode.shape[1] for iriscode in template_probe.iriscode])\n    half_width = [iriscode.shape[1] // 2 for iriscode in template_probe.iriscode]\n\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        toal_codesize, half_width, weights\n    )\n\n    min_HD = float(\"inf\")\n    min_HD_rotation_shift = 0\n    for rotation_shift in range(rotation_shift):\n        irisbits = [\n            np.roll(iriscode, rotation_shift, axis=1) & maskcode\n            for iriscode, maskcode in zip(template_probe.iriscode, template_gallery.iriscode)\n        ]\n        maskbits = [\n            np.roll(maskcode, rotation_shift, axis=1) for maskcode in template_gallery.iriscode\n        ]\n\n        irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n            irisbits, maskbits, half_width, weights\n        )\n\n        if nm_dist:\n            HD_top = normalized_HD(irisbitcount_top, maskbitcount_top, sqrt_totalbitcount_top, nm_dist)\n            HD_bot = normalized_HD(irisbitcount_bot, maskbitcount_bot, sqrt_totalbitcount_bot, nm_dist)\n        else:\n            HD_top = irisbitcount_top / maskbitcount_top\n           "}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if template_probe.iriscode.shape != template_gallery.iriscode.shape:\n        raise MatcherError(\n            f\"IrisTemplate probe and gallery have different shape of iriscodes: {template_probe.iriscode.shape} and {template_gallery.iriscode.shape}\"\n        )\n    if template_probe.maskcode.shape != template_gallery.maskcode.shape:\n        raise MatcherError(\n            f\"IrisTemplate probe and gallery have different shape of maskcodes: {template_probe.maskcode.shape} and {template_gallery.maskcode.shape}\"\n        )\n\n    irisbits_probe = np.unpackbits(template_probe.iriscode, axis=1, bitorder=\"little\")\n    maskbits_probe = np.unpackbits(template_probe.maskcode, axis=1, bitorder=\"little\")\n\n    irisbits_gallery = np.unpackbits(template_gallery.iriscode, axis=1, bitorder=\"little\")\n    maskbits_gallery = np.unpackbits(template_gallery.maskcode, axis=1, bitorder=\"little\")\n\n    toal_codesize = irisbits_probe.shape[1]\n    half_width = [toal_codesize // 2] * irisbits_probe.shape[0]\n\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        toal_codesize, half_width, weights\n    )\n\n    irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n        irisbits_probe, maskbits_gallery, half_width, weights\n    )\n\n    if nm_dist:\n        hd_top = normalized_HD(irisbitcount_top, maskbitcount_top, sqrt_totalbitcount_top, nm_dist)\n        hd_bot = normalized_HD(irisbit"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if len(template_probe.iriscode) != len(template_gallery.iriscode):\n        raise MatcherError(\"template_probe and template_gallery must have the same number of iriscodes.\")\n\n    if weights:\n        if len(template_probe.iriscode) != len(weights):\n            raise MatcherError(\"template_probe and weights must have the same number of iriscodes.\")\n\n    toal_codesize = sum(x.size for x in template_probe.iriscode)\n    half_width = [x.shape[1] // 2 for x in template_probe.iriscode]\n\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        toal_codesize, half_width, weights\n    )\n\n    min_hd = np.inf\n    min_rot = rotation_shift\n\n    for rot in range(rotation_shift + 1):\n        irisbits = [np.roll(x, rot, axis=1) for x in template_probe.iriscode]\n        maskbits = [np.roll(x, rot, axis=1) for x in template_gallery.iriscode]\n\n        irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n            irisbits, maskbits, half_width, weights\n        )\n\n        if nm_dist:\n            norm_HD_top = normalized_HD(irisbitcount_top, maskbitcount_top, sqrt_totalbitcount_top, nm_dist)\n            norm_HD_bot = normalized_HD(irisbitcount_bot, maskbitcount_bot, sqrt_totalbitcount_bot, nm_dist)\n\n            if norm_HD_top < min_hd:\n                min_hd = norm_HD_top\n                min_rot = rot\n            if norm_HD_bot < min_hd:\n                min_hd = norm_HD_bot\n                min_rot = rot\n        else"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if template_probe.n_iris != template_gallery.n_iris:\n        raise MatcherError(\"The number of irises in the probe and gallery templates do not match.\")\n\n    n_iris = template_probe.n_iris\n    half_width = template_probe.half_width\n    toal_codesize = template_probe.total_code_size\n    irisbits_probe = template_probe.irisbits\n    irisbits_gallery = template_gallery.irisbits\n    maskbits_probe = template_probe.maskbits\n    maskbits_gallery = template_gallery.maskbits\n\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        toal_codesize, half_width, weights\n    )\n\n    irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n        irisbits_probe, maskbits_gallery, half_width, weights\n    )\n\n    if nm_dist is None:\n        HD_top = np.sum(np.sum(irisbitcount_top & maskbitcount_top)) / np.sum(np.sum(maskbitcount_top))\n        HD_bot = np.sum(np.sum(irisbitcount_bot & maskbitcount_bot)) / np.sum(np.sum(maskbitcount_bot))\n        HD = (HD_top + HD_bot) / 2\n    else:\n        HD_top = normalized_HD(irisbitcount_top, maskbitcount_top, sqrt_totalbitcount_top, nm_dist)\n        HD_bot = normalized_HD(irisbitcount_bot, maskbitcount_bot, sqrt_totalbitcount_bot, nm_dist)\n        HD = (HD_top + HD_bot) / 2\n\n    for i in range(1, rotation_shift + 1):\n        irisbitcount_top, maskbitcount_top, irisbitcount_bot"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if template_probe.code_size != template_gallery.code_size:\n        raise MatcherError(\"Codes sizes are not equal\")\n\n    toal_codesize = template_probe.code_size\n    half_width = [int(toal_codesize / 2)] * len(template_probe.iriscode)\n\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        toal_codesize, half_width, weights\n    )\n\n    min_HD = np.inf\n    min_rotation_shift = 0\n\n    for rotation_shift_i in range(rotation_shift + 1):\n        irisbits = np.roll(template_probe.iriscode, rotation_shift_i, axis=2)\n        maskbits = np.roll(template_gallery.iriscode, rotation_shift_i, axis=2)\n\n        irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n            irisbits, maskbits, half_width, weights\n        )\n\n        HD_top = irisbitcount_top / maskbitcount_top\n        HD_bot = irisbitcount_bot / maskbitcount_bot\n        HD = (HD_top + HD_bot) / 2\n\n        if nm_dist is not None:\n            HD = normalized_HD(irisbitcount_top + irisbitcount_bot, maskbitcount_top + maskbitcount_bot, sqrt_totalbitcount, nm_dist)\n\n        if HD < min_HD:\n            min_HD = HD\n            min_rotation_shift = rotation_shift_i\n\n    return min_HD, min_rotation_shift"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if template_probe.data.shape != template_gallery.data.shape:\n        raise MatcherError(\"Template probe and gallery must have same shape\")\n\n    # Calculate the number of columns to shift the iris code\n    cols_to_shift = rotation_shift * template_probe.data.shape[1] // 360\n\n    # Shift the iris code in the probe template\n    shifted_probe_data = np.roll(template_probe.data, cols_to_shift, axis=1)\n\n    # Calculate the Hamming distance between the shifted probe data and the gallery data\n    hd_top, hd_bot = np.inf, np.inf\n    if weights:\n        hd_top = np.sum(\n            [\n                np.sum(np.multiply(x[:, hw:, ...] ^ y[:, hw:, ...], z[:, hw:, ...]))\n                for x, y, hw, z in zip(shifted_probe_data, template_gallery.data, template_probe.half_width, weights)\n            ]\n        )\n        hd_bot = np.sum(\n            [\n                np.sum(np.multiply(x[:, :hw, ...] ^ y[:, :hw, ...], z[:, :hw, ...]))\n                for x, y, hw, z in zip(shifted_probe_data, template_gallery.data, template_probe.half_width, weights)\n            ]\n        )\n    else:\n        hd_top = np.sum(\n            [\n                np.sum(x[:, hw:, ...] ^ y[:, hw:, ...])\n                for x, y, hw in zip(shifted_probe_data, template_gallery.data, template_probe.half_width)\n            ]\n        )\n        hd_bot = np.sum(\n            [\n                np.sum(x[:, :hw, ...] ^ y[:, :hw, ...])\n                for x, y, hw in zip(shifted_probe_data, template"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if rotation_shift < 0:\n        raise MatcherError(\"rotation shift must be non-negative\")\n    if nm_dist and nm_dist < 0:\n        raise MatcherError(\"nm_dist must be non-negative\")\n    if weights and len(weights) != len(template_probe.iriscode):\n        raise MatcherError(\"weights must have the same length as iriscode\")\n    if weights and len(weights) != len(template_gallery.iriscode):\n        raise MatcherError(\"weights must have the same length as iriscode\")\n\n    half_width = [w // 2 for w in template_probe.width]\n\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        template_probe.total_codesize, half_width, weights\n    )\n\n    irisbits = [\n        np.unpackbits(x, axis=-1, count=w, bitorder=\"little\") for x, w in zip(template_probe.iriscode, template_probe.width)\n    ]\n    maskbits = [\n        np.unpackbits(x, axis=-1, count=w, bitorder=\"little\") for x, w in zip(template_gallery.iriscode, template_gallery.width)\n    ]\n\n    irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n        irisbits, maskbits, half_width, weights\n    )\n\n    if nm_dist:\n        HD_top = normalized_HD(irisbitcount_top, maskbitcount_top, sqrt_totalbitcount_top, nm_dist)\n        HD_bot = normalized_HD(irisbitcount_bot, maskbitcount_bot, sqrt_totalbitcount_bot, nm_dist)\n        HD = (HD_top + HD_bot) / 2\n    else:\n        HD_top = (irisbitcount_top - maskbitcount_top"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if weights and len(weights) != len(template_probe.iriscode):\n        raise MatcherError(\"weights list length must be equal to iriscodes length.\")\n\n    toal_codesize = sum([iriscode.shape[0] * iriscode.shape[1] for iriscode in template_probe.iriscode])\n    half_width = [iriscode.shape[1] // 2 for iriscode in template_probe.iriscode]\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        toal_codesize, half_width, weights\n    )\n\n    irisbits_probe = [\n        np.unpackbits(np.ascontiguousarray(iriscode), axis=1).reshape(iriscode.shape) for iriscode in template_probe.iriscode\n    ]\n    maskbits_probe = [\n        np.unpackbits(np.ascontiguousarray(maskcode), axis=1).reshape(maskcode.shape) for maskcode in template_probe.maskcode\n    ]\n    irisbits_gallery = [\n        np.unpackbits(np.ascontiguousarray(iriscode), axis=1).reshape(iriscode.shape) for iriscode in template_gallery.iriscode\n    ]\n    maskbits_gallery = [\n        np.unpackbits(np.ascontiguousarray(maskcode), axis=1).reshape(maskcode.shape) for maskcode in template_gallery.maskcode\n    ]\n\n    irisbits_probe_rotated = [\n        np.roll(irisbits, rotation_shift, axis=1) for irisbits in irisbits_probe\n    ]  # rotated irisbits for matching\n    maskbits_probe_rotated = [\n        np.roll(maskbits, rotation_shift, axis=1) for maskbits in maskbits_probe\n    ]  # rotated maskbits for matching\n\n    irisbits_gallery_rot"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if weights:\n        if not nm_dist:\n            raise MatcherError(\"nm_dist is required for weighted Hamming distance calculation.\")\n        if len(weights) != len(template_probe.iriscode) or len(weights) != len(template_gallery.iriscode):\n            raise MatcherError(\"The number of weights tables must match the number of iriscodes.\")\n        for i in range(len(weights)):\n            if weights[i].shape != template_probe.iriscode[i].shape:\n                raise MatcherError(\"The shape of weights table must match the shape of iriscodes.\")\n\n    half_width = [int(template_probe.iriscode[0].shape[1] / 2) for _ in range(len(template_probe.iriscode))]\n    toal_codesize = sum([x.size for x in template_probe.iriscode])\n\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        toal_codesize, half_width, weights\n    )\n\n    irisbits = [\n        np.unpackbits(x.astype(np.uint8), axis=2, bitorder=\"little\") for x in template_probe.iriscode\n    ]\n    maskbits = [\n        np.unpackbits(x.astype(np.uint8), axis=2, bitorder=\"little\") for x in template_gallery.iriscode\n    ]\n\n    irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n        irisbits, maskbits, half_width, weights\n    )\n\n    if nm_dist:\n        HD_top = normalized_HD(irisbitcount_top, maskbitcount_top, sqrt_totalbitcount_top, nm_dist)\n        HD_bot = normalized_HD(irisbitcount_bot, maskbitcount_bot, sqrt_totalbitcount_bot, nm_dist)"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if template_probe.shape != template_gallery.shape:\n        raise MatcherError(\"IrisTemplate shapes are not equal.\")\n\n    toal_codesize = template_probe.shape[1] * template_probe.shape[2]\n    half_width = [int(template_probe.shape[2] / 2)] * template_probe.shape[0]\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        toal_codesize, half_width, weights\n    )\n\n    irisbits_probe = template_probe.get_bits()\n    irisbits_gallery = template_gallery.get_bits()\n\n    irisbits_probe_rotated = [np.roll(x, rotation_shift, axis=2) for x in irisbits_probe]\n\n    irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n        irisbits_probe_rotated, irisbits_gallery, half_width, weights\n    )\n\n    irisbitcount = irisbitcount_top + irisbitcount_bot\n    maskbitcount = maskbitcount_top + maskbitcount_bot\n\n    if nm_dist is None:\n        HD = (\n            max(0, irisbitcount - maskbitcount)\n            + max(0, irisbitcount_top - maskbitcount_top)\n            + max(0, irisbitcount_bot - maskbitcount_bot)\n        )\n    else:\n        HD = (\n            normalized_HD(irisbitcount, maskbitcount, sqrt_totalbitcount, nm_dist)\n            + normalized_HD(irisbitcount_top, maskbitcount_top, sqrt_totalbitcount_top, nm_dist)\n            + normalized_HD(irisbitcount_bot, maskbitcount_bot, sqrt_totalbitcount_bot, nm_dist)\n        )\n\n    return HD, rotation_shift"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if weights:\n        if len(weights) != len(template_probe.iris_codes):\n            raise MatcherError(\"The number of weights tables does not match the number of iris codes in the probe template.\")\n\n    half_width = [hw // 2 for hw in template_probe.code_size]\n    toal_codesize = np.sum(template_probe.code_size)\n\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        toal_codesize, half_width, weights\n    )\n\n    irisbits, maskbits = template_probe.get_irisbits_and_maskbits(\n        template_gallery, rotation_shift=rotation_shift\n    )\n\n    irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n        irisbits, maskbits, half_width, weights\n    )\n\n    if nm_dist:\n        nm_dist_top = normalized_HD(irisbitcount_top, maskbitcount_top, sqrt_totalbitcount_top, nm_dist)\n        nm_dist_bot = normalized_HD(irisbitcount_bot, maskbitcount_bot, sqrt_totalbitcount_bot, nm_dist)\n\n        if nm_dist_top < nm_dist_bot:\n            return nm_dist_top, rotation_shift\n        else:\n            return nm_dist_bot, rotation_shift\n\n    else:\n        HD_top = irisbitcount_top / maskbitcount_top\n        HD_bot = irisbitcount_bot / maskbitcount_bot\n        if HD_top < HD_bot:\n            return HD_top, rotation_shift\n        else:\n            return HD_bot, rotation_shift"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if template_probe.code.shape != template_gallery.code.shape:\n        raise MatcherError(\n            f\"IrisTemplate.code shape mismatch: {template_probe.code.shape} vs {template_gallery.code.shape}\"\n        )\n\n    half_width = [hw // 2 for hw in template_probe.code.shape[1:]]\n    toal_codesize = np.prod(template_probe.code.shape[1:])\n\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        toal_codesize, half_width, weights\n    )\n\n    ##\n    ## Rotate the iris code according to the rotation shift\n    ##\n    irisbits_top = np.roll(template_probe.code[:, :, :half_width[0], ...], rotation_shift, axis=2)\n    irisbits_bot = np.roll(template_probe.code[:, :, half_width[0] :, ...], rotation_shift, axis=2)\n    maskbits_top = np.roll(template_gallery.code[:, :, :half_width[0], ...], rotation_shift, axis=2)\n    maskbits_bot = np.roll(template_gallery.code[:, :, half_width[0] :, ...], rotation_shift, axis=2)\n\n    irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n        irisbits_top, maskbits_top, half_width, weights\n    )\n\n    irisbitcount_top_flip = toal_codesize - irisbitcount_top\n    irisbitcount_bot_flip = toal_codesize - irisbitcount_bot\n\n    ##\n    ## Calculate the Hamming distance\n    ##\n    if nm_dist is None:\n        HD_top = (irisbitcount_top + maskbitcount_top -"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if template_probe.code.shape != template_gallery.code.shape:\n        raise MatcherError(\"Cannot compare iris codes of different sizes\")\n    if rotation_shift < 0:\n        raise MatcherError(\"Rotation shift must be non-negative\")\n    if nm_dist is not None and nm_dist < 0:\n        raise MatcherError(\"Nonmatch distance must be non-negative\")\n\n    toal_codesize = template_probe.code.size\n    half_width = [int(w / 2) for w in template_probe.code.shape[1:]]\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        toal_codesize, half_width, weights\n    )\n\n    min_hd = np.inf\n    min_rotation_shift = 0\n\n    for rotation_shift in range(rotation_shift + 1):\n        irisbits = np.roll(template_probe.code, rotation_shift, axis=-1)\n        maskbits = template_gallery.code\n\n        irisbitcount_top, maskbitcount_top, irisbitcount_bot, maskbitcount_bot = count_nonmatchbits(\n            irisbits, maskbits, half_width, weights\n        )\n\n        if nm_dist:\n            hd_top = normalized_HD(irisbitcount_top, maskbitcount_top, sqrt_totalbitcount_top, nm_dist)\n            hd_bot = normalized_HD(irisbitcount_bot, maskbitcount_bot, sqrt_totalbitcount_bot, nm_dist)\n            hd = np.sqrt(hd_top**2 + hd_bot**2)\n        else:\n            hd = np.sqrt(\n                (irisbitcount_top / maskbitcount_top) ** 2\n                + (irisbitcount_bot / maskbitcount_bot) ** 2\n                + ((maskbitcount_top + maskbitcount_bot) / sqrt_totalbitcount) ** 2\n           "}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if template_probe.codesize != template_gallery.codesize:\n        raise MatcherError(\"Codesize of probe and gallery are not equal.\")\n\n    if rotation_shift < 0:\n        raise MatcherError(\"Rotation shift must be greater than or equal to 0.\")\n\n    if nm_dist is not None and nm_dist <= 0:\n        raise MatcherError(\"Nonmatch distance must be greater than 0.\")\n\n    if weights is not None:\n        if len(weights) != len(template_probe.codes):\n            raise MatcherError(\"Number of weights tables does not match number of codes in probe.\")\n\n        for weight in weights:\n            if weight.shape != template_probe.codes[0].shape:\n                raise MatcherError(\"Shape of weight table does not match shape of codes in probe.\")\n\n    # Calculate the Hamming distance for each rotation shift\n    hd_values = []\n    for i in range(rotation_shift + 1):\n        # Shift the iris codes in the gallery template\n        shifted_gallery_codes = [np.roll(code, i, axis=1) for code in template_gallery.codes]\n\n        # Calculate the Hamming distance for the current rotation shift\n        if weights is not None:\n            hd = np.sum(\n                [\n                    np.sum(np.multiply(x & y, z))\n                    for x, y, z in zip(template_probe.codes, shifted_gallery_codes, weights)\n                ]\n            )\n        else:\n            hd = np.sum([np.sum(x & y) for x, y in zip(template_probe.codes, shifted_gallery_codes)])\n\n        hd_values.append(hd)\n\n    # Find the minimum Hamming distance and corresponding rotation shift\n    min_hd = np.min(hd_values)\n    min_shift = np.argmin(hd_values)\n\n    if nm_dist is not None:\n        # Calculate the normalized Hamming distance\n        sqrt_totalbitcount, sqrt_"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_vertices = polygon.shape[0]\n        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        first_bisectors_point = np.zeros((num_bisectors, 2))\n        second_bisectors_point = np.zeros((num_bisectors, 2))\n\n        for i in range(num_bisectors):\n            for j in range(max_iterations):\n                first_point_index, second_point_index = np.random.choice(num_vertices, 2, replace=False)\n                first_point = polygon[first_point_index]\n                second_point = polygon[second_point_index]\n\n                if np.linalg.norm(second_point - first_point) > min_distance_between_sector_points_in_px:\n                    first_bisectors_point[i] = first_point\n                    second_bisectors_point[i] = second_point\n                    break\n            else:\n                raise EyeCentersEstimationError(\n                    f\"Failed to find a sufficient number of point pairs that meet the distance criterion within {max_iterations} iterations.\"\n                )\n\n        return first_bisectors_point, second_bisectors_point"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        first_bisectors_point = np.zeros((num_bisectors, 2))\n        second_bisectors_point = np.zeros((num_bisectors, 2))\n\n        for i in range(num_bisectors):\n            for j in range(max_iterations):\n                random_indices = np.random.choice(polygon.shape[0], 2, replace=False)\n                first_point = polygon[random_indices[0]]\n                second_point = polygon[random_indices[1]]\n\n                if np.linalg.norm(first_point - second_point) > min_distance_between_sector_points_in_px:\n                    first_bisectors_point[i] = first_point\n                    second_bisectors_point[i] = second_point\n                    break\n            else:\n                raise EyeCentersEstimationError(\n                    f\"Failed to find a sufficient number of point pairs that meet the distance criterion within {max_iterations} iterations.\"\n                )\n\n        return first_bisectors_point, second_bisectors_point"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        for _ in range(max_iterations):\n            first_bisectors_point, second_bisectors_point = [], []\n            for _ in range(num_bisectors):\n                while True:\n                    point_1, point_2 = polygon[np.random.choice(len(polygon), 2, replace=False)]\n                    if np.linalg.norm(point_1 - point_2) > min_distance_between_sector_points_in_px:\n                        break\n                first_bisectors_point.append(point_1)\n                second_bisectors_point.append(point_2)\n            first_bisectors_point = np.array(first_bisectors_point)\n            second_bisectors_point = np.array(second_bisectors_point)\n            if np.linalg.norm(first_bisectors_point - second_bisectors_point) > min_distance_between_sector_points_in_px:\n                break\n        else:\n            raise EyeCentersEstimationError(\n                f\"Failed to find {num_bisectors} pairs of points with a distance greater than {min_distance_between_sector_points_in_px} in {max_iterations} iterations.\"\n            )\n\n        return first_bisectors_point, second_bisectors_point"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Generate random pairs of points from the polygon's vertices\n        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n        first_bisectors_point = np.zeros((num_bisectors, 2))\n        second_bisectors_point = np.zeros((num_bisectors, 2))\n        for i in range(num_bisectors):\n            for j in range(max_iterations):\n                # Randomly choose two points from the polygon's vertices\n                point_1 = polygon[np.random.randint(len(polygon))]\n                point_2 = polygon[np.random.randint(len(polygon))]\n\n                # Calculate the distance between the two points\n                distance = np.linalg.norm(point_1 - point_2)\n\n                # Check if the distance between the two points is greater than the minimum distance\n                if distance > min_distance_between_sector_points_in_px:\n                    # Calculate the perpendicular bisector of the line segment formed by the two points\n                    midpoint = (point_1 + point_2) / 2\n                    direction = np.array([point_2[1] - point_1[1], point_1[0] - point_2[0]])\n                    direction = direction / np.linalg.norm(direction)\n                    first_bisectors_point[i] = midpoint - direction * distance / 2\n                    second_bisectors_point[i] = midpoint + direction * distance / 2\n                    break\n            else:\n                raise EyeCentersEstimationError(\n                    f\"Failed to find {num_bisectors} pairs of points with a distance greater than {min_distance_between_sector_points_in_px} in {max_iterations} iterations.\"\n                )\n\n        return first_bisectors_point, second_bisectors_point"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Generate random pairs of points from the polygon's vertices\n        points_pairs = self._get_points_pairs(polygon, min_distance_between_sector_points_in_px)\n\n        # Calculate the perpendicular bisectors of each pair of points\n        first_bisectors_point, second_bisectors_point = self._calculate_bisectors(points_pairs)\n\n        return first_bisectors_point, second_bisectors_point\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Generate random pairs of points\n        for _ in range(self.params.max_iterations):\n            first_points = self._generate_random_points(polygon, self.params.num_bisectors)\n            second_points = self._generate_random_points(polygon, self.params.num_bisectors)\n\n            # Check if the distance between each pair of points is greater than the minimum distance\n            distances = np.linalg.norm(first_points - second_points, axis=1)\n            if np.all(distances > min_distance_between_sector_points_in_px):\n                return first_points, second_points\n\n        raise EyeCentersEstimationError(\n            \"Failed to find a sufficient number of point pairs that meet the distance criterion within the maximum number of iterations allowed.\"\n        )\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        first_bisectors_point = np.empty((self.params.num_bisectors, 2))\n        second_bisectors_point = np.empty((self.params.num_bisectors, 2))\n\n        for i in range(self.params.num_bisectors):\n            for _ in range(self.params.max_iterations):\n                point_1, point_2 = self._random_pair_of_points(polygon, min_distance_between_sector_points_in_px)\n                if point_1 is not None and point_2 is not None:\n                    first_bisectors_point[i] = point_1\n                    second_bisectors_point[i] = point_2\n                    break\n            else:\n                raise EyeCentersEstimationError(\n                    \"Failed to find a sufficient number of point pairs that meet the distance criterion within the maximum number of iterations allowed.\"\n                )\n\n        return first_bisectors_point, second_bisectors_point\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        for _ in range(max_iterations):\n            first_bisectors_point = []\n            second_bisectors_point = []\n\n            for _ in range(num_bisectors):\n                first_point, second_point = self._get_random_pair_of_points(polygon, min_distance_between_sector_points_in_px)\n\n                first_bisectors_point.append(first_point)\n                second_bisectors_point.append(second_point)\n\n            first_bisectors_point = np.array(first_bisectors_point)\n            second_bisectors_point = np.array(second_bisectors_point)\n\n            if (\n                np.linalg.norm(first_bisectors_point - second_bisectors_point, axis=1)\n                > min_distance_between_sector_points_in_px\n            ).all():\n                return first_bisectors_point, second_bisectors_point\n\n        raise EyeCentersEstimationError(\"Failed to find a sufficient number of point pairs that meet the distance criterion.\")\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Generate random pairs of points\n        first_points, second_points = self._generate_random_pairs_of_points(\n            polygon, min_distance_between_sector_points_in_px\n        )\n\n        # Calculate the bisectors\n        first_bisectors_point, second_bisectors_point = self._calculate_bisectors_points(first_points, second_points)\n\n        return first_bisectors_point, second_bisectors_point\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        for _ in range(max_iterations):\n            first_bisectors_point, second_bisectors_point = self._find_random_bisectors_points(\n                polygon, min_distance_between_sector_points_in_px, num_bisectors\n            )\n\n            if first_bisectors_point.shape[0] == num_bisectors:\n                break\n\n        else:\n            raise EyeCentersEstimationError(\"Failed to find suitable pairs of points for bisectors\")\n\n        return first_bisectors_point, second_bisectors_point\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        for _ in range(max_iterations):\n            first_points = []\n            second_points = []\n\n            for _ in range(num_bisectors):\n                first_point, second_point = self._find_random_points_pair(polygon, min_distance_between_sector_points_in_px)\n                first_points.append(first_point)\n                second_points.append(second_point)\n\n            if len(first_points) == num_bisectors:\n                return np.array(first_points), np.array(second_points)\n\n        raise EyeCentersEstimationError(\n            f\"Failed to find {num_bisectors} pairs of points with a minimum distance of {min_distance_between_sector_points_in_px} in {max_iterations} iterations.\"\n        )\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        num_vertices = len(polygon)\n        if num_bisectors > num_vertices:\n            num_bisectors = num_vertices\n\n        # Generate random pairs of points from the polygon's vertices\n        for _ in range(max_iterations):\n            first_points = np.random.choice(polygon, size=(num_bisectors, 2), replace=True)\n            second_points = np.random.choice(polygon, size=(num_bisectors, 2), replace=True)\n\n            # Calculate the distance between each pair of points\n            distances = np.linalg.norm(first_points - second_points, axis=1)\n\n            # Check if all distances are greater than the minimum distance\n            if np.all(distances > min_distance_between_sector_points_in_px):\n                break\n        else:\n            raise EyeCentersEstimationError(\"Could not find a sufficient number of point pairs that meet the distance criterion.\")\n\n        # Calculate the perpendicular bisectors of the chosen pairs of points\n        first_bisectors_point = (first_points + second_points) / 2\n        second_bisectors_point = first_bisectors_point + np.array([-1, 1]) * (second_points - first_points)\n\n        return first_bisectors_point, second_bisectors_point"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize empty arrays to store the starting and ending points of the bisectors\n        first_bisectors_point = np.empty((self.params.num_bisectors, 2))\n        second_bisectors_point = np.empty((self.params.num_bisectors, 2))\n\n        # Iterate over the number of bisectors to be calculated\n        for i in range(self.params.num_bisectors):\n            # Initialize variables to track the number of iterations and whether a suitable pair of points has been found\n            iterations = 0\n            found_pair = False\n\n            # Continue looping until a suitable pair of points is found or the maximum number of iterations is reached\n            while not found_pair and iterations < self.params.max_iterations:\n                # Randomly select two points from the polygon's vertices\n                point_1, point_2 = np.random.choice(polygon, 2, replace=False)\n\n                # Calculate the distance between the two points\n                distance = np.linalg.norm(point_1 - point_2)\n\n                # Check if the distance between the points is greater than the minimum distance specified\n                if distance > min_distance_between_sector_points_in_px:\n                    # If the distance is sufficient, set the found_pair flag to True and store the points in the arrays\n                    found_pair = True\n                    first_bisectors_point[i] = point_1\n                    second_bisectors_point[i] = point_2\n                else:\n                    # If the distance is not sufficient, increment the iteration counter and continue the loop\n                    iterations += 1\n\n            # If the maximum number of iterations is reached and a suitable pair of points has not been found, raise an error\n            if not found_pair:\n                raise EyeCentersEstimationError(\"Failed to find a pair of points with sufficient distance.\")\n\n        # Return the arrays of starting and ending points for the calculated bisectors\n        return first_bisectors_point, second_bisectors_point"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        first_bisectors_point = np.zeros((self.params.num_bisectors, 2))\n        second_bisectors_point = np.zeros((self.params.num_bisectors, 2))\n\n        for i in range(self.params.num_bisectors):\n            for j in range(self.params.max_iterations):\n                first_point, second_point = self._select_random_pair_of_points(polygon, min_distance_between_sector_points_in_px)\n\n                if first_point is not None and second_point is not None:\n                    first_bisectors_point[i] = first_point\n                    second_bisectors_point[i] = second_point\n                    break\n            else:\n                raise EyeCentersEstimationError(\n                    f\"Failed to find a sufficient number of point pairs that meet the distance criterion within the maximum number of iterations allowed. \"\n                    f\"The minimum distance between points is set to {min_distance_between_sector_points_in_px}, and the maximum number of iterations is set to {self.params.max_iterations}.\"\n                )\n\n        return first_bisectors_point, second_bisectors_point\n"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        # Initialize arrays to store the starting and ending points of the bisectors\n        first_bisectors_point = np.zeros((num_bisectors, 2))\n        second_bisectors_point = np.zeros((num_bisectors, 2))\n\n        # Initialize a counter to keep track of the number of iterations\n        iterations = 0\n\n        # Initialize a flag to indicate whether the loop should continue\n        continue_loop = True\n\n        # Loop until the number of iterations reaches the maximum allowed or the desired number of pairs of points is found\n        while continue_loop and iterations < max_iterations:\n            # Reset the counter of pairs of points that meet the distance criterion\n            num_pairs_meeting_criterion = 0\n\n            # Loop until the desired number of pairs of points is found or the maximum number of iterations is reached\n            while num_pairs_meeting_criterion < num_bisectors and iterations < max_iterations:\n                # Randomly choose two points from the polygon's vertices\n                point_1, point_2 = np.random.choice(polygon, 2, replace=False)\n\n                # Calculate the distance between the two points\n                distance_between_points = np.linalg.norm(point_1 - point_2)\n\n                # Check if the distance between the two points is greater than the minimum distance\n                if distance_between_points > min_distance_between_sector_points_in_px:\n                    # Store the points in the arrays\n                    first_bisectors_point[num_pairs_meeting_criterion] = point_1\n                    second_bisectors_point[num_pairs_meeting_criterion] = point_2\n\n                    # Increment the counter of pairs of points that meet the distance criterion\n                    num_pairs_meeting_criterion += 1\n\n                # Increment the counter of iterations\n                iterations += 1\n\n            # Check if the desired number of"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize arrays to store the starting and ending points of the perpendicular bisectors\n        first_bisectors_point = np.zeros((self.params.num_bisectors, 2))\n        second_bisectors_point = np.zeros((self.params.num_bisectors, 2))\n\n        # Initialize a counter to track the number of iterations\n        iterations = 0\n\n        # Initialize a flag to indicate whether a sufficient number of point pairs have been found\n        found_enough_points = False\n\n        # Loop until a sufficient number of point pairs have been found or the maximum number of iterations has been reached\n        while not found_enough_points and iterations < self.params.max_iterations:\n            # Increment the iteration counter\n            iterations += 1\n\n            # Initialize a counter to track the number of pairs of points that meet the minimum distance criterion\n            num_points_meeting_criterion = 0\n\n            # Loop through the number of bisectors to calculate\n            for i in range(self.params.num_bisectors):\n                # Select two random points from the polygon's vertices\n                first_point, second_point = self._select_random_points(polygon, min_distance_between_sector_points_in_px)\n\n                # Calculate the midpoint between the two points\n                midpoint = (first_point + second_point) / 2\n\n                # Calculate the vector perpendicular to the line segment defined by the two points\n                perpendicular_vector = np.array([-(second_point[1] - first_point[1]), second_point[0] - first_point[0]])\n\n                # Normalize the perpendicular vector\n                perpendicular_vector /= np.linalg.norm(perpendicular_vector)\n\n                # Calculate the starting and ending points of the perpendicular bisector\n                first_bisectors_point[i] = midpoint - perpendicular_vector * diameter / 2\n                second_bisectors_point[i] = midpoint + perpendicular_vector * diameter / 2\n\n                # Check if the distance between the two points meets the minimum distance criterion\n                if np"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize empty arrays to store the starting and ending points of the perpendicular bisectors\n        first_bisectors_point = np.empty((self.params.num_bisectors, 2))\n        second_bisectors_point = np.empty((self.params.num_bisectors, 2))\n\n        # Calculate the number of points in the polygon\n        num_points = polygon.shape[0]\n\n        # Initialize a counter to keep track of the number of pairs of points that meet the distance criterion\n        num_valid_pairs = 0\n\n        # Initialize a counter to keep track of the number of iterations\n        iteration_count = 0\n\n        # Loop until the number of pairs of points that meet the distance criterion is equal to the desired number\n        while num_valid_pairs < self.params.num_bisectors:\n            # Randomly choose two indices from the range of the number of points in the polygon\n            point_indices = np.random.choice(num_points, size=2, replace=False)\n\n            # Extract the points at the chosen indices\n            point_1 = polygon[point_indices[0]]\n            point_2 = polygon[point_indices[1]]\n\n            # Calculate the distance between the chosen points\n            distance = np.linalg.norm(point_1 - point_2)\n\n            # Check if the distance between the chosen points is greater than the minimum distance\n            if distance > min_distance_between_sector_points_in_px:\n                # Calculate the midpoint between the chosen points\n                midpoint = (point_1 + point_2) / 2\n\n                # Calculate the vector from the midpoint to the first point\n                vector = point_1 - midpoint\n\n                # Calculate the perpendicular bisector of the vector\n                perpendicular_bisector = np.array([-vector[1], vector[0]])\n\n                # Normalize the perpendicular bisector\n                perpendicular_bisector /= np.linalg.norm(perpendicular_bisector)\n\n                # Calculate the starting and ending points of the perpendicular bisector\n                first_bisectors_"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Calculate the number of vertices in the polygon\n        num_vertices = polygon.shape[0]\n\n        # Initialize the first and second bisectors arrays\n        first_bisectors = np.zeros((self.params.num_bisectors, 2))\n        second_bisectors = np.zeros((self.params.num_bisectors, 2))\n\n        # Iterate over the number of bisectors to be calculated\n        for i in range(self.params.num_bisectors):\n            # Initialize the iteration counter\n            iteration = 0\n\n            # Start the loop to find a pair of points that satisfy the minimum distance criterion\n            while True:\n                # Increment the iteration counter\n                iteration += 1\n\n                # Generate two random indices between 0 and the number of vertices\n                rand_indices = np.random.randint(0, num_vertices, size=2)\n\n                # Check if the indices are the same\n                if rand_indices[0] == rand_indices[1]:\n                    # If the indices are the same, continue to the next iteration\n                    continue\n\n                # Extract the points at the random indices\n                point_1 = polygon[rand_indices[0]]\n                point_2 = polygon[rand_indices[1]]\n\n                # Calculate the distance between the points\n                distance = np.linalg.norm(point_1 - point_2)\n\n                # Check if the distance is greater than the minimum distance\n                if distance > min_distance_between_sector_points_in_px:\n                    # If the distance is greater than the minimum distance, break out of the loop\n                    break\n\n                # Check if the maximum number of iterations has been reached\n                if iteration >= self.params.max_iterations:\n                    # If the maximum number of iterations has been reached, raise an error\n                    raise EyeCentersEstimationError(\n                        \"Failed to find a sufficient number of point pairs that meet the distance criterion.\"\n                    )\n\n            # Calculate the bisector line between the two points\n            bisector = (point_1 + point_2) / 2"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize an empty list to store the starting and ending points of the perpendicular bisectors\n        fst_points = []\n        sec_points = []\n\n        # Initialize a counter to track the number of iterations\n        iterations = 0\n\n        # Loop until we have enough pairs of points or reach the maximum number of iterations\n        while len(fst_points) < self.params.num_bisectors and iterations < self.params.max_iterations:\n            # Increment the iteration counter\n            iterations += 1\n\n            # Randomly select two points from the polygon's vertices\n            fst_point, sec_point = self._randomly_select_points(polygon, min_distance_between_sector_points_in_px)\n\n            # Calculate the perpendicular bisector of the line segment formed by the two points\n            bisector = self._calculate_bisector(fst_point, sec_point)\n\n            # Check if the perpendicular bisector is valid (not too close to the polygon's edge)\n            if self._is_bisector_valid(polygon, bisector):\n                # If the bisector is valid, add it to the list of bisectors\n                fst_points.append(bisector[0])\n                sec_points.append(bisector[1])\n\n        # If we have enough pairs of points, return them as numpy arrays\n        if len(fst_points) == self.params.num_bisectors:\n            return np.array(fst_points), np.array(sec_points)\n\n        # If we have not found enough pairs of points, raise an exception\n        raise EyeCentersEstimationError(\n            f\"Failed to find {self.params.num_bisectors} pairs of points with a minimum distance of {min_distance_between_sector_points_in_px} pixels after {iterations} iterations.\"\n        )\n\n    "}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize an empty list to store the starting points of the bisectors\n        first_bisectors_point = []\n        # Initialize an empty list to store the ending points of the bisectors\n        second_bisectors_point = []\n\n        # Iterate over the number of bisectors specified in the algorithm parameters\n        for _ in range(self.params.num_bisectors):\n            # Initialize a flag to track whether a pair of points meeting the minimum distance criterion has been found\n            found = False\n            # Initialize a counter to track the number of iterations\n            iteration = 0\n            # Continue iterating until a pair of points meeting the criterion is found or the maximum number of iterations is reached\n            while not found and iteration < self.params.max_iterations:\n                # Randomly select two points from the polygon's vertices\n                point_1, point_2 = np.random.choice(polygon, 2, replace=False)\n                # Calculate the distance between the two points\n                distance_between_points = np.linalg.norm(point_1 - point_2)\n                # Check if the distance between the points is greater than the minimum distance specified in the algorithm parameters\n                if distance_between_points > min_distance_between_sector_points_in_px:\n                    # If the distance is greater, set the flag to True and break out of the loop\n                    found = True\n                # Increment the iteration counter\n                iteration += 1\n            # If the flag is still False after the maximum number of iterations, raise an exception\n            if not found:\n                raise EyeCentersEstimationError(\n                    \"Could not find a pair of points with a distance greater than the minimum distance\"\n                )\n            # Calculate the midpoint of the two points\n            midpoint = (point_1 + point_2) / 2\n            # Calculate the vector representing the line between the two points\n            line_vector = point_2 - point_1\n            # Calculate the normal vector to the line vector\n            normal_vector = np.array([-line_vector[1], line_vector[0]])\n            # Calculate the starting point of the bisector"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(self, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(self, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(self, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(self, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_execute(self, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_execute(self, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_execute(self, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(self, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_execute(self, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_execute(self, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.on_pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.on_post_execute(self, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.on_pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.on_post_execute(self, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.on_pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.on_post_execute(self, *args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_execution(self)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_execution(self)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_execution(self)\n\n        output = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_execution(self)\n\n        return output"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        self._pre_execution_hook(*args, **kwargs)\n        result = self.run(*args, **kwargs)\n        self._post_execution_hook(*args, **kwargs)\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        self._pre_execution_hook(*args, **kwargs)\n        result = self.run(*args, **kwargs)\n        self._post_execution_hook(*args, **kwargs)\n\n        return result"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        self.pre_execute(*args, **kwargs)\n        result = self.run(*args, **kwargs)\n        self.post_execute(*args, **kwargs)\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        self._pre_execute(*args, **kwargs)\n        result = self.run(*args, **kwargs)\n        self._post_execute(*args, **kwargs)\n        return result\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        self.pre_execution_hook(*args, **kwargs)\n        result = self.run(*args, **kwargs)\n        self.post_execution_hook(*args, **kwargs)\n        return result\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except json.JSONDecodeError:\n            return False"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except json.JSONDecodeError:\n            return False"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except json.JSONDecodeError:\n            return False"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n        return self.check_type(deserialized_output, type_definition)"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except json.JSONDecodeError:\n            return False\n        except TypeError:\n            return False"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except (json.JSONDecodeError, TypeError):\n            return False\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except json.JSONDecodeError:\n            return False\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = func_object.__doc__\n\n        input_type_hints = {}\n        output_type_hints = {}\n        for name, param in signature.parameters.items():\n            if param.kind == param.VAR_POSITIONAL:\n                input_type_hints[name] = param.annotation\n            elif param.kind == param.VAR_KEYWORD:\n                input_type_hints[name] = param.annotation\n            else:\n                input_type_hints[name] = param.annotation\n\n        for name, param in signature.parameters.items():\n            if param.kind == param.VAR_POSITIONAL:\n                output_type_hints[name] = param.annotation\n            elif param.kind == param.VAR_KEYWORD:\n                output_type_hints[name] = param.annotation\n            else:\n                output_type_hints[name] = param.annotation\n\n        input_class_definitions = {}\n        output_class_definitions = {}\n        for name, type_hint in input_type_hints.items():\n            input_class_definitions[name] = Register.get_class_definition(type_hint)\n\n        for name, type_hint in output_type_hints.items():\n            output_class_definitions[name] = Register.get_class_definition(type_hint)\n\n        function_type = FunctionType.SYMBOLIC\n        output_class_definition = None\n        for name, type_hint in output_type_hints.items():\n            if isinstance(type_hint, type) and issubclass(type_hint, Embedding):\n                function_type = FunctionType.EMBEDDABLE\n                output_class_definition = Register.get_class_definition(type_hint)\n            elif get_origin(type_hint) is Union:\n                for arg in get_args(type_hint):\n                    if isinstance(arg, type) and issub"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = inspect.getdoc(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n\n        for key, value in type_hints.items():\n            if key in signature.parameters:\n                input_type_hints[key] = value\n            else:\n                output_type_hints[key] = value\n\n        input_class_definitions = {}\n        output_class_definitions = {}\n\n        for key, value in input_type_hints.items():\n            input_class_definitions[key] = Register.get_class_definition(value)\n\n        for key, value in output_type_hints.items():\n            output_class_definitions[key] = Register.get_class_definition(value)\n\n        function_type = FunctionType.SYMBOLIC\n\n        for key, value in output_type_hints.items():\n            if inspect.isclass(value) and issubclass(value, Embedding):\n                function_type = FunctionType.EMBEDDABLE\n                break\n            elif get_origin(value) is Union:\n                for arg in get_args(value):\n                    if inspect.isclass(arg) and issubclass(arg, Embedding):\n                        function_type = FunctionType.EMBEDDABLE\n                        break\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hints=input_type_hints,\n            output_type_hints=output_type_hints,\n            input_class_definitions=input_class_definitions,\n            output_class_definitions=output_class_definitions,\n            type=function_type,\n            source=get_source(func_object)\n        )\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        sig = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = func_object.__doc__\n\n        input_type_hints = {}\n        output_type_hints = {}\n        for param in sig.parameters.values():\n            if param.name in type_hints:\n                if param.kind == param.POSITIONAL_ONLY:\n                    input_type_hints[param.name] = type_hints[param.name]\n                elif param.kind == param.POSITIONAL_OR_KEYWORD:\n                    input_type_hints[param.name] = type_hints[param.name]\n                elif param.kind == param.VAR_POSITIONAL:\n                    input_type_hints[param.name] = type_hints[param.name]\n                elif param.kind == param.KEYWORD_ONLY:\n                    input_type_hints[param.name] = type_hints[param.name]\n                elif param.kind == param.VAR_KEYWORD:\n                    input_type_hints[param.name] = type_hints[param.name]\n\n        for param in sig.parameters.values():\n            if param.name not in type_hints:\n                if param.kind == param.POSITIONAL_ONLY:\n                    output_type_hints[param.name] = sig.return_annotation\n                elif param.kind == param.POSITIONAL_OR_KEYWORD:\n                    output_type_hints[param.name] = sig.return_annotation\n                elif param.kind == param.VAR_POSITIONAL:\n                    output_type_hints[param.name] = sig.return_annotation\n                elif param.kind == param.KEYWORD_ONLY:\n                    output_type_hints[param.name] = sig.return_annotation\n                elif param.kind == param.VAR_KEYWORD:\n                    output_type_hints[param.name] = sig.return_annotation\n\n        input_class_definitions = {}\n        for key, value in input_type_hints"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        # Get the signature of the function\n        signature = inspect.signature(func_object)\n\n        # Get the type hints of the function's parameters\n        type_hints = get_type_hints(func_object)\n\n        # Get the docstring of the function\n        docstring = inspect.getdoc(func_object)\n\n        # Determine the input and output type hints\n        input_type_hints = {}\n        output_type_hints = {}\n        for param_name, param_type in type_hints.items():\n            if param_name in signature.parameters:\n                input_type_hints[param_name] = param_type\n            else:\n                output_type_hints[param_name] = param_type\n\n        # Get the class definitions for the input and output types\n        input_class_definitions = {param_name: get_class_definition(param_type) for param_name, param_type in input_type_hints.items()}\n        output_class_definitions = {param_name: get_class_definition(param_type) for param_name, param_type in output_type_hints.items()}\n\n        # Determine the function type\n        if len(output_type_hints) == 1:\n            output_type = list(output_type_hints.values())[0]\n            if inspect.isclass(output_type) and issubclass(output_type, Embedding):\n                function_type = FunctionType.EMBEDDABLE\n            elif get_origin(output_type) is Union and any(issubclass(arg, Embedding) for arg in get_args(output_type)):\n                function_type = FunctionType.EMBEDDABLE\n            else:\n                function_type = FunctionType.SYMBOLIC\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        # Create the FunctionDescription object\n        function_description = FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hints=input_type_hints,\n            output"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = inspect.getdoc(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n\n        for param_name, param in signature.parameters.items():\n            if param.annotation != inspect._empty:\n                if param.name in type_hints:\n                    input_type_hints[param.name] = type_hints[param.name]\n                else:\n                    input_type_hints[param.name] = param.annotation\n\n        if signature.return_annotation != inspect._empty:\n            if 'return' in type_hints:\n                output_type_hints['return'] = type_hints['return']\n            else:\n                output_type_hints['return'] = signature.return_annotation\n\n        def get_class_definition(type_hint):\n            if inspect.isclass(type_hint):\n                if issubclass(type_hint, Embedding):\n                    return type_hint\n                else:\n                    return None\n            elif get_origin(type_hint) is Union:\n                args = get_args(type_hint)\n                for arg in args:\n                    if issubclass(arg, Embedding):\n                        return arg\n                return None\n            else:\n                return None\n\n        input_class_definitions = {}\n        for param_name, param_type in input_type_hints.items():\n            class_definition = get_class_definition(param_type)\n            if class_definition is not None:\n                input_class_definitions[param_name] = class_definition\n\n        output_class_definitions = {}\n        for param_name, param_type in output_type_hints.items():\n            class_definition = get_class_definition(param_type)\n            if class_definition is not None:\n                output_class_definitions[param_name] = class_definition\n\n        function_type = FunctionType.SYMBOLIC"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = inspect.getdoc(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n        for param_name, param in signature.parameters.items():\n            if param.kind == param.POSITIONAL_OR_KEYWORD:\n                if param.annotation != inspect._empty:\n                    input_type_hints[param_name] = param.annotation\n                else:\n                    input_type_hints[param_name] = None\n            elif param.kind == param.VAR_POSITIONAL:\n                input_type_hints[param_name] = Tuple[param.annotation, ...]\n            elif param.kind == param.VAR_KEYWORD:\n                input_type_hints[param_name] = Dict[str, param.annotation]\n\n        if signature.return_annotation != inspect._empty:\n            output_type_hints[\"output\"] = signature.return_annotation\n        else:\n            output_type_hints[\"output\"] = None\n\n        def get_class_definition(type_hint):\n            if type_hint is None:\n                return None\n            elif inspect.isclass(type_hint):\n                return type_hint\n            elif get_origin(type_hint) is Union:\n                args = get_args(type_hint)\n                for arg in args:\n                    if inspect.isclass(arg) and issubclass(arg, Embedding):\n                        return arg\n                return None\n            elif get_origin(type_hint) is Literal:\n                return None\n            else:\n                return None\n\n        input_class_definitions = {param_name: get_class_definition(type_hint) for param_name, type_hint in input_type_hints.items()}\n        output_class_definitions = {param_name: get_class_definition(type_hint) for param_name, type_hint in output_type_hints.items"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = inspect.getdoc(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n        for param_name, param in signature.parameters.items():\n            if param.kind == param.POSITIONAL_OR_KEYWORD:\n                input_type_hints[param_name] = type_hints.get(param_name, param.annotation)\n            elif param.kind == param.VAR_POSITIONAL:\n                input_type_hints[param_name] = Tuple[type_hints.get(param_name, param.annotation)]\n            elif param.kind == param.VAR_KEYWORD:\n                input_type_hints[param_name] = Dict[str, type_hints.get(param_name, param.annotation)]\n\n        output_type_hint = type_hints.get('return', None)\n\n        input_class_definitions = {}\n        for param_name, param_type in input_type_hints.items():\n            input_class_definitions[param_name] = get_class_definition(param_type)\n\n        output_class_definition = get_class_definition(output_type_hint)\n\n        if issubclass(output_class_definition, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        elif issubclass(output_class_definition, Union):\n            for arg in output_class_definition.__args__:\n                if issubclass(arg, Embedding):\n                    function_type = FunctionType.EMBEDDABLE\n                    output_class_definition = arg\n                    break\n            else:\n                function_type = FunctionType.SYMBOLIC\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        function_description = FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hints=input_type_hints,\n           "}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = inspect.getdoc(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n\n        for param_name, param in signature.parameters.items():\n            if param.kind == param.KEYWORD_ONLY:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_KEYWORD:\n                continue\n            if param.name in type_hints:\n                input_type_hints[param.name] = type_hints[param.name]\n\n        for param_name, param in signature.parameters.items():\n            if param.kind == param.KEYWORD_ONLY:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_KEYWORD:\n                continue\n            if param.name in type_hints:\n                output_type_hints[param.name] = type_hints[param.name]\n\n        input_class_definitions = {param_name: get_class_definition(param_type) for param_name, param_type in\n                                   input_type_hints.items()}\n        output_class_definitions = {param_name: get_class_definition(param_type) for param_name, param_type in\n                                    output_type_hints.items()}\n\n        function_type = FunctionType.SYMBOLIC\n        output_class_definition = None\n        for param_name, param_type in output_type_hints.items():\n            if isinstance(param_type, type) and issubclass(param_type, Embedding):\n                function_type = FunctionType.EMBEDDABLE\n                output_class_definition = get_class_definition(param_type)\n            elif get_origin(param_type) is Union:\n                for arg in param_type.__args__:\n                    if isinstance(arg, type) and"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = inspect.getdoc(func_object)\n        source = get_source(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n        for key, value in signature.parameters.items():\n            if key == \"self\":\n                continue\n            if value.annotation != inspect._empty:\n                input_type_hints[key] = value.annotation\n\n        for key, value in type_hints.items():\n            if key == \"return\":\n                output_type_hints[key] = value\n\n        input_class_definitions = {}\n        for key, value in input_type_hints.items():\n            input_class_definitions[key] = Register.get_class_definition(value)\n\n        output_class_definitions = {}\n        for key, value in output_type_hints.items():\n            output_class_definitions[key] = Register.get_class_definition(value)\n\n        function_type = FunctionType.SYMBOLIC\n        if output_class_definitions[\"return\"] is not None:\n            if issubclass(output_class_definitions[\"return\"], Embedding):\n                function_type = FunctionType.EMBEDDABLE\n            elif get_origin(output_class_definitions[\"return\"]) == Union and \\\n                    any(issubclass(cls, Embedding) for cls in output_class_definitions[\"return\"].__args__):\n                function_type = FunctionType.EMBEDDABLE\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hints=input_type_hints,\n            output_type_hints=output_type_hints,\n            input_class_definitions=input_class_definitions,\n            output_class_definitions=output_class_definitions,\n            function_type=function_type,\n            source=source\n        )"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = inspect.getdoc(func_object)\n        source = get_source(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n        for param_name, param in signature.parameters.items():\n            if param.kind == param.POSITIONAL_OR_KEYWORD:\n                if param.annotation != inspect.Parameter.empty:\n                    input_type_hints[param_name] = param.annotation\n                else:\n                    input_type_hints[param_name] = Any\n\n        for param_name, param in signature.parameters.items():\n            if param.kind == param.POSITIONAL_OR_KEYWORD:\n                if param.annotation != inspect.Parameter.empty:\n                    output_type_hints[param_name] = param.annotation\n                else:\n                    output_type_hints[param_name] = Any\n\n        input_class_definitions = {\n            param_name: get_class_definition(param_type)\n            for param_name, param_type in input_type_hints.items()\n        }\n\n        output_class_definitions = {\n            param_name: get_class_definition(param_type)\n            for param_name, param_type in output_type_hints.items()\n        }\n\n        output_type = list(output_type_hints.values())[0]\n\n        if isinstance(output_type, type) and issubclass(output_type, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        elif isinstance(output_type, Union):\n            for arg in output_type.__args__:\n                if issubclass(arg, Embedding):\n                    function_type = FunctionType.EMBEDDABLE\n                    break\n            else:\n                function_type = FunctionType.SYMBOLIC\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        return FunctionDescription(\n           "}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = func_object.__doc__\n\n        input_type_hints = {}\n        output_type_hints = {}\n        for param_name, param in signature.parameters.items():\n            if param.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD:\n                if param_name in type_hints:\n                    input_type_hints[param_name] = type_hints[param_name]\n                else:\n                    input_type_hints[param_name] = None\n\n        if signature.return_annotation is not inspect.Signature.empty:\n            output_type_hints = type_hints[signature.return_annotation]\n        else:\n            output_type_hints = None\n\n        input_class_definitions = {}\n        for param_name, param_type in input_type_hints.items():\n            input_class_definitions[param_name] = Register.get_class_definition(param_type)\n\n        output_class_definitions = {}\n        for param_name, param_type in output_type_hints.items():\n            output_class_definitions[param_name] = Register.get_class_definition(param_type)\n\n        function_type = FunctionType.SYMBOLIC\n        if output_type_hints and issubclass(output_type_hints, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        elif output_type_hints and get_origin(output_type_hints) is Union:\n            for arg in get_args(output_type_hints):\n                if issubclass(arg, Embedding):\n                    function_type = FunctionType.EMBEDDABLE\n                    break\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hints=input_type_hints,\n            output_type_hints=output_type_hints,\n            input_class"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = func_object.__doc__\n\n        input_type_hints = {}\n        output_type_hints = {}\n\n        for param_name, param in signature.parameters.items():\n            if param.kind == param.POSITIONAL_OR_KEYWORD:\n                if param.name in type_hints:\n                    input_type_hints[param.name] = type_hints[param.name]\n\n        output_type_hint = type_hints.get('return')\n\n        if output_type_hint:\n            if get_origin(output_type_hint) is Union:\n                output_type_hint = [arg for arg in get_args(output_type_hint) if issubclass(arg, Embedding)]\n                if len(output_type_hint) == 0:\n                    raise ValueError(\"Output type hint is not a subclass of an Embedding\")\n                elif len(output_type_hint) == 1:\n                    output_type_hint = output_type_hint[0]\n                else:\n                    raise ValueError(\"Output type hint is not a subclass of an Embedding\")\n            else:\n                if not issubclass(output_type_hint, Embedding):\n                    raise ValueError(\"Output type hint is not a subclass of an Embedding\")\n\n        output_class_definition = get_class_definition(output_type_hint)\n\n        if issubclass(output_type_hint, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hints=input_type_hints,\n            output_type_hints=output_type_hints,\n            input_class_definitions={param_name: get_class_definition(param_type) for param_"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        sig = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = func_object.__doc__\n        input_type_hints = {}\n        output_type_hints = {}\n        input_class_definitions = {}\n        output_class_definitions = {}\n\n        for param_name, param in sig.parameters.items():\n            if param.annotation != inspect._empty:\n                input_type_hints[param_name] = param.annotation\n\n        output_type_hint = type_hints.get('return', None)\n        if output_type_hint:\n            if inspect.isclass(output_type_hint) or issubclass(output_type_hint, Union):\n                output_type_hints['return'] = output_type_hint\n                output_class_definitions['return'] = Register.get_class_definition(output_type_hint)\n            else:\n                output_type_hints['return'] = output_type_hint\n                output_class_definitions['return'] = Register.get_class_definition(output_type_hint)\n\n        if any(issubclass(output_type_hint, Embedding) for output_type_hint in output_type_hints.values()):\n            function_type = FunctionType.EMBEDDABLE\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hints=input_type_hints,\n            output_type_hints=output_type_hints,\n            input_class_definitions=input_class_definitions,\n            output_class_definitions=output_class_definitions,\n            type=function_type,\n            source=get_source(func_object)\n        )\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = inspect.getdoc(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n\n        for param_name, param in signature.parameters.items():\n            if param.kind == param.VAR_POSITIONAL or param.kind == param.VAR_KEYWORD:\n                continue\n            if param.annotation == param.empty:\n                continue\n            if param.name in type_hints:\n                if param.kind == param.KEYWORD_ONLY:\n                    input_type_hints[param.name] = type_hints[param.name]\n                else:\n                    output_type_hints[param.name] = type_hints[param.name]\n\n        input_class_definitions = {\n            param_name: Register.get_class_definition(param_type)\n            for param_name, param_type in input_type_hints.items()\n        }\n\n        output_class_definitions = {\n            param_name: Register.get_class_definition(param_type)\n            for param_name, param_type in output_type_hints.items()\n        }\n\n        if len(output_class_definitions) == 0:\n            function_type = FunctionType.SYMBOLIC\n        else:\n            output_type = list(output_class_definitions.values())[0]\n            if isinstance(output_type, Embedding):\n                function_type = FunctionType.EMBEDDABLE\n            elif issubclass(output_type, Union):\n                for arg in get_args(output_type):\n                    if issubclass(arg, Embedding):\n                        function_type = FunctionType.EMBEDDABLE\n                        break\n                else:\n                    function_type = FunctionType.SYMBOLIC\n            else:\n                function_type = FunctionType.SYMBOLIC\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = inspect.getdoc(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n        for param_name, param in signature.parameters.items():\n            if param.kind == param.VAR_POSITIONAL:\n                input_type_hints['*' + param.name] = get_class_definition(type_hints[param.name])\n            elif param.kind == param.VAR_KEYWORD:\n                input_type_hints['**' + param.name] = get_class_definition(type_hints[param.name])\n            elif param.kind == param.POSITIONAL_ONLY:\n                input_type_hints[param.name] = get_class_definition(type_hints[param.name])\n            elif param.kind == param.POSITIONAL_OR_KEYWORD:\n                input_type_hints[param.name] = get_class_definition(type_hints[param.name])\n            elif param.kind == param.KEYWORD_ONLY:\n                input_type_hints[param.name] = get_class_definition(type_hints[param.name])\n            else:\n                raise ValueError(\"Unknown parameter kind: \" + str(param.kind))\n\n        output_type_hint = type_hints['return']\n        if isinstance(output_type_hint, type):\n            output_class_definition = get_class_definition(output_type_hint)\n        elif issubclass(output_type_hint, Union):\n            output_class_definition = get_class_definition(output_type_hint.__args__[0])\n        else:\n            raise ValueError(\"Unknown output type hint: \" + str(output_type_hint))\n\n        if issubclass(output_class_definition, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        else:\n            function_type = FunctionType.SYMBOLIC"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = inspect.getdoc(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n\n        for param_name, param in signature.parameters.items():\n            if param.annotation is not inspect.Parameter.empty:\n                if param.name in type_hints:\n                    if param.kind == param.POSITIONAL_OR_KEYWORD:\n                        input_type_hints[param.name] = type_hints[param.name]\n                    elif param.kind == param.VAR_POSITIONAL:\n                        input_type_hints[param.name] = Tuple[type_hints[param.name], ...]\n                    elif param.kind == param.KEYWORD_ONLY:\n                        input_type_hints[param.name] = type_hints[param.name]\n                    elif param.kind == param.VAR_KEYWORD:\n                        input_type_hints[param.name] = Dict[str, type_hints[param.name]]\n                else:\n                    input_type_hints[param.name] = param.annotation\n            else:\n                input_type_hints[param.name] = None\n\n        if signature.return_annotation is not inspect.Signature.empty:\n            if signature.return_annotation is not None:\n                output_type_hints['output'] = signature.return_annotation\n            else:\n                output_type_hints['output'] = None\n        else:\n            output_type_hints['output'] = None\n\n        input_class_definitions = {}\n        for param_name, type_hint in input_type_hints.items():\n            if type_hint is not None:\n                input_class_definitions[param_name] = get_class_definition(type_hint)\n            else:\n                input_class_definitions[param_name] = None\n\n        output_class_definitions = {}\n        for param_name, type_h"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        sig = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = inspect.getdoc(func_object)\n        input_type_hints = {}\n        output_type_hints = {}\n        for param in sig.parameters.values():\n            if param.name in type_hints:\n                if param.kind == param.POSITIONAL_OR_KEYWORD:\n                    input_type_hints[param.name] = type_hints[param.name]\n                elif param.kind == param.VAR_POSITIONAL:\n                    input_type_hints[param.name] = Tuple[type_hints[param.name], ...]\n                elif param.kind == param.VAR_KEYWORD:\n                    input_type_hints[param.name] = Dict[str, type_hints[param.name]]\n        if \"return\" in type_hints:\n            output_type_hints[\"return\"] = type_hints[\"return\"]\n\n        def get_class_definition(type_hint):\n            if get_origin(type_hint) is Union:\n                args = get_args(type_hint)\n                for arg in args:\n                    if arg is not None and isinstance(arg, type) and issubclass(arg, Embedding):\n                        return arg\n            elif isinstance(type_hint, type) and issubclass(type_hint, Embedding):\n                return type_hint\n            else:\n                return None\n\n        function_type = FunctionType.SYMBOLIC\n        output_class_definition = None\n        for param_name, type_hint in output_type_hints.items():\n            class_definition = get_class_definition(type_hint)\n            if class_definition is not None:\n                function_type = FunctionType.EMBEDDABLE\n                output_class_definition = class_definition\n                break\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hints=input"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = func_object.__doc__\n        input_class_definitions = []\n        output_class_definitions = []\n        input_type_hints = []\n        output_type_hints = []\n        function_type = FunctionType.SYMBOLIC\n        output_class_definition = None\n        for param_name, param in signature.parameters.items():\n            if param.kind == param.POSITIONAL_OR_KEYWORD:\n                type_hint = type_hints.get(param_name, None)\n                if type_hint:\n                    input_type_hints.append(type_hint)\n                    class_definition = get_class_definition(type_hint)\n                    if class_definition:\n                        input_class_definitions.append(class_definition)\n\n        output_type_hint = type_hints.get(\"return\", None)\n        if output_type_hint:\n            output_type_hints.append(output_type_hint)\n            class_definition = get_class_definition(output_type_hint)\n            if class_definition:\n                output_class_definitions.append(class_definition)\n                if issubclass(class_definition, Embedding):\n                    function_type = FunctionType.EMBEDDABLE\n                elif get_origin(output_type_hint) is Union:\n                    for arg in get_args(output_type_hint):\n                        if issubclass(arg, Embedding):\n                            function_type = FunctionType.EMBEDDABLE\n                            output_class_definition = get_class_definition(arg)\n                            break\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hints=input_type_hints,\n            output_type_hints=output_type_hints,\n            input_class_definitions=input_class_definitions,\n            output_class_definitions=output"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = inspect.getdoc(func_object)\n        input_type_hints = {}\n        output_type_hints = {}\n        for param_name, param in signature.parameters.items():\n            if param.kind == param.VAR_POSITIONAL or param.kind == param.VAR_KEYWORD:\n                continue\n            if param.annotation != inspect._empty:\n                input_type_hints[param_name] = param.annotation\n            else:\n                input_type_hints[param_name] = None\n\n        for param_name, param in signature.parameters.items():\n            if param.kind == param.VAR_POSITIONAL or param.kind == param.VAR_KEYWORD:\n                if param.annotation != inspect._empty:\n                    output_type_hints[param_name] = param.annotation\n                else:\n                    output_type_hints[param_name] = None\n\n        def get_class_definition(type_hint):\n            if type_hint is None:\n                return None\n            if inspect.isclass(type_hint):\n                if issubclass(type_hint, Embedding):\n                    return type_hint\n                else:\n                    return None\n            origin = get_origin(type_hint)\n            if origin is None:\n                return None\n            if origin is Union:\n                args = get_args(type_hint)\n                for arg in args:\n                    class_def = get_class_definition(arg)\n                    if class_def is not None:\n                        return class_def\n                return None\n            else:\n                return get_class_definition(origin)\n\n        output_class_definition = None\n        output_type = None\n        for param_name, param in signature.parameters.items():\n            if param.kind == param.VAR_POSITIONAL or param.kind == param.VAR_KEYWORD:\n                output_type_hint = output_type_hints[param_name]\n                output_"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = inspect.getdoc(func_object)\n        source = get_source(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n\n        for param_name, param in signature.parameters.items():\n            if param.kind == param.POSITIONAL_OR_KEYWORD:\n                if param_name in type_hints:\n                    input_type_hints[param_name] = type_hints[param_name]\n\n        if \"return\" in type_hints:\n            output_type_hints[\"return\"] = type_hints[\"return\"]\n\n        def get_class_definition(type_hint):\n            if inspect.isclass(type_hint):\n                if issubclass(type_hint, Embedding):\n                    return type_hint\n                else:\n                    return None\n            elif get_origin(type_hint) is Union:\n                union_types = get_args(type_hint)\n                for union_type in union_types:\n                    class_def = get_class_definition(union_type)\n                    if class_def:\n                        return class_def\n            else:\n                return None\n\n        output_class_def = None\n        output_type = None\n\n        if \"return\" in output_type_hints:\n            output_type_hint = output_type_hints[\"return\"]\n            if inspect.isclass(output_type_hint) or get_origin(output_type_hint) is Union:\n                output_class_def = get_class_definition(output_type_hint)\n\n                if output_class_def and issubclass(output_class_def, Embedding):\n                    output_type = FunctionType.EMBEDDABLE\n                else:\n                    output_type = FunctionType.SYMBOLIC\n\n        function_description = FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n           "}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] += 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] += 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] += 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] += 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] += 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] += 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] += 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] += 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            "}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            "}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        expected_length = self.size\n        if len(loaded_bit_array) != expected_length:\n            logging.warning(f\"BloomFilter: Bit array length mismatch. Expected {expected_length}, but got {len(loaded_bit_array)}. Reinitializing and saving.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        expected_length = self.size\n        if len(loaded_bit_array) != expected_length:\n            logging.warning(f\"Loaded bit array length ({len(loaded_bit_array)}) does not match expected length ({expected_length}). Reinitializing and saving.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array\n            self.indices = np.zeros(self.size, dtype=np.int32)"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        if len(loaded_bit_array) != self.size:\n            logging.warning(f\"Loaded bit array length ({len(loaded_bit_array)}) does not match expected size ({self.size}). Reinitializing and saving.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n\n        if len(loaded_bit_array) != self.size:\n            logging.warning(\"BloomFilter bit array length mismatch. Reinitializing and saving.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        expected_length = self.size\n        if len(loaded_bit_array) != expected_length:\n            logging.warning(f\"Loaded bit array length ({len(loaded_bit_array)}) does not match expected length ({expected_length}). Reinitializing and saving new state.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array\n            self.indices = np.zeros(self.size, dtype=np.int32)"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        expected_length = self.size\n        if len(bit_array) != expected_length:\n            logging.warning(f\"BloomFilter bit array length mismatch. Expected {expected_length}, but got {len(bit_array)}. Reinitializing and saving.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n\n        if len(loaded_bit_array) != self.size:\n            logging.warning(f\"BloomFilter: Expected bit array length {self.size} does not match loaded bit array length {len(loaded_bit_array)}. Reinitializing and saving.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        expected_length = self.size\n\n        if len(loaded_bit_array) != expected_length:\n            logging.warning(\"BloomFilter bit array length mismatch. Reinitializing and saving new state.\")\n            self.bit_array, self.indices = self.init_bit_array(expected_length)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n\n        if len(loaded_bit_array) != self.size:\n            logging.warning(f\"BloomFilter: Loaded bit array length ({len(loaded_bit_array)}) does not match expected length ({self.size}). \"\n                            f\"Reinitializing and saving new state.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        expected_length = self.size\n\n        if len(loaded_bit_array) != expected_length:\n            logging.warning(\"BloomFilter bit array length mismatch. Reinitializing and saving.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        expected_length = self.size\n\n        if len(loaded_bit_array) != expected_length:\n            logging.warning(f\"Loaded bit array length ({len(loaded_bit_array)}) does not match expected length ({expected_length}). \"\n                            f\"Reinitializing and saving new state.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        expected_length = self.size\n        if len(bit_array) != expected_length:\n            logging.warning(f\"Bloom filter bit array length mismatch: expected {expected_length}, got {len(bit_array)}. Reinitializing and saving.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        if loaded_bit_array is not None:\n            if len(loaded_bit_array) != self.size:\n                logging.warning(f\"BloomFilter: Loaded bit array length {len(loaded_bit_array)} does not match expected length {self.size}. Reinitializing and saving.\")\n                self.bit_array, self.indices = self.init_bit_array(self.size)\n                self.save()\n            else:\n                self.bit_array = loaded_bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        try:\n            self.bit_array, self.indices = self.persistence.load()\n        except Exception as e:\n            logging.warning(f\"Error loading bit array from persistence: {e}\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n\n        if len(self.bit_array) != self.size:\n            logging.warning(f\"Loaded bit array length ({len(self.bit_array)}) does not match expected size ({self.size}). Reinitializing and saving.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        if len(bit_array) != self.size:\n            logging.warning(f\"BloomFilter: Bit array length mismatch. Expected {self.size}, but got {len(bit_array)}. Reinitializing and saving.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = bit_array\n            self.indices = np.zeros(self.size, dtype=np.int32)"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n\n        expected_length = self.size\n        actual_length = len(loaded_bit_array)\n\n        if actual_length != expected_length:\n            logging.warning(f\"Bloom filter bit array length mismatch. Expected: {expected_length}, Actual: {actual_length}. Reinitializing and saving.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array\n            self.indices = np.zeros(self.size, dtype=np.int32)"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        expected_length = self.size\n        if len(loaded_bit_array) != expected_length:\n            logging.warning(f\"BloomFilter: Loaded bit array length ({len(loaded_bit_array)}) does not match expected length ({expected_length}). Reinitializing and saving.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array\n            self.indices = np.zeros(self.size, dtype=np.int32)\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        logging.info(\"Loading Bloom Filter from persistence.\")\n        loaded_bit_array = self.persistence.load()\n        expected_length = self.size\n        if len(loaded_bit_array) != expected_length:\n            logging.warning(f\"Loaded bit array length ({len(loaded_bit_array)}) does not match expected length ({expected_length}). Reinitializing Bloom Filter.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array\n            self.indices = np.zeros(self.size, dtype=np.int32)"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        try:\n            loaded_bit_array = self.persistence.load()\n\n            # Check if the loaded bit array's length matches the expected length\n            if len(loaded_bit_array) != self.size:\n                logging.warning(\n                    f\"BloomFilter: Loaded bit array length ({len(loaded_bit_array)}) does not match expected length ({self.size}). Reinitializing and saving.\")\n                self.bit_array, self.indices = self.init_bit_array(self.size)\n                self.save()\n            else:\n                self.bit_array = loaded_bit_array\n        except FileNotFoundError:\n            # If the file is not found, initialize a new bit array and save it\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        try:\n            loaded_bit_array = self.persistence.load()\n            expected_length = self.size\n            if len(loaded_bit_array) != expected_length:\n                logging.warning(f\"Loaded bit array length ({len(loaded_bit_array)}) does not match expected length ({expected_length}). \"\n                                f\"Potential data corruption. Reinitializing Bloom filter.\")\n                self.bit_array, self.indices = self.init_bit_array(self.size)\n                self.save()\n            else:\n                self.bit_array = loaded_bit_array\n                self.indices = np.zeros(self.size, dtype=np.int32)\n        except Exception as e:\n            logging.error(f\"Error loading bit array: {str(e)}\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if not self.bit_array[index]:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n\n    "}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if not self.bit_array[index]:\n                return False\n        return True"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.get_model_config(json_dict[DISTILLED_MODEL])\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if TEACHER_MODEL in json_dict:\n            self.teacher_models = [config_factory.get_model_config(teacher_model) for teacher_model in json_dict[TEACHER_MODEL]]\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.get_model_config(json_dict[DISTILLED_MODEL])\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if \"teacher_models\" in json_dict:\n            self.teacher_models = [config_factory.get_model_config(teacher_model) for teacher_model in json_dict[\"teacher_models\"]]\n        else:\n            self.teacher_models = [config_factory.get_model_config(teacher_model) for teacher_model in DEFAULT_TEACHER_MODEL_NAMES]\n        return self\n    "}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.get_config(json_dict[DISTILLED_MODEL])\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if \"teacher_models\" in json_dict:\n            self.teacher_models = [config_factory.get_config(teacher_model) for teacher_model in json_dict[\"teacher_models\"]]\n        else:\n            self.teacher_models = [config_factory.get_config(teacher_model) for teacher_model in DEFAULT_TEACHER_MODEL_NAMES]\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.get_model_config(json_dict[DISTILLED_MODEL])\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if \"teacher_models\" in json_dict:\n            self.teacher_models = [config_factory.get_model_config(teacher_model) for teacher_model in json_dict[\"teacher_models\"]]\n        else:\n            self.teacher_models = [config_factory.get_model_config(teacher_model) for teacher_model in DEFAULT_TEACHER_MODEL_NAMES]\n        return self\n    "}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        if DISTILLED_MODEL in json_dict:\n            self.distilled_model = config_factory.get_config(json_dict[DISTILLED_MODEL])\n        if \"current_model_stats\" in json_dict:\n            self.current_model_stats = json_dict[\"current_model_stats\"]\n        if \"last_training_run\" in json_dict:\n            self.last_training_run = json_dict[\"last_training_run\"]\n        if \"current_training_run\" in json_dict:\n            self.current_training_run = json_dict[\"current_training_run\"]\n        if \"nr_of_training_runs\" in json_dict:\n            self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if TEACHER_MODEL in json_dict:\n            self.teacher_models = [config_factory.get_config(teacher_model) for teacher_model in json_dict[TEACHER_MODEL]]\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.get_model_config(\n            json_dict[DISTILLED_MODEL])\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if \"teacher_models\" in json_dict:\n            self.teacher_models = [\n                config_factory.get_model_config(teacher_model)\n                for teacher_model in json_dict[\"teacher_models\"]\n            ]\n        else:\n            self.teacher_models = [\n                config_factory.get_model_config(teacher_model)\n                for teacher_model in DEFAULT_TEACHER_MODEL_NAMES\n            ]\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.get_model_config(json_dict[DISTILLED_MODEL])\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if \"teacher_models\" in json_dict:\n            self.teacher_models = [config_factory.get_model_config(teacher_model) for teacher_model in json_dict[\"teacher_models\"]]\n        else:\n            self.teacher_models = [config_factory.get_model_config(teacher_model) for teacher_model in DEFAULT_TEACHER_MODEL_NAMES]\n        return self\n\n    "}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.load_from_dict(json_dict[DISTILLED_MODEL])\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if TEACHER_MODEL in json_dict:\n            self.teacher_models = [config_factory.load_from_dict(teacher_model) for teacher_model in json_dict[TEACHER_MODEL]]\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.get_model_config(json_dict[\"distilled_model\"])\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if \"teacher_models\" in json_dict:\n            self.teacher_models = [config_factory.get_model_config(teacher_model) for teacher_model in json_dict[\"teacher_models\"]]\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        if json_dict.get(DISTILLED_MODEL):\n            self.distilled_model = config_factory.get_config(json_dict[DISTILLED_MODEL])\n        if json_dict.get(TEACHER_MODEL):\n            self.teacher_models = [config_factory.get_config(teacher_model) for teacher_model in json_dict[TEACHER_MODEL]]\n        if json_dict.get(\"current_model_stats\"):\n            self.current_model_stats = json_dict[\"current_model_stats\"]\n        if json_dict.get(\"last_training_run\"):\n            self.last_training_run = json_dict[\"last_training_run\"]\n        if json_dict.get(\"current_training_run\"):\n            self.current_training_run = json_dict[\"current_training_run\"]\n        if json_dict.get(\"nr_of_training_runs\"):\n            self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.load_from_dict(json_dict[DISTILLED_MODEL])\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if \"teacher_models\" in json_dict:\n            self.teacher_models = [config_factory.load_from_dict(teacher_model) for teacher_model in json_dict[\"teacher_models\"]]\n        else:\n            self.teacher_models = [config_factory.load_from_dict(teacher_model) for teacher_model in DEFAULT_TEACHER_MODELS]\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.get_model_config(json_dict[\"distilled_model\"][\"model_name\"], json_dict[\"distilled_model\"][\"model_type\"])\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if \"teacher_models\" in json_dict:\n            self.teacher_models = [config_factory.get_model_config(teacher_model[\"model_name\"], teacher_model[\"model_type\"]) for teacher_model in json_dict[\"teacher_models\"]]\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        if \"distilled_model\" in json_dict:\n            self.distilled_model = config_factory.get_config(json_dict[\"distilled_model\"])\n        if \"current_model_stats\" in json_dict:\n            self.current_model_stats = json_dict[\"current_model_stats\"]\n        if \"last_training_run\" in json_dict:\n            self.last_training_run = json_dict[\"last_training_run\"]\n        if \"current_training_run\" in json_dict:\n            self.current_training_run = json_dict[\"current_training_run\"]\n        if \"nr_of_training_runs\" in json_dict:\n            self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if \"teacher_models\" in json_dict:\n            self.teacher_models = [config_factory.get_config(teacher_model) for teacher_model in json_dict[\"teacher_models\"]]\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.create_config(json_dict[DISTILLED_MODEL])\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if \"teacher_models\" in json_dict:\n            self.teacher_models = [config_factory.create_config(teacher_model) for teacher_model in json_dict[\"teacher_models\"]]\n        else:\n            self.teacher_models = [config_factory.create_config(teacher_model) for teacher_model in DEFAULT_TEACHER_MODEL_NAMES]\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        if json_dict.get(DISTILLED_MODEL):\n            self.distilled_model = config_factory.get_model_config(json_dict.get(DISTILLED_MODEL))\n        if json_dict.get(\"current_model_stats\"):\n            self.current_model_stats = json_dict.get(\"current_model_stats\")\n        if json_dict.get(\"last_training_run\"):\n            self.last_training_run = json_dict.get(\"last_training_run\")\n        if json_dict.get(\"current_training_run\"):\n            self.current_training_run = json_dict.get(\"current_training_run\")\n        if json_dict.get(\"nr_of_training_runs\"):\n            self.nr_of_training_runs = json_dict.get(\"nr_of_training_runs\")\n        if json_dict.get(TEACHER_MODEL):\n            self.teacher_models = [config_factory.get_model_config(teacher_model_config) for teacher_model_config in json_dict.get(TEACHER_MODEL)]\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.create_model_config(json_dict[DISTILLED_MODEL])\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if \"teacher_models\" in json_dict:\n            self.teacher_models = [config_factory.create_model_config(teacher_model_config) for teacher_model_config in json_dict[\"teacher_models\"]]\n        else:\n            self.teacher_models = [config_factory.create_model_config(teacher_model_config) for teacher_model_config in DEFAULT_TEACHER_MODEL_NAMES]\n\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.create_config(\n            json_dict[\"distilled_model\"][\"model_name\"],\n            json_dict[\"distilled_model\"][\"model_type\"],\n            json_dict[\"distilled_model\"][\"model_config\"])\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if \"teacher_models\" in json_dict:\n            self.teacher_models = [\n                config_factory.create_config(\n                    teacher_model[\"model_name\"],\n                    teacher_model[\"model_type\"],\n                    teacher_model[\"model_config\"])\n                for teacher_model in json_dict[\"teacher_models\"]]\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        if json_dict is None:\n            return\n        if DISTILLED_MODEL in json_dict:\n            self.distilled_model = config_factory.get_config(\n                json_dict[DISTILLED_MODEL])\n        if \"current_model_stats\" in json_dict:\n            self.current_model_stats = json_dict[\"current_model_stats\"]\n        if \"last_training_run\" in json_dict:\n            self.last_training_run = json_dict[\"last_training_run\"]\n        if \"current_training_run\" in json_dict:\n            self.current_training_run = json_dict[\"current_training_run\"]\n        if \"nr_of_training_runs\" in json_dict:\n            self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if TEACHER_MODEL in json_dict:\n            self.teacher_models = [\n                config_factory.get_config(teacher_model)\n                for teacher_model in json_dict[TEACHER_MODEL]\n            ]\n\n        return self"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.load_from_dict(\n            json_dict[DISTILLED_MODEL])\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if \"teacher_models\" in json_dict:\n            self.teacher_models = [config_factory.load_from_dict(\n                teacher_model_dict) for teacher_model_dict in json_dict[\"teacher_models\"]]\n        else:\n            self.teacher_models = [config_factory.load_from_dict(\n                TEACHER_MODEL) for teacher_model_dict in json_dict[\"teacher_models\"]]\n        return self\n    "}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.get_config(\n            json_dict[\"distilled_model\"][\"model_name\"],\n            json_dict[\"distilled_model\"][\"model_type\"],\n            json_dict[\"distilled_model\"][\"model_path\"],\n            json_dict[\"distilled_model\"][\"model_class\"],\n            json_dict[\"distilled_model\"][\"model_kwargs\"],\n            json_dict[\"distilled_model\"][\"model_api_key\"],\n            json_dict[\"distilled_model\"][\"model_api_base\"],\n            json_dict[\"distilled_model\"][\"model_api_type\"],\n            json_dict[\"distilled_model\"][\"model_api_version\"],\n            json_dict[\"distilled_model\"][\"model_api_params\"],\n            json_dict[\"distilled_model\"][\"model_api_batch_size\"],\n            json_dict[\"distilled_model\"][\"model_api_max_tokens\"],\n            json_dict[\"distilled_model\"][\"model_api_temperature\"],\n            json_dict[\"distilled_model\"][\"model_api_top_p\"],\n            json_dict[\"distilled_model\"][\"model_api_frequency_penalty\"],\n            json_dict[\"distilled_model\"][\"model_api_presence_penalty\"],\n            json_dict[\"distilled_model\"][\"model_api_stop\"],\n            json_dict[\"distilled_model\"][\"model_api_best_of\"],\n            json_dict[\"distilled_model\"][\"model_api_n\"],\n            json_dict[\"distilled_model\"][\"model_api_logprobs\"],\n            json_dict[\"distilled_model\"][\"model_api_echo\"],\n            json_dict[\"distilled_model\"][\"model_api_stop_sequence\"],\n            json_dict[\"distilled_model\"][\"model_api_logit_bias\"],\n            json_dict[\"distilled_model\"][\"model_api_user\"],\n            json_dict[\"distilled_model\"][\"model_api_headers\"],\n            json_dict[\"distilled_model\"][\"model_api_timeout"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n        # check if the model name is valid\n        if not model.model_name:\n            raise ValueError(\"Model name is not set\")\n\n        # check if the prompt is valid\n        if not prompt:\n            raise ValueError(\"Prompt is not set\")\n\n        # check if the system message is valid\n        if not system_message:\n            raise ValueError(\"System message is not set\")\n\n        # check if the system message is valid\n        if not prompt:\n            raise ValueError(\"Prompt is not set\")\n\n        # check if the model name is valid\n        if not model.model_name:\n            raise ValueError(\"Model name is not set\")\n\n        # check if the prompt is valid\n        if not prompt:\n            raise ValueError(\"Prompt is not set\")\n\n        # check if the system message is valid\n        if not system_message:\n            raise ValueError(\"System message is not set\")\n\n        # check if the model name is valid\n        if not model.model_name:\n            raise ValueError(\"Model name is not set\")\n\n        # check if the prompt is valid\n        if not prompt:\n            raise ValueError(\"Prompt is not set\")\n\n        # check if the system message is valid\n        if not system_message:\n            raise ValueError(\"System message is not set\")\n\n        # check if the model name is valid\n        if not model.model_name:\n            raise ValueError(\"Model name is not set\")\n\n        # check if the prompt is valid\n        if not prompt:\n            raise ValueError(\"Prompt is not set\")\n\n        # check if the system message is valid\n        if not system_message:\n            raise ValueError(\"System message is not set\")\n\n        # check if the model name is valid\n        if not model.model_name:\n            raise ValueError(\"Model name is not set\")\n\n        # check if the prompt is valid\n        if not prompt:\n            raise ValueError(\"Prompt is not set\")\n\n        # check if the system message is valid\n        if not system_message:\n            raise ValueError(\"System message is not set\")"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # check if api key is not none\n        self.check_api_key()\n\n        # check if the model is valid\n        if not model.model_name:\n            raise ValueError(\"Model name is not set\")\n\n        # check if the system message is valid\n        if not system_message:\n            raise ValueError(\"System message is not set\")\n\n        # check if the prompt is valid\n        if not prompt:\n            raise ValueError(\"Prompt is not set\")\n\n        # check if the temperature is valid\n        if \"temperature\" in kwargs:\n            if not 0 <= kwargs[\"temperature\"] <= 1:\n                raise ValueError(\"Temperature must be between 0 and 1\")\n\n        # check if the top_p is valid\n        if \"top_p\" in kwargs:\n            if not 0 <= kwargs[\"top_p\"] <= 1:\n                raise ValueError(\"Top_p must be between 0 and 1\")\n\n        # check if the frequency_penalty is valid\n        if \"frequency_penalty\" in kwargs:\n            if not 0 <= kwargs[\"frequency_penalty\"] <= 1:\n                raise ValueError(\"Frequency_penalty must be between 0 and 1\")\n\n        # check if the presence_penalty is valid\n        if \"presence_penalty\" in kwargs:\n            if not 0 <= kwargs[\"presence_penalty\"] <= 1:\n                raise ValueError(\"Presence_penalty must be between 0 and 1\")\n\n        # check if the max_new_tokens is valid\n        if \"max_new_tokens\" in kwargs:\n            if not 0 <= kwargs[\"max_new_tokens\"] <= 4096:\n                raise ValueError(\"Max_new_tokens must be between 0 and 4096\")\n\n        # check if the n is valid\n        if \"n\" in kwargs:\n            if not 1 <= kwargs[\"n\"] <= 5:\n                raise ValueError(\"N must be between 1 and 5"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n        # validate the parameters\n        for parameter in LLM_GENERATION_PARAMETERS:\n            if parameter in kwargs:\n                # check if the parameter is a float\n                if isinstance(kwargs[parameter], float):\n                    # check if the parameter is between 0 and 1\n                    if not 0 <= kwargs[parameter] <= 1:\n                        raise ValueError(f\"{parameter} must be between 0 and 1\")\n                else:\n                    raise ValueError(f\"{parameter} must be a float\")\n\n        # check if the model is an OpenAIConfig\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"model must be an OpenAIConfig\")\n\n        # check if the system message is a string\n        if not isinstance(system_message, str):\n            raise ValueError(\"system_message must be a string\")\n\n        # check if the prompt is a string\n        if not isinstance(prompt, str):\n            raise ValueError(\"prompt must be a string\")\n\n        # check if the model name is a string\n        if not isinstance(model.model_name, str):\n            raise ValueError(\"model.model_name must be a string\")\n\n        # check if the model name is in the list of supported models\n        if model.model_name not in self.supported_models:\n            raise ValueError(f\"model.model_name must be one of {self.supported_models}\")\n\n        # check if the model name is in the list of supported models\n        if model.model_name not in self.supported_models:\n            raise ValueError(f\"model.model_name must be one of {self.supported_models}\")\n\n        # check if the model name is in the list of supported models\n        if model.model_name not in self.supported_models:\n            raise ValueError(f\"model.model_name must be one of {self.supported_models}\")\n\n        # check if the model name is in the list of supported models\n        if model.model_name not in self.supported_models:\n            raise ValueError("}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # check if any of the parameters are in the kwargs\n        for parameter in LLM_GENERATION_PARAMETERS:\n            if parameter in kwargs:\n                # if the parameter is in the kwargs, remove it\n                kwargs.pop(parameter)\n\n        # check if the model is an OpenAIConfig object\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"model must be an OpenAIConfig object\")\n\n        # check if the model has a model name\n        if not model.model_name:\n            raise ValueError(\"model must have a model name\")\n\n        # check if the model has a parsing helper token\n        if not model.parsing_helper_token:\n            raise ValueError(\"model must have a parsing helper token\")\n\n        # check if the model has a parsing helper token\n        if not model.parsing_helper_token:\n            raise ValueError(\"model must have a parsing helper token\")\n\n        # check if the model has a parsing helper token\n        if not model.parsing_helper_token:\n            raise ValueError(\"model must have a parsing helper token\")\n\n        # check if the model has a parsing helper token\n        if not model.parsing_helper_token:\n            raise ValueError(\"model must have a parsing helper token\")\n\n        # check if the model has a parsing helper token\n        if not model.parsing_helper_token:\n            raise ValueError(\"model must have a parsing helper token\")\n\n        # check if the model has a parsing helper token\n        if not model.parsing_helper_token:\n            raise ValueError(\"model must have a parsing helper token\")\n\n        # check if the model has a parsing helper token\n        if not model.parsing_helper_token:\n            raise ValueError(\"model must have a parsing helper token\")\n\n        # check if the model has a parsing helper token\n        if not model.parsing_helper_token:\n            raise ValueError(\"model must have a parsing helper token\")\n\n        # check if the model has a"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # Validate the parameters\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"Invalid model configuration\")\n        if not isinstance(system_message, str):\n            raise ValueError(\"Invalid system message\")\n        if not isinstance(prompt, str):\n            raise ValueError(\"Invalid prompt\")\n\n        # Set default values for optional parameters\n        temperature = kwargs.get(\"temperature\", 0.7)\n        top_p = kwargs.get(\"top_p\", 1)\n        frequency_penalty = kwargs.get(\"frequency_penalty\", 0)\n        presence_penalty = kwargs.get(\"presence_penalty\", 0)\n        max_new_tokens = kwargs.get(\"max_new_tokens\", 200)\n\n        # Validate the optional parameters\n        if not isinstance(temperature, (int, float)) or temperature < 0 or temperature > 1:\n            raise ValueError(\"Invalid temperature value\")\n        if not isinstance(top_p, (int, float)) or top_p < 0 or top_p > 1:\n            raise ValueError(\"Invalid top_p value\")\n        if not isinstance(frequency_penalty, (int, float)) or frequency_penalty < 0:\n            raise ValueError(\"Invalid frequency_penalty value\")\n        if not isinstance(presence_penalty, (int, float)) or presence_penalty < 0:\n            raise ValueError(\"Invalid presence_penalty value\")\n        if not isinstance(max_new_tokens, int) or max_new_tokens < 0:\n            raise ValueError(\"Invalid max_new_tokens value\")\n\n        # Set the parameters for the API request\n        params = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"frequency_"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # Validate the parameters\n        for param in kwargs:\n            if param not in LLM_GENERATION_PARAMETERS:\n                raise ValueError(f\"Invalid parameter: {param}\")\n\n        # Handle the API key verification\n        try:\n            # Try to generate the response\n            response = self.client.chat.completions.create(\n                model=model.model_name,\n                messages=[\n                    {\"role\": \"system\", \"content\": system_message},\n                    {\"role\": \"user\", \"content\": prompt},\n                ],\n                **kwargs\n            )\n        except Exception as e:\n            # If there's an error, log it and retry with exponential backoff\n            logging.error(f\"Error generating response: {e}\")\n            for attempt in range(5):\n                try:\n                    time.sleep(2 ** attempt)\n                    response = self.client.chat.completions.create(\n                        model=model.model_name,\n                        messages=[\n                            {\"role\": \"system\", \"content\": system_message},\n                            {\"role\": \"user\", \"content\": prompt},\n                        ],\n                        **kwargs\n                    )\n                    break\n                except Exception as e:\n                    logging.error(f\"Error generating response: {e}\")\n            else:\n                raise e\n\n        # Process the response to remove any parsing helper tokens\n        text = response.choices[0].message.content.strip()\n        for token in model.parsing_helper_tokens:\n            text = text.replace(token, \"\")\n\n        return text\n\n    "}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # validate parameters\n        for parameter in LLM_GENERATION_PARAMETERS:\n            if parameter in kwargs:\n                if not isinstance(kwargs[parameter], (int, float)):\n                    raise ValueError(f\"{parameter} must be a number\")\n\n        # handle api key verification\n        if not self.client:\n            self.client = OpenAI(api_key=self.api_key)\n\n        # handle retries\n        max_retries = 5\n        retry_count = 0\n        while retry_count < max_retries:\n            try:\n                # send the request to the OpenAI API\n                response = self.client.chat.completions.create(\n                    model=model.model_name,\n                    messages=[\n                        {\"role\": \"system\", \"content\": system_message},\n                        {\"role\": \"user\", \"content\": prompt},\n                    ],\n                    **kwargs\n                )\n                # process the response\n                response_text = response.choices[0].message.content\n                for token in model.parsing_helper_tokens:\n                    response_text = response_text.replace(token, \"\")\n                return response_text\n            except Exception as e:\n                logging.error(f\"Error: {e}\")\n                retry_count += 1\n                if retry_count < max_retries:\n                    time.sleep(2 ** retry_count)\n                else:\n                    raise e"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # check if api key is not none\n        self.check_api_key()\n\n        # validate the parameters\n        for parameter in kwargs:\n            if parameter not in LLM_GENERATION_PARAMETERS:\n                raise ValueError(f\"Invalid parameter: {parameter}\")\n\n        # create the request\n        request = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n            **kwargs,\n        }\n\n        # make the request\n        for attempt in range(5):\n            try:\n                response = requests.post(\n                    OPENAI_URL,\n                    headers={\"Authorization\": f\"Bearer {self.api_key}\"},\n                    json=request,\n                )\n                response.raise_for_status()\n                break\n            except requests.exceptions.RequestException as e:\n                # handle errors\n                if attempt == 4:\n                    raise e\n                else:\n                    time.sleep(2 ** attempt)\n\n        # process the response\n        response_text = response.json()[\"choices\"][0][\"message\"][\"content\"]\n        # remove any parsing helper tokens\n        response_text = response_text.replace(model.parsing_helper_tokens, \"\")\n        return response_text"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # validate the parameters\n        if \"temperature\" not in kwargs:\n            kwargs[\"temperature\"] = 0.5\n        if \"top_p\" not in kwargs:\n            kwargs[\"top_p\"] = 0.9\n        if \"max_new_tokens\" not in kwargs:\n            kwargs[\"max_new_tokens\"] = 2048\n        if \"frequency_penalty\" not in kwargs:\n            kwargs[\"frequency_penalty\"] = 0.0\n        if \"presence_penalty\" not in kwargs:\n            kwargs[\"presence_penalty\"] = 0.0\n\n        # remove the parsing helper tokens from the prompt\n        prompt = prompt.replace(model.start_token, \"\").replace(model.end_token, \"\")\n\n        # retry the request up to 5 times with exponential backoff\n        for i in range(5):\n            try:\n                response = self.client.chat.completions.create(\n                    model=model.model_name,\n                    messages=[\n                        {\"role\": \"system\", \"content\": system_message},\n                        {\"role\": \"user\", \"content\": prompt}\n                    ],\n                    **kwargs\n                )\n                break\n            except Exception as e:\n                logging.warning(f\"Error occurred while generating text: {e}\")\n                if i < 4:\n                    time.sleep(2 ** i)\n                else:\n                    raise e\n\n        # process the response to remove any parsing helper tokens\n        text = response.choices[0].message.content.strip()\n        text = text.replace(model.start_token, \"\").replace(model.end_token, \"\")\n\n        return text\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Check if API key is set\n        self.check_api_key()\n\n        # Check if model is set\n        if not model:\n            raise ValueError(\"Model is not set\")\n\n        # Check if system message is set\n        if not system_message:\n            raise ValueError(\"System message is not set\")\n\n        # Check if prompt is set\n        if not prompt:\n            raise ValueError(\"Prompt is not set\")\n\n        # Check if additional parameters are valid\n        for parameter in kwargs:\n            if parameter not in LLM_GENERATION_PARAMETERS:\n                raise ValueError(f\"Invalid parameter: {parameter}\")\n\n        # Retry up to 5 times with exponential backoff\n        for i in range(5):\n            try:\n                # Send request to OpenAI API\n                response = requests.post(\n                    OPENAI_URL,\n                    headers={\n                        \"Content-Type\": \"application/json\",\n                        \"Authorization\": f\"Bearer {self.api_key}\",\n                    },\n                    json={\n                        \"model\": model.model_name,\n                        \"messages\": [\n                            {\"role\": \"system\", \"content\": system_message},\n                            {\"role\": \"user\", \"content\": prompt},\n                        ],\n                        **kwargs,\n                    },\n                )\n\n                # Check if response is successful\n                if response.status_code != 200:\n                    raise Exception(f\"OpenAI API returned status code {response.status_code}\")\n\n                # Parse response\n                response_json = response.json()\n                generated_text = response_json[\"choices\"][0][\"message\"][\"content\"]\n\n                # Remove parsing helper tokens from generated text\n                generated_text = generated_text.replace(model.start_token, \"\").replace(model.end_token, \"\")\n\n                return generated_text\n            except Exception as e:\n                # Log error\n                logging.error(f\"Error in OpenAI API request: {e}\")\n\n                # Retry with exponential backoff\n                time.sleep((2 ** i) + random.random())\n\n       "}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Check if the API key is set\n        self.check_api_key()\n\n        # Validate the parameters\n        self.validate_parameters(**kwargs)\n\n        # Retry the API call up to 5 times with exponential backoff\n        for attempt in range(5):\n            try:\n                # Make the API call\n                response = self.client.chat.completions.create(\n                    model=model.model_name,\n                    messages=[\n                        {\"role\": \"system\", \"content\": system_message},\n                        {\"role\": \"user\", \"content\": prompt},\n                    ],\n                    **kwargs\n                )\n                # If the API call is successful, process the response and return the generated text\n                generated_text = response.choices[0].message.content\n                generated_text = generated_text.replace(model.parsing_helper_token, \"\")\n                return generated_text\n            except Exception as e:\n                # If the API call fails, log the error and wait for the next attempt\n                logging.error(f\"Error in OpenAI API call: {e}\")\n                time.sleep(2 ** attempt)\n\n        # If all attempts fail, raise an error\n        raise Exception(\"Failed to generate response from OpenAI API\")\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # Validate and process the parameters\n        for param in kwargs:\n            if param not in LLM_GENERATION_PARAMETERS:\n                raise ValueError(f\"Invalid parameter: {param}\")\n\n        # Set default values for optional parameters\n        kwargs.setdefault(\"temperature\", 0.7)\n        kwargs.setdefault(\"top_p\", 1)\n        kwargs.setdefault(\"frequency_penalty\", 0)\n        kwargs.setdefault(\"presence_penalty\", 0)\n        kwargs.setdefault(\"max_new_tokens\", 100)\n\n        # Set up the request payload\n        payload = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt},\n            ],\n            **kwargs,\n        }\n\n        # Make the API request\n        retry_count = 0\n        max_retries = 5\n        backoff_factor = 2\n        while True:\n            try:\n                response = requests.post(\n                    OPENAI_URL,\n                    headers={\"Authorization\": f\"Bearer {self.api_key}\"},\n                    json=payload,\n                )\n                response.raise_for_status()\n                break\n            except requests.exceptions.RequestException as e:\n                if retry_count < max_retries:\n                    retry_count += 1\n                    backoff_time = backoff_factor ** retry_count\n                    logging.warning(\n                        f\"Request failed ({e}). Retrying in {backoff_time} seconds...\"\n                    )\n                    time.sleep(backoff_time)\n                else:\n                    logging.error(f\"Request failed after {max_retries} retries.\")\n                    raise e\n\n        # Process the response\n        response_text = response.json()[\"choices\"][0][\"message\"][\"content\"]\n        # Remove any parsing helper tokens from the response\n        for token in model."}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # check if api key is not none\n        self.check_api_key()\n\n        # validate the parameters\n        for parameter in LLM_GENERATION_PARAMETERS:\n            if parameter not in kwargs:\n                kwargs[parameter] = None\n\n        # retry the request up to 5 times with exponential backoff\n        for i in range(5):\n            try:\n                # generate the response\n                response = self.client.chat.completions.create(\n                    model=model.model_name,\n                    messages=[\n                        {\"role\": \"system\", \"content\": system_message},\n                        {\"role\": \"user\", \"content\": prompt},\n                    ],\n                    **kwargs\n                )\n                # check if the response is valid\n                if response.choices:\n                    # process the response to remove any parsing helper tokens\n                    text = self.process_response(response.choices[0].message.content, model)\n                    return text\n                else:\n                    logging.warning(f\"No response from OpenAI API. Retrying in {2 ** i} seconds.\")\n                    time.sleep(2 ** i)\n            except Exception as e:\n                logging.warning(f\"An error occurred: {e}. Retrying in {2 ** i} seconds.\")\n                time.sleep(2 ** i)\n\n        # if the request fails after 5 retries, raise an error\n        raise ValueError(\"Failed to generate response from OpenAI API.\")\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # Validate the parameters\n        for param in kwargs:\n            if param not in LLM_GENERATION_PARAMETERS:\n                raise ValueError(f\"Invalid parameter: {param}\")\n\n        # Retry the request with exponential backoff up to 5 times\n        for i in range(5):\n            try:\n                # Send the request to the OpenAI API\n                response = requests.post(\n                    OPENAI_URL,\n                    headers={\"Authorization\": f\"Bearer {self.api_key}\"},\n                    json={\n                        \"model\": model.model_name,\n                        \"messages\": [\n                            {\"role\": \"system\", \"content\": system_message},\n                            {\"role\": \"user\", \"content\": prompt},\n                        ],\n                        **kwargs,\n                    },\n                )\n\n                # Check the status code of the response\n                if response.status_code != 200:\n                    raise Exception(f\"Request failed with status code {response.status_code}\")\n\n                # Parse the response\n                response_json = response.json()\n                choices = response_json[\"choices\"]\n                if not choices:\n                    raise Exception(\"No choices returned from OpenAI API\")\n\n                # Remove any parsing helper tokens from the response\n                text = choices[0][\"message\"][\"content\"].replace(model.parsing_helper_tokens, \"\")\n\n                # Return the final text\n                return text\n\n            except Exception as e:\n                # Log the error\n                logging.error(f\"Error occurred while generating text: {e}\")\n\n                # Wait for the next retry\n                time.sleep(2 ** i)\n\n        # If the request has not succeeded after 5 retries, raise an exception\n        raise Exception(\"Failed to generate text after 5 retries\")\n\n    "}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n        # Validate parameters\n        for param in kwargs:\n            if param not in LLM_GENERATION_PARAMETERS:\n                raise ValueError(f\"Invalid parameter: {param}\")\n\n        # Set default parameters\n        kwargs.setdefault(\"temperature\", 0.7)\n        kwargs.setdefault(\"top_p\", 1)\n        kwargs.setdefault(\"max_new_tokens\", 200)\n        kwargs.setdefault(\"frequency_penalty\", 0)\n        kwargs.setdefault(\"presence_penalty\", 0)\n\n        # Add the system message to the prompt\n        prompt = f\"{system_message}\\n{prompt}\"\n\n        # Set the model name\n        model_name = model.model_name\n\n        # Retry up to 5 times with exponential backoff\n        for i in range(5):\n            try:\n                # Generate a response from the OpenAI API\n                response = self.client.chat.completions.create(\n                    model=model_name,\n                    messages=[{\"role\": \"user\", \"content\": prompt}],\n                    **kwargs\n                )\n                # Extract the generated text from the response\n                generated_text = response.choices[0].message.content\n\n                # Remove any parsing helper tokens from the generated text\n                generated_text = generated_text.replace(model.parsing_helper_tokens, \"\")\n\n                # Return the generated text\n                return generated_text\n            except Exception as e:\n                logging.error(f\"Error generating text: {e}\")\n                if i < 4:\n                    sleep_time = 2 ** i\n                    logging.info(f\"Retrying in {sleep_time} seconds\")\n                    time.sleep(sleep_time)\n                else:\n                    raise e\n\n        return None\n\n    "}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n        # Validate the model\n        if not isinstance(model, OpenAIConfig):\n            raise ValueError(\"Invalid model configuration. Please provide an instance of OpenAIConfig.\")\n\n        # Validate the system message\n        if not isinstance(system_message, str):\n            raise ValueError(\"Invalid system message. Please provide a string.\")\n\n        # Validate the prompt\n        if not isinstance(prompt, str):\n            raise ValueError(\"Invalid prompt. Please provide a string.\")\n\n        # Validate and extract the generation parameters\n        generation_params = self.validate_generation_params(**kwargs)\n\n        # Validate the parsing helper tokens\n        if not isinstance(model.parsing_helper_tokens, list):\n            raise ValueError(\"Invalid parsing helper tokens. Please provide a list of strings.\")\n\n        # Retry logic with exponential backoff\n        max_retries = 5\n        retry_count = 0\n        while retry_count < max_retries:\n            try:\n                # Generate the response from the OpenAI API\n                response = self.client.chat.completions.create(\n                    model=model.model_name,\n                    messages=[\n                        {\"role\": \"system\", \"content\": system_message},\n                        {\"role\": \"user\", \"content\": prompt}\n                    ],\n                    **generation_params\n                )\n                # Extract the generated text from the response\n                generated_text = response.choices[0].message.content\n\n                # Process the generated text to remove any parsing helper tokens\n                processed_text = self.process_generated_text(generated_text, model.parsing_helper_tokens)\n\n                return processed_text\n            except Exception as e:\n                print(f\"An error occurred: {e}\")\n                if retry_count < max_retries - 1:\n                    # Exponential backoff\n                    sleep_time = 2 ** retry_count\n                    print(f\"Retrying in {sleep_time} seconds...\")\n                    time.sleep(sleep_time)\n                    retry_count += 1\n                else:\n                   "}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Check if the API key is set\n        self.check_api_key()\n\n        # Validate the parameters\n        self.validate_generation_parameters(**kwargs)\n\n        # Retry the API call up to 5 times with exponential backoff\n        for i in range(5):\n            try:\n                # Make the API call\n                response = self.client.chat.completions.create(\n                    model=model.model_name,\n                    messages=[\n                        {\"role\": \"system\", \"content\": system_message},\n                        {\"role\": \"user\", \"content\": prompt},\n                    ],\n                    **kwargs\n                )\n\n                # Process the response\n                text = response.choices[0].message.content\n                text = text.replace(model.parsing_helper_tokens.start, \"\")\n                text = text.replace(model.parsing_helper_tokens.end, \"\")\n\n                # Return the text\n                return text\n            except Exception as e:\n                # If there's an error, log it and retry after an exponential backoff\n                logging.error(f\"Error occurred during API call: {e}\")\n                time.sleep(2 ** i)\n\n        # If the API call fails after 5 retries, raise an exception\n        raise Exception(\"API call failed after 5 retries\")\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n        # check if any of the optional parameters are in the kwargs\n        if any(param in kwargs for param in LLM_GENERATION_PARAMETERS):\n            # if so, set them in the request\n            request_kwargs = {param: kwargs[param] for param in LLM_GENERATION_PARAMETERS if param in kwargs}\n        else:\n            # otherwise, set the default values\n            request_kwargs = {\n                \"temperature\": 0.7,\n                \"top_p\": 1,\n                \"max_new_tokens\": 100,\n                \"frequency_penalty\": 0,\n                \"presence_penalty\": 0,\n            }\n\n        # add the system message to the request\n        request_kwargs[\"messages\"] = [{\"role\": \"system\", \"content\": system_message}]\n\n        # add the user prompt to the request\n        request_kwargs[\"messages\"].append({\"role\": \"user\", \"content\": prompt})\n\n        # try to generate a response up to 5 times with exponential backoff\n        for i in range(5):\n            try:\n                # generate a response from the OpenAI API\n                response = self.client.chat.completions.create(\n                    model=model.model_name,\n                    **request_kwargs\n                )\n                # extract the text from the response\n                text = response.choices[0].message.content\n                # remove any parsing helper tokens from the text\n                for token in model.parsing_helper_tokens:\n                    text = text.replace(token, \"\")\n                # return the text\n                return text\n            except Exception as e:\n                # if there is an error, log it and wait for the next retry\n                logging.error(f\"Error generating response: {e}\")\n                time.sleep((2 ** i) + (2 ** (i + 1)))\n\n        # if all retries fail, raise an error\n        raise Exception(\"Failed to generate response after 5 retries\")\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n        # validate the parameters\n        if not model.model_name:\n            raise ValueError(\"Model name is not set\")\n        if not system_message:\n            raise ValueError(\"System message is not set\")\n        if not prompt:\n            raise ValueError(\"Prompt is not set\")\n\n        # Set default parameters\n        default_parameters = {\n            \"temperature\": 0.7,\n            \"top_p\": 1,\n            \"frequency_penalty\": 0,\n            \"presence_penalty\": 0,\n            \"max_new_tokens\": 200,\n            \"stop\": [\"\\n\\nHuman:\", \"\\n\\nAssistant:\"]\n        }\n        # update the default parameters with the provided parameters\n        default_parameters.update(kwargs)\n\n        # Set the model name\n        model_name = model.model_name\n\n        # Set the prompt\n        prompt = system_message + \"\\n\\nHuman: \" + prompt + \"\\n\\nAssistant:\"\n\n        # Set the parameters\n        parameters = {\n            \"model\": model_name,\n            \"prompt\": prompt,\n            \"temperature\": default_parameters[\"temperature\"],\n            \"top_p\": default_parameters[\"top_p\"],\n            \"frequency_penalty\": default_parameters[\"frequency_penalty\"],\n            \"presence_penalty\": default_parameters[\"presence_penalty\"],\n            \"max_tokens\": default_parameters[\"max_new_tokens\"],\n            \"stop\": default_parameters[\"stop\"]\n        }\n\n        # Set the number of retries\n        max_retries = 5\n        retry_count = 0\n\n        # Set the initial delay\n        delay = 1\n\n        # Set the maximum delay\n        max_delay = 60\n\n        # Set the response variable to None\n        response = None\n\n        # Loop until a response is received or the maximum number of retries is reached\n        while response is None and retry_count < max_retries:\n            try:\n                # Send the request to the OpenAI API"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Check if API key is set\n        self.check_api_key()\n\n        # Validate the parameters\n        for param in LLM_GENERATION_PARAMETERS:\n            if param in kwargs:\n                assert isinstance(kwargs[param], (int, float)), f\"{param} must be a number\"\n\n        # Set default parameters\n        kwargs[\"temperature\"] = kwargs.get(\"temperature\", 0.7)\n        kwargs[\"top_p\"] = kwargs.get(\"top_p\", 1)\n        kwargs[\"frequency_penalty\"] = kwargs.get(\"frequency_penalty\", 0)\n        kwargs[\"presence_penalty\"] = kwargs.get(\"presence_penalty\", 0)\n        kwargs[\"max_tokens\"] = kwargs.get(\"max_tokens\", 1024)\n\n        # Create the messages\n        messages = [\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n\n        # Retry the API call with exponential backoff\n        max_retries = 5\n        retry_delay = 1\n        for i in range(max_retries):\n            try:\n                # Call the API\n                response = self.client.chat.completions.create(\n                    model=model.model_name,\n                    messages=messages,\n                    **kwargs\n                )\n\n                # Process the response\n                text = response.choices[0].message.content\n\n                # Remove any parsing helper tokens\n                for token in model.parsing_helper_tokens:\n                    text = text.replace(token, \"\")\n\n                return text\n\n            except Exception as e:\n                # Handle any exceptions\n                logging.error(f\"Error: {e}\")\n\n                # Check if the error is a rate limit error\n                if \"rate limit\" in str(e).lower():\n                    # Wait for the specified amount of time before retrying\n                    time.sleep(retry_delay)\n                    retry_delay *"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T, atol=1e-5):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T, atol=1e-5):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T, atol=1e-5):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T, atol=1e-5):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T, atol=1e-13):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T, atol=1e-13):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must have a zero diagonal\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must have a zero diagonal\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must have a diagonal of zeros\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must have a diagonal of zeros\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must have a diagonal of zeros\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must have a zero diagonal\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix diagonal must be close to zero\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must have a zero diagonal\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must have a zero diagonal\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal elements of the matrix must be close to zero\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal elements of the matrix must be close to zero\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal elements of the matrix must be close to zero\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix diagonal must be close to zero\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must have zero diagonal elements\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must have a diagonal of zeros\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must have zero diagonal elements\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal elements of the matrix must be close to zero\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must have a diagonal of zeros\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must have a zero diagonal\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n    if not np.allclose(np.diag(x), np.zeros(x.shape[0])):\n        raise ValueError(\"The matrix must have a zero diagonal\")"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # check if the function is already initialized\n        if func_hash in self.initialized_functions:\n            # get the initialized function\n            initialized_function = self.initialized_functions[func_hash]\n            # get the model\n            model = initialized_function[\"model\"]\n            # get the examples\n            examples = initialized_function[\"examples\"]\n            # get the distilled model\n            distilled_model = initialized_function[\"distilled_model\"]\n        else:\n            # get the models\n            models = self.function_modeler.get_models(function_description)\n            # get the distilled model\n            distilled_model = models[0]\n            # get the teacher models\n            teacher_models = models[1]\n            # get the examples\n            examples = self.function_modeler.get_symbolic_alignments(func_hash, max=5)\n            # get the model\n            model = self.choose_model_from_tokens(teacher_models, 0)\n            # initialize the function\n            self.initialized_functions[func_hash] = {\"model\": model, \"examples\": examples, \"distilled_model\": distilled_model}\n\n        # check if the function is already initialized\n        if func_hash in self.token_counts:\n            # get the token count\n            token_count = self.token_counts[func_hash]\n        else:\n            # get the token count\n            token_count = 0\n\n        # check if the function is already initialized\n        if func_hash in self.initialized_functions:\n            # get the initialized function\n            initialized_function = self.initialized_functions[func_hash]\n            # get the model\n            model = initialized_function[\"model\"]\n            # get the examples\n            examples = initialized_function[\"examples\"]\n            # get the distilled model\n            distilled_model = initialized_function[\"distilled_model\"]\n        else:\n            # get the models\n            models = self.function_modeler.get_models(function_description)\n            # get the distilled model\n            distilled_model = models[0]\n            # get the teacher models"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the alignments\n        aligns = self.function_modeler.get_symbolic_alignments(func_hash, max=5)\n        # get the models\n        distilled_models, teacher_models = self.function_modeler.get_models(function_description)\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"examples\": [], \"model\": \"\"}\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"examples\": [], \"model\": \"\"}\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"examples\": [], \"model\": \"\"}\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"examples\": [], \"model\": \"\"}\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"examples\": [], \"model\": \"\"}\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"examples\": [], \"model\": \"\"}\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"examples\": [], \"model\": \"\"}\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"examples\": [], \"model\": \"\"}\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"examples\": [],"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the models\n        models = self.function_modeler.get_models(function_description)\n        distilled_model = models[0]\n        teacher_models = models[1]\n        # get the alignments\n        aligns = self.function_modeler.get_symbolic_alignments(func_hash, max=5)\n        # get the function description\n        f = str(function_description.__dict__.__repr__() + \"\\n\")\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"examples\": [], \"model\": \"\"}\n        # check if the function is already initialized\n        if func_hash not in self.token_counts:\n            self.token_counts[func_hash] = {\"distilled_model\": 0, \"teacher_model\": 0}\n        # check if finetunable\n        suitable_for_finetune, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs, f, distilled_model)\n        # check if the model is suitable for distillation\n        if suitable_for_finetune:\n            # if it is, use the distilled model\n            model = distilled_model\n            self.token_counts[func_hash][\"distilled_model\"] += input_prompt_token_count\n            # check if the function is already initialized\n            if func_hash not in self.initialized_functions:\n                self.initialized_functions[func_hash] = {\"examples\": [], \"model\": \"\"}\n            # check if the function is already initialized\n            if func_hash not in self.token_counts:\n                self.token_counts[func_hash] = {\"distilled_model\": 0, \"teacher_model\": 0}\n            # check if the function is already initialized\n            if func_hash not in self.initialized_functions:\n                self.initialized_functions[func_hash] = {\"examples\": [], \"model\": \"\"}\n            #"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the models\n        models = self.function_modeler.get_models(function_description)\n        distilled_model = models[0]\n        teacher_model = models[1]\n\n        # get the token counts\n        examples = self.function_modeler.get_symbolic_alignments(func_hash, max=5)\n        examples_token_count = sum([approximate_token_count(example.inputs) for example in examples])\n        input_prompt_token_count = approximate_token_count(f\"Function: {function_description}\\n---\\nInputs:\\nArgs: {args}\\nKwargs: {kwargs}\\nOutput:\")\n        generation_tokens = llm_parameters.get(\"max_new_tokens\", self.default_generation_length)\n\n        # check if finetunable\n        finetuning_prompt = f\"Function: {function_description}\\n---\\nInputs:\\nArgs: {args}\\nKwargs: {kwargs}\\nOutput:\"\n        input_prompt_token_count = approximate_token_count(finetuning_prompt)\n        if distilled_model.system_message_token_count < 0:\n            distilled_model.system_message_token_count = approximate_token_count(distilled_model.system_message)\n        if distilled_model.instruction_token_count < 0:\n            distilled_model.instruction_token_count = approximate_token_count(distilled_model.instructions)\n\n        suitable_for_finetune = input_prompt_token_count + distilled_model.instruction_token_count + distilled_model.system_message_token_count < distilled_model.context_length\n\n        # check if the model is suitable for distillation\n        distilled_model_suitable_for_distillation = self.suitable_for_distillation(args, kwargs, function_description, distilled_model)\n        if distilled_model_suitable_for_distillation:\n            # if the model is suitable for distillation"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the models\n        distilled_models, teacher_models = self.function_modeler.get_models(function_description)\n        # get the alignments\n        aligns = self.function_modeler.get_symbolic_alignments(function_hash, max=5)\n        # get the function description\n        f = str(function_description.__dict__.__repr__() + \"\\n\")\n        # check if the function is initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n        # check if the function is already initialized\n        if self.initialized_functions[func_hash][\"model\"] != \"\":\n            model = self.api_provider[self.initialized_functions[func_hash][\"model\"]]\n            prompt = self.construct_prompt(f, args, kwargs, self.initialized_functions[func_hash][\"examples\"], model)\n            return prompt, model, False, False\n        # check if the function is suitable for distillation\n        suitable_for_distillation, input_prompt_token_count = self.suitable_for_distillation_token_check(args, kwargs, f, distilled_models)\n        if suitable_for_distillation:\n            # if it is, use the distilled model\n            model = self.api_provider[distilled_models[0].model_name]\n            prompt = self.construct_prompt(f, args, kwargs, [], model)\n            return prompt, model, True, True\n        # check if the function is suitable for fine-tuning\n        suitable_for_finetune, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs, f, teacher_models[0])\n        if suitable_for_finetune:\n            # if it is, use the teacher model\n            model = self.api_provider[teacher_models[0].model_name]\n            prompt = self.construct_prompt(f,"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the alignments\n        aligns = self.function_modeler.get_symbolic_alignments(func_hash, max=5)\n        # get the distilled models\n        distilled_models = self.function_modeler.get_models(function_description)[0]\n        # get the teacher models\n        teacher_models = self.function_modeler.get_models(function_description)[1]\n\n        # get the token counts\n        examples = [f\"Inputs:\\nArgs: {align['args']}\\nKwargs: {align['kwargs']}\\nOutput: {align['output']}\" for align in\n                 aligns]\n        examples_token_count = sum([approximate_token_count(example) for example in examples])\n        input_prompt_token_count = approximate_token_count(f\"Function: {function_description.__dict__.__repr__()}\\n---\\nInputs:\\nArgs: {args}\\nKwargs: {kwargs}\\nOutput:\")\n        generation_tokens = llm_parameters.get(\"max_new_tokens\", self.default_generation_length)\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\n                \"model\": \"\",\n                \"examples\": []\n            }\n            self.token_counts[func_hash] = {\n                \"input_prompt_token_count\": input_prompt_token_count,\n                \"examples_token_count\": examples_token_count,\n                \"generation_tokens\": generation_tokens,\n            }\n\n        # check if finetunable\n        finetuning_prompt = f\"Function: {function_description.__dict__.__repr__()}\\n---\\nInputs:\\nArgs: {args}\\nKwargs: {kwargs}\\nOutput:\"\n        input_prompt_token_count = approximate_token_count(finetuning_prompt)\n        suitable_for_finetune, input_prompt_token_count = self.suitable_for_fin"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the models\n        models = self.function_modeler.get_models(function_description)\n        # get the alignments\n        aligns = self.function_modeler.get_symbolic_alignments(func_hash, max=5)\n        # get the function description\n        f = str(function_description.__dict__.__repr__() + \"\\n\")\n        # get the generation parameters\n        generation_tokens = llm_parameters.get(\"max_new_tokens\", self.default_generation_length)\n        # get the distilled model\n        distilled_model = models[0]\n        # get the teacher model\n        teacher_model = models[1]\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize it\n            self.initialized_functions[func_hash] = {\n                \"model\": \"\",\n                \"examples\": []\n            }\n        # check if the function is already initialized\n        if func_hash not in self.token_counts:\n            # if not, initialize it\n            self.token_counts[func_hash] = {\n                \"distilled_model\": 0,\n                \"teacher_model\": 0\n            }\n        # get the current token count\n        current_token_count = self.token_counts[func_hash]\n        # check if the function is already initialized\n        if func_hash in self.initialized_functions:\n            # if it is, get the examples\n            examples = self.initialized_functions[func_hash][\"examples\"]\n            # get the model\n            model = self.initialized_functions[func_hash][\"model\"]\n        else:\n            # if not, initialize it\n            examples = []\n            model = \"\"\n        # check if the function is already initialized\n        if func_hash in self.token_counts:\n            # if it is, get the token count\n            current_token_count = self.token_counts[func_hash]\n        else:\n            # if not, initialize it\n            current_token_count = {\n                \""}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the distilled model\n        distilled_model = self.function_modeler.get_models(function_description)[0]\n        # get the teacher models\n        teacher_models = self.function_modeler.get_models(function_description)[1]\n        # get the alignments\n        aligns = self.function_modeler.get_symbolic_alignments(function_description.__hash__(), max=5)\n        # get the function description\n        f = str(function_description.__dict__.__repr__() + \"\\n\")\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize it\n            self.initialized_functions[func_hash] = {\"examples\": [], \"model\": \"\"}\n\n        # check if the function is already initialized\n        if func_hash not in self.token_counts:\n            # if not, initialize it\n            self.token_counts[func_hash] = {\"examples\": [], \"model\": \"\"}\n\n        # check if finetunable\n        suitable_for_finetune, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs, f, distilled_model)\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize it\n            self.initialized_functions[func_hash] = {\"examples\": [], \"model\": \"\"}\n            # check if the function is already initialized\n        if func_hash not in self.token_counts:\n            # if not, initialize it\n            self.token_counts[func_hash] = {\"examples\": [], \"model\": \"\"}\n\n        # check if finetunable\n        suitable_for_finetune, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs, f, distilled_model)\n        # if finetunable, use the distilled model\n        if suitable_for_finetune:\n            model = dist"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the models\n        models = self.function_modeler.get_models(function_description)\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize it\n            self.initialized_functions[func_hash] = {\n                \"model\": \"\",\n                \"examples\": []\n            }\n            # check if the function is suitable for distillation\n            # this is a bit hacky, but it works for now\n            # if the function is not suitable for distillation, then the teacher model is used\n            # if the function is suitable for distillation, then the distilled model is used\n            # if the function is not suitable for distillation, then the teacher model is used\n            # if the function is suitable for distillation, then the distilled model is used\n            # if the function is not suitable for distillation, then the teacher model is used\n            # if the function is suitable for distillation, then the distilled model is used\n            # if the function is not suitable for distillation, then the teacher model is used\n            # if the function is suitable for distillation, then the distilled model is used\n            # if the function is not suitable for distillation, then the teacher model is used\n            # if the function is suitable for distillation, then the distilled model is used\n            # if the function is not suitable for distillation, then the teacher model is used\n            # if the function is suitable for distillation, then the distilled model is used\n            # if the function is not suitable for distillation, then the teacher model is used\n            # if the function is suitable for distillation, then the distilled model is used\n            # if the function is not suitable for distillation, then the teacher model is used\n            # if the function is suitable for distillation, then the distilled model is used\n            # if the function is not suitable for distillation, then the teacher model is used\n            # if the function is suitable for distillation, then the distilled model is used\n            # if the function is not suitable for distillation, then the teacher model is used\n            #"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # check if the function is already initialized\n        if func_hash in self.initialized_functions:\n            model = self.initialized_functions[func_hash][\"model\"]\n            examples = self.initialized_functions[func_hash][\"examples\"]\n            distilled_model = self.initialized_functions[func_hash][\"distilled_model\"]\n            # check if the function is already initialized and does not require saving examples for fine-tuning\n            if self.initialized_functions[func_hash][\"initialized\"]:\n                return self.construct_prompt(str(function_description.__dict__.__repr__() + \"\\n\"), args, kwargs, examples, model), model, distilled_model, True\n        else:\n            # initialize the function\n            self.initialized_functions[func_hash] = {\n                \"model\": \"\",\n                \"examples\": [],\n                \"initialized\": False,\n                \"distilled_model\": False\n            }\n\n        # get the models\n        models = self.function_modeler.get_models(function_description)\n        # get the distilled model\n        distilled_model = models[0]\n        # check if the function is suitable for distillation\n        suitable_for_distillation, input_prompt_token_count = self.suitable_for_distillation_token_check(args, kwargs, f, distilled_model)\n        if suitable_for_distillation:\n            # if it is, use the distilled model\n            model = distilled_model\n            self.initialized_functions[func_hash][\"distilled_model\"] = True\n            # check if the function is already initialized and does not require saving examples for fine-tuning\n            if self.initialized_functions[func_hash][\"initialized\"]:\n                return self.construct_prompt(str(function_description.__dict__.__repr__() + \"\\n\"), args, kwargs, [], model), model, distilled_model, True\n        else:\n            # if it is not, use the teacher model\n            model = models[1][0]\n            # check if"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the models\n        models = self.function_modeler.get_models(function_description)\n        # get the alignments\n        aligns = self.function_modeler.get_symbolic_alignments(func_hash, max=5)\n        # get the function description\n        f = str(function_description.__dict__.__repr__() + \"\\n\")\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize it\n            self.initialized_functions[func_hash] = {\n                \"model\": \"\",\n                \"examples\": [],\n                \"suitable_for_finetuning\": False,\n                \"suitable_for_distillation\": False\n            }\n            self.token_counts[func_hash] = 0\n        # check if the function is already initialized\n        if func_hash in self.initialized_functions:\n            # if it is, update the examples\n            self.initialized_functions[func_hash][\"examples\"] = aligns\n        # check if the function is suitable for distillation\n        self.initialized_functions[func_hash][\"suitable_for_distillation\"] = self.suitable_for_distillation(args, kwargs, f, models[0])\n        # check if the function is suitable for finetuning\n        self.initialized_functions[func_hash][\"suitable_for_finetuning\"], input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs, f, models[1])\n        self.token_counts[func_hash] += input_prompt_token_count\n        # check if the function is suitable for distillation\n        if self.initialized_functions[func_hash][\"suitable_for_distillation\"]:\n            # if it is, use the distilled model\n            prompt = self.construct_prompt(f, args, kwargs, aligns, models[0])\n            model = models[0]\n            save_to_finet"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the models\n        models = self.function_modeler.get_models(function_description)\n        # check if finetunable\n        distilled_model = models[0]\n        if not self.initialized_functions.get(func_hash, None):\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n        # check if finetunable\n        suitable_for_finetune, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs, f, distilled_model)\n        if suitable_for_finetune:\n            # if it's finetunable, use the finetuned model\n            model = distilled_model\n            prompt = self.construct_prompt(f, args, kwargs, self.initialized_functions[func_hash][\"examples\"], model)\n            return prompt, model, True, False\n        else:\n            # if it's not finetunable, use the teacher model\n            model = models[1][0]\n            # check if the function is already initialized\n            if not self.initialized_functions[func_hash][\"examples\"]:\n                # if it's not, initialize it\n                prompt = self.construct_prompt(f, args, kwargs, self.initialized_functions[func_hash][\"examples\"], model)\n                return prompt, model, False, False\n            else:\n                # if it is, update the examples\n                aligns = self.function_modeler.get_symbolic_alignments(func_hash, max=5)\n                self.initialized_functions[func_hash][\"examples\"] = [f\"Inputs:\\nArgs: {align['args']}\\nKwargs: {align['kwargs']}\\nOutput: {align['output']}\" for align in aligns]\n                prompt = self.construct_prompt(f, args, kwargs, self.initialized_functions[func_hash][\"examples\"], model)\n                return prompt, model, False, False\n\n    "}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the alignments\n        aligns = self.function_modeler.get_symbolic_alignments(func_hash, max=5)\n        # get the distilled models\n        distilled_models = self.function_modeler.get_models(function_description)[0]\n        # get the teacher models\n        teacher_models = self.function_modeler.get_models(function_description)[1]\n        # get the model to use\n        model = self.choose_model(args, kwargs, function_description, distilled_models, teacher_models, llm_parameters, aligns)\n        # get the function description\n        f = str(function_description.__dict__.__repr__() + \"\\n\")\n        # get the examples\n        examples = [f\"Inputs:\\nArgs: {align['args']}\\nKwargs: {align['kwargs']}\\nOutput: {align['output']}\" for align in\n                   aligns]\n        # get the prompt\n        prompt = self.construct_prompt(f, args, kwargs, examples, model)\n        # get the save to finetune\n        save_to_finetune = False\n        is_distilled_model = False\n        return prompt, model, save_to_finetune, is_distilled_model\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the models\n        distilled_model, teacher_models = self.function_modeler.get_models(function_description)\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n        # check if the function is already initialized\n        if func_hash not in self.token_counts:\n            self.token_counts[func_hash] = {\"distilled\": 0, \"teacher\": 0}\n\n        # get the examples\n        examples = self.function_modeler.get_symbolic_alignments(func_hash)\n        # get the function description\n        f = str(function_description.__dict__.__repr__() + \"\\n\")\n\n        # check if the function is suitable for distillation\n        suitable_for_distillation, input_prompt_token_count = self.suitable_for_distillation_token_check(args, kwargs, f, distilled_model)\n        if suitable_for_distillation:\n            self.token_counts[func_hash][\"distilled\"] += input_prompt_token_count\n            prompt = self.construct_prompt(f, args, kwargs, examples, distilled_model)\n            return prompt, distilled_model, False, True\n        else:\n            # check if the function is suitable for fine-tuning\n            suitable_for_finetune, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs, f, distilled_model)\n            if suitable_for_finetune:\n                self.token_counts[func_hash][\"teacher\"] += input_prompt_token_count\n                self.initialized_functions[func_hash][\"examples\"].append({\"args\": args, \"kwargs\": kwargs, \"output\": f\"{choice}\"})\n                prompt = self.construct_prompt(f, args, kwargs, examples, distilled_model)\n               "}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the alignments\n        aligns = self.function_modeler.get_symbolic_alignments(func_hash, max=5)\n        # get the models\n        distilled_model, teacher_models = self.function_modeler.get_models(function_description)\n        # check if finetunable\n        suitable_for_finetune, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs, f, distilled_model)\n        # if the model is suitable for finetuning, use the distilled model\n        if suitable_for_finetune:\n            model = distilled_model\n        else:\n            # if not, use the teacher model\n            model = self.choose_model_from_tokens(teacher_models, input_prompt_token_count, len(aligns))\n            if not model:\n                # if no model is found, use the distilled model\n                model = distilled_model\n        # if the model is not suitable for finetuning, use the distilled model\n        if not suitable_for_finetune:\n            is_distilled_model = True\n        else:\n            is_distilled_model = False\n        # if the model is the distilled model, use the distilled model\n        if model == distilled_model:\n            is_distilled_model = True\n        # if the model is not the distilled model, use the teacher model\n        else:\n            is_distilled_model = False\n        # if the function is not initialized, initialize it\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\n                \"model\": \"\",\n                \"examples\": []\n            }\n        # if the function is not initialized, initialize it\n        if func_hash not in self.token_counts:\n            self.token_counts[func_hash] = {\n                \"model\": \"\",\n                \"token_count\": 0\n            }\n        # update the token count\n        self.token_counts[func_hash][\"token_"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the models\n        models = self.function_modeler.get_models(function_description)\n        # get the alignments\n        aligns = self.function_modeler.get_symbolic_alignments(func_hash, max=5)\n        # get the function description\n        f = str(function_description.__dict__.__repr__() + \"\\n\")\n        # get the distilled model\n        distilled_model = models[0]\n        # get the teacher model\n        teacher_model = models[1]\n        # get the generation parameters\n        generation_tokens = llm_parameters.get(\"max_new_tokens\", self.default_generation_length)\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize it\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # check if the function is already initialized\n        if func_hash in self.initialized_functions:\n            # if it is, check if it's already initialized with the distilled model\n            if self.initialized_functions[func_hash][\"model\"] == distilled_model.model_name:\n                # if it is, check if it's already initialized with the correct number of examples\n                if len(self.initialized_functions[func_hash][\"examples\"]) == 0:\n                    # if it is not, update the examples\n                    self.initialized_functions[func_hash][\"examples\"] = aligns\n                # if it is, check if the inputs are suitable for finetuning\n                suitable_for_finetune, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs, f, distilled_model)\n                if not suitable_for_finetune:\n                    # if not, check if the inputs are suitable for finetuning with the teacher model\n                    suitable_for_finetune, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs,"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the distilled models\n        distilled_models = self.function_modeler.get_models(function_description)[0]\n        # get the teacher models\n        teacher_models = self.function_modeler.get_models(function_description)[1]\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize it\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n            # check if the function is suitable for distillation\n            if self.function_modeler.check_suitable_for_distillation(function_description):\n                # if it is, choose the model with the lowest token count\n                model = self.choose_model_from_tokens(distilled_models, 0)\n                # if there is no model with the lowest token count, then the function is not suitable for distillation\n                if not model:\n                    # if there is no model with the lowest token count, then the function is not suitable for distillation\n                    raise ValueError(\n                        f\"Function {function_description.name} is not suitable for distillation.\")\n                # construct the prompt\n                prompt = self.construct_prompt(str(function_description.__dict__.__repr__() + \"\\n\"), args, kwargs, [], model)\n                # set the model to the distilled model\n                model = model\n                # set the save to finetune to False\n                save_to_finetune = False\n                # set the is_distilled_model to True\n                is_distilled_model = True\n            else:\n                # if it is not, choose the model with the lowest token count\n                model = self.choose_model_from_tokens(teacher_models, 0)\n                # if there is no model with the lowest token count, then the function is not suitable for distillation\n                if not model:\n                    # if there is no model with the lowest token count, then the function is not suitable for distillation\n                    raise ValueError(\n                        f\"Function {function_description.name} is not suitable for"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # check if the function is already initialized\n        if func_hash in self.initialized_functions:\n            # get the model\n            model = self.initialized_functions[func_hash][\"model\"]\n            # get the examples\n            examples = self.initialized_functions[func_hash][\"examples\"]\n            # get the model\n            model = self.api_provider[model.provider]\n            # construct the prompt\n            prompt = self.construct_prompt(function_description, args, kwargs, examples, model)\n            # check if the model is suitable for distillation\n            suitable_for_distillation = self.suitable_for_distillation(args, kwargs, function_description, model)\n            # check if the model is suitable for finetuning\n            suitable_for_finetuning, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs, function_description, model)\n            # check if the model is suitable for finetuning\n            if suitable_for_distillation:\n                # if it is, then use the distilled model\n                model = self.function_modeler.get_distilled_model(function_description)\n                # set the model to the distilled model\n                self.initialized_functions[func_hash][\"model\"] = model\n                # set the model to the distilled model\n                self.initialized_functions[func_hash][\"distilled_model\"] = True\n                # set the model to the distilled model\n                self.initialized_functions[func_hash][\"distilled_model_token_count\"] = input_prompt_token_count\n            # if it's not, then use the teacher model\n            else:\n                # get the teacher models\n                teacher_models = self.function_modeler.get_models(function_description)[1]\n                # check if the model is suitable for finetuning\n                if suitable_for_finetuning:\n                    # if it is, then use the teacher model\n                    model = self.choose_model_from_tokens(teacher_models, input_prom"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the models\n        distilled_model, teacher_models = self.function_modeler.get_models(function_description)\n        # check if finetunable\n        finetuning_prompt = f\"Function: {function_description.__dict__.__repr__()}\\n---\\nInputs:\\nArgs: {args}\\nKwargs: {kwargs}\\nOutput:\"\n        input_prompt_token_count = approximate_token_count(finetuning_prompt)\n        if distilled_model.system_message_token_count < 0:\n            distilled_model.system_message_token_count = approximate_token_count(distilled_model.system_message)\n        if distilled_model.instruction_token_count < 0:\n            distilled_model.instruction_token_count = approximate_token_count(distilled_model.instructions)\n        suitable_for_finetune, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs, function_description.__dict__.__repr__(), distilled_model)\n\n        # check if already initialized\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n        if not self.initialized_functions[func_hash][\"examples\"]:\n            # if not initialized, initialize\n            self.initialized_functions[func_hash][\"examples\"] = self.function_modeler.get_symbolic_alignments(func_hash)\n        # get the examples\n        examples = self.initialized_functions[func_hash][\"examples\"]\n        # get the model\n        model = self.choose_model_from_tokens(teacher_models, input_prompt_token_count, len(examples))\n        # check if model is suitable for finetuning\n        if suitable_for_finetune:\n            model = distilled_model\n        # construct the prompt\n        f = str(function_description.__dict__.__repr__() + \"\\n\")\n        prompt ="}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the models\n        distilled_model, teacher_models = self.function_modeler.get_models(function_description)\n\n        # get the alignments\n        aligns = self.function_modeler.get_symbolic_alignments(func_hash, max=5)\n\n        # check if finetunable\n        suitable_for_finetune, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs, f, distilled_model)\n\n        # get the function setup\n        function_setup = self.initialized_functions.get(func_hash, None)\n        if not function_setup:\n            # if it's not initialized, initialize it\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n            function_setup = self.initialized_functions[func_hash]\n\n        # check if the function is already initialized\n        if function_setup[\"model\"] == \"\":\n            # if it's not initialized, initialize it\n            if not suitable_for_finetune:\n                # if it's not suitable for finetuning, use the teacher model\n                model = self.choose_model_from_tokens(teacher_models, input_prompt_token_count)\n                if not model:\n                    raise ValueError(\n                        f\"Input is too long for any of the teacher models, consider using a distilled model or a smaller input.\")\n                is_distilled_model = False\n                save_to_finetune = False\n            else:\n                # if it's suitable for finetuning, use the distilled model\n                model = distilled_model\n                is_distilled_model = True\n                save_to_finetune = True\n        else:\n            # if it's initialized, use the model that was initialized\n            if function_setup[\"model\"] == distilled_model.model_name:\n                # if it's the distilled model, use the distilled model\n                model = distilled_model\n                is_distilled_model = True\n                save_to_fin"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        cov_ = cov.copy()\n        for _ in range(higham_max_iteration):\n            cov_ = (cov_ + cov_.T) / 2\n            eigvals, eigvecs = np.linalg.eigh(cov_)\n            eigvals[eigvals < _CLIPPING_VALUE] = _CLIPPING_VALUE\n            cov_ = eigvecs @ np.diag(eigvals) @ eigvecs.T\n            if is_positive_definite(cov_):\n                return cov_\n        raise ValueError(\"Higham's algorithm did not converge\")\n\n    cov_ = cov.copy()\n    eigvals, eigvecs = np.linalg.eigh(cov_)\n    eigvals[eigvals < _CLIPPING_VALUE] = _CLIPPING_VALUE\n    cov_ = eigvecs @ np.diag(eigvals) @ eigvecs.T\n    return cov_"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        cov_ = cov.copy()\n        for _ in range(higham_max_iteration):\n            cov_ = (cov_ + cov_.T) / 2\n            try:\n                np.linalg.cholesky(cov_)\n                break\n            except np.linalg.LinAlgError:\n                eigs, eigv = np.linalg.eigh(cov_)\n                eigs[eigs < _CLIPPING_VALUE] = _CLIPPING_VALUE\n                cov_ = eigv @ np.diag(eigs) @ eigv.T\n    else:\n        eigs, eigv = np.linalg.eigh(cov)\n        eigs[eigs < _CLIPPING_VALUE] = _CLIPPING_VALUE\n        cov_ = eigv @ np.diag(eigs) @ eigv.T\n    return cov_"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if not is_positive_definite(cov):\n        if higham:\n            cov = higham_nearest(cov, max_iteration=higham_max_iteration)\n        else:\n            cov = cov_nearest_corr(cov)\n    return cov\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if not higham:\n        # Clip the eigenvalues of the covariance matrix to ensure it is positive definite\n        eigvals, eigvecs = np.linalg.eigh(cov)\n        eigvals[eigvals < _CLIPPING_VALUE] = _CLIPPING_VALUE\n        cov = eigvecs @ np.diag(eigvals) @ eigvecs.T\n    else:\n        # Use the Higham & Nick (2002) algorithm to find the nearest positive definite matrix\n        cov = np.copy(cov)\n        for _ in range(higham_max_iteration):\n            cov_old = cov\n            eigvals, eigvecs = np.linalg.eigh(cov)\n            eigvals[eigvals < _CLIPPING_VALUE] = _CLIPPING_VALUE\n            cov = eigvecs @ np.diag(eigvals) @ eigvecs.T\n            if np.allclose(cov, cov_old):\n                break\n    return cov"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if not is_positive_definite(cov):\n        if higham:\n            cov = _cov_nearest_higham(cov, higham_max_iteration)\n        else:\n            cov = _cov_nearest_clip(cov)\n    return cov\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    if higham:\n        cov_ = cov\n        for _ in range(higham_max_iteration):\n            eig_val, eig_vec = np.linalg.eigh(cov_)\n            eig_val[eig_val < _CLIPPING_VALUE] = _CLIPPING_VALUE\n            cov_ = eig_vec @ np.diag(eig_val) @ eig_vec.T\n            if is_positive_definite(cov_):\n                return cov_\n        raise ValueError(\n            f\"Could not find a positive definite matrix after {higham_max_iteration} iterations\"\n        )\n    else:\n        eig_val, eig_vec = np.linalg.eigh(cov)\n        eig_val[eig_val < _CLIPPING_VALUE] = _CLIPPING_VALUE\n        cov_ = eig_vec @ np.diag(eig_val) @ eig_vec.T\n        return cov_"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        cov = cov.copy()\n        cov[cov < _CLIPPING_VALUE] = _CLIPPING_VALUE\n        cov_prev = cov.copy()\n        for _ in range(higham_max_iteration):\n            cov_prev = cov.copy()\n            cov = (cov + cov.T) / 2\n            eig_val, eig_vec = np.linalg.eigh(cov)\n            eig_val[eig_val < _CLIPPING_VALUE] = _CLIPPING_VALUE\n            cov = eig_vec @ np.diag(eig_val) @ eig_vec.T\n            if np.allclose(cov, cov_prev):\n                break\n    else:\n        eig_val, eig_vec = np.linalg.eigh(cov)\n        eig_val[eig_val < _CLIPPING_VALUE] = _CLIPPING_VALUE\n        cov = eig_vec @ np.diag(eig_val) @ eig_vec.T\n    return cov"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if not is_positive_definite(cov):\n        cov = np.copy(cov)\n        if higham:\n            cov = _cov_nearest_higham(cov, higham_max_iteration)\n        else:\n            cov = _cov_nearest_clipped(cov)\n    return cov\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    assert_is_symmetric(cov)\n    if higham:\n        cov_ = cov\n        for _ in range(higham_max_iteration):\n            cov_ = (cov_ + cov_.T) / 2\n            if is_positive_definite(cov_):\n                return cov_\n            cov_ = np.linalg.multi_dot([cov_, np.linalg.inv(np.linalg.cholesky(cov_)), cov_])\n        raise ValueError(\n            f\"Higham algorithm failed to find a positive definite matrix after {higham_max_iteration} iterations\"\n        )\n    else:\n        cov_ = cov.copy()\n        cov_[np.diag_indices_from(cov_)] = np.diag(cov_) + _CLIPPING_VALUE\n        return cov_"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if not isinstance(cov, np.ndarray):\n        raise TypeError(\"`cov` must be a numpy array\")\n    if not isinstance(higham, bool):\n        raise TypeError(\"`higham` must be a boolean\")\n    if not isinstance(higham_max_iteration, int):\n        raise TypeError(\"`higham_max_iteration` must be an integer\")\n\n    if not is_positive_definite(cov):\n        cov = cov_nearest(cov, higham=higham, higham_max_iteration=higham_max_iteration)\n    if not is_cholesky_dec(cov):\n        if higham:\n            cov = cov_nearest(cov, higham=higham, higham_max_iteration=higham_max_iteration)\n        else:\n            cov = cov_nearest(cov, higham=higham, higham_max_iteration=higham_max_iteration)\n    return cov"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if not higham:\n        cov = cov_nearest_clipped(cov)\n    else:\n        cov = cov_nearest_higham(cov, higham_max_iteration)\n    return cov\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    assert_is_symmetric(cov)\n    if higham:\n        cov_ = cov\n        for _ in range(higham_max_iteration):\n            cov_ = (cov_ + cov_.T) / 2\n            cov_ = cov_ @ np.linalg.inv(scipy.linalg.sqrtm(cov_ @ cov_.T)) @ cov_\n            if is_positive_definite(cov_):\n                return cov_\n        raise ValueError(\"Higham algorithm failed to find a positive definite matrix\")\n    else:\n        # Clip the eigenvalues to ensure the resulting matrix is positive definite\n        eig_val, eig_vec = np.linalg.eigh(cov)\n        eig_val = np.clip(eig_val, a_min=_CLIPPING_VALUE, a_max=None)\n        return eig_vec @ np.diag(eig_val) @ eig_vec.T"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    assert_is_symmetric(cov)\n    assert_is_positive_definite(cov)\n\n    if higham:\n        cov = np.copy(cov)\n        n = cov.shape[0]\n        for _ in range(higham_max_iteration):\n            cov_old = cov\n            r1 = np.linalg.cholesky(cov)\n            r1_inv = np.linalg.inv(r1)\n            cov = np.dot(r1_inv.T, np.dot(cov, r1_inv))\n            cov = (cov + cov.T) / 2\n            if np.allclose(cov, cov_old):\n                break\n    else:\n        cov = np.copy(cov)\n        np.fill_diagonal(cov, 0)\n        cov = np.clip(cov, a_min=_CLIPPING_VALUE, a_max=None)\n        np.fill_diagonal(cov, 1)\n    return cov"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    cov = np.asarray(cov)\n    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    if not is_positive_definite(cov):\n        raise ValueError(\"The covariance matrix must be positive definite\")\n\n    if higham:\n        return _cov_nearest_higham(cov, higham_max_iteration)\n\n    # Clipping method\n    cov = (cov + cov.T) / 2\n    eig_val, eig_vec = np.linalg.eigh(cov)\n    eig_val = np.maximum(eig_val, _CLIPPING_VALUE)\n    eig_val_matrix = np.diag(eig_val)\n    cov = eig_vec @ eig_val_matrix @ eig_vec.T\n    cov = (cov + cov.T) / 2\n    return cov\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if not is_positive_definite(cov):\n        if higham:\n            cov = _higham_near_psd(cov, max_iteration=higham_max_iteration)\n        else:\n            cov = _clip_near_psd(cov)\n    return cov\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        return _cov_nearest_higham(cov, higham_max_iteration)\n    else:\n        return _cov_nearest_clipping(cov)\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if not is_positive_definite(cov):\n        if higham:\n            cov = _higham_near_psd(cov, max_iteration=higham_max_iteration)\n        else:\n            cov = _clip_eigenvalues(cov)\n    return cov\n\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if cov.ndim != 2:\n        raise ValueError(\"`cov` must be a 2D array\")\n    if not is_positive_definite(cov):\n        raise ValueError(\"`cov` must be positive definite\")\n    if higham:\n        for _ in range(higham_max_iteration):\n            cov_old = cov\n            v, l = np.linalg.eigh(cov)\n            v = np.maximum(v, _CLIPPING_VALUE)\n            cov = (l * v) @ l.T\n            if is_positive_definite(cov):\n                return cov\n            if np.allclose(cov, cov_old):\n                break\n        raise ValueError(\n            \"`cov` is not positive definite after Higham & Nick (2002) algorithm\"\n        )\n    else:\n        v, l = np.linalg.eigh(cov)\n        v = np.maximum(v, _CLIPPING_VALUE)\n        return (l * v) @ l.T"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        cov = cov.copy()\n        for _ in range(higham_max_iteration):\n            cov_old = cov.copy()\n            try:\n                cov = np.linalg.cholesky(cov)\n                cov = np.dot(cov, cov.T)\n                if np.allclose(cov, cov_old):\n                    break\n            except np.linalg.LinAlgError:\n                pass\n        if not np.allclose(cov, cov_old):\n            raise ValueError(\n                \"Could not find a positive definite matrix using the Higham algorithm\"\n            )\n    else:\n        cov = cov.copy()\n        cov[np.diag_indices_from(cov)] = np.clip(\n            cov[np.diag_indices_from(cov)], a_min=_CLIPPING_VALUE, a_max=None\n        )\n    return cov"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    assert_is_symmetric(cov)\n    assert_is_positive_definite(cov)\n\n    if higham:\n        # Higham & Nick (2002) algorithm\n        n = cov.shape[0]\n        diag_cov = np.diag(cov)\n        eig_val, eig_vec = np.linalg.eigh(cov)\n        eig_val = np.maximum(eig_val, _CLIPPING_VALUE)\n        eig_val_new = np.zeros(n)\n        for i in range(higham_max_iteration):\n            eig_val_new = np.maximum(eig_val, eig_val_new)\n            eig_val_new = np.maximum(eig_val_new, _CLIPPING_VALUE)\n            cov_new = eig_vec @ np.diag(eig_val_new) @ eig_vec.T\n            eig_val, eig_vec = np.linalg.eigh(cov_new)\n            if np.allclose(eig_val_new, eig_val):\n                break\n        cov_new = eig_vec @ np.diag(eig_val_new) @ eig_vec.T\n        cov_new = np.diag(diag_cov) + cov_new - np.diag(np.diag(cov_new))\n    else:\n        # Clip eigenvalues\n        eig_val, eig_vec = np.linalg.eigh(cov)\n        eig_val = np.maximum(eig_val, _CLIPPING_VALUE)\n        cov_new = eig_vec @ np.diag(eig_val) @ eig_vec.T\n        cov_new = np.diag(np.diag(cov)) + cov_new - np.diag(np.diag(cov_new))\n\n    return cov_new"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home=data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n    os.makedirs(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home=data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home=data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    if os.path.exists(data_home):\n        shutil.rmtree(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home=data_home)\n    shutil.rmtree(data_home)\n    os.makedirs(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    if isinstance(obj, collections.abc.Sequence):\n        return ListSchema.flatten(obj)\n    if isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    if isinstance(obj, (Boxes, ROIMasks)):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    raise ValueError(f\"Unsupported type {type(obj)}\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    if isinstance(obj, collections.abc.Sequence):\n        return ListSchema.flatten(obj)\n    if isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    if isinstance(obj, (Boxes, ROIMasks)):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    raise ValueError(f\"Unsupported type {type(obj)}\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, collections.abc.Sequence):\n        return ListSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    elif isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    else:\n        raise NotImplementedError(f\"Cannot flatten {type(obj)}\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, (list, tuple)):\n        return TupleSchema.flatten(obj)\n    if isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    if isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    if isinstance(obj, (Boxes, ROIMasks)):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    raise ValueError(f\"Unsupported type {type(obj)}\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, (list, tuple)):\n        return ListSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    elif isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    else:\n        raise ValueError(f\"Cannot flatten object of type {type(obj)}\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, collections.abc.Sequence):\n        return ListSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, (Boxes, ROIMasks)):\n        return TensorWrapSchema.flatten(obj)\n    elif isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    else:\n        raise ValueError(f\"Unsupported type {type(obj)} for flattening to tuple.\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    if isinstance(obj, collections.abc.Sequence):\n        return ListSchema.flatten(obj)\n    if isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    if isinstance(obj, (Boxes, ROIMasks)):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    raise ValueError(f\"Cannot flatten object of type {type(obj)}\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, bytes):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    elif isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    elif isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    elif isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    else:\n        raise TypeError(f\"Unsupported type {type(obj)}\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, (Boxes, ROIMasks)):\n        return TensorWrapSchema.flatten(obj)\n    elif isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    elif isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    else:\n        raise ValueError(f\"Cannot flatten object of type {type(obj)}.\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return IdentitySchema.flatten(obj)\n    if isinstance(obj, bytes):\n        return IdentitySchema.flatten(obj)\n    if isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    if isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    if isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    if isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    if isinstance(obj, torch.Tensor):\n        return IdentitySchema.flatten(obj)\n    raise NotImplementedError(f\"Cannot flatten object of type {type(obj)}\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, torch.Tensor):\n        return obj.flatten(), IdentitySchema()\n    elif isinstance(obj, (str, bytes)):\n        return obj, IdentitySchema()\n    elif isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, collections.abc.Sequence):\n        return ListSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    elif isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    else:\n        raise TypeError(f\"Unsupported type {type(obj)}\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, collections.abc.Sequence):\n        return ListSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, (Boxes, ROIMasks)):\n        return TensorWrapSchema.flatten(obj)\n    elif isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    else:\n        raise TypeError(\n            f\"Unsupported type {type(obj)} for flattening to tuple. \"\n            \"Supported types are str, bytes, Mapping, Sequence, Instances, \"\n            \"Boxes, ROIMasks, and Tensor.\"\n        )"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, collections.abc.Sequence):\n        return ListSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    elif isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    else:\n        raise ValueError(\n            f\"Unsupported type {type(obj)} for flattening to tuple. \"\n            \"Supported types: str, bytes, collections.abc.Mapping, \"\n            \"collections.abc.Sequence, Instances, Boxes, ROIMasks.\"\n        )"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, collections.abc.Sequence):\n        if isinstance(obj, (list, tuple)):\n            return ListSchema.flatten(obj)\n        else:\n            raise ValueError(\n                \"Only support flattening list/tuple, but got {}.\".format(type(obj))\n            )\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, (Boxes, ROIMasks)):\n        return TensorWrapSchema.flatten(obj)\n    else:\n        raise ValueError(\n            \"Only support flattening str/bytes/list/tuple/dict/Instances/Boxes/ROIMasks, but got {}.\".format(\n                type(obj)\n            )\n        )"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    if isinstance(obj, collections.abc.Sequence):\n        return ListSchema.flatten(obj)\n    if isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    if isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, (list, tuple)):\n        if all(isinstance(x, torch.Tensor) for x in obj):\n            return (obj,), IdentitySchema()\n        else:\n            return ListSchema.flatten(obj)\n    raise ValueError(f\"Unsupported type {type(obj)}\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    if isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    if isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    if isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    if isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, torch.nn.Module):\n        return (obj,), IdentitySchema()\n    if obj is None:\n        return (obj,), IdentitySchema()\n    raise ValueError(f\"Cannot flatten object of type {type(obj)}.\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, (list, tuple)):\n        if all(isinstance(x, torch.Tensor) for x in obj):\n            return (obj,), IdentitySchema()\n        else:\n            return ListSchema.flatten(obj)\n    elif isinstance(obj, collections.abc.Mapping):\n        if all(isinstance(x, torch.Tensor) for x in obj.values()):\n            return (obj,), IdentitySchema()\n        else:\n            return DictSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    elif isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    else:\n        raise ValueError(f\"Cannot flatten object of type {type(obj)}\")"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, collections.abc.Sequence):\n        return ListSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    elif isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    else:\n        raise ValueError(f\"Unsupported type {type(obj)}.\")\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, collections.abc.Sequence):\n        return ListSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    elif isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    else:\n        raise ValueError(\n            f\"Cannot flatten object of type {type(obj)} \"\n            \"(only support str, bytes, list, tuple, dict, \"\n            \"Instances, Boxes, or ROIMasks).\"\n        )"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return IdentitySchema.flatten(obj)\n    if isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    if isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    if isinstance(obj, (Boxes, ROIMasks)):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, (list, tuple)):\n        if len(obj) == 0:\n            return IdentitySchema.flatten(obj)\n        if isinstance(obj, tuple):\n            return TupleSchema.flatten(obj)\n        return ListSchema.flatten(obj)\n    if isinstance(obj, torch.Tensor):\n        return IdentitySchema.flatten(obj)\n    raise NotImplementedError(f\"Cannot flatten object of type {type(obj)}\")"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n    if groups.ndim != 2:\n        raise EquationToMatrixError(\n            f\"{names[0]} should be a 2D array, but it is a {groups.ndim}D array\"\n        )\n    if equations.ndim != 1:\n        raise EquationToMatrixError(\n            f\"{names[1]} should be a 1D array, but it is a {equations.ndim}D array\"\n        )\n\n    n = groups.shape[1]\n    m = len(equations)\n    left = np.zeros((m, n))\n    right = np.zeros(m)\n\n    for i, equation in enumerate(equations):\n        try:\n            left[i], right[i] = _string_to_equation(\n                groups=groups,\n                string=equation,\n                sum_to_one=sum_to_one,\n            )\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            else:\n                warnings.warn(str(e))\n                return None\n\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n    n_groups, n_assets = groups.shape\n    n_equations = len(equations)\n\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n\n    for i, eq in enumerate(equations):\n        try:\n            left[i], right[i] = _string_to_equation(\n                groups=groups, string=eq, sum_to_one=sum_to_one\n            )\n        except (GroupNotFoundError, EquationToMatrixError) as e:\n            if raise_if_group_missing:\n                raise e\n            else:\n                warnings.warn(e.args[0])\n                return None\n\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n    if groups.ndim != 2:\n        raise EquationToMatrixError(\n            f\"{names[0]} must be a 2D-array. \"\n            f\"Got {groups.ndim}D-array instead.\"\n        )\n    if equations.ndim != 1:\n        raise EquationToMatrixError(\n            f\"{names[1]} must be a 1D-array. \"\n            f\"Got {equations.ndim}D-array instead.\"\n        )\n    n_groups, n_assets = groups.shape\n    n_equations = len(equations)\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n    for i, equation in enumerate(equations):\n        try:\n            left[i, :], right[i] = _string_to_equation(\n                groups=groups, string=equation, sum_to_one=sum_to_one\n            )\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            else:\n                warnings.warn(str(e))\n                return None\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n\n    if groups.ndim != 2:\n        raise EquationToMatrixError(\n            f\"The {names[0]} should be a 2D array, but got a {groups.ndim}D array\"\n        )\n\n    if equations.ndim != 1:\n        raise EquationToMatrixError(\n            f\"The {names[1]} should be a 1D array, but got a {equations.ndim}D array\"\n        )\n\n    n = groups.shape[1]\n    m = len(equations)\n    left = np.zeros((m, n))\n    right = np.zeros(m)\n    for i, eq in enumerate(equations):\n        try:\n            left[i], right[i] = _string_to_equation(\n                groups=groups, string=eq, sum_to_one=sum_to_one\n            )\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            else:\n                warnings.warn(str(e))\n                return None\n\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n    if groups.ndim != 2:\n        raise EquationToMatrixError(\n            f\"{names[0]} should be a 2D array, but it is a {groups.ndim}D array\"\n        )\n    if equations.ndim != 1:\n        raise EquationToMatrixError(\n            f\"{names[1]} should be a 1D array, but it is a {equations.ndim}D array\"\n        )\n    if groups.shape[1] != equations.shape[0]:\n        raise EquationToMatrixError(\n            f\"{names[0]} and {names[1]} should have the same number of columns\"\n        )\n\n    n_rows = len(equations)\n    n_cols = groups.shape[1]\n    left = np.zeros((n_rows, n_cols))\n    right = np.zeros(n_rows)\n    for i, equation in enumerate(equations):\n        try:\n            left[i, :], right[i] = _string_to_equation(\n                groups=groups, string=equation, sum_to_one=sum_to_one\n            )\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            else:\n                warnings.warn(str(e))\n                return None\n\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.array(groups, dtype=\"<U50\")\n    equations = np.array(equations, dtype=\"<U50\")\n    n_groups, n_assets = groups.shape\n    n_equations = len(equations)\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n    for i, equation in enumerate(equations):\n        try:\n            left[i], right[i] = _string_to_equation(\n                groups=groups, string=equation, sum_to_one=sum_to_one\n            )\n        except (EquationToMatrixError, GroupNotFoundError) as e:\n            if raise_if_group_missing:\n                raise e\n            else:\n                warnings.warn(str(e))\n                return None\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n    if len(groups.shape) != 2:\n        raise EquationToMatrixError(\n            f\"{names[0]} should be a 2D-array of shape (n_groups, n_assets)\"\n        )\n    if len(equations.shape) != 1:\n        raise EquationToMatrixError(\n            f\"{names[1]} should be a 1D-array of shape (n_equations,)\"\n        )\n    n = groups.shape[1]\n    m = len(equations)\n    left = np.zeros((m, n))\n    right = np.zeros(m)\n    for i, equation in enumerate(equations):\n        try:\n            left[i, :], right[i] = _string_to_equation(\n                groups=groups, string=equation, sum_to_one=sum_to_one\n            )\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            else:\n                warnings.warn(e.args[0])\n                return None\n\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n    n_groups, n_assets = groups.shape\n    n_equations = equations.shape[0]\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n    for i in range(n_equations):\n        left[i], right[i] = _string_to_equation(\n            groups=groups, string=equations[i], sum_to_one=sum_to_one\n        )\n\n    if np.all(left == 0):\n        if raise_if_group_missing:\n            raise GroupNotFoundError(\n                f\"None of the {names[0]} in {names[1]} are part of {names[0]}.\"\n            )\n        else:\n            warnings.warn(\n                f\"None of the {names[0]} in {names[1]} are part of {names[0]}.\"\n            )\n            return None\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n    if len(groups.shape) != 2:\n        raise EquationToMatrixError(\n            f\"The {names[0]} array should be a 2D array, but it has {len(groups.shape)}\"\n            f\" dimensions\"\n        )\n    if len(equations.shape) != 1:\n        raise EquationToMatrixError(\n            f\"The {names[1]} array should be a 1D array, but it has {len(equations.shape)}\"\n            f\" dimensions\"\n        )\n\n    n = groups.shape[1]\n    m = len(equations)\n    left = np.zeros((m, n))\n    right = np.zeros(m)\n    for i, equation in enumerate(equations):\n        try:\n            left[i], right[i] = _string_to_equation(\n                groups=groups, string=equation, sum_to_one=sum_to_one\n            )\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            else:\n                warnings.warn(str(e))\n                return None\n\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n    if groups.ndim != 2:\n        raise EquationToMatrixError(\n            f\"The {names[0]} must be a 2D array, but its shape is {groups.shape}\"\n        )\n    if equations.ndim != 1:\n        raise EquationToMatrixError(\n            f\"The {names[1]} must be a 1D array, but its shape is {equations.shape}\"\n        )\n\n    n_equations = equations.shape[0]\n    n_assets = groups.shape[1]\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n    for i, eq in enumerate(equations):\n        try:\n            left[i, :], right[i] = _string_to_equation(\n                groups=groups, string=eq, sum_to_one=sum_to_one\n            )\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            warnings.warn(str(e))\n            return None\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n    if groups.ndim != 2:\n        raise ValueError(\n            f\"The {names[0]} array should be 2D, but it is {groups.ndim}D.\"\n        )\n    if equations.ndim != 1:\n        raise ValueError(\n            f\"The {names[1]} array should be 1D, but it is {equations.ndim}D.\"\n        )\n    if len(equations) == 0:\n        return None, None\n\n    n_groups, n_assets = groups.shape\n    left = np.zeros((len(equations), n_assets))\n    right = np.zeros(len(equations))\n    for i, equation in enumerate(equations):\n        try:\n            left[i], right[i] = _string_to_equation(\n                groups=groups,\n                string=equation,\n                sum_to_one=sum_to_one,\n            )\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            else:\n                warnings.warn(str(e))\n                return None, None\n\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n    if groups.ndim != 2:\n        raise EquationToMatrixError(\n            f\"The parameter {names[0]} should be a 2D array. The shape of the input is {groups.shape}\"\n        )\n    if equations.ndim != 1:\n        raise EquationToMatrixError(\n            f\"The parameter {names[1]} should be a 1D array. The shape of the input is {equations.shape}\"\n        )\n\n    n_groups, n_assets = groups.shape\n    n_equations = equations.shape[0]\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n    for i, equation in enumerate(equations):\n        try:\n            left[i, :], right[i] = _string_to_equation(\n                groups=groups, string=equation, sum_to_one=sum_to_one\n            )\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            else:\n                warnings.warn(str(e))\n    if np.all(left == 0):\n        return None\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n    n_groups, n_assets = groups.shape\n    n_equations = len(equations)\n\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n\n    for i, equation in enumerate(equations):\n        try:\n            left[i, :], right[i] = _string_to_equation(\n                groups=groups,\n                string=equation,\n                sum_to_one=sum_to_one,\n            )\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            else:\n                warnings.warn(\n                    f\"{e}. This equation will be ignored. If this is not the expected behavior, please set raise_if_group_missing=True\",\n                    UserWarning,\n                )\n                left[i, :] = np.nan\n                right[i] = np.nan\n\n    if np.all(np.isnan(left)):\n        return None, None\n\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.array(groups, dtype=str)\n    equations = np.array(equations, dtype=str)\n    if len(groups.shape) != 2:\n        raise ValueError(f\"{names[0]} must be a 2D-array\")\n    if len(equations.shape) != 1:\n        raise ValueError(f\"{names[1]} must be a 1D-array\")\n    if groups.shape[1] != len(equations):\n        raise ValueError(\n            f\"{names[0]} and {names[1]} must have the same number of columns\"\n        )\n\n    left = []\n    right = []\n    for equation in equations:\n        try:\n            left_i, right_i = _string_to_equation(\n                groups=groups, string=equation, sum_to_one=sum_to_one\n            )\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e from None\n            else:\n                warnings.warn(str(e))\n                continue\n        left.append(left_i)\n        right.append(right_i)\n    if len(left) == 0:\n        return None, None\n    left = np.array(left)\n    right = np.array(right)\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n\n    if groups.ndim != 2:\n        raise ValueError(\n            f\"{names[0]} should be a 2D-array, but a {groups.ndim}-array was given.\"\n        )\n    if equations.ndim != 1:\n        raise ValueError(\n            f\"{names[1]} should be a 1D-array, but a {equations.ndim}-array was given.\"\n        )\n\n    n_groups, n_assets = groups.shape\n    n_equations = len(equations)\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n\n    for i, eq in enumerate(equations):\n        try:\n            left[i], right[i] = _string_to_equation(\n                groups=groups, string=eq, sum_to_one=sum_to_one\n            )\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            else:\n                warnings.warn(str(e))\n                return None\n\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n    if groups.ndim != 2:\n        raise EquationToMatrixError(\n            f\"The {names[0]} must be a 2D-array, but a {groups.ndim}-D array was provided\"\n        )\n    if equations.ndim != 1:\n        raise EquationToMatrixError(\n            f\"The {names[1]} must be a 1D-array, but a {equations.ndim}-D array was provided\"\n        )\n\n    n_groups, n_assets = groups.shape\n    n_equations = len(equations)\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n\n    for i, equation in enumerate(equations):\n        try:\n            left[i, :], right[i] = _string_to_equation(\n                groups=groups, string=equation, sum_to_one=sum_to_one\n            )\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            warnings.warn(e.args[0])\n            return None, None\n\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n\n    if groups.ndim != 2:\n        raise EquationToMatrixError(\n            f\"{names[0]} should be a 2D array of shape (n_groups, n_assets), \"\n            f\"but it has {groups.ndim} dimensions\"\n        )\n    if equations.ndim != 1:\n        raise EquationToMatrixError(\n            f\"{names[1]} should be a 1D array of shape (n_equations,), \"\n            f\"but it has {equations.ndim} dimensions\"\n        )\n\n    if not np.issubdtype(groups.dtype, np.str_):\n        raise EquationToMatrixError(\n            f\"{names[0]} should be a 2D array of strings, \"\n            f\"but it has dtype {groups.dtype}\"\n        )\n    if not np.issubdtype(equations.dtype, np.str_):\n        raise EquationToMatrixError(\n            f\"{names[1]} should be a 1D array of strings, \"\n            f\"but it has dtype {equations.dtype}\"\n        )\n\n    n_equations = len(equations)\n    n_assets = groups.shape[1]\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n\n    for i, equation in enumerate(equations):\n        try:\n            left[i], right[i] = _string_to_equation(\n                groups=groups,\n                string=equation,\n                sum_to_one=sum_to_one,\n            )\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            else:\n                warnings.warn(str(e))\n\n    if np.all(left == 0):\n        return None, None\n\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n\n    if groups.ndim != 2:\n        raise EquationToMatrixError(\n            f\"The {names[0]} must be a 2D array, but {names[0]} has {groups.ndim} dimensions\"\n        )\n    if equations.ndim != 1:\n        raise EquationToMatrixError(\n            f\"The {names[1]} must be a 1D array, but {names[1]} has {equations.ndim} dimensions\"\n        )\n\n    n_groups, n_assets = groups.shape\n    n_equations = equations.shape[0]\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n    for i, equation in enumerate(equations):\n        try:\n            left[i, :], right[i] = _string_to_equation(\n                groups,\n                equation,\n                sum_to_one,\n            )\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            warnings.warn(str(e), stacklevel=2)\n            return None\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n    if groups.ndim != 2:\n        raise ValueError(\n            f\"{names[0]} should be a 2D array, but it is a {groups.ndim}D array\"\n        )\n    if equations.ndim != 1:\n        raise ValueError(\n            f\"{names[1]} should be a 1D array, but it is a {equations.ndim}D array\"\n        )\n    if groups.shape[1] > 1:\n        warnings.warn(\n            \"The groups array should have only one column. \"\n            \"The other columns will be ignored.\",\n            stacklevel=2,\n        )\n\n    n = groups.shape[1]\n    m = len(equations)\n    left = np.zeros((m, n))\n    right = np.zeros(m)\n    for i, eq in enumerate(equations):\n        try:\n            left[i], right[i] = _string_to_equation(\n                groups=groups, string=eq, sum_to_one=sum_to_one\n            )\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            warnings.warn(str(e), stacklevel=2)\n    if np.all(left == 0):\n        return None\n    return left, right"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n    if groups.ndim != 2:\n        raise ValueError(\n            f\"{names[0]} should be a 2D array of shape (n_groups, n_assets)\"\n        )\n    if equations.ndim != 1:\n        raise ValueError(f\"{names[1]} should be a 1D array of shape (n_equations,)\")\n\n    n_groups, n_assets = groups.shape\n    n_equations = equations.shape[0]\n\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n\n    for i, equation in enumerate(equations):\n        try:\n            left[i], right[i] = _string_to_equation(\n                groups=groups, string=equation, sum_to_one=sum_to_one\n            )\n        except EquationToMatrixError as e:\n            if raise_if_group_missing:\n                raise e\n            warnings.warn(\n                f\"{e}. The equation '{equation}' is ignored and the returned matrix \"\n                \"will be missing some rows.\"\n            )\n            left[i] = np.nan\n            right[i] = np.nan\n\n    if np.all(np.isnan(left)):\n        return None\n\n    return left, right"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n\n    with tempfile.TemporaryDirectory() as tmpdir:\n        path = os.path.join(tmpdir, f\"instances{_counter}.py\")\n        with open(path, \"w\") as f:\n            cls_name, cls_def = _gen_instance_module(fields)\n            f.write(cls_def)\n        _clear_jit_cache()\n        newInstances = _import(path)\n        _add_instances_conversion_methods(newInstances)\n        yield newInstances\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n\n    with tempfile.NamedTemporaryFile(\n        mode=\"w\", suffix=\".py\", delete=False, dir=os.path.dirname(__file__)\n    ) as f:\n        cls_name, module_str = _gen_instance_module(fields)\n        f.write(module_str)\n        f.flush()\n\n    try:\n        with patch_builtin_len():\n            newInstances = _import(f.name).ScriptedInstances\n            _add_instances_conversion_methods(newInstances)\n            yield newInstances\n    finally:\n        os.unlink(f.name)\n        _clear_jit_cache()\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n\n    newInstances = _gen_instance_module(fields)\n    newInstances = _import(newInstances)\n    _add_instances_conversion_methods(newInstances)\n\n    yield newInstances\n\n    del sys.modules[newInstances.__name__]\n    _clear_jit_cache()\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    with tempfile.TemporaryDirectory(prefix=\"detectron2_scriptable_instances\") as d:\n        fname = os.path.join(d, \"instances.py\")\n        with open(fname, \"w\") as f:\n            cls_name, s = _gen_instance_module(fields)\n            f.write(s)\n        newInstances = _import(fname)\n        _add_instances_conversion_methods(newInstances)\n        yield newInstances\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n    newInstances = _gen_instance_module(fields)\n    _add_instances_conversion_methods(newInstances)\n\n    with tempfile.TemporaryDirectory(prefix=\"detectron2_patch_instances\") as tmpdir:\n        tmpfile = os.path.join(tmpdir, \"instances.py\")\n        with open(tmpfile, \"w\") as f:\n            f.write(newInstances)\n        with _import(tmpfile) as m:\n            yield m.ScriptedInstances"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    # Generate a new class that replaces 'Instances' for the duration of the context.\n    cls_name, module_s = _gen_instance_module(fields)\n\n    # Write the module to a temporary file.\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\") as f:\n        f.write(module_s)\n        f.flush()\n\n        # Import the module and set up the environment so that torchscript can recognize and work with this new class.\n        with _import(f.name) as module:\n            newInstances = getattr(module, cls_name)\n            _add_instances_conversion_methods(newInstances)\n            yield newInstances\n\n            # Clean up by removing modifications.\n            del sys.modules[module.__name__]\n            _clear_jit_cache()\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    with tempfile.TemporaryDirectory() as tmpdir:\n        # write a new module to a temporary file\n        path = os.path.join(tmpdir, \"instances.py\")\n        with open(path, \"w\") as f:\n            f.write(_gen_instance_module(fields)[1])\n\n        # import the module\n        module = _import(path)\n\n        # replace the Instances class\n        newInstances = getattr(module, \"ScriptedInstances\")\n        _add_instances_conversion_methods(newInstances)\n        detectron2.structures.Instances = newInstances\n\n        # clear jit cache\n        _clear_jit_cache()\n\n        try:\n            yield newInstances\n        finally:\n            # restore the original Instances class\n            detectron2.structures.Instances = Instances\n\n            # clear jit cache\n            _clear_jit_cache()\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\") as f:\n        cls_name, cls_def = _gen_instance_module(fields)\n        f.write(cls_def)\n        f.flush()\n        newInstances = _import(f.name).ScriptedInstances\n\n        # add from_instances method\n        _add_instances_conversion_methods(newInstances)\n\n        # replace the Instances class\n        oldInstances = Instances\n        detectron2.structures.Instances = newInstances\n        yield newInstances\n        # restore the Instances class\n        detectron2.structures.Instances = oldInstances\n\n        # cleanup\n        del sys.modules[f.name]\n        _clear_jit_cache()\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n\n    with tempfile.TemporaryDirectory() as tmpdir:\n        path = os.path.join(tmpdir, f\"instances_{_counter}.py\")\n        with open(path, \"w\") as f:\n            f.write(_gen_instance_module(fields)[1])\n\n        # import the new class\n        newInstances = _import(path)\n\n        # add conversion methods\n        _add_instances_conversion_methods(newInstances)\n\n        # patch the instances class\n        oldInstances = Instances\n        Instances = newInstances\n\n        # patch the builtin len() function\n        with patch_builtin_len([\"detectron2.modeling.box_regression\"]):\n            yield\n\n        # restore the instances class\n        Instances = oldInstances\n        _clear_jit_cache()\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    with tempfile.TemporaryDirectory() as tmpdir:\n        tmpfile = os.path.join(tmpdir, \"instances.py\")\n        with open(tmpfile, \"w\") as f:\n            f.write(_gen_instance_module(fields)[1])\n\n        # import the module\n        module = _import(tmpfile)\n        # replace the Instances class\n        newInstances = getattr(module, \"ScriptedInstances{}\".format(_counter))\n        # add conversion methods to the new class\n        _add_instances_conversion_methods(newInstances)\n        # replace the Instances class in detectron2\n        detectron2.structures.Instances = newInstances\n\n        # clear jit cache\n        _clear_jit_cache()\n        yield newInstances\n\n        # remove the new class\n        delattr(module, \"ScriptedInstances{}\".format(_counter))\n        # restore the original Instances class\n        detectron2.structures.Instances = Instances\n        # clear jit cache\n        _clear_jit_cache()\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    with tempfile.TemporaryDirectory() as tmpdir:\n        tmpfile = os.path.join(tmpdir, \"instances.py\")\n        with open(tmpfile, \"w\") as f:\n            f.write(\n                \"\"\""}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n\n    # generate a new class\n    cls_name, cls_def = _gen_instance_module(fields)\n    # write to a temporary file\n    with tempfile.NamedTemporaryFile(\"w+t\", suffix=\".py\", prefix=\"detectron2_\") as f:\n        f.write(cls_def)\n        f.flush()\n        # import the new class\n        newInstances = _import(f.name)\n    # set up the environment\n    _add_instances_conversion_methods(newInstances)\n    _clear_jit_cache()\n\n    # replace the old class\n    oldInstances = Instances\n    Instances = newInstances\n    yield\n    # revert the change\n    Instances = oldInstances\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n\n    # create a temporary file to store the new class\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n        cls_name, cls_def = _gen_instance_module(fields)\n        f.write(cls_def)\n        f.flush()\n        temp_module_path = f.name\n\n    # import the new class\n    newInstances = _import(temp_module_path)\n\n    # add conversion methods to the new class\n    _add_instances_conversion_methods(newInstances)\n\n    # replace the original Instances class with the new one\n    detectron2.structures.Instances = newInstances\n\n    # set up torchscript environment\n    torch.jit.register_class(newInstances)\n    _clear_jit_cache()\n\n    yield newInstances\n\n    # clean up\n    detectron2.structures.Instances = Instances\n    _clear_jit_cache()\n    os.remove(temp_module_path)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    # Generate the new class and write it to a temporary file\n    cls_name, cls_def = _gen_instance_module(fields)\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n        f.write(cls_def)\n        f.flush()\n        # import the new class\n        module = _import(f.name)\n        # get the new class\n        newInstances = getattr(module, cls_name)\n\n    # Add from_instances method to the new class\n    _add_instances_conversion_methods(newInstances)\n\n    # Set up the environment to use the new class\n    oldInstances = Instances\n    Instances = newInstances\n\n    # Clean up after the context manager is exited\n    try:\n        yield\n    finally:\n        # Remove the temporary file\n        os.unlink(f.name)\n        # Restore the original class\n        Instances = oldInstances\n        # Clear the jit cache\n        _clear_jit_cache()\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n\n    with tempfile.TemporaryDirectory() as tmpdir:\n        mod_file = os.path.join(tmpdir, \"module.py\")\n        with open(mod_file, \"w\") as f:\n            cls_name, mod_str = _gen_instance_module(fields)\n            f.write(mod_str)\n        # import module\n        module = _import(mod_file)\n        # replace Instances with new class\n        newInstances = getattr(module, cls_name)\n        _add_instances_conversion_methods(newInstances)\n        with mock.patch(\"detectron2.structures.Instances\", newInstances, create=True):\n            # clear jit cache to force re-tracing\n            _clear_jit_cache()\n            yield newInstances\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    assert isinstance(fields, (tuple, list))\n    fields = {k: v for k, v in fields}\n    newInstances = _gen_instance_module(fields)\n    with tempfile.TemporaryDirectory(prefix=\"detectron2_scriptable_instances\") as d:\n        path = os.path.join(d, \"instances.py\")\n        with open(path, \"w\") as f:\n            f.write(newInstances[1])\n        # reload to make sure the new class is imported\n        _clear_jit_cache()\n        newInstances = _import(path)\n\n    # make sure the new class is used\n    detectron2.structures.Instances = newInstances[0]\n\n    # add conversion methods\n    _add_instances_conversion_methods(newInstances[0])\n\n    yield newInstances[0]\n\n    # revert the change\n    detectron2.structures.Instances = Instances\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    # create a new class\n    cls_name, cls_def = _gen_instance_module(fields)\n\n    # write the new class to a temporary file\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n        f.write(cls_def)\n\n    # import the new class\n    newInstances = _import(f.name)\n\n    # add conversion methods to the new class\n    _add_instances_conversion_methods(newInstances)\n\n    # replace Instances with the new class\n    oldInstances = Instances\n    detectron2.structures.Instances = newInstances\n\n    # clear jit cache\n    _clear_jit_cache()\n\n    try:\n        yield newInstances\n    finally:\n        # restore Instances\n        detectron2.structures.Instances = oldInstances\n        # clear jit cache\n        _clear_jit_cache()\n\n        # remove the temporary file\n        os.unlink(f.name)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n\n    # Create a new module that defines the new class.\n    module_name = \"detectron2.structures.instances_patch_{}\".format(_counter)\n    module_file = tempfile.NamedTemporaryFile(suffix=\".py\")\n    module_file.write(\n        _gen_instance_module(fields)[1].encode(\"utf-8\")\n    )  # write to file to avoid pytorch bug\n    module_file.flush()\n\n    # Import the new module and set the environment.\n    newInstances = _import(module_file.name).ScriptedInstances\n    _add_instances_conversion_methods(newInstances)\n    with patch_instances_meta(newInstances):\n        yield newInstances\n\n    # Clean up the temporary file and reset the environment.\n    module_file.close()\n    del sys.modules[module_name]\n    _clear_jit_cache()\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    # Need to clear jit caches to make sure the new class is used in the context.\n    _clear_jit_cache()\n\n    # Generate the new class and write it to a temporary file.\n    cls_name, cls_def = _gen_instance_module(fields)\n    with tempfile.NamedTemporaryFile(\"w\", suffix=\".py\", delete=False) as f:\n        f.write(cls_def)\n        f.flush()\n\n    # Import the new class.\n    newInstances = _import(f.name)\n    _add_instances_conversion_methods(newInstances)\n\n    # Set up the environment.\n    oldInstances = Instances\n    Instances = newInstances\n    yield newInstances\n\n    # Clean up.\n    Instances = oldInstances\n    os.unlink(f.name)\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    # make sure the fields are a dict\n    fields = dict(fields)\n\n    # make sure the fields are valid\n    for k, v in fields.items():\n        assert isinstance(k, str), f\"Field name must be str, got {k}\"\n        assert isinstance(v, type), f\"Field type must be type, got {v}\"\n\n    # generate a new module for the new class\n    cls_name, module = _gen_instance_module(fields)\n\n    # write the module to a temporary file\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n        f.write(module)\n        f.flush()\n        # import the module\n        module = _import(f.name)\n\n    # get the new class\n    newInstances = getattr(module, cls_name)\n\n    # add conversion methods\n    _add_instances_conversion_methods(newInstances)\n\n    # replace the original class with the new class\n    with patch_nonscriptable_classes():\n        with patch_builtin_len():\n            with torch.jit.unused():\n                Instances.__class__ = newInstances\n                # torchscript does not support __setattr__, so we need to\n                # remove the __setattr__ method to avoid errors\n                Instances.__setattr__ = object.__setattr__\n                yield\n\n    # clean up\n    Instances.__class__ = Instances\n    del Instances.__setattr__\n    _clear_jit_cache()\n    os.remove(f.name)\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        return convert_PIL_to_numpy(image, format)"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        return convert_PIL_to_numpy(image, format)"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        return convert_PIL_to_numpy(image, format)"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        return convert_PIL_to_numpy(image, format)"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        return convert_PIL_to_numpy(image, format)"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        return convert_PIL_to_numpy(image, format)"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        return convert_PIL_to_numpy(image, format)"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        return convert_PIL_to_numpy(image, format)"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        return convert_PIL_to_numpy(image, format)"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        return convert_PIL_to_numpy(image, format)"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        return convert_PIL_to_numpy(image, format)"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        return convert_PIL_to_numpy(image, format)"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        return convert_PIL_to_numpy(image, format)"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        return convert_PIL_to_numpy(image, format)"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        return convert_PIL_to_numpy(image, format)"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        return convert_PIL_to_numpy(image, format)"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        return convert_PIL_to_numpy(image, format)"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        return convert_PIL_to_numpy(image, format)"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        try:\n            image = _apply_exif_orientation(image)\n        except Exception:\n            logger = logging.getLogger(__name__)\n            logger.warning(\n                \"Failed to apply EXIF orientation. Please verify the image file is not corrupted!\"\n            )\n            raise\n        image = convert_PIL_to_numpy(image, format)\n    return image"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        try:\n            image = _apply_exif_orientation(image)\n        except Exception:\n            logger = logging.getLogger(__name__)\n            logger.warning(\n                \"Failed to apply EXIF orientation to image {}, \"\n                \"image loading may be slightly different from the original one.\".format(file_name)\n            )\n\n        image = convert_PIL_to_numpy(image, format)\n    return image"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if \"bbox\" in annotation:\n        bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n        # Note that bbox is 1d (per-instance bounding box)\n        annotation[\"bbox\"] = transforms.apply_box([bbox])[0]\n        annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    if \"segmentation\" in annotation:\n        segm = annotation[\"segmentation\"]\n        if isinstance(segm, list):\n            # polygons\n            polygons = [np.asarray(p).reshape(-1, 2) for p in segm]\n            annotation[\"segmentation\"] = [\n                p.reshape(-1) for p in transforms.apply_polygons(polygons)\n            ]\n        else:\n            # RLE\n            mask = mask_util.decode(segm)\n            mask = transforms.apply_segmentation(mask)\n            annotation[\"segmentation\"] = mask_util.encode(\n                np.array(mask[:, :, None], order=\"F\", dtype=\"uint8\")\n            )\n\n    if \"keypoints\" in annotation:\n        keypoints = transform_keypoint_annotations(\n            annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints\n\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(image_size)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform polygon/mask if exist\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            polygons = [np.asarray(p).reshape(-1, 2) for p in annotation[\"segmentation\"]]\n            annotation[\"segmentation\"] = [\n                p.reshape(-1) for p in transforms.apply_polygons(polygons)\n            ]\n        else:\n            # mask\n            mask = transforms.apply_segmentation(annotation[\"segmentation\"])\n            annotation[\"segmentation\"] = mask\n\n    # Transform keypoints if exist\n    if \"keypoints\" in annotation:\n        keypoints = transform_keypoint_annotations(\n            annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints\n\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(image_size)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform polygon/mask if exist\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygons\n            polygons = [np.asarray(p).reshape(-1, 2) for p in annotation[\"segmentation\"]]\n            annotation[\"segmentation\"] = [\n                p.reshape(-1) for p in transforms.apply_polygons(polygons)\n            ]\n        else:\n            # mask\n            mask = transforms.apply_segmentation(annotation[\"segmentation\"])\n            annotation[\"segmentation\"] = mask\n\n    # Transform keypoints if exist\n    if \"keypoints\" in annotation:\n        keypoints = transform_keypoint_annotations(\n            annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints\n\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # bboxes\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # Note that bbox is 1d (per-instance bounding box)\n    annotation[\"bbox\"] = transforms.apply_box([bbox])[0]\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # segmentation\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygons\n            polygons = [\n                np.asarray(p).reshape(-1).tolist() for p in annotation[\"segmentation\"]\n            ]\n            annotation[\"segmentation\"] = [\n                p.clip(0, image_size[key]) for p in transforms.apply_polygons(polygons)\n            ]\n        else:\n            # RLE\n            mask = mask_util.decode(annotation[\"segmentation\"])\n            mask = transforms.apply_segmentation(mask)\n            annotation[\"segmentation\"] = mask_util.encode(\n                np.array(mask[:, :, None], order=\"F\", dtype=\"uint8\")[0, :, 0]\n            )\n\n    # keypoints\n    if \"keypoints\" in annotation:\n        keypoints = transform_keypoint_annotations(\n            annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints\n\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # bboxes\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # Note that bbox transform will not change the class label, only the bbox itself.\n    bbox = transforms.apply_box([bbox])[0]\n    bbox = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # segmentation\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            polygons = [np.asarray(p).reshape(-1, 2) for p in annotation[\"segmentation\"]]\n            annotation[\"segmentation\"] = [\n                p.reshape(-1) for p in transforms.apply_polygons(polygons)\n            ]\n        else:\n            # mask\n            mask = PolygonMasks([annotation[\"segmentation\"]])\n            mask = transforms.apply_segmentation(mask)\n            annotation[\"segmentation\"] = mask.polygons[0].flatten().tolist()\n\n    # keypoints\n    if \"keypoints\" in annotation:\n        keypoints = transform_keypoint_annotations(\n            annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints\n\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(image_size)\n    annotation[\"bbox\"] = np.minimum(bbox, list(image_size + image_size)[::-1])\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform segmentation polygons or masks\n    segmentation = annotation.get(\"segmentation\", None)\n    if segmentation is not None:\n        # Pytorch's dataloader is efficient on torch.Tensor due to shared-memory,\n        # but not efficient on large generic data structures due to the use of pickle & mp.Queue.\n        # Therefore it's important to use torch.Tensor.\n        if isinstance(segmentation, list):\n            # polygons\n            polygons = [np.asarray(p).reshape(-1, 2) for p in segmentation]\n            annotation[\"segmentation\"] = [\n                p.reshape(-1) for p in transforms.apply_polygons(polygons)\n            ]\n        else:\n            # RLE\n            mask = mask_util.decode(segmentation)\n            mask = transforms.apply_segmentation(mask)\n            annotation[\"segmentation\"] = mask_util.encode(\n                np.array(mask[:, :, None], order=\"F\", dtype=\"uint8\")[0, :, 0]\n            )\n\n    # Transform keypoints\n    keypoints = annotation.get(\"keypoints\", None)\n    if keypoints is not None:\n        keypoints = transform_keypoint_annotations(keypoints, transforms, image_size, keypoint_hfl"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(image_size)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # segmentation = a list[list[float]] or a dict, one for each instance.\n    # The first one is the polygons, the second one is the uncompressed RLE\n    if \"segmentation\" in annotation:\n        segm = annotation[\"segmentation\"]\n        if isinstance(segm, list):\n            # polygons\n            polygons = [np.asarray(p).reshape(-1, 2) for p in segm]\n            annotation[\"segmentation\"] = [\n                p.reshape(-1) for p in transforms.apply_polygons(polygons)\n            ]\n        else:\n            # rle\n            mask = mask_util.decode(segm)\n            mask = transforms.apply_segmentation(mask)\n            assert tuple(mask.shape[:2]) == image_size\n            annotation[\"segmentation\"] = mask\n\n    # keypoints = a list[int] with the number of elements\n    # in the pattern [x1, y1, v1, x2, y2, v2, ...]\n    if \"keypoints\" in annotation:\n        keypoints = transform_keypoint_annotations(\n            annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints\n\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(image_size)\n    annotation[\"bbox\"] = np.minimum(bbox, list(image_size + image_size)[::-1])\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform polygon or uncompressed RLE to tight polygon\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            polygons = [np.asarray(p).reshape(-1, 2) for p in annotation[\"segmentation\"]]\n        else:\n            # uncompressed RLE\n            polygons = [mask_util.decode(annotation[\"segmentation\"])]\n        # useful only if instances is cropped\n        annotation[\"segmentation\"] = [p.flatten() for p in transforms.apply_polygons(polygons)]\n\n    # Transform keypoints\n    if \"keypoints\" in annotation:\n        keypoints = transform_keypoint_annotations(\n            annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints\n\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(image_size)\n    annotation[\"bbox\"] = np.minimum(bbox, list(image_size + image_size)[::-1])\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform polygon or uncompressed RLE to tight polygon\n    segmentation = annotation.get(\"segmentation\", None)\n    if segmentation is not None:\n        # Pytorch transforms operations are not working with uint8\n        if isinstance(segmentation, dict):\n            # RLE\n            masks = mask_util.decode(segmentation)\n            # Pytorch's dataloader is efficient on torch.uint8\n            masks = torch.from_numpy(masks.astype(\"uint8\"))\n            segmentation = masks\n\n        # polygons\n        segmentation = transforms.apply_segmentation(segmentation)\n        annotation[\"segmentation\"] = segmentation\n\n    # Transform keypoints\n    keypoints = annotation.get(\"keypoints\", None)\n    if keypoints is not None:\n        keypoints = transform_keypoint_annotations(keypoints, transforms, image_size, keypoint_hflip_indices)\n        annotation[\"keypoints\"] = keypoints\n\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(image_size)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform segmentation polygons\n    segmentation = annotation.get(\"segmentation\", None)\n    if segmentation is not None:\n        # PyTorch's dataloader is efficient on torch.Tensor due to shared-memory,\n        # but not efficient on large generic data structures due to the use of pickle & mp.Queue.\n        # Therefore it's important to use torch.Tensor.\n        if isinstance(segmentation, list):\n            # Polygons\n            polygons = [np.asarray(p).reshape(-1, 2) for p in segmentation]\n            annotation[\"segmentation\"] = [\n                p.reshape(-1) for p in transforms.apply_polygons(polygons)\n            ]\n        else:\n            # RLE\n            mask = mask_util.decode(segmentation)\n            mask = transforms.apply_segmentation(mask)\n            annotation[\"segmentation\"] = mask_util.encode(\n                np.array(mask[:, :, None], order=\"F\", dtype=\"uint8\")[0]\n            )\n\n    # Transform keypoints\n    keypoints = annotation.get(\"keypoints\", None)\n    if keypoints is not None:\n        keypoints = transform_keypoint_annotations(\n            keypoints, transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(image_size)\n    annotation[\"bbox\"] = np.minimum(bbox, list(image_size + image_size)[::-1])\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform polygon or uncompressed RLE to tight polygon\n    segmentation = annotation.get(\"segmentation\", None)\n    if segmentation is not None:\n        # Pytorch transforms operations are not working for polygons\n        if isinstance(transforms, T.TransformList) and transforms.is_identity():\n            segmentation = transforms.apply_segmentation(segmentation)\n        else:\n            segmentation = [\n                transforms.apply_segmentation(obj) for obj in segmentation\n            ]\n        annotation[\"segmentation\"] = segmentation\n\n    # Transform keypoints\n    keypoints = annotation.get(\"keypoints\", None)\n    if keypoints is not None:\n        keypoints = transform_keypoint_annotations(keypoints, transforms, image_size, keypoint_hflip_indices)\n        annotation[\"keypoints\"] = keypoints\n\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(image_size)\n    annotation[\"bbox\"] = np.minimum(bbox, list(image_size + image_size)[::-1])\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform polygon or uncompressed RLE to tight polygon\n    segmentation = annotation.get(\"segmentation\", None)\n    if segmentation is not None:\n        # PyCOCOTools requires int32\n        segmentation = [np.asarray(p, dtype=\"int32\") for p in segmentation]\n        # clip polygons (optional)\n        if not isinstance(transforms, T.NoOpTransform):\n            segmentation = transforms.apply_polygons(segmentation)\n        annotation[\"segmentation\"] = segmentation\n\n    # Transform keypoints\n    keypoints = annotation.get(\"keypoints\", None)\n    if keypoints is not None:\n        keypoints = transform_keypoint_annotations(keypoints, transforms, image_size, keypoint_hflip_indices)\n        annotation[\"keypoints\"] = keypoints\n\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # Transform bbox into a new bbox\n    bbox = transforms.apply_box([bbox])[0]\n    bbox = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = annotation[\"bbox_mode\"]\n\n    # Transform polygon (if available)\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # Polygon\n            polygons = [np.asarray(p).reshape(-1, 2) for p in annotation[\"segmentation\"]]\n            annotation[\"segmentation\"] = [\n                p.reshape(-1) for p in transforms.apply_polygons(polygons)\n            ]\n        else:\n            # RLE\n            mask = mask_util.decode(annotation[\"segmentation\"])\n            mask = transforms.apply_segmentation(mask)\n            annotation[\"segmentation\"] = mask_util.encode(\n                np.array(mask[:, :, None], order=\"F\", dtype=\"uint8\")[0, :, 0]\n            )\n\n    # Transform keypoints (if available)\n    if \"keypoints\" in annotation:\n        keypoints = transform_keypoint_annotations(\n            annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints\n\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(image_size)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform segmentation polygons or masks\n    segmentation = annotation.get(\"segmentation\", None)\n    if segmentation is not None:\n        # PyTorch TransformList forward is explicit about the\n        # dimensionality of the images it transforms\n        # (ie. `apply_polygons` expects `[[segmentation]]`\n        # and not `[segmentation]`)\n        if isinstance(segmentation, list):\n            # polygons\n            segmentation = [transforms.apply_polygons(polygons)[0] for polygons in segmentation]\n        else:\n            # RLE\n            segmentation = transforms.apply_segmentation(segmentation)\n        annotation[\"segmentation\"] = segmentation\n\n    # Transform keypoints\n    keypoints = annotation.get(\"keypoints\", None)\n    if keypoints is not None:\n        keypoints = transform_keypoint_annotations(\n            keypoints, transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints\n\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # bboxes\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(image_size)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # segmentation\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            polygons = [\n                np.asarray(p).reshape(-1, 2) for p in annotation[\"segmentation\"]\n            ]\n        else:\n            # COCO RLE\n            polygons = mask_util.decode(annotation[\"segmentation\"])\n        segmentations = [\n            p.reshape(-1) for p in transforms.apply_polygons(polygons)\n        ]\n        annotation[\"segmentation\"] = segmentations\n\n    # keypoints\n    if \"keypoints\" in annotation:\n        keypoints = transform_keypoint_annotations(\n            annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints\n\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # Transform bbox to a new box, and optionally clip to image boundary\n    bbox = transforms.apply_box([bbox])[0].clip(image_size)\n    # Transform polygon or uncompressed RLE to a new segmentation mask,\n    # and optionally resize the mask\n    if \"segmentation\" in annotation:\n        segm = annotation[\"segmentation\"]\n        if isinstance(segm, list):\n            # polygon\n            polygons = [np.asarray(p).reshape(-1, 2) for p in segm]\n            polygons = [p.reshape(-1) for p in transforms.apply_polygons(polygons)]\n            annotation[\"segmentation\"] = [p.reshape(-1, 2) for p in polygons]\n        else:\n            # uncompressed RLE\n            annotation[\"segmentation\"] = transforms.apply_segmentation(segm)\n    # Transform keypoints\n    if \"keypoints\" in annotation:\n        keypoints = transform_keypoint_annotations(\n            annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints\n\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(image_size)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform polygon or RLE masks:\n    segmentation = annotation.get(\"segmentation\", None)\n    if segmentation is not None:\n        # Pytorch transforms are not efficient for large masks, use OpenCV implementation instead\n        if isinstance(segmentation, list):\n            # Polygon\n            polygons = [np.asarray(p).reshape(-1, 2) for p in segmentation]\n            segmentations = [\n                p.reshape(-1) for p in transforms.apply_polygons(polygons)\n            ]\n        else:\n            # RLE\n            mask = mask_util.decode(segmentation)\n            mask = transforms.apply_segmentation(mask)\n            segmentation = mask_util.encode(\n                np.array(mask[:, :, None], order=\"F\", dtype=\"uint8\")\n            )[0]\n            segmentations = [segmentation]\n        annotation[\"segmentation\"] = segmentations\n\n    # Transform keypoints\n    keypoints = annotation.get(\"keypoints\", None)\n    if keypoints is not None:\n        keypoints = transform_keypoint_annotations(\n            keypoints, transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints\n\n    return annotation"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # Transform bbox to a tensor (Tensor is better for indexing, right?)\n    bbox = transforms.apply_box([bbox])[0]\n    bbox = bbox.clip(image_size)\n\n    # Transform polygon or RLE masks:\n    segmentation = annotation.get(\"segmentation\", None)\n    if segmentation is not None:\n        # Pytorch transformation operations are not applied to the mask tensor.\n        # Therefore, we need to convert the tensor to a bitmap mask and then\n        # convert it back to a polygon mask.\n        if isinstance(segmentation, list):\n            # polygon\n            polygons = [np.asarray(p).reshape(-1, 2) for p in segmentation]\n            segmentations = [\n                p.reshape(-1) for p in transforms.apply_polygons(polygons)\n            ]\n            annotation[\"segmentation\"] = segmentations\n        else:\n            # COCO RLE\n            mask = mask_util.decode(segmentation)\n            mask = transforms.apply_segmentation(mask)\n            annotation[\"segmentation\"] = mask_util.encode(\n                np.asarray(mask, order=\"F\", dtype=\"uint8\")\n            )\n\n    # Transform keypoints\n    keypoints = annotation.get(\"keypoints\", None)\n    if keypoints is not None:\n        keypoints = transform_keypoint_annotations(keypoints, transforms, image_size, keypoint_hflip_indices)\n        annotation[\"keypoints\"] = keypoints\n\n    annotation[\"bbox\"] = np.asarray(bbox, dtype=np.float32)\n    annotation[\"bbox"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # Transform bbox to a new bbox\n    bbox = transforms.apply_box([bbox])[0]\n    bbox = np.array(bbox, dtype=np.float64)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    if \"segmentation\" in annotation:\n        # each segment is a list[float]\n        if isinstance(annotation[\"segmentation\"], dict):\n            # RLE\n            masks = [annotation[\"segmentation\"]]\n            annotation[\"segmentation\"] = masks\n        else:\n            # polygons\n            segment = annotation[\"segmentation\"]\n            polygons = [np.asarray(p).reshape(-1, 2) for p in segment]\n            # transform polygons w.r.t. the transforms\n            new_segment = []\n            for polygon in polygons:\n                # NOTE The polygons returned by CocoCaptions are in x, y order.\n                # COCOAPI does not require points to have a minimum X or Y value\n                # (ie they can be negative)\n                # Standard Detectron2 Transforms however do require points to\n                # have a minimum X or Y value of 0. So we need to clip the polygon\n                # here.\n                np.clip(polygon, 0, None, out=polygon)\n                new_segment.append(transforms.apply_segmentation(polygon))\n            annotation[\"segmentation\"] = new_segment\n\n    if \"keypoints\" in annotation:\n        keypoints = transform_keypoint_annotations(\n            annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n        )"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(image_size)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform segmentation polygons or masks.\n    segmentation = annotation.get(\"segmentation\", None)\n    if segmentation is not None:\n        # Pytorch transforms operations are not working for polygons\n        if isinstance(transforms, T.TransformList) and transforms.is_polygon_transform():\n            segmentation = [\n                transforms.apply_segmentation(obj) for obj in segmentation\n            ]\n        else:\n            # transform polygon/mask RLEs\n            if isinstance(segmentation, list):\n                # Polygon\n                masks = [\n                    PolygonMasks([PolygonMasks.polygon_to_rle(obj) for obj in segm])\n                    for segm in segmentation\n                ]\n            else:\n                # mask\n                masks = [\n                    PolygonMasks(polygons=None, rles=PolygonMasks.frpy_decode(segm))\n                    for segm in segmentation\n                ]\n            # transform to tensor\n            masks = [x.tensor for x in masks]\n            # paste the masks in the right position\n            bboxes = Boxes(bbox.reshape(1, -1))\n            masks = [paste_masks_in_bbox(m[None, :, :], bboxes, 1).squeeze(0) for m in masks]\n            # convert to COCO RLE format\n            segmentation = [x.get_mask_tensor() for x in masks]\n            annotation[\"segmentation\"] ="}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        return cv2.transform(coords[:, np.newaxis, :], self.rm_coords[:2, :])[:, 0, :]"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        return cv2.transform(coords[:, np.newaxis, :], self.rm_coords[:2, :])[:, 0, :]"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        return cv2.transform(coords[:, np.newaxis, :], self.rm_coords[:2, :])[:, 0, :]"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        return cv2.transform(coords[:, np.newaxis, :], self.rm_coords[:2, :])[:, 0, :]"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        return cv2.transform(coords[:, np.newaxis, :], self.rm_coords[:2, :])[:, 0, :]"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        return cv2.transform(coords[:, np.newaxis, :], self.rm_coords[:, :2])[:, 0, :]"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        return cv2.transform(coords[:, np.newaxis, :], self.rm_coords[:2, :])"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        return cv2.transform(coords[:, np.newaxis, :], self.rm_coords[:, :2])[:, 0, :]"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        # Transform the coordinates to the new coordinate system\n        coords = coords - self.center\n        coords = np.dot(coords, self.rm_coords[:, :2].T) + self.center\n        return coords"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        coords = coords + 0.5  # center to pixel\n        coords = cv2.transform(coords[:, np.newaxis, :], self.rm_coords)\n        coords = coords - 0.5  # pixel to center\n        return coords"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        return cv2.transform(coords[:, np.newaxis, :], self.rm_coords[:, :2])[:, 0, :]"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        # NOTE: opencv is y,x for some reason.\n        coords = coords[:, [1, 0]]\n        # Need to subtract center coords before rotation\n        coords = coords - self.center\n        coords = np.dot(coords, self.rm_coords[:, :2].T) + self.center\n        return coords"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        # Transform coordinates into a Nx2 array\n        coords = np.array(coords, dtype=np.float32)\n        coords = np.reshape(coords, (-1, 2))\n        # Map the coordinates to the new image coordinates\n        new_coords = cv2.transform(coords[:, np.newaxis, :], self.rm_coords)\n        new_coords = np.squeeze(new_coords)\n        return new_coords"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        # NOTE: opencv/numpy uses xy convention (x, y), while our convention is yx (row, col)\n        # so we need to convert the point first before/after manipulation\n        coords = coords[:, ::-1]\n        coords = cv2.transform(coords, self.rm_coords)\n        coords = coords[:, ::-1]\n        return coords"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        # NOTE: opencv is y,x\n        coords = coords[:, [1, 0]]\n        # Transform the center of each point to the center of the image\n        coords = coords - self.center\n        coords = np.dot(coords, self.rm_coords[:, :2].T) + self.center\n        return coords[:, [1, 0]]"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        # Transform the coordinates to the center of the image\n        coords -= self.center\n        # Apply the actual rotation\n        coords = np.dot(coords, self.rm_coords[:, :2].T)\n        # Transform the coordinates back to the original frame where (0, 0) is the top left corner\n        coords += self.center + np.array([self.bound_w / 2, self.bound_h / 2]) - np.array(\n            [self.w / 2, self.h / 2]\n        )\n        return coords"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        # NOTE: opencv/opencv#11784: using negative of angle for coordinate\n        # system shift, instead of rot3x3 matrix\n        rm = self.rm_coords[:, :2].T\n        return cv2.transform(coords[:, np.newaxis, :], rm, flags=cv2.WARP_INVERSE_MAP)[:, 0, :]\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        # NOTE: im_scale_x and im_scale_y are not used here\n        # TODO: what is im_scale_x and im_scale_y?\n        im_scale_x, im_scale_y = self.im_scale\n        self.rm_coords[:, 0] *= im_scale_x\n        self.rm_coords[:, 1] *= im_scale_y\n        coords = coords - self.center\n        coords = np.dot(coords, self.rm_coords[:, :2].T) + self.center\n        return coords"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        # NOTE: im_center is the center of the image, not the coordinate origin.\n        # This is because the image center will be translated to the origin\n        # to perform rotation, and will be translated back at the end.\n        im_center = np.array([self.w / 2.0, self.h / 2.0])\n        # Compute the rotation matrix\n        rm = self.rm_coords.astype(coords.dtype)\n        # Apply the rotation matrix on the coordinates\n        coords = coords - im_center\n        coords = np.dot(coords, rm[:, :2]) + im_center\n        return coords"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        # NOTE: im_scale_x, im_scale_y are not used here.\n        # TODO: Need to double check if im_scale_x, im_scale_y are used\n        # in the rest of the code. If not, then remove them\n        im_scale_x, im_scale_y = self.scale_factor(self.w, self.h)\n        coords[:, 0] -= self.center[0]\n        coords[:, 1] -= self.center[1]\n        coords = np.dot(coords, self.rm_coords.T)\n        coords[:, 0] += self.center[0]\n        coords[:, 1] += self.center[1]\n        return coords"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE, **kwargs)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE, **kwargs)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE, **kwargs)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE, **kwargs)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE, **kwargs)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE, **kwargs)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE, **kwargs)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE, **kwargs)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE, **kwargs)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE, **kwargs)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE, **kwargs)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE, **kwargs)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(\n            img,\n            self.rm_image,\n            (self.bound_w, self.bound_h),\n            flags=interp,\n            borderMode=cv2.BORDER_CONSTANT,\n        )"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if self.angle % 360 == 0:\n            return img\n        if img.dtype == np.uint8:\n            interp = interp if interp is not None else self.interp\n            return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n        else:\n            assert self.expand, \"Unexpected situation\"\n            img = cv2.warpAffine(\n                img, self.rm_image, (self.bound_w, self.bound_h), flags=cv2.INTER_LINEAR\n            )\n            ret = np.empty(\n                (self.h, self.w, img.shape[2]), dtype=img.dtype\n            )  # the warpAffine functions always returns float32, so here we need to convert to the original type\n            for i in range(img.shape[2]):\n                ret[:, :, i] = cv2.warpAffine(\n                    img[:, :, i], self.rm_image, (self.bound_w, self.bound_h), flags=cv2.INTER_LINEAR\n                )\n            return ret\n\n    "}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        self.overlay_instances(\n            labels=_create_text_labels(\n                predictions.pred_classes,\n                scores=predictions.scores if predictions.has(\"scores\") else None,\n                class_names=self.metadata.get(\"thing_classes\"),\n                is_crowd=predictions.iscrowd if predictions.has(\"iscrowd\") else None,\n            ),\n            boxes=predictions.pred_boxes if predictions.has(\"pred_boxes\") else None,\n            masks=predictions.pred_masks if predictions.has(\"pred_masks\") else None,\n            keypoints=predictions.pred_keypoints\n            if predictions.has(\"pred_keypoints\")\n            else None,\n        )\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if self._instance_mode == ColorMode.IMAGE_BW:  # change to BW mode for non-person classes\n            self.output.reset_image(self._create_grayscale_image(None))\n        self.overlay_instances(\n            labels=_create_text_labels(\n                predictions.pred_classes,\n                predictions.scores,\n                self.metadata.get(\"thing_classes\"),\n                [x.get(\"iscrowd\", 0) for x in predictions],\n            ),\n            boxes=predictions.pred_boxes,\n            masks=predictions.pred_masks,\n            keypoints=predictions.pred_keypoints,\n        )\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        self.overlay_instances(\n            labels=_create_text_labels(\n                predictions.pred_classes,\n                scores=predictions.scores,\n                class_names=self.metadata.get(\"thing_classes\"),\n                is_crowd=predictions.pred_masks is None,\n            ),\n            boxes=predictions.pred_boxes if predictions.has(\"pred_boxes\") else None,\n            masks=predictions.pred_masks if predictions.has(\"pred_masks\") else None,\n            keypoints=predictions.pred_keypoints\n            if predictions.has(\"pred_keypoints\")\n            else None,\n        )\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        self.overlay_instances(\n            labels=_create_text_labels(\n                predictions.pred_classes,\n                predictions.scores,\n                self.metadata.get(\"thing_classes\"),\n                [x.get(\"iscrowd\", 0) for x in predictions],\n            ),\n            boxes=predictions.pred_boxes if predictions.has(\"pred_boxes\") else None,\n            masks=predictions.pred_masks if predictions.has(\"pred_masks\") else None,\n            keypoints=predictions.pred_keypoints\n            if predictions.has(\"pred_keypoints\")\n            else None,\n        )\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if predictions.has(\"pred_masks\"):\n            masks = np.asarray(predictions.pred_masks)\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n        else:\n            masks = None\n        if predictions.has(\"pred_masks_rle\"):\n            masks = [\n                GenericMask(x, self.output.height, self.output.width)\n                for x in predictions.pred_masks_rle\n            ]\n\n        if self._instance_mode == ColorMode.IMAGE_BW:  # grayscale image\n            self.output.reset_image(self._create_grayscale_image(masks, self.output.height, self.output.width))\n            alpha = 0.3\n        else:\n            alpha = 0.5\n\n        self.overlay_instances(\n            masks=masks,\n            boxes=boxes,\n            labels=labels,\n            keypoints=keypoints,\n            assigned_colors=None,\n            alpha=alpha,\n        )\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if self._instance_mode == ColorMode.IMAGE_BW:  # change color of text to black\n            self.output.reset_image(self._create_grayscale_image(None))\n        self.overlay_instances(\n            labels=[self.metadata.thing_classes[i] for i in predictions.pred_classes],\n            boxes=predictions.pred_boxes if predictions.has(\"pred_boxes\") else None,\n            masks=predictions.pred_masks if predictions.has(\"pred_masks\") else None,\n            keypoints=predictions.pred_keypoints\n            if predictions.has(\"pred_keypoints\")\n            else None,\n        )\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        num_instances = len(predictions)\n        if num_instances == 0:\n            return self.output\n\n        boxes = self._convert_boxes(predictions.pred_boxes)\n        boxes = BoxMode.convert(boxes, BoxMode.XYXY_ABS, BoxMode.XYXY_REL)\n        num_instances = len(predictions)\n\n        if predictions.has(\"pred_masks\"):\n            masks = np.asarray(predictions.pred_masks)\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n        else:\n            masks = None\n\n        if predictions.has(\"pred_keypoints\"):\n            keypoints = predictions.pred_keypoints\n\n        labels = _create_text_labels(\n            predictions.pred_classes,\n            predictions.scores,\n            self.metadata.get(\"thing_classes\", None),\n            keypoints=keypoints,\n        )\n\n        if self._instance_mode == ColorMode.IMAGE_BW:  # grayscale image\n            self.output.reset_image(self._create_grayscale_image(masks, boxes))\n        elif self._instance_mode == ColorMode.IMAGE:\n            self.output.reset_image(self.img)\n\n        self.overlay_boxes(boxes=boxes, labels=labels)\n        if predictions.has(\"pred_masks\"):\n            self.overlay_mask(masks=masks)\n        if predictions.has(\"pred_keypoints\"):\n            self.overlay_keypoints(keypoints=keypoints)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if self._instance_mode == ColorMode.IMAGE_BW:  # change to BW mode for segmentation\n            self.output.reset_image(self._create_grayscale_image(self.img))\n\n        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if predictions.has(\"pred_masks\"):\n            masks = np.asarray(predictions.pred_masks)\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n        else:\n            masks = None\n\n        if predictions.has(\"pred_masks_rle\"):\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in predictions.pred_masks_rle]\n\n        if predictions.has(\"pred_masks_rle\"):\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in predictions.pred_masks_rle]\n\n        if self.metadata.get(\"thing_colors\"):\n            colors = [\n                self._jitter([x / 255 for x in self.metadata.thing_colors[c]])\n                for c in classes\n            ]\n            alpha = 0.7\n        else:\n            colors = None\n            alpha = 0.5\n\n        if self._instance_mode == ColorMode.IMAGE_BW:\n            alpha = 0.3\n\n        self.overlay_instances(\n            masks=masks,\n            boxes=boxes,\n            labels=labels,\n            keypoints=keypoints"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if self._instance_mode == ColorMode.IMAGE_BW:  # change to BW mode for segmentation\n            self.output.reset_image(self._create_grayscale_image(self.img))\n        top_predictions = predictions.get(\"instances\").to(self.cpu_device)\n\n        boxes = top_predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = top_predictions.scores if predictions.has(\"scores\") else None\n        classes = top_predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        keypoints = top_predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if predictions.has(\"pred_masks\"):\n            masks = np.asarray(top_predictions.pred_masks)\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n        else:\n            masks = None\n\n        if predictions.has(\"pred_masks_rle\"):\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in top_predictions.pred_masks_rle]\n\n        if self.metadata.get(\"thing_colors\"):\n            colors = [\n                self._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in classes\n            ]\n            alpha = 0.7\n        else:\n            colors = None\n            alpha = 0.5\n\n        if self._instance_mode == ColorMode.IMAGE_BW:\n            alpha = 0.3\n\n        self.overlay_instances(\n            masks=masks,\n            boxes=boxes,\n            labels=labels,\n            keypoints=keypoints,\n            assigned_colors=colors,\n            alpha=alpha"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if self._instance_mode == ColorMode.IMAGE_BW:  # change to BW mode for segmentation\n            self.output.reset_image(self._create_grayscale_image(predictions.non_empty_mask()))\n\n        # draw masks\n        if predictions.has(\"pred_masks\"):\n            self.overlay_instances(\n                masks=predictions.pred_masks,\n                boxes=predictions.pred_boxes,\n                labels=predictions.pred_classes,\n                keypoints=predictions.pred_keypoints,\n                assigned_colors=self.metadata.get(\"thing_colors\"),\n            )\n        else:\n            self.overlay_instances(\n                masks=predictions.pred_masks_rle,\n                boxes=predictions.pred_boxes,\n                labels=predictions.pred_classes,\n                keypoints=predictions.pred_keypoints,\n                assigned_colors=self.metadata.get(\"thing_colors\"),\n            )\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        num_instances = len(predictions)\n        if num_instances == 0:\n            return self.output\n\n        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\n            colors = [\n                self._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in classes\n            ]\n            alpha = 0.8\n        else:\n            colors = None\n            alpha = 0.5\n\n        if self._instance_mode == ColorMode.IMAGE_BW:\n            self.output.reset_image(self._create_grayscale_image(self.img.shape[:2]))\n            alpha = 0.3\n\n        self.overlay_instances(\n            masks=predictions.pred_masks,\n            boxes=boxes,\n            labels=labels,\n            keypoints=keypoints,\n            assigned_colors=colors,\n            alpha=alpha,\n        )\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if self._instance_mode == ColorMode.IMAGE_BW:  # change to BW mode for non-person classes\n            self.output.reset_image(self._create_grayscale_image(np.asarray(predictions.pred_masks)))\n\n        # draw masks, if they exist\n        if predictions.has(\"pred_masks\"):\n            self.overlay_instances(\n                masks=predictions.pred_masks,\n                labels=predictions.pred_classes,\n                keypoints=predictions.pred_keypoints,\n            )\n\n        # draw boxes, if they exist\n        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if predictions.has(\"pred_boxes\"):\n            self.overlay_boxes(boxes=boxes, labels=labels, keypoints=keypoints)\n        if predictions.has(\"pred_masks\"):\n            self.overlay_masks(predictions.pred_masks)\n        if predictions.has(\"pred_keypoints\"):\n            self.draw_and_connect_keypoints(predictions.pred_keypoints)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        self.overlay_instances(\n            labels=[self.metadata.thing_classes[i] for i in predictions.pred_classes],\n            boxes=predictions.pred_boxes if predictions.has(\"pred_boxes\") else None,\n            masks=predictions.pred_masks if predictions.has(\"pred_masks\") else None,\n            keypoints=predictions.pred_keypoints\n            if predictions.has(\"pred_keypoints\")\n            else None,\n        )\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if self._instance_mode == ColorMode.IMAGE_BW:  # change to BW mode for ground truth\n            self.output.reset_image(self._create_grayscale_image(None))\n\n        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if predictions.has(\"pred_masks\"):\n            masks = np.asarray(predictions.pred_masks)\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n        else:\n            masks = None\n        if predictions.has(\"pred_masks_rle\"):\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in predictions.pred_masks_rle]\n\n        if predictions.has(\"pred_boxes_rle\"):\n            boxes = BoxMode.convert(predictions.pred_boxes_rle, BoxMode.XYWH_ABS, BoxMode.XYXY_ABS)\n            boxes = boxes.tensor.cpu().numpy()\n\n        if predictions.has(\"pred_keypoints_rle\"):\n            keypoints = predictions.pred_keypoints_rle\n\n        if self.metadata.get(\"thing_colors\"):\n            colors = [\n                self._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in classes\n            ]\n            alpha = 0.7\n        else:\n            colors = None\n            alpha = 0.5\n\n        if self.metadata.get(\"thing_dataset_id_to_contigu"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if self._instance_mode == ColorMode.IMAGE_BW:\n            self.output.reset_image(self._create_grayscale_image(predictions.has(\"pred_masks\")))\n\n        # draw masks\n        if predictions.has(\"pred_masks\"):\n            self.overlay_instances(\n                masks=predictions.pred_masks,\n                labels=None,\n                boxes=predictions.pred_boxes,\n                keypoints=predictions.pred_keypoints,\n            )\n\n        # draw boxes\n        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if self._instance_mode == ColorMode.IMAGE_BW:  # grayscale image\n            self.overlay_instances(\n                labels=labels, boxes=boxes, keypoints=keypoints, assigned_colors=None\n            )\n        else:\n            colors = self._jitter([self._random_color(x) for x in classes])\n            self.overlay_instances(\n                labels=labels, boxes=boxes, keypoints=keypoints, assigned_colors=colors\n            )\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        num_instances = len(predictions)\n        if num_instances == 0:\n            return self.output\n\n        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if self._instance_mode == ColorMode.IMAGE_BW:  # grayscale image\n            self.output.reset_image(self._create_grayscale_image(self.img.shape))\n            alpha = 0.3\n        else:\n            alpha = 0.5\n\n        # draw boxes\n        if boxes is not None:\n            self.overlay_boxes(boxes=boxes, labels=labels, alpha=alpha)\n\n        # draw masks\n        if predictions.has(\"pred_masks\"):\n            self.overlay_mask(masks=predictions.pred_masks, alpha=alpha)\n\n        # draw keypoints\n        if keypoints is not None:\n            self.overlay_keypoints(keypoints=keypoints, alpha=alpha)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if self._instance_mode == ColorMode.IMAGE_BW:\n            self.output.reset_image(self._create_grayscale_image(self.img))\n        top_predictions = predictions.get(\"instances\", None)\n        if top_predictions is None:\n            return self.output\n\n        boxes = top_predictions.pred_boxes if top_predictions.has(\"pred_boxes\") else None\n        scores = top_predictions.scores if top_predictions.has(\"scores\") else None\n        classes = top_predictions.pred_classes.tolist() if top_predictions.has(\"pred_classes\") else None\n\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        keypoints = top_predictions.pred_keypoints if top_predictions.has(\"pred_keypoints\") else None\n\n        if self.metadata.get(\"thing_colors\", None) is not None and len(self.metadata.thing_colors) > 0:\n            colors = [\n                self._jitter([x / 255 for x in self.metadata.thing_colors[c]])\n                for c in classes\n            ]\n            alpha = 0.7\n        else:\n            colors = None\n            alpha = 0.5\n\n        if top_predictions.has(\"pred_masks\"):\n            masks = np.asarray(top_predictions.pred_masks)\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n        else:\n            masks = None\n        if top_predictions.has(\"pred_masks_rle\"):\n            masks = top_predictions.pred_masks_rle\n        if top_predictions.has(\"pred_masks_rle\"):\n            masks = top_predictions.pred_masks_rle\n\n        if self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if self._instance_mode == ColorMode.IMAGE_BW:  # change color of text to black\n            self.output.reset_image(self._create_grayscale_image(None))\n        predictions = predictions.to(self.cpu_device)\n\n        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\n\n        if predictions.has(\"pred_masks\") and isinstance(predictions.pred_masks, BitMasks):\n            masks = predictions.pred_masks.tensor\n        else:\n            masks = None\n\n        if predictions.has(\"pred_keypoints\"):\n            keypoints = predictions.pred_keypoints\n        else:\n            keypoints = None\n\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if predictions.has(\"pred_masks\"):\n            masks = np.asarray(predictions.pred_masks)\n            # masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n\n        if predictions.has(\"pred_boxes\"):\n            boxes = predictions.pred_boxes.tensor.numpy() if predictions.pred_boxes.tensor.dim() == 2 else predictions.pred_boxes.tensor.cpu().numpy()\n\n        if self.metadata.get(\"thing_colors\"):\n            colors = [self._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in classes]\n            alpha = 0.8\n        else:\n            colors = None\n            alpha = 0.5\n\n        if self._instance_mode == ColorMode.IMAGE_BW:\n           "}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if self._instance_mode == ColorMode.IMAGE_BW:\n            self.output.reset_image(self._create_grayscale_image(None))\n        top_predictions = predictions.get(\"instances\").to(self.cpu_device)\n\n        boxes = top_predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = top_predictions.scores if predictions.has(\"scores\") else None\n        classes = top_predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        keypoints = top_predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if predictions.has(\"pred_masks\"):\n            masks = np.asarray(predictions.pred_masks)\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n        else:\n            masks = None\n        if predictions.has(\"pred_masks_rle\"):\n            masks = [\n                GenericMask(x, self.output.height, self.output.width) for x in predictions.pred_masks_rle\n            ]\n\n        if predictions.has(\"pred_keypoints\"):\n            keypoints = predictions.pred_keypoints\n\n        if self.metadata.get(\"thing_colors\"):\n            colors = [\n                self._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in classes\n            ]\n            alpha = 0.7\n        else:\n            colors = None\n            alpha = 0.5\n\n        if self.metadata.get(\"thing_dataset_id_to_contiguous_id\"):\n            try:\n                class_colors = [\n                    self.metadata.thing_colors[self.metadata.thing_dataset_id_to_contiguous_id["}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if self._instance_mode == ColorMode.IMAGE_BW:  # change to BW mode for ground truth\n            self.output.reset_image(self._create_grayscale_image(None))\n\n        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes.tolist() if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if predictions.has(\"pred_masks\"):\n            masks = np.asarray(predictions.pred_masks)\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n        else:\n            masks = None\n        if predictions.has(\"pred_masks_rle\"):\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in predictions.pred_masks_rle]\n\n        if predictions.has(\"pred_boxes\"):\n            boxes = self._convert_boxes(boxes)\n\n        if predictions.has(\"pred_masks\"):\n            masks = self._convert_masks(masks)\n            # TODO: remove the if condition below\n            # Because there is no clean way to remove masks without any computation\n            if len(masks) == 0:\n                masks = None\n\n        if predictions.has(\"pred_keypoints\"):\n            keypoints = self._convert_keypoints(keypoints)\n\n        if self.metadata.get(\"thing_colors\"):\n            colors = [\n                self._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in classes\n            ]\n            alpha = 0.7\n        else:"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        canvas = self.canvas\n        s, (width, height) = canvas.print_to_buffer()\n        buffer = np.frombuffer(s, np.uint8)\n        img_rgba = buffer.reshape(height, width, 4)\n        rgb, alpha = np.split(img_rgba, [3], axis=2)\n        return rgb.astype(\"uint8\")"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        canvas = self.canvas\n        s, (width, height) = canvas.print_to_buffer()\n        buffer = np.frombuffer(s, np.uint8)\n\n        img_rgba = buffer.reshape(height, width, 4)\n        rgb, alpha = np.split(img_rgba, [3], axis=2)\n        return rgb.astype(\"uint8\")"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        canvas = self.canvas\n        s, (width, height) = canvas.print_to_buffer()\n        buffer = np.frombuffer(s, np.uint8)\n        img_rgba = buffer.reshape(height, width, 4)\n        rgb, alpha = np.split(img_rgba, [3], axis=2)\n        return rgb.astype(\"uint8\")"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        canvas = self.canvas\n        s, (width, height) = canvas.print_to_buffer()\n        buffer = np.frombuffer(s, np.uint8)\n\n        img_bgr = buffer.reshape(height, width, 4)\n        img_rgb = img_bgr[:, :, :3][:, :, ::-1]\n        return img_rgb"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        canvas = self.canvas\n        s, (width, height) = canvas.print_to_buffer()\n        buffer = np.frombuffer(s, np.uint8)\n\n        img_bgr = buffer.reshape(height, width, 4)\n        img_rgb = img_bgr[:, :, :3][:, :, ::-1]\n        return img_rgb.copy()"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        canvas = self.canvas\n        s, (width, height) = canvas.print_to_buffer()\n        buffer = np.frombuffer(s, np.uint8)\n\n        img_rgba = buffer.reshape(height, width, 4)\n        rgb, alpha = np.split(img_rgba, [3], axis=2)\n        return rgb.astype(\"uint8\")"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        canvas = self.canvas\n        s, (width, height) = canvas.print_to_buffer()\n        buffer = np.frombuffer(s, np.uint8)\n\n        img_bgr = buffer.reshape(height, width, 4)\n        img_rgb = img_bgr[:, :, :3][:, :, ::-1]\n        return img_rgb"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        canvas = self.canvas\n        s, (width, height) = canvas.print_to_buffer()\n        buffer = np.frombuffer(s, np.uint8)\n\n        img_bgr = cv2.imdecode(buffer, cv2.IMREAD_COLOR)\n        img_rgb = img_bgr[:, :, ::-1]\n        return img_rgb.copy()"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        canvas = self.canvas\n        s, (width, height) = canvas.print_to_buffer()\n        buffer = np.fromstring(s, np.uint8)\n\n        img_rgba = buffer.reshape(height, width, 4)\n        rgb, alpha = np.split(img_rgba, [3], axis=2)\n        return rgb.astype(\"uint8\")"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        canvas = self.canvas\n        s, (width, height) = canvas.print_to_buffer()\n        buffer = np.frombuffer(s, np.uint8)\n\n        img_bgr = buffer.reshape(height, width, 4)\n        img_rgb = img_bgr[:, :, :3][:, :, ::-1]\n        return img_rgb.copy()"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        canvas = self.canvas\n        s, (width, height) = canvas.print_to_buffer()\n        buffer = np.frombuffer(s, np.uint8)\n\n        img_bgr = cv2.imdecode(buffer, cv2.IMREAD_COLOR)\n        img_rgb = img_bgr[:, :, ::-1]\n        return img_rgb\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        canvas = self.canvas\n        s, (width, height) = canvas.print_to_buffer()\n        buffer = np.fromstring(s, np.uint8)\n\n        img_rgba = buffer.reshape(height, width, 4)\n        rgb, alpha = np.split(img_rgba, [3], axis=2)\n        return rgb.astype(\"uint8\")\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        canvas = self.canvas\n        s, (width, height) = canvas.print_to_buffer()\n        buffer = np.frombuffer(s, np.uint8)\n\n        img_bgr = buffer.reshape(height, width, 4)\n        img_rgb = img_bgr[:, :, :3][:, :, ::-1]\n        return img_rgb\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        canvas = self.canvas\n        s, (width, height) = canvas.print_to_buffer()\n        buffer = np.fromstring(s, np.uint8)\n\n        img_rgba = buffer.reshape(height, width, 4)\n        rgb, alpha = np.split(img_rgba, [3], axis=2)\n        return rgb.astype(\"uint8\")\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        canvas = self.canvas\n        s, (width, height) = canvas.print_to_buffer()\n        buffer = np.frombuffer(s, np.uint8)\n\n        img_bgr = buffer.reshape(height, width, 4)\n        img_rgb = img_bgr[:, :, :3][:, :, ::-1]\n        return img_rgb.copy()\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        canvas = self.canvas\n        s, (width, height) = canvas.print_to_buffer()\n        buffer = np.frombuffer(s, np.uint8)\n\n        img_bgr = buffer.reshape(height, width, 4)\n        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGRA2RGB)\n        img_rgb = img_rgb.astype(\"uint8\")\n        return img_rgb\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        return self.fig.canvas.renderer.buffer_rgba()\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        self.canvas.draw()\n        s, (width, height) = self.canvas.print_to_buffer()\n        buffer = np.frombuffer(s, np.uint8)\n\n        # Reshape from (width, height, ARGB) to (height, width, RGBA).\n        buffer = buffer.reshape((height, width, 4))\n        buffer = buffer[:, :, :3]\n        buffer = buffer[:, :, ::-1]\n        return buffer\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        canvas = self.canvas\n        s, (width, height) = canvas.print_to_buffer()\n        buffer = np.frombuffer(s, np.uint8)\n\n        img_bgr = buffer.reshape(height, width, 4)\n        img_rgb = img_bgr[:, :, :3][:, :, ::-1]\n        return img_rgb.astype(\"uint8\")\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        canvas = self.canvas\n        s, (width, height) = canvas.print_to_buffer()\n        buffer = np.frombuffer(s, np.uint8)\n\n        # What does the above code block do?\n        # 1. Calls the `print_to_buffer` method of the `canvas` attribute of the object.\n        # 2. This method returns a tuple containing a string `s` and a tuple `(width, height)`.\n        # 3. The string `s` is converted to a NumPy array of unsigned 8-bit integers using `np.frombuffer`.\n        # 4. The resulting NumPy array is assigned to the variable `buffer`.\n\n        # The purpose of this code block is to extract the image data from the canvas and convert it to a NumPy array.\n        # The resulting array can be used for further processing or visualization.\n\n        # The `buffer` variable contains the image data as a 1D NumPy array.\n        # The `width` and `height` variables contain the dimensions of the image.\n\n        # Note that the image data in the buffer is in RGBA format, with each pixel represented by 4 bytes (RGBA channels).\n        # If you need a specific format or color space, you may need to convert the buffer using appropriate libraries or functions.\n\n        # Example usage:\n        # You can use the `buffer` variable to create a PIL image or save it to a file using libraries like PIL or OpenCV.\n        # Here's an example of creating a PIL image from the buffer:\n        # ```\n        # from PIL import Image\n        # img = Image.frombuffer(\"RGBA\", (width, height), buffer, \"raw\", \"RGBA\", 0, 1)\n        # ```\n        # This creates a PIL image object `img` from the buffer data, with dimensions `width` and `height`,\n        # in RGBA format.\n\n        # Similarly, you can use OpenCV to save the buffer data to a file:\n        # ```\n        # import cv2\n        # img = np.frombuffer(buffer, np.uint8).resh"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        annos = dic.get(\"annotations\", None)\n        if annos:\n            if \"segmentation\" in annos[0]:\n                masks = [x[\"segmentation\"] for x in annos]\n            else:\n                masks = None\n            if \"keypoints\" in annos[0]:\n                keypoints = [x[\"keypoints\"] for x in annos]\n            else:\n                keypoints = None\n            boxes = [BoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYXY_ABS) for x in annos]\n            labels = [x[\"category_id\"] for x in annos]\n            names = self.metadata.get(\"thing_classes\", None)\n            labels = [names[i] for i in labels] if names else labels\n            colors = [x[\"color\"] for x in annos] if \"color\" in annos[0] else None\n            area = [x[\"area\"] for x in annos] if \"area\" in annos[0] else None\n            iscrowd = [x[\"iscrowd\"] for x in annos] if \"iscrowd\" in annos[0] else None\n            self.overlay_instances(\n                labels=labels,\n                boxes=boxes,\n                masks=masks,\n                keypoints=keypoints,\n                assigned_colors=colors,\n                alpha=0.3,\n            )\n\n        sem_seg = dic.get(\"sem_seg\", None)\n        if sem_seg is None and \"sem_seg_file_name\" in dic:\n            sem_seg = PathManager.get_local_path(dic[\"sem_seg_file_name\"])\n        if sem_seg is not None:\n            self.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)\n\n        pan_seg = dic.get(\"pan_seg\", None)\n        segments_info = dic.get(\"segments_info\", None)\n        if pan_seg is not None:\n            self.draw"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        annos = dic.get(\"annotations\", None)\n        if annos:\n            if \"segmentation\" in annos[0]:\n                masks = [x[\"segmentation\"] for x in annos]\n            else:\n                masks = None\n            if \"keypoints\" in annos[0]:\n                keypoints = [x[\"keypoints\"] for x in annos]\n            else:\n                keypoints = None\n            boxes = [BoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYXY_ABS) for x in annos]\n            labels = [x[\"category_id\"] for x in annos]\n            names = self.metadata.get(\"thing_classes\", None)\n            labels = [names[i] for i in labels] if names else labels\n            colors = [x[\"color\"] for x in annos] if \"color\" in annos[0] else None\n            areas = [x[\"area\"] for x in annos] if \"area\" in annos[0] else None\n            self.overlay_instances(\n                labels=labels,\n                boxes=boxes,\n                masks=masks,\n                keypoints=keypoints,\n                assigned_colors=colors,\n                alpha=0.3,\n            )\n\n        sem_seg = dic.get(\"sem_seg\", None)\n        if sem_seg is None and \"sem_seg_file_name\" in dic:\n            sem_seg = PathManager.get_local_path(dic[\"sem_seg_file_name\"])\n        if sem_seg is not None:\n            self.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)\n\n        pan_seg = dic.get(\"pan_seg\", None)\n        segments_info = dic.get(\"segments_info\", None)\n        if pan_seg is not None:\n            self.draw_panoptic_seg(pan_seg, segments_info, area_threshold=0, alpha=0.3)\n\n        return self."}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"], area_threshold=area_threshold)\n        if \"instances\" in dic:\n            self.draw_instance_predictions(dic[\"instances\"])\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(\n                dic[\"panoptic_seg\"][\"segments_info\"],\n                dic[\"panoptic_seg\"][\"segments_info\"],\n                area_threshold=area_threshold,\n            )\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        annos = dic.get(\"annotations\", None)\n        if annos:\n            if \"segmentation\" in annos[0]:\n                masks = [x[\"segmentation\"] for x in annos]\n            else:\n                masks = None\n            if \"keypoints\" in annos[0]:\n                # TODO return a list[list[float]] instead?\n                keypoints = [x[\"keypoints\"] for x in annos]\n            else:\n                keypoints = None\n            boxes = [BoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYXY_ABS) for x in annos]\n            labels = [x[\"category_id\"] for x in annos]\n            names = self.metadata.get(\"thing_classes\", None)\n            labels = [names[i] for i in labels] if names else labels\n            colors = [x[\"color\"] for x in annos] if \"color\" in annos[0] else None\n            areas = [x[\"area\"] for x in annos] if \"area\" in annos[0] else None\n            self.overlay_instances(\n                labels=labels,\n                boxes=boxes,\n                masks=masks,\n                keypoints=keypoints,\n                assigned_colors=colors,\n                alpha=0.3,\n            )\n\n        sem_seg = dic.get(\"sem_seg\", None)\n        if sem_seg is None and \"sem_seg_file_name\" in dic:\n            sem_seg = PathManager.get_local_path(dic[\"sem_seg_file_name\"])\n        if sem_seg is not None:\n            self.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)\n\n        pan_seg = dic.get(\"pan_seg\", None)\n        segments_info = dic.get(\"segments_info\", None)\n        if pan_seg is None and \"pan_seg_file_name\" in dic:\n            pan_seg = PathManager.get_local_path"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"], area_threshold=area_threshold)\n        if \"instances\" in dic:\n            self.draw_instance_predictions(dic[\"instances\"])\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(\n                dic[\"panoptic_seg\"][0],\n                dic[\"panoptic_seg\"][1],\n                area_threshold=area_threshold,\n                alpha=0.6,\n            )\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        annos = dic.get(\"annotations\", None)\n        if annos:\n            if \"segmentation\" in annos[0]:\n                masks = [x[\"segmentation\"] for x in annos]\n            else:\n                masks = None\n            if \"keypoints\" in annos[0]:\n                keypts = [x[\"keypoints\"] for x in annos]\n                keypts = np.array(keypts).reshape(len(keypts), -1, 3)\n            else:\n                keypts = None\n\n            boxes = [BoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYXY_ABS) for x in annos]\n            boxes = np.array(\n                [\n                    [\n                        (box[0], box[1]),\n                        (box[2], box[1]),\n                        (box[2], box[3]),\n                        (box[0], box[3]),\n                    ]\n                    for box in boxes\n                ]\n            )\n            labels = [x[\"category_id\"] for x in annos]\n            names = self.metadata.get(\"thing_classes\", None)\n            labels = _create_text_labels(labels, None, names)\n            colors = _create_text_labels(labels, None, names, [x.get(\"iscrowd\", 0) for x in annos])\n            self.overlay_instances(\n                labels=labels, boxes=boxes, masks=masks, keypoints=keypts, assigned_colors=colors\n            )\n\n        sem_seg = dic.get(\"sem_seg\", None)\n        if sem_seg is None and \"sem_seg_file_name\" in dic:\n            sem_seg = PathManager.get_local_path(dic[\"sem_seg_file_name\"])\n        if sem_seg is not None:\n            self.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)\n        pan_seg = dic.get(\"pan_seg\", None)"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        annos = dic.get(\"annotations\", None)\n        if annos:\n            if \"segmentation\" in annos[0]:\n                masks = [x[\"segmentation\"] for x in annos]\n            else:\n                masks = None\n            if \"keypoints\" in annos[0]:\n                keypts = [x[\"keypoints\"] for x in annos]\n                keypts = np.array(keypts).reshape(len(keypts), -1, 3)\n            else:\n                keypts = None\n\n            boxes = [BoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYXY_ABS) for x in annos]\n            boxes = np.array(\n                [\n                    [\n                        (x1, y1, x2, y2),\n                    ]\n                    for x1, y1, x2, y2 in boxes\n                ]\n            )\n            labels = [x[\"category_id\"] for x in annos]\n            names = self.metadata.get(\"thing_classes\", None)\n            labels = _create_text_labels(labels, None, names)\n            colors = None\n            alpha = 0.5\n            if self._instance_mode == ColorMode.SEGMENTATION:\n                colors = [\n                    self._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in labels\n                ]\n                alpha = 0.7\n\n            self.overlay_instances(\n                labels=labels,\n                boxes=boxes,\n                masks=masks,\n                keypoints=keypts,\n                assigned_colors=colors,\n                alpha=alpha,\n            )\n\n        sem_seg = dic.get(\"sem_seg\", None)\n        if sem_seg is None and \"sem_seg_file_name\" in dic:\n            sem_seg = PathManager.get_local_path(dic[\"sem_seg_file_name\"])\n        if sem_seg is not None:\n            self.draw_sem_seg"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        annos = dic.get(\"annotations\", None)\n        if annos:\n            if \"segmentation\" in annos[0]:\n                masks = [x[\"segmentation\"] for x in annos]\n            else:\n                masks = None\n            if \"keypoints\" in annos[0]:\n                keypoints = [x[\"keypoints\"] for x in annos]\n            else:\n                keypoints = None\n            boxes = [BoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYXY_ABS) for x in annos]\n            labels = [x[\"category_id\"] for x in annos]\n            names = self.metadata.get(\"thing_classes\", None)\n            labels = [names[i] for i in labels] if names else labels\n            colors = [x[\"color\"] for x in annos] if \"color\" in annos[0] else None\n            area = [x[\"area\"] for x in annos] if \"area\" in annos[0] else None\n            iscrowd = [x.get(\"iscrowd\", 0) for x in annos]\n            self.overlay_instances(\n                labels=labels,\n                boxes=boxes,\n                masks=masks,\n                keypoints=keypoints,\n                assigned_colors=colors,\n                alpha=0.3,\n            )\n\n        sem_seg = dic.get(\"sem_seg\", None)\n        if sem_seg is None:\n            if \"sem_seg_file_name\" in dic:\n                sem_seg = PathManager.get_local_path(dic[\"sem_seg_file_name\"])\n            else:\n                sem_seg = dic.get(\"semantic\", None)\n        if sem_seg is not None:\n            self.draw_sem_seg(sem_seg, area_threshold=area_threshold, alpha=0.5)\n\n        pan_seg = dic.get(\"pan_seg\", None)\n        segments_info = dic.get(\"segments_info\", None"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic.pop(\"sem_seg\"), area_threshold=area_threshold)\n        if \"instances\" in dic:\n            self.draw_instance_predictions(dic.pop(\"instances\"))\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg_predictions(\n                dic.pop(\"panoptic_seg\")[0],\n                dic.pop(\"panoptic_seg\")[1],\n                area_threshold=area_threshold,\n                alpha=alpha,\n            )\n        if \"segmentation\" in dic:\n            dic.pop(\"segmentation\")\n        if \"keypoints\" in dic:\n            dic.pop(\"keypoints\")\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        dic = copy.deepcopy(dic)  # don't modify original data, assume they will be reused\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic.pop(\"sem_seg\"), area_threshold=area_threshold)\n        if \"instances\" in dic:\n            self.draw_instance_predictions(dic.pop(\"instances\"))\n        if \"panoptic_seg\" in dic:\n            panoptic_seg, segments_info = dic.pop(\"panoptic_seg\")\n            self.draw_panoptic_seg(\n                panoptic_seg, segments_info, area_threshold=area_threshold, alpha=0.3\n            )\n        if \"keypoints\" in dic:\n            keypoints = dic.pop(\"keypoints\")\n            self.draw_and_connect_keypoints(keypoints)\n        if \"keypoints_crop\" in dic:\n            keypoints = dic.pop(\"keypoints_crop\")\n            self.draw_and_connect_keypoints(keypoints)\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        dic = copy.deepcopy(dic)  # don't modify the input dict\n        if \"annotations\" in dic:\n            self._draw_annotations(dic.pop(\"annotations\"), self.output)\n        if \"sem_seg\" in dic:\n            self._draw_sem_seg(dic.pop(\"sem_seg\"), self.output)\n        if \"panoptic_seg\" in dic:\n            panoptic_seg, segments_info = dic.pop(\"panoptic_seg\")\n            self._draw_panoptic_seg(panoptic_seg, segments_info, self.output)\n        if \"keypoints\" in dic:\n            self._draw_keypoints(dic.pop(\"keypoints\"), self.output)\n\n        if \"instances\" in dic:\n            self._draw_instances(dic.pop(\"instances\"), self.output)\n        if \"proposals\" in dic:\n            self._draw_boxes(dic.pop(\"proposals\"), self.output)\n        if \"gt_masks\" in dic:\n            self._draw_masks(dic.pop(\"gt_masks\"), self.output)\n        if \"gt_keypoints\" in dic:\n            self._draw_keypoints(dic.pop(\"gt_keypoints\"), self.output)\n        if \"gt_boxes\" in dic:\n            self._draw_boxes(dic.pop(\"gt_boxes\"), self.output)\n\n        if \"pred_masks\" in dic:\n            self._draw_masks(dic.pop(\"pred_masks\"), self.output)\n        if \"pred_keypoints\" in dic:\n            self._draw_keypoints(dic.pop(\"pred_keypoints\"), self.output)\n        if \"pred_boxes\" in dic:\n            self._draw_boxes(dic.pop(\"pred_boxes\"), self.output)\n\n        if len(dic):\n            logger.warning(\n                \"Extra data in Detectron2 Dataset format. \"\n                \"The following keys are not drawn:\\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic.pop(\"sem_seg\"), area_threshold=None)\n        if \"instances\" in dic:\n            self.draw_instance_predictions(dic.pop(\"instances\"))\n        if \"panoptic_seg\" in dic:\n            panoptic_seg, segments_info = dic.pop(\"panoptic_seg\")\n            self.draw_panoptic_seg(panoptic_seg, segments_info, area_threshold=None)\n        if \"keypoints\" in dic:\n            self.draw_and_connect_keypoints(dic.pop(\"keypoints\"))\n        if \"proposals\" in dic and len(dic[\"proposals\"]) > 0:\n            self.overlay_boxes(dic.pop(\"proposals\"), alpha=0.5)\n        if \"keypoints\" in dic and len(dic[\"keypoints\"]) > 0:\n            self.overlay_keypoints(dic.pop(\"keypoints\"))\n        if \"gt_masks\" in dic and len(dic[\"gt_masks\"]) > 0:\n            self.overlay_masks(dic.pop(\"gt_masks\"), alpha=0.5)\n        if \"gt_boxes\" in dic and len(dic[\"gt_boxes\"]) > 0:\n            self.overlay_boxes(dic.pop(\"gt_boxes\"), alpha=0.5)\n        if \"gt_keypoints\" in dic and len(dic[\"gt_keypoints\"]) > 0:\n            self.overlay_keypoints(dic.pop(\"gt_keypoints\"))\n        if \"gt_masks\" in dic and len(dic[\"gt_masks\"]) > 0:\n            self.overlay_masks(dic.pop(\"gt_masks\"), alpha=0.5)\n        if \"gt_boxes\" in dic and len(dic[\"gt_boxes\"]) > 0:\n            self.overlay_boxes(dic.pop(\"gt"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"], area_threshold=area_threshold)\n        if \"instances\" in dic:\n            self.draw_instance_predictions(dic[\"instances\"])\n        if \"panoptic_seg\" in dic:\n            panoptic_seg, segments_info = dic[\"panoptic_seg\"]\n            self.draw_panoptic_seg(\n                panoptic_seg, segments_info, area_threshold=area_threshold, alpha=0.5\n            )\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic.pop(\"sem_seg\"), area_threshold=0.5)\n        if \"instances\" in dic:\n            self.draw_instance_predictions(dic.pop(\"instances\"))\n        if \"panoptic_seg\" in dic:\n            panoptic_seg, segments_info = dic.pop(\"panoptic_seg\")\n            self.draw_panoptic_seg(\n                panoptic_seg, segments_info, area_threshold=0.5, alpha=0.3\n            )\n        if \"keypoints\" in dic:\n            self.draw_keypoints(dic.pop(\"keypoints\"))\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"sem_seg\" in dic:\n            sem_seg = dic.pop(\"sem_seg\")\n            self.draw_sem_seg(sem_seg, area_threshold=None, alpha=0.7)\n        if \"instances\" in dic:\n            self.draw_instance_predictions(dic.pop(\"instances\"))\n        if \"panoptic_seg\" in dic:\n            panoptic_seg, segments_info = dic.pop(\"panoptic_seg\")\n            self.draw_panoptic_seg(\n                panoptic_seg, segments_info, area_threshold=None, alpha=0.7\n            )\n        if \"keypoints\" in dic:\n            keypoints = dic.pop(\"keypoints\")\n            self.draw_keypoints(keypoints)\n        if \"groundtruth_boxes\" in dic:\n            groundtruth_boxes = dic.pop(\"groundtruth_boxes\")\n            self.draw_groundtruth_boxes(groundtruth_boxes)\n        if \"groundtruth_polygons\" in dic:\n            groundtruth_polygons = dic.pop(\"groundtruth_polygons\")\n            self.draw_groundtruth_polygons(groundtruth_polygons)\n        if \"groundtruth_masks\" in dic:\n            groundtruth_masks = dic.pop(\"groundtruth_masks\")\n            self.draw_groundtruth_masks(groundtruth_masks)\n        if \"groundtruth_keypoints\" in dic:\n            groundtruth_keypoints = dic.pop(\"groundtruth_keypoints\")\n            self.draw_groundtruth_keypoints(groundtruth_keypoints)\n        if \"groundtruth_segmentation\" in dic:\n            groundtruth_segmentation = dic.pop(\"groundtruth_segmentation\")\n            self.draw_groundtruth_segmentation(groundtruth_segmentation)\n        if \"groundtruth_boxes_vis\" in dic:\n            groundtruth_boxes_vis = dic.pop(\"groundtruth_boxes_vis\")"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic.pop(\"sem_seg\"), area_threshold=area_threshold)\n        if \"instances\" in dic:\n            self.draw_instance_predictions(dic.pop(\"instances\"))\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(\n                dic.pop(\"panoptic_seg\"),\n                dic.pop(\"segments_info\", None),\n                area_threshold=area_threshold,\n            )\n        if \"keypoints\" in dic:\n            self.draw_keypoints(dic.pop(\"keypoints\"))\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        dic = copy.deepcopy(dic)  # don't modify the input dict\n        if \"annotations\" in dic:\n            self._draw_annotations(dic.pop(\"annotations\"), self.output)\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic.pop(\"sem_seg\"), area_threshold=area_threshold)\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg_predictions(\n                dic.pop(\"panoptic_seg\")[\"segments_info\"],\n                dic.pop(\"panoptic_seg\")[\"segments_info\"],\n                area_threshold=area_threshold,\n            )\n        if \"keypoints\" in dic:\n            self._draw_keypoints(dic.pop(\"keypoints\"), self.output)\n        if \"pred_masks\" in dic:\n            self.overlay_instances(masks=dic.pop(\"pred_masks\"), labels=None, assigned_colors=None)\n        if \"gt_masks\" in dic:\n            self.overlay_instances(masks=dic.pop(\"gt_masks\"), labels=None, assigned_colors=None)\n        if \"pred_boxes\" in dic:\n            self.overlay_instances(\n                boxes=dic.pop(\"pred_boxes\"), labels=dic.pop(\"pred_classes\"), assigned_colors=None\n            )\n        if \"gt_boxes\" in dic:\n            self.overlay_instances(\n                boxes=dic.pop(\"gt_boxes\"), labels=dic.pop(\"gt_classes\"), assigned_colors=None\n            )\n        if \"pred_keypoints\" in dic:\n            self._draw_keypoints(dic.pop(\"pred_keypoints\"), self.output)\n        if \"gt_keypoints\" in dic:\n            self._draw_keypoints(dic.pop(\"gt_keypoints\"), self.output)\n        if \"gt_boxes\" in dic and \"gt_masks\""}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic.pop(\"sem_seg\"), area_threshold=0.5)\n        if \"segments_info\" in dic:\n            self.draw_panoptic_seg(dic.pop(\"panoptic_seg\"), dic.pop(\"segments_info\"), area_threshold=0.5)\n        else:\n            self.draw_instance_predictions(dic.pop(\"instances\"))\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic.pop(\"sem_seg\"), area_threshold=None)\n        if \"instances\" in dic:\n            self.draw_instance_predictions(dic.pop(\"instances\"))\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(dic.pop(\"panoptic_seg\"), dic.pop(\"segments_info\"))\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic.pop(\"sem_seg\"), area_threshold=0.5)\n        if \"segmentation\" in dic:\n            self.draw_instance_predictions(dic.pop(\"segmentation\"))\n        if \"annotations\" in dic:\n            annos = dic.pop(\"annotations\")\n            if \"segmentation\" in annos:\n                self.draw_instance_predictions(annos.pop(\"segmentation\"))\n            if \"keypoints\" in annos:\n                self.draw_instance_predictions(annos.pop(\"keypoints\"))\n\n        if \"panoptic_seg\" in dic:\n            panoptic_seg, segments_info = dic[\"panoptic_seg\"]\n            self.draw_panoptic_seg(panoptic_seg, segments_info, area_threshold=0.5)\n        elif \"panoptic_seg_predictions\" in dic:\n            self.draw_panoptic_seg(dic.pop(\"panoptic_seg_predictions\"), segments_info=None, area_threshold=0.5)\n\n        self.draw_dataset_dict(dic)\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        if edge_color is None:\n            # make edge color darker than the polygon color\n            if alpha > 0.8:\n                edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n            else:\n                edge_color = color\n        edge_color = mplc.to_rgb(edge_color) + (1,)\n\n        shape2d = (binary_mask.shape[0], binary_mask.shape[1])\n        rgba = np.zeros(shape2d + (4,), dtype=\"float32\")\n        rgba[:, :, :3] = color\n        rgba[:, :, 3] = binary_mask * alpha\n        self.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\n\n        if text is not None:\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            self._draw_text_in_mask(binary_mask, text, lighter_color)\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        if edge_color is None:\n            # make edge color darker than the polygon color\n            if alpha > 0.8:\n                edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n            else:\n                edge_color = color\n        edge_color = mplc.to_rgb(edge_color) + (1,)\n\n        mask = GenericMask(binary_mask, self.output.height, self.output.width)\n        if mask.area() < area_threshold:\n            return self.output\n\n        # draw mask\n        for segment in mask.polygons:\n            self.draw_polygon(segment.reshape(-1, 2), color, edge_color=edge_color, alpha=alpha)\n\n        if text is not None:\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            self._draw_text_in_mask(binary_mask, text, lighter_color)\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        if edge_color is None:\n            # make edge color darker than the polygon color\n            if alpha > 0.8:\n                edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n            else:\n                edge_color = color\n        edge_color = mplc.to_rgb(edge_color) + (1,)\n\n        # Draw the mask on the image\n        for c in range(3):\n            binary_mask[:, :, c] = np.where(\n                binary_mask[:, :, c] == 1,\n                binary_mask[:, :, c] * color[c] * 255,\n                binary_mask[:, :, c] * edge_color[c] * 255,\n            )\n        self.output.ax.imshow(binary_mask.astype(np.uint8), alpha=alpha)\n\n        # Draw text in the center\n        if text is not None:\n            self._draw_text_in_mask(binary_mask[:, :, 0], text, color)\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        if edge_color is None:\n            # make edge color darker than the polygon color\n            if alpha > 0.8:\n                edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n            else:\n                edge_color = color\n        edge_color = mplc.to_rgb(edge_color) + (1,)\n\n        if binary_mask.shape[0] == 0 or binary_mask.shape[1] == 0:\n            return self.output\n\n        # Draw the mask on the image\n        binary_mask = np.asfortranarray(binary_mask)\n        mask = mpl.patches.PathPatch(\n            mpl.path.Path(binary_mask),\n            facecolor=mplc.to_rgba(color, alpha),\n            edgecolor=edge_color,\n            linewidth=max(self._default_font_size // 15 * self.output.scale, 1),\n        )\n        self.output.ax.add_patch(mask)\n\n        if text is not None:\n            self._draw_text_in_mask(binary_mask, text, color)\n\n        return self.output\n\n    "}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n        if edge_color is None:\n            # make edge color darker than the polygon color\n            if alpha > 0.8:\n                edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n            else:\n                edge_color = color\n        edge_color = mplc.to_rgb(edge_color) + (1,)\n\n        if binary_mask.ndim == 2:\n            binary_mask = np.repeat(binary_mask[:, :, None], 3, axis=2)\n\n        # add mask to image\n        self.output.ax.imshow(binary_mask, alpha=alpha)\n\n        # add text\n        if text is not None:\n            self._draw_text_in_mask(binary_mask, text, color)\n\n        # add outline\n        binary_mask = binary_mask.astype(np.uint8)\n        contours, _ = cv2.findContours(binary_mask[:, :, 0], cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n        for contour in contours:\n            contour = np.flip(contour[:, 0, :], 1)\n            self.output.ax.add_patch(\n                mpl.patches.Polygon(\n                    contour,\n                    fill=False,\n                    edgecolor=edge_color,\n                    linewidth=max(self._default_font_size // 15 * self.output.scale, 1),\n                )\n            )\n\n        return self.output\n\n    "}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        # Convert the binary mask to a polygon mask\n        polygon_mask = GenericMask(binary_mask, self.output.height, self.output.width)\n\n        # If the mask has holes, use a different approach to draw it\n        if polygon_mask.has_holes:\n            # Draw the mask with holes using the draw_polygon function\n            self.draw_polygon(polygon_mask.polygons, color=color, edge_color=edge_color, alpha=alpha)\n        else:\n            # Draw the mask without holes using the draw_binary_mask function\n            self.draw_binary_mask(polygon_mask.mask, color=color, edge_color=edge_color, text=text, alpha=alpha, area_threshold=area_threshold)\n\n        return self.output\n\n    "}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        if edge_color is None:\n            # make edge color darker than the polygon color\n            if alpha > 0.8:\n                edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n            else:\n                edge_color = color\n        edge_color = mplc.to_rgb(edge_color) + (1,)\n\n        binary_mask = np.asarray(binary_mask, dtype=np.uint8)\n        polygons = []\n        for contour in measure.find_contours(binary_mask, 0.5):\n            # Flip from (row, col) representation to (x, y)\n            # and subtract the padding pixel\n            contour = np.flip(contour, axis=1)\n            contour = np.asarray(contour, dtype=np.float32)\n            contour[:, 0] -= binary_mask.shape[1] / 2.0\n            contour[:, 1] -= binary_mask.shape[0] / 2.0\n            polygons.append(contour)\n\n        for polygon in polygons:\n            if polygon.shape[0] < 3:\n                continue\n            polygon = Polygon(polygon)\n            if polygon.area < area_threshold:\n                continue\n            self.draw_polygon(polygon.simplify(1.0, preserve_topology=False), color, edge_color, alpha)\n            if text is not None:\n                self._draw_text_in_mask(binary_mask, text, edge_color)\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        if edge_color is None:\n            # make edge color darker than the polygon color\n            if alpha > 0.8:\n                edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n            else:\n                edge_color = color\n        edge_color = mplc.to_rgb(edge_color) + (1,)\n\n        if binary_mask.ndim == 2:\n            binary_mask = np.repeat(binary_mask[..., None], 3, axis=-1)\n\n        # find all connected components\n        num_cc, cc_labels, stats, centroids = cv2.connectedComponentsWithStats(\n            binary_mask.astype(np.uint8), connectivity=4\n        )\n        # find the largest connected component\n        largest_component_id = np.argmax(stats[1:, -1]) + 1\n        # find the area of the largest connected component\n        largest_component_area = stats[largest_component_id, -1]\n        # if the largest connected component is not large enough, use the whole mask\n        if largest_component_area < area_threshold:\n            binary_mask = (binary_mask > 0).astype(np.uint8)\n            num_cc, cc_labels, stats, centroids = cv2.connectedComponentsWithStats(\n                binary_mask.astype(np.uint8), connectivity=4\n            )\n            largest_component_id = 1\n            largest_component_area = stats[largest_component_id, -1]\n\n        # find the contours of the largest connected component\n        largest_component_contour = (cc_labels == largest_component_id).astype(np.uint8)\n        contours, _ = cv2.findContours(\n            largest_component_contour, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMP"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        if edge_color is None:\n            # make edge color darker than the polygon color\n            if alpha > 0.8:\n                edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n            else:\n                edge_color = color\n        edge_color = mplc.to_rgb(edge_color) + (1,)\n\n        if binary_mask.ndim == 2:\n            binary_mask = np.stack([binary_mask] * 3, axis=2)\n\n        if binary_mask.shape[0] != self.output.height or binary_mask.shape[1] != self.output.width:\n            binary_mask = cv2.resize(binary_mask, (self.output.width, self.output.height))\n\n        binary_mask = (binary_mask > 0).astype(\"uint8\")\n\n        # find all connected components\n        num_cc, cc_labels, stats, centroids = cv2.connectedComponentsWithStats(\n            binary_mask, connectivity=8\n        )\n\n        # find the largest connected component\n        largest_component_id = np.argmax(stats[1:, -1]) + 1\n\n        # draw all connected components\n        for cid in range(1, num_cc):\n            if cid == largest_component_id or stats[cid, -1] > area_threshold:\n                # draw the mask\n                mask = (cc_labels == cid).astype(\"uint8\")\n                self.output.ax.imshow(\n                    np.ones(binary_mask.shape, dtype=\"uint8\") * mask[:, :, np.newaxis],\n                    alpha=alpha,\n                    extent=(0, self.output.width, self.output.height, 0),\n                )\n\n                # draw the text\n                if text is not None:\n                    lighter_color = self._change_color_brightness"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        # TODO: This function is not used anywhere. Should we remove it?\n        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n        if edge_color is None:\n            # make edge color darker than the polygon color\n            if alpha > 0.8:\n                edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n            else:\n                edge_color = color\n        edge_color = mplc.to_rgb(edge_color) + (1,)\n\n        # Draw polygons on the mask\n        contours = []\n        for c in measure.find_contours(binary_mask, 0.5):\n            # Close contour\n            if np.array_equal(c[0], c[-1]):\n                c = np.concatenate([c, [c[-2:]]])\n            contours.append(c)\n        # Draw the contours on the image\n        for contour in contours:\n            self.draw_polygon(contour, color, edge_color, alpha)\n\n        if text is not None:\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            self._draw_text_in_mask(binary_mask, text, lighter_color)\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n\n        if edge_color is None:\n            # make edge color darker than the polygon color\n            if alpha > 0.8:\n                edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n            else:\n                edge_color = color\n        edge_color = mplc.to_rgb(edge_color) + (1,)\n\n        # convert to numpy array\n        binary_mask = np.asarray(binary_mask)\n        # check type and range\n        assert binary_mask.dtype == np.uint8, binary_mask.dtype\n        assert np.all(binary_mask >= 0) and np.all(binary_mask <= 1), np.unique(binary_mask)\n\n        # find connected components\n        num_cc, cc_labels, stats, centroids = cv2.connectedComponentsWithStats(binary_mask, 8)\n\n        # find largest connected component\n        largest_component_id = np.argmax(stats[1:, -1]) + 1\n        largest_component_area = stats[largest_component_id, -1]\n        if largest_component_area < area_threshold:\n            # if no component is large enough, use the entire mask as-is\n            return self.draw_bitmask(binary_mask, color, alpha=alpha)\n        else:\n            # use only the largest component\n            binary_mask = (cc_labels == largest_component_id).astype(np.uint8)\n\n        # use bitmask draw function\n        return self.draw_bitmask(binary_mask, color, edge_color, text, alpha)\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        if edge_color is None:\n            # make edge color darker than the polygon color\n            if alpha > 0.8:\n                edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n            else:\n                edge_color = color\n        edge_color = mplc.to_rgb(edge_color) + (1,)\n\n        # convert to numpy\n        binary_mask = np.asarray(binary_mask)\n\n        # check type and perform conversion for polygon masks\n        if isinstance(binary_mask, np.ndarray):\n            # check if it is a polygon mask\n            if binary_mask.ndim == 2:\n                # check if it is a polygon mask with holes\n                if np.any(binary_mask[0]):\n                    # draw polygon mask with holes\n                    self._draw_polygon_mask_with_holes(binary_mask, color, edge_color, alpha)\n                else:\n                    # draw polygon mask without holes\n                    self._draw_polygon_mask_without_holes(binary_mask, color, edge_color, alpha)\n            else:\n                # draw binary mask\n                self._draw_binary_mask(binary_mask, color, alpha)\n        else:\n            raise ValueError(\"binary_mask should be of type numpy array\")\n\n        if text is not None:\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            self._draw_text_in_mask(binary_mask, text, lighter_color)\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        if edge_color is None:\n            # make edge color darker than the polygon color\n            if alpha > 0.8:\n                edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n            else:\n                edge_color = color\n        edge_color = mplc.to_rgb(edge_color) + (1,)\n\n        binary_mask = np.asarray(binary_mask)\n        if binary_mask.shape[0] != self.output.height or binary_mask.shape[1] != self.output.width:\n            binary_mask = cv2.resize(binary_mask, (self.output.width, self.output.height))\n\n        # find all contours in the binary mask\n        contours, hierarchy = cv2.findContours(\n            binary_mask.astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE\n        )\n\n        # draw all contours\n        for i, cnt in enumerate(contours):\n            # find the area of the contour\n            area = cv2.contourArea(cnt)\n\n            # if the contour has no area, just skip\n            if area < area_threshold:\n                continue\n\n            # find the centroid of the contour\n            M = cv2.moments(cnt)\n            cx = int(M[\"m10\"] / M[\"m00\"])\n            cy = int(M[\"m01\"] / M[\"m00\"])\n\n            # find the orientation of the contour\n            (x, y), (MA, ma), angle = cv2.fitEllipse(cnt)\n\n            # if the contour has no orientation, just draw a rectangle\n            if MA == ma:\n                angle = 0\n\n            # draw a polygon if it is a large enough cont"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        if edge_color is None:\n            # make edge color darker than the polygon color\n            if alpha > 0.8:\n                edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n            else:\n                edge_color = color\n        edge_color = mplc.to_rgb(edge_color) + (1,)\n\n        # If the binary mask is a polygon, draw it using matplotlib.patches.Polygon.\n        # Otherwise, draw it using matplotlib.patches.PathPatch.\n        if binary_mask.ndim == 2:\n            # If the binary mask is a polygon, draw it using matplotlib.patches.Polygon.\n            polygon = mpl.patches.Polygon(\n                binary_mask,\n                fill=True,\n                facecolor=mplc.to_rgb(color) + (alpha,),\n                edgecolor=edge_color,\n                linewidth=max(self._default_font_size // 15 * self.output.scale, 1),\n            )\n            self.output.ax.add_patch(polygon)\n        else:\n            # If the binary mask is a mask with holes, draw it using matplotlib.patches.PathPatch.\n            # First, find all the connected components in the binary mask.\n            # Then, for each connected component, find the contours and draw them using matplotlib.patches.PathPatch.\n            # Finally, draw the text on the mask.\n            binary_mask = np.ascontiguousarray(binary_mask)\n            _, contours, _ = cv2.findContours(binary_mask, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\n            for contour in contours:\n                # Draw the contour if it is large enough.\n                if cv2.contourArea(contour) >= area_th"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        if edge_color is None:\n            # make edge color darker than the polygon color\n            if alpha > 0.8:\n                edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n            else:\n                edge_color = color\n        edge_color = mplc.to_rgb(edge_color) + (1,)\n\n        # convert binary_mask to a polygon\n        contours = []\n        contours_holes = []\n        if isinstance(binary_mask, GenericMask):\n            binary_mask = binary_mask.mask\n        if binary_mask.ndim == 2:\n            binary_mask = np.ascontiguousarray(binary_mask)\n            contours = cv2.findContours(\n                binary_mask.astype(\"uint8\"), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE\n            )[-2:]\n            # remove the hole relationship\n            for cnt in contours[0]:\n                area = cv2.contourArea(cnt)\n                if area < area_threshold:\n                    continue\n                contours.append(cnt.astype(\"float\"))\n            # first contour is the exterior boundary\n            if len(contours) >= 1:\n                # draw the exterior boundary\n                self.draw_polygon(contours[0], color, edge_color=edge_color, alpha=alpha)\n            # other contours are interior holes\n            for hole in contours[1:]:\n                if hole.shape[0] <= area_threshold:\n                    continue  # too small\n                contours_holes.append(hole.astype(\"float\"))\n        else:\n            # convert the polygon to a list of np arrays\n            for polygon in binary_mask:\n                polygon = np.array(polygon).reshape(-1, 2)\n                # check validity of polygon\n                if len(polygon)"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n\n        if binary_mask.ndim == 2:\n            binary_mask = np.stack([binary_mask] * 3, axis=-1)\n        if edge_color is None:\n            # make edge color darker than the polygon color\n            if alpha > 0.8:\n                edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n            else:\n                edge_color = color\n        edge_color = mplc.to_rgb(edge_color) + (1,)\n\n        # draw mask\n        binary_mask_to_draw = binary_mask\n        if self._instance_mode == ColorMode.IMAGE_BW:\n            binary_mask_to_draw = binary_mask_to_draw.astype(\"uint8\")\n\n        self.output.ax.imshow(\n            binary_mask_to_draw,\n            alpha=alpha,\n            extent=(0, self.output.width, self.output.height, 0),\n            interpolation=\"nearest\",\n        )\n\n        # draw text\n        if text is not None:\n            self._draw_text_in_mask(binary_mask, text, color)\n\n        # draw outline\n        binary_mask = binary_mask.astype(\"uint8\")\n        contours = cv2.findContours(\n            binary_mask, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE\n        )[-2]  # contours are returned in a different format\n        if len(contours) > 0:\n            areas = np.asarray([cv2.contourArea(c) for c in contours])\n            contour_masks = [\n                cv2.drawContours(np.zeros_like(binary_mask), [c], -1, 1, cv2.FILLED)\n                for c in contours\n            ]\n            contour_masks = [\n                x for x, a in zip(contour"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        if edge_color is None:\n            # make edge color darker than the polygon color\n            if alpha > 0.8:\n                edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n            else:\n                edge_color = color\n        edge_color = mplc.to_rgb(edge_color) + (1,)\n\n        # Draw the mask on the image\n        for cnt in find_contours(binary_mask):\n            # Bounding boxes are often off by one pixel in the x direction, and by one pixel in\n            # the y direction. As an extreme example, a (100, 200) mask that is the exact\n            # bounding box of an object should be (99, 199) with corners (0, 0) and (99, 199)\n            # (rather than (100, 200) with corners (0, 0) and (100, 200)).\n            # Therefore, we do not call .astype(np.int32) or np.int32(mask) here,\n            # but rather deal with it afterwards.\n            cnt = np.flip(cnt, axis=1)\n            # Closed contours have a perimeter of 0, but most binary_masks have nonzero perimeter\n            if len(cnt) >= 4 and cv2.contourArea(cnt) >= area_threshold:\n                contour = Polygon(cnt)\n                if not contour.is_valid:\n                    contour = contour.buffer(0)\n                    contour = np.array(contour.exterior.coords)\n                self.draw_polygon(contour, color, edge_color=edge_color, alpha=alpha)\n                if text is not None:\n                    self._draw_text_in_mask(binary_mask, text, edge_color)\n        return"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        if edge_color is None:\n            # make edge color darker than the polygon color\n            if alpha > 0.8:\n                edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n            else:\n                edge_color = color\n        edge_color = mplc.to_rgb(edge_color) + (1,)\n\n        if binary_mask.ndim == 2:\n            binary_mask = np.repeat(binary_mask[..., None], 3, axis=-1)\n        # TODO: use a more efficient way to draw mask\n        #       (e.g. use cv2.polylines to draw polygons)\n        binary_mask = binary_mask.astype(\"uint8\")\n        contours, _ = cv2.findContours(binary_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n        for contour in contours:\n            contour = np.flip(contour[:, 0, :], 1)\n            self.draw_polygon(contour, color, edge_color=edge_color, alpha=alpha)\n\n        if text is not None:\n            # TODO: use a more efficient way to draw text\n            #       (e.g. use cv2.putText to draw text)\n            self._draw_text_in_mask(binary_mask, text, color)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        # if no color is given, pick a random one\n        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n        # if no edge color is given, make it darker than color\n        if edge_color is None:\n            if alpha > 0.8:\n                edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n            else:\n                edge_color = color\n        edge_color = mplc.to_rgb(edge_color) + (1,)\n\n        # Draw the mask on the image\n        binary_mask = np.asarray(binary_mask)\n        if binary_mask.shape[0] == 0 or binary_mask.shape[0] == 0:\n            return self.output\n\n        # If the mask is a polygon, draw it as a polygon\n        if len(binary_mask.shape) == 2:\n            # If the mask is a polygon, draw it as a polygon\n            if binary_mask.shape[0] > 0 and binary_mask.shape[1] > 0:\n                # If the mask is a polygon, draw it as a polygon\n                binary_mask = GenericMask(binary_mask, self.output.height, self.output.width)\n                polygons = binary_mask.polygons\n                for polygon in polygons:\n                    self.draw_polygon(polygon, color, edge_color=edge_color, alpha=alpha)\n        else:\n            # If the mask is a mask with holes, draw it as a mask with holes\n            # First, find the contours of the mask\n            contours, _ = cv2.findContours(binary_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n            # Then, draw the contours\n            for contour in contours:\n                # If the contour is too small, skip it\n                if cv2.contourArea(contour) < area_threshold:\n                    continue\n                # If the contour"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        if edge_color is None:\n            # make edge color darker than the polygon color\n            if alpha > 0.8:\n                edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n            else:\n                edge_color = color\n        edge_color = mplc.to_rgb(edge_color) + (1,)\n\n        # convert to numpy array\n        binary_mask = np.asarray(binary_mask)\n        # check type and range\n        assert binary_mask.dtype == bool, binary_mask.dtype\n        assert np.allclose(np.unique(binary_mask), [0, 1]), np.unique(binary_mask)\n        # check that width, height line up\n        assert (\n            binary_mask.shape[1:] == self.output.shape[:2]\n        ), f\"Shape mismatch: Expected image shape {self.output.shape[:2]}, got {binary_mask.shape}\"\n\n        # Bounding box\n        x0 = np.clip(float(np.min(binary_mask.nonzero()[1])), 0, self.output.width)\n        x1 = np.clip(float(np.max(binary_mask.nonzero()[1])), 0, self.output.width)\n        y0 = np.clip(float(np.min(binary_mask.nonzero()[0])), 0, self.output.height)\n        y1 = np.clip(float(np.max(binary_mask.nonzero()[0])), 0, self.output.height)\n        bbox = (x0, y0, x1, y1)\n\n        # Polygon\n        contours = []\n        segmentations = []\n        for object_id in range(1, binary_mask.max() + 1):\n            contours.append(binary_mask == object_id)\n            segmentation = []\n            for"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if size_as_tensor:\n        assert torch.allclose(input.image_size, other.image_size, rtol=rtol), msg\n    else:\n        assert input.image_size == other.image_size, msg\n    for name in input._field_names:\n        val = getattr(input, name)\n        other_val = getattr(other, name)\n        if isinstance(val, Boxes):\n            assert torch.allclose(val.tensor, other_val.tensor, rtol=rtol), msg\n        elif isinstance(val, ROIMasks):\n            assert val.tensor.allclose(other_val.tensor), msg\n        elif isinstance(val, torch.Tensor):\n            assert torch.allclose(val, other_val, rtol=rtol), msg\n        elif isinstance(val, Instances):\n            assert_instances_allclose(val, other_val, rtol=rtol, msg=msg, size_as_tensor=size_as_tensor)\n        else:\n            assert val == other_val, msg"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances) and isinstance(other, Instances), msg\n    assert input.image_size == other.image_size, msg\n    if size_as_tensor:\n        assert torch.tensor(input.image_size) == torch.tensor(\n            other.image_size\n        ), msg\n\n    for name in input._field_names:\n        val = getattr(input, name, None)\n        other_val = getattr(other, name, None)\n        if val is None and other_val is None:\n            continue\n        if isinstance(val, Boxes):\n            assert torch.allclose(val.tensor, other_val.tensor, rtol=rtol), msg\n        elif isinstance(val, ROIMasks):\n            assert torch.allclose(val.tensor, other_val.tensor, rtol=rtol), msg\n        elif isinstance(val, torch.Tensor):\n            assert torch.allclose(val, other_val, rtol=rtol), msg\n        else:\n            raise ValueError(f\"Unknown type {type(val)}\")"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if not isinstance(input, Instances):\n        raise ValueError(f\"input is not an Instances object: {type(input)}\")\n    if not isinstance(other, Instances):\n        raise ValueError(f\"other is not an Instances object: {type(other)}\")\n\n    if size_as_tensor:\n        assert torch.allclose(\n            torch.tensor(input.image_size), torch.tensor(other.image_size)\n        ), f\"{msg} image_size mismatch: {input.image_size} vs {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg} image_size mismatch: {input.image_size} vs {other.image_size}\"\n\n    for name in input._field_names:\n        val = getattr(input, name)\n        other_val = getattr(other, name)\n        if isinstance(val, Boxes):\n            assert torch.allclose(val.tensor, other_val.tensor), f\"{msg} {name} mismatch\"\n        elif isinstance(val, ROIMasks):\n            assert val.tensor.allclose(other_val.tensor), f\"{msg} {name} mismatch\"\n        elif isinstance(val, torch.Tensor):\n            assert torch.allclose(val, other_val, rtol=rtol), f\"{msg} {name} mismatch\"\n        else:\n            assert val == other_val, f\"{msg} {name} mismatch\""}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if size_as_tensor:\n        assert torch.allclose(\n            torch.as_tensor(input.image_size), torch.as_tensor(other.image_size)\n        ), f\"{msg} image_size mismatch: {input.image_size} != {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg} image_size mismatch: {input.image_size} != {other.image_size}\"\n    for name in input._field_names:\n        val = getattr(input, name, None)\n        other_val = getattr(other, name, None)\n        if val is None:\n            assert other_val is None, f\"{msg} {name} mismatch: {val} != {other_val}\"\n        elif isinstance(val, Boxes):\n            assert torch.allclose(val.tensor, other_val.tensor), f\"{msg} {name} mismatch: {val} != {other_val}\"\n        elif isinstance(val, ROIMasks):\n            assert torch.allclose(val.tensor, other_val.tensor), f\"{msg} {name} mismatch: {val} != {other_val}\"\n        elif isinstance(val, torch.Tensor):\n            assert torch.allclose(val, other_val, rtol=rtol), f\"{msg} {name} mismatch: {val} != {other_val}\"\n        else:\n            raise ValueError(f\"{msg} {name} has unsupported type {type(val)}\")"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances) and isinstance(other, Instances), msg\n    if size_as_tensor:\n        assert torch.allclose(\n            torch.as_tensor(input.image_size), torch.as_tensor(other.image_size)\n        ), msg\n    else:\n        assert input.image_size == other.image_size, msg\n\n    for name in input._field_names:\n        val = getattr(input, name)\n        other_val = getattr(other, name)\n        if val is None:\n            assert other_val is None, msg\n            continue\n        if isinstance(val, Boxes):\n            assert torch.allclose(val.tensor, other_val.tensor), msg\n        elif isinstance(val, ROIMasks):\n            assert val.tensor.shape == other_val.tensor.shape, msg\n            assert torch.allclose(val.tensor, other_val.tensor), msg\n        elif isinstance(val, torch.Tensor):\n            assert torch.allclose(val, other_val, rtol=rtol), msg\n        elif isinstance(val, (list, tuple)):\n            assert len(val) == len(other_val), msg\n            for v, ov in zip(val, other_val):\n                assert torch.allclose(v, ov, rtol=rtol), msg\n        else:\n            raise ValueError(f\"Unknown type {type(val)}\")"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), type(input)\n    assert isinstance(other, Instances), type(other)\n\n    if size_as_tensor:\n        assert torch.allclose(\n            torch.as_tensor(input.image_size), torch.as_tensor(other.image_size)\n        ), f\"{msg} image_size mismatch: {input.image_size} vs {other.image_size}\"\n    else:\n        assert (\n            input.image_size == other.image_size\n        ), f\"{msg} image_size mismatch: {input.image_size} vs {other.image_size}\"\n\n    for name in input._field_names:\n        val = getattr(input, name)\n        other_val = getattr(other, name)\n        if isinstance(val, Boxes):\n            assert torch.allclose(val.tensor, other_val.tensor), f\"{msg} {name} mismatch\"\n        elif isinstance(val, ROIMasks):\n            assert torch.allclose(val.tensor, other_val.tensor), f\"{msg} {name} mismatch\"\n        elif isinstance(val, torch.Tensor):\n            assert torch.allclose(val, other_val, rtol=rtol), f\"{msg} {name} mismatch\"\n        elif isinstance(val, list):\n            assert len(val) == len(\n                other_val\n            ), f\"{msg} {name} length mismatch: {len(val)} vs {len(other_val)}\"\n            for i, (a, b) in enumerate(zip(val, other_val)):\n                assert torch.allclose(a, b, rtol=rtol), f\"{msg} {name}[{i}] mismatch\"\n        elif val is None:\n            assert other_val is None, f\"{msg} {name} mismatch: {val} vs {other_val}\"\n        else:\n            raise ValueError(f\"Unknown type {type(val)}\")"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances) and isinstance(\n        other, Instances\n    ), f\"Expect an Instances object, but got {type(input)} and {type(other)}!\"\n    if size_as_tensor:\n        assert torch.allclose(\n            torch.as_tensor(input.image_size), torch.as_tensor(other.image_size)\n        ), f\"{msg} image_size mismatch: {input.image_size} vs {other.image_size}\"\n    else:\n        assert (\n            input.image_size == other.image_size\n        ), f\"{msg} image_size mismatch: {input.image_size} vs {other.image_size}\"\n\n    for name in input._field_names:\n        val = getattr(input, name, None)\n        other_val = getattr(other, name, None)\n        if val is None:\n            assert other_val is None, f\"{msg} {name} mismatch: {val} vs {other_val}\"\n        elif isinstance(val, Boxes):\n            assert torch.allclose(val.tensor, other_val.tensor, rtol=rtol), (\n                f\"{msg} {name} mismatch: {val} vs {other_val}\"\n            )\n        elif isinstance(val, ROIMasks):\n            assert val.tensor.allclose(other_val.tensor), f\"{msg} {name} mismatch: {val} vs {other_val}\"\n        elif isinstance(val, torch.Tensor):\n            assert torch.allclose(val, other_val, rtol=rtol), f\"{msg} {name} mismatch: {val} vs {other_val}\"\n        else:\n            raise ValueError(f\"{msg} {name} is not supported\")"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances) and isinstance(\n        other, Instances\n    ), f\"Expect inputs to be Instances, but got {type(input)} and {type(other)}!\"\n    assert (\n        input.image_size == other.image_size\n    ), f\"image_size mismatch: {input.image_size} vs {other.image_size}\"\n    for name in input._field_names:\n        val = getattr(input, name)\n        other_val = getattr(other, name)\n        if isinstance(val, Boxes):\n            assert torch.allclose(val.tensor, other_val.tensor), (\n                f\"{msg} {name} mismatch: {val} vs {other_val}\"\n            )\n        elif isinstance(val, ROIMasks):\n            assert torch.allclose(val.tensor, other_val.tensor), (\n                f\"{msg} {name} mismatch: {val} vs {other_val}\"\n            )\n        elif isinstance(val, torch.Tensor):\n            assert torch.allclose(val, other_val, rtol=rtol), (\n                f\"{msg} {name} mismatch: {val} vs {other_val}\"\n            )\n        elif isinstance(val, (int, float, str)):\n            assert val == other_val, f\"{msg} {name} mismatch: {val} vs {other_val}\"\n        else:\n            raise ValueError(f\"Unsupported type {type(val)}\")"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances) and isinstance(\n        other, Instances\n    ), \"Expect inputs to be Instances!\"\n    if size_as_tensor:\n        assert torch.allclose(\n            torch.as_tensor(input.image_size), torch.as_tensor(other.image_size)\n        ), f\"{msg} image_size mismatch: {input.image_size} vs. {other.image_size}\"\n    else:\n        assert (\n            input.image_size == other.image_size\n        ), f\"{msg} image_size mismatch: {input.image_size} vs. {other.image_size}\"\n\n    for name in input._field_names:\n        val = getattr(input, name)\n        other_val = getattr(other, name)\n        if val is None:\n            assert (\n                other_val is None\n            ), f\"{msg} {name} mismatch: {val} vs. {other_val}\"\n        elif isinstance(val, Boxes):\n            assert torch.allclose(\n                val.tensor, other_val.tensor\n            ), f\"{msg} {name} mismatch: {val} vs. {other_val}\"\n        elif isinstance(val, ROIMasks):\n            assert torch.allclose(\n                val.tensor, other_val.tensor\n            ), f\"{msg} {name} mismatch: {val} vs. {other_val}\"\n        elif isinstance(val, torch.Tensor):\n            assert torch.allclose(val, other_val, rtol=rtol), f\"{msg} {name} mismatch: {val} vs. {other_val}\"\n        else:\n            raise ValueError(f\"Unknown type {type(val)} for {name}\")"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances) and isinstance(\n        other, Instances\n    ), f\"Expect input and other to be Instances, but got {type(input)} and {type(other)}!\"\n    assert (\n        input.image_size == other.image_size\n    ), f\"image_size mismatch: {input.image_size} vs. {other.image_size}\"\n    for name in input._field_names:\n        val = getattr(input, name)\n        val_other = getattr(other, name)\n        if isinstance(val, Boxes):\n            assert torch.allclose(val.tensor, val_other.tensor), f\"{msg} {name} mismatch!\"\n        elif isinstance(val, ROIMasks):\n            assert torch.allclose(\n                val.tensor.float(), val_other.tensor.float()\n            ), f\"{msg} {name} mismatch!\"\n        elif isinstance(val, torch.Tensor):\n            assert torch.allclose(val, val_other, rtol=rtol), f\"{msg} {name} mismatch!\"\n        else:\n            assert val == val_other, f\"{msg} {name} mismatch!\""}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), f\"Expect an Instances object, but got {type(input)}!\"\n    assert isinstance(other, Instances), f\"Expect an Instances object, but got {type(other)}!\"\n    assert input.image_size == other.image_size, (\n        f\"image_size mismatch: {input.image_size} vs {other.image_size}\"\n    )\n    if size_as_tensor:\n        assert torch.allclose(\n            torch.as_tensor(input.image_size), torch.as_tensor(other.image_size)\n        )\n    for name in input._field_names:\n        val = getattr(input, name)\n        other_val = getattr(other, name)\n        if val is None:\n            assert other_val is None, f\"{msg} {name} mismatch: {val} vs {other_val}\"\n        elif isinstance(val, Boxes):\n            assert torch.allclose(val.tensor, other_val.tensor), (\n                f\"{msg} {name} mismatch: {val} vs {other_val}\"\n            )\n        elif isinstance(val, ROIMasks):\n            assert val == other_val, f\"{msg} {name} mismatch: {val} vs {other_val}\"\n        elif isinstance(val, torch.Tensor):\n            assert torch.allclose(val, other_val, rtol=rtol), (\n                f\"{msg} {name} mismatch: {val} vs {other_val}\"\n            )\n        else:\n            assert val == other_val, f\"{msg} {name} mismatch: {val} vs {other_val}\""}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances) and isinstance(\n        other, Instances\n    ), f\"Expect two Instances objects, but got {type(input)} and {type(other)}!\"\n    assert (\n        input.image_size == other.image_size\n    ), f\"image_size mismatch: {input.image_size} vs. {other.image_size}\"\n\n    for name in input._field_names:\n        val = getattr(input, name, None)\n        other_val = getattr(other, name, None)\n        if val is None:\n            assert other_val is None, f\"{msg} {name} mismatch: {val} vs. {other_val}\"\n        elif other_val is None:\n            assert False, f\"{msg} {name} mismatch: {val} vs. {other_val}\"\n        elif isinstance(val, Boxes):\n            assert torch.allclose(\n                val.tensor, other_val.tensor, rtol=rtol\n            ), f\"{msg} {name} mismatch: {val} vs. {other_val}\"\n        elif isinstance(val, ROIMasks):\n            assert val.tensor.allclose(\n                other_val.tensor\n            ), f\"{msg} {name} mismatch: {val} vs. {other_val}\"\n        elif isinstance(val, torch.Tensor):\n            assert torch.allclose(\n                val, other_val, rtol=rtol\n            ), f\"{msg} {name} mismatch: {val} vs. {other_val}\"\n        elif isinstance(val, (list, tuple)):\n            assert len(val) == len(\n                other_val\n            ), f\"{msg} {name} length mismatch: {len(val)} vs. {len(other_val)}\"\n            for v, ov in zip(val, other_val):\n                assert_instances_allclose(v, ov, rtol=rtol, msg=msg + f\"[{name}]\", size_as_tensor=size_as_tensor)\n        else"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances) and isinstance(\n        other, Instances\n    ), f\"Expect two Instances objects, but got {type(input)} and {type(other)}!\"\n    if size_as_tensor:\n        assert torch.allclose(\n            torch.as_tensor(input.image_size), torch.as_tensor(other.image_size)\n        ), msg + f\"image_size mismatch: {input.image_size} != {other.image_size}\"\n    else:\n        assert (\n            input.image_size == other.image_size\n        ), msg + f\"image_size mismatch: {input.image_size} != {other.image_size}\"\n\n    for name in input._field_names:\n        val = getattr(input, name)\n        other_val = getattr(other, name)\n        if isinstance(val, Boxes):\n            assert torch.allclose(val.tensor, other_val.tensor), msg + f\"{name} mismatch\"\n        elif isinstance(val, ROIMasks):\n            assert torch.allclose(\n                val.tensor.float(), other_val.tensor.float()\n            ), msg + f\"{name} mismatch\"\n        elif isinstance(val, torch.Tensor):\n            assert torch.allclose(val, other_val, rtol=rtol), msg + f\"{name} mismatch\"\n        elif isinstance(val, list):\n            assert len(val) == len(other_val), msg + f\"{name} length mismatch\"\n            for v, ov in zip(val, other_val):\n                assert_instances_allclose(v, ov, rtol=rtol, msg=msg + f\"{name}. \", size_as_tensor=size_as_tensor)\n        else:\n            assert val == other_val, msg + f\"{name} mismatch\""}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances) and isinstance(\n        other, Instances\n    ), f\"Expect input and other to be Instances, but got {type(input)} and {type(other)}!\"\n\n    if size_as_tensor:\n        assert (\n            isinstance(input.image_size, torch.Tensor)\n            and isinstance(other.image_size, torch.Tensor)\n        ) or (\n            isinstance(input.image_size, tuple) and isinstance(other.image_size, tuple)\n        ), f\"Expect input.image_size and other.image_size to be the same type, but got {type(input.image_size)} and {type(other.image_size)}!\"\n    else:\n        assert isinstance(input.image_size, tuple) and isinstance(\n            other.image_size, tuple\n        ), f\"Expect input.image_size and other.image_size to be tuple, but got {type(input.image_size)} and {type(other.image_size)}!\"\n\n    if size_as_tensor:\n        assert torch.equal(\n            input.image_size, other.image_size\n        ), f\"{msg} Expect image_size to be equal, but got {input.image_size} and {other.image_size}!\"\n    else:\n        assert (\n            input.image_size == other.image_size\n        ), f\"{msg} Expect image_size to be equal, but got {input.image_size} and {other.image_size}!\"\n\n    for name in input._field_names:\n        val = getattr(input, name, None)\n        other_val = getattr(other, name, None)\n        assert (\n            val is not None and other_val is not None\n        ), f\"{msg} Expect both instances to have field '{name}', but only one does!\"\n\n        if isinstance(val, Boxes):\n            assert torch.allclose(val.tensor, other_val.tensor), f\"{msg} Expect field '{name}' to be"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances) and isinstance(other, Instances), msg\n    assert (\n        input.image_size == other.image_size\n        if size_as_tensor\n        else tuple(input.image_size) == tuple(other.image_size)\n    ), f\"{msg}image_size mismatch: {input.image_size} vs {other.image_size}\"\n    for name in input._field_names:\n        val = getattr(input, name)\n        other_val = getattr(other, name)\n        if isinstance(val, Boxes):\n            assert torch.allclose(val.tensor, other_val.tensor, rtol=rtol), (\n                f\"{msg}Boxes mismatch in field '{name}': {val} vs {other_val}\"\n            )\n        elif isinstance(val, ROIMasks):\n            assert val.tensor.allclose(other_val.tensor), (\n                f\"{msg}ROIMasks mismatch in field '{name}': {val} vs {other_val}\"\n            )\n        elif isinstance(val, torch.Tensor):\n            assert torch.allclose(val, other_val, rtol=rtol), (\n                f\"{msg}Tensor mismatch in field '{name}': {val} vs {other_val}\"\n            )\n        elif isinstance(val, list):\n            assert len(val) == len(other_val), (\n                f\"{msg}List length mismatch in field '{name}': {len(val)} vs {len(other_val)}\"\n            )\n            for i, (a, b) in enumerate(zip(val, other_val)):\n                assert a == b, f\"{msg}List element mismatch in field '{name}': {a} vs {b} at index {i}\"\n        else:\n            assert val == other_val, f\"{msg}Value mismatch in field '{name}': {val} vs {other_val}\""}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances) and isinstance(other, Instances), (\n        f\"Expect input and other to be Instances, but got {type(input)} and {type(other)}!\"\n    )\n    if size_as_tensor:\n        assert torch.equal(input.image_size, other.image_size), msg\n    else:\n        assert input.image_size == other.image_size, msg\n\n    for name in input._field_names:\n        val = getattr(input, \"_\" + name, None)\n        other_val = getattr(other, \"_\" + name, None)\n        if val is None and other_val is None:\n            continue\n        elif val is None or other_val is None:\n            raise ValueError(f\"{name} is None, but the other is not None\")\n        elif isinstance(val, Boxes):\n            assert torch.allclose(val.tensor, other_val.tensor, rtol=rtol), msg\n        elif isinstance(val, ROIMasks):\n            assert val.tensor.shape == other_val.tensor.shape, msg\n            assert torch.allclose(val.tensor, other_val.tensor, rtol=rtol), msg\n        elif isinstance(val, torch.Tensor):\n            assert torch.allclose(val, other_val, rtol=rtol), msg\n        else:\n            raise ValueError(f\"Don't know how to compare {name} which is type {type(val)}\")"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if size_as_tensor:\n        assert torch.allclose(\n            torch.tensor(input.image_size, dtype=torch.int),\n            torch.tensor(other.image_size, dtype=torch.int),\n            rtol=rtol,\n            msg=msg + \"image_size\",\n        )\n    else:\n        assert input.image_size == other.image_size, msg + \"image_size\"\n\n    for name in input._field_names:\n        val = getattr(input, name, None)\n        other_val = getattr(other, name, None)\n        if isinstance(val, Boxes):\n            assert torch.allclose(val.tensor, other_val.tensor, rtol=rtol), msg + name\n        elif isinstance(val, ROIMasks):\n            assert val.tensor.allclose(other_val.tensor), msg + name\n        elif isinstance(val, torch.Tensor):\n            assert torch.allclose(val, other_val, rtol=rtol), msg + name\n        elif isinstance(val, Instances):\n            assert_instances_allclose(val, other_val, rtol=rtol, msg=msg + name + \".\")\n        else:\n            raise ValueError(f\"Unknown type {type(val)}\")\n\n    for name in other._field_names:\n        val = getattr(input, name, None)\n        other_val = getattr(other, name, None)\n        if val is None and other_val is None:\n            continue\n        if val is None:\n            assert other_val is None, msg + name\n        if other_val is None:\n            assert val is None, msg + name"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances) and isinstance(other, Instances), msg + \"Expect two Instances objects\"\n    if size_as_tensor:\n        assert torch.allclose(\n            torch.tensor(input.image_size), torch.tensor(other.image_size), rtol=rtol\n        ), msg + \"image_size mismatch\"\n    else:\n        assert input.image_size == other.image_size, msg + \"image_size mismatch\"\n    for name in input._field_names:\n        val = getattr(input, name)\n        other_val = getattr(other, name)\n        if isinstance(val, Boxes):\n            assert torch.allclose(val.tensor, other_val.tensor, rtol=rtol), msg + name + \" mismatch\"\n        elif isinstance(val, ROIMasks):\n            assert val == other_val, msg + name + \" mismatch\"\n        elif isinstance(val, torch.Tensor):\n            assert torch.allclose(val, other_val, rtol=rtol), msg + name + \" mismatch\"\n        elif isinstance(val, list):\n            assert len(val) == len(other_val), msg + name + \" length mismatch\"\n            for i, (v, o) in enumerate(zip(val, other_val)):\n                assert_instances_allclose(v, o, rtol=rtol, msg=msg + f\"{name}[{i}].\", size_as_tensor=size_as_tensor)\n        elif isinstance(val, dict):\n            assert len(val) == len(other_val), msg + name + \" length mismatch\"\n            for k in val:\n                assert k in other_val, msg + name + f\"[{k}] missing\"\n                assert_instances_allclose(val[k], other_val[k], rtol=rtol, msg=msg + f\"{name}[{k}].\", size_as_tensor=size_as_tensor)\n        elif isinstance(val, tuple):\n            assert len"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances)\n    assert isinstance(other, Instances)\n    assert input.image_size == other.image_size\n    if size_as_tensor:\n        assert torch.allclose(input.image_size_tensor, other.image_size_tensor)\n    else:\n        assert input.image_size == other.image_size\n\n    for name in input._field_names:\n        val = getattr(input, name, None)\n        other_val = getattr(other, name, None)\n        assert val is not None\n        assert other_val is not None\n        assert type(val) == type(other_val), f\"{msg} {name} type mismatch: {type(val)} vs {type(other_val)}\"\n        if isinstance(val, Boxes):\n            assert torch.allclose(val.tensor, other_val.tensor, rtol=rtol)\n        elif isinstance(val, ROIMasks):\n            assert val.tensor.allclose(other_val.tensor, rtol=rtol)\n        elif isinstance(val, torch.Tensor):\n            assert torch.allclose(val, other_val, rtol=rtol)\n        else:\n            raise ValueError(f\"{msg} {name} unsupported type: {type(val)}\")\n        assert val.device == other_val.device\n        assert val.dtype == other_val.dtype"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances) and isinstance(other, Instances), (\n        \"Expect input and other to be Instances, but got {} and {}\".format(type(input), type(other))\n    )\n    if size_as_tensor:\n        assert torch.equal(input.image_size, other.image_size), \"image_size mismatch: {} vs {}\".format(\n            input.image_size, other.image_size\n        )\n    else:\n        assert input.image_size == other.image_size, \"image_size mismatch: {} vs {}\".format(\n            input.image_size, other.image_size\n        )\n    for name in input._field_names:\n        val = getattr(input, name)\n        other_val = getattr(other, name)\n        if val is None and other_val is None:\n            continue\n        if isinstance(val, Boxes):\n            assert torch.allclose(val.tensor, other_val.tensor, rtol=rtol), msg + f\"{name} mismatch\"\n        elif isinstance(val, ROIMasks):\n            assert val.tensor.allclose(other_val.tensor), msg + f\"{name} mismatch\"\n        elif isinstance(val, torch.Tensor):\n            assert torch.allclose(val, other_val, rtol=rtol), msg + f\"{name} mismatch\"\n        elif isinstance(val, (list, tuple)) and isinstance(val[0], Boxes):\n            assert len(val) == len(other_val), msg + f\"{name} length mismatch\"\n            for v, ov in zip(val, other_val):\n                assert torch.allclose(v.tensor, ov.tensor, rtol=rtol), msg + f\"{name} mismatch\"\n        elif isinstance(val, (list, tuple)) and isinstance(val[0], ROIMasks):\n            assert len(val) == len(other_val), msg + f\"{name} length mismatch\"\n            for v, ov in zip(val,"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        box = self.tensor\n        area = box[:, 2] * box[:, 3]\n        return area"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        box = self.tensor\n        area = box[:, 2] * box[:, 3]\n        return area"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        box = self.tensor\n        area = box[:, 2] * box[:, 3]\n        return area"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        box = self.tensor\n        area = box[:, 2] * box[:, 3]\n        return area"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        box = self.tensor\n        area = box[:, 2] * box[:, 3]\n        return area"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        box = self.tensor\n        area = box[:, 2] * box[:, 3]\n        return area"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        box = self.tensor\n        area = box[:, 2] * box[:, 3]\n        return area"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        box = self.tensor\n        area = (box[:, 2] * box[:, 3]).to(box.dtype)\n        return area"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        box = self.tensor\n        area = (box[:, 2] * box[:, 3]).to(box.dtype)\n        return area"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        area = self.tensor[:, 2] * self.tensor[:, 3]\n        return area"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        box = self.tensor\n        area = (box[:, 2] * box[:, 3]).to(box.dtype)\n        return area\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        box = self.tensor\n        area = box[:, 2] * box[:, 3]\n        return area\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        box = self.tensor\n        area = (box[:, 2] * box[:, 3]).to(dtype=torch.float32)\n        return area\n\n    "}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        box = self.tensor\n        area = box[:, 2] * box[:, 3]\n        return area\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        box = self.tensor\n        area = box[:, 2] * box[:, 3]\n        return area\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        box = self.tensor\n        area = box[:, 2] * box[:, 3]\n        return area\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        box = self.tensor\n        area = box[:, 2] * box[:, 3]\n        return area\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        box = self.tensor\n        area = (box[:, 2] * box[:, 3]).to(box.dtype)\n        return area\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        box = self.tensor\n        area = box[:, 2] * box[:, 3]\n        return area\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        area = self.tensor[:, 2] * self.tensor[:, 3]\n        return area\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # parse classification outputs\n        gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n        _log_classification_stats(scores, gt_classes)\n\n        # parse box regression outputs\n        if len(proposal_deltas) > 0:\n            proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)  # Nx4\n            assert not proposal_boxes.requires_grad, \"Proposals do not need gradients!\"\n            gt_boxes = cat([(p.gt_boxes if p.has(\"gt_boxes\") else p.proposal_boxes).tensor for p in proposals], dim=0)\n        else:\n            proposal_boxes = gt_boxes = torch.empty((0, 4), device=proposal_deltas.device)\n\n        losses = {\n            \"loss_cls\": cross_entropy(scores, gt_classes, reduction=\"mean\"),\n            \"loss_box_reg\": _dense_box_regression_loss(\n                proposal_boxes,\n                gt_boxes,\n                proposal_deltas,\n                self.box2box_transform,\n                self.smooth_l1_beta,\n                self.box_reg_loss_type,\n            ),\n        }\n        return {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # parse classification outputs\n        gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n        _log_classification_stats(scores, gt_classes)\n\n        # parse box regression outputs\n        if len(proposal_deltas) > 0:\n            proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)  # Nx4\n            gt_boxes = cat([(p.gt_boxes if p.has(\"gt_boxes\") else p.proposal_boxes).tensor for p in proposals], dim=0)\n            # Nx(Kx4)\n            if self.cls_agnostic_bbox_reg:\n                proposal_deltas = proposal_deltas.view(-1, 4)\n                box_dim = gt_boxes.shape[1]  # 4 or 5\n                cls_agnostic_bbox_reg = proposal_deltas.new_zeros(gt_boxes.shape[0], box_dim)\n                gt_classes = cat([gt_classes.new_zeros(gt_boxes.shape[0])], dim=-1)\n            else:\n                proposal_deltas = proposal_deltas.view(-1, self.num_classes, 4)\n\n            losses = {\n                \"loss_cls\": cross_entropy(scores, gt_classes, reduction=\"mean\"),\n                \"loss_box_reg\": self.box_reg_loss(\n                    proposal_boxes, gt_boxes, proposal_deltas, gt_classes\n                ),\n            }\n        else:\n            losses = {\n                \"loss_cls\": cross_entropy(scores, gt_classes, reduction=\"mean\"),\n                \"loss_box_reg\": proposal_deltas.sum() * 0,\n            }\n\n        return {k: v * self.loss_weight.get(k, 1.0) for k, v in losses."}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # parse classification outputs\n        gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n        _log_classification_stats(scores, gt_classes)\n\n        if len(proposals) == 0:\n            return {\n                \"loss_cls\": F.cross_entropy(scores, gt_classes),\n                \"loss_box_reg\": _dense_box_regression_loss(\n                    [proposal_boxes],\n                    self.box2box_transform,\n                    [torch.zeros_like(proposal_deltas)],\n                    [gt_boxes],\n                    ...,\n                    self.box_reg_loss_type,\n                    self.smooth_l1_beta,\n                ),\n            }\n\n        proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)  # Rx4\n        assert not proposal_boxes.requires_grad, \"Proposals do not need gradients!\"\n        gt_boxes = cat([(p.gt_boxes if p.has(\"gt_boxes\") else p.proposal_boxes).tensor for p in proposals], dim=0)\n\n        if self.cls_agnostic_bbox_reg:\n            proposal_deltas = proposal_deltas[:, -self.box2box_transform.weights.shape[0] :]\n\n        losses = {\n            \"loss_cls\": cross_entropy(scores, gt_classes, reduction=\"mean\"),\n            \"loss_box_reg\": self.box_reg_loss(\n                proposal_boxes, gt_boxes, proposal_deltas, gt_classes\n            ),\n        }\n        return {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # parse classification outputs\n        gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n        _log_classification_stats(scores, gt_classes)\n\n        # parse box regression outputs\n        if len(proposal_deltas) > 0:\n            proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)  # Rx4\n        else:\n            proposal_boxes = torch.empty((0, 4), device=proposal_deltas.device)\n\n        if len(proposal_deltas) > 0:\n            gt_boxes = cat([(p.gt_boxes if p.has(\"gt_boxes\") else p.proposal_boxes).tensor for p in proposals], dim=0)\n        else:\n            gt_boxes = torch.empty((0, 4), device=proposal_deltas.device)\n\n        if len(proposal_deltas) == 0:\n            losses = {\n                \"loss_cls\": F.cross_entropy(scores, gt_classes),\n                \"loss_box_reg\": torch.tensor(0),\n            }\n        else:\n            losses = {\n                \"loss_cls\": F.cross_entropy(scores, gt_classes),\n                \"loss_box_reg\": self.box_reg_loss(\n                    proposal_boxes,\n                    gt_boxes,\n                    proposal_deltas,\n                    gt_classes,\n                ),\n            }\n        return {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # parse classification outputs\n        gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n        _log_classification_stats(scores, gt_classes)\n\n        # parse box regression outputs\n        if len(proposal_deltas) > 0:\n            proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)  # Nx4\n            gt_boxes = cat([(p.gt_boxes if p.has(\"gt_boxes\") else p.proposal_boxes).tensor for p in proposals], dim=0)\n        else:\n            proposal_boxes = gt_boxes = torch.empty((0, 4), device=proposal_deltas.device)\n\n        # cls and box loss\n        loss_cls = cross_entropy(scores, gt_classes, reduction=\"mean\")\n        loss_box_reg = self.box_reg_loss(proposal_boxes, gt_boxes, proposal_deltas, gt_classes)\n\n        return {\n            \"loss_cls\": self.loss_weight.get(\"loss_cls\", 1) * loss_cls,\n            \"loss_box_reg\": self.loss_weight.get(\"loss_box_reg\", 1) * loss_box_reg,\n        }"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # parse classification outputs\n        gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n        _log_classification_stats(scores, gt_classes)\n\n        # parse box regression outputs\n        if len(proposal_deltas) > 0:\n            proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)  # Nx4\n            gt_boxes = cat([(p.gt_boxes if p.has(\"gt_boxes\") else p.proposal_boxes).tensor for p in proposals], dim=0)\n        else:\n            proposal_boxes = gt_boxes = torch.empty((0, 4), device=proposal_deltas.device)\n\n        if self.cls_agnostic_bbox_reg:\n            box_dim = proposal_deltas.size(1)  # 4 or 5\n            cls_agnostic_bbox_reg = proposal_deltas.reshape(-1, self.num_classes, box_dim)\n            box_cls_agnostic_deltas = cls_agnostic_bbox_reg[\n                torch.arange(gt_classes.numel(), device=gt_classes.device), gt_classes\n            ]\n        else:\n            box_cls_agnostic_deltas = proposal_deltas\n\n        losses = {\n            \"loss_cls\": self.softmax_cross_entropy_loss(scores, gt_classes),\n            \"loss_box_reg\": self.box_reg_loss(proposal_boxes, gt_boxes, box_cls_agnostic_deltas, gt_classes),\n        }\n        return {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        src_boxes = [x.proposal_boxes for x in proposals]\n        dst_boxes = [x.gt_boxes for x in proposals]\n        dst_classes = [x.gt_classes for x in proposals]\n\n        # classification loss\n        loss_cls = cross_entropy(scores, dst_classes, reduction=\"mean\")\n\n        # box regression loss\n        loss_box_reg = self.box_reg_loss(src_boxes, dst_boxes, proposal_deltas, dst_classes)\n\n        # summarize losses\n        loss_dict = dict(loss_cls=loss_cls, loss_box_reg=loss_box_reg)\n        for name, weight in self.loss_weight.items():\n            if weight > 0:\n                loss_dict[name] *= weight\n        return loss_dict"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # parse classification outputs\n        gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n        _log_classification_stats(scores, gt_classes)\n\n        # parse box regression outputs\n        if len(proposal_deltas) > 0:\n            proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)  # Nx4\n        else:\n            proposal_boxes = torch.empty((0, 4), device=proposal_deltas.device)\n\n        if len(proposal_deltas) > 0:\n            box_features = cat([p.proposal_boxes.tensor for p in proposals], dim=0)\n        else:\n            box_features = torch.empty((0, 4), device=proposal_deltas.device)\n        gt_boxes = cat([(p.gt_boxes if len(p) > 0 else p.proposal_boxes).tensor for p in proposals], dim=0)\n\n        if self.cls_agnostic_bbox_reg:\n            proposal_deltas = proposal_deltas.view(-1, 4)\n            box_features = box_features.view(-1, 4)\n            gt_boxes = gt_boxes.view(-1, 4)\n\n        losses = {\n            \"loss_cls\": cross_entropy(scores, gt_classes, reduction=\"mean\"),\n            \"loss_box_reg\": self.box_reg_loss(\n                proposal_boxes, gt_boxes, proposal_deltas, gt_classes\n            ),\n        }\n        return {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # parse classification outputs\n        gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n        _log_classification_stats(scores, gt_classes)\n\n        # parse box regression outputs\n        if len(proposal_deltas) > 0:\n            proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)  # Nx4\n            gt_boxes = cat([(p.gt_boxes if p.has(\"gt_boxes\") else p.gt_masks).tensor for p in proposals], dim=0)\n            # Nx(Kx4)\n            if self.box2box_transform.rotated:\n                proposal_deltas = proposal_deltas.view(-1, self.box2box_transform.dims)\n            else:\n                proposal_deltas = proposal_deltas.view(-1, self.num_classes, self.box2box_transform.dims)\n\n        losses = {\n            \"loss_cls\": cross_entropy(scores, gt_classes, reduction=\"mean\"),\n            \"loss_box_reg\": _dense_box_regression_loss(\n                proposal_boxes,\n                gt_boxes,\n                proposal_deltas,\n                self.box2box_transform,\n                self.smooth_l1_beta,\n                self.box_reg_loss_type,\n            ),\n        }\n        return {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # parse classification outputs\n        gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n        _log_classification_stats(scores, gt_classes)\n\n        # parse box regression outputs\n        if len(proposal_deltas) > 0:\n            proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)  # Nx4\n            assert not proposal_boxes.requires_grad, \"Proposals do not need gradients!\"\n            # If \"gt_boxes\" does not exist, the proposals must be all negative and\n            # should not be included in regression loss computation and proposal_boxes\n            # should be None.\n            # Here we just use proposal_boxes as an arbitrary placeholder because the\n            # Fast R-CNN loss requires them.\n            if \"gt_boxes\" in proposals[0]:\n                gt_boxes = cat([p.gt_boxes.tensor for p in proposals], dim=0)\n            else:\n                gt_boxes = Boxes(\n                    torch.zeros(len(proposal_deltas), 4, device=proposal_boxes.device)\n                )\n        else:\n            gt_boxes = None\n\n        # regression loss\n        if gt_boxes is not None:\n            if self.box_reg_loss_type == \"smooth_l1\":\n                loss_box_reg = self.box_reg_loss(\n                    proposal_boxes, gt_boxes, proposal_deltas, gt_classes\n                )\n            else:\n                raise ValueError(f\"Invalid bbox reg loss type '{self.box_reg_loss_type}'\")\n        else:\n            loss_box_reg = proposal_deltas.sum() * 0\n\n        return {\n            \"loss_cls\": self.loss_weight.get(\"loss_cls\", 1) * loss_cls,\n            \"loss_box_reg\": self.loss_weight.get(\""}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        src_boxes = [x.proposal_boxes for x in proposals]\n        dst_boxes = [x.gt_boxes for x in proposals]\n        dst_classes = [x.gt_classes for x in proposals]\n        box_reg_loss = self.box_reg_loss(src_boxes, dst_boxes, proposal_deltas, dst_classes)\n        losses = {\"loss_box_reg\": self.loss_weight[\"loss_box_reg\"] * box_reg_loss}\n        if self.cls_agnostic_bbox_reg:\n            dst_deltas = cat([x.gt_boxes.tensor for x in proposals], dim=0)\n        else:\n            dst_deltas = cat([x.gt_boxes.tensor[x.gt_classes] for x in proposals], dim=0)\n\n        gt_classes = cat(dst_classes, dim=0)\n        loss_cls = cross_entropy(scores, gt_classes, reduction=\"mean\")\n        losses[\"loss_cls\"] = self.loss_weight[\"loss_cls\"] * loss_cls\n\n        _log_classification_stats(scores, gt_classes)\n        return losses"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # parse classification outputs\n        gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n        _log_classification_stats(scores, gt_classes)\n\n        if len(proposals) == 0:\n            return {\n                \"loss_cls\": F.cross_entropy(scores, gt_classes),\n                \"loss_box_reg\": _dense_box_regression_loss(\n                    [proposal_deltas], self.box2box_transform, [torch.zeros_like(gt_classes)], [gt_classes]\n                ),\n            }\n\n        # parse box regression outputs\n        if self.box_reg_loss_type == \"smooth_l1\":\n            gt_classes = gt_classes.repeat(proposal_deltas.size(1) // 4)\n        deltas = [res.proposal_deltas for res in proposals]\n\n        # concatenate all deltas into a tensor\n        deltas = cat(deltas, dim=0)\n        gt_classes = cat(gt_classes, dim=0)\n\n        valid_mask = gt_classes >= 0  # exclude background anchor\n        gt_classes = gt_classes[valid_mask]\n        deltas = deltas[valid_mask]\n        gt_boxes = cat([p.gt_boxes.tensor for p in proposals], dim=0)\n        gt_boxes = gt_boxes[valid_mask]\n\n        if self.cls_agnostic_bbox_reg:\n            gt_classes = gt_classes[:, 0]\n\n        # Log the classification accuracy\n        _log_classification_stats(scores, gt_classes)\n\n        if self.box_reg_loss_type == \"smooth_l1\":\n            loss_box_reg = smooth_l1_loss(\n                deltas,\n                gt_boxes,\n                beta=self.smooth_l1_beta,"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # parse classification outputs\n        gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n        _log_classification_stats(scores, gt_classes)\n\n        # parse box regression outputs\n        if len(proposals):\n            proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)  # Nx4\n            assert not proposal_boxes.requires_grad, \"Proposals do not need gradients!\"\n            # If \"gt_classes\" does not exist, the proposals must be all negative and\n            # should not be included in regression loss computation and proposal_boxes\n            # should be None.\n            # Here we just use a dummy, all-negative tensor in proposal_boxes.\n            if \"gt_classes\" in proposals[0]:\n                gt_boxes = cat([(p.gt_boxes if p.has(\"gt_boxes\") else p.proposal_boxes).tensor for p in proposals], dim=0)\n            else:\n                log_first_n(\n                    logging.WARN,\n                    \"'gt_classes' does not exist in proposal predictions, \"\n                    \"therefore the proposals in this batch will not be used for \"\n                    \"bounding box regression loss computation.\",\n                    n=10,\n                )\n                gt_boxes = torch.zeros((len(proposals), 4), device=proposal_boxes.device)\n        else:\n            gt_boxes = torch.zeros((0, 4), device=proposal_boxes.device)\n\n        # box regression\n        losses = {\n            \"loss_box_reg\": self.box_reg_loss(proposal_boxes, gt_boxes, proposal_deltas, gt_classes)\n            * self.loss_weight.get(\"loss_box_reg\", 1.0)\n        }\n\n        # classification\n        losses_cls = self.losses_cls(scores, gt_classes)\n        losses"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # parse classification outputs\n        gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n        _log_classification_stats(scores, gt_classes)\n\n        # parse box regression outputs\n        if len(proposal_deltas) > 0:\n            proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)  # Nx4\n            assert not proposal_boxes.requires_grad, \"Proposals do not need gradients!\"\n            # If \"gt_boxes\" does not exist, the proposals must be all negative and\n            # should not be included in regression loss computation.\n            # Here we just use proposal_boxes as an arbitrary placeholder because the\n            # Fast R-CNN loss requires positive proposals.\n            gt_boxes = cat(\n                [(p.gt_boxes if p.has(\"gt_boxes\") else p.proposal_boxes).tensor for p in proposals],\n                dim=0,\n            )\n        else:\n            # empty targets or proposals not supported during training\n            gt_boxes = torch.empty((0, 4), device=proposal_deltas.device)\n\n        # classification and box regression loss\n        # compute loss_cls and loss_box_reg\n        loss_cls = self.loss_cls(scores, gt_classes)\n        loss_box_reg = self.loss_box_reg(proposal_boxes, gt_boxes, proposal_deltas, gt_classes)\n        return {\n            \"loss_cls\": loss_cls,\n            \"loss_box_reg\": loss_box_reg,\n        }\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # parse classification outputs\n        gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n        _log_classification_stats(scores, gt_classes)\n\n        # parse box regression outputs\n        if len(proposal_deltas) > 0:\n            proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)  # Nx4\n            assert not proposal_boxes.requires_grad, \"Proposals do not need gradients!\"\n            # If \"gt_classes\" does not exist, the returned tensor is filled with dummy value of 0.\n            # Therefore \"gt_classes[gt_classes == -1]\" will represent the dummy boxes which\n            # should be ignored in loss computation.\n            gt_classes = cat(\n                [(p.gt_classes if p.has(\"gt_classes\") else p.gt_classes.new_full((len(p),), 0)) for p in proposals],\n                dim=0,\n            )\n            box_reg_loss = self.box_reg_loss(\n                proposal_boxes, gt_classes, proposal_deltas, gt_classes\n            )\n        else:\n            box_reg_loss = proposal_deltas.sum() * 0\n\n        return {\n            \"loss_cls\": cross_entropy(scores, gt_classes, reduction=\"mean\"),\n            \"loss_box_reg\": box_reg_loss,\n        }"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # parse classification outputs\n        gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n        _log_classification_stats(scores, gt_classes)\n\n        # parse box regression outputs\n        if isinstance(self.box2box_transform, Box2BoxTransform):\n            # When loss is class-agnostic, proposal_deltas is a tensor of shape (N, 4 or 5)\n            # whether the proposals are class-agnostic or not.\n            box_dim = proposal_deltas.shape[1]  # 4 or 5\n            cls_agnostic_bbox_reg = box_dim == 4\n        else:\n            box_dim = proposal_deltas.shape[1]\n            cls_agnostic_bbox_reg = False\n\n        if cls_agnostic_bbox_reg:\n            box_deltas = [res.proposal_deltas for res in proposals]\n        else:\n            # split the box delta into K groups\n            box_deltas = [res.proposal_deltas.view(-1, self.num_classes, box_dim) for res in proposals]\n\n        losses = {\n            \"loss_cls\": cross_entropy(scores, gt_classes, reduction=\"mean\"),\n            \"loss_box_reg\": _dense_box_regression_loss(\n                [proposal_boxes.tensor for proposal_boxes in proposals],\n                self.box2box_transform,\n                box_deltas,\n                [proposal_boxes.gt_boxes.tensor for proposal_boxes in proposals],\n                ...,\n                self.box_reg_loss_type,\n                self.smooth_l1_beta,\n            ),\n        }\n\n        return {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # parse classification outputs\n        gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n        _log_classification_stats(scores, gt_classes)\n\n        # parse box regression outputs\n        if len(proposal_deltas) > 0:\n            proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)  # Nx4\n            box_features = cat([p.proposal_boxes.tensor for p in proposals], dim=0)\n        else:\n            proposal_boxes = cat([p.gt_boxes.tensor for p in proposals], dim=0)\n            # gt_boxes are already normalized\n            box_features = proposal_boxes\n        gt_classes = cat(\n            [p.gt_classes * torch.ones_like(p.gt_boxes.tensor) for p in proposals], dim=0\n        )\n        gt_boxes = cat([p.gt_boxes.tensor for p in proposals], dim=0)\n\n        if self.cls_agnostic_bbox_reg:\n            box_features = box_features[:, 4:]\n            proposal_deltas = proposal_deltas[:, 4:]\n\n        # classification and box regression loss\n        # compute loss_cls and loss_box_reg\n        loss_cls = F.cross_entropy(scores, gt_classes, reduction=\"mean\")\n        loss_box_reg = self.box_reg_loss(proposal_boxes, gt_boxes, proposal_deltas, gt_classes)\n\n        return {\n            \"loss_cls\": loss_cls,\n            \"loss_box_reg\": loss_box_reg * self.loss_weight[\"loss_box_reg\"],\n        }"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # parse classification outputs\n        gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n        _log_classification_stats(scores, gt_classes)\n\n        # parse box regression outputs\n        if len(proposals):\n            proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)  # Rx4\n            assert not proposal_boxes.requires_grad, \"Proposals do not need gradients!\"\n            # If \"gt_classes\" does not exist, the returned tensor should be empty.\n            gt_classes = cat(\n                [p.gt_classes for p in proposals], dim=0\n            )  # R, always >0\n            gt_boxes = cat([(p.gt_boxes if p.has(\"gt_boxes\") else p.proposal_boxes).tensor for p in proposals], dim=0)\n        else:  # empty targets\n            proposal_boxes = gt_boxes = torch.empty((0, 4), device=proposal_deltas.device)\n            gt_classes = torch.empty(0, dtype=torch.int64, device=proposal_deltas.device)\n\n        # logits.shape=(R,K+1) scores to K+1 dof, gt_classes.shape=(R,)\n        loss_cls = cross_entropy(scores, gt_classes, reduction=\"mean\")\n\n        # regression\n        if self.box_reg_loss_type == \"smooth_l1\":\n            gt_classes = gt_classes.unsqueeze(1).expand(-1, proposal_deltas.shape[-1])\n            loss_box_reg = smooth_l1_loss(\n                proposal_deltas,\n                gt_boxes,\n                gt_classes,\n                self.smooth_l1_beta,\n                reduction=\"mean\",\n            )\n        else:\n            loss_box_reg ="}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # parse classification outputs\n        gt_classes = []\n        for proposals_per_image in proposals:\n            if proposals_per_image.has(\"gt_classes\"):\n                gt_classes.append(proposals_per_image.gt_classes)\n\n        if len(gt_classes) == 0:\n            return {\n                \"loss_cls\": scores.sum() * 0,\n                \"loss_box_reg\": proposal_deltas.sum() * 0,\n            }\n\n        gt_classes = cat(gt_classes, dim=0)\n\n        # parse box regression outputs\n        if len(proposal_deltas.shape) > 2:\n            proposal_deltas = proposal_deltas.view(-1, proposal_deltas.shape[-1])\n\n        # classification and box regression loss\n        loss_cls = cross_entropy(scores, gt_classes, reduction=\"mean\")\n        loss_box_reg = self.box_reg_loss(\n            cat([p.proposal_boxes.tensor for p in proposals], dim=0),\n            cat([p.gt_boxes.tensor for p in proposals], dim=0),\n            proposal_deltas,\n            gt_classes,\n        )\n        return {\n            \"loss_cls\": loss_cls * self.loss_weight.get(\"loss_cls\", 1.0),\n            \"loss_box_reg\": loss_box_reg * self.loss_weight.get(\"loss_box_reg\", 1.0),\n        }\n"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # parse classification outputs\n        gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n        _log_classification_stats(scores, gt_classes)\n\n        # parse box regression outputs\n        if len(proposal_deltas) > 0:\n            box_dim = proposal_deltas.shape[1] // self.num_classes\n            cls_agnostic_bbox_reg = self.box2box_transform.weights is None\n            box_deltas = proposal_deltas.view(-1, self.num_classes, box_dim)\n            if cls_agnostic_bbox_reg:\n                box_deltas = box_deltas[:, 0]\n\n            proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)\n            assert not cls_agnostic_bbox_reg or len(box_deltas) == len(proposal_boxes)\n            if cls_agnostic_bbox_reg:\n                assert proposal_boxes.tensor.size()[1] == 4\n                gt_boxes = cat([p.gt_boxes.tensor for p in proposals], dim=0)\n            else:\n                # Here is an example to explain where the iou_thresh is used.\n                # In the network predictions are sorted in the following order.\n                #   self.cls_score.weight @ fc7_reshaped, which are nothing but some\n                #   scores assigned to the 80 classes (class-specific) or some\n                #   confidence scores (class-agnostic) for each box. The ground\n                #   truth boxes are in the fixed order of the 80 classes.\n                #   Say if iou_thresh is 0.5, the proposals with iou > 0.5 with\n                #   respect to 6th class will be kept.\n                gt_boxes = torch.cat([p.gt_boxes.tensor[p.gt_classes] for p in proposals"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.TRACKER.NAME\n    return TRACKER_HEADS_REGISTRY.get(name)(cfg)"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.TRACKER.NAME\n    return TRACKER_HEADS_REGISTRY.get(name)(cfg)"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.TRACKER.NAME\n    return TRACKER_HEADS_REGISTRY.get(name)(cfg)"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.TRACKER.NAME\n    return TRACKER_HEADS_REGISTRY.get(name)(cfg)"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.TRACKER.NAME\n    return TRACKER_HEADS_REGISTRY.get(name)(cfg)"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.TRACKER.NAME\n    return TRACKER_HEADS_REGISTRY.get(name)(cfg)"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.TRACKER.NAME\n    return TRACKER_HEADS_REGISTRY.get(name)(cfg)"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.TRACKER.NAME\n    return TRACKER_HEADS_REGISTRY.get(name)(cfg)"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.TRACKER.NAME\n    return TRACKER_HEADS_REGISTRY.get(name)(cfg)"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.MODEL.TRACKER_HEAD.NAME\n    return TRACKER_HEADS_REGISTRY.get(name)(cfg)"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.MODEL.TRACKER_HEAD.NAME\n    return TRACKER_HEADS_REGISTRY.get(name)(cfg)"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.MODEL.TRACKER_HEAD.NAME\n    return TRACKER_HEADS_REGISTRY.get(name)(cfg)"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.TRACKER.TRACKER_NAME\n    return TRACKER_HEADS_REGISTRY.get(name)(cfg)"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.TRACKER.TRACKER_NAME\n    return TRACKER_HEADS_REGISTRY.get(name)(cfg)"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.TRACKER_NAME\n    tracker = TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.MODEL.TRACKER.NAME\n    return TRACKER_HEADS_REGISTRY.get(name)(cfg)"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.MODEL.TRACKER.NAME\n    return TRACKER_HEADS_REGISTRY.get(name)(cfg)"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.TRACKER_HEAD\n    tracker = TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.MODEL.TRACKER_HEAD.NAME\n    tracker = TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)\n    return tracker"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.MODEL.TRACKER_HEAD.NAME\n    tracker = TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)\n    return tracker"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        if deltas.numel() == 0:\n            return torch.empty_like(deltas)\n\n        boxes = boxes.to(deltas.dtype)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w  # x1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h  # y1\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w  # x2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h  # y2\n\n        return"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[1] % 4 == 0 and boxes.shape[1] == 4\n\n        boxes = boxes.to(deltas.dtype)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w  # x1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h  # y1\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w  # x2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h  # y2\n\n        return"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        if deltas.numel() == 0:\n            return torch.empty_like(deltas)\n\n        deltas = deltas.float()  # ensure fp32 for decoding precision\n        boxes = boxes.to(deltas.dtype)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w  # x1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h  # y1\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w  # x2\n        pred_boxes[:, 3::"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        if deltas.numel() == 0:\n            return torch.empty_like(deltas)\n\n        boxes = boxes.to(deltas.dtype)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w  # x1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h  # y1\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w  # x2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h  # y2\n\n       "}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[1] % 4 == 0 and boxes.shape[1] == 4\n\n        boxes = boxes.to(deltas.dtype)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w  # x1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h  # y1\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w  # x2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h  # y2\n\n        return"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[1] % 4 == 0 and boxes.shape[1] == 4\n\n        boxes = boxes.to(deltas.dtype)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w  # x1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h  # y1\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w  # x2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h  # y2\n\n        return pred"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        if deltas.numel() == 0:\n            return torch.empty_like(deltas)\n\n        deltas = deltas.float()  # ensure fp32 for decoding precision\n        boxes = boxes.to(deltas.dtype)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w  # x1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h  # y1\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w  # x2\n        pred_boxes[:, 3::"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        deltas = deltas.float()  # ensure fp32 for decoding precision\n        boxes = boxes.to(deltas.dtype)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w  # x1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h  # y1\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w  # x2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h  # y2\n\n        return pred_boxes"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[1] % 4 == 0 and boxes.shape[1] == 4\n\n        boxes = boxes.to(deltas.dtype)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w  # x1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h  # y1\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w  # x2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h  # y2\n\n        return pred"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[1] % 4 == 0 and boxes.shape[1] == 4\n\n        boxes = boxes.to(deltas.dtype)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w  # x1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h  # y1\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w  # x2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h  # y2\n\n        return pred"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[1] % 4 == 0 and boxes.shape[1] == 4\n\n        boxes = boxes.to(deltas.dtype)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w  # x1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h  # y1\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w  # x2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h  # y2\n\n        return pred"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        if deltas.numel() == 0:\n            return torch.empty_like(deltas)\n\n        deltas = deltas.float()  # ensure fp32 for decoding precision\n        boxes = boxes.to(deltas.dtype)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w  # x1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h  # y1\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w  # x2\n        pred_boxes[:, 3::4"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        if deltas.numel() == 0:\n            return torch.empty_like(deltas)\n\n        deltas = deltas.float()  # ensure fp32 for decoding precision\n        boxes = boxes.to(deltas.dtype)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w  # x1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h  # y1\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w  # x2\n        pred_boxes[:, 3::4"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert isinstance(deltas, torch.Tensor), type(deltas)\n        assert isinstance(boxes, torch.Tensor), type(boxes)\n\n        deltas = deltas.float()  # ensure fp32 for decoding precision\n        boxes = boxes.to(deltas.dtype)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w  # x1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h  # y1\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w "}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[1] % 4 == 0 and boxes.shape[1] == 4\n\n        boxes = boxes.to(deltas.dtype)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w  # x1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h  # y1\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w  # x2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h  # y2\n\n        return pred_"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # Ensure the output is a valid box. See Sec 2.1 of https://arxiv.org/abs/2006.09214\n        deltas = F.relu(deltas)\n        boxes = boxes.to(deltas.dtype)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w  # x1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h  # y1\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w  # x2\n        pred_boxes[:,"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # Ensure the output is a valid box. See Sec 2.1 of https://arxiv.org/abs/2006.09214\n        deltas = F.relu(deltas)\n        boxes = boxes.to(deltas.dtype)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w  # x1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h  # y1\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w  # x2\n        pred_boxes[:,"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[1] % 4 == 0 and boxes.shape[1] == 4\n\n        deltas = deltas.float()  # ensure fp32 for decoding precision\n        boxes = boxes.to(deltas.dtype)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w  # x1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h  # y1\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w  # x2\n        pred_boxes[:, 3::4]"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # Ensure the output is a valid box. See Sec 2.1 of https://arxiv.org/abs/2006.09214\n        deltas = F.relu(deltas)\n        boxes = boxes.to(deltas.dtype)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w  # x1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h  # y1\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w  # x2\n        pred_boxes[:"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        # The deltas are relative to src_boxes. Convert to absolute boxes before\n        # applying the deltas.\n        boxes = boxes.to(deltas.dtype)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w  # x1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h  # y1\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w  # x2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h  # y2\n\n       "}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            return self.general_ins(image)\n        elif isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        output = self.general_ins(image)\n        if len(anno_type) == 1:\n            return output[anno_type[0]]\n        else:\n            return {tp: output[tp] for tp in anno_type}"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is not None:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n\n        output = self.general_ins(image)\n        if anno_type is not None:\n            if len(anno_type) == 1:\n                return output[anno_type[0]]\n            else:\n                return {tp: output[tp] for tp in anno_type}\n        else:\n            return output"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        output = self.general_ins.run(image)\n        if anno_type is not None:\n            if isinstance(anno_type, str):\n                assert anno_type in output.keys()\n                return output[anno_type]\n            elif isinstance(anno_type, (list, tuple)):\n                assert all(tp in output.keys() for tp in anno_type)\n                return {tp: output[tp] for tp in anno_type}\n            else:\n                raise Exception(f'Error anno_type: {anno_type}')\n        else:\n            return output"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            return self.general_ins.run(image)\n        else:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n            output = self.general_ins.run(image)\n            if len(anno_type) == 1:\n                return output[anno_type[0]]\n            else:\n                return {tp: output[tp] for tp in anno_type}"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        anno = self.general_ins(image)\n        if anno_type is None:\n            return anno\n        else:\n            if len(anno_type) == 1:\n                return anno[anno_type[0]]\n            else:\n                return {tp: anno[tp] for tp in anno_type}"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            return self.general_ins(image)\n        else:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n            out = self.general_ins(image)\n            if len(anno_type) == 1:\n                return out[anno_type[0]]\n            else:\n                return {tp: out[tp] for tp in anno_type}"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            return self.general_ins(image)\n        else:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n            output = self.general_ins(image)\n            return {tp: output[tp] for tp in anno_type}"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            return self.general_ins.run(image)\n        elif isinstance(anno_type, str):\n            return self.general_ins.run(image)[anno_type]\n        else:\n            return {tp: self.general_ins.run(image)[tp] for tp in anno_type}"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is not None:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n\n        processed_output = self.general_ins(image)\n\n        if anno_type is None:\n            return processed_output\n        else:\n            if len(anno_type) == 1:\n                return processed_output[anno_type[0]]\n            else:\n                return {tp: processed_output[tp] for tp in anno_type}"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        output = self.general_ins(image)\n        if anno_type is not None:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            anno_dict = {}\n            for tp in anno_type:\n                if tp in output.keys():\n                    anno_dict[tp] = output[tp]\n            return anno_dict\n        return output"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            anno_type = self.anno_type_map.keys()\n        elif isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        output = self.general_ins.run(image)\n        if len(anno_type) == 1:\n            return output[anno_type[0]]\n        else:\n            return {tp: output[tp] for tp in anno_type if tp in output.keys()}"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is not None:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n            anno_type = set(anno_type)\n        output = self.general_ins.run(image)\n        if anno_type is None:\n            return output\n        else:\n            return {k: v for k, v in output.items() if k in anno_type}"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        from scepter.modules.utils.distribute import we\n\n        output = self.general_ins(image)\n        if anno_type is None:\n            return output\n        elif isinstance(anno_type, str):\n            assert anno_type in output.keys()\n            return output[anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            assert all(tp in output.keys() for tp in anno_type)\n            return {tp: output[tp] for tp in anno_type}\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is not None:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            anno_type = [tp.lower() for tp in anno_type]\n            assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n\n        result = self.general_ins(image)\n        if anno_type is None:\n            return result\n        else:\n            if len(anno_type) == 1:\n                return result[anno_type[0]]\n            else:\n                return {tp: result[tp] for tp in anno_type}"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            return self.general_ins(image)\n\n        elif isinstance(anno_type, str):\n            assert anno_type in self.anno_type_map.keys()\n            anno_type = [anno_type]\n\n        elif isinstance(anno_type, (list, tuple)):\n            assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        output = self.general_ins(image)\n        anno_dict = {}\n        for tp in anno_type:\n            if tp in output.keys():\n                anno_dict[tp] = output[tp]\n\n        if len(anno_dict) == 1:\n            return list(anno_dict.values())[0]\n        else:\n            return anno_dict"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is not None:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n\n        image = image.astype(np.float32) / 255.0\n        image = image[..., ::-1]\n        image = np.transpose(image, (2, 0, 1))\n        image = np.expand_dims(image, axis=0)\n        image = np.ascontiguousarray(image)\n\n        output = self.general_ins(image)\n        if anno_type is None:\n            return output\n        else:\n            if len(anno_type) == 1:\n                return output[anno_type[0]]\n            else:\n                return {tp: output[tp] for tp in anno_type}"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            anno_type = self.anno_type_map.keys()\n        elif isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        anno_result = self.general_ins(image)\n        if len(anno_type) == 1:\n            anno_type = anno_type[0]\n            if anno_type in anno_result:\n                return anno_result[anno_type]\n            else:\n                raise Exception(f'{anno_type} not found in output.')\n        else:\n            return {\n                tp: anno_result[tp]\n                for tp in anno_type if tp in anno_result\n            }"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif anno_type is None:\n            anno_type = []\n\n        assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n\n        anno = self.general_ins.run(image)\n        if anno_type:\n            anno = {k: v for k, v in anno.items() if k in anno_type}\n\n        return anno"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            pass\n        elif anno_type is None:\n            anno_type = []\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        output = self.general_ins(image)\n        if len(anno_type) > 0:\n            output = {k: v for k, v in output.items() if k in anno_type}\n            if len(output) == 1:\n                output = list(output.values())[0]\n        return output\n"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            return self.general_ins.run(image)\n        else:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n            output = self.general_ins.run(image)\n            if len(anno_type) == 1:\n                return output[anno_type[0]]\n            else:\n                return {tp: output[tp] for tp in anno_type}\n\n"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        keywords = normalize_string(query).split(\" \")\n        scores = defaultdict(float)\n        for keyword in keywords:\n            scores = update_url_scores(scores, self.bm25(keyword))\n        return scores\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        keywords = normalize_string(query).split(\" \")\n        scores = defaultdict(float)\n        for kw in keywords:\n            scores = update_url_scores(scores, self.bm25(kw))\n        return scores\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        keywords = normalize_string(query).split(\" \")\n        scores = defaultdict(float)\n        for keyword in keywords:\n            scores = update_url_scores(scores, self.bm25(keyword))\n        return scores\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        keywords = normalize_string(query).split(\" \")\n        scores = defaultdict(float)\n        for keyword in keywords:\n            scores = update_url_scores(scores, self.bm25(keyword))\n        return scores\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        keywords = normalize_string(query).split(\" \")\n        scores = defaultdict(float)\n        for kw in keywords:\n            scores = update_url_scores(scores, self.bm25(kw))\n        return scores\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        keywords = normalize_string(query).split(\" \")\n        scores = {}\n        for keyword in keywords:\n            scores = update_url_scores(scores, self.bm25(keyword))\n        return scores\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        keywords = normalize_string(query).split(\" \")\n        scores = {}\n        for keyword in keywords:\n            scores = update_url_scores(scores, self.bm25(keyword))\n        return scores\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        keywords = normalize_string(query).split(\" \")\n        scores = {}\n        for keyword in keywords:\n            scores = update_url_scores(scores, self.bm25(keyword))\n        return scores\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        keywords = normalize_string(query).split(\" \")\n        scores = defaultdict(float)\n        for kw in keywords:\n            scores = update_url_scores(scores, self.bm25(kw))\n        return scores\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        keywords = normalize_string(query).split(\" \")\n        scores = {}\n        for keyword in keywords:\n            scores = update_url_scores(scores, self.bm25(keyword))\n        return scores\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        keywords = normalize_string(query).split(\" \")\n        scores = {}\n        for keyword in keywords:\n            scores = update_url_scores(scores, self.bm25(keyword))\n        return scores\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = normalize_string(query)\n        keywords = query.split(\" \")\n        scores = {}\n        for keyword in keywords:\n            scores = update_url_scores(scores, self.bm25(keyword))\n        return scores\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = normalize_string(query)\n        keywords = query.split(\" \")\n        scores = {}\n        for kw in keywords:\n            scores = update_url_scores(scores, self.bm25(kw))\n        return scores\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = normalize_string(query)\n        keywords = query.split(\" \")\n        scores = defaultdict(float)\n        for keyword in keywords:\n            scores = update_url_scores(scores, self.bm25(keyword))\n        return scores\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        keywords = normalize_string(query).split(\" \")\n        scores = defaultdict(float)\n        for keyword in keywords:\n            update_url_scores(scores, self.bm25(keyword))\n        return scores\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        keywords = normalize_string(query).split(\" \")\n        scores = defaultdict(float)\n        for keyword in keywords:\n            update_url_scores(scores, self.bm25(keyword))\n        return scores\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        result = {}\n        for kw in normalize_string(query).split(\" \"):\n            update_url_scores(result, self.bm25(kw))\n        return result\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        keywords = normalize_string(query).split(\" \")\n        result = {}\n        for keyword in keywords:\n            update_url_scores(result, self.bm25(keyword))\n        return result\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        result = defaultdict(float)\n        keywords = normalize_string(query).split(\" \")\n        for kw in keywords:\n            result = update_url_scores(result, self.bm25(kw))\n        return result\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        result = {}\n        keywords = normalize_string(query).split(\" \")\n        for kw in keywords:\n            result = update_url_scores(result, self.bm25(kw))\n        return result\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n\n    "}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n\n    "}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        h, w = box_size\n        x1 = torch.clamp(self.tensor[:, 0] - self.tensor[:, 2] / 2.0, min=0, max=w)\n        y1 = torch.clamp(self.tensor[:, 1] - self.tensor[:, 3] / 2.0, min=0, max=h)\n        x2 = torch.clamp(self.tensor[:, 0] + self.tensor[:, 2] / 2.0, min=0, max=w)\n        y2 = torch.clamp(self.tensor[:, 1] + self.tensor[:, 3] / 2.0, min=0, max=h)\n\n        angle = self.tensor[:, 4]\n        nearly_horizontal = torch.abs(angle) < clip_angle_threshold\n\n        # Convert nearly horizontal boxes to boxes represented by (x1, y1, x2, y2)\n        x1_nearly_horizontal = self.tensor[nearly_horizontal, 0] - self.tensor[nearly_horizontal, 2] / 2.0\n        y1_nearly_horizontal = self.tensor[nearly_horizontal, 1] - self.tensor[nearly_horizontal, 3] / 2.0\n        x2_nearly_horizontal = self.tensor[nearly_horizontal, 0] + self.tensor[nearly_horizontal, 2] / 2.0\n        y2_nearly_horizontal = self.tensor[nearly_horizontal, 1] + self.tensor[nearly_horizontal, 3] / 2.0\n\n        # Clamp the coordinates of nearly horizontal boxes\n        x1[nearly_horizontal] = torch.clamp(x1_nearly_horizontal, min=0, max=w)\n        y1[nearly_horizontal] = torch.clamp(y1_nearly_horizontal, min=0, max=h)\n        x2[nearly_horizontal] = torch.clamp(x"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        h, w = box_size\n        x1 = self.tensor[:, 0] - self.tensor[:, 2] / 2.0\n        y1 = self.tensor[:, 1] - self.tensor[:, 3] / 2.0\n        x2 = self.tensor[:, 0] + self.tensor[:, 2] / 2.0\n        y2 = self.tensor[:, 1] + self.tensor[:, 3] / 2.0\n\n        angle = self.tensor[:, 4]\n        near_horizontal_inds = torch.abs(angle) < clip_angle_threshold\n        near_horizontal_boxes = self.tensor[near_horizontal_inds]\n        if near_horizontal_boxes.numel() > 0:\n            x1_box = near_horizontal_boxes[:, 0] - near_horizontal_boxes[:, 2] / 2.0\n            x2_box = near_horizontal_boxes[:, 0] + near_horizontal_boxes[:, 2] / 2.0\n            y1_box = near_horizontal_boxes[:, 1] - near_horizontal_boxes[:, 3] / 2.0\n            y2_box = near_horizontal_boxes[:, 1] + near_horizontal_boxes[:, 3] / 2.0\n            x1 = torch.where(near_horizontal_inds, x1_box, x1)\n            x2 = torch.where(near_horizontal_inds, x2_box, x2)\n            y1 = torch.where(near_horizontal_inds, y1_box, y1)\n            y2 = torch.where(near_horizontal_inds, y2_box, y2)\n\n        x1 = torch.clamp(x1, min=0, max=w)\n        x2 = torch.clamp(x2, min=0, max=w)\n        y1 = torch.clamp(y1, min=0"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n\n        # Clip boxes to image\n        h, w = box_size\n        x1 = torch.clamp(self.tensor[:, 0] - self.tensor[:, 2] / 2.0, min=0, max=w)\n        y1 = torch.clamp(self.tensor[:, 1] - self.tensor[:, 3] / 2.0, min=0, max=h)\n        x2 = torch.clamp(self.tensor[:, 0] + self.tensor[:, 2] / 2.0, min=0, max=w)\n        y2 = torch.clamp(self.tensor[:, 1] + self.tensor[:, 3] / 2.0, min=0, max=h)\n\n        angle = self.tensor[:, 4]\n        # Identify boxes that are nearly horizontal\n        nearly_horizontal = torch.abs(angle) < clip_angle_threshold\n\n        # Convert nearly horizontal boxes to boxes represented by (x1, y1, x2, y2)\n        x1_horz = self.tensor[nearly_horizontal, 0] - self.tensor[nearly_horizontal, 2] / 2.0\n        x2_horz = self.tensor[nearly_horizontal, 0] + self.tensor[nearly_horizontal, 2] / 2.0\n        y1_horz = self.tensor[nearly_horizontal, 1] - self.tensor[nearly_horizontal, 3] / 2.0\n        y2_horz = self.tensor[nearly_horizontal, 1] + self.tensor[nearly_horizontal, 3] / 2.0\n\n        # Clamp the x and y coordinates of nearly horizontal boxes\n        x1_horz = torch.clamp(x1_horz, min=0, max=w)\n        x2_horz = torch.clamp(x2_horz, min=0, max=w)\n        y1_horz = torch.clamp("}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        box = self.tensor\n        self.normalize_angles()\n        w, h = box_size\n\n        # Check if the boxes are nearly horizontal\n        near_horizontal = torch.abs(box[:, 4]) < clip_angle_threshold\n\n        # Convert the boxes to (x1, y1, x2, y2) format\n        x1 = box[:, 0] - box[:, 2] / 2.0\n        y1 = box[:, 1] - box[:, 3] / 2.0\n        x2 = box[:, 0] + box[:, 2] / 2.0\n        y2 = box[:, 1] + box[:, 3] / 2.0\n\n        # Clamp the x coordinates\n        x1 = torch.clamp(x1, min=0, max=w)\n        x2 = torch.clamp(x2, min=0, max=w)\n\n        # Clamp the y coordinates\n        y1 = torch.clamp(y1, min=0, max=h)\n        y2 = torch.clamp(y2, min=0, max=h)\n\n        # Convert the boxes back to (center x, center y, width, height, angle) format\n        box[:, 0] = (x1 + x2) / 2.0\n        box[:, 1] = (y1 + y2) / 2.0\n        box[:, 2] = x2 - x1\n        box[:, 3] = y2 - y1\n\n        # Ensure numerical stability\n        box[:, 2:4] = torch.max(box[:, 2:4], torch.finfo(box.dtype).eps)\n\n        return"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        w, h = box_size\n\n        boxes = self.tensor\n        x_ctr = boxes[:, 0]\n        y_ctr = boxes[:, 1]\n        widths = boxes[:, 2]\n        heights = boxes[:, 3]\n        angles = boxes[:, 4]\n\n        # Step 1: Convert the boxes to (x1, y1, x2, y2) format\n        x1 = x_ctr - widths / 2.0\n        y1 = y_ctr - heights / 2.0\n        x2 = x_ctr + widths / 2.0\n        y2 = y_ctr + heights / 2.0\n\n        # Step 2: Identify boxes that are nearly horizontal\n        nearly_horizontal = torch.abs(angles) < clip_angle_threshold\n\n        # Step 3: Clamp the x and y coordinates\n        x1[nearly_horizontal] = torch.clamp(x1[nearly_horizontal], min=0, max=w)\n        x2[nearly_horizontal] = torch.clamp(x2[nearly_horizontal], min=0, max=w)\n        y1[nearly_horizontal] = torch.clamp(y1[nearly_horizontal], min=0, max=h)\n        y2[nearly_horizontal] = torch.clamp(y2[nearly_horizontal], min=0, max=h)\n\n        # Step 4: Convert the boxes back to (x_ctr, y_ctr, width, height, angle) format\n        boxes[:, 0] = (x1 + x2) / 2.0\n        boxes[:, 1] = (y1 + y2) / 2.0\n        boxes[:, 2] = x2 - x1\n        boxes[:, 3] = y2 - y1\n\n        # Step 5: Ensure numerical errors do not increase the size of the boxes\n        boxes[:, 2] = torch.clamp(boxes[:, 2], min=0)\n        boxes[:"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        h, w = box_size\n        x1 = self.tensor[:, 0] - self.tensor[:, 2] / 2.0\n        y1 = self.tensor[:, 1] - self.tensor[:, 3] / 2.0\n        x2 = self.tensor[:, 0] + self.tensor[:, 2] / 2.0\n        y2 = self.tensor[:, 1] + self.tensor[:, 3] / 2.0\n\n        angle = self.tensor[:, 4]\n        near_horizontal = (angle.abs() % 90.0).le(clip_angle_threshold)\n\n        self.tensor[:, :4][near_horizontal, :] = torch.stack(\n            (\n                x1[near_horizontal],\n                y1[near_horizontal],\n                x2[near_horizontal],\n                y2[near_horizontal],\n            ),\n            dim=1,\n        )\n\n        self.tensor[:, 0:2].clamp_(min=0, max=w)\n        self.tensor[:, 2].clamp_(min=0, max=w)\n        self.tensor[:, 1:3].clamp_(min=0, max=h)\n        self.tensor[:, 3].clamp_(min=0, max=h)\n\n        self.tensor[:, 2:4] = self.tensor[:, 0:2] + self.tensor[:, 2:4] - self.tensor[:, 0:2]"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        h, w = box_size\n\n        # Clip the rotated boxes to the image size\n        x1 = torch.clamp(self.tensor[:, 0] - self.tensor[:, 2] / 2.0, min=0, max=w)\n        y1 = torch.clamp(self.tensor[:, 1] - self.tensor[:, 3] / 2.0, min=0, max=h)\n        x2 = torch.clamp(self.tensor[:, 0] + self.tensor[:, 2] / 2.0, min=0, max=w)\n        y2 = torch.clamp(self.tensor[:, 1] + self.tensor[:, 3] / 2.0, min=0, max=h)\n\n        # Clip the nearly horizontal boxes to fit the image width,\n        # by keeping the center point unchanged\n        rot_90_idxs = torch.where(\n            torch.abs(self.tensor[:, 4]) % 180.0 < clip_angle_threshold\n        )[0]\n        if len(rot_90_idxs) > 0:\n            rot_90_boxes = self.tensor[rot_90_idxs]\n            rot_90_boxes[:, 2] = h\n            rot_90_boxes[:, 3] = rot_90_boxes[:, 2] / w * rot_90_boxes[:, 3]\n            rot_90_boxes[:, 1] = torch.clamp(\n                rot_90_boxes[:, 1] - rot_90_boxes[:, 3] / 2.0, min=0, max=h\n            )\n            self.tensor[rot_90_idxs] = rot_90_boxes\n\n        self.tensor = torch.stack((x1, y1, x2, y2, self.tensor[:, 4]), dim=1)\n\n        # Clip"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        h, w = box_size\n\n        self.normalize_angles()\n\n        # Clip boxes that are nearly horizontal\n        inds_almost_horizontal = torch.abs(self.tensor[:, 4]) <= clip_angle_threshold\n        self.tensor[inds_almost_horizontal, 0] = torch.clamp(\n            self.tensor[inds_almost_horizontal, 0], min=0, max=w\n        )\n        self.tensor[inds_almost_horizontal, 1] = torch.clamp(\n            self.tensor[inds_almost_horizontal, 1], min=0, max=h\n        )\n\n        # Clip boxes that are nearly vertical\n        inds_almost_vertical = torch.abs(torch.abs(self.tensor[:, 4]) - 90) <= clip_angle_threshold\n        self.tensor[inds_almost_vertical, 0] = torch.clamp(\n            self.tensor[inds_almost_vertical, 0], min=0, max=w\n        )\n        self.tensor[inds_almost_vertical, 1] = torch.clamp(\n            self.tensor[inds_almost_vertical, 1], min=0, max=h\n        )\n\n        # Clip boxes that are not nearly horizontal or vertical\n        inds_other = torch.bitwise_not(torch.bitwise_or(inds_almost_horizontal, inds_almost_vertical))\n        self.tensor[inds_other, 0] = torch.clamp(self.tensor[inds_other, 0], min=0, max=w)\n        self.tensor[inds_other, 1] = torch.clamp(self.tensor[inds_other, 1], min=0, max=h)\n\n        # Convert boxes to (x1, y1, x2, y2) format\n        theta = self.tensor[:, 4] * math.pi / 180.0\n        c = torch.cos(theta)\n        s = torch"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n\n        w, h = box_size\n\n        # The distance from the center of the box to the nearest edge\n        half_w = self.tensor[:, 2] / 2.0\n        half_h = self.tensor[:, 3] / 2.0\n        # The distance from the center of the box to the farthest edge\n        max_rect_half_w = torch.sqrt(half_w * half_w + half_h * half_h)\n\n        # Find boxes that are nearly horizontal\n        near_horizontal_inds = torch.abs(self.tensor[:, 4]) < clip_angle_threshold\n        # Compute the offset for the nearly horizontal boxes\n        offset = max_rect_half_w - half_w\n        offset[~near_horizontal_inds] = 0\n\n        # Convert the boxes to (x1, y1, x2, y2) format\n        x1 = self.tensor[:, 0] - offset\n        y1 = self.tensor[:, 1] - half_h\n        x2 = self.tensor[:, 0] + offset\n        y2 = self.tensor[:, 1] + half_h\n\n        # Clamp the boxes to the image boundaries\n        x1 = torch.clamp(x1, min=0, max=w)\n        y1 = torch.clamp(y1, min=0, max=h)\n        x2 = torch.clamp(x2, min=0, max=w)\n        y2 = torch.clamp(y2, min=0, max=h)\n\n        # Convert the boxes back to (x_center, y_center, width, height, angle) format\n        self.tensor[:, 0] = (x1 + x2) / 2\n        self.tensor[:, 1] = (y1 + y2) / 2\n        self.tensor[:, 2] = x2 - x1\n        self.tensor[:, 3] = y2 - y1\n\n        # Ensure that the boxes are not degenerate\n       "}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        box = self.tensor\n        w, h = box_size\n\n        # Avoid the complexities and ambiguities involved in clipping highly rotated boxes\n        # by only clipping boxes that are almost horizontal\n        # (determined by the given angle threshold)\n        inds_almost_horizontal = torch.abs(box[:, 4]) < clip_angle_threshold\n        box_almost_horizontal = box[inds_almost_horizontal]\n        if box_almost_horizontal.numel() > 0:\n            x1 = box_almost_horizontal[:, 0] - box_almost_horizontal[:, 2] / 2.0\n            y1 = box_almost_horizontal[:, 1] - box_almost_horizontal[:, 3] / 2.0\n            x2 = box_almost_horizontal[:, 0] + box_almost_horizontal[:, 2] / 2.0\n            y2 = box_almost_horizontal[:, 1] + box_almost_horizontal[:, 3] / 2.0\n            x1 = torch.clamp(x1, min=0, max=w)\n            x2 = torch.clamp(x2, min=0, max=w)\n            y1 = torch.clamp(y1, min=0, max=h)\n            y2 = torch.clamp(y2, min=0, max=h)\n            box_almost_horizontal = torch.stack((x1, y1, x2, y2), dim=-1)\n            box_almost_horizontal = torch.cat(\n                (\n                    (box_almost_horizontal[:, :2] + box_almost_horizontal[:, 2:]) / 2,\n                    box_almost_horizontal[:, 2:] - box_almost_horizontal[:, :2],\n                    box_almost_horizontal[:, 4:],\n                ),\n                dim=-1,\n            )\n            box[inds_almost_horizontal] = box_almost_horizontal\n\n        # Clip the boxes that are not almost horizontal\n        inds_not"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        h, w = box_size\n\n        # The near-horizontal boxes that are almost horizontal,\n        # whose angles are smaller than \"clip_angle_threshold\" or\n        # larger than (180 - clip_angle_threshold)\n        almost_horizontal_inds = torch.abs(self.tensor[:, 4]) < clip_angle_threshold\n        # To avoid the ambiguous case of angle == 180\n        other_inds = torch.abs(self.tensor[:, 4] - 90) < clip_angle_threshold\n\n        near_horizontal_inds = almost_horizontal_inds | other_inds\n\n        if near_horizontal_inds.any():\n            xc, yc, w, h, a = self.tensor[near_horizontal_inds].unbind(1)\n            box = torch.stack(\n                [\n                    xc - w / 2.0,\n                    yc - h / 2.0,\n                    xc + w / 2.0,\n                    yc + h / 2.0,\n                ],\n                dim=1,\n            )\n            box[:, 0::2].clamp_(min=0, max=w)\n            box[:, 1::2].clamp_(min=0, max=h)\n            wh = box[:, 2:] - box[:, :2]\n            assert (wh >= 0).all()\n            area = wh[:, 0] * wh[:, 1]\n            alpha = area / (4.0 * torch.min(wh[:, 0], wh[:, 1]) - area)\n            w = torch.sqrt(area / alpha)\n            h = alpha * w\n            self.tensor[near_horizontal_inds] = torch.stack(\n                [xc, yc, w, h, a], dim=1\n            )\n\n        self.tensor[:, 0].clamp_(min=0, max=w)\n        self.tensor[:, 1].clamp_(min=0, max=h)\n        self"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        h, w = box_size\n        x1 = self.tensor[:, 0] - self.tensor[:, 2] / 2.0\n        y1 = self.tensor[:, 1] - self.tensor[:, 3] / 2.0\n        x2 = self.tensor[:, 0] + self.tensor[:, 2] / 2.0\n        y2 = self.tensor[:, 1] + self.tensor[:, 3] / 2.0\n\n        # Clip boxes to image boundaries\n        x1 = torch.clamp(x1, min=0, max=w)\n        y1 = torch.clamp(y1, min=0, max=h)\n        x2 = torch.clamp(x2, min=0, max=w)\n        y2 = torch.clamp(y2, min=0, max=h)\n\n        # Clip boxes to ensure  x1 < x2, y1 < y2\n        x1 = torch.min(x1, x2)\n        y1 = torch.min(y1, y2)\n        x2 = torch.max(x1, x2)\n        y2 = torch.max(y1, y2)\n\n        angle = self.tensor[:, 4]\n        # Clip boxes to horizontal boxes\n        ws = x2 - x1\n        hs = y2 - y1\n        # Keep boxes with widths and heights that are at least 1 pixel\n        keep = (ws > 1) & (hs > 1)\n        # Keep boxes with widths and heights that are at least 1 pixel\n        keep &= (torch.abs(angle) < clip_angle_threshold) | (torch.abs(angle) > 180 - clip_angle_threshold)\n        self.tensor = torch.stack(((x1 + x2) / 2, (y1 + y2) / 2, x2 - x1, y2 - y1, angle), dim=1)[\n            keep\n        ]"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        w, h = box_size\n\n        # For boxes with angle ~ 0 or 180, we can simply clip the x and y coordinates\n        # to ensure they are within the box size.\n        # For boxes with angle ~ 90 or 270, we need to convert them to the format\n        # (x1, y1, x2, y2) before clipping, and convert them back to the format\n        # (x_center, y_center, width, height, angle) after clipping.\n        # The reason is that clipping rotated boxes with angle ~ 90 or 270 is\n        # more complex and ambiguous, and it's easier to handle if we convert them\n        # to the format (x1, y1, x2, y2) first.\n        #\n        # For boxes with angle ~ 0 or 180, we can simply clip the x and y coordinates\n        # to ensure they are within the box size.\n        # For boxes with angle ~ 90 or 270, we need to convert them to the format\n        # (x1, y1, x2, y2) before clipping, and convert them back to the format\n        # (x_center, y_center, width, height, angle) after clipping.\n        # The reason is that clipping rotated boxes with angle ~ 90 or 270 is\n        # more complex and ambiguous, and it's easier to handle if we convert them\n        # to the format (x1, y1, x2, y2) first.\n        #\n        # For boxes with angle ~ 0 or 180, we can simply clip the x and y coordinates\n        # to ensure they are within the box size.\n        # For boxes with angle ~ 90 or 270, we need to convert them to the format\n        # (x1, y1, x2, y2) before clipping, and convert them back to the format\n        # (x_center, y_center, width, height, angle) after clipping.\n        # The reason is that clipping rotated boxes"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        box = self.tensor\n        w, h = box_size\n\n        # The following codes are modified from torchvision.ops.box_convert\n        # Assume the boxes are in the format of (x_center, y_center, width, height, angle)\n        # where angle is represented in degrees.\n        # x_center, y_center, width, height = box[:, :4].T\n        # angle = box[:, 4]\n\n        # Convert the boxes to (x1, y1, x2, y2) format\n        # x1, y1 = x_center - width / 2, y_center - height / 2\n        # x2, y2 = x_center + width / 2, y_center + height / 2\n        # boxes = torch.stack((x1, y1, x2, y2, angle), dim=1)\n        boxes = box\n\n        # Check if the boxes are nearly horizontal\n        is_near_horizontal = torch.abs(boxes[:, 4]) < clip_angle_threshold\n\n        # Adjust the boxes to fit within the image boundaries\n        boxes[:, 0::2].clamp_(min=0, max=w)  # Clamp x coordinates\n        boxes[:, 1::2].clamp_(min=0, max=h)  # Clamp y coordinates\n\n        # Convert the boxes back to (x_center, y_center, width, height, angle) format\n        # x1, y1, x2, y2 = boxes[:, :4].T\n        # x_center = (x1 + x2) / 2\n        # y_center = (y1 + y2) / 2\n        # width = x2 - x1\n        # height = y2 - y1\n        # angle = boxes[:, 4]\n        # boxes = torch.stack((x_center, y_center, width, height, angle), dim=1)\n        boxes = boxes\n\n        # Ensure the numerical errors do not increase the box sizes\n        boxes[:, 2:4].clamp_(min"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        box = self.tensor\n        w, h = box_size\n\n        # For nearly horizontal boxes, convert them to axis-aligned boxes\n        # (x1, y1, x2, y2) by converting (center x, center y, width, height, angle)\n        # to (x1, y1, x2, y2, angle), where (x1, y1) and (x2, y2) are the coordinates of\n        # the top-left and bottom-right corners, respectively.\n        # This is done to avoid the complexities and ambiguities involved in clipping\n        # highly rotated boxes.\n        nearly_horizontal = torch.abs(box[:, 4]) < clip_angle_threshold\n        if nearly_horizontal.any():\n            x_ctr, y_ctr, width, height, angle = box[nearly_horizontal].unbind(1)\n            x1 = x_ctr - width / 2.0\n            x2 = x_ctr + width / 2.0\n            y1 = y_ctr - height / 2.0\n            y2 = y_ctr + height / 2.0\n            clipped_box = torch.stack((x1, y1, x2, y2, angle), dim=1)\n            box[nearly_horizontal] = clipped_box\n\n        box[:, 0].clamp_(min=0, max=w)\n        box[:, 1].clamp_(min=0, max=h)\n        box[:, 2].clamp_(min=0, max=w)\n        box[:, 3].clamp_(min=0, max=h)\n\n        # Convert the boxes back to their original representation\n        # to ensure numerical stability.\n        if nearly_horizontal.any():\n            x1, y1, x2, y2, angle = box[nearly_horizontal].unbind(1)\n            x_ctr = (x1 + x2) / 2.0\n            y_ctr = (y1 + y2) / 2.0\n            width = x2 - x1\n            height ="}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize angles to be within [-180, 180) degrees\n        self.normalize_angles()\n\n        # Find the indices of boxes that are nearly horizontal\n        # (i.e., their angle is within the range [-clip_angle_threshold, clip_angle_threshold])\n        horizontal_indices = torch.abs(self.tensor[:, 4]) <= clip_angle_threshold\n\n        # Convert the representation of these boxes from (center x, center y, width, height, angle)\n        # to (x1, y1, x2, y2) to simplify clipping\n        boxes_horizontal = self.tensor[horizontal_indices]\n        boxes_horizontal[:, 2:] /= 2\n        boxes_horizontal[:, :2] -= boxes_horizontal[:, 2:]\n        boxes_horizontal[:, 2:] *= 2\n\n        # Clip the x and y coordinates of these boxes to ensure they fit within the box size\n        boxes_horizontal[:, 0::2].clamp_(min=0, max=box_size[1])\n        boxes_horizontal[:, 1::2].clamp_(min=0, max=box_size[0])\n\n        # Convert the boxes back to the original representation\n        # to ensure that any numerical errors do not increase their sizes\n        boxes_horizontal[:, 2:] /= 2\n        boxes_horizontal[:, :2] += boxes_horizontal[:, 2:]\n\n        # Update the original boxes with the clipped boxes\n        self.tensor[horizontal_indices] = boxes_horizontal\n\n        # Clip the remaining boxes that are not nearly horizontal\n        # (i.e., their angle is outside the range [-clip_angle_threshold, clip_angle_threshold])\n        other_indices = ~horizontal_indices\n        boxes_other = self.tensor[other_indices]\n        boxes_other[:, 2:4].clamp_(min=0, max=box_size)\n\n        self.tensor[other_indices] = boxes_other\n\n        # Normalize the angles again to ensure they are within the range [-180, 1"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n\n        # Identify the indices of boxes that are nearly horizontal\n        box = self.tensor\n        a = box[:, 4]\n        inds_almost_horizontal = torch.abs(a) < clip_angle_threshold\n\n        # Convert the boxes to (x1, y1, x2, y2) format\n        x_ctr, y_ctr, width, height, a = box.unbind(1)\n        x1 = x_ctr - width / 2.0\n        x2 = x_ctr + width / 2.0\n        y1 = y_ctr - height / 2.0\n        y2 = y_ctr + height / 2.0\n\n        # Clamp the x and y coordinates\n        x1 = torch.clamp(x1, min=0, max=box_size[1])\n        x2 = torch.clamp(x2, min=0, max=box_size[1])\n        y1 = torch.clamp(y1, min=0, max=box_size[0])\n        y2 = torch.clamp(y2, min=0, max=box_size[0])\n\n        # Convert the boxes back to (x_ctr, y_ctr, width, height, angle) format\n        # Note: The width and height are not strictly accurate after clipping\n        #       since we are clipping the box corners, but it's sufficient for our purposes\n        width = x2 - x1\n        height = y2 - y1\n        x_ctr = (x1 + x2) / 2.0\n        y_ctr = (y1 + y2) / 2.0\n\n        # Ensure numerical stability\n        eps = 1e-6\n        width = torch.clamp(width, min=eps)\n        height = torch.clamp(height, min=eps)\n\n        # Update the tensor with the clipped boxes\n        self.tensor = torch.stack((x_ctr, y_ctr, width, height, a), dim=1)"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n\n        h, w = box_size\n        x_ctr = self.tensor[:, 0]\n        y_ctr = self.tensor[:, 1]\n        w = self.tensor[:, 2]\n        h = self.tensor[:, 3]\n        a = self.tensor[:, 4]\n        keep = (w > 0) & (h > 0)\n\n        inds_inside = (\n            (x_ctr >= -clip_angle_threshold)\n            & (y_ctr >= -clip_angle_threshold)\n            & (x_ctr + w < w + clip_angle_threshold)\n            & (y_ctr + h < h + clip_angle_threshold)\n        )\n        keep &= inds_inside\n\n        # if no inds_inside, keep all inds\n        if not inds_inside.any():\n            return\n\n        # convert the boxes to x1, y1, x2, y2\n        x_ctr, y_ctr, w, h, a = self.tensor.unbind(1)\n        x1 = x_ctr - 0.5 * w\n        y1 = y_ctr - 0.5 * h\n        x2 = x_ctr + 0.5 * w\n        y2 = y_ctr + 0.5 * h\n\n        # clip boxes by img_shape, not by boundary_threshold\n        boxes = torch.stack((x1, y1, x2, y2), dim=1)\n        boxes[:, 0].clamp_(min=0, max=w)\n        boxes[:, 1].clamp_(min=0, max=h)\n        boxes[:, 2].clamp_(min=0, max=w)\n        boxes[:, 3].clamp_(min=0, max=h)\n\n        # clip boxes by boundary_threshold\n        boxes = torch.stack((x1, y1, x2, y2), dim=1)\n        keep = self.inside_box(box_size, boundary_threshold=clip_angle"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        assert torch.isfinite(self.tensor).all(), \"Box tensor contains infinite or NaN!\"\n        self.normalize_angles()\n        h, w = box_size\n\n        boxes = self.tensor\n        x_ctr = boxes[:, 0]\n        y_ctr = boxes[:, 1]\n        width = boxes[:, 2]\n        height = boxes[:, 3]\n        angle = boxes[:, 4]\n\n        cosa = torch.cos(angle)\n        sina = torch.sin(angle)\n        dx = cosa * width / 2.0\n        dy = sina * width / 2.0\n        dw = -sina * height\n        dh = cosa * height\n\n        # Prevent sending too large values into torch.where()\n        dw = torch.clamp(dw, min=-w, max=w)\n        dh = torch.clamp(dh, min=-h, max=h)\n\n        # xmin, ymin, xmax, ymax\n        x_min = x_ctr - dx + dw\n        x_max = x_ctr + dx + dw\n        y_min = y_ctr - dy + dh\n        y_max = y_ctr + dy + dh\n\n        # Clip boxes to image size\n        x_min = torch.clamp(x_min, min=0, max=w)\n        y_min = torch.clamp(y_min, min=0, max=h)\n        x_max = torch.clamp(x_max, min=0, max=w)\n        y_max = torch.clamp(y_max, min=0, max=h)\n\n        # Clip boxes to ensure they are nearly horizontal\n        # This is to ensure compatibility with the original implementation\n        # of the function when rotated boxes are near horizontal\n        clip_angle_threshold = clip_angle_threshold * math.pi / 180.0\n        boxes_nearly_horizontal = torch.abs(angle) < clip_angle_threshold\n        boxes_nearly_horizontal = boxes_"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n\n        # Clip boxes represented with (x_center, y_center, width, height, angle)\n        # to make sure they are within the image region defined by box_size\n        w, h = box_size\n        x_ctr, y_ctr, width, height, angle = self.tensor.unbind(dim=1)\n        # Convert (x_center, y_center, width, height, angle) representation to (x1, y1, x2, y2)\n        x1 = x_ctr - width / 2.0\n        y1 = y_ctr - height / 2.0\n        x2 = x_ctr + width / 2.0\n        y2 = y_ctr + height / 2.0\n        # Clip each of the boxes to be within the image region defined by box_size\n        x1 = x1.clamp(min=0, max=w)\n        y1 = y1.clamp(min=0, max=h)\n        x2 = x2.clamp(min=0, max=w)\n        y2 = y2.clamp(min=0, max=h)\n        # Convert (x1, y1, x2, y2) representation to (x_center, y_center, width, height, angle)\n        x_ctr = (x1 + x2) / 2.0\n        y_ctr = (y1 + y2) / 2.0\n        width = x2 - x1\n        height = y2 - y1\n        # Keep boxes with angles close to 0 or 90 degrees\n        # to avoid complexities and ambiguities\n        # in clipping rotated boxes\n        angles_close_to_0_90 = torch.abs(angle) < clip_angle_threshold\n        inds_close_to_0_90 = torch.nonzero(angles_close_to_0_90).squeeze(1)\n        x_ctr[inds_close_to_0_90] = x_ctr[inds_close_to_0_90].clamp"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n        for item in self.data:\n            statistics[item['type']] += 1\n        return statistics"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n        for item in self.data:\n            statistics[item['type']] += 1\n        return statistics"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0,\n        }\n        for item in self.data:\n            statistics[item['type']] += 1\n        return statistics"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n        for item in self.data:\n            statistics[item['type']] += 1\n        return statistics"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0,\n        }\n        for d in self.data:\n            statistics[d['type']] += 1\n        return statistics"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n        for item in self.data:\n            if item['type'] == 'doc':\n                statistics['doc'] += 1\n            elif item['type'] == 'gen':\n                statistics['gen'] += 1\n            elif item['type'] == 'kno':\n                statistics['kno'] += 1\n            elif item['type'] == 'num':\n                statistics['num'] += 1\n        return statistics"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n        for item in self.data:\n            statistics[item['type']] += 1\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0,\n        }\n        for d in self.data:\n            statistics[d['type']] += 1\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n        for d in self.data:\n            statistics[d['type']] += 1\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n\n        for item in self.data:\n            statistics[item['type']] += 1\n\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n        for item in self.data:\n            statistics[item['type']] += 1\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n        for item in self.data:\n            if item['type'] in statistics:\n                statistics[item['type']] += 1\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0,\n        }\n        for item in self.data:\n            statistics[item['type']] += 1\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n        for item in self.data:\n            if item['type'] in statistics:\n                statistics[item['type']] += 1\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        stats = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n        for item in self.data:\n            stats[item['type']] += 1\n        return stats\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n        for item in self.data:\n            if item['type'] in statistics:\n                statistics[item['type']] += 1\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n        for item in self.data:\n            if item['type'] in statistics:\n                statistics[item['type']] += 1\n        return statistics\n\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0,\n        }\n\n        for d in self.data:\n            if d['type'] == 'doc':\n                statistics['doc'] += 1\n            elif d['type'] == 'gen':\n                statistics['gen'] += 1\n            elif d['type'] == 'kno':\n                statistics['kno'] += 1\n            elif d['type'] == 'num':\n                statistics['num'] += 1\n\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0,\n        }\n        for d in self.data:\n            statistics[d['type']] += 1\n        return statistics\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        stats = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0,\n        }\n        for item in self.data:\n            if item['type'] in stats:\n                stats[item['type']] += 1\n        return stats\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMDET_LOSSES._module_dict.keys():\n        return MMDET_LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        raise NotImplementedError"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMDET_LOSSES._module_dict.keys():\n        return MMDET_LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        raise NotImplementedError"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMDET_LOSSES._module_dict.keys():\n        return MMDET_LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        raise NotImplementedError"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMDET_LOSSES._module_dict.keys():\n        return MMDET_LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        raise NotImplementedError"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMDET_LOSSES._module_dict.keys():\n        return MMDET_LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        raise NotImplementedError"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    elif cfg['type'] in MMDET_LOSSES._module_dict.keys():\n        return MMDET_LOSSES.build(cfg)\n    else:\n        raise NotImplementedError(f'{cfg[\"type\"]} is not supported now.')"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMDET_LOSSES._module_dict.keys():\n        return MMDET_LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        raise NotImplementedError(\n            f'{cfg[\"type\"]} is not supported by the factory')"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMDET_LOSSES._module_dict.keys():\n        return MMDET_LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        raise NotImplementedError(f'Unrecognized type of loss: {cfg[\"type\"]}')"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(dt_annos) == len(gt_annos)\n    pred = {}  # map {class_id: pred}\n    gt = {}  # map {class_id: gt}\n    for img_id in range(len(dt_annos)):\n        # parse detected annotations\n        det_anno = dt_annos[img_id]\n        for i in range(len(det_anno['name'])):\n            class_name = label2cat[det_anno['name'][i]]\n            bbox = det_anno['bbox'][i]\n            score = det_anno['score'][i]\n            if class_name not in pred:\n                pred[class_name] = {}\n            if img_id not in pred[class_name]:\n                pred[class_name][img_id] = []\n            x, y, z = bbox[0], bbox[1], bbox[2]\n            l, w, h, r = bbox[3], bbox[4], bbox[5], bbox[6]\n            score = score\n            if box_type_3d == 'Depth':\n                box_pts = box_np_ops.center_to_corner_box3d(\n                    bbox[0:3], bbox[3:6], bbox[6], origin=(0.5, 0.5, 0.5),\n                    axis=2)\n                box_pts_fl = box_pts.reshape(-1, 3)\n            else:\n                raise NotImplementedError\n            pred[class_name][img_id].append((box_pts_fl, score))\n\n        # parse gt annotations\n        gt_anno = gt_annos[img_id]\n        if len(gt_anno['name']) == 0:\n            continue\n        for i in range(len(gt_anno['name'])):\n            class_name = label2cat[gt_anno['name'][i]]\n            bbox = gt_anno['bbox']["}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(dt_annos) == len(gt_annos)\n    pred = {}  # map {class_id: pred}\n    gt = {}  # map {class_id: gt}\n    for img_id in range(len(dt_annos)):\n        # parse detected annotations\n        det_anno = dt_annos[img_id]\n        for i in range(len(det_anno['name'])):\n            class_name = label2cat[det_anno['name'][i]]\n            bbox = det_anno['bbox'][i]\n            score = det_anno['score'][i]\n            if class_name in pred:\n                pred[class_name].append((bbox, score))\n            else:\n                pred[class_name] = [(bbox, score)]\n\n        # parse gt annotations\n        anno = gt_annos[img_id]\n        if anno['name'] is None:\n            continue\n        for i in range(len(anno['name'])):\n            class_name = label2cat[anno['name'][i]]\n            bbox = anno['bbox'][i]\n            if not box_type_3d:\n                bbox = box_type_3d(bbox, box_mode_3d, Box3DMode.DEPTH,\n                                   np.array([0, -1, 0]))\n            if class_name in gt:\n                gt[class_name].append(bbox)\n            else:\n                gt[class_name] = [bbox]\n\n    rec, prec, ap = eval_map_recall(pred, gt, metric)\n    ret_dict = dict()\n    header = ['class', 'gts', 'dets', 'recall', 'ap']\n    table_rows = [header]\n    for i, cls_name in enumerate(ap.keys()):\n        row = [\n            cls_name, gt[cls_name].shape[0], pred[cls_name].shape[0],"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(dt_annos) == len(gt_annos)\n    pred = {}\n    gt = {}\n    for cat in label2cat.keys():\n        pred[cat] = []\n        gt[cat] = []\n\n    for img_id in range(len(dt_annos)):\n        for i in range(len(dt_annos[img_id]['name'])):\n            if dt_annos[img_id]['name'][i] in label2cat:\n                bbox = dt_annos[img_id]['bbox'][i]\n                score = dt_annos[img_id]['score'][i]\n                label = dt_annos[img_id]['label'][i]\n                bbox[2] -= bbox[0]\n                bbox[3] -= bbox[1]\n                pred[label2cat[label]].append([bbox, score])\n        for i in range(len(gt_annos[img_id]['name'])):\n            if gt_annos[img_id]['name'][i] in label2cat:\n                bbox = gt_annos[img_id]['bbox'][i]\n                label = gt_annos[img_id]['label'][i]\n                bbox[2] -= bbox[0]\n                bbox[3] -= bbox[1]\n                gt[label2cat[label]].append(bbox)\n\n    rec, prec, ap = eval_map_recall(pred, gt, metric)\n    ret_dict = {}\n    header = ['classes']\n    table_columns = [[label2cat[label] for label in label2cat.keys()]]\n    for i, iou_thresh in enumerate(metric):\n        header.append('AP_{:.2f}'.format(iou_thresh))\n        header.append('AR_{:.2f}'.format(iou_thresh))\n        table_columns.append(list(ap[i].values()))\n        table_"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(dt_annos) == len(gt_annos)\n    pred = {}  # map {class_id: pred}\n    gt = {}  # map {class_id: gt}\n    for img_id in range(len(dt_annos)):\n        # parse detected annotations\n        det_anno = dt_annos[img_id]\n        for i in range(len(det_anno['name'])):\n            class_name = label2cat[det_anno['name'][i]]\n            bbox = det_anno['bbox'][i]\n            score = det_anno['score'][i]\n            if class_name in pred:\n                pred[class_name].append((bbox, score))\n            else:\n                pred[class_name] = [(bbox, score)]\n\n        # parse gt annotations\n        anno = gt_annos[img_id]\n        if anno['gt_num'] != 0:\n            gt_inds = anno['class']\n            for i in range(len(gt_inds)):\n                class_name = label2cat[gt_inds[i]]\n                bbox = anno['bbox'][i]\n                if class_name in gt:\n                    gt[class_name].append(bbox)\n                else:\n                    gt[class_name] = [bbox]\n\n    rec, prec, ap = eval_map_recall(pred, gt, metric)\n    ret_dict = dict()\n    header = ['class', 'gts', 'dets', 'recall', 'ap', 'precision']\n    table_columns = [[\n        class_name, gt[class_name].shape[0], pred[class_name].shape[0] if class_name in pred else 0,\n        np.mean(rec[iou_thresh][class_name]) if class_name in rec[iou_thresh] else 'nan',\n        np.mean(ap[iou_thresh][class_name]) if class_name"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(dt_annos) == len(gt_annos)\n    pred = {}  # map {class_id: pred}\n    gt = {}  # map {class_id: gt}\n    for img_id in range(len(dt_annos)):\n        # parse detected annotations\n        det_anno = dt_annos[img_id]\n        for i in range(len(det_anno['name'])):\n            class_name = det_anno['name'][i]\n            bbox = det_anno['bbox'][i]\n            score = det_anno['score'][i]\n            if class_name not in pred:\n                pred[class_name] = {}\n            if img_id not in pred[class_name]:\n                pred[class_name][img_id] = []\n            x, y, z = bbox[0], bbox[1], bbox[2]\n            w, l, h, yaw = bbox[3], bbox[4], bbox[5], bbox[6]\n            if box_type_3d == 'Depth':\n                box_pts = box_np_ops.depth_to_corners_3d(bbox[0:7])\n            elif box_type_3d == 'LiDAR':\n                box_pts = box_np_ops.rotation_2d(bbox[0:7][-1],\n                                                bbox[0:7][0:3])\n            else:\n                raise ValueError('error')\n            if score < score_thresh:\n                continue\n            pred[class_name][img_id].append((box_pts, score))\n\n        # parse gt annotations\n        gt_anno = gt_annos[img_id]\n        if len(gt_anno['name']) == 0:\n            continue\n        for i in range(len(gt_anno['name'])):\n            class_name = gt_anno['name'][i]\n            bbox = gt_anno['bbox'][i]"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(dt_annos) == len(gt_annos)\n    if box_type_3d == 'Depth' or box_type_3d == 'Camera':\n        gt_annos = indoor_eval.convert_valid_bboxes_3d_to_z_bbox(gt_annos)\n        dt_annos = indoor_eval.convert_valid_bboxes_3d_to_z_bbox(dt_annos)\n    if box_mode_3d != 'lidar':\n        gt_annos = indoor_eval.box_mode_3d_to_lidar(gt_annos)\n        gt_annos = indoor_eval.box_mode_3d_to_lidar(gt_annos)\n    if box_type_3d == 'Depth':\n        gt_annos = depth_box3d_to_camera_box(gt_annos)\n        dt_annos = depth_box3d_to_camera_box(dt_annos)\n        gt_annos = camera_to_lidar(gt_annos)\n        dt_annos = camera_to_lidar(dt_annos)\n    for i in range(len(gt_annos)):\n        indoor_eval.check_bbox_type(gt_annos[i], 'gt')\n        indoor_eval.check_bbox_type(dt_annos[i], 'det')\n    if logger is None:\n        logger = logging.getLogger(__name__)\n    if not isinstance(metric, list):\n        metric = [metric]\n    allowed_metrics = ['mAP', 'mAR']\n    for i in range(len(metric)):\n        if metric[i] not in allowed_metrics:\n            raise KeyError('metric {} not supported'.format(metric[i]))\n\n    class_to_eval = {}\n    for i in range(len(gt_annos)):\n        if gt_annos[i]['name'] not in class_to_eval.keys():\n            class"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(dt_annos) == len(gt_annos)\n    pred = {}\n    gt = {}\n    for label in range(len(label2cat)):\n        pred[label] = []\n        gt[label] = []\n    for img_id in range(len(dt_annos)):\n        for i in range(len(dt_annos[img_id]['name'])):\n            label_str = dt_annos[img_id]['name'][i]\n            bbox = dt_annos[img_id]['bbox'][i]\n            score = dt_annos[img_id]['score'][i]\n            label = label2cat[label_str]\n            bbox[2] -= bbox[0]\n            bbox[3] -= bbox[1]\n            pred[label].append([bbox, score])\n        for i in range(len(gt_annos[img_id]['name'])):\n            label_str = gt_annos[img_id]['name'][i]\n            bbox = gt_annos[img_id]['bbox'][i]\n            label = label2cat[label_str]\n            bbox[2] -= bbox[0]\n            bbox[3] -= bbox[1]\n            gt[label].append(bbox)\n\n    rec, prec, ap = eval_map_recall(pred, gt, metric)\n    ret_dict = dict()\n    header = ['classes']\n    for i, thresh in enumerate(metric):\n        header.append('mAP_%s' % str(thresh))\n    table_data = [header]\n    for i, label in enumerate(label2cat):\n        row_data = [label]\n        for j in range(len(metric)):\n            row_data.append(ap[j][label])\n        table_data.append(row_data)\n    table = AsciiTable(table_data)\n    table.inner_footing_row_border ="}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(dt_annos) == len(gt_annos)\n    pred = {}  # map {classname: pred}\n    gt = {}  # map {classname: gt}\n    pred_no_match = {}  # map {classname: pred}\n    gt_no_match = {}  # map {classname: gt}\n    for img_id in range(len(dt_annos)):\n        # parse detected annotations\n        det_anno = dt_annos[img_id]\n        for i in range(len(det_anno['name'])):\n            class_name = label2cat[det_anno['name'][i]]\n            if class_name in pred:\n                pred[class_name].append([det_anno['bbox'][i], det_anno['score'][i]])\n            else:\n                pred[class_name] = [[det_anno['bbox'][i], det_anno['score'][i]]]\n\n        # parse gt annotations\n        anno = gt_annos[img_id]\n        if anno['name'] is not None:\n            for i in range(len(anno['name'])):\n                class_name = label2cat[anno['name'][i]]\n                if class_name in gt:\n                    gt[class_name].append(anno['bbox'][i])\n                else:\n                    gt[class_name] = [anno['bbox'][i]]\n\n    rec, prec, ap = eval_map_recall(pred, gt, metric)\n    ret_dict = dict()\n    header = ['classes']\n    for i, thr in enumerate(metric):\n        header.append('mAP_{:.2f}'.format(thr))\n        header.append('mAR_{:.2f}'.format(thr))\n    table_data = [header]\n    for i, (cls, rec_i, prec_i, ap_i) in enumerate(\n            zip(label2cat.values(), rec, prec, ap"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(dt_annos) == len(gt_annos)\n    if box_type_3d == 'Depth' or box_type_3d == 'BEV':\n        gt_annos = indore_eval.bbox3d2depthbbox(gt_annos)\n        dt_annos = indore_eval.bbox3d2depthbbox(dt_annos)\n    elif box_type_3d == 'Camera' or box_type_3d == 'LIDAR':\n        gt_annos = indore_eval.bbox3d2camera(gt_annos)\n        dt_annos = indore_eval.bbox3d2camera(dt_annos)\n    else:\n        raise NotImplementedError(\n            f'Unrecognized box_type_3d: {box_type_3d}. Only Depth and BEV are supported.'\n        )\n\n    pred = {}  # map {class_id: pred}\n    gt = {}  # map {class_id: gt}\n    for i in range(len(gt_annos)):\n        # filter out ignored gts\n        gt_inds = np.where(gt_annos[i]['ignore'] == False)[0]\n        # filter out dontcare gts\n        gt_classes = gt_annos[i]['class'][gt_inds]\n        gt_bboxes = gt_annos[i]['bbox'][gt_inds]\n        gt_names = gt_annos[i]['name']\n        difficultys = gt_annos[i]['difficulty']\n        for j in range(len(gt_annos[i]['name'])):\n            if gt_classes[j] in gt:\n                gt[gt_classes[j]].append(\n                    (gt_bboxes[j], gt_names[j], difficultys[j]))\n            else:\n                gt[gt_classes[j]] = [(gt_bboxes[j], gt_"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(dt_annos) == len(gt_annos)\n    if box_type_3d == 'Depth':\n        gt_annos = ind_eval_utils.depth_2_annos(gt_annos)\n        dt_annos = ind_eval_utils.depth_2_annos(dt_annos)\n    if box_type_3d == 'LiDAR':\n        gt_annos = ind_eval_utils.lidar_2_annos(gt_annos)\n        dt_annos = ind_eval_utils.lidar_2_annos(dt_annos)\n    else:\n        raise NotImplementedError\n\n    if box_mode_3d != 'wlh':\n        raise NotImplementedError\n\n    pred = {}  # map {class_id: pred}\n    gt = {}  # map {class_id: gt}\n    for i in range(len(gt_annos)):\n        # filter out radar annotations for evaluation\n        gt_inds = ind_eval_utils.filter_gt_annos_class(gt_annos[i], used_classes)\n        gt_annos[i] = gt_annos[i][gt_inds]\n        for j in range(len(gt_annos[i])):\n            if gt_annos[i][j]['name'] not in gt:\n                gt[gt_annos[i][j]['name']] = {}\n            if gt_annos[i][j]['name'] not in pred:\n                pred[gt_annos[i][j]['name']] = {}\n            gt[gt_annos[i][j]['name']][i] = gt_annos[i][j]\n            box_idx = gt_annos[i][j]['box3d_lidar'].tensor[:, 7]\n            score = np.ones((gt_annos[i][j]['box3d_lidar'].tensor.shape[0]),\n                            d"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(dt_annos) == len(gt_annos)\n    if box_type_3d == 'Depth':\n        gt_annos = ind_eval_utils.depth_2_annos(gt_annos)\n        dt_annos = ind_eval_utils.depth_2_annos(dt_annos)\n\n    if box_type_3d == 'Camera':\n        gt_annos = ind_eval_utils.camera_to_lidar_box(gt_annos)\n        dt_annos = ind_eval_utils.camera_to_lidar_box(dt_annos)\n    else:\n        if box_mode_3d != 'wlh_camera':\n            gt_annos = ind_eval_utils.box3d_kitti_camera_to_lidar(gt_annos)\n            dt_annos = ind_eval_utils.box3d_kitti_camera_to_lidar(dt_annos)\n\n    if box_type_3d == 'Depth' or box_type_3d == 'Camera':\n        gt_annos = ind_eval_utils.filter_gt_box_outside_range(gt_annos, 0, 0, 70.4, 40, 0, 70.4)\n        dt_annos = ind_eval_utils.filter_gt_box_outside_range(dt_annos, 0, 0, 70.4, 40, 0, 70.4)\n    else:\n        gt_annos = ind_eval_utils.filter_gt_box_outside_range(gt_annos, 0, -40, 70.4, 40, -1, 3)\n        dt_annos = ind_eval_utils.filter_gt_box_outside_range(dt_annos, 0, -40, 70.4, 40, -1, 3)\n\n    if box_type"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(dt_annos) == len(gt_annos)\n    if box_type_3d is None:\n        box_type_3d, box_mode_3d = get_box_type(gt_annos[0]['gt_boxes_upright_depth'])\n\n    # obtain gt information from each sample\n    class_to_gt = {}\n    for i in range(len(gt_annos)):\n        gt_num = len(gt_annos[i]['name'])\n        if gt_num != 0:\n            gt_boxes_upright_depth = gt_annos[i]['gt_boxes_upright_depth'].tensor.numpy()\n            for j in range(gt_num):\n                gt_class_name = gt_annos[i]['name'][j]\n                if gt_class_name not in class_to_gt.keys():\n                    class_to_gt[gt_class_name] = {}\n                if i not in class_to_gt[gt_class_name].keys():\n                    class_to_gt[gt_class_name][i] = []\n                class_to_gt[gt_class_name][i].append(gt_boxes_upright_depth[j])\n\n    # obtain gt information from each sample\n    class_to_dt = {}\n    for i in range(len(dt_annos)):\n        # convert detection results to box structure\n        box_list = []\n        for j in range(len(dt_annos[i]['name'])):\n            cls_name = dt_annos[i]['name'][j]\n            bbox = dt_annos[i]['bbox'][j]\n            score = dt_annos[i]['score'][j]\n            if label2cat is not None:\n                label = label2cat[cls_name]\n            else:\n                label = cls_name\n            box_list.append([bbox, label, score])\n        box_list = box_type_3d("}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(dt_annos) == len(gt_annos)\n    if box_type_3d == 'Depth' or box_type_3d == 'BEV':\n        gt_annos = indoor_eval_box_type_3d(gt_annos, box_type_3d, box_mode_3d)\n        dt_annos = indoor_eval_box_type_3d(dt_annos, box_type_3d, box_mode_3d)\n    else:\n        gt_annos = indoor_eval_box_type_2d(gt_annos)\n        dt_annos = indoor_eval_box_type_2d(dt_annos)\n\n    pred = {}\n    gt = {}\n    for label in label2cat.keys():\n        pred[label] = []\n        gt[label] = []\n\n    pred['name'] = []\n    pred['truncated'] = []\n    pred['occluded'] = []\n    pred['alpha'] = []\n    pred['bbox'] = []\n    pred['dimensions'] = []\n    pred['location'] = []\n    pred['rotation_y'] = []\n    pred['score'] = []\n\n    gt['name'] = []\n    gt['truncated'] = []\n    gt['occluded'] = []\n    gt['alpha'] = []\n    gt['bbox'] = []\n    gt['dimensions'] = []\n    gt['location'] = []\n    gt['rotation_y'] = []\n    gt['score'] = []\n\n    num_example = len(gt_annos)\n    for i in range(num_example):\n        gt_boxes = gt_annos[i]['gt_boxes_lidar']\n        for j in range(gt_boxes.shape[0]):\n            gt_label = gt_boxes[j, -1]\n            gt_bbox = gt_boxes[j, :-1]\n            gt[label2cat[gt"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(dt_annos) == len(gt_annos)\n    if box_type_3d is None:\n        box_type_3d, box_mode_3d = get_box_type(gt_annos[0])\n    recalls = [\n        np.zeros([len(metric), len(label2cat)], dtype=np.float32)\n        for i in range(len(metric))\n    ]\n    precisions = [\n        np.zeros([len(metric), len(label2cat)], dtype=np.float32)\n        for i in range(len(metric))\n    ]\n    ap_dict = {}\n    for iou_idx, iou_thr in enumerate(metric):\n        for label, cat_id in label2cat.items():\n            recalls[iou_idx][cat_id - 1], precisions[iou_idx][\n                cat_id - 1], ap_dict[label] = indoor_eval_one_cls(\n                    gt_annos, dt_annos, label, iou_thr, box_type_3d,\n                    box_mode_3d)\n    ret_dict = {}\n    for iou_idx, iou_thr in enumerate(metric):\n        for label, ap in ap_dict.items():\n            category = label2cat[label]\n            ret_dict['recall_%s_%s' % (category, str(iou_thr))] = float(\n                recalls[iou_idx][cat_id - 1])\n            ret_dict['precision_%s_%s' % (category, str(iou_thr))] = float(\n                precisions[iou_idx][cat_id - 1])\n    ret_dict['mAR_11'] = np.mean(\n        [ap for catId, ap in ap_dict.items() if catId != -1])\n    ret_dict['mAR_11_c7'] = np.mean(\n        [ap for catId, ap in ap_dict.items() if cat"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(metric) > 0\n    if box_type_3d is None:\n        box_type_3d = LiDARInstance3DBoxes\n    if box_mode_3d is None:\n        box_mode_3d = LiDARInstance3DBoxes.box_mode_3d\n    if logger is None:\n        logger = logging.getLogger(__name__)\n    if not isinstance(metric, list):\n        metric = [metric]\n\n    # parse detected annotations\n    dt_annos = indoor_eval_utils.eval_det_annos(dt_annos, metric, label2cat)\n\n    # parse gt annotations\n    gt_annos = indoor_eval_utils.eval_gt_annos(gt_annos, metric, label2cat)\n\n    # parse mask annotations\n    mask_annos = indoor_eval_utils.eval_mask_annos(gt_annos, dt_annos,\n                                                    metric, label2cat)\n\n    # eval gt\n    rec, prec, ap = eval_map_recall(mask_annos, gt_annos, metric)\n    ret_dict = indoor_eval_utils.eval_recalls(rec, metric, label2cat)\n    ret_dict.update(indoor_eval_utils.eval_precisions(prec, metric, label2cat))\n    ret_dict.update(indoor_eval_utils.eval_ap(ap, metric, label2cat))\n\n    # prepare for logging\n    for i, thresh in enumerate(metric):\n        mAP = ret_dict['%s_mAP' % thresh]\n        recall = {\n            'rec_%s' % label: rec[i][label]\n            for label in label2cat.values()\n        }\n        precision = {\n            'prec_%s' % label: prec[i][label]\n            for label in label2cat.values()\n        }\n        ret_dict.update({'%s_rec' % thresh: recall})\n        ret_dict.update({'%s_prec' %"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(metric) > 0\n    if box_type_3d is None:\n        box_type_3d = 'LiDAR'\n    if box_mode_3d is None:\n        box_mode_3d = 'lidar'\n    from .kitti_utils import get_label_annos\n\n    gt_annos = [get_label_annos(gt, label2cat) for gt in gt_annos]\n    if box_type_3d == 'LiDAR':\n        gt_boxes = []\n        for anno in gt_annos:\n            anno.pop('name', None)\n            gt_boxes.append(anno)\n    else:\n        raise ValueError('Unsupported box_type_3d %s!' % box_type_3d)\n\n    if box_type_3d == 'LiDAR':\n        dt_annos = get_label_annos(dt_annos)\n        dt_boxes = []\n        for anno in dt_annos:\n            anno.pop('name', None)\n            dt_boxes.append(anno)\n    else:\n        raise ValueError('Unsupported box_type_3d %s!' % box_type_3d)\n\n    result_str, result_dict = indoor_eval_core(\n        gt_boxes, dt_boxes, metric, label2cat)\n\n    if logger is None:\n        print(result_str)\n    else:\n        if isinstance(logger, str):\n            print(f'| {logger}')\n        else:\n            logger.info(result_str)\n\n    return result_dict"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(dt_annos) == len(gt_annos)\n    if box_type_3d == 'Depth' or box_type_3d == 'BEV':\n        gt_annos = ind_eval_utils.depth_2_annos(gt_annos)\n        dt_annos = ind_eval_utils.depth_2_annos(dt_annos)\n    else:\n        gt_annos = ind_eval_utils.bbox_2_annos(gt_annos, label2cat)\n        if box_type_3d == 'Camera' or box_type_3d == 'LIDAR':\n            dt_annos = ind_eval_utils.bbox_2_annos(dt_annos, label2cat)\n    if box_type_3d == 'Camera':\n        # TODO: add function in the future\n        raise NotImplementedError\n    else:\n        ret_dict = ind_eval_utils.eval_prepare(gt_annos, dt_annos)\n\n    # eval recall\n    if metric == 'recall':\n        gt_num_dict = {}\n        for label, p in gt_annos.items():\n            gt_num_dict[label] = list(range(len(p)))\n        dt_num_dict = {}\n        for label, p in dt_annos.items():\n            dt_num_dict[label] = list(range(len(p)))\n        for label in gt_num_dict:\n            for iou in metric:\n                ind_eval_utils.eval_recalls(\n                    gt_num_dict[label],\n                    pred_infos=dt_num_dict[label],\n                    current_class=label,\n                    iou_threshold=iou,\n                    ground_truth=gt_annos,\n                    dt_annos=dt_annos)\n\n    # eval map\n    if metric == 'mAP':\n        for iou in metric:\n            if box_type_3d == 'Depth':\n                mean_aps = ind_eval_utils"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(dt_annos) == len(gt_annos)\n    if box_type_3d == 'Depth' or box_type_3d == 'Camera':\n        gt_annos = camera_to_lidar_box(gt_annos, box_mode_3d)\n        dt_annos = camera_to_lidar_box(dt_annos, box_mode_3d)\n    elif box_type_3d != 'LiDAR':\n        raise ValueError('Only support lidar and depth boxes.')\n\n    metric = [0.2, 0.4, 0.5] if metric is None else metric\n    ap_result_str, ap_dict = do_indoor_eval(gt_annos, dt_annos, label2cat,\n                                            metric)\n    ret_dict = {}\n    for i, cls_result in enumerate(ap_result_str):\n        ret_dict.update({metric[i]: float(cls_result)})\n    ret_dict.update({'AR@{}'.format(metric[i]): ap_dict[i] for i in range(len(metric))})\n\n    detail_log = ''\n    for i in range(1, len(ap_result_str)):\n        detail_log += '{}: {}'.format(metric[i - 1], ap_result_str[i]) + '\\n'\n    ret_dict.update({'details': detail_log})\n\n    if logger is None:\n        print_log('\\n' + ap_result_str, logger='root')\n    else:\n        print_log('\\n' + ap_result_str, logger=logger)\n    return ret_dict"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(dt_annos) == len(gt_annos)\n    if box_type_3d == 'Depth' or box_type_3d == 'BEV':\n        gt_annos = camera_to_lidar_box(gt_annos, box_mode_3d)\n        dt_annos = camera_to_lidar_box(dt_annos, box_mode_3d)\n    else:\n        gt_annos = camera_to_lidar_box(gt_annos, box_mode_3d)\n    num_example = len(gt_annos)\n    split_parts = split_parts = [(0, num_example)]\n    split_parts = split_parts\n    num_parts = len(split_parts)\n    all_dt_annos = {}\n    all_gt_annos = {}\n    total_dt_num = {}\n    total_gt_num = {}\n    total_dt_num_cum = {}\n    total_gt_num_cum = {}\n    for k in metric:\n        total_dt_num[k] = [0 for _ in range(num_parts)]\n        total_gt_num[k] = [0 for _ in range(num_parts)]\n        total_dt_num_cum[k] = [0 for _ in range(num_parts)]\n        total_gt_num_cum[k] = [0 for _ in range(num_parts)]\n    for part_id, (start_idx, end_idx) in enumerate(split_parts):\n        dt_annos_part = get_part_results(dt_annos, result_path, start_idx,\n                                         end_idx)\n        gt_annos_part = gt_annos[start_idx:end_idx]\n        for current_class in range(len(label2cat)):\n            result_str, ret_dict = indoor_eval_class(gt_annos_part,\n                                                     dt_annos_part,\n                                                     current_class, label2cat,\n                                                     metric)\n           "}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(dt_annos) == len(gt_annos)\n    if box_type_3d == 'Depth':\n        gt_annos = eval_det_3d(gt_annos, metric, label2cat)\n        dt_annos = eval_det_3d(dt_annos, metric, label2cat)\n    else:\n        gt_annos = eval_det_2d(gt_annos, metric, label2cat)\n        dt_annos = eval_det_2d(dt_annos, metric, label2cat)\n    rec, prec, ap = eval_map_recall(dt_annos, gt_annos, metric)\n    ret_dict = dict()\n    header_map = ['class', 'gts', 'dets', 'recall', 'ap', 'precision']\n    table_rows = [header_map]\n    for i, (l, tp, fp, rec, prec, ap) in enumerate(\n            zip(label2cat.values(), gt_annos['num_pos'],\n                dt_annos['num_pos'], rec, prec, ap)):\n        row_map = [l, tp, fp, rec[-1], ap[-1], prec[-1]]\n        table_rows.append(row_map)\n        ret_dict[l] = ap[-1]\n    ret_dict.update({'AR@100': np.mean(rec[-1])})\n    ret_dict.update({'AR@1000': np.mean(rec[20])})\n    ret_dict.update({'AR_11': np.mean(rec)})\n    ret_dict.update({'mAP': np.mean(ap)})\n    table_data = [table_rows]\n    table = AsciiTable(table_data)\n    table.inner_footing_row_border = True\n    if logger is None:\n        print_log('\\n' + table.table)\n    else:\n        logger.info('\\n' + table.table)\n    return"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    from . import (Box3DMode, CameraInstance3DBoxes, DepthInstance3DBoxes,\n                   LiDARInstance3DBoxes)\n    box_type_lower = box_type.lower()\n    if box_type_lower == 'lidar':\n        box_type_3d = LiDARInstance3DBoxes\n        box_mode_3d = Box3DMode.LIDAR\n    elif box_type_lower == 'camera':\n        box_type_3d = CameraInstance3DBoxes\n        box_mode_3d = Box3DMode.CAM\n    elif box_type_lower == 'depth':\n        box_type_3d = DepthInstance3DBoxes\n        box_mode_3d = Box3DMode.DEPTH\n    else:\n        raise ValueError('Only \"box_type\" of \"camera\", \"lidar\", \"depth\" are supported for visualization!'\n                         f'Got unsupported {box_type}')\n\n    return box_type_3d, box_mode_3d"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    from . import (Box3DMode, CameraInstance3DBoxes, DepthInstance3DBoxes,\n                   LiDARInstance3DBoxes)\n    box_type_lower = box_type.lower()\n    if box_type_lower == 'lidar':\n        box_type_3d = LiDARInstance3DBoxes\n        box_mode_3d = Box3DMode.LIDAR\n    elif box_type_lower == 'camera':\n        box_type_3d = CameraInstance3DBoxes\n        box_mode_3d = Box3DMode.CAM\n    elif box_type_lower == 'depth':\n        box_type_3d = DepthInstance3DBoxes\n        box_mode_3d = Box3DMode.DEPTH\n    else:\n        raise ValueError('Only \"box_type\" of \"camera\", \"lidar\", \"depth\" are supported for visualization!'\n                         f'Got unsupported {box_type}')\n\n    return box_type_3d, box_mode_3d"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    from . import (Box3DMode, CameraInstance3DBoxes, DepthInstance3DBoxes,\n                   LiDARInstance3DBoxes)\n    box_type_lower = box_type.lower()\n    if box_type_lower == 'lidar':\n        box_type_3d = LiDARInstance3DBoxes\n        box_mode_3d = Box3DMode.LIDAR\n    elif box_type_lower == 'camera':\n        box_type_3d = CameraInstance3DBoxes\n        box_mode_3d = Box3DMode.CAM\n    elif box_type_lower == 'depth':\n        box_type_3d = DepthInstance3DBoxes\n        box_mode_3d = Box3DMode.DEPTH\n    else:\n        raise ValueError('Only \"box_type\" of \"camera\", \"lidar\", \"depth\" are supported for visualization!'\n                         f'Got unsupported {box_type}')\n\n    return box_type_3d, box_mode_3d"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    from . import (Box3DMode, CameraInstance3DBoxes, DepthInstance3DBoxes,\n                   LiDARInstance3DBoxes)\n    box_type_lower = box_type.lower()\n    if box_type_lower == 'lidar':\n        box_type_3d = LiDARInstance3DBoxes\n        box_mode_3d = Box3DMode.LIDAR\n    elif box_type_lower == 'camera':\n        box_type_3d = CameraInstance3DBoxes\n        box_mode_3d = Box3DMode.CAM\n    elif box_type_lower == 'depth':\n        box_type_3d = DepthInstance3DBoxes\n        box_mode_3d = Box3DMode.DEPTH\n    else:\n        raise ValueError('Only \"box_type\" of \"camera\", \"lidar\", \"depth\" are '\n                         f'supported, got {box_type}')\n\n    return box_type_3d, box_mode_3d"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    from .box_3d_mode import (Box3DMode, CameraInstance3DBoxes, DepthInstance3DBoxes,\n                             LiDARInstance3DBoxes)\n    box_type_lower = box_type.lower()\n    if box_type_lower == 'lidar':\n        box_type_3d = LiDARInstance3DBoxes\n        box_mode_3d = Box3DMode.LIDAR\n    elif box_type_lower == 'camera':\n        box_type_3d = CameraInstance3DBoxes\n        box_mode_3d = Box3DMode.CAM\n    elif box_type_lower == 'depth':\n        box_type_3d = DepthInstance3DBoxes\n        box_mode_3d = Box3DMode.DEPTH\n    else:\n        raise ValueError('Only \"box_type\" of \"camera\", \"lidar\", \"depth\" are supported for visualization!'\n                         f'Got unsupported {box_type}')\n\n    return box_type_3d, box_mode_3d"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    from . import (Box3DMode, CameraInstance3DBoxes, DepthInstance3DBoxes,\n                   LiDARInstance3DBoxes)\n    box_type_lower = box_type.lower()\n    if box_type_lower == 'lidar':\n        box_type_3d = LiDARInstance3DBoxes\n        box_mode_3d = Box3DMode.LIDAR\n    elif box_type_lower == 'camera':\n        box_type_3d = CameraInstance3DBoxes\n        box_mode_3d = Box3DMode.CAM\n    elif box_type_lower == 'depth':\n        box_type_3d = DepthInstance3DBoxes\n        box_mode_3d = Box3DMode.DEPTH\n    else:\n        raise ValueError(\n            f'Only \"box_type\" of \"camera\", \"lidar\", \"depth\" are allowed, '\n            f'got {box_type}')\n\n    return box_type_3d, box_mode_3d"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    from . import (Box3DMode, CameraInstance3DBoxes, DepthInstance3DBoxes,\n                   LiDARInstance3DBoxes)\n    box_type_lower = box_type.lower()\n    if box_type_lower == 'lidar':\n        box_type_3d = LiDARInstance3DBoxes\n        box_mode_3d = Box3DMode.LIDAR\n    elif box_type_lower == 'camera':\n        box_type_3d = CameraInstance3DBoxes\n        box_mode_3d = Box3DMode.CAM\n    elif box_type_lower == 'depth':\n        box_type_3d = DepthInstance3DBoxes\n        box_mode_3d = Box3DMode.DEPTH\n    else:\n        raise ValueError(\n            f'Only \"box_type\" of \"camera\"/\"lidar\"/\"depth\" is supported but got {box_type}'\n        )\n\n    return box_type_3d, box_mode_3d"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    from mmdet3d.core.bbox import (Box3DMode, CameraInstance3DBoxes, DepthInstance3DBoxes, LiDARInstance3DBoxes)\n\n    box_type_lower = box_type.lower()\n    if box_type_lower == 'lidar':\n        box_type_3d = LiDARInstance3DBoxes\n        box_mode_3d = Box3DMode.LIDAR\n    elif box_type_lower == 'camera':\n        box_type_3d = CameraInstance3DBoxes\n        box_mode_3d = Box3DMode.CAM\n    elif box_type_lower == 'depth':\n        box_type_3d = DepthInstance3DBoxes\n        box_mode_3d = Box3DMode.DEPTH\n    else:\n        raise ValueError('Only \"box_type\" of \"camera\", \"lidar\", \"depth\" are supported for visualization!'\n                         f'Got unsupported {box_type}')\n\n    return box_type_3d, box_mode_3d"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    from mmdet3d.core.bbox import (Box3DMode, CameraInstance3DBoxes,\n                                  DepthInstance3DBoxes, LiDARInstance3DBoxes)\n    box_type_lower = box_type.lower()\n    if box_type_lower == 'lidar':\n        box_type_3d = LiDARInstance3DBoxes\n        box_mode_3d = Box3DMode.LIDAR\n    elif box_type_lower == 'camera':\n        box_type_3d = CameraInstance3DBoxes\n        box_mode_3d = Box3DMode.CAM\n    elif box_type_lower == 'depth':\n        box_type_3d = DepthInstance3DBoxes\n        box_mode_3d = Box3DMode.DEPTH\n    else:\n        raise ValueError('Only \"box_type\" of \"camera\", \"lidar\", \"depth\" are supported for visualization!'\n                         f'Got unsupported {box_type}')\n\n    return box_type_3d, box_mode_3d"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    from .box_3d_mode import (Box3DMode, CameraInstance3DBoxes,\n                             DepthInstance3DBoxes, LiDARInstance3DBoxes)\n    box_type_lower = box_type.lower()\n    if box_type_lower == 'lidar':\n        box_type_3d = LiDARInstance3DBoxes\n        box_mode_3d = Box3DMode.LIDAR\n    elif box_type_lower == 'camera':\n        box_type_3d = CameraInstance3DBoxes\n        box_mode_3d = Box3DMode.CAM\n    elif box_type_lower == 'depth':\n        box_type_3d = DepthInstance3DBoxes\n        box_mode_3d = Box3DMode.DEPTH\n    else:\n        raise ValueError('Only \"box_type\" of \"camera\", \"lidar\", \"depth\" are supported for visualization!'\n                         f'Got unsupported {box_type}')\n\n    return box_type_3d, box_mode_3d"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    from .box_3d_mode import (Box3DMode, CameraInstance3DBoxes, DepthInstance3DBoxes,\n                             LiDARInstance3DBoxes)\n    box_type_lower = box_type.lower()\n    if box_type_lower == 'lidar':\n        box_type_3d = LiDARInstance3DBoxes\n        box_mode_3d = Box3DMode.LIDAR\n    elif box_type_lower == 'camera':\n        box_type_3d = CameraInstance3DBoxes\n        box_mode_3d = Box3DMode.CAM\n    elif box_type_lower == 'depth':\n        box_type_3d = DepthInstance3DBoxes\n        box_mode_3d = Box3DMode.DEPTH\n    else:\n        raise ValueError(\n            f'Only \"box_type\" of \"camera\", \"lidar\", \"depth\" are allowed, '\n            f'got {box_type}')\n\n    return box_type_3d, box_mode_3d"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    from .box_3d_mode import Box3DMode\n    from .cam_box3d import CameraInstance3DBoxes\n    from .depth_box3d import DepthInstance3DBoxes\n    from .lidar_box3d import LiDARInstance3DBoxes\n\n    box_type_lower = box_type.lower()\n    if box_type_lower == 'lidar':\n        box_type_3d = LiDARInstance3DBoxes\n        box_mode_3d = Box3DMode.LIDAR\n    elif box_type_lower == 'camera':\n        box_type_3d = CameraInstance3DBoxes\n        box_mode_3d = Box3DMode.CAM\n    elif box_type_lower == 'depth':\n        box_type_3d = DepthInstance3DBoxes\n        box_mode_3d = Box3DMode.DEPTH\n    else:\n        raise ValueError(\n            f'Only \"box_type\" of \"camera\"/\"lidar\"/\"depth\" is supported but got {box_type}'\n        )\n\n    return box_type_3d, box_mode_3d"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type in ['LiDAR', 'LIDAR', 'lidar', 'Lidar']:\n        box_type_3d = LiDARInstance3DBoxes\n        box_mode_3d = Box3DMode.LIDAR\n    elif box_type in ['Camera', 'CAMERA', 'camera']:\n        box_type_3d = CameraInstance3DBoxes\n        box_mode_3d = Box3DMode.CAM\n    elif box_type in ['Depth', 'DEPTH', 'depth']:\n        box_type_3d = DepthInstance3DBoxes\n        box_mode_3d = Box3DMode.DEPTH\n    else:\n        raise ValueError(\n            f'Only \"LiDAR\", \"Camera\", \"Depth\" are supported for box_type, '\n            f'but got {box_type}')\n\n    return box_type_3d, box_mode_3d"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    from .box_3d_mode import Box3DMode\n    from .cam_box3d import CameraInstance3DBoxes\n    from .depth_box3d import DepthInstance3DBoxes\n    from .lidar_box3d import LiDARInstance3DBoxes\n\n    box_type_lower = box_type.lower()\n    if box_type_lower == 'lidar':\n        box_type_3d = LiDARInstance3DBoxes\n        box_mode_3d = Box3DMode.LIDAR\n    elif box_type_lower == 'camera':\n        box_type_3d = CameraInstance3DBoxes\n        box_mode_3d = Box3DMode.CAM\n    elif box_type_lower == 'depth':\n        box_type_3d = DepthInstance3DBoxes\n        box_mode_3d = Box3DMode.DEPTH\n    else:\n        raise ValueError('Only \"box_type\" of \"camera\", \"lidar\", \"depth\"'\n                         f' are supported, got {box_type}')\n\n    return box_type_3d, box_mode_3d"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    from .box_3d_mode import (Box3DMode, CameraInstance3DBoxes,\n                             DepthInstance3DBoxes, LiDARInstance3DBoxes)\n\n    box_type_lower = box_type.lower()\n    if box_type_lower == 'lidar':\n        box_type_3d = LiDARInstance3DBoxes\n        box_mode_3d = Box3DMode.LIDAR\n    elif box_type_lower == 'camera':\n        box_type_3d = CameraInstance3DBoxes\n        box_mode_3d = Box3DMode.CAM\n    elif box_type_lower == 'depth':\n        box_type_3d = DepthInstance3DBoxes\n        box_mode_3d = Box3DMode.DEPTH\n    else:\n        raise ValueError('Only \"box_type\" of \"camera\", \"lidar\", \"depth\" are supported for visualization!'\n                         f'Got unsupported {box_type}')\n\n    return box_type_3d, box_mode_3d"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type in ['LiDAR', 'LIDAR', 'lidar', 'Lidar']:\n        box_type_3d = LiDARInstance3DBoxes\n        box_mode_3d = Box3DMode.LIDAR\n    elif box_type in ['Camera', 'CAMERA', 'camera']:\n        box_type_3d = CameraInstance3DBoxes\n        box_mode_3d = Box3DMode.CAM\n    elif box_type in ['Depth', 'DEPTH', 'depth']:\n        box_type_3d = DepthInstance3DBoxes\n        box_mode_3d = Box3DMode.DEPTH\n    else:\n        raise ValueError(\n            f'Unsupported box_type {box_type}. Supported box_type are:'\n            \"['LiDAR', 'Camera', 'Depth']\")\n\n    return box_type_3d, box_mode_3d"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type.lower() == 'lidar':\n        box_type_3d = LiDARInstance3DBoxes\n        box_mode_3d = Box3DMode.LIDAR\n    elif box_type.lower() == 'camera':\n        box_type_3d = CameraInstance3DBoxes\n        box_mode_3d = Box3DMode.CAM\n    elif box_type.lower() == 'depth':\n        box_type_3d = DepthInstance3DBoxes\n        box_mode_3d = Box3DMode.DEPTH\n    else:\n        raise ValueError(\n            f'Only \"box_type\" of \"camera\"/\"lidar\"/\"depth\" is supported for visualization, '\n            f'but got unsupported {box_type}')\n\n    return box_type_3d, box_mode_3d\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type in ['LiDAR', 'LIDAR', 'lidar', 'Lidar']:\n        box_type_3d = LiDARInstance3DBoxes\n        box_mode_3d = Box3DMode.LIDAR\n    elif box_type in ['Camera', 'CAM', 'cam', 'CAMERA']:\n        box_type_3d = CameraInstance3DBoxes\n        box_mode_3d = Box3DMode.CAM\n    elif box_type in ['Depth', 'DEPTH', 'Depth']:\n        box_type_3d = DepthInstance3DBoxes\n        box_mode_3d = Box3DMode.DEPTH\n    else:\n        raise ValueError(\n            f'Only \"LiDAR\", \"Camera\", \"Depth\" are supported for visualization,'\n            f' but got {box_type}')\n\n    return box_type_3d, box_mode_3d\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    from . import (Box3DMode, CameraInstance3DBoxes, DepthInstance3DBoxes,\n                   LiDARInstance3DBoxes)\n    box_type_lower = box_type.lower()\n    if box_type_lower == 'lidar':\n        box_cls = LiDARInstance3DBoxes\n        mode = Box3DMode.LIDAR\n    elif box_type_lower == 'camera':\n        box_cls = CameraInstance3DBoxes\n        mode = Box3DMode.CAM\n    elif box_type_lower == 'depth':\n        box_cls = DepthInstance3DBoxes\n        mode = Box3DMode.DEPTH\n    else:\n        raise ValueError('Only \"box_type\" of \"camera\", \"lidar\", \"depth\" are '\n                         f'supported, got {box_type}')\n\n    return box_cls, mode\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    # Convert the input string to lowercase to ensure case-insensitive matching\n    box_type = box_type.lower()\n\n    # Define a dictionary that maps the input string to the corresponding 3D box class and mode\n    type_mapping = {\n        'lidar': (LiDARInstance3DBoxes, 'lidar'),\n        'camera': (CameraInstance3DBoxes, 'camera'),\n        'depth': (DepthInstance3DBoxes, 'lidar'),\n    }\n\n    # Check if the input string is in the dictionary\n    if box_type not in type_mapping:\n        # If not, raise a ValueError with a message indicating that the input string is not recognized\n        raise ValueError(f'Unknown box type {box_type}')\n\n    # If the input string is recognized, return the corresponding 3D box class and mode\n    return type_mapping[box_type]\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n  "}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n  "}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n  "}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n\n  "}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    async with aiofiles.open(path, 'rb') as r:\n      while True:\n        chunk = await r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      async with aiofiles.open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n  "}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    async with aiofiles.open(path, 'rb') as r:\n      while True:\n        chunk = await r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      async with aiofiles.open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    async with open(path, 'rb') as r:\n      while True:\n        chunk = await r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      async with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine user code and test code\n        code = user_code + test_code\n\n        # Create a temporary file to store the code\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(code)\n            f.flush()\n\n        # Run Pyright on the temporary file\n        result = subprocess.run(\n            [\"pyright\", f.name, \"--outputjson\"], capture_output=True, text=True\n        )\n\n        # Delete the temporary file\n        Path(f.name).unlink()\n\n        # Parse Pyright output\n        messages = cls._parse_pyright_output(result.stdout)\n\n        # Identify lines with expected type errors\n        expected_errors = cls._identify_expected_errors(test_code)\n\n        # Check if the type check passed\n        passed = cls._check_type_check_passed(messages, expected_errors)\n\n        # Prepare the result message\n        message = cls._prepare_result_message(passed, messages, expected_errors)\n\n        # Return the result\n        return TypeCheckResult(message=message, passed=passed)\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine user code and test code\n        code = user_code + test_code\n\n        # Write code to a temporary file\n        with tempfile.NamedTemporaryFile(\"w\", suffix=\".py\", delete=False) as f:\n            f.write(code)\n            filename = f.name\n\n        # Run Pyright on the temporary file\n        result = subprocess.run(\n            [\"pyright\", filename, \"--outputjson\"],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            universal_newlines=True,\n        )\n\n        # Read the output from Pyright\n        output = result.stdout\n\n        # Parse the output from Pyright\n        errors = cls._parse_pyright_output(output)\n\n        # Determine if the type check passed or failed\n        passed = cls._check_type_check_passed(errors)\n\n        # Generate a message detailing the outcome of the type check\n        message = cls._generate_type_check_message(errors, passed)\n\n        # Delete the temporary file\n        Path(filename).unlink()\n\n        # Return the result of the type check\n        return TypeCheckResult(message=message, passed=passed, debug_info=errors)\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine user code and test code\n        combined_code = user_code + test_code\n\n        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(suffix=\".py\", delete=False) as temp_file:\n            temp_file.write(combined_code.encode(\"utf-8\"))\n            temp_file_path = temp_file.name\n\n        # Run Pyright on the temporary file\n        try:\n            pyright_output = subprocess.check_output(\n                [\"pyright\", temp_file_path], stderr=subprocess.STDOUT, text=True\n            )\n        except subprocess.CalledProcessError as e:\n            pyright_output = e.output\n\n        # Parse Pyright output to extract error messages and line numbers\n        error_messages = []\n        expected_error_lines = set()\n        for line in pyright_output.splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_no, message = match.groups()\n                if cls.EXPECT_ERROR_COMMENT in message:\n                    expected_error_lines.add(int(line_no))\n                else:\n                    error_messages.append(message)\n\n        # Check if the type check passed or failed\n        passed = not error_messages\n        if not passed:\n            # If the type check failed, check if any expected errors were found\n            unexpected_error_lines = set(\n                int(line_no) for line_no, _ in tokenize.generate_tokens(io.StringIO(user_code).readline)\n            ) - expected_error_lines\n            if unexpected_error_lines:\n                message = (\n                    \"Type check failed. \"\n                    \"Unexpected errors found on lines: \"\n                    f\"{', '.join(str(line_no) for line_no in unexpected_error_lines)}\"\n                )\n            else:\n                message = \"Type check failed. \" + \" \".join(error_messages)\n        else:\n            message = \"Type"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine user code and test code\n        code = user_code + test_code\n\n        # Write code to a temporary file\n        with tempfile.NamedTemporaryFile(suffix=\".py\", delete=False) as f:\n            f.write(code.encode(\"utf-8\"))\n            f.close()\n\n        # Run Pyright on the temporary file\n        result = subprocess.run(\n            [\"pyright\", f.name], capture_output=True, text=True\n        )\n\n        # Remove the temporary file\n        Path(f.name).unlink()\n\n        # Parse Pyright output\n        lines = result.stdout.splitlines()\n        error_lines = []\n        for line in lines:\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_no, message = match.groups()\n                error_lines.append((int(line_no), message))\n\n        # Sort error lines by line number\n        error_lines.sort(key=lambda x: x[0])\n\n        # Identify expected type errors\n        expected_errors = []\n        for token in tokenize.generate_tokens(io.StringIO(test_code).readline):\n            if token.type == tokenize.COMMENT and token.string.strip() == cls.EXPECT_ERROR_COMMENT:\n                expected_errors.append(token.start[0])\n\n        # Check if expected type errors match actual errors\n        passed = True\n        if len(expected_errors) != len(error_lines):\n            passed = False\n        else:\n            for i, (line_no, message) in enumerate(error_lines):\n                if line_no != expected_errors[i]:\n                    passed = False\n                    break\n\n        # Construct result message\n        if passed:\n            message = \"Type check passed!\"\n        else:\n            message = \"Type check failed. Expected errors at lines: \" + \", \".join(\n                map(str, expected_errors)\n            )\n            if error_lines:\n                message += \"\\nActual errors"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine user code and test code\n        code = user_code + test_code\n\n        # Create a temporary file to store the code\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(code)\n            f.flush()\n\n        # Run Pyright on the temporary file\n        process = subprocess.run(\n            [\"pyright\", f.name, \"--outputjson\"],\n            capture_output=True,\n            text=True,\n        )\n\n        # Delete the temporary file\n        Path(f.name).unlink()\n\n        # Check if Pyright returned an error\n        if process.returncode != 0:\n            return TypeCheckResult(\n                message=\"Pyright returned an error\",\n                passed=False,\n                debug_info={\n                    \"stdout\": process.stdout,\n                    \"stderr\": process.stderr,\n                },\n            )\n\n        # Parse Pyright output\n        try:\n            pyright_output = json.loads(process.stdout)\n        except json.JSONDecodeError:\n            return TypeCheckResult(\n                message=\"Failed to parse Pyright output\",\n                passed=False,\n                debug_info={\n                    \"stdout\": process.stdout,\n                    \"stderr\": process.stderr,\n                },\n            )\n\n        # Check if Pyright returned an error\n        if pyright_output.get(\"generalDiagnostics\"):\n            return TypeCheckResult(\n                message=\"Pyright returned an error\",\n                passed=False,\n                debug_info={\n                    \"stdout\": process.stdout,\n                    \"stderr\": process.stderr,\n                },\n            )\n\n        # Extract expected type error lines from test code\n        expected_type_error_lines = set()\n        for line in test_code.splitlines():\n            if line.strip().startswith(cls.EXPECT_ERROR_COMMENT):\n                line_number = int(line.strip().split(\":\")[1])\n                expected_type_error_lines."}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create temporary files for user code and test code\n        with tempfile.NamedTemporaryFile(\n            suffix=\".py\", delete=False\n        ) as user_code_file, tempfile.NamedTemporaryFile(\n            suffix=\".py\", delete=False\n        ) as test_code_file:\n            user_code_file.write(user_code.encode())\n            test_code_file.write(test_code.encode())\n\n        # Run Pyright with the temporary files\n        process = subprocess.run(\n            [\n                \"pyright\",\n                user_code_file.name,\n                test_code_file.name,\n                \"--outputjson\",\n                \"--project\",\n                str(ROOT_DIR / \"pyrightconfig.json\"),\n            ],\n            capture_output=True,\n            text=True,\n        )\n\n        # Delete the temporary files\n        user_code_file.close()\n        test_code_file.close()\n        Path(user_code_file.name).unlink()\n        Path(test_code_file.name).unlink()\n\n        # Check if Pyright returned an error\n        if process.returncode != 0:\n            return TypeCheckResult(\n                message=\"Pyright returned an error: \" + process.stderr,\n                passed=False,\n                debug_info={\n                    \"stdout\": process.stdout,\n                    \"stderr\": process.stderr,\n                },\n            )\n\n        # Parse the Pyright output\n        pyright_output = json.loads(process.stdout)\n\n        # Extract the error messages and line numbers from the Pyright output\n        error_messages = []\n        for message in pyright_output[\"generalDiagnostics\"]:\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, message[\"message\"])\n            if match:\n                error_messages.append((int(match.group(1)), match.group(2)))\n\n        # Read the test code file line by line\n        with io.StringIO(test_code) as test_code_file:\n           "}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_dir = Path(temp_dir)\n            temp_file = temp_dir / \"temp.py\"\n            temp_file.write_text(user_code + test_code, encoding=\"utf-8\")\n\n            # Run pyright and capture the output\n            process = subprocess.Popen(\n                [\"pyright\", temp_file],\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                universal_newlines=True,\n            )\n            stdout, stderr = process.communicate()\n\n            # Parse the output to extract error messages and line numbers\n            errors = []\n            for line in stderr.split(\"\\n\"):\n                match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n                if match:\n                    line_no, message = match.groups()\n                    errors.append((int(line_no), message))\n\n            # Sort errors by line number\n            errors.sort()\n\n            # Check if the expected type errors are present\n            expected_errors = cls._get_expected_type_errors(user_code)\n            if len(errors) != len(expected_errors):\n                return TypeCheckResult(\n                    message=\"Wrong number of errors\",\n                    passed=False,\n                    debug_info={\"errors\": errors, \"expected_errors\": expected_errors},\n                )\n\n            for (line_no, message), (expected_line_no, expected_message) in zip(\n                errors, expected_errors\n            ):\n                if line_no != expected_line_no:\n                    return TypeCheckResult(\n                        message=f\"Wrong line number, expected {expected_line_no}, got {line_no}\",\n                        passed=False,\n                        debug_info={\n                            \"errors\": errors,\n                            \"expected_errors\": expected_errors,\n                        },\n                    )\n                if message != expected_message:\n                    return TypeCheckResult(\n                        message=f\"Wrong error message, expected {expected_message}, got {message"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary directory to store the user code and test code\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_dir_path = Path(temp_dir)\n\n            # Write the user code and test code to temporary files\n            user_code_file = temp_dir_path / \"user_code.py\"\n            user_code_file.write_text(user_code)\n            test_code_file = temp_dir_path / \"test_code.py\"\n            test_code_file.write_text(test_code)\n\n            # Run Pyright on the combined code\n            pyright_result = subprocess.run(\n                [\"pyright\", str(user_code_file), str(test_code_file)],\n                capture_output=True,\n                text=True,\n            )\n\n            # Check if Pyright returned an error\n            if pyright_result.returncode != 0:\n                return TypeCheckResult(\n                    message=\"Pyright returned an error.\", passed=False\n                )\n\n            # Parse Pyright's output to extract error messages and line numbers\n            error_messages = []\n            expected_error_lines = set()\n            for line in io.StringIO(pyright_result.stdout):\n                match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n                if match:\n                    line_no = int(match.group(1))\n                    message = match.group(2)\n                    if cls.EXPECT_ERROR_COMMENT in message:\n                        expected_error_lines.add(line_no)\n                    else:\n                        error_messages.append((line_no, message))\n\n            # Sort error messages by line number\n            error_messages.sort(key=lambda x: x[0])\n\n            # Check if any expected errors were not found\n            for line_no in expected_error_lines:\n                if not any(line_no == error_line for error_line, _ in error_messages):\n                    return TypeCheckResult(\n                        message=f\"Expected error on line {line_no} not found.\","}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine user code and test code\n        code = user_code + test_code\n\n        # Write code to a temporary file\n        with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as f:\n            f.write(code)\n            filename = f.name\n\n        # Run Pyright on the temporary file\n        process = subprocess.run(\n            [\"pyright\", filename, \"--outputjson\"],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            text=True,\n        )\n\n        # Parse Pyright output\n        output = process.stdout.strip()\n        lines = output.split(\"\\n\")\n        messages = []\n        line_numbers = []\n        for line in lines:\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_number, message = match.groups()\n                line_numbers.append(int(line_number))\n                messages.append(message)\n\n        # Identify expected type errors\n        expected_type_errors = []\n        for token in tokenize.generate_tokens(io.StringIO(test_code).readline):\n            if token.type == tokenize.COMMENT and token.string.startswith(\n                f\"# {cls.EXPECT_ERROR_COMMENT}\"\n            ):\n                line_number = token.start[0]\n                expected_type_errors.append(line_number)\n\n        # Check if type errors match expected type errors\n        passed = set(line_numbers) == set(expected_type_errors)\n\n        # Construct result message\n        if passed:\n            message = \"Congratulations, you passed the type check!\"\n        else:\n            message = \"Type check failed. \"\n            if not line_numbers:\n                message += \"No type errors found.\"\n            else:\n                message += \"Expected type errors at lines: \" + \", \".join(\n                    str(line) for line in expected_type_errors\n                )\n                message += \". Found type errors at lines: \" + \", \"."}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine user code and test code\n        code = user_code + test_code\n\n        # Write code to a temporary file\n        with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as temp_file:\n            temp_file.write(code)\n            temp_file_path = Path(temp_file.name)\n\n        # Run Pyright on the temporary file\n        try:\n            result = subprocess.run(\n                [\"pyright\", temp_file_path], capture_output=True, text=True\n            )\n        except FileNotFoundError:\n            return TypeCheckResult(\n                message=\"Pyright is not installed, please install it first.\", passed=False\n            )\n        finally:\n            # Delete the temporary file\n            temp_file_path.unlink()\n\n        # Parse Pyright output\n        lines = result.stdout.splitlines()\n        error_messages = []\n        line_numbers = []\n        for line in lines:\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_number = int(match.group(1))\n                error_message = match.group(2)\n                error_messages.append(error_message)\n                line_numbers.append(line_number)\n\n        # Check if the error messages match the expected error messages\n        expected_error_messages = cls._get_expected_error_messages(test_code)\n        if len(error_messages) != len(expected_error_messages):\n            return TypeCheckResult(\n                message=\"The number of error messages does not match the expected number of error messages.\",\n                passed=False,\n            )\n\n        # Check if the line numbers match the expected line numbers\n        expected_line_numbers = cls._get_expected_line_numbers(test_code)\n        if len(line_numbers) != len(expected_line_numbers):\n            return TypeCheckResult(\n                message=\"The number of error lines does not match the expected number of error lines.\",\n                passed=False,\n            )"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary directory for storing the test code and user code\n        with tempfile.TemporaryDirectory() as temp_dir:\n            test_file = Path(temp_dir) / \"test.py\"\n            user_file = Path(temp_dir) / \"user.py\"\n\n            # Write the test code and user code to the temporary files\n            test_file.write_text(test_code, encoding=\"utf-8\")\n            user_file.write_text(user_code, encoding=\"utf-8\")\n\n            # Run Pyright on the temporary files and capture the output\n            try:\n                output = subprocess.check_output(\n                    [\"pyright\", str(test_file), str(user_file)],\n                    stderr=subprocess.STDOUT,\n                    universal_newlines=True,\n                )\n            except subprocess.CalledProcessError as e:\n                output = e.output\n\n            # Process the output to extract error messages and line numbers\n            errors: dict[int, str] = {}\n            for line in output.splitlines():\n                match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n                if match:\n                    line_no, message = match.groups()\n                    errors[int(line_no)] = message\n\n            # Read the test code file to identify lines with expected type errors\n            expected_errors: dict[int, str] = {}\n            with open(test_file, \"r\", encoding=\"utf-8\") as f:\n                for token in tokenize.generate_tokens(f.readline):\n                    if token.type == tokenize.COMMENT and token.string.endswith(\n                        cls.EXPECT_ERROR_COMMENT\n                    ):\n                        line_no = token.start[0]\n                        expected_errors[line_no] = cls.EXPECT_ERROR_COMMENT\n\n            # Compare the expected errors with the actual errors\n            passed = True\n            messages = []\n            for line_no, message in errors.items():\n                if line_no in expected_errors:\n                    # The line has an expected type"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary directory to store the combined code\n        with tempfile.TemporaryDirectory() as temp_dir:\n            temp_dir = Path(temp_dir)\n            temp_file = temp_dir / \"combined_code.py\"\n\n            # Write the combined code to the temporary file\n            temp_file.write_text(user_code + test_code, encoding=\"utf-8\")\n\n            # Run Pyright on the combined code\n            pyright_output = subprocess.run(\n                [\"pyright\", str(temp_file)], capture_output=True, text=True\n            ).stderr\n\n            # Parse the Pyright output to identify lines with expected type errors\n            expected_errors = cls._parse_expected_errors(user_code)\n            expected_error_lines = {\n                line_no for line_no, _ in expected_errors.items()\n            }\n\n            # Parse the Pyright output to identify lines with actual type errors\n            actual_errors = cls._parse_pyright_output(pyright_output)\n            actual_error_lines = {\n                line_no for line_no, _ in actual_errors.items()\n            }\n\n            # Check if the expected and actual type errors match\n            passed = expected_error_lines == actual_error_lines\n\n            # Generate a message detailing the outcome of the type check\n            message = cls._generate_message(\n                passed, expected_errors, actual_errors\n            )\n\n            # Return the result of the type check\n            return TypeCheckResult(\n                message=message,\n                passed=passed,\n                debug_info={\n                    \"expected_errors\": expected_errors,\n                    \"actual_errors\": actual_errors,\n                },\n            )\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine user code and test code\n        combined_code = user_code + test_code\n\n        # Create a temporary file to write the combined code\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as temp_file:\n            temp_file.write(combined_code)\n            temp_file.close()\n\n        # Run Pyright on the temporary file\n        process = subprocess.run(\n            [\"pyright\", temp_file.name], capture_output=True, text=True\n        )\n\n        # Read the output from Pyright\n        pyright_output = process.stdout\n\n        # Parse Pyright output to identify lines with expected type errors\n        expected_type_errors = cls._parse_pyright_output(pyright_output)\n\n        # Remove the temporary file\n        Path(temp_file.name).unlink()\n\n        # Check if the type check passed or failed\n        passed = not expected_type_errors\n\n        # Prepare the message for the result\n        message = \"Type check passed.\" if passed else \"Type check failed.\"\n\n        # Create a TypeCheckResult object and return it\n        return TypeCheckResult(message=message, passed=passed)\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine the user code and test code\n        code = user_code + test_code\n\n        # Create a temporary file to write the code for type checking\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(code)\n            f.close()\n            filename = f.name\n\n        # Run Pyright on the temporary file\n        result = subprocess.run(\n            [\"pyright\", filename, \"--outputjson\"],\n            capture_output=True,\n            text=True,\n        )\n\n        # Parse the Pyright output\n        try:\n            pyright_output = json.loads(result.stdout)\n        except json.JSONDecodeError:\n            # If the output is not valid JSON, return a failed result\n            return TypeCheckResult(\n                message=\"Pyright output is not valid JSON.\",\n                passed=False,\n                debug_info={\"stdout\": result.stdout, \"stderr\": result.stderr},\n            )\n\n        # Extract the expected type error lines\n        expected_type_error_lines = cls._extract_expected_type_error_lines(test_code)\n\n        # Check if the type check passed\n        passed = cls._check_type_check_passed(\n            pyright_output, expected_type_error_lines\n        )\n\n        # Generate the result message\n        message = cls._generate_result_message(\n            pyright_output, expected_type_error_lines\n        )\n\n        # Delete the temporary file\n        Path(filename).unlink()\n\n        # Return the result\n        return TypeCheckResult(message=message, passed=passed)\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary file to write the combined code\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as tmp_file:\n            tmp_file.write(user_code)\n            tmp_file.write(test_code)\n            tmp_file_path = tmp_file.name\n\n        # Run Pyright on the combined code\n        command = [\"pyright\", tmp_file_path]\n        process = subprocess.Popen(\n            command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True\n        )\n        stdout, stderr = process.communicate()\n\n        # Check if Pyright encountered any errors\n        if process.returncode != 0:\n            return TypeCheckResult(\n                message=\"Pyright encountered an error: \" + stderr, passed=False\n            )\n\n        # Parse the Pyright output to identify lines with expected type errors\n        expected_type_errors = cls._get_expected_type_errors(test_code)\n\n        # Parse the Pyright output to identify lines with type errors\n        type_errors = cls._get_type_errors(stdout)\n\n        # Check if the type check passed or failed\n        passed = not type_errors\n\n        # Prepare the result message\n        if passed:\n            message = \"Type check passed!\"\n        else:\n            message = \"Type check failed. \"\n            if expected_type_errors:\n                message += \"Expected type errors: \" + \", \".join(\n                    str(line) for line in expected_type_errors\n                )\n            else:\n                message += \"Unexpected type errors: \" + \", \".join(\n                    str(line) for line in type_errors\n                )\n\n        return TypeCheckResult(message=message, passed=passed)\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine the user code and test code\n        code = user_code + test_code\n\n        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as tmp_file:\n            tmp_file.write(code)\n            tmp_file_name = tmp_file.name\n\n        # Run Pyright on the temporary file\n        try:\n            # Execute Pyright and capture the output\n            output = subprocess.run(\n                [\"pyright\", tmp_file_name], capture_output=True, text=True\n            )\n\n            # Check if Pyright returned an error\n            if output.returncode != 0:\n                # If Pyright returned an error, return a TypeCheckResult object indicating failure\n                return TypeCheckResult(\n                    message=\"Pyright returned an error\", passed=False\n                )\n\n            # If Pyright did not return an error, parse the output to identify expected type errors\n            expected_errors = cls._parse_expected_errors(test_code)\n            error_messages = cls._parse_pyright_output(output.stdout, expected_errors)\n\n            # If there are no errors, return a TypeCheckResult object indicating success\n            if not error_messages:\n                return TypeCheckResult(\n                    message=\"Congratulations, your code passed the type check\", passed=True\n                )\n\n            # If there are errors, return a TypeCheckResult object with the error messages\n            return TypeCheckResult(\n                message=\"\\n\".join(error_messages), passed=False, debug_info=expected_errors\n            )\n\n        finally:\n            # Delete the temporary file\n            Path(tmp_file_name).unlink()\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary file for the test code\n        with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as test_file:\n            test_file.write(test_code)\n            test_file.close()\n\n        # Create a temporary file for the user code\n        with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as user_file:\n            user_file.write(user_code)\n            user_file.close()\n\n        # Run Pyright on the test file and capture its output\n        with tempfile.TemporaryFile(mode=\"w+\") as output_file:\n            subprocess.run(\n                [\"pyright\", test_file.name, user_file.name],\n                stdout=output_file,\n                stderr=subprocess.STDOUT,\n            )\n            output_file.seek(0)\n            output = output_file.read()\n\n        # Parse the Pyright output to identify lines with expected type errors\n        expected_type_errors = cls._get_expected_type_errors(test_code)\n        # Get the line numbers of the expected type errors\n        expected_type_error_lines = [line for line, _ in expected_type_errors]\n\n        # Parse the Pyright output to identify lines with actual type errors\n        actual_type_errors = cls._parse_pyright_output(output)\n        # Get the line numbers of the actual type errors\n        actual_type_error_lines = [line for line, _ in actual_type_errors]\n\n        # Check if the actual type errors match the expected type errors\n        passed = actual_type_error_lines == expected_type_error_lines\n\n        # Generate a message detailing the outcome of the type check\n        message = cls._generate_message(\n            passed, expected_type_errors, actual_type_errors\n        )\n\n        # Return the result of the type check along with the message\n        return TypeCheckResult(\n            message=message,\n            passed=passed,\n            debug_info={\n                \"expected_type_errors\": expected_type_errors,"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary directory to store the test file\n        with tempfile.TemporaryDirectory() as temp_dir:\n            test_file = Path(temp_dir) / \"test.py\"\n\n            # Write the test code to the test file\n            test_file.write_text(test_code, encoding=\"utf-8\")\n\n            # Parse the test code using the tokenize module\n            with io.StringIO(test_code) as test_code_io:\n                tokens = tokenize.generate_tokens(test_code_io.readline)\n                lines_with_expected_errors = set()\n                for token_type, token_string, _, _, _ in tokens:\n                    if token_type == tokenize.COMMENT and token_string.strip() == cls.EXPECT_ERROR_COMMENT:\n                        # If the current token is a comment and contains the expected error comment,\n                        # add the line number to the set of lines with expected errors\n                        lines_with_expected_errors.add(_)\n\n            # Write the user code to the test file\n            test_file.write_text(user_code, encoding=\"utf-8\")\n\n            # Run Pyright on the test file and capture the output\n            try:\n                output = subprocess.check_output(\n                    [\"pyright\", test_file], stderr=subprocess.STDOUT, text=True\n                )\n            except subprocess.CalledProcessError as e:\n                output = e.output\n\n            # Parse the Pyright output to extract error messages and line numbers\n            error_messages = []\n            error_lines = set()\n            for line in output.splitlines():\n                match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n                if match:\n                    line_number, error_message = match.groups()\n                    error_messages.append(error_message)\n                    error_lines.add(int(line_number))\n\n            # Check if the expected error lines were found in the Pyright output\n            expected_error_lines_found = lines_with_expected_errors.issubset(error_lines)"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine the user code and test code into a single string\n        code = user_code + test_code\n\n        # Write the combined code to a temporary file\n        with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as f:\n            f.write(code)\n            filename = f.name\n\n        # Run Pyright on the temporary file and capture its output\n        result = subprocess.run(\n            [\"pyright\", filename], capture_output=True, text=True, check=False\n        )\n\n        # Remove the temporary file\n        Path(filename).unlink()\n\n        # Check if Pyright returned an error\n        if result.returncode != 0:\n            return TypeCheckResult(\n                message=\"Pyright returned an error: \" + result.stderr, passed=False\n            )\n\n        # Parse the Pyright output to identify lines with expected type errors\n        expected_type_error_lines = cls._parse_expected_type_error_lines(test_code)\n\n        # Parse the Pyright output to identify lines with type errors\n        type_error_lines = cls._parse_type_error_lines(result.stdout)\n\n        # Check if the expected type errors were found\n        if not type_error_lines:\n            return TypeCheckResult(\n                message=\"No type errors found. You're good to go.\", passed=True\n            )\n\n        # Check if the expected type errors were found in the correct lines\n        if expected_type_error_lines != type_error_lines:\n            return TypeCheckResult(\n                message=\"Type errors found in incorrect lines.\", passed=False\n            )\n\n        # If all expected type errors were found in the correct lines, return a success result\n        return TypeCheckResult(\n            message=\"All type errors found in correct lines.\", passed=True\n        )\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary file to write the combined user code and test code\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as tmp_file:\n            tmp_file.write(user_code)\n            tmp_file.write(test_code)\n            tmp_file.close()\n\n        # Run Pyright on the temporary file\n        process = subprocess.run(\n            [\"pyright\", tmp_file.name], capture_output=True, text=True\n        )\n\n        # Delete the temporary file\n        Path(tmp_file.name).unlink()\n\n        # Parse the Pyright output to identify lines with expected type errors\n        expected_error_lines = cls._parse_expected_error_lines(test_code)\n        error_lines = cls._parse_error_lines(process.stdout)\n        unexpected_error_lines = error_lines - expected_error_lines\n\n        # Check if the type check passed or failed\n        passed = not unexpected_error_lines\n\n        # Prepare the result message\n        if passed:\n            message = \"Type check passed!\"\n        else:\n            message = \"Type check failed. Expected errors at line(s): \"\n            message += \", \".join(str(line) for line in expected_error_lines)\n            message += \".\"\n\n        # Create the result object\n        result = TypeCheckResult(message=message, passed=passed)\n\n        # Add debug information to the result\n        result.debug_info[\"expected_error_lines\"] = expected_error_lines\n        result.debug_info[\"error_lines\"] = error_lines\n        result.debug_info[\"unexpected_error_lines\"] = unexpected_error_lines\n\n        return result\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = await self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, fw_compiler=get_compiler_fn(\"Forward\"),\n                          bw_compiler=get_compiler_fn(\"Backward\"))\n    else:\n        return aot_function(fn, fw_compiler=get_compiler_fn(\"Forward\"),\n                            bw_compiler=get_compiler_fn(\"Backward\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, fw_compiler=get_compiler_fn(\"Forward\"),\n                          bw_compiler=get_compiler_fn(\"Backward\"))\n    else:\n        return aot_function(fn, fw_compiler=get_compiler_fn(\"Forward\"),\n                            bw_compiler=get_compiler_fn(\"Backward\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, fw_compiler=get_compiler_fn(\"Forward\"),\n                          bw_compiler=get_compiler_fn(\"Backward\"))\n    else:\n        return aot_function(fn, fw_compiler=get_compiler_fn(\"Forward\"),\n                            bw_compiler=get_compiler_fn(\"Backward\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, fw_compiler=get_compiler_fn(\"Forward\"),\n                          bw_compiler=get_compiler_fn(\"Backward\"))\n    else:\n        return aot_function(fn, fw_compiler=get_compiler_fn(\"Forward\"),\n                            bw_compiler=get_compiler_fn(\"Backward\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, fw_compiler=get_compiler_fn(\"Forward\"),\n                          bw_compiler=get_compiler_fn(\"Backward\"))\n    else:\n        return aot_function(fn, fw_compiler=get_compiler_fn(\"Forward\"),\n                            bw_compiler=get_compiler_fn(\"Backward\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, fw_compiler=get_compiler_fn(\"Forward\"),\n                          bw_compiler=get_compiler_fn(\"Backward\"))\n    else:\n        return aot_function(fn, fw_compiler=get_compiler_fn(\"Forward\"),\n                            bw_compiler=get_compiler_fn(\"Backward\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, fw_compiler=get_compiler_fn(\"Forward\"),\n                          bw_compiler=get_compiler_fn(\"Backward\"))\n    else:\n        return aot_function(fn, fw_compiler=get_compiler_fn(\"Forward\"),\n                            bw_compiler=get_compiler_fn(\"Backward\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, fw_compiler=get_compiler_fn(\"forward\"),\n                          bw_compiler=get_compiler_fn(\"backward\"))\n    else:\n        return aot_function(fn, fw_compiler=get_compiler_fn(\"forward\"),\n                            bw_compiler=get_compiler_fn(\"backward\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, fw_compiler=get_compiler_fn(\"forward\"),\n                          bw_compiler=get_compiler_fn(\"backward\"))\n    else:\n        return aot_function(fn, fw_compiler=get_compiler_fn(\"forward\"),\n                            bw_compiler=get_compiler_fn(\"backward\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, fw_compiler=get_compiler_fn(\"forward\"),\n                          bw_compiler=get_compiler_fn(\"backward\"))\n    else:\n        return aot_function(fn, fw_compiler=get_compiler_fn(\"forward\"),\n                            bw_compiler=get_compiler_fn(\"backward\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, fw_compiler=get_compiler_fn(\"Forward Graph\"),\n                          bw_compiler=get_compiler_fn(\"Backward Graph\"))\n    else:\n        return aot_function(fn, fw_compiler=get_compiler_fn(\"Forward Graph\"),\n                            bw_compiler=get_compiler_fn(\"Backward Graph\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, fw_compiler=get_compiler_fn(\"forward\"),\n                          bw_compiler=get_compiler_fn(\"backward\"))\n    else:\n        return aot_function(fn, fw_compiler=get_compiler_fn(\"forward\"),\n                            bw_compiler=get_compiler_fn(\"backward\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, fw_compiler=get_compiler_fn(\"Forward\"),\n                          bw_compiler=get_compiler_fn(\"Backward\"))\n    else:\n        return aot_function(fn, fw_compiler=get_compiler_fn(\"Forward\"),\n                            bw_compiler=get_compiler_fn(\"Backward\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, fw_compiler=get_compiler_fn(\"Forward\"),\n                          bw_compiler=get_compiler_fn(\"Backward\"))\n    else:\n        return aot_function(fn, fw_compiler=get_compiler_fn(\"Forward\"),\n                            bw_compiler=get_compiler_fn(\"Backward\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(\n            fn,\n            fw_compiler=get_compiler_fn(\"Forward graph\"),\n            bw_compiler=get_compiler_fn(\"Backward graph\"))\n    else:\n        return aot_function(\n            fn,\n            fw_compiler=get_compiler_fn(\"Forward graph\"),\n            bw_compiler=get_compiler_fn(\"Backward graph\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(\n            fn,\n            fw_compiler=get_compiler_fn(\"Forward Graph\"),\n            bw_compiler=get_compiler_fn(\"Backward Graph\"),\n        )\n    else:\n        return aot_function(fn, fw_compiler=get_compiler_fn(\"Forward Graph\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, fw_compiler=get_compiler_fn(title=\"Forward\"),\n                          bw_compiler=get_compiler_fn(title=\"Backward\"))\n    else:\n        return aot_function(fn, fw_compiler=get_compiler_fn(title=\"Forward\"),\n                            bw_compiler=get_compiler_fn(title=\"Backward\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        # Compile the module using aot_module\n        return aot_module(fn, fw_compiler=get_compiler_fn(\"Forward Graph\"),\n                          bw_compiler=get_compiler_fn(\"Backward Graph\"))\n    else:\n        # Compile the function using aot_function\n        return aot_function(fn, fw_compiler=get_compiler_fn(\"Forward Graph\"),\n                            bw_compiler=get_compiler_fn(\"Backward Graph\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        with no_fake_tensor():\n            return aot_module(fn, fw_compiler=get_compiler_fn(\"forward\"),\n                              bw_compiler=get_compiler_fn(\"backward\"))\n    else:\n        return aot_function(fn, fw_compiler=get_compiler_fn(\"forward\"),\n                            bw_compiler=get_compiler_fn(\"backward\"))"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        # Compile the module using the forward and backward compilers specific for modules\n        compiled_module = aot_module(fn, fw_compiler=get_compiler_fn(\n            \"Module Forward\"), bw_compiler=get_compiler_fn(\"Module Backward\"))\n        return compiled_module\n    else:\n        # Compile the function using the forward and backward compilers specific for functions\n        compiled_fn = aot_function(fn, fw_compiler=get_compiler_fn(\n            \"Function Forward\"), bw_compiler=get_compiler_fn(\"Function Backward\"))\n        return compiled_fn"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        try:\n            config_dict = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            logger.error(exc)\n            raise exc\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError(\"The output_path must be a yaml file.\")\n        with open(output_path, 'w') as f:\n            yaml.dump(best_config, f)\n    return best_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        config_dict = yaml.safe_load(f)\n    config_dict = summary_df_to_yaml(summary_df, config_dict)\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError(\"output_path must be .yaml or .yml\")\n        with open(output_path, 'w') as f:\n            yaml.dump(config_dict, f)\n    return config_dict"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        config_dict = yaml.safe_load(f)\n    config_dict = summary_df_to_yaml(summary_df, config_dict)\n    if output_path is not None:\n        with open(output_path, 'w') as f:\n            yaml.dump(config_dict, f)\n    return config_dict"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        config_dict = yaml.safe_load(f)\n    config_dict = summary_df_to_yaml(summary_df, config_dict)\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError(\"The file extension must be .yaml or .yml\")\n        with open(output_path, 'w') as f:\n            yaml.dump(config_dict, f)\n    return config_dict"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        try:\n            config_dict = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            logger.error(exc)\n            raise exc\n\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n    if output_path is not None:\n        with open(output_path, 'w') as f:\n            yaml.dump(best_config, f)\n    return best_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        try:\n            config_dict = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            logger.error(exc)\n            raise exc\n\n    config_yaml = summary_df_to_yaml(summary_df, config_dict)\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError(\"The output path must be a yaml file.\")\n        with open(output_path, 'w') as f:\n            yaml.dump(config_yaml, f)\n    return config_yaml"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        config_dict = yaml.safe_load(f)\n    config_dict = summary_df_to_yaml(summary_df, config_dict)\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError('output_path must be end with .yaml or .yml')\n        with open(output_path, 'w') as f:\n            yaml.dump(config_dict, f)\n    return config_dict"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        try:\n            config_dict = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            logger.error(exc)\n            raise exc\n    config_dict = summary_df_to_yaml(summary_df, config_dict)\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError(\"The output_path must be .yaml or .yml file.\")\n        with open(output_path, 'w') as f:\n            yaml.dump(config_dict, f, default_flow_style=False)\n    return config_dict"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        try:\n            config_dict = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            logger.error(exc)\n            raise exc\n    config_dict = summary_df_to_yaml(summary_df, config_dict)\n\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError(\"The output_path must be a yaml file.\")\n        with open(output_path, 'w') as f:\n            yaml.dump(config_dict, f)\n    return config_dict"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        config_dict = yaml.safe_load(f)\n    config_dict = summary_df_to_yaml(summary_df, config_dict)\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError(\"The output_path must be .yaml or .yml\")\n        with open(output_path, 'w') as f:\n            yaml.dump(config_dict, f)\n    return config_dict"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        try:\n            config_dict = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            logger.error(exc)\n            raise exc\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n    if output_path is not None:\n        with open(output_path, 'w') as f:\n            try:\n                yaml.dump(best_config, f, default_flow_style=False)\n            except yaml.YAMLError as exc:\n                logger.error(exc)\n                raise exc\n    return best_config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        try:\n            config_dict = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            logger.error(exc)\n            raise exc\n    config_dict = summary_df_to_yaml(summary_df, config_dict)\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError('Output path must be .yaml or .yml')\n        with open(output_path, 'w') as f:\n            yaml.dump(config_dict, f)\n    return config_dict"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        try:\n            config_dict = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            logger.error(exc)\n            raise exc\n\n    config_dict = summary_df_to_yaml(summary_df, config_dict)\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError(\"The output file must be a yaml file.\")\n        with open(output_path, 'w') as f:\n            yaml.dump(config_dict, f)\n    return config_dict"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        config_dict = yaml.safe_load(f)\n    config_dict = summary_df_to_yaml(summary_df, config_dict)\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError(\"The file extension of output_path must be .yaml or .yml.\")\n        with open(output_path, 'w') as f:\n            yaml.dump(config_dict, f)\n    return config_dict"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        config_dict = yaml.safe_load(f)\n\n    config_yaml = summary_df_to_yaml(summary_df, config_dict)\n\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError(\"The output path must be a .yaml or .yml file.\")\n        with open(output_path, 'w') as f:\n            yaml.dump(config_yaml, f)\n\n    return config_yaml"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        try:\n            config_dict = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            logger.error(exc)\n            raise exc\n    config_yaml = summary_df_to_yaml(summary_df, config_dict)\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError('output_path must be .yaml or .yml file')\n        with open(output_path, 'w') as f:\n            yaml.dump(config_yaml, f)\n    return config_yaml"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        config_dict = yaml.safe_load(f)\n\n    config_dict = summary_df_to_yaml(summary_df, config_dict)\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError('output_path must be a .yaml or .yml file.')\n        with open(output_path, 'w') as f:\n            yaml.dump(config_dict, f)\n    return config_dict"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        config_dict = yaml.safe_load(f)\n\n    config_dict = summary_df_to_yaml(summary_df, config_dict)\n\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError('output_path must be a yaml file.')\n        with open(output_path, 'w') as f:\n            yaml.dump(config_dict, f, default_flow_style=False)\n\n    return config_dict"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        config_dict = yaml.safe_load(f)\n\n    config = summary_df_to_yaml(summary_df, config_dict)\n\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError(\"Output file must be a YAML file.\")\n        with open(output_path, 'w') as f:\n            yaml.dump(config, f)\n\n    return config"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        config_dict = yaml.safe_load(f)\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError(\"output_path must be a .yaml or .yml file\")\n        with open(output_path, 'w') as f:\n            yaml.dump(best_config, f, default_flow_style=False)\n    return best_config"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    cache = {}\n    lock = threading.Lock()\n\n    def wrapper(*args, **kwargs):\n        nonlocal cache, lock\n        key = hash_arg((args, kwargs))\n        with lock:\n            if key not in cache:\n                cache[key] = func(*args, **kwargs)\n        return cache[key]\n\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    cache = {}\n    lock = threading.Lock()\n\n    def wrapper(*args, **kwargs_):\n        nonlocal cache\n        nonlocal lock\n        with lock:\n            key = (args, kwargs_)\n            if key not in cache:\n                cache[key] = func(*args, **kwargs_)\n            return cache[key]\n\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    kwargs = {}\n    kwargs.update(kwargs_)\n    if ts_compiler is not None:\n        kwargs['ts_compiler'] = ts_compiler\n    compiler = AutoTraceCompiler(**kwargs)\n    lock = threading.Lock()\n    cache = {}\n\n    def wrapper(*args, **kwargs):\n        nonlocal cache\n        nonlocal lock\n        with lock:\n            key = compiler.get_inputs_key(func, args, kwargs)\n            if key is None:\n                return func(*args, **kwargs)\n            if key not in cache:\n                cache[key] = compiler.compile(func, args, kwargs)\n            return cache[key](*args, **kwargs)\n\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    kwargs = kwargs_.copy()\n    kwargs.pop('training', None)\n    if ts_compiler is not None:\n        kwargs['ts_compiler'] = ts_compiler\n    compiler = AutoTraceCompiler(**kwargs)\n    return compiler.compile(func, (), {})"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    kwargs = dict(kwargs_)\n    compiler = AutoTraceCompiler(ts_compiler=ts_compiler, **kwargs)\n\n    def wrapper(*args, **kwargs):\n        return compiler.compile(func, args, kwargs)\n\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    if ts_compiler is not None:\n        kwargs_ = dict(ts_compiler=ts_compiler, **kwargs_)\n    lock = threading.Lock()\n    cache = {}\n\n    def wrapper(*args, **kwargs):\n        nonlocal cache\n        with lock:\n            key = (func, args, kwargs)\n            if key not in cache:\n                cache[key] = AutoTraceCompiler(**kwargs_).compile(\n                    func, args, kwargs)\n        return cache[key](*args, **kwargs)\n\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    kwargs = kwargs_.copy()\n    if ts_compiler is not None:\n        kwargs['ts_compiler'] = ts_compiler\n    compiler = AutoTraceCompiler(**kwargs)\n\n    @functools.wraps(func)\n    def wrapped(*args, **kwargs):\n        if compiler.is_compiling():\n            return func(*args, **kwargs)\n        inputs_key = compiler.get_inputs_key(func, args, kwargs)\n        outputs_key = compiler.get_outputs_key(func, args)\n        if inputs_key is None or outputs_key is None:\n            return func(*args, **kwargs)\n        with compiler.lock:\n            if inputs_key not in compiler.cache:\n                compiler.cache[inputs_key] = compiler.compile(\n                    func, args, kwargs)\n            return compiler.cache[inputs_key](*args, **kwargs)\n\n    return wrapped"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    if ts_compiler is not None:\n        kwargs_['ts_compiler'] = ts_compiler\n\n    lock = threading.Lock()\n    cache = {}\n\n    @functools.wraps(func)\n    def wrapped(*args, **kwargs):\n        key = (args, tuple(sorted(kwargs.items(), key=lambda x: x[0])))\n        if key not in cache:\n            with lock:\n                if key not in cache:\n                    cache[key] = AutoTraceCompiler(**kwargs_).compile(\n                        func, args, kwargs)\n        return cache[key](*args, **kwargs)\n\n    return wrapped"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    if ts_compiler is not None:\n        kwargs = dict(ts_compiler=ts_compiler, **kwargs_)\n    else:\n        kwargs = kwargs_\n    cache = {}\n    lock = threading.Lock()\n\n    def wrapper(*args, **kwargs):\n        nonlocal cache, lock\n        inputs = flat_tensors.flattern(args)\n        kwargs = flat_tensors.flattern(kwargs)\n        key = (func, inputs, kwargs)\n        with lock:\n            if key not in cache:\n                cache[key] = AutoTraceCompiler(**kwargs).compile(func,\n                                                                 inputs,\n                                                                 kwargs)\n            return cache[key](*args, **kwargs)\n\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    if ts_compiler is not None:\n        kwargs = kwargs_.copy()\n        kwargs['ts_compiler'] = ts_compiler\n    else:\n        kwargs = kwargs_\n    lock = threading.Lock()\n    cache = {}\n\n    def trace_and_cache(func, *args, **kwargs):\n        key = (func, args, kwargs)\n        with lock:\n            if key not in cache:\n                cache[key] = trace_with_kwargs(func, *args, **kwargs)\n        return cache[key]\n\n    def wrapped(*args, **kwargs):\n        traced_module, call_helper = trace_and_cache(func, args, kwargs)\n        return call_helper(traced_module(*args, **kwargs))\n\n    return wrapped"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    kwargs = dict(kwargs_)\n    compiler = AutoTraceCompiler(ts_compiler=ts_compiler, **kwargs)\n    lock = threading.Lock()\n    cache = {}\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        inputs = flat_tensors.flattern((args, kwargs))\n        with lock:\n            key = compiler.get_inputs_key(func, inputs, kwargs)\n            if key is None:\n                return compiler.compile(func, inputs, kwargs)(*args, **kwargs)\n            if key not in cache:\n                cache[key] = compiler.compile(func, inputs, kwargs)(*args,\n                                                                   **kwargs)\n            return cache[key]\n\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    if ts_compiler is not None:\n        kwargs_['ts_compiler'] = ts_compiler\n    kwargs = kwargs_\n    lock = threading.Lock()\n    cache = {}\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        nonlocal cache\n        nonlocal lock\n        with lock:\n            key = (func, args, tuple(sorted(kwargs.items())))\n            if key not in cache:\n                cache[key] = AutoTraceCompiler(**kwargs).compile(func, *args,\n                                                                 **kwargs)\n            return cache[key](*args, **kwargs)\n\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    kwargs = kwargs_.copy()\n    if 'example_inputs' in kwargs:\n        kwargs['example_inputs'] = tree_copy(kwargs['example_inputs'],\n                                             detach=True)\n    if 'example_kwarg_inputs' in kwargs:\n        kwargs['example_kwarg_inputs'] = tree_copy(\n            kwargs['example_kwarg_inputs'], detach=True)\n\n    lock = threading.Lock()\n    cache = {}\n\n    def wrapper(*args, **kwargs):\n        key = (args, tuple(sorted(kwargs.items(), key=lambda x: x[0])))\n        with lock:\n            if key in cache:\n                return cache[key]\n            traced_func = AutoTraceCompiler(ts_compiler=ts_compiler,\n                                            **kwargs).compile(func, args, kwargs)\n            cache[key] = traced_func\n            return traced_func(*args, **kwargs)\n\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    if ts_compiler is None:\n        ts_compiler = lambda x, *args, **kwargs: x\n\n    def wrapper(*args, **kwargs):\n        nonlocal ts_compiler\n        nonlocal kwargs_\n        inputs = flat_tensors.flattern((args, kwargs))\n        key = hash_arg(inputs)\n        if key in cache:\n            return cache[key]\n        with lock:\n            if key in cache:\n                return cache[key]\n            traced_module = ts_compiler(\n                trace_with_kwargs(func,\n                                  example_inputs=args,\n                                  example_kwarg_inputs=kwargs,\n                                  **kwargs_)[0])\n            cache[key] = traced_module\n            return traced_module\n\n    cache = {}\n    lock = threading.Lock()\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    cache = {}\n    lock = threading.Lock()\n\n    def wrapper(*args, **kwargs):\n        nonlocal cache, lock\n        inputs, kwargs = flat_tensors.flattern(args), kwargs\n        key = (inputs, hash_arg(kwargs))\n        with lock:\n            if key not in cache:\n                cache[key] = trace_with_kwargs(func, inputs, kwargs,\n                                               **kwargs_)\n            traced_module, call_helper = cache[key]\n        outputs = call_helper(traced_module)(*inputs, **kwargs)\n        unflat_outputs = flat_tensors.unflattern(outputs)\n        return unflat_outputs\n\n    if ts_compiler is not None:\n        return ts_compiler(wrapper, inputs=example_inputs,\n                            kwargs=example_kwarg_inputs, **kwargs_)\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    cache = {}\n    lock = threading.Lock()\n\n    @functools.wraps(func)\n    def wrapped(*args, **kwargs):\n        nonlocal cache, lock\n        with lock:\n            key = (args, kwargs)\n            if key not in cache:\n                if ts_compiler is not None:\n                    if 'call_helper' in inspect.signature(\n                            ts_compiler).parameters:\n                        traced_m, call_helper = trace_with_kwargs(\n                            func, *args, **kwargs, **kwargs_)\n                        traced_m = ts_compiler(traced_m, call_helper, *args,\n                                               **kwargs)\n                    else:\n                        traced_m, call_helper = trace_with_kwargs(\n                            func, *args, **kwargs, **kwargs_)\n                        converted_args = call_helper(traced_m).convert_inputs(\n                            *args, **kwargs)\n                        traced_m = ts_compiler(traced_m, converted_args)\n                    traced_module = call_helper(traced_m)\n                else:\n                    traced_module, _ = trace_with_kwargs(\n                        func, *args, **kwargs, **kwargs_)\n                cache[key] = traced_module\n            return cache[key](*args, **kwargs)\n\n    return wrapped"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    def wrapper(*args, **kwargs_):\n        nonlocal ts_compiler\n        if len(args) == 0 and len(kwargs_) == 0:\n            return func\n        if ts_compiler is not None:\n            if 'call_helper' in inspect.signature(\n                    ts_compiler).parameters:\n                ts_compiler = functools.partial(ts_compiler, None, None,\n                                                args, kwargs_)\n            else:\n                ts_compiler = functools.partial(ts_compiler, None,\n                                                args, kwargs_)\n        return lazy_trace_with_kwargs(func, ts_compiler=ts_compiler,\n                                      **kwargs_)\n\n    return wrapper\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    # initialize cache and lock\n    cache = {}\n    lock = threading.Lock()\n\n    def wrapper(*args, **kwargs_):\n        nonlocal cache, lock\n        with lock:\n            # check if the function or module is already in the cache\n            key = (func, args, tuple(kwargs_.items()))\n            if key in cache:\n                # return the cached traced function or module\n                return cache[key]\n\n        # trace the function or module\n        if ts_compiler is None:\n            traced_func = trace_with_kwargs(func, args, kwargs_, **kwargs_)[0]\n        else:\n            traced_func = ts_compiler(\n                trace_with_kwargs(func, args, kwargs_, **kwargs_)[0])\n\n        # cache the traced function or module\n        with lock:\n            cache[key] = traced_func\n\n        # return the traced function or module\n        return traced_func\n\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    # check if the function is a PyTorch module\n    if isinstance(func, torch.nn.Module):\n        # get the forward method of the module\n        func = func.forward\n\n    # create a lock to ensure thread safety\n    lock = threading.Lock()\n    # create a cache to store traced modules\n    cache = {}\n\n    # define a wrapper function that will be used to trace the function\n    def wrapper(*args, **kwargs_):\n        # get the hashable key for the input arguments\n        key = (args, tuple(sorted(kwargs_.items())))\n        # check if the key is in the cache\n        if key in cache:\n            # get the traced module from the cache\n            traced_module = cache[key]\n        else:\n            # lock the cache to prevent multiple threads from accessing it simultaneously\n            with lock:\n                # check if the key is in the cache again (in case another thread has already added it)\n                if key in cache:\n                    # get the traced module from the cache\n                    traced_module = cache[key]\n                else:\n                    # trace the function with the given arguments and keyword arguments\n                    traced_module = trace_with_kwargs(\n                        func, args, kwargs_, ts_compiler=ts_compiler)\n                    # add the traced module to the cache\n                    cache[key] = traced_module\n        # return the traced module\n        return traced_module(*args, **kwargs_)\n\n    # return the wrapper function\n    return wrapper"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    # This is a global variable that holds the cache of traced modules.\n    # It is a dictionary where the keys are tuples of (function or module, inputs, kwargs) and the values are the traced modules.\n    # The cache is used to store the traced modules for future calls to the same function or module with the same inputs and kwargs.\n    # This helps to avoid tracing the same function or module multiple times, which can be time-consuming.\n    cache = {}\n    lock = threading.Lock()\n\n    def wrapper(*args, **kwargs_):\n        # Get the inputs and kwargs from the function or module's forward method.\n        # If the function is a module, use its forward method to get the inputs and kwargs.\n        # Otherwise, use the function itself to get the inputs and kwargs.\n        if isinstance(func, torch.nn.Module):\n            inputs = args\n            kwargs_ = kwargs_\n            func_ = func.forward\n        else:\n            inputs = args\n            kwargs_ = kwargs_\n            func_ = func\n\n        # Get the key for the cache based on the function or module, inputs, and kwargs.\n        key = (func_, inputs, kwargs_)\n\n        # Check if the key is in the cache.\n        # If it is, return the cached traced module.\n        # Otherwise, trace the function or module and add it to the cache.\n        with lock:\n            if key in cache:\n                traced_module = cache[key]\n            else:\n                traced_module = func_(*args, **kwargs_)\n                if ts_compiler is not None:\n                    traced_module = ts_compiler(traced_module, *args,\n                                                 **kwargs_)\n                cache[key] = traced_module\n\n        # Return the traced module.\n        return traced_module\n\n    # Return the wrapped function or module's forward method.\n    return wrapper"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        project_dir = os.path.dirname(trial_path)\n        return cls(config, project_dir=project_dir)"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        project_dir = os.path.dirname(trial_path)\n        return cls(config, project_dir=project_dir)"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        project_dir = os.path.dirname(trial_path)\n        return cls(config, project_dir=project_dir)\n\n    "}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        yaml_dict = extract_best_config(trial_path)\n        project_dir = os.path.dirname(trial_path)\n        return cls(yaml_dict, project_dir=project_dir)"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        yaml_dict = extract_best_config(trial_path)\n        return cls(yaml_dict, project_dir=os.path.dirname(trial_path))\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        yaml_dict = extract_best_config(trial_path)\n        project_dir = os.path.dirname(trial_path)\n        return cls(yaml_dict, project_dir=project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        project_dir = os.path.dirname(trial_path)\n        return cls(config, project_dir=project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        if not os.path.exists(trial_path):\n            raise ValueError(f\"{trial_path} does not exist.\")\n        trial_summary_path = os.path.join(trial_path, 'summary.csv')\n        if not os.path.exists(trial_summary_path):\n            raise ValueError(f\"summary.csv does not exist in {trial_path}.\")\n        trial_summary_df = load_summary_file(trial_summary_path, dict_columns=['best_module_params'])\n        config_yaml_path = os.path.join(trial_path, 'config.yaml')\n        with open(config_yaml_path, 'r') as f:\n            config_dict = yaml.safe_load(f)\n        yaml_dict = summary_df_to_yaml(trial_summary_df, config_dict)\n        return cls(yaml_dict, project_dir=os.path.dirname(trial_path))"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        project_dir = os.path.dirname(trial_path)\n        return cls(config, project_dir=project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        project_dir = os.path.abspath(os.path.join(trial_path, os.pardir))\n        return cls(config, project_dir=project_dir)\n\n    "}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        if not os.path.exists(trial_path):\n            raise ValueError(f\"{trial_path} does not exist.\")\n        summary_path = os.path.join(trial_path, 'summary.csv')\n        if not os.path.exists(summary_path):\n            raise ValueError(f\"summary.csv does not exist in {trial_path}.\")\n        trial_summary_df = load_summary_file(summary_path, dict_columns=['best_module_params'])\n        config_yaml_path = os.path.join(trial_path, 'config.yaml')\n        with open(config_yaml_path, 'r') as f:\n            config_dict = yaml.safe_load(f)\n        yaml_dict = summary_df_to_yaml(trial_summary_df, config_dict)\n        return cls(yaml_dict, project_dir=os.path.dirname(trial_path))\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        yaml_dict = extract_best_config(trial_path)\n        project_dir = os.path.dirname(trial_path)\n        return cls(yaml_dict, project_dir=project_dir)"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        yaml_path = os.path.join(trial_path, 'best_config.yaml')\n        if not os.path.exists(yaml_path):\n            raise ValueError(f\"best_config.yaml does not exist in {trial_path}.\")\n        return cls.from_yaml(yaml_path, project_dir=os.path.dirname(trial_path))\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        if not os.path.exists(trial_path):\n            raise ValueError(f\"trial_path {trial_path} does not exist.\")\n        if not os.path.isdir(trial_path):\n            raise ValueError(f\"trial_path {trial_path} is not a directory.\")\n        if not os.path.exists(os.path.join(trial_path, 'summary.csv')):\n            raise ValueError(f\"summary.csv does not exist in {trial_path}.\")\n        yaml_dict = extract_best_config(trial_path)\n        return cls(yaml_dict, project_dir=os.path.dirname(trial_path))"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        summary_path = os.path.join(trial_path, 'summary.csv')\n        if not os.path.exists(summary_path):\n            raise ValueError(f\"summary.csv does not exist in {trial_path}.\")\n        trial_summary_df = load_summary_file(summary_path, dict_columns=['best_module_params'])\n        config_yaml_path = os.path.join(trial_path, 'config.yaml')\n        with open(config_yaml_path, 'r') as f:\n            config_dict = yaml.safe_load(f)\n        yaml_dict = summary_df_to_yaml(trial_summary_df, config_dict)\n        return cls(yaml_dict, project_dir=os.path.dirname(trial_path))\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        summary_path = os.path.join(trial_path, 'summary.csv')\n        if not os.path.exists(summary_path):\n            raise ValueError(f\"summary.csv does not exist in {trial_path}.\")\n        trial_summary_df = load_summary_file(summary_path, dict_columns=['best_module_params'])\n        config_yaml_path = os.path.join(trial_path, 'config.yaml')\n        with open(config_yaml_path, 'r') as f:\n            config_dict = yaml.safe_load(f)\n        yaml_dict = summary_df_to_yaml(trial_summary_df, config_dict)\n        project_dir = os.path.dirname(trial_path)\n        return cls(yaml_dict, project_dir=project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        project_dir = os.path.dirname(trial_path)\n        return cls(config, project_dir=project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        yaml_path = os.path.join(trial_path, 'config.yaml')\n        if not os.path.exists(yaml_path):\n            raise ValueError(f\"config.yaml does not exist in {trial_path}.\")\n        with open(yaml_path, 'r') as f:\n            config = yaml.safe_load(f)\n        project_dir = os.path.dirname(trial_path)\n        return cls(config, project_dir=project_dir)\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        if not os.path.exists(trial_path):\n            raise ValueError(f\"{trial_path} does not exist.\")\n        if not os.path.isdir(trial_path):\n            raise ValueError(f\"{trial_path} is not a directory.\")\n        summary_path = os.path.join(trial_path, 'summary.csv')\n        if not os.path.exists(summary_path):\n            raise ValueError(f\"summary.csv does not exist in {trial_path}.\")\n        trial_summary_df = load_summary_file(summary_path, dict_columns=['best_module_params'])\n        config_yaml_path = os.path.join(trial_path, 'config.yaml')\n        with open(config_yaml_path, 'r') as f:\n            config_dict = yaml.safe_load(f)\n        yaml_dict = summary_df_to_yaml(trial_summary_df, config_dict)\n        return cls(yaml_dict, project_dir=os.path.dirname(trial_path))\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        if not os.path.exists(trial_path):\n            raise ValueError(f\"Trial folder does not exist in {trial_path}.\")\n        if not os.path.exists(os.path.join(trial_path, 'summary.csv')):\n            raise ValueError(f\"Trial folder is not evaluated yet. Please evaluate the trial folder first.\")\n        config = extract_best_config(trial_path)\n        project_dir = os.path.dirname(trial_path)\n        return cls(config, project_dir=project_dir)\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    assert len(modules) == len(module_params), \"The number of modules and module_params must be the same.\"\n\n    # Create node_line_dir if not exists\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Evaluate and select the best module\n    best_result = evaluate_and_select_best_module(modules, module_params, previous_result, node_line_dir, strategies)\n\n    # Save the best result to disk\n    best_result_filename = f\"best_result.parquet\"\n    best_result.to_parquet(os.path.join(node_line_dir, best_result_filename))\n\n    # Return the best result dataframe\n    return best_result\n\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create a directory for this node line if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Run each module and measure execution time\n    results = []\n    for module, params in zip(modules, module_params):\n        result = module(**params)\n        result['module_name'] = module.__name__\n        result['module_params'] = params\n        result['execution_time'] = measure_speed(module, params, previous_result)\n        results.append(result)\n\n    # Evaluate and select the best result\n    result_df = pd.concat(results)\n    result_df = evaluate_retrieval_node(result_df, previous_result, strategies['metrics'])\n    result_df = filter_by_threshold(result_df, strategies['speed_threshold'])\n    best_result = select_best_average(result_df, strategies['metrics'])\n\n    # Save the results and summary to disk\n    result_df.to_parquet(os.path.join(node_line_dir, 'results.parquet'))\n    summary_df = result_df.groupby('module_name').mean().reset_index()\n    summary_df.to_csv(os.path.join(node_line_dir, 'summary.csv'), index=False)\n\n    # Return the best result dataframe\n    return best_result"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    logger.info(f\"Running retrieval node: {node_line_dir}\")\n\n    # create directory if not exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # run modules\n    results = []\n    for module, params in zip(modules, module_params):\n        result_df = module(**params)\n        results.append(result_df)\n\n    # measure speed\n    results = measure_speed(results, strategies)\n\n    # filter by speed threshold\n    results = filter_by_threshold(results, strategies)\n\n    # select best\n    best_result = select_best_average(results, strategies)\n\n    # evaluate\n    best_result = evaluate_retrieval_node(best_result, previous_result, strategies['metrics'])\n\n    # save results\n    best_result.to_parquet(os.path.join(node_line_dir, \"best_result.parquet\"))\n\n    # save summary\n    summary = pd.DataFrame(results)\n    summary.to_csv(os.path.join(node_line_dir, \"summary.csv\"), index=False)\n\n    return pd.concat([previous_result, best_result], axis=1)"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create node line directory if not exists\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Evaluate each module and measure execution time\n    results = []\n    for module, params in zip(modules, module_params):\n        module_name = module.__name__\n        logger.info(f\"Evaluating module {module_name}\")\n        result = evaluate_retrieval_node(module(**params), previous_result, strategies['metrics'])\n        result['module_name'] = module_name\n        result['module_params'] = params\n        result = measure_speed(result)\n        results.append(result)\n\n    # Save results to disk\n    for i, result in enumerate(results):\n        result.to_parquet(os.path.join(node_line_dir, f\"{i}.parquet\"))\n\n    # Select best result based on strategies\n    best_result = select_best_average(results, strategies['metrics'])\n\n    # Filter by speed threshold\n    if 'speed_threshold' in strategies:\n        best_result = filter_by_threshold(best_result, strategies['speed_threshold'])\n\n    # Save summary to disk\n    summary = pd.DataFrame(results)\n    summary = summary.drop(columns=['retrieved_contents', 'retrieved_ids', 'retrieve_scores'])\n    summary.to_csv(os.path.join(node_line_dir, \"summary.csv\"), index=False)\n\n    # Combine previous result with best result\n    best_result = pd.concat([previous_result, best_result], axis=1)\n    return best_result"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    metrics = strategies['metrics']\n    speed_thresholds = strategies['speed_thresholds']\n\n    result_df_list = []\n    execution_times = []\n    for module, params in zip(modules, module_params):\n        module_name = module.__name__\n        module_params_str = str(params)\n        module_result_df, module_execution_time = measure_speed(module, params, previous_result)\n        result_df_list.append(module_result_df)\n        execution_times.append(module_execution_time)\n        module_result_df = evaluate_retrieval_node(module_result_df, previous_result, metrics)\n        module_result_df['module_name'] = module_name\n        module_result_df['module_params'] = module_params_str\n        module_result_df['execution_time'] = module_execution_time\n        module_result_df.to_parquet(os.path.join(node_line_dir, f\"{module_name}_{module_params_str}.parquet\"))\n\n    result_df = pd.concat(result_df_list, axis=1)\n    result_df['execution_time'] = execution_times\n    result_df['module_name'] = [module.__name__ for module in modules]\n    result_df['module_params'] = [str(params) for params in module_params]\n    result_df.to_parquet(os.path.join(node_line_dir, \"result.parquet\"))\n\n    summary_df = pd.DataFrame({\n        'filename': [f\"{module.__name__}_{str(params)}.parquet\" for module, params in zip(modules, module_params)],\n        'module_name': [module.__name__ for module in modules],\n        'module_params': [str(params) for params in module_params],\n        'execution_time': execution_times,\n    })\n\n    if speed_thresholds is not None:\n        speed_thresholds_df = filter_by_threshold(summary_df, speed_"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    assert len(modules) == len(module_params), \"modules and module_params must have the same length\"\n    assert len(modules) > 0, \"modules must not be empty\"\n\n    result_dfs = []\n    execution_times = []\n\n    for module, params in zip(modules, module_params):\n        module_name = module.__name__\n        logger.info(f\"Running {module_name} with params: {params}\")\n        result_df = module(**params)\n        result_dfs.append(result_df)\n        execution_time = measure_speed(module, params)\n        execution_times.append(execution_time)\n\n    summary_df = pd.DataFrame({\n        'filename': [f\"{module.__name__}_{i}.parquet\" for i, module in enumerate(modules)],\n        'module_name': [module.__name__ for module in modules],\n        'module_params': module_params,\n        'execution_time': execution_times,\n    })\n\n    if strategies['speed_threshold'] is not None:\n        summary_df = filter_by_threshold(summary_df, 'execution_time', strategies['speed_threshold'])\n\n    if strategies['metrics'] is not None:\n        summary_df = evaluate_retrieval_node(summary_df, previous_result, strategies['metrics'])\n\n    if strategies['select_best'] is not None:\n        summary_df = select_best_average(summary_df, strategies['select_best'])\n\n    best_result_filename = summary_df.iloc[0]['filename']\n    best_result_df = pd.read_parquet(os.path.join(node_line_dir, best_result_filename))\n\n    # Combine the previous result columns with the selected retrieval node's result columns\n    best_result_df = pd.concat([previous_result, best_result_df], axis=1)\n\n    # Save the results and summary\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Run each module with its parameters\n    module_results = list(map(lambda module, params: module(**params), modules, module_params))\n\n    # Evaluate each module's result\n    module_results = list(map(lambda df: evaluate_retrieval_node(df, previous_result, strategies['metrics']), module_results))\n\n    # Measure execution time for each module\n    execution_times = list(map(lambda df: measure_speed(df), module_results))\n\n    # Save results and summaries to disk\n    save_retrieval_node_results(module_results, execution_times, strategies, node_line_dir)\n\n    # Filter results by speed threshold\n    if 'speed_threshold' in strategies:\n        module_results = list(filter(lambda df: filter_by_threshold(df, strategies['speed_threshold']), module_results))\n\n    # Select best result among remaining results\n    best_result = select_best_average(module_results, strategies['metrics'])\n\n    # Combine the previous result columns with the selected retrieval node's result columns\n    best_result = pd.concat([previous_result, best_result], axis=1)\n\n    return best_result"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create node_line_dir if not exists\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create summary.csv if not exists\n    summary_file = os.path.join(node_line_dir, \"summary.csv\")\n    if not os.path.exists(summary_file):\n        summary_df = pd.DataFrame(columns=['filename', 'module_name', 'module_params', 'execution_time'])\n        summary_df.to_csv(summary_file, index=False)\n\n    # Run each module\n    for module, params in zip(modules, module_params):\n        module_name = module.__name__\n        logger.info(f\"Running module {module_name} with params {params}\")\n        result_df, execution_time = measure_speed(module)(previous_result, **params)\n        logger.info(f\"Module {module_name} execution time: {execution_time}\")\n        result_df['module_name'] = module_name\n        result_df['module_params'] = params\n        result_df['execution_time'] = execution_time\n\n        # Evaluate the result\n        result_df = evaluate_retrieval_node(result_df, strategies['metrics'])\n\n        # Save the result and summary\n        result_filename = f\"{module_name}_{len(summary_df)}.parquet\"\n        result_df.to_parquet(os.path.join(node_line_dir, result_filename))\n        result_df.to_csv(os.path.join(node_line_dir, f\"{module_name}_{len(summary_df)}.csv\"), index=False)\n        summary_df = pd.concat([summary_df, pd.DataFrame({\n            'filename': [result_filename],\n            'module_name': [module_name],\n            'module_params': [params],\n            'execution_time': [execution_time]\n        })], ignore_index=True)\n        summary_df.to_csv(summary_file, index=False)"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    assert len(modules) == len(module_params), \"modules and module_params must have the same length\"\n\n    node_line_dir = pathlib.Path(node_line_dir)\n    node_line_dir.mkdir(parents=True, exist_ok=True)\n\n    summary_df = pd.DataFrame(columns=['filename', 'module_name', 'module_params', 'execution_time'])\n    results = []\n\n    for module, params in zip(modules, module_params):\n        module_name = module.__name__\n        logger.info(f\"Running {module_name} with params {params}\")\n        result_df = module(**params)\n        execution_time = measure_speed(result_df)\n        logger.info(f\"Execution time: {execution_time}\")\n\n        summary_df = pd.concat([summary_df, pd.DataFrame({\n            'filename': [f\"{module_name}.parquet\"],\n            'module_name': [module_name],\n            'module_params': [params],\n            'execution_time': [execution_time],\n        })], ignore_index=True)\n\n        result_df.to_parquet(os.path.join(node_line_dir, f\"{module_name}.parquet\"))\n        results.append(result_df)\n\n    summary_df.to_csv(os.path.join(node_line_dir, \"summary.csv\"), index=False)\n\n    if 'metrics' in strategies:\n        for result_df in results:\n            result_df = evaluate_retrieval_node(result_df, previous_result, strategies['metrics'])\n\n    if 'speed_threshold' in strategies:\n        results = filter_by_threshold(results, strategies['speed_threshold'])\n\n    if 'select_best_average' in strategies:\n        results = select_best_average(results, strategies['select_best_average'])\n\n    best_result = results[0]\n    for result in results[1:]:\n        best_result = pd.concat"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    assert len(modules) == len(module_params), \"The number of modules and module parameters must match.\"\n    assert len(strategies) > 0, \"At least one strategy must be specified.\"\n    assert 'metrics' in strategies, \"At least one metric must be specified in strategies.\"\n\n    results = []\n    for module, params in zip(modules, module_params):\n        result = module(**params)\n        results.append(result)\n\n    results = measure_speed(results)\n    results = filter_by_threshold(results, strategies)\n    results = evaluate_retrieval_node(results, previous_result, strategies['metrics'])\n    results = select_best_average(results, strategies['metrics'])\n\n    results.to_parquet(os.path.join(node_line_dir, 'result.parquet'))\n    summary_df = load_summary_file(os.path.join(node_line_dir, 'summary.csv'))\n    summary_df = pd.concat([summary_df, results[['filename', 'module_name', 'module_params', 'execution_time'] + strategies['metrics']]])\n    summary_df.to_csv(os.path.join(node_line_dir, 'summary.csv'), index=False)\n\n    return pd.concat([previous_result, results[['retrieved_contents', 'retrieved_ids', 'retrieve_scores']]], axis=1)"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    if not os.path.exists(node_line_dir):\n        pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    result_dfs = []\n    summary_df = pd.DataFrame(columns=['filename', 'module_name', 'module_params', 'execution_time'])\n\n    for module, module_param in zip(modules, module_params):\n        module_name = module.__name__\n        module_param_str = str(module_param)\n        filename = f\"{module_name}_{module_param_str}.parquet\"\n        filepath = os.path.join(node_line_dir, filename)\n        logger.info(f\"Running {module_name} with parameters {module_param_str}...\")\n\n        if os.path.exists(filepath):\n            logger.info(f\"{filename} already exists, loading from disk...\")\n            result_df = pd.read_parquet(filepath)\n        else:\n            logger.info(f\"{filename} does not exist, executing {module_name}...\")\n            result_df = module(**module_param, previous_result=previous_result)\n            result_df.to_parquet(filepath, index=False)\n\n        execution_time = measure_speed(result_df)\n        logger.info(f\"Execution time: {execution_time}\")\n        result_df['execution_time'] = execution_time\n        result_dfs.append(result_df)\n\n        summary_df = pd.concat([summary_df, pd.DataFrame({\n            'filename': [filename],\n            'module_name': [module_name],\n            'module_params': [module_param],\n            'execution_time': [execution_time]\n        })], ignore_index=True)\n\n    summary_df.to_csv(os.path.join(node_line_dir, \"summary.csv\"), index=False)\n\n    result_dfs = filter_by_threshold(result_dfs, strategies)\n    best_result_"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Initialize the result dataframe with the previous result columns\n    result_df = previous_result.copy()\n\n    # Initialize the summary dataframe with column names\n    summary_df = pd.DataFrame(columns=['filename', 'module_name', 'module_params', 'execution_time'])\n\n    # Iterate over each module and its corresponding parameters\n    for module, params in zip(modules, module_params):\n        module_name = module.__name__\n        module_params_str = str(params)\n        filename = f\"{module_name}_{module_params_str}.parquet\"\n        filepath = os.path.join(node_line_dir, filename)\n        logger.info(f\"Running {module_name} with parameters {module_params_str}...\")\n\n        # Check if the result file already exists\n        if os.path.exists(filepath):\n            logger.info(f\"{filename} already exists. Loading from disk...\")\n            result = pd.read_parquet(filepath)\n            execution_time = load_summary_file(os.path.join(node_line_dir, \"summary.csv\")) \\\n                .loc[lambda row: (row['filename'] == filename) & (row['module_name'] == module_name) & (row['module_params'] == module_params_str), 'execution_time'].iloc[0]\n        else:\n            # Run the module and measure the execution time\n            result, execution_time = measure_speed(module)(previous_result, **params)\n\n            # Save the result and execution time to disk\n            pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n            result.to_parquet(filepath)\n            pd.DataFrame([[filename, module_name, module_params_str, execution_time]],\n                         columns=['filename', 'module_name', 'module_params', 'execution_time']).to_csv(\n                os.path.join(node_line_dir, \"summary.csv\"), mode='a', header=False, index=False)"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    assert len(modules) == len(module_params)\n    assert len(modules) > 0\n\n    module_names = [module.__name__ for module in modules]\n    module_params_str = [str(param) for param in module_params]\n    module_params_str = [param.replace(' ', '') for param in module_params_str]\n    module_params_str = [param.replace('\\'', '') for param in module_params_str]\n    filenames = [f\"{name}_{param}.parquet\" for name, param in zip(module_names, module_params_str)]\n    summary_df = pd.DataFrame(columns=['filename', 'module_name', 'module_params', 'execution_time'])\n\n    for module, params, filename in zip(modules, module_params, filenames):\n        result_df = module(**params)\n        execution_time = measure_speed(module, params)\n        summary_df = summary_df.append({'filename': filename,\n                                       'module_name': module.__name__,\n                                       'module_params': params,\n                                       'execution_time': execution_time}, ignore_index=True)\n        result_df.to_parquet(os.path.join(node_line_dir, filename))\n\n    summary_df.to_csv(os.path.join(node_line_dir, \"summary.csv\"), index=False)\n\n    # Evaluate and select best result\n    metrics = strategies['metrics']\n    speed_threshold = strategies['speed_threshold']\n    result_dfs = [pd.read_parquet(os.path.join(node_line_dir, filename)) for filename in filenames]\n    result_dfs = [evaluate_retrieval_node(df, previous_result, metrics) for df in result_dfs]\n    result_dfs = filter_by_threshold(result_dfs, speed_threshold)\n    best_result_df = select_best_average(result_dfs, metrics)\n    best_result_df = pd.concat([previous"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    assert len(modules) == len(module_params)\n\n    # Create node_line_dir if not exists\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Execute each module and save result\n    result_dfs = []\n    for module, params in zip(modules, module_params):\n        result_df = module(**params)\n        result_dfs.append(result_df)\n        result_df.to_parquet(os.path.join(node_line_dir, f\"{module.__name__}.parquet\"))\n\n    # Evaluate each module result and save evaluation summary\n    summary_df = pd.DataFrame()\n    for result_df in result_dfs:\n        module_name = result_df.attrs['module_name']\n        module_params = result_df.attrs['module_params']\n        execution_time = result_df.attrs['execution_time']\n        evaluation_df = evaluate_retrieval_node(result_df, previous_result, strategies['metrics'])\n        summary_df = pd.concat([summary_df, pd.DataFrame({\n            'filename': [f\"{module_name}.parquet\"],\n            'module_name': [module_name],\n            'module_params': [module_params],\n            'execution_time': [execution_time],\n            **evaluation_df.to_dict('records')[0],\n        })])\n    summary_df.to_csv(os.path.join(node_line_dir, \"summary.csv\"), index=False)\n\n    # Select best result\n    if strategies['speed_threshold'] is not None:\n        result_dfs = list(map(lambda df: filter_by_threshold(df, strategies['speed_threshold']), result_dfs))\n    best_result_df = select_best_average(result_dfs, strategies['metrics'])\n\n    # Combine best result with previous result\n    best_result_df = measure_speed(best_result_df)"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    if not os.path.exists(node_line_dir):\n        pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Execute modules\n    module_results = list(map(lambda module, params: module(**params), modules, module_params))\n\n    # Evaluate modules\n    module_results_with_metrics = list(map(\n        lambda module_result: evaluate_retrieval_node(module_result, strategies['metrics'], previous_result),\n        module_results))\n\n    # Save results\n    for module_result, module_name in zip(module_results_with_metrics, strategies['module_names']):\n        module_result.to_parquet(os.path.join(node_line_dir, f\"{module_name}.parquet\"))\n\n    # Measure execution times\n    execution_times = measure_speed(module_results)\n\n    # Save execution times\n    execution_times_df = pd.DataFrame(execution_times, index=strategies['module_names'])\n    execution_times_df.to_csv(os.path.join(node_line_dir, \"execution_times.csv\"))\n\n    # Select best result\n    best_result_df = select_best_average(module_results_with_metrics, strategies['metrics'])\n\n    # Filter by speed threshold\n    best_result_df = filter_by_threshold(best_result_df, execution_times_df, strategies['speed_thresholds'])\n\n    # Save summary\n    summary_df = pd.DataFrame(\n        {\n            'filename': [f\"{module_name}.parquet\" for module_name in strategies['module_names']],\n            'module_name': strategies['module_names'],\n            'module_params': module_params,\n            'execution_time': execution_times,\n        }\n    )\n    summary_df = pd.concat([summary_df, best_result_df.drop(columns=['retrieved_contents', 'retrieved_ids', 'retrieve_"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # create directory for this node line\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # run each module with given parameters\n    results = []\n    for module, module_param in zip(modules, module_params):\n        module_name = module.__name__\n        result = module(**module_param)\n        results.append(result)\n\n    # measure execution time for each module\n    execution_times = measure_speed(results)\n\n    # evaluate each module\n    metrics = strategies['metrics']\n    evaluation_results = list(map(lambda result: evaluate_retrieval_node(result, previous_result, metrics), results))\n\n    # add execution time and evaluation results to each module\n    results = list(map(lambda result, execution_time, evaluation_result: result.assign(\n        execution_time=execution_time, **evaluation_result), results, execution_times, evaluation_results))\n\n    # filter results by speed threshold\n    speed_threshold = strategies.get('speed_threshold', None)\n    if speed_threshold is not None:\n        results = list(map(lambda result, execution_time: filter_by_threshold(result, execution_time, speed_threshold),\n                           results, execution_times))\n\n    # save results and execution times to disk\n    for result, execution_time in zip(results, execution_times):\n        filename = result['filename'].iloc[0]\n        result.to_parquet(os.path.join(node_line_dir, filename))\n\n    # save summary to disk\n    summary = pd.DataFrame({\n        'filename': [result['filename'].iloc[0] for result in results],\n        'module_name': [result['module_name'].iloc[0] for result in results],\n        'module_params': [result['module_params'].iloc[0] for result in results],\n        'execution_time': execution_times,\n        **{metric: [evaluation_result[metric] for evaluation_result in evaluation_results] for metric in metrics}\n    })"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create node directory\n    node_dir = pathlib.Path(node_line_dir, \"retrieval_node\")\n    node_dir.mkdir(parents=True, exist_ok=True)\n\n    # Evaluate each module\n    result_dfs = []\n    for module, module_param in zip(modules, module_params):\n        module_name = module.__name__\n        module_dir = pathlib.Path(node_dir, module_name)\n        module_dir.mkdir(parents=True, exist_ok=True)\n        result_df = module(**module_param)\n        result_df = evaluate_retrieval_node(result_df, previous_result, strategies['metrics'])\n        result_dfs.append(result_df)\n        result_df.to_parquet(pathlib.Path(module_dir, \"result.parquet\"))\n\n    # Combine the results and evaluate the combined result\n    result_df = pd.concat(result_dfs, ignore_index=True)\n    result_df = evaluate_retrieval_node(result_df, previous_result, strategies['metrics'])\n    result_df.to_parquet(pathlib.Path(node_dir, \"result.parquet\"))\n\n    # Measure execution times\n    execution_times = measure_speed(result_dfs, strategies['speed_threshold'])\n    execution_times = pd.DataFrame(execution_times, columns=['module_name', 'execution_time'])\n\n    # Select the best result\n    best_result_df = select_best_average(result_df, strategies['metrics'])\n\n    # Save the results and summaries\n    summary_df = pd.DataFrame({\n        'filename': result_df['filename'],\n        'module_name': result_df['module_name'],\n        'module_params': result_df['module_params'],\n        'execution_time': execution_times['execution_time'],\n    })\n    summary_df = pd.concat([summary_df, result_df.drop(columns=['filename', 'module_name"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the node line directory if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Initialize a list to store the results\n    results = []\n\n    # Iterate over each module and its parameters\n    for module, params in zip(modules, module_params):\n        # Get the module name\n        module_name = module.__name__\n\n        # Run the module with the given parameters\n        result = module(previous_result, **params)\n\n        # Measure the execution time of the module\n        execution_time = measure_speed(module, previous_result, **params)\n\n        # Evaluate the result using the specified metrics\n        evaluation_result = evaluate_retrieval_node(result, previous_result, strategies['metrics'])\n\n        # Add the module name, parameters, and evaluation result to the results list\n        results.append((module_name, params, execution_time, evaluation_result))\n\n    # Filter the results based on the speed threshold\n    results = filter_by_threshold(results, strategies['speed_threshold'])\n\n    # Select the best result based on the specified evaluation metrics\n    best_result = select_best_average(results, strategies['metrics'])\n\n    # Save the best result to disk\n    best_result.to_parquet(os.path.join(node_line_dir, \"best_result.parquet\"))\n\n    # Save the summary of results to disk\n    summary_df = pd.DataFrame(results, columns=['module_name', 'module_params', 'execution_time', 'evaluation_result'])\n    summary_df['filename'] = summary_df.apply(lambda row: f\"{row['module_name']}_{str(row['module_params']).replace(' ', '').replace('{', '').replace('}', '').replace(':', '').replace(',', '')}.parquet\", axis=1)\n    summary_df.to_csv(os.path.join(node_line_dir, \"summary.csv\"), index=False)\n\n    # Return the best"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the directory for this node line if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Evaluate each module and measure execution time\n    results = []\n    for module, params in zip(modules, module_params):\n        module_name = module.__name__\n        module_dir = os.path.join(node_line_dir, module_name)\n        pathlib.Path(module_dir).mkdir(parents=True, exist_ok=True)\n        result_df = module(**params)\n        execution_time = measure_speed(module, params)\n        result_df['execution_time'] = execution_time\n        results.append((module_name, module_dir, params, result_df))\n\n    # Evaluate each result and select the best one\n    summary = []\n    for module_name, module_dir, params, result_df in results:\n        result_df['module_name'] = module_name\n        result_df['module_params'] = params\n        result_df['previous_result'] = previous_result\n        result_df = evaluate_retrieval_node(result_df, strategies['metrics'])\n        result_df.to_parquet(os.path.join(module_dir, f\"{module_name}.parquet\"))\n        summary.append((module_name, params, execution_time, result_df))\n\n    # Filter the summary by speed threshold\n    if 'speed_threshold' in strategies:\n        summary = filter_by_threshold(summary, strategies['speed_threshold'])\n\n    # Select the best result based on the evaluation metrics\n    best_result = select_best_average(summary, strategies['metrics'])\n    best_result_df = best_result[3]\n\n    # Save the summary to disk\n    summary_df = pd.DataFrame(summary, columns=['module_name', 'module_params', 'execution_time', 'result_df'])\n    summary_df.to_csv(os.path.join(node_line_dir, \""}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the node line directory if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Get the metrics from the strategies\n    metrics = strategies.get('metrics')\n    if metrics is None:\n        raise ValueError(\"No 'metrics' key found in the strategies dictionary.\")\n\n    # Initialize the best result DataFrame with the previous result columns\n    best_result = previous_result.copy()\n\n    # Initialize the best execution time and best evaluation metrics\n    best_execution_time = float('inf')\n    best_evaluation_metrics = None\n\n    # Initialize the best module name and best module parameters\n    best_module_name = None\n    best_module_params = None\n\n    # Initialize the best result filename\n    best_result_filename = None\n\n    # Iterate over each module and its parameters\n    for module, module_param in zip(modules, module_params):\n        # Get the module name\n        module_name = module.__name__\n\n        # Log the module name and parameters\n        logger.info(f\"Running module: {module_name} with parameters: {module_param}\")\n\n        # Run the module with the given parameters and measure its execution time\n        result, execution_time = measure_speed(module)(previous_result, **module_param)\n\n        # Evaluate the module result and get the evaluation metrics\n        evaluation_metrics = evaluate_retrieval_node(result, previous_result, metrics)\n\n        # Log the execution time and evaluation metrics\n        logger.info(f\"Execution time: {execution_time:.4f} seconds\")\n        logger.info(f\"Evaluation metrics: {evaluation_metrics}\")\n\n        # Check if the execution time is within the specified threshold\n        if filter_by_threshold(execution_time, strategies):\n            # If the execution time is within the threshold, update the best result and metrics\n            best_result = result\n            best_execution_time = execution_time\n            best_evaluation_metrics = evaluation_metrics\n            best_module_name"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create a directory for the query expansion node if it does not exist\n    node_dir = os.path.join(node_line_dir, 'query_expansion')\n    pathlib.Path(node_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the query expansion node results if it does not exist\n    node_results_dir = os.path.join(node_dir, 'results')\n    pathlib.Path(node_results_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the query expansion node summaries if it does not exist\n    node_summaries_dir = os.path.join(node_dir, 'summaries')\n    pathlib.Path(node_summaries_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the query expansion node best result if it does not exist\n    node_best_result_dir = os.path.join(node_dir, 'best_result')\n    pathlib.Path(node_best_result_dir).mkdir(parents=True, exist_ok=True)\n\n    # Evaluate and select the best query expansion module\n    best_result, best_summary = evaluate_query_expansion_node(modules, module_params, previous_result, node_results_dir,\n                                                             node_summaries_dir, strategies)\n\n    # Save the best result to the best result directory\n    best_result.to_csv(os.path.join(node_best_result_dir, 'best_result.csv'), index=False)\n\n    return best_result"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    logger.info(f\"Running query expansion node\")\n    os.makedirs(node_line_dir, exist_ok=True)\n    results = []\n    for module, params in zip(modules, module_params):\n        logger.info(f\"Running module {module.__name__}\")\n        module_dir = os.path.join(node_line_dir, module.__name__)\n        os.makedirs(module_dir, exist_ok=True)\n        module_result = module(previous_result=previous_result, **params)\n        module_result.to_csv(os.path.join(module_dir, 'result.csv'), index=False)\n        results.append(module_result)\n    summary = measure_speed(results)\n    summary.to_csv(os.path.join(node_line_dir, 'summary.csv'), index=False)\n    best_result = select_best_average(results, strategies['metrics'])\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'), index=False)\n    return best_result"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create the directory if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Evaluate and select the best module\n    best_result, best_result_summary = select_best_query_expansion_node(modules, module_params, previous_result, strategies)\n\n    # Save the results and summary\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'), index=False)\n    best_result_summary.to_csv(os.path.join(node_line_dir, 'best_result_summary.csv'), index=False)\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # create directory for the node\n    node_dir = os.path.join(node_line_dir, 'query_expansion')\n    pathlib.Path(node_dir).mkdir(parents=True, exist_ok=True)\n\n    # evaluate all modules\n    evaluation_results = list(map(lambda x: evaluate_query_expansion_node(*x, previous_result=previous_result),\n                                  zip(modules, module_params)))\n\n    # save results\n    for i, result in enumerate(evaluation_results):\n        result.to_csv(os.path.join(node_dir, f'result_{i}.csv'), index=False)\n\n    # save summary\n    summary = pd.DataFrame(evaluation_results)\n    summary.to_csv(os.path.join(node_dir, 'summary.csv'), index=False)\n\n    # select best result\n    best_result, best_summary = select_best_average(evaluation_results, strategies['metrics'])\n    best_result.to_csv(os.path.join(node_dir, 'best_result.csv'), index=False)\n    best_summary.to_csv(os.path.join(node_dir, 'best_summary.csv'), index=False)\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    module_combinations = list(zip(*make_retrieval_callable_params(strategies)))\n\n    logger.info(f\"Evaluating {len(module_combinations)} query expansion modules...\")\n    evaluation_results = list(map(lambda x: evaluate_one_query_expansion_node(*x),\n                                  zip(module_combinations[0], module_combinations[1],\n                                      [previous_result] * len(module_combinations[0]),\n                                      [previous_result['queries']] * len(module_combinations[0]),\n                                      [previous_result['ground_truth']] * len(module_combinations[0]),\n                                      [strategies['metrics']] * len(module_combinations[0]),\n                                      [node_line_dir] * len(module_combinations[0]))))\n\n    logger.info(f\"Evaluating {len(module_combinations)} query expansion modules...\")\n    evaluation_results = list(map(lambda x: measure_speed(x, strategies['speed_thresholds']),\n                                  evaluation_results))\n\n    logger.info(f\"Evaluating {len(module_combinations)} query expansion modules...\")\n    evaluation_results = list(map(lambda x: filter_by_threshold(x, strategies['thresholds']),\n                                  evaluation_results))\n\n    logger.info(f\"Evaluating {len(module_combinations)} query expansion modules...\")\n    best_result, best_result_summary = select_best_average(evaluation_results, strategies['metrics'])\n\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'), index=False)\n    best_result_summary.to_csv(os.path.join(node_line_dir, 'best_result_summary.csv'), index=False)\n\n    return best_result"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    logger.info(\"Running Query Expansion Node\")\n    module_combinations = make_combinations(modules, module_params)\n    module_combinations = list(zip(*module_combinations))\n    module_combinations = list(map(lambda x: list(zip(*x)), module_combinations))\n    module_combinations = list(map(lambda x: (list(x[0]), list(x[1])), module_combinations))\n    module_combinations = list(map(lambda x: (x[0][0], x[1]), module_combinations))\n    module_combinations = list(map(lambda x: (x[0](**x[1]), x[1]), module_combinations))\n    module_combinations = list(map(lambda x: (x[0], x[1], x[0](**x[1])), module_combinations))\n    module_combinations = list(map(lambda x: (x[0], x[1], x[2](previous_result)), module_combinations))\n    module_combinations = list(map(lambda x: (x[0], x[1], measure_speed(x[2])), module_combinations))\n    module_combinations = list(map(lambda x: (x[0], x[1], filter_by_threshold(x[2], strategies)), module_combinations))\n    module_combinations = list(map(lambda x: (x[0], x[1], x[2].to_dict()), module_combinations))\n    module_combinations = list(map(lambda x: (x[0], x[1], x[2]), module_combinations))\n    module_combinations = list(filter(lambda x: x[2] is not None, module_combinations))\n    module_combinations = list(map(lambda x: (x[0], x[1], x[2]['speed']), module_combinations))\n    module_combinations = list(map(lambda x: (x"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    module_params = list(map(lambda x: x[0] | x[1], zip(module_params, modules)))\n    expanded_queries = list(map(lambda x: x[0](previous_result, **x[1]), zip(modules, module_params)))\n\n    metrics = strategies['metrics']\n    speed_thresholds = strategies['speed_thresholds']\n    retrieval_gt = strategies['retrieval_gt']\n\n    retrieval_modules, retrieval_params = make_retrieval_callable_params(strategies)\n\n    execution_times = measure_speed(modules, module_params, previous_result)\n    execution_times.to_csv(os.path.join(node_line_dir, 'execution_times.csv'))\n\n    execution_times = filter_by_threshold(execution_times, speed_thresholds)\n    execution_times.to_csv(os.path.join(node_line_dir, 'filtered_execution_times.csv'))\n\n    evaluation_results = list(map(lambda x: evaluate_one_query_expansion_node(retrieval_modules,\n                                                                            retrieval_params,\n                                                                            expanded_queries,\n                                                                            retrieval_gt,\n                                                                            metrics,\n                                                                            node_line_dir,\n                                                                            previous_result),\n                                  execution_times.iterrows()))\n\n    evaluation_results = pd.concat(evaluation_results)\n    evaluation_results.to_csv(os.path.join(node_line_dir, 'evaluation_results.csv'))\n\n    best_result, _ = select_best_average(evaluation_results, metrics)\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'))\n\n    return best_result"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Make sure the directory exists\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Evaluate the modules and measure their execution times\n    evaluation_results = list(map(lambda x: evaluate_query_expansion_node(*x),\n                                  zip(modules, module_params, [previous_result] * len(modules))))\n\n    # Save the results to a CSV file\n    results_df = pd.concat(evaluation_results)\n    results_df.to_csv(os.path.join(node_line_dir, 'results.csv'), index=False)\n\n    # Filter the results by speed threshold\n    speed_threshold = strategies.get('speed_threshold', None)\n    if speed_threshold is not None:\n        evaluation_results = list(filter(lambda x: filter_by_threshold(x, speed_threshold), evaluation_results))\n\n    # Select the best result based on the specified strategies\n    best_result, _ = select_best_average(evaluation_results, strategies['metrics'])\n\n    # Save the best result to a CSV file\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'), index=False)\n\n    # Save the summary to a CSV file\n    summary_df = pd.DataFrame(evaluation_results)\n    summary_df.to_csv(os.path.join(node_line_dir, 'summary.csv'), index=False)\n\n    return best_result"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    logger.info(\"Running query expansion node\")\n    strategies = deepcopy(strategies)\n    metrics = strategies.pop('metrics', ['retrieval_f1', 'retrieval_recall'])\n    speed_thresholds = strategies.pop('speed_thresholds', {})\n    project_dir = os.path.join(node_line_dir, 'query_expansion')\n    pathlib.Path(project_dir).mkdir(parents=True, exist_ok=True)\n    expanded_queries = list(map(lambda x: x(previous_result), modules))\n    expanded_queries = list(map(lambda x: x.queries.to_list(), expanded_queries))\n    retrieval_funcs, retrieval_params = make_retrieval_callable_params(strategies)\n    result = evaluate_one_query_expansion_node(retrieval_funcs, retrieval_params, expanded_queries, previous_result.queries.to_list(), metrics, project_dir, previous_result)\n    result = measure_speed(result, speed_thresholds)\n    result = filter_by_threshold(result, speed_thresholds)\n    result = select_best_average(result, metrics)\n    result[0].to_csv(os.path.join(project_dir, 'result.csv'), index=False)\n    result[1].to_csv(os.path.join(project_dir, 'summary.csv'), index=False)\n    return result[0]"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create a directory for the query expansion node if it doesn't exist\n    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Create a directory for the results of the query expansion node if it doesn't exist\n    node_result_dir = os.path.join(node_line_dir, \"results\")\n    if not os.path.exists(node_result_dir):\n        os.makedirs(node_result_dir)\n\n    # Create a directory for the summaries of the query expansion node if it doesn't exist\n    node_summary_dir = os.path.join(node_line_dir, \"summaries\")\n    if not os.path.exists(node_summary_dir):\n        os.makedirs(node_summary_dir)\n\n    # Initialize a list to store the results of all query expansion modules\n    results = []\n\n    # Loop through each query expansion module and its parameters\n    for module, params in zip(modules, module_params):\n        # Get the name of the query expansion module\n        module_name = module.__name__\n        # Create a directory for the results of the current query expansion module\n        module_result_dir = os.path.join(node_result_dir, module_name)\n        if not os.path.exists(module_result_dir):\n            os.makedirs(module_result_dir)\n\n        # Create a directory for the summaries of the current query expansion module\n        module_summary_dir = os.path.join(node_summary_dir, module_name)\n        if not os.path.exists(module_summary_dir):\n            os.makedirs(module_summary_dir)\n\n        # Run the current query expansion module with the given parameters\n        result = module(previous_result=previous_result, **params)\n\n        # Save the results of the current query expansion module to a file\n        result.to_csv(os.path.join(module_result_dir, \"result.csv\"), index=False)\n\n        # Append the results of the current query expansion"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Evaluate each module with given parameters\n    query_expansion_results = list(map(lambda x: x[0](previous_result, **x[1]),\n                                      zip(modules, module_params)))\n\n    # Measure execution times of each module\n    query_expansion_results = measure_speed(query_expansion_results)\n\n    # Evaluate the performance of each module based on specified strategies\n    evaluation_results = list(map(lambda x: evaluate_retrieval_node(x, previous_result['queries'], strategies['metrics']),\n                                  query_expansion_results))\n\n    # Save the results and summaries\n    for result in evaluation_results:\n        result.to_csv(os.path.join(node_line_dir, f\"{result.name}.csv\"))\n\n    # Select the best result based on evaluation criteria\n    best_result, best_result_summary = select_best_average(evaluation_results, strategies['metrics'])\n    best_result.to_csv(os.path.join(node_line_dir, f\"{best_result.name}.csv\"))\n    best_result_summary.to_csv(os.path.join(node_line_dir, f\"{best_result_summary.name}.csv\"))\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    logger.info(f\"Running query expansion node\")\n    node_dir = os.path.join(node_line_dir, 'query_expansion')\n    os.makedirs(node_dir, exist_ok=True)\n    expanded_queries = list(map(lambda module, params: module(previous_result, **params),\n                                modules, module_params))\n    expanded_queries = list(map(lambda x: list(map(lambda y: y[0], x)), expanded_queries))\n    execution_times = measure_speed(modules, module_params, previous_result)\n    strategies = strategies.copy()\n    metrics = strategies.pop('metrics')\n    speed_thresholds = strategies.pop('speed_thresholds')\n    execution_times = filter_by_threshold(execution_times, speed_thresholds)\n    strategies.update(execution_times)\n    best_result, best_result_summary = select_best_average(expanded_queries, metrics, **strategies)\n    best_result_summary.to_csv(os.path.join(node_dir, 'summary.csv'))\n    best_result.to_csv(os.path.join(node_dir, 'result.csv'))\n    return best_result"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    logger.info(\"Running query expansion node\")\n    # Create directory for query expansion results\n    node_dir = os.path.join(node_line_dir, \"query_expansion\")\n    pathlib.Path(node_dir).mkdir(parents=True, exist_ok=True)\n\n    # Evaluate each query expansion module and save results\n    evaluation_results = list(map(lambda x: evaluate_query_expansion_node(*x, node_dir, strategies, previous_result),\n                                  zip(modules, module_params)))\n\n    # Save evaluation results to file\n    evaluation_summary = pd.concat(evaluation_results)\n    evaluation_summary.to_csv(os.path.join(node_dir, \"evaluation_summary.csv\"), index=False)\n\n    # Select best query expansion module and save result\n    best_result, best_module_name = select_best_average(evaluation_results, strategies['metrics'])\n    best_result.to_csv(os.path.join(node_dir, \"best_result.csv\"), index=False)\n\n    # Save best module name to file\n    with open(os.path.join(node_dir, \"best_module_name.txt\"), \"w\") as f:\n        f.write(best_module_name)\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    logger.info(\"Running query expansion node\")\n    node_dir = os.path.join(node_line_dir, \"query_expansion\")\n    os.makedirs(node_dir, exist_ok=True)\n    metrics = strategies['metrics']\n    speed_thresholds = strategies.get('speed_thresholds', {})\n    best_result = None\n    best_result_summary = None\n    best_result_speed = None\n\n    for module, module_param in zip(modules, module_params):\n        logger.info(f\"Running query expansion module: {module.__name__}\")\n        module_result = module(previous_result, **module_param)\n        module_result_summary = measure_speed(module_result, module, module_param, speed_thresholds)\n        module_result_summary['module'] = module.__name__\n        module_result_summary['module_params'] = str(module_param)\n        module_result_summary['result'] = module_result\n        module_result_summary.to_csv(os.path.join(node_dir, f\"{module.__name__}.csv\"))\n        if best_result is None:\n            best_result = module_result\n            best_result_summary = module_result_summary\n            best_result_speed = module_result_summary['speed']\n        else:\n            if best_result_speed > module_result_summary['speed']:\n                best_result = module_result\n                best_result_summary = module_result_summary\n                best_result_speed = module_result_summary['speed']\n    best_result_summary.to_csv(os.path.join(node_dir, \"best_result_summary.csv\"))\n    best_result.to_csv(os.path.join(node_dir, \"best_result.csv\"))\n    return best_result\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    strategies = deepcopy(strategies)\n    metrics = strategies.pop('metrics', None)\n    if metrics is None:\n        metrics = ['retrieval_f1', 'retrieval_recall']\n    speed_thresholds = strategies.pop('speed_thresholds', None)\n    if speed_thresholds is None:\n        speed_thresholds = {\n            'retrieval_f1': 0.001,\n            'retrieval_recall': 0.001\n        }\n    node_dir = os.path.join(node_line_dir, 'query_expansion')\n    os.makedirs(node_dir, exist_ok=True)\n    logger.info('Running query expansion node')\n    results = []\n    for module, params in zip(modules, module_params):\n        module_name = module.__name__\n        module_dir = os.path.join(node_dir, module_name)\n        os.makedirs(module_dir, exist_ok=True)\n        logger.info(f'Running module: {module_name}')\n        module_result = module(previous_result=previous_result, **params)\n        module_result['module_name'] = module_name\n        module_result['params'] = params\n        results.append(module_result)\n        module_result.to_csv(os.path.join(module_dir, 'result.csv'), index=False)\n    logger.info('Evaluating query expansion node')\n    evaluation_results = list(map(lambda x: evaluate_query_expansion_node(x, metrics), results))\n    evaluation_results = list(map(lambda x: measure_speed(x, speed_thresholds), evaluation_results))\n    evaluation_results = list(map(lambda x: filter_by_threshold(x, speed_thresholds), evaluation_results))\n    best_result, _ = select_best_average(evaluation_results, metrics)\n    best_result.to_csv(os.path.join(node_dir, 'best_result.csv'), index"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    metrics = strategies['metrics']\n    speed_threshold = strategies['speed_thresholds']\n    best_result = None\n    best_result_summary = None\n    best_result_speed = None\n    best_result_name = None\n    best_result_params = None\n    best_result_metrics = None\n    best_result_path = None\n    best_result_summary_path = None\n    best_result_speed_path = None\n\n    for module, params in zip(modules, module_params):\n        module_name = module.__name__\n        logger.info(f\"Evaluating {module_name}\")\n        result_path = os.path.join(node_line_dir, f\"{module_name}_result.csv\")\n        result_summary_path = os.path.join(node_line_dir, f\"{module_name}_summary.csv\")\n        result_speed_path = os.path.join(node_line_dir, f\"{module_name}_speed.csv\")\n\n        # Evaluate the module\n        result = evaluate_query_expansion_node(module, params, previous_result, metrics, result_path)\n\n        # Save the result\n        result.to_csv(result_path, index=False)\n\n        # Measure the speed\n        speed = measure_speed(module, params, previous_result)\n\n        # Save the speed\n        speed.to_csv(result_speed_path, index=False)\n\n        # Filter by speed threshold\n        result = filter_by_threshold(result, speed, speed_threshold)\n\n        # Save the filtered result\n        result.to_csv(result_path, index=False)\n\n        # Evaluate the filtered result\n        result_summary = evaluate_retrieval_node(result, previous_result, metrics)\n\n        # Save the result summary\n        result_summary.to_csv(result_summary_path, index=False)\n\n        # Select the best result\n        if best_result is None or result_summary[metrics].mean() > best_result_summary[metrics].mean"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    module_names = list(map(lambda x: x.__name__, modules))\n    module_params = list(map(lambda x: [x], module_params))\n    module_combinations = list(map(lambda x: make_combinations(*x), zip(modules, module_params)))\n    module_combinations = list(map(lambda x: list(zip(*x)), module_combinations))\n    module_combinations = list(map(lambda x: list(zip(*x)), module_combinations))\n    module_combinations = list(map(lambda x: list(zip(*x)), module_combinations))\n    module_combinations = list(map(lambda x: list(zip(*x)), module_combinations))\n\n    logger.info(f\"Evaluating {len(module_combinations)} combinations of query expansion modules\")\n    execution_times = measure_speed(module_combinations, previous_result, strategies['speed_thresholds'])\n    execution_times = pd.DataFrame(execution_times, columns=['module_name', 'execution_time'])\n    execution_times.to_csv(os.path.join(node_line_dir, 'execution_times.csv'), index=False)\n\n    module_combinations = filter_by_threshold(module_combinations, execution_times, strategies['speed_thresholds'])\n    logger.info(f\"Evaluating {len(module_combinations)} combinations of query expansion modules after filtering\")\n\n    evaluation_results = list(map(lambda x: x[0](previous_result=previous_result, **x[1]), module_combinations))\n    evaluation_results = list(map(lambda x: x.assign(module_name=x['module_name']), evaluation_results))\n    evaluation_results = pd.concat(evaluation_results)\n    evaluation_results.to_csv(os.path.join(node_line_dir, 'evaluation_results.csv'), index=False)\n\n    best_result, _ = select_best_average(evaluation_results, strategies['"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create the node directory if it doesn't exist\n    node_dir = os.path.join(node_line_dir, 'query_expansion')\n    pathlib.Path(node_dir).mkdir(parents=True, exist_ok=True)\n\n    # Get the metrics and speed thresholds from the strategies\n    metrics = strategies.get('metrics', [])\n    speed_thresholds = strategies.get('speed_thresholds', {})\n\n    # Evaluate and select the best module for each query\n    results = []\n    for query in previous_result['queries']:\n        # Create a dataframe with the query as the only entry\n        query_df = pd.DataFrame({'queries': [query]})\n        # Evaluate and select the best module for the query\n        best_result = evaluate_one_query_expansion_node(modules, module_params, query_df, previous_result,\n                                                        metrics, node_dir, speed_thresholds)\n        # Append the best result to the results list\n        results.append(best_result)\n\n    # Concatenate the results into a single dataframe\n    results = pd.concat(results)\n\n    # Save the results to a CSV file\n    results.to_csv(os.path.join(node_dir, 'results.csv'), index=False)\n\n    # Save the summary to a JSON file\n    summary = {\n        'best_module': best_result['module_type'][0],\n        'best_params': best_result['module_params'][0],\n        'best_score': best_result['score'][0],\n        'execution_time': best_result['execution_time'][0],\n        'metrics': metrics,\n        'speed_thresholds': speed_thresholds,\n    }\n    with open(os.path.join(node_dir, 'summary.json'), 'w') as f:\n        json.dump(summary, f, indent=4)\n\n    # Return the best result\n    return best_result"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    logger.info(\"Starting query expansion node\")\n\n    module_combinations = list(zip(*make_query_expansion_callable_params(strategies)))\n    module_combinations = list(zip(module_combinations[0], module_combinations[1], module_combinations[2]))\n    module_combinations = list(map(lambda x: (x[0], x[1], x[2], node_line_dir), module_combinations))\n\n    execution_times = measure_speed(module_combinations)\n    execution_times = pd.DataFrame(execution_times)\n    execution_times.to_csv(os.path.join(node_line_dir, 'execution_times.csv'))\n\n    execution_times = execution_times.sort_values(by=['execution_time'])\n    execution_times = filter_by_threshold(execution_times, strategies)\n\n    module_combinations = list(map(lambda x: (modules[x[0]], module_params[x[0]]),\n                                   zip(execution_times['module_idx'].tolist(), execution_times['param_idx'].tolist())))\n\n    results = list(map(lambda x: x[0](**x[1]), module_combinations))\n    results = pd.concat(results, axis=1)\n    results.to_csv(os.path.join(node_line_dir, 'results.csv'))\n\n    evaluation_results = list(map(lambda x: evaluate_retrieval_node(x, strategies['retrieval_gt'], strategies['metrics']),\n                                  results))\n    best_result, _ = select_best_average(evaluation_results, strategies['metrics'])\n    best_result = pd.concat([previous_result, best_result], axis=1)\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'))\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    logger.info(\"Running Query Expansion Node\")\n    logger.info(f\"Running {len(modules)} modules\")\n    logger.info(f\"Running {len(module_params)} parameter combinations\")\n    logger.info(f\"Running {len(previous_result)} queries\")\n\n    metrics = strategies['metrics']\n    speed_thresholds = strategies['speed_thresholds']\n    best_module_name = strategies['best_module_name']\n    best_module_params = strategies['best_module_params']\n\n    # Get the directory path for the current node\n    node_dir = os.path.join(node_line_dir, \"query_expansion_node\")\n    pathlib.Path(node_dir).mkdir(parents=True, exist_ok=True)\n\n    # Run each module with the given parameters\n    results = []\n    for module, params in zip(modules, module_params):\n        module_result = module(previous_result, **params)\n        results.append(module_result)\n\n    # Measure the execution time of each module\n    execution_times = measure_speed(results)\n\n    # Evaluate the performance of each module based on the specified metrics\n    evaluation_results = list(map(lambda x: evaluate_retrieval_node(x, previous_result, metrics), results))\n\n    # Save the results and execution times to the node directory\n    for i, (result, execution_time) in enumerate(zip(results, execution_times)):\n        result.to_csv(os.path.join(node_dir, f\"module_{i}_result.csv\"), index=False)\n        with open(os.path.join(node_dir, f\"module_{i}_execution_time.txt\"), \"w\") as f:\n            f.write(str(execution_time))\n\n    # Save the evaluation results to the node directory\n    for i, evaluation_result in enumerate(evaluation_results):\n        evaluation_result.to_csv(os.path.join(node_dir, f\"module_{i}_evaluation_result.csv\"), index=False)\n\n    #"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create the directory for this node's output\n    node_dir = os.path.join(node_line_dir, 'prompt_maker')\n    os.makedirs(node_dir, exist_ok=True)\n\n    # Create the directory for this node's output\n    node_dir = os.path.join(node_line_dir, 'prompt_maker')\n    os.makedirs(node_dir, exist_ok=True)\n\n    # Create the directory for this node's output\n    node_dir = os.path.join(node_line_dir, 'prompt_maker')\n    os.makedirs(node_dir, exist_ok=True)\n\n    # Create the directory for this node's output\n    node_dir = os.path.join(node_line_dir, 'prompt_maker')\n    os.makedirs(node_dir, exist_ok=True)\n\n    # Create the directory for this node's output\n    node_dir = os.path.join(node_line_dir, 'prompt_maker')\n    os.makedirs(node_dir, exist_ok=True)\n\n    # Create the directory for this node's output\n    node_dir = os.path.join(node_line_dir, 'prompt_maker')\n    os.makedirs(node_dir, exist_ok=True)\n\n    # Create the directory for this node's output\n    node_dir = os.path.join(node_line_dir, 'prompt_maker')\n    os.makedirs(node_dir, exist_ok=True)\n\n    # Create the directory for this node's output\n    node_dir = os.path.join(node_line_dir, 'prompt_maker')\n    os.makedirs(node_dir, exist_ok=True)\n\n    # Create the directory for this node's output\n    node_dir = os.path.join(node_line_dir, 'prompt_maker')\n    os.makedirs(node_dir, exist_ok=True)\n\n    # Create"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create a directory for the node's output\n    node_dir = os.path.join(node_line_dir, 'prompt_maker')\n    os.makedirs(node_dir, exist_ok=True)\n\n    # Create a directory for the node's output\n    node_dir = os.path.join(node_line_dir, 'prompt_maker')\n    os.makedirs(node_dir, exist_ok=True)\n\n    # Get the generator module and its parameters from the strategies dictionary\n    generator_module_list: Optional[List[Dict]] = strategies.get('generator_modules', None)\n    if generator_module_list is None:\n        generator_module_list = [{\n            'module_type': 'llama_index_llm',\n            'llm': 'openai',\n            'model': 'gpt-3.5-turbo',\n        }]\n\n    # Create a directory for the node's output\n    node_dir = os.path.join(node_line_dir, 'prompt_maker')\n    os.makedirs(node_dir, exist_ok=True)\n\n    # Get the generator module and its parameters from the strategies dictionary\n    generator_module_list: Optional[List[Dict]] = strategies.get('generator_modules', None)\n    if generator_module_list is None:\n        generator_module_list = [{\n            'module_type': 'llama_index_llm',\n            'llm': 'openai',\n            'model': 'gpt-3.5-turbo',\n        }]\n\n    # Get the generator module and its parameters from the strategies dictionary\n    generator_module_list: Optional[List[Dict]] = strategies.get('generator_modules', None)\n    if generator_module_list is None:\n        generator_module_list = [{\n            'module_type': 'llama_index_llm',\n            'llm': 'openai',\n            'model': 'gpt-3.5-turbo',\n        }]\n\n    # Get the generator module"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create a directory for the node's output\n    node_line_dir = os.path.join(node_line_dir, 'prompt_maker')\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create a directory for the prompt maker's output\n    prompt_maker_dir = os.path.join(node_line_dir, 'prompt_maker_output')\n    os.makedirs(prompt_maker_dir, exist_ok=True)\n\n    # Create a directory for the generator's output\n    generator_dir = os.path.join(node_line_dir, 'generator_output')\n    os.makedirs(generator_dir, exist_ok=True)\n\n    # Create a directory for the evaluation results\n    evaluation_dir = os.path.join(node_line_dir, 'evaluation')\n    os.makedirs(evaluation_dir, exist_ok=True)\n\n    # Create a directory for the summary\n    summary_dir = os.path.join(node_line_dir, 'summary')\n    os.makedirs(summary_dir, exist_ok=True)\n\n    # Get the generator module and its parameters\n    generator_module_list: Optional[List[Dict]] = strategies.pop('generator_modules', None)\n    if generator_module_list is None:\n        generator_module_list = [{\n            'module_type': 'llama_index_llm',\n            'llm': 'openai',\n            'model': 'gpt-3.5-turbo',\n        }]\n    node_params = strategies\n    modules = list(map(lambda module_dict: get_support_modules(module_dict.pop('module_type')),\n                       generator_module_list))\n    param_combinations = list(map(lambda module_dict: make_combinations({**module_dict, **node_params}),\n                                  generator_module_list))\n    generator_funcs, generator_params = explode(modules, param_combinations)\n\n    # Create a directory for the generator"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'results'), exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'summary'), exist_ok=True)\n\n    # Generate all combinations of prompts\n    prompts = list(map(lambda x: x[0](**x[1]), zip(modules, module_params)))\n    prompts = list(map(lambda x: x[0](**x[1]), zip(modules, module_params)))\n    prompts = list(map(lambda x: x[0](**x[1]), zip(modules, module_params)))\n    prompts = list(map(lambda x: x[0](**x[1]), zip(modules, module_params)))\n    prompts = list(map(lambda x: x[0](**x[1]), zip(modules, module_params)))\n    prompts = list(map(lambda x: x[0](**x[1]), zip(modules, module_params)))\n    prompts = list(map(lambda x: x[0](**x[1]), zip(modules, module_params)))\n    prompts = list(map(lambda x: x[0](**x[1]), zip(modules, module_params)))\n    prompts = list(map(lambda x: x[0](**x[1]), zip(modules, module_params)))\n    prompts = list(map(lambda x: x[0](**x[1]), zip(modules, module_params)))\n    prompts = list(map(lambda x: x[0](**x[1]), zip(modules, module_params)))\n    prompts = list(map(lambda x: x[0](**x[1]), zip(modules, module_params)))\n    prompts = list(map(lambda x: x[0](**x[1]), zip(modules, module_params)))\n    prompts = list(map(lambda x: x["}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    node_dir = os.path.join(node_line_dir, 'prompt_maker')\n    os.makedirs(node_dir, exist_ok=True)\n\n    # Generate combinations of modules and parameters\n    module_combinations = list(map(lambda module_dict: make_combinations({**module_dict, **module_params}),\n                                  modules))\n    module_combinations = explode(modules, module_combinations)\n\n    # Get generator module specifications\n    generator_module_list: Optional[List[Dict]] = strategies.pop('generator_modules', None)\n    if generator_module_list is None:\n        generator_module_list = [{\n            'module_type': 'llama_index_llm',\n            'llm': 'openai',\n            'model': 'gpt-3.5-turbo',\n        }]\n\n    # Generate combinations of generator modules and parameters\n    generator_combinations = list(map(lambda module_dict: make_combinations({**module_dict, **strategies}),\n                                      generator_module_list))\n    generator_combinations = explode(list(map(lambda module_dict: get_support_modules(module_dict.pop('module_type')),\n                                             generator_module_list)), generator_combinations)\n\n    # Evaluate each prompt maker module and generator combination\n    evaluation_results = []\n    for module_func, module_params, generator_func, generator_params in zip(*module_combinations, *generator_combinations):\n        # Run the prompt maker module\n        prompts, generation_gt = module_func(**module_params)\n        validate_qa_dataset(prompts, generation_gt)\n\n        # Evaluate the prompt maker module with the generator module\n        evaluation_result = evaluate_one_prompt_maker_node(generator_func, generator_params, prompts, generation_gt,\n                                                          strategies['metrics'], node_dir)\n        evaluation_results.append(evaluation_result)\n\n    # Select the best prompt maker"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary subdirectories for the node\n    os.makedirs(node_line_dir, exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'results'), exist_ok=True)\n\n    # Generate combinations of modules and parameters\n    module_combinations = list(map(lambda module_dict: make_combinations({**module_dict, **module_params}), modules))\n    module_combinations = list(map(lambda x: explode(*x), zip(modules, module_combinations)))\n\n    # Evaluate each prompt maker module\n    evaluation_results = []\n    for module, params in module_combinations:\n        result = evaluate_one_prompt_maker_node(module, params, previous_result, node_line_dir)\n        evaluation_results.append(result)\n\n    # Select the best prompt maker module based on the specified strategies\n    best_result = select_best_prompt_maker_node(evaluation_results, strategies)\n\n    # Save the results and summary to the specified directory\n    save_prompt_maker_node_results(evaluation_results, best_result, node_line_dir)\n\n    # Return the combined results of the previous operation and the best prompt maker's output\n    return pd.concat([previous_result, best_result], axis=1)\n\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    os.makedirs(node_line_dir, exist_ok=True)\n    # run prompt maker modules\n    prompt_maker_results = list(map(lambda x: x[0](**x[1]), zip(modules, module_params)))\n    # save results\n    for i, result in enumerate(prompt_maker_results):\n        result.to_csv(os.path.join(node_line_dir, f'prompt_maker_{i}.csv'), index=False)\n    # evaluate prompt maker modules\n    generator_funcs, generator_params = make_generator_callable_params(strategies)\n    evaluation_results = list(map(lambda x: evaluate_one_prompt_maker_node(generator_funcs,\n                                                                         generator_params,\n                                                                         x['prompts'].tolist(),\n                                                                         x['generation_gt'].tolist(),\n                                                                         strategies['metrics'],\n                                                                         node_line_dir),\n                                  prompt_maker_results))\n    # save evaluation results\n    for i, result in enumerate(evaluation_results):\n        result.to_csv(os.path.join(node_line_dir, f'prompt_maker_evaluation_{i}.csv'), index=False)\n    # select best prompt maker module\n    metric_names = list(map(lambda x: x['metric_name'], strategies['metrics'])) if isinstance(strategies['metrics'][0], dict) else strategies['metrics']\n    best_result, best_index = select_best_average(evaluation_results, metric_names)\n    # save best result\n    best_result.to_csv(os.path.join(node_line_dir, 'prompt_maker_best.csv'), index=False)\n    # save summary\n    summary = pd.DataFrame({'module_name': [modules[i].__name__ for i in range(len(modules))],\n                            'module_params': module_params,\n                            'metrics': evaluation_results,\n                            'best': [i == best_index for i in range(len(modules))]})"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # 1. Create a directory for each module and save the module's parameters\n    module_dirs = []\n    for module, params in zip(modules, module_params):\n        module_dir = os.path.join(node_line_dir, module.__name__)\n        if not os.path.exists(module_dir):\n            os.makedirs(module_dir)\n        module_dirs.append(module_dir)\n        with open(os.path.join(module_dir, 'params.json'), 'w') as f:\n            json.dump(params, f)\n\n    # 2. Run each module and save the results in the corresponding directory\n    module_results = []\n    for module, params, module_dir in zip(modules, module_params, module_dirs):\n        result = module(**params)\n        result.to_csv(os.path.join(module_dir, 'result.csv'), index=False)\n        module_results.append(result)\n\n    # 3. Evaluate the results and select the best module based on the specified strategies\n    best_module_dir, best_result = select_best_module(module_results, module_dirs, strategies)\n\n    # 4. Save the best result and the summary to the node's directory\n    best_result.to_csv(os.path.join(node_line_dir, 'result.csv'), index=False)\n    summary = {\n        'best_module': best_module_dir,\n        'execution_times': [result['execution_time'] for result in module_results],\n        'evaluation_metrics': strategies['metrics']\n    }\n    with open(os.path.join(node_line_dir, 'summary.json'), 'w') as f:\n        json.dump(summary, f)\n\n    # 5. Combine the best result with the previous operation's result and return\n    best_result = pd.concat([previous_result, best_"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create directories for the node's output\n    os.makedirs(node_line_dir, exist_ok=True)\n    node_dir = os.path.join(node_line_dir, 'prompt_maker')\n    os.makedirs(node_dir, exist_ok=True)\n\n    # Combine the previous result with the current result\n    combined_result = previous_result.copy()\n\n    # Create a list of tuples containing the module and its parameters\n    module_param_pairs = list(zip(modules, module_params))\n\n    # Filter the module-parameter pairs based on the speed threshold\n    module_param_pairs = filter_by_threshold(module_param_pairs, strategies.get('speed_threshold', None))\n\n    # Measure the speed of each module and filter based on the speed threshold\n    module_param_pairs = measure_speed(module_param_pairs, strategies.get('speed_threshold', None))\n\n    # Select the best module based on the evaluation metrics\n    best_module, best_params = select_best_average(module_param_pairs, strategies['metrics'])\n\n    # Execute the best module and get the results\n    best_result = best_module(**best_params)\n\n    # Combine the previous result with the best result\n    combined_result = pd.concat([combined_result, best_result], axis=1)\n\n    # Save the combined result to a CSV file\n    combined_result.to_csv(os.path.join(node_dir, 'result.csv'), index=False)\n\n    # Save the summary to a JSON file\n    summary = {\n        'best_module': best_module.__name__,\n        'best_params': best_params,\n        'metrics': cast_metrics(strategies['metrics']),\n        'speed_threshold': strategies.get('speed_threshold', None),\n    }\n    with open(os.path.join(node_dir, 'summary.json'), 'w') as f:\n        json.dump(summary, f,"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    if not os.path.exists(node_line_dir):\n        os.makedirs(node_line_dir)\n\n    # Generate prompts\n    prompt_maker_module_list = list(map(lambda module_dict: get_support_modules(module_dict.pop('module_type')),\n                                        modules))\n    prompt_maker_params_list = list(map(lambda module_dict: make_combinations({**module_dict, **module_params}),\n                                        modules))\n    prompt_maker_params_list = explode(prompt_maker_module_list, prompt_maker_params_list)\n    prompts = list(map(lambda x: x[0](**x[1]), zip(prompt_maker_module_list, prompt_maker_params_list)))\n\n    # Evaluate prompts\n    evaluation_results = list(map(lambda x: evaluate_one_prompt_maker_node(\n        generator_funcs=x[0],\n        generator_params=x[1],\n        prompts=prompts,\n        generation_gt=previous_result['generation_gt'].tolist(),\n        metrics=strategies['metrics'],\n        project_dir=node_line_dir,\n    ), zip(\n        make_generator_callable_params(strategies),\n        make_generator_callable_params(strategies),\n    )))\n\n    # Filter by speed\n    if 'speed_threshold' in strategies:\n        evaluation_results = list(map(lambda x: filter_by_threshold(x, strategies['speed_threshold']),\n                                      evaluation_results))\n\n    # Select best prompt maker\n    metric_names = list(map(lambda x: x['metric_name'], strategies['metrics'])) if isinstance(strategies['metrics'][0], dict) else strategies['metrics']\n    best_result, _ = select_best_average(evaluation_results, metric_names)\n    best_result = pd.concat([previous_result, best_result], axis=1)\n\n    # Save results\n    best_result"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create directories\n    os.makedirs(node_line_dir, exist_ok=True)\n    os.makedirs(node_line_dir + '/results', exist_ok=True)\n    os.makedirs(node_line_dir + '/summary', exist_ok=True)\n\n    # Run prompt maker modules\n    prompt_maker_results = list(map(lambda x: x[0](**x[1]), zip(modules, module_params)))\n\n    # Evaluate prompt maker modules\n    if strategies['generator_modules'] is None:\n        generator_module_list = [{\n            'module_type': 'llama_index_llm',\n            'llm': 'openai',\n            'model': 'gpt-3.5-turbo',\n        }]\n    else:\n        generator_module_list = strategies['generator_modules']\n    generator_module_list, generator_params = make_generator_callable_params(strategies)\n    generator_funcs = list(map(lambda module_dict: get_support_modules(module_dict.pop('module_type')),\n                               generator_module_list))\n    generator_params = list(map(lambda module_dict: {**module_dict, **strategies}, generator_params))\n    evaluation_results = list(map(lambda x: evaluate_one_prompt_maker_node(generator_funcs,\n                                                                          generator_params,\n                                                                          x[0],\n                                                                          x[1],\n                                                                          strategies['metrics'],\n                                                                          node_line_dir),\n                                  zip(prompt_maker_results, module_params)))\n\n    # Select best prompt maker module\n    metrics = strategies['metrics']\n    metric_names = list(map(lambda x: x['metric_name'], metrics)) if isinstance(metrics[0], dict) else metrics\n    best_result, best_index = select_best_average(evaluation_results, metric_names)\n\n    # Save results and summary\n    best_result.to_csv(node_line_dir + '/results/best_result.csv', index"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Prepare the generator module\n    generator_funcs, generator_params = make_generator_callable_params(strategies)\n\n    # Prepare the prompt maker modules\n    prompt_maker_funcs, prompt_maker_params = make_prompt_maker_callable_params(modules, module_params)\n\n    # Prepare the metrics\n    metrics = cast_metrics(strategies.get('metrics', ['bleu']))\n\n    # Prepare the speed threshold\n    speed_threshold = strategies.get('speed_threshold', 0.0)\n\n    # Prepare the project directory\n    project_dir = pathlib.Path(node_line_dir) / 'project_dir'\n    os.makedirs(project_dir, exist_ok=True)\n\n    # Prepare the result dataframe\n    result_df = pd.DataFrame()\n\n    # Evaluate each prompt maker module\n    for prompt_maker_func, prompt_maker_param in zip(prompt_maker_funcs, prompt_maker_params):\n        # Run the prompt maker module\n        prompts, generation_gt = prompt_maker_func(**prompt_maker_param)\n\n        # Evaluate the prompt maker module\n        result_df = evaluate_one_prompt_maker_node(generator_funcs,\n                                                   generator_params,\n                                                   prompts,\n                                                   generation_gt,\n                                                   metrics,\n                                                   project_dir)\n\n        # Measure the speed of the prompt maker module\n        speed = measure_speed(prompt_maker_func, prompt_maker_param, prompts, generation_gt)\n\n        # Filter the prompt maker module based on the speed threshold\n        if filter_by_threshold(speed, speed_threshold):\n            break\n\n    # Combine the results\n    result_df = pd.concat([previous_result, result_df], axis=1)\n\n    # Save the results\n    result_df.to_csv(os.path."}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create the node directory if it does not exist\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Combine the previous result with the best prompt maker's result\n    best_result = run_prompt_maker_node_without_saving(modules, module_params, previous_result, strategies)\n    combined_result = pd.concat([previous_result, best_result], axis=1)\n\n    # Save the combined result to the node directory\n    combined_result.to_csv(os.path.join(node_line_dir, 'combined_result.csv'), index=False)\n\n    # Save the summary of the best prompt maker's performance to the node directory\n    summary = {\n        'best_prompt_maker': best_result['prompt_maker'].iloc[0],\n        'best_prompt_maker_params': best_result['prompt_maker_params'].iloc[0],\n        'best_prompt_maker_time': best_result['prompt_maker_time'].iloc[0],\n        'best_prompt_maker_metrics': best_result['prompt_maker_metrics'].iloc[0]\n    }\n    with open(os.path.join(node_line_dir, 'summary.txt'), 'w') as f:\n        f.write(str(summary))\n\n    return combined_result\n\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    node_dir = os.path.join(node_line_dir, 'prompt_maker')\n    os.makedirs(node_dir, exist_ok=True)\n\n    prompts = previous_result['prompts'].tolist()\n    generation_gt = previous_result['generation_gt'].tolist()\n    validate_qa_dataset(prompts, generation_gt)\n\n    generator_funcs, generator_params = make_generator_callable_params(strategies)\n    metrics = cast_metrics(strategies.get('metrics', ['rouge']))\n    speed_threshold = strategies.get('speed_threshold', 10)\n\n    # 1. Run all prompt maker modules\n    module_combinations = make_combinations(modules, module_params)\n    all_results = list(map(lambda x: x[0](**x[1]), module_combinations))\n\n    # 2. Evaluate all prompt maker modules\n    all_results = list(map(lambda x: evaluate_one_prompt_maker_node(generator_funcs, generator_params, prompts, generation_gt, metrics, node_dir), all_results))\n    all_results = list(map(lambda x: measure_speed(x, speed_threshold), all_results))\n\n    # 3. Select the best prompt maker module\n    best_result, _ = select_best_average(all_results, metrics)\n\n    # 4. Save the best prompt maker module's result and summary\n    best_result.to_csv(os.path.join(node_dir, 'best_result.csv'), index=False)\n    summary = pd.DataFrame(all_results)\n    summary.to_csv(os.path.join(node_dir, 'summary.csv'), index=False)\n\n    # 5. Return the combined results of the previous operation and the best prompt maker's output\n    return pd.concat([previous_result, best_result], axis=1)"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    generator_funcs, generator_params = make_generator_callable_params(strategies)\n    # create node_line_dir\n    os.makedirs(node_line_dir, exist_ok=True)\n    # create node_line_dir/prompts\n    prompts_dir = pathlib.Path(node_line_dir) / 'prompts'\n    os.makedirs(prompts_dir, exist_ok=True)\n    # create node_line_dir/prompts/prompt_maker_name\n    prompt_maker_dirs = list(map(lambda x: pathlib.Path(prompts_dir) / x[0].__name__, modules))\n    for prompt_maker_dir in prompt_maker_dirs:\n        os.makedirs(prompt_maker_dir, exist_ok=True)\n\n    # run prompt makers\n    prompt_maker_results = list(map(lambda x: x[0](**x[1]), zip(modules, module_params)))\n    # evaluate prompt makers\n    prompt_maker_evaluation_results = list(map(lambda x: evaluate_one_prompt_maker_node(generator_funcs,\n                                                                                      generator_params,\n                                                                                      x[0],\n                                                                                      x[1],\n                                                                                      strategies['metrics'],\n                                                                                      node_line_dir),\n                                               zip(prompt_maker_results, module_params)))\n    # filter prompt makers by speed\n    prompt_maker_evaluation_results = list(map(lambda x: filter_by_threshold(x,\n                                                                            strategies['speed_thresholds']),\n                                               prompt_maker_evaluation_results))\n    # select best prompt maker\n    best_prompt_maker_result, best_prompt_maker_name = select_best_average(prompt_maker_evaluation_results,\n                                                                           strategies['metrics'])\n    # save best prompt maker result\n    best_prompt_maker_result.to_csv(pathlib.Path(node_line_dir) / 'result.csv', index=False)\n    # save best prompt"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create directories for the node's output\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Get the default generator module from the strategy dictionary\n    default_generator_module = strategies.get('generator_modules', [{'module_type': 'llama_index_llm'}])[0]\n\n    # Get the generator module type and parameters from the strategy dictionary\n    generator_module_type = default_generator_module.pop('module_type')\n    generator_module_params = default_generator_module\n\n    # Get the generator module and its parameters\n    generator_module = get_support_modules(generator_module_type)\n    generator_module_params = generator_module_params\n\n    # Get the prompts and generation ground truth from the previous result\n    prompts = previous_result['prompts'].tolist()\n    generation_gt = previous_result['generation_gt'].tolist()\n\n    # Get the metrics from the strategy dictionary\n    metrics = strategies.get('metrics', ['bleu'])\n\n    # Get the speed threshold from the strategy dictionary\n    speed_threshold = strategies.get('speed_threshold', 0.0)\n\n    # Create a list of combinations of prompt maker modules and their parameters\n    module_combinations = list(map(lambda module_dict: make_combinations({**module_dict, **module_params}),\n                                  modules))\n    module_combinations = list(map(lambda module_dict: module_dict['module_type'], module_combinations))\n\n    # Create a list of combinations of generator modules and their parameters\n    generator_module_combinations = list(map(lambda module_dict: make_combinations({**module_dict, **generator_module_params}),\n                                             [generator_module]))\n\n    # Create a list of combinations of prompt maker modules, generator modules, and their parameters\n    prompt_maker_combinations = list(map(lambda module_dict: make_combinations({**module_dict, **module_params}),\n                                         modules))\n\n    # Create a list of combinations of generator modules and their parameters\n   "}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'results'), exist_ok=True)\n\n    # create combinations of modules and parameters\n    modules_params_combinations = list(map(lambda x: make_combinations({**x[0], **x[1]}),\n                                           zip(modules, module_params)))\n\n    # evaluate modules and select best one\n    results = list(map(lambda x: evaluate_one_prompt_maker_node(**x),\n                       zip(modules_params_combinations,\n                           previous_result['prompts'].tolist(),\n                           previous_result['generation_gt'].tolist(),\n                           strategies['metrics'],\n                           [node_line_dir] * len(modules_params_combinations))))\n    results = cast_metrics(results, strategies['metrics'])\n    best_result, best_result_idx = select_best_average(results, strategies['metrics'])\n    best_result = pd.concat([previous_result, best_result], axis=1)\n\n    # save results\n    best_result.to_csv(os.path.join(node_line_dir, 'results', 'best_result.csv'), index=False)\n\n    # save summary\n    summary = pd.DataFrame({'module_name': [modules[best_result_idx].__name__],\n                            'params': [modules_params_combinations[best_result_idx]],\n                            'metrics': [best_result.drop(columns=['prompts', 'generation_gt']).to_dict('records')[0]]})\n    summary.to_csv(os.path.join(node_line_dir, 'summary.csv'), index=False)\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Validate the QA dataset\n    validate_qa_dataset(previous_result)\n\n    # Create the node's directory\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the prompt maker node\n    node_dir = os.path.join(node_line_dir, 'prompt_maker')\n    pathlib.Path(node_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the prompt maker's results\n    node_results_dir = os.path.join(node_dir, 'results')\n    pathlib.Path(node_results_dir).mkdir(parents=True, exist_ok=True)\n\n    # Initialize the best_result dataframe with the previous_result\n    best_result = previous_result\n\n    # Get the generator module and its parameters\n    generator_funcs, generator_params = make_generator_callable_params(strategies)\n\n    # Iterate through each prompt maker module and its parameters\n    for module, params in zip(modules, module_params):\n        # Get the module's name\n        module_name = module.__name__.split('.')[-1]\n\n        # Create a directory for the current prompt maker module\n        module_dir = os.path.join(node_results_dir, module_name)\n        pathlib.Path(module_dir).mkdir(parents=True, exist_ok=True)\n\n        # Create a directory for the current prompt maker module's results\n        module_results_dir = os.path.join(module_dir, 'results')\n        pathlib.Path(module_results_dir).mkdir(parents=True, exist_ok=True)\n\n        # Run the prompt maker module with the given parameters\n        module_result = module(**params, previous_result=previous_result)\n\n        # Validate the QA dataset\n        validate_qa_dataset(module_result)\n\n        # Save the prompt maker module's result to a CSV file\n        module_result_path = os."}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create the necessary directories if they don't exist\n    node_dir = os.path.join(node_line_dir, 'prompt_maker')\n    os.makedirs(node_dir, exist_ok=True)\n\n    # Create a dataframe with the module names and parameters\n    module_df = pd.DataFrame({'module': modules, 'params': module_params})\n\n    # Execute each prompt maker module and evaluate its performance\n    for index, row in module_df.iterrows():\n        module = row['module']\n        params = row['params']\n        module_dir = os.path.join(node_dir, f'module_{index}')\n        os.makedirs(module_dir, exist_ok=True)\n        module_result = execute_prompt_maker_module(module, params, module_dir)\n        module_result = measure_speed(module_result, strategies)\n        module_result.to_csv(os.path.join(module_dir, 'result.csv'), index=False)\n\n    # Filter the results based on the speed threshold\n    module_result = filter_by_threshold(module_df, strategies)\n\n    # Select the best prompt maker module based on the evaluation metrics\n    best_module_result = select_best_average(module_result, strategies['metrics'])\n    best_module_result = best_module_result.drop(columns=['module', 'params'])\n\n    # Combine the best prompt maker's result with the previous operation's result\n    combined_result = pd.concat([previous_result, best_module_result], axis=1)\n\n    # Save the combined result to a CSV file\n    combined_result.to_csv(os.path.join(node_dir, 'combined_result.csv'), index=False)\n\n    # Save the summary of execution times and evaluation metrics to a text file\n    summary_file = os.path.join(node_dir, 'summary.txt')\n    with open(summary_file, 'w') as f:\n        f.write('Execution Times:\\n')\n        for index,"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'output'), exist_ok=True)\n\n    # Create a list of module names\n    module_names = [module.__name__ for module in modules]\n\n    # Create a list of combinations of module parameters\n    param_combinations = list(map(lambda module_dict: make_combinations({**module_dict, **strategies}), module_params))\n\n    # Combine module names and parameter combinations into a list of dictionaries\n    module_param_combinations = list(zip(module_names, param_combinations))\n\n    # Filter module parameter combinations based on speed threshold\n    module_param_combinations = filter_by_threshold(module_param_combinations, strategies)\n\n    # Create a list of generator module callable and parameter combinations\n    generator_callable_params = make_generator_callable_params(strategies)\n\n    # Create a list of generator module names\n    generator_module_names = [generator_module.__name__ for generator_module in generator_callable_params[0]]\n\n    # Create a list of generator module parameter combinations\n    generator_param_combinations = generator_callable_params[1]\n\n    # Create a list of combinations of module names, parameter combinations, and generator module names\n    module_param_generator_combinations = list(\n        zip(module_param_combinations, generator_module_names, generator_param_combinations))\n\n    # Measure the execution time of each module parameter combination\n    execution_times = measure_speed(module_param_combinations, modules, strategies)\n\n    # Create a list of prompts\n    prompts = previous_result['prompts'].tolist()\n\n    # Create a list of generation ground truth\n    generation_gt = previous_result['generation_gt'].tolist()\n\n    # Create a list of metrics\n    metrics = strategies.get('metrics', ['bleu'])\n\n    #"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    return list(set(list(itertools.chain.from_iterable(list(map(lambda node: extract_values(node, key), nodes))))))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_default_model()\n    return calculate_cosine_similarity(embedding_model, pred, generation_gt)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_default_model()\n    gt_embeddings = embedding_model.get_query_embedding(generation_gt)\n    pred_embedding = embedding_model.get_query_embedding(pred)\n    return calculate_cosine_similarity(gt_embeddings, pred_embedding)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_default_model()\n    return calculate_cosine_similarity(embedding_model, pred, generation_gt)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_default_embedding_model()\n    return calculate_cosine_similarity(embedding_model, pred, generation_gt)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_default_embedding_model()\n\n    gt_embeddings = list(map(lambda x: embedding_model.get_text_embedding(x), generation_gt))\n    pred_embedding = embedding_model.get_text_embedding(pred)\n    return calculate_cosine_similarity(pred_embedding, gt_embeddings)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_default_embedding_model()\n\n    gt_embeddings = embedding_model.get_query_embedding(generation_gt)\n    pred_embedding = embedding_model.get_query_embedding(pred)\n    return calculate_cosine_similarity(gt_embeddings, pred_embedding)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_model('all-mpnet-base-v2')\n    return calculate_cosine_similarity(embedding_model, generation_gt, pred)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_default_embedding_model()\n\n    gt_embeddings = list(map(embedding_model.get_text_embedding, generation_gt))\n    pred_embedding = embedding_model.get_text_embedding(pred)\n    return calculate_cosine_similarity(gt_embeddings, pred_embedding)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_default_embedding_model()\n\n    gt_embeddings = list(map(lambda x: embedding_model.get_text_embedding(x), generation_gt))\n    pred_embedding = embedding_model.get_text_embedding(pred)\n\n    return calculate_cosine_similarity(gt_embeddings, pred_embedding)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_default_embedding_model()\n    gt_embeddings = embedding_model.get_query_embedding(generation_gt)\n    pred_embedding = embedding_model.get_query_embedding(pred)\n    return calculate_cosine_similarity(pred_embedding, gt_embeddings)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_model('all-mpnet-base-v2')\n    pred_embedding = embedding_model.get_text_embedding(pred)\n    gt_embeddings = list(map(lambda x: embedding_model.get_text_embedding(x), generation_gt))\n    return max(list(map(lambda x: calculate_cosine_similarity(pred_embedding, x), gt_embeddings)))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_default_model()\n\n    pred_embedding = embedding_model.get_text_embedding(pred)\n    gt_embeddings = list(map(lambda x: embedding_model.get_text_embedding(x), generation_gt))\n    return max(list(map(lambda x: calculate_cosine_similarity(pred_embedding, x), gt_embeddings)))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_default_embedding_model()\n\n    gt_embeddings = embedding_model.get_text_embedding(generation_gt)\n    pred_embedding = embedding_model.get_text_embedding(pred)\n    return calculate_cosine_similarity(gt_embeddings, pred_embedding)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_default_model()\n    pred_embedding = embedding_model.get_query_embedding(pred)\n    gt_embedding = list(map(lambda x: embedding_model.get_query_embedding(x), generation_gt))\n    return max(list(map(lambda x: calculate_cosine_similarity(pred_embedding, x), gt_embedding)))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.OpenAIEmbedding()\n\n    pred_embedding = embedding_model.get_query_embedding(pred)\n    gt_embeddings = list(map(lambda x: embedding_model.get_query_embedding(x), generation_gt))\n    return max(list(map(lambda x: calculate_cosine_similarity(pred_embedding, x), gt_embeddings)))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_default_model()\n\n    gt_embeddings = embedding_model.get_query_embedding(gt)\n    pred_embedding = embedding_model.get_query_embedding(pred)\n    return calculate_cosine_similarity(pred_embedding, gt_embeddings)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_default_model()\n    gt_embeddings = list(map(lambda x: embedding_model.get_text_embedding(x), generation_gt))\n    pred_embedding = embedding_model.get_text_embedding(pred)\n    return calculate_cosine_similarity(pred_embedding, gt_embeddings)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_default_model()\n\n    gt_embeddings = list(map(embedding_model.get_text_embedding, generation_gt))\n    pred_embeddings = embedding_model.get_text_embedding(pred)\n    return calculate_cosine_similarity(gt_embeddings, pred_embeddings)"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_model('all-mpnet-base-v2')\n\n    pred_embedding = embedding_model.get_query_embedding(pred)\n    gt_embeddings = list(map(lambda x: embedding_model.get_query_embedding(x), generation_gt))\n\n    return max(list(map(lambda x: calculate_cosine_similarity(pred_embedding, x), gt_embeddings)))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_model()\n\n    gt_embeddings = list(map(lambda x: embedding_model.get_text_embedding(x), generation_gt))\n    pred_embedding = embedding_model.get_text_embedding(pred)\n\n    return max(list(map(lambda x: calculate_cosine_similarity(x, pred_embedding), gt_embeddings)))"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN is not set up. Skipping face restoration.\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN not loaded, skipping\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN not loaded, cannot restore faces\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer is not set up\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer is not set up\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN is not set up. Skipping face restoration.\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN not loaded, skipping\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer is not set up. Skipping face restoration.\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN not loaded, falling back to original image\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN not found, falling back to original image\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN not available. Skipping face restoration.\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN not available. Skipping face restoration\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN not loaded, skipping\")\n        return np_image\n\n    try:\n        return gfpgan_face_restorer.restore(np_image)\n    except Exception as e:\n        logger.error(f\"GFPGAN error: {e}\")\n        return np_image"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN not initialized, cannot restore faces\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN not loaded, so no face restoration performed.\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN not found. Skipping GFPGAN restoration.\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer not set up, using original image\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN not loaded, skipping face restoration\")\n        return np_image\n\n    try:\n        return gfpgan_face_restorer.restore(np_image)\n    except Exception as e:\n        logger.warning(f\"GFPGAN failed to restore faces: {e}\")\n        return np_image"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN is not set up, so faces won't be restored\")\n        return np_image\n\n    try:\n        return gfpgan_face_restorer.restore(np_image)\n    except Exception:\n        logger.warning(\"GFPGAN restore failed, returning original image\", exc_info=True)\n        return np_image"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    global gfpgan_face_restorer\n\n    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN not initialized, skipping\")\n        return np_image\n\n    try:\n        return gfpgan_face_restorer.restore(np_image)\n    except Exception:\n        logger.warning(\"GFPGAN failed to restore face\", exc_info=True)\n        return np_image"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global codeformer\n        codeformer = FaceRestorerCodeFormer(dirname)\n        face_restoration.face_restorers.append(codeformer)\n    except Exception:\n        errors.report(f\"Failed to set up CodeFormer in directory {dirname}\", exc_info=True)"}
{"namespace": "codeformer_model.setup_model", "completion": "    global codeformer\n\n    try:\n        codeformer = FaceRestorerCodeFormer(dirname)\n        face_restoration.face_restorers.append(codeformer)\n    except Exception as e:\n        errors.report(f\"Error setting up CodeFormer model: {e}\", exc_info=True)"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global codeformer\n        codeformer = FaceRestorerCodeFormer(dirname)\n        face_restoration.face_restorers.append(codeformer)\n    except Exception as e:\n        logger.error(f\"Error setting up CodeFormer: {e}\")"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restorer = FaceRestorerCodeFormer(dirname)\n        face_restorers.append(face_restorer)\n    except Exception as e:\n        errors.report(f\"Error setting up CodeFormer model: {e}\", exc_info=True)"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restorer = FaceRestorerCodeFormer(dirname)\n        face_restorers.append(face_restorer)\n    except Exception as e:\n        errors.report(f\"Error setting up CodeFormer model: {e}\", exc_info=True)"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restorer = FaceRestorerCodeFormer(dirname)\n        face_restorers.append(face_restorer)\n    except Exception:\n        errors.report(f\"Failed to load CodeFormer in {dirname}\", exc_info=True)"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global face_restorers\n        face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except Exception:\n        errors.report(f\"Failed to load CodeFormer in directory {dirname}\", exc_info=True)"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global codeformer\n        codeformer = FaceRestorerCodeFormer(dirname)\n        face_restoration.face_restorers.append(codeformer)\n    except Exception:\n        errors.report(\n            f\"Error setting up the CodeFormer face restorer:\",\n            exc_info=True,\n        )"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global codeformer\n        codeformer = FaceRestorerCodeFormer(dirname)\n        face_restoration.face_restorers.append(codeformer)\n    except Exception as e:\n        errors.report(f\"Error setting up CodeFormer: {e}\", exc_info=True)\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global codeformer\n        codeformer = FaceRestorerCodeFormer(dirname)\n        face_restoration.face_restorers.append(codeformer)\n    except Exception:\n        errors.report(f\"Failed to set up CodeFormer model\", exc_info=True)"}
{"namespace": "codeformer_model.setup_model", "completion": "    global codeformer\n\n    try:\n        codeformer = FaceRestorerCodeFormer(dirname)\n        face_restoration.face_restorers.append(codeformer)\n    except Exception:\n        errors.report(f\"Error setting up CodeFormer model in directory {dirname}\", exc_info=True)"}
{"namespace": "codeformer_model.setup_model", "completion": "    global codeformer\n    try:\n        codeformer = FaceRestorerCodeFormer(dirname)\n        codeformer.to(devices.device_codeformer)\n    except Exception as e:\n        errors.report(f\"Error setting up CodeFormer model: {e}\", exc_info=True)"}
{"namespace": "codeformer_model.setup_model", "completion": "    global codeformer\n    try:\n        codeformer = FaceRestorerCodeFormer(dirname)\n        logger.info('CodeFormer initialized')\n    except Exception as e:\n        logger.error('Error initializing CodeFormer: %s', e)\n        codeformer = None"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global codeformer\n        codeformer = FaceRestorerCodeFormer(dirname)\n        face_restoration.face_restorers.append(codeformer)\n    except Exception:\n        errors.report(f\"Failed to load CodeFormer in directory {dirname}\", exc_info=True)\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    global codeformer\n    try:\n        codeformer = FaceRestorerCodeFormer(dirname)\n        codeformer.net.to(devices.device_codeformer)\n        logger.info(\"CodeFormer initialized.\")\n    except Exception as e:\n        errors.report(f\"Failed to initialize CodeFormer: {e}\", exc_info=True)"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global face_restorers\n\n        restorer = FaceRestorerCodeFormer(dirname)\n        face_restorers.append(restorer)\n\n        logger.info(f'CodeFormer initialized')\n    except Exception:\n        errors.report(f'Failed to initialize CodeFormer in {dirname}', exc_info=True)"}
{"namespace": "codeformer_model.setup_model", "completion": "    global codeformer\n\n    try:\n        codeformer = FaceRestorerCodeFormer(dirname)\n        face_restoration.face_restorers.append(codeformer)\n    except Exception:\n        errors.report(f\"Failed to initialize CodeFormer in directory {dirname}\", exc_info=True)\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    global codeformer\n\n    try:\n        codeformer = FaceRestorerCodeFormer(dirname)\n        codeformer.model_path = dirname\n        face_restoration.face_restorers.append(codeformer)\n    except Exception as e:\n        logger.error(f\"Error setting up CodeFormer: {e}\")\n        errors.display(e, f\"{dirname}/error.txt\")"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restorer = FaceRestorerCodeFormer(dirname)\n        face_restorers.append(face_restorer)\n    except Exception as e:\n        errors.report(f\"Error setting up CodeFormer model: {e}\", exc_info=True)\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restorer = FaceRestorerCodeFormer(dirname)\n        face_restorer.load_weights()\n        face_restorer.to(face_restorer.get_device())\n        face_restorers.append(face_restorer)\n    except Exception as e:\n        errors.report(f\"Error setting up CodeFormer model: {e}\", exc_info=True)\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    try:\n        facexlib_model_path = os.path.join(dirname, model_download_name)\n        modelloader.patch_facexlib(facexlib_model_path)\n        gfpgan_face_restorer = FaceRestorerGFPGAN(facexlib_model_path)\n    except Exception as e:\n        logger.error(f\"GFPGAN setup failed: {e}\")\n        errors.display(e, f\"GFPGAN setup failed: {e}\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        global gfpgan_face_restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except Exception as e:\n        logger.error(f\"GFPGAN setup failed: {e}\")\n        raise errors.CheckpointNotFound(e)"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    try:\n        facexlib_patch(dirname)\n        gfpgan_face_restorer = FaceRestorerGFPGAN(\n            model_path=dirname,\n            upscale=shared.opts.gfpgan_upscale,\n            arch=shared.opts.gfpgan_arch,\n            channel_multiplier=shared.opts.gfpgan_channel_multiplier,\n            bg_upsampler=shared.opts.gfpgan_bg_upsampler,\n        )\n    except Exception as e:\n        logger.error(f\"GFPGAN setup failed: {e}\")\n        errors.display(e, \"GFPGAN setup failed\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    try:\n        gfpgan_face_restorer = FaceRestorerGFPGAN(\n            dirname,\n            verbose=False,\n            device=devices.device_gfpgan,\n        )\n    except Exception as e:\n        logger.error(f\"GFPGAN setup error: {e}\")\n        errors.display(e, f\"GFPGAN setup error: {e}\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    try:\n        facexlib_model_path = os.path.join(dirname, model_download_name)\n        modelloader.patch_facexlib(facexlib_model_path)\n        gfpgan_face_restorer = FaceRestorerGFPGAN(\n            model_path=facexlib_model_path,\n            upscale=1,\n            arch='clean',\n            channel_multiplier=2,\n            bg_upsampler=None,\n        )\n    except Exception as e:\n        logger.error(f\"GFPGAN model setup failed: {e}\")\n        errors.display(e, f\"GFPGAN model setup failed: {e}\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    try:\n        facexlib_path = os.path.join(dirname, \"facexlib\")\n        if not os.path.exists(facexlib_path):\n            os.makedirs(facexlib_path)\n        if not os.path.exists(os.path.join(facexlib_path, \"__init__.py\")):\n            with open(os.path.join(facexlib_path, \"__init__.py\"), \"w\") as f:\n                f.write(\"\")\n        if not os.path.exists(os.path.join(facexlib_path, \"arch\", \"__init__.py\")):\n            with open(os.path.join(facexlib_path, \"arch\", \"__init__.py\"), \"w\") as f:\n                f.write(\"\")\n        if not os.path.exists(os.path.join(facexlib_path, \"arch\", \"gfpganv1.py\")):\n            with open(os.path.join(facexlib_path, \"arch\", \"gfpganv1.py\"), \"w\") as f:\n                f.write(\"\")\n        if not os.path.exists(os.path.join(facexlib_path, \"arch\", \"gfpganv1_clean.py\")):\n            with open(os.path.join(facexlib_path, \"arch\", \"gfpganv1_clean.py\"), \"w\") as f:\n                f.write(\"\")\n        if not os.path.exists(os.path.join(facexlib_path, \"utils\", \"__init__.py\")):\n            with open(os.path.join(facexlib_path, \"utils\", \"__init__.py\"), \"w\") as f:\n                f.write(\"\")\n        if not os.path.exists(os.path.join(facexlib_path, \"utils\", \"face_restoration_helper.py\")):\n            with open(os.path.join(facexlib_path, \"utils\", \"face_restoration_"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n\n    try:\n        facexlib_path = shared.cmd_opts.facexlib_path\n        if facexlib_path is None:\n            facexlib_path = os.path.join(shared.cmd_opts.venv_dir, 'lib', 'python3.10', 'site-packages', 'facexlib')\n        facexlib_path = os.path.abspath(facexlib_path)\n        facexlib_path = os.path.dirname(facexlib_path)\n        facexlib_path = os.path.dirname(facexlib_path)\n        facexlib_path = os.path.dirname(facexlib_path)\n        facexlib_path = os.path.dirname(facexlib_path)\n        facexlib_path = os.path.dirname(facexlib_path)\n        facexlib_path = os.path.dirname(facexlib_path)\n        facexlib_path = os.path.dirname(facexlib_path)\n        facexlib_path = os.path.dirname(facexlib_path)\n        facexlib_path = os.path.dirname(facexlib_path)\n        facexlib_path = os.path.dirname(facexlib_path)\n        facexlib_path = os.path.dirname(facexlib_path)\n        facexlib_path = os.path.dirname(facexlib_path)\n        facexlib_path = os.path.dirname(facexlib_path)\n        facexlib_path = os.path.dirname(facexlib_path)\n        facexlib_path = os.path.dirname(facexlib_path)\n        facexlib_path = os.path.dirname(facexlib_path)\n        facexlib_path = os.path.dirname(facexlib_path)\n        facexlib_path = os.path.dirname(facexlib_path)\n        facexlib_path = os"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    try:\n        facexlib_path = os.path.join(dirname, \"facexlib\")\n        modelloader.patch_facexlib(facexlib_path)\n        gfpgan_face_restorer = FaceRestorerGFPGAN(\n            model_path=os.path.join(dirname, \"GFPGANv1.4.pth\"),\n            upscale=1,\n            arch=\"clean\",\n            channel_multiplier=2,\n            bg_upsampler=None,\n        )\n        gfpgan_face_restorer.net.to(devices.device_gfpgan)\n        gfpgan_face_restorer.net.eval()\n    except Exception as e:\n        logger.error(f\"Error setting up GFPGAN face restorer: {e}\")\n        raise errors.InitError(f\"Error setting up GFPGAN face restorer: {e}\") from e"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    try:\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n        gfpgan_face_restorer.load_weights()\n        gfpgan_face_restorer.to(devices.device_gfpgan)\n        logger.info(\"GFPGAN face restorer initialized\")\n    except Exception as e:\n        logger.error(f\"GFPGAN face restorer initialization failed: {e}\")\n        errors.display(e, \"GFPGAN face restorer initialization failed\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    try:\n        facexlib_model_path = os.path.join(dirname, \"GFPGANv1.4.pth\")\n        if not os.path.exists(facexlib_model_path):\n            logger.warning(\"GFPGAN model not found, skipping\")\n            return\n\n        patch_facexlib(facexlib_model_path)\n\n        gfpgan_face_restorer = FaceRestorerGFPGAN(\n            model_path=facexlib_model_path,\n            upscale=1,\n            arch=\"clean\",\n            channel_multiplier=2,\n            bg_upsampler=None,\n        )\n    except Exception as e:\n        logger.error(f\"GFPGAN setup error: {e}\")\n        errors.display(e, f\"GFPGAN setup error: {e}\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        global gfpgan_face_restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(\n            model_path=dirname,\n            upscale=shared.opts.gfpgan_upscale,\n            bg_upsampler=shared.opts.gfpgan_bg_upsampler,\n            upscale_order=shared.opts.gfpgan_upscale_order,\n            face_size=shared.opts.face_restoration_face_size,\n            crop_expand=shared.opts.gfpgan_crop_expand,\n            strength=shared.opts.gfpgan_strength,\n            device=shared.device,\n            inpainting_padding=shared.opts.gfpgan_inpainting_padding,\n            inpainting_size=shared.opts.gfpgan_inpainting_size,\n        )\n        gfpgan_face_restorer.load_weights(\n            os.path.join(dirname, model_download_name),\n            download_name=model_download_name,\n        )\n    except Exception as e:\n        logger.error(f\"GFPGAN setup error: {e}\")\n        errors.display(e, \"GFPGAN setup error\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    try:\n        if not os.path.exists(dirname):\n            logger.error(f\"GFPGAN model not found in {dirname}\")\n            return\n        facexlib_path = os.path.join(dirname, \"facexlib\")\n        if not os.path.exists(facexlib_path):\n            logger.error(f\"facexlib not found in {facexlib_path}\")\n            return\n        facexlib_path = os.path.abspath(facexlib_path)\n        gfpgan_face_restorer = FaceRestorerGFPGAN(\n            facexlib_path,\n            model_path=dirname,\n            upscale=1,\n            arch=\"clean\",\n            channel_multiplier=2,\n            bg_upsampler=None,\n        )\n    except Exception as e:\n        logger.error(f\"Error setting up GFPGAN model: {e}\")\n        gfpgan_face_restorer = None\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    try:\n        if not os.path.isdir(dirname):\n            logger.error(f\"{dirname} is not a directory\")\n            return\n        if not os.path.exists(os.path.join(dirname, model_download_name)):\n            logger.error(f\"{dirname} does not contain {model_download_name}\")\n            return\n\n        # patch facexlib with the given directory\n        face_restoration_utils.patch_facexlib(dirname)\n\n        # initialize GFPGAN face restorer with the model located in the specified directory\n        gfpgan_face_restorer = FaceRestorerGFPGAN(\n            model_path=os.path.join(dirname, model_download_name),\n            upscale=1,\n            arch='clean',\n            channel_multiplier=2,\n            bg_upsampler=None,\n        )\n    except Exception as e:\n        logger.error(f\"Error setting up GFPGAN face restorer: {e}\")\n        gfpgan_face_restorer = None\n        shared.opts.gfpgan_model_path = None\n        shared.opts.gfpgan_model_path = None\n        shared.opts.gfpgan_model_path = None\n        shared.opts.gfpgan_model_path = None\n        shared.opts.gfpgan_model_path = None\n        shared.opts.gfpgan_model_path = None\n        shared.opts.gfpgan_model_path = None\n        shared.opts.gfpgan_model_path = None\n        shared.opts.gfpgan_model_path = None\n        shared.opts.gfpgan_model_path = None\n        shared.opts.gfpgan_model_path = None\n        shared.opts.gfpgan_model_path = None\n        shared.opts.gfpgan_model_path = None\n        shared.opts.gfpgan_model_path = None\n        shared.opts.gfpgan_model_path = None\n        shared.opts."}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    try:\n        facexlib_path = face_restoration_utils.get_facexlib_path()\n        if facexlib_path is None:\n            logger.warning(\"Could not find facexlib\")\n            return\n        facexlib_path = os.path.dirname(facexlib_path)\n        facexlib_path = os.path.join(facexlib_path, \"face_restoration\")\n        facexlib_path = os.path.join(facexlib_path, \"gfpgan\")\n        facexlib_path = os.path.join(facexlib_path, \"weights\")\n        if not os.path.exists(facexlib_path):\n            os.makedirs(facexlib_path)\n        facexlib_path = os.path.join(facexlib_path, \"GFPGANv1.4.pth\")\n        if not os.path.exists(facexlib_path):\n            os.symlink(os.path.join(dirname, \"GFPGANv1.4.pth\"), facexlib_path)\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname, 0, 0, 1, 1)\n        gfpgan_face_restorer.load_weights(facexlib_path)\n    except Exception as e:\n        logger.warning(f\"GFPGAN face restorer setup failed: {e}\")\n        gfpgan_face_restorer = None"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    try:\n        facexlib_path = shared.cmd_opts.facexlib_path\n        if facexlib_path is None:\n            raise errors.MissingFaceXLib()\n\n        facexlib_path = os.path.abspath(facexlib_path)\n        if not os.path.exists(facexlib_path):\n            raise errors.MissingFaceXLib()\n\n        import sys\n        sys.path.insert(0, facexlib_path)\n        from facexlib.utils.face_restore_helper import FaceRestoreHelper\n        from facexlib.utils.face_restore_helper import face_restore_helper\n\n        class FaceRestoreHelperGFPGAN(FaceRestoreHelper):\n            def __init__(self, upscale_factor, face_size, crop_ratio, step, thresholds, paste_back, save_root, only_center_face, draw_box):\n                super().__init__(upscale_factor, face_size, crop_ratio, step, thresholds, paste_back, save_root, only_center_face, draw_box)\n\n            def restore_faces(self, input_img):\n                return gfpgan_face_restorer.restore(input_img)\n\n        face_restore_helper.face_restore_helper = FaceRestoreHelperGFPGAN(\n            upscale_factor=1,\n            face_size=512,\n            crop_ratio=(1, 1),\n            step=8,\n            thresholds=[0.5, 0.6, 0.8],\n            paste_back=True,\n            save_root='results',\n            only_center_face=False,\n            draw_box=False\n        )\n\n        gfpgan_face_restorer = FaceRestorerGFPGAN(\n            dirname,\n            upscale=1,\n            arch='clean',\n            channel_multiplier=2,\n            bg_upsampler=None,\n        )\n        gfpgan_face_restorer.model"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    try:\n        if gfpgan_face_restorer is None:\n            from basicsr.utils.facexlib.facexlib.facexlib import facexlib\n            from basicsr.utils.facexlib.facexlib.facexlib.utils.face_restoration_helper import FaceRestoreHelper\n\n            facexlib.face_restore_helper.FaceRestoreHelper = FaceRestoreHelper\n            facexlib.face_restore_helper.FaceRestoreHelper.face_restorers = {\n                \"GFPGAN\": FaceRestorerGFPGAN(\n                    model_path=dirname,\n                    upscale_factor=1,\n                    face_size=512,\n                    crop_ratio=(1, 1),\n                    pad_mod=0,\n                    save_ext=\"png\",\n                )\n            }\n            gfpgan_face_restorer = facexlib.face_restore_helper.FaceRestoreHelper.get_face_restorers()[\"GFPGAN\"]\n    except Exception as e:\n        errors.report(f\"GFPGAN setup error: {e}\", exc_info=True)\n        raise"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    if not os.path.exists(dirname):\n        logger.warning(f\"GFPGAN model not found in {dirname}\")\n        return\n\n    try:\n        facexlib_path = os.path.join(dirname, \"facexlib\")\n        if os.path.exists(facexlib_path):\n            logger.info(\"Patching facexlib\")\n            face_restoration_utils.patch_facexlib(facexlib_path)\n\n        logger.info(\"Initializing GFPGAN face restorer\")\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except Exception as e:\n        logger.error(f\"Error initializing GFPGAN face restorer: {e}\")\n        gfpgan_face_restorer = None\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        global gfpgan_face_restorer\n        if not os.path.exists(dirname):\n            logger.error(f\"GFPGAN model not found in {dirname}\")\n            return\n        patch_facexlib(dirname)\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except Exception as e:\n        logger.error(f\"GFPGAN model setup failed: {e}\")\n        raise errors.Error(f\"GFPGAN model setup failed: {e}\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n\n    try:\n        gfpgan_face_restorer = FaceRestorerGFPGAN(\n            dirname,\n            weight_path=os.path.join(dirname, model_download_name),\n            model_path=dirname,\n            upscale=1,\n            arch=\"clean\",\n            channel_multiplier=2,\n            bg_upsampler=None,\n        )\n    except Exception:\n        logger.exception(\"GFPGAN model setup failed\")\n        errors.display(\n            f\"GFPGAN model setup failed. Please download the model manually.\\n\"\n            f\"{model_url}\",\n            title=\"Error: GFPGAN model setup failed\",\n        )\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    if not gfpgan_face_restorer:\n        try:\n            facexlib_dir = os.path.join(dirname, 'facexlib')\n            if os.path.exists(facexlib_dir):\n                sys.path.insert(0, facexlib_dir)\n            from facexlib.utils.face_restoration_helper import FaceRestoreHelper\n            FaceRestoreHelper.face_restore_helper = FaceRestoreHelper(\n                upscale_factor=2,\n                face_size=512,\n                crop_ratio=(1, 1),\n                det_model='retinaface_resnet50',\n                save_ext='png',\n                use_parse=True,\n                device=devices.device_gfpgan.type,\n                model_rootpath=os.path.join(dirname, 'weights'),\n                net_name='GFPGAN',\n                use_sr=False,\n                sr_model=None,\n                sr_scale=2,\n                sr_tile=0,\n                sr_tile_mode='replicate',\n                sr_gpu_id=0,\n                sr_cpu_threads=0,\n                sr_tta=False,\n                sr_model_path=None,\n                sr_bg_tile=None,\n                use_cpu=False,\n                gpu_tile=False,\n                sr_model_arch='restormer',\n            )\n            gfpgan_face_restorer = FaceRestorerGFPGAN(\n                model_path=os.path.join(dirname, 'weights', model_download_name),\n                upscale=2,\n                arch='clean',\n                channel_multiplier=2,\n                bg_upsampler=None,\n            )\n        except Exception as e:\n            logger.error(e)\n            raise errors.CheckpointNotFoundException(\n                f\"GFPGAN model not found in {dirname}\") from e"}
{"namespace": "quaternion.rotate", "completion": "  v = jnp.concatenate([v, jnp.zeros_like(v[Ellipsis, :1])], axis=-1)\n  return (multiply(q, v) * conjugate(q))[Ellipsis, :3]"}
{"namespace": "quaternion.rotate", "completion": "  v = jnp.concatenate([v, jnp.zeros_like(v[Ellipsis, :1])], axis=-1)\n  return (multiply(q, v) * conjugate(q))[Ellipsis, :3]"}
{"namespace": "quaternion.rotate", "completion": "  v = jnp.concatenate([v, jnp.zeros_like(v[Ellipsis, :1])], axis=-1)\n  return (multiply(q, v) * conjugate(q))[Ellipsis, :3]"}
{"namespace": "quaternion.rotate", "completion": "  v = jnp.concatenate([v, jnp.zeros_like(v[Ellipsis, :1])], axis=-1)\n  return (multiply(q, v) * conjugate(q))[Ellipsis, :3]"}
{"namespace": "quaternion.rotate", "completion": "  v = jnp.concatenate([v, jnp.zeros_like(v[Ellipsis, :1])], axis=-1)\n  return (multiply(q, v) * conjugate(q))[Ellipsis, :3]"}
{"namespace": "quaternion.rotate", "completion": "  v = jnp.concatenate([v, jnp.zeros_like(v[Ellipsis, :1])], axis=-1)\n  return multiply(q, multiply(v, conjugate(q)))[Ellipsis, :3]"}
{"namespace": "quaternion.rotate", "completion": "  v = jnp.concatenate([v, jnp.zeros_like(v[Ellipsis, :1])], axis=-1)\n  return multiply(q, multiply(v, conjugate(q)))[Ellipsis, :3]"}
{"namespace": "quaternion.rotate", "completion": "  v = jnp.concatenate([v, jnp.zeros_like(v[Ellipsis, :1])], axis=-1)\n  return im(multiply(multiply(q, v), conjugate(q)))"}
{"namespace": "quaternion.rotate", "completion": "  v = jnp.concatenate([v, jnp.zeros_like(v[Ellipsis, :1])], axis=-1)\n  return (multiply(q, v) * conjugate(q))[Ellipsis, :3]"}
{"namespace": "quaternion.rotate", "completion": "  v_q = jnp.concatenate([v, jnp.zeros_like(v[Ellipsis, :1])], axis=-1)\n  return (multiply(q, multiply(v_q, conjugate(q)))[Ellipsis, :3]).squeeze()"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector into a quaternion format\n  v = jnp.concatenate([v, jnp.zeros_like(v[Ellipsis, :1])], axis=-1)\n\n  # Apply the rotation\n  rotated_v = multiply(multiply(q, v), conjugate(q))\n\n  # Convert the quaternion back to a vector format\n  return rotated_v[Ellipsis, :3]"}
{"namespace": "quaternion.rotate", "completion": "  v = jnp.concatenate([v, jnp.zeros_like(v[Ellipsis, :1])], axis=-1)\n  q_inv = inverse(q)\n  q_v = multiply(q, v)\n  q_v_q_inv = multiply(q_v, q_inv)\n  return im(q_v_q_inv)"}
{"namespace": "quaternion.rotate", "completion": "  v = jnp.concatenate([v, jnp.zeros_like(v[Ellipsis, :1])], axis=-1)\n  return im(multiply(multiply(q, v), conjugate(q)))\n\n"}
{"namespace": "quaternion.rotate", "completion": "  v = jnp.concatenate([v, jnp.zeros_like(v[Ellipsis, :1])], axis=-1)\n  return (multiply(q, multiply(v, conjugate(q)))[Ellipsis, :3]).squeeze(-1)\n\n"}
{"namespace": "quaternion.rotate", "completion": "  v = jnp.concatenate([v, jnp.zeros_like(v[Ellipsis, :1])], axis=-1)\n  return im(multiply(multiply(q, v), conjugate(q)))\n\n"}
{"namespace": "quaternion.rotate", "completion": "  v = jnp.concatenate([v, jnp.zeros_like(v[Ellipsis, :1])], axis=-1)\n  return multiply(multiply(q, v), conjugate(q))[Ellipsis, :3]\n\n"}
{"namespace": "quaternion.rotate", "completion": "  v = jnp.concatenate([v, jnp.zeros_like(v[Ellipsis, :1])], axis=-1)\n  return (multiply(q, multiply(v, conjugate(q)))[Ellipsis, :3])\n\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion with zero real part.\n  v_quat = jnp.concatenate([v, jnp.zeros_like(v[Ellipsis, :1])], axis=-1)\n\n  # Compute the product of the two quaternions.\n  result_quat = multiply(q, v_quat)\n  result_quat = multiply(result_quat, conjugate(q))\n\n  # Extract the vector part of the result.\n  result_vec = im(result_quat)\n\n  return result_vec\n\n"}
{"namespace": "quaternion.rotate", "completion": "  v = jnp.concatenate([v, jnp.zeros_like(v[Ellipsis, :1])], axis=-1)\n  return im(multiply(multiply(q, v), conjugate(q)))\n\n"}
{"namespace": "quaternion.rotate", "completion": "  v = jnp.concatenate([v, jnp.zeros_like(v[Ellipsis, :1])], axis=-1)\n  return im(multiply(multiply(q, v), conjugate(q)))\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  angle = linalg.norm(axis_angle)\n  axis = axis_angle / (angle + eps)\n  return jnp.concatenate(\n      [jnp.sin(angle / 2) * axis, jnp.cos(angle / 2)], axis=-1\n  )"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  theta = linalg.norm(axis_angle)\n  axis = axis_angle / (theta + eps)\n  return jnp.concatenate(\n      [\n          axis * jnp.sin(theta / 2.0),\n          jnp.cos(theta / 2.0),\n      ],\n      axis=-1,\n  )"}
{"namespace": "quaternion.from_axis_angle", "completion": "  angle = linalg.norm(axis_angle, axis=-1, keepdims=True)\n  axis = axis_angle / angle\n  s = jnp.sin(0.5 * angle) / jnp.maximum(eps, angle)\n  return jnp.concatenate((axis * s, jnp.cos(0.5 * angle)), axis=-1)"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  angle = linalg.norm(axis_angle, axis=-1)\n  axis = axis_angle / (angle[Ellipsis, None] + eps)\n  s = jnp.sin(0.5 * angle)\n  c = jnp.cos(0.5 * angle)\n  return jnp.concatenate((axis * s[Ellipsis, None], c[Ellipsis, None]), axis=-1)"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  angle = linalg.norm(axis_angle)\n  axis = axis_angle / (angle + eps)\n  return jnp.concatenate(\n      [jnp.sin(angle / 2) * axis, jnp.cos(angle / 2)[..., None]], axis=-1\n  )"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  angle = linalg.norm(axis_angle, axis=-1, keepdims=True)\n  axis_normalized = axis_angle / angle\n  return jnp.where(\n      angle > eps,\n      jnp.concatenate(\n          [\n              axis_normalized * jnp.sin(angle / 2.0),\n              jnp.cos(angle / 2.0),\n          ],\n          axis=-1,\n      ),\n      jnp.concatenate(\n          [jnp.zeros_like(axis_normalized), jnp.ones_like(angle)], axis=-1\n      ),\n  )"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  angle = linalg.norm(axis_angle, axis=-1)\n  axis_normalized = axis_angle / (angle + eps)\n  return jnp.concatenate(\n      [\n          axis_normalized * jnp.sin(angle / 2)[..., None],\n          jnp.cos(angle / 2)[..., None],\n      ],\n      axis=-1,\n  )"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  axis_angle_norm = jnp.linalg.norm(axis_angle)\n  angle = axis_angle_norm\n  axis = axis_angle / (angle + eps)\n  half_angle = 0.5 * angle\n  w = jnp.cos(half_angle)\n  xyz = jnp.sin(half_angle) * axis\n  return jnp.concatenate([xyz, w[Ellipsis, None]], axis=-1)"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  angle = jnp.linalg.norm(axis_angle, axis=-1)\n  axis_normalized = axis_angle / (angle + eps)\n  return jnp.concatenate(\n      [\n          axis_normalized * jnp.sin(angle / 2)[..., None],\n          jnp.cos(angle / 2)[..., None],\n      ],\n      axis=-1,\n  )"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  angle = linalg.norm(axis_angle)\n  axis = axis_angle / (angle + eps)\n  q = jnp.concatenate([jnp.sin(angle / 2.0) * axis, jnp.cos(angle / 2.0)], axis=-1)\n  return q"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Normalize the axis of rotation.\n  axis_angle = axis_angle / jnp.linalg.norm(axis_angle, axis=-1, keepdims=True)\n\n  # Compute the angle of rotation.\n  angle = jnp.linalg.norm(axis_angle, axis=-1, keepdims=True)\n\n  # Compute the quaternion representation of the rotation.\n  quaternion = jnp.concatenate([axis_angle * jnp.sin(angle / 2), jnp.cos(angle / 2)], axis=-1)\n\n  # Ensure numerical stability for small angles.\n  quaternion = jnp.where(angle < eps, jnp.zeros_like(quaternion), quaternion)\n\n  return quaternion"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Normalize the axis of rotation.\n  axis = axis_angle / jnp.linalg.norm(axis_angle)\n\n  # Calculate the angle of rotation.\n  angle = jnp.linalg.norm(axis_angle)\n\n  # Calculate the quaternion.\n  q = jnp.concatenate([jnp.sin(angle / 2) * axis, jnp.cos(angle / 2)], axis=-1)\n\n  # Ensure numerical stability for small angles.\n  q = jnp.where(angle < eps, jnp.zeros_like(q), q)\n\n  return q"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Normalize axis_angle.\n  axis_angle = axis_angle / jnp.linalg.norm(axis_angle)\n\n  # Compute the angle.\n  angle = jnp.linalg.norm(axis_angle)\n\n  # Compute the quaternion.\n  q = jnp.concatenate(\n      [\n          jnp.sin(angle / 2) * axis_angle,\n          jnp.cos(angle / 2),\n      ],\n      axis=-1,\n  )\n\n  # Ensure numerical stability for small angles.\n  q = jnp.where(angle < eps, jnp.zeros_like(q), q)\n\n  return q"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Normalize axis_angle vector\n  axis_angle_norm = jnp.linalg.norm(axis_angle)\n  axis_angle_normalized = axis_angle / axis_angle_norm\n\n  # Calculate the angle of rotation\n  angle = axis_angle_norm\n\n  # Calculate the quaternion using the axis-angle representation\n  q = jnp.concatenate(\n      [\n          axis_angle_normalized * jnp.sin(angle / 2),\n          jnp.cos(angle / 2),\n      ],\n      axis=-1,\n  )\n\n  return q"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  axis = axis_angle[:3]\n  angle = axis_angle[3]\n\n  # Compute the sine and cosine of the half angle.\n  half_angle = 0.5 * angle\n  sin_half_angle = jnp.sin(half_angle)\n  cos_half_angle = jnp.cos(half_angle)\n\n  # Normalize the axis vector.\n  axis_norm = linalg.norm(axis)\n  axis_normalized = axis / (axis_norm + eps)\n\n  # Compute the quaternion components.\n  x = sin_half_angle * axis_normalized[0]\n  y = sin_half_angle * axis_normalized[1]\n  z = sin_half_angle * axis_normalized[2]\n  w = cos_half_angle\n\n  # Concatenate the quaternion components and return.\n  return jnp.array([x, y, z, w])"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Normalize axis_angle to unit vector.\n  axis_angle = axis_angle / jnp.linalg.norm(axis_angle)\n\n  # Compute the angle of rotation.\n  angle = jnp.linalg.norm(axis_angle)\n\n  # If the angle is very small, use a small epsilon value to avoid division by zero.\n  # Otherwise, compute the quaternion components using the formula for a quaternion from an axis-angle representation.\n  def small_angle():\n    return jnp.array([0.0, 0.0, 0.0, jnp.cos(angle / 2.0)]) + jnp.sin(angle / 2.0) * axis_angle\n\n  def large_angle():\n    return jnp.array([\n        axis_angle[0] * jnp.sin(angle / 2.0),\n        axis_angle[1] * jnp.sin(angle / 2.0),\n        axis_angle[2] * jnp.sin(angle / 2.0),\n        jnp.cos(angle / 2.0),\n    ])\n\n  return jnp.where(angle < eps, small_angle(), large_angle())"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Normalize axis_angle\n  axis_angle = axis_angle / jnp.linalg.norm(axis_angle)\n\n  # Calculate the angle of rotation (theta)\n  theta = jnp.linalg.norm(axis_angle)\n\n  # Check if the angle of rotation is close to zero\n  if jnp.abs(theta) < eps:\n    # If the angle of rotation is close to zero, return the identity quaternion\n    return jnp.array([0.0, 0.0, 0.0, 1.0])\n\n  # Calculate the half angle (theta/2)\n  half_theta = theta / 2.0\n\n  # Calculate the real and imaginary parts of the quaternion\n  real = jnp.cos(half_theta)\n  imaginary = jnp.sin(half_theta) * axis_angle\n\n  # Concatenate the real and imaginary parts to form the quaternion\n  return jnp.concatenate((imaginary, real[..., jnp.newaxis]), axis=-1)"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  axis = axis_angle[:3]\n  angle = axis_angle[3]\n\n  # Compute the half angle.\n  half_angle = 0.5 * angle\n\n  # Normalize the axis vector.\n  axis = axis / jnp.linalg.norm(axis)\n\n  # Compute the sine and cosine of the half angle.\n  sin_half_angle = jnp.sin(half_angle)\n  cos_half_angle = jnp.cos(half_angle)\n\n  # Compute the quaternion components.\n  x = axis[0] * sin_half_angle\n  y = axis[1] * sin_half_angle\n  z = axis[2] * sin_half_angle\n  w = cos_half_angle\n\n  # Handle the case where the angle is close to zero.\n  if jnp.abs(angle) < eps:\n    # Use a Taylor series approximation to handle the small angle case.\n    x = 0.5 * axis[0] * angle\n    y = 0.5 * axis[1] * angle\n    z = 0.5 * axis[2] * angle\n    w = 1.0\n\n  # Normalize the quaternion components.\n  norm = jnp.sqrt(x**2 + y**2 + z**2 + w**2)\n  x /= norm\n  y /= norm\n  z /= norm\n  w /= norm\n\n  # Return the quaternion as a numpy array.\n  return jnp.array([x, y, z, w])"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # The following algorithm is from:\n  # https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation#Quaternion-derived_rotation_matrix\n  #\n  # The quaternion is represented as (x, y, z, w).\n  # The axis_angle is represented as (theta, v), where theta is the angle of rotation and v is the axis of rotation.\n  #\n  # The norm of the axis_angle vector is the angle of rotation.\n  # The axis of rotation is the unit vector that is perpendicular to the axis_angle vector.\n  #\n  # The quaternion is constructed as follows:\n  #   q = (v * sin(theta/2), cos(theta/2))\n  # where v is the unit vector that is perpendicular to the axis_angle vector.\n  #\n  # The quaternion is normalized to ensure that it is a unit quaternion.\n  #\n  # The quaternion is returned as a 4-vector.\n  #\n  # The quaternion is returned as a 4-vector.\n  #\n  # The quaternion is returned as a 4-vector.\n  #\n  # The quaternion is returned as a 4-vector.\n  #\n  # The quaternion is returned as a 4-vector.\n  #\n  # The quaternion is returned as a 4-vector.\n  #\n  # The quaternion is returned as a 4-vector.\n  #\n  # The quaternion is returned as a 4-vector.\n  #\n  # The quaternion is returned as a 4-vector.\n  #\n  # The quaternion is returned as a 4-vector.\n  #\n  # The quaternion is returned as a 4-vector.\n  #\n  # The quaternion is returned as a 4-vector.\n  #\n  # The quaternion is returned as a 4-vector.\n  #\n  # The quaternion is returned as a 4-vector.\n  #\n  # The quaternion is returned as a 4"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  theta_squared = jnp.sum(axis_angle**2, axis=-1)\n  theta = jnp.sqrt(theta_squared)\n  half_theta = theta / 2.0\n  sin_half_theta = jnp.sin(half_theta)\n  cos_half_theta = jnp.cos(half_theta)\n\n  # For small angles, use a small epsilon value to avoid division by zero.\n  # This is a numerical stability trick that ensures that the rotation\n  # matrix is well-defined even for very small angles.\n  small_angle = theta_squared < eps**2\n  sin_half_theta_div = jnp.where(\n      small_angle, 1.0 - theta_squared / 8.0, sin_half_theta / theta\n  )\n\n  # Compute the quaternion components using the axis-angle representation.\n  # The quaternion components are given by:\n  #   qw = cos(theta/2)\n  #   qx = axis_angle[0] * sin(theta/2) / theta\n  #   qy = axis_angle[1] * sin(theta/2) / theta\n  #   qz = axis_angle[2] * sin(theta/2) / theta\n  #\n  # Note that we use the \"where\" function to handle the case where theta is\n  # very small, in which case we use the Taylor expansion of sin(theta/2)\n  # and cos(theta/2) to avoid numerical instability.\n  qw = jnp.where(small_angle, 1.0 - theta_squared / 2.0, cos_half_theta)\n  qx = jnp.where(\n      small_angle,\n      axis_angle[Ellipsis, 0] * 0.5,\n      axis_angle[Ellipsis, 0] * sin_half_theta_div,\n  )\n  qy = jnp.where(\n      small_angle,\n     "}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n    high = logit_bias[idx]\n\n    # improve estimate\n    mid = (high + low) / 2\n    while high >= low + eps:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n    return -mid, k"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n    high = logit_bias[idx]\n\n    # improve estimate\n    mid = (high + low) / 2\n    while high >= low + eps:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n    return -mid, 1"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n    high = logit_bias[idx]\n\n    # improve estimate\n    mid = (high + low) / 2\n    while high >= low + eps:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n    return -mid, 1"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n    high = logit_bias[idx]\n\n    # improve estimate\n    mid = (high + low) / 2\n    while high >= low + eps:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n    return -mid, 1"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n    high = logit_bias[idx]\n\n    # improve estimate\n    mid = (high + low) / 2\n    while high >= low + eps:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n    return -mid, 1"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n    high = logit_bias[idx]\n\n    # improve estimate\n    mid = (high + low) / 2\n    while high >= low + eps:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n    return -mid, k"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n    high = logit_bias[idx]\n\n    # improve estimate\n    mid = (high + low) / 2\n    while high >= low + eps:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n    return -mid, k"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n    high = logit_bias[idx]\n\n    # improve estimate\n    mid = (high + low) / 2\n    while high >= low + eps:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n    return -mid, k"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # check if idx is the argmax\n    num_calls = k\n    if model.argmax(prefix) == idx:\n        return 0, num_calls\n\n    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n        num_calls += k\n    high = logit_bias[idx]\n\n    # improve estimate\n    mid = (high + low) / 2\n    while high >= low + eps:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n        num_calls += k\n    return -mid, num_calls"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n    high = logit_bias[idx]\n\n    # improve estimate\n    mid = (high + low) / 2\n    while high >= low + eps:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n    return -mid, k"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n    high = logit_bias[idx]\n\n    # improve estimate\n    mid = (high + low) / 2\n    while high >= low + eps:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n    return -mid, 1"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n    high = logit_bias[idx]\n\n    # improve estimate\n    mid = (high + low) / 2\n    while high >= low + eps:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n    return -mid, 1"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n    high = logit_bias[idx]\n\n    # improve estimate\n    mid = (high + low) / 2\n    while high >= low + eps:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n    return -mid, k"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n\n    # improve estimate\n    mid = (high + low) / 2\n    while high >= low + eps:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n    return -mid, k"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    topk_words = model.topk(prefix, logit_bias)\n    if idx in topk_words:\n        return topk_words[idx], k\n    while idx not in topk_words:\n        logit_bias[idx] -= 1\n        topk_words = model.topk(prefix, logit_bias)\n        num_calls += k\n    return topk_words[idx], num_calls"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    topk_words, topk_idxs, topk_logprobs = model.topk(prefix, k=k)\n    if idx in topk_idxs:\n        return topk_logprobs[topk_idxs.index(idx)], k\n\n    # initialize high\n    logit_bias = {idx: high}\n    while idx not in topk_idxs:\n        logit_bias[idx] *= 2\n        k += 1\n        topk_words, topk_idxs, topk_logprobs = model.topk(prefix, logit_bias, k=k)\n    high = logit_bias[idx]\n\n    # improve estimate\n    mid = (high + low) / 2\n    while high >= low + eps:\n        logit_bias[idx] = mid\n        topk_words, topk_idxs, topk_logprobs = model.topk(prefix, logit_bias, k=k)\n        if idx in topk_idxs:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n        k += 1\n    return -mid, k"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    topk_words = model.topk(prefix, logit_bias)\n    if idx in topk_words:\n        return -high, k\n    else:\n        low = high\n        while idx not in topk_words:\n            logit_bias[idx] *= 2\n            topk_words = model.topk(prefix, logit_bias)\n        high = logit_bias[idx]\n\n    # improve estimate\n    mid = (high + low) / 2\n    while high >= low + eps:\n        logit_bias[idx] = mid\n        if idx in topk_words:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n        k += 1\n    return -mid, k"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    topk_words = model.topk(prefix)\n    if idx in topk_words:\n        return topk_words[idx], k\n    else:\n        low = 0\n        mid = high\n        while high >= low + eps:\n            logit_bias = {idx: mid}\n            if model.argmax(prefix, logit_bias) == idx:\n                high = mid\n            else:\n                low = mid\n            mid = (high + low) / 2\n            k += 1\n        return mid, k"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get raw topk, could be done outside and passed in\n    topk_words = model.topk(prefix, k=k)\n    topk_words_idx = [model.token_to_idx(w) for w in topk_words]\n\n    # check if idx is the argmax\n    num_calls = k\n    if idx == topk_words_idx[0]:\n        return 0, num_calls\n\n    # initialize high\n    logit_bias = {idx: high}\n    while idx not in topk_words_idx:\n        logit_bias[idx] *= 2\n        num_calls += k\n    high = logit_bias[idx]\n\n    # improve estimate\n    mid = (high + low) / 2\n    while high >= low + eps:\n        logit_bias[idx] = mid\n        topk_words = model.topk(prefix, logit_bias, k=k)\n        topk_words_idx = [model.token_to_idx(w) for w in topk_words]\n        if idx == topk_words_idx[0]:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n        num_calls += k\n    return -mid, num_calls"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    topk_words = model.topk(prefix, logit_bias)\n    if idx in topk_words:\n        return logit_bias[idx], k\n    else:\n        low = 0\n        while idx not in topk_words:\n            logit_bias[idx] *= 2\n            topk_words = model.topk(prefix, logit_bias)\n        high = logit_bias[idx]\n        mid = (high + low) / 2\n        while high >= low + eps:\n            logit_bias[idx] = mid\n            if idx in topk_words:\n                high = mid\n            else:\n                low = mid\n            mid = (high + low) / 2\n            k += 1\n        return -mid, k\n\n"}
{"namespace": "resample.resample_3d", "completion": "  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError('Unsupported edge_behavior: %s' % edge_behavior)\n\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError('Unsupported method: %s' % method)\n\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError('Unsupported coordinate_order: %s' % coordinate_order)\n\n  if data.shape[0] != locations.shape[0]:\n    raise ValueError('First dimension of data and locations must match.')\n\n  if data.ndim != 4:\n    raise ValueError('data must have 4 dimensions.')\n\n  if locations.ndim < 2:\n    raise ValueError('locations must have at least 2 dimensions.')\n\n  if data.shape[0] != locations.shape[0]:\n    raise ValueError('First dimension of data and locations must match.')\n\n  if data.shape[0] != locations.shape[0]:\n    raise ValueError('First dimension of data and locations must match.')\n\n  if data.shape[0] != locations.shape[0]:\n    raise ValueError('First dimension of data and locations must match.')\n\n  if data.shape[0] != locations.shape[0]:\n    raise ValueError('First dimension of data and locations must match.')\n\n  if data.shape[0] != locations.shape[0]:\n    raise ValueError('First dimension of data and locations must match.')\n\n  if data.shape[0] != locations.shape[0]:\n    raise ValueError('First dimension of data and locations must match.')\n\n  if data.shape[0] != locations.shape[0]:\n    raise ValueError('First dimension of data and locations must match.')\n\n  if data.shape[0] != locations.shape[0]:\n    raise ValueError('First dimension of data and locations must match.')\n\n  if data.shape[0] != locations.shape[0]:\n    raise Value"}
{"namespace": "resample.resample_3d", "completion": "  # Check if the input data is a JAX array.\n  if not isinstance(data, jnp.ndarray):\n    raise ValueError('Input data must be a JAX array.')\n\n  # Check if the input locations is a JAX array.\n  if not isinstance(locations, jnp.ndarray):\n    raise ValueError('Input locations must be a JAX array.')\n\n  # Check if the input data has the correct number of dimensions.\n  if data.ndim != 4:\n    raise ValueError('Input data must have 4 dimensions.')\n\n  # Check if the input locations has the correct number of dimensions.\n  if locations.ndim < 2 or locations.shape[-1] != 3:\n    raise ValueError('Input locations must have at least 2 dimensions and the last dimension must be of size 3.')\n\n  # Check if the input edge_behavior is valid.\n  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError('Invalid edge_behavior. Must be either \"CONSTANT_OUTSIDE\" or \"CLAMP\".')\n\n  # Check if the input constant_values is a scalar.\n  if not jnp.isscalar(constant_values):\n    raise ValueError('Input constant_values must be a scalar.')\n\n  # Check if the input coordinate_order is valid.\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError('Invalid coordinate_order. Must be either \"xyz\" or \"zyx\".')\n\n  # Check if the input method is valid.\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError('Invalid method. Must be either \"TRILINEAR\" or \"NEAREST\".')\n\n  # Check if the input half_pixel_center is a boolean.\n  if not isinstance(half_pixel_center, bool):\n    raise ValueError('Input half_pixel_center must be a boolean.')\n\n  # Get the input data shape.\n  data_shape = data.shape\n\n  # Get the input"}
{"namespace": "resample.resample_3d", "completion": "  if coordinate_order == 'xyz':\n    x_coordinate = locations[Ellipsis, 0]\n    y_coordinate = locations[Ellipsis, 1]\n    z_coordinate = locations[Ellipsis, 2]\n  elif coordinate_order == 'zyx':\n    z_coordinate = locations[Ellipsis, 0]\n    y_coordinate = locations[Ellipsis, 1]\n    x_coordinate = locations[Ellipsis, 2]\n\n  if method == 'NEAREST':\n    if half_pixel_center:\n      x_coordinate = x_coordinate + 0.5\n      y_coordinate = y_coordinate + 0.5\n      z_coordinate = z_coordinate + 0.5\n    x_coordinate = jnp.round(x_coordinate).astype(jnp.int32)\n    y_coordinate = jnp.round(y_coordinate).astype(jnp.int32)\n    z_coordinate = jnp.round(z_coordinate).astype(jnp.int32)\n    return gather_volume(data,\n                         jnp.stack([z_coordinate, y_coordinate, x_coordinate],\n                                   axis=-1),\n                         coordinate_order=coordinate_order)\n\n  if method == 'TRILINEAR':\n    if half_pixel_center:\n      x_coordinate = x_coordinate + 0.5\n      y_coordinate = y_coordinate + 0.5\n      z_coordinate = z_coordinate + 0.5\n\n    # Get the coordinates of the 8 nearest neighbors.\n    x_low = jnp.floor(x_coordinate).astype(jnp.int32)\n    y_low = jnp.floor(y_coordinate).astype(jnp.int32)\n    z_low = jnp.floor(z_coordinate).astype(jnp.int32)\n    x_high = x_low + 1\n    y_high = y_low"}
{"namespace": "resample.resample_3d", "completion": "  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError('coordinate_order must be \\'xyz\\' or \\'zyx\\'.')\n\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError('method must be \\'TRILINEAR\\' or \\'NEAREST\\'.')\n\n  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError('edge_behavior must be \\'CONSTANT_OUTSIDE\\' or \\'CLAMP\\'.')\n\n  if half_pixel_center:\n    locations = locations + 0.5\n\n  if method == 'TRILINEAR':\n    return trilinear_resample_3d(\n        data,\n        locations,\n        edge_behavior=edge_behavior,\n        constant_values=constant_values,\n        coordinate_order=coordinate_order,\n    )\n  elif method == 'NEAREST':\n    return nearest_resample_3d(\n        data,\n        locations,\n        edge_behavior=edge_behavior,\n        constant_values=constant_values,\n        coordinate_order=coordinate_order,\n    )\n\n"}
{"namespace": "resample.resample_3d", "completion": "  # Check if the data has the correct shape.\n  if data.ndim != 4:\n    raise ValueError(\n        f'data must have shape [D, H, W, C], but got shape {data.shape}.'\n    )\n\n  # Check if the locations have the correct shape.\n  if locations.ndim < 2 or locations.shape[-1] != 3:\n    raise ValueError(\n        f'locations must have shape [D, ..., 3], but got shape {locations.shape}.'\n    )\n\n  # Check if the edge behavior is valid.\n  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError(\n        f'edge_behavior must be either \"CONSTANT_OUTSIDE\" or \"CLAMP\", but got {edge_behavior}.'\n    )\n\n  # Check if the coordinate order is valid.\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError(\n        f'coordinate_order must be either \"xyz\" or \"zyx\", but got {coordinate_order}.'\n    )\n\n  # Check if the method is valid.\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError(\n        f'method must be either \"TRILINEAR\" or \"NEAREST\", but got {method}.'\n    )\n\n  # Check if the half_pixel_center is valid.\n  if not isinstance(half_pixel_center, bool):\n    raise ValueError(\n        f'half_pixel_center must be a boolean, but got {half_pixel_center}.'\n    )\n\n  # Get the shape of the data.\n  data_shape = data.shape\n\n  # Get the shape of the locations.\n  locations_shape = locations.shape\n\n  # Get the number of dimensions in the locations.\n  num_dims = locations_shape[0]\n\n  # Get the number of samples.\n  num_samples = np.prod(locations_shape[1:-1])"}
{"namespace": "resample.resample_3d", "completion": "  # Check the shape of the input data.\n  if data.ndim != 4:\n    raise ValueError('data must have 4 dimensions [D, H, W, C].')\n\n  # Check the shape of the locations.\n  if locations.ndim < 2 or locations.shape[-1] != 3:\n    raise ValueError('locations must have at least 2 dimensions and '\n                     'the last dimension must be 3.')\n\n  # Check the coordinate order.\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError('coordinate_order must be either \"xyz\" or \"zyx\".')\n\n  # Check the edge behavior.\n  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError('edge_behavior must be either \"CONSTANT_OUTSIDE\" '\n                     'or \"CLAMP\".')\n\n  # Check the method.\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError('method must be either \"TRILINEAR\" or \"NEAREST\".')\n\n  # Check the shape of the data.\n  if data.ndim != 4:\n    raise ValueError('data must have 4 dimensions [D, H, W, C].')\n\n  # Check the shape of the locations.\n  if locations.ndim < 2 or locations.shape[-1] != 3:\n    raise ValueError('locations must have at least 2 dimensions and '\n                     'the last dimension must be 3.')\n\n  # Check the coordinate order.\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError('coordinate_order must be either \"xyz\" or \"zyx\".')\n\n  # Check the edge behavior.\n  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError('edge_behavior must be either \"CONSTANT_OUTSIDE\" '\n                     'or \"CLAMP\".')\n\n  # Check the method.\n  if method not in ['TRIL"}
{"namespace": "resample.resample_3d", "completion": "  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError('Unsupported resampling method.')\n\n  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError('Unsupported edge behavior.')\n\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError('Unsupported coordinate order.')\n\n  if method == 'TRILINEAR':\n    resample_fn = functools.partial(\n        trilinear_interpolation,\n        edge_behavior=edge_behavior,\n        constant_values=constant_values,\n        coordinate_order=coordinate_order,\n        half_pixel_center=half_pixel_center,\n    )\n  elif method == 'NEAREST':\n    resample_fn = functools.partial(\n        nearest_neighbor_interpolation,\n        edge_behavior=edge_behavior,\n        constant_values=constant_values,\n        coordinate_order=coordinate_order,\n    )\n\n  return resample_fn(data, locations)\n\n"}
{"namespace": "resample.resample_3d", "completion": "  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError('Unknown edge behavior: {}'.format(edge_behavior))\n\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError('Unknown interpolation method: {}'.format(method))\n\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError('Unknown coordinate order: {}'.format(coordinate_order))\n\n  if data.ndim != 4:\n    raise ValueError('data must have 4 dimensions, but got {}'.format(data.ndim))\n\n  if locations.ndim < 2 or locations.shape[-1] != 3:\n    raise ValueError('locations must have at least 2 dimensions, and the last dimension must be size 3, but got {}'.format(locations.shape))\n\n  if locations.shape[0] != data.shape[0]:\n    raise ValueError('First dimension of locations must be same as data.shape[0], but got {} and {}'.format(locations.shape[0], data.shape[0]))\n\n  if locations.dtype != jnp.float32:\n    raise ValueError('locations must be float32, but got {}'.format(locations.dtype))\n\n  if data.dtype != jnp.float32:\n    raise ValueError('data must be float32, but got {}'.format(data.dtype))\n\n  if half_pixel_center:\n    offset = 0.5\n  else:\n    offset = 0.0\n\n  if coordinate_order == 'xyz':\n    x_coordinate = locations[Ellipsis, 0] + offset\n    y_coordinate = locations[Ellipsis, 1] + offset\n    z_coordinate = locations[Ellipsis, 2] + offset\n  elif coordinate_order == 'zyx':\n    z_coordinate = locations[Ellipsis, 0] + offset\n    y_coordinate = locations[Ellipsis, 1] + offset"}
{"namespace": "resample.resample_3d", "completion": "  if coordinate_order == 'xyz':\n    x_coordinate = locations[Ellipsis, 0]\n    y_coordinate = locations[Ellipsis, 1]\n    z_coordinate = locations[Ellipsis, 2]\n  elif coordinate_order == 'zyx':\n    z_coordinate = locations[Ellipsis, 0]\n    y_coordinate = locations[Ellipsis, 1]\n    x_coordinate = locations[Ellipsis, 2]\n\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    # Pad the data with constant values.\n    data = jnp.pad(\n        data,\n        pad_width=((1, 1), (1, 1), (1, 1), (0, 0)),\n        mode='constant',\n        constant_values=constant_values)\n    # Adjust the locations to account for the padding.\n    x_coordinate = x_coordinate + 1\n    y_coordinate = y_coordinate + 1\n    z_coordinate = z_coordinate + 1\n\n  if method == 'TRILINEAR':\n    if half_pixel_center:\n      # Adjust the locations for half-pixel centering.\n      x_coordinate = x_coordinate + 0.5\n      y_coordinate = y_coordinate + 0.5\n      z_coordinate = z_coordinate + 0.5\n\n    # Compute the neighboring indices.\n    x_0 = jnp.floor(x_coordinate).astype(jnp.int32)\n    x_1 = x_0 + 1\n    y_0 = jnp.floor(y_coordinate).astype(jnp.int32)\n    y_1 = y_0 + 1\n    z_0 = jnp.floor(z_coordinate).astype(jnp.int32)\n    z_1 = z_0 + 1\n\n    # Compute the weights for trilinear interpolation.\n    x_0_weight = x_1 - x_coordinate"}
{"namespace": "resample.resample_3d", "completion": "  if edge_behavior == 'CONSTANT_OUTSIDE':\n    # Pad the input volume with a constant value outside the volume.\n    data = jnp.pad(\n        data,\n        ((1, 1), (1, 1), (1, 1), (0, 0)),\n        mode='constant',\n        constant_values=constant_values,\n    )\n    locations = locations + 1\n\n  # Get the coordinates of the sample points.\n  if coordinate_order == 'xyz':\n    x_coordinate = locations[Ellipsis, 0]\n    y_coordinate = locations[Ellipsis, 1]\n    z_coordinate = locations[Ellipsis, 2]\n  elif coordinate_order == 'zyx':\n    z_coordinate = locations[Ellipsis, 0]\n    y_coordinate = locations[Ellipsis, 1]\n    x_coordinate = locations[Ellipsis, 2]\n\n  # Get the coordinates of the 8 nearest neighbors for each sample point.\n  x0 = jnp.floor(x_coordinate)\n  x1 = x0 + 1\n  y0 = jnp.floor(y_coordinate)\n  y1 = y0 + 1\n  z0 = jnp.floor(z_coordinate)\n  z1 = z0 + 1\n\n  # Calculate the weights for trilinear interpolation.\n  if method == 'TRILINEAR':\n    if half_pixel_center:\n      x0_weight = x1 - x_coordinate\n      x1_weight = x_coordinate - x0\n      y0_weight = y1 - y_coordinate\n      y1_weight = y_coordinate - y0\n      z0_weight = z1 - z_coordinate\n      z1_weight = z_coordinate - z0\n    else:\n      x0_weight = 1 - (x_coordinate - x0)\n      x1_weight = 1 - (x1 - x_coordinate)\n      y0_weight = 1 - (y_coordinate"}
{"namespace": "resample.resample_3d", "completion": "  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError(\n        'Invalid edge_behavior: {}. Must be one of \"CONSTANT_OUTSIDE\" or \"CLAMP\".'\n        .format(edge_behavior))\n\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError('Invalid method: {}. Must be one of \"TRILINEAR\" or \"NEAREST\".'\n                     .format(method))\n\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError('Invalid coordinate_order: {}. Must be one of \"xyz\" or \"zyx\".'\n                     .format(coordinate_order))\n\n  # Pad the volume to handle edge behavior.\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    padded_data = jnp.pad(\n        data,\n        jnp.array([[1, 1], [1, 1], [1, 1], [0, 0]]),\n        mode='constant',\n        constant_values=constant_values)\n  elif edge_behavior == 'CLAMP':\n    padded_data = jnp.pad(\n        data,\n        jnp.array([[1, 1], [1, 1], [1, 1], [0, 0]]),\n        mode='edge')\n\n  # Adjust the sample locations to account for half-pixel centering.\n  if half_pixel_center:\n    locations = locations + 0.5\n\n  # Get the coordinates of the top-left corner of each sample point.\n  if coordinate_order == 'xyz':\n    x_coordinate = jnp.floor(locations[Ellipsis, 0])\n    y_coordinate = jnp.floor(locations[Ellipsis, 1])\n    z_coordinate = jnp.floor(locations[Ellipsis, 2])\n  elif coordinate_order == 'zyx':\n    z_coordinate = jnp.floor(locations"}
{"namespace": "resample.resample_3d", "completion": "  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError(\n        f'Invalid edge behavior: {edge_behavior}. Must be one of CONSTANT_OUTSIDE or CLAMP.'\n    )\n\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError(\n        f'Invalid resample method: {method}. Must be one of TRILINEAR or NEAREST.'\n    )\n\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError(\n        f'Invalid coordinate order: {coordinate_order}. Must be one of xyz or zyx.'\n    )\n\n  if data.ndim != 4:\n    raise ValueError(\n        f'Invalid data shape: {data.shape}. Data must have 4 dimensions.'\n    )\n\n  if locations.ndim < 2 or locations.shape[-1] != 3:\n    raise ValueError(\n        f'Invalid locations shape: {locations.shape}. Locations must have at least 2 dimensions and the last dimension must be 3.'\n    )\n\n  if locations.dtype != jnp.float32:\n    raise ValueError(\n        f'Invalid locations dtype: {locations.dtype}. Locations must have dtype float32.'\n    )\n\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    pad_width = ((1, 1), (1, 1), (1, 1), (0, 0))\n    data = jnp.pad(data, pad_width, constant_values=constant_values)\n    locations = locations + 1\n\n  if edge_behavior == 'CLAMP':\n    data = jnp.pad(data, 1, mode='edge')\n    locations = jnp.clip(locations, 0, np.array(data.shape[1:4]) - 1)\n\n  if half_pixel_center:\n    locations = locations + 0.5\n\n  if method == 'NEAREST':\n    locations = jnp"}
{"namespace": "resample.resample_3d", "completion": "  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError(\n        'Invalid value for edge_behavior, must be CONSTANT_OUTSIDE or CLAMP'\n    )\n\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError('Invalid value for method, must be TRILINEAR or NEAREST')\n\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError('Invalid value for coordinate_order, must be xyz or zyx')\n\n  # Pad the volume to handle edge behavior.\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    pad_width = ((1, 1), (1, 1), (1, 1), (0, 0))\n    data = jnp.pad(\n        data, pad_width, mode='constant', constant_values=constant_values\n    )\n    locations = locations + 1\n\n  # Clamp the locations to the volume.\n  if edge_behavior == 'CLAMP':\n    locations = jnp.clip(locations, 0, jnp.array(data.shape[:3]) - 1)\n\n  # Half pixel center the locations.\n  if half_pixel_center:\n    locations = locations + 0.5\n\n  # Compute the sample locations.\n  if coordinate_order == 'xyz':\n    x_coordinate = locations[Ellipsis, 0]\n    y_coordinate = locations[Ellipsis, 1]\n    z_coordinate = locations[Ellipsis, 2]\n  elif coordinate_order == 'zyx':\n    z_coordinate = locations[Ellipsis, 0]\n    y_coordinate = locations[Ellipsis, 1]\n    x_coordinate = locations[Ellipsis, 2]\n\n  # Compute the sample locations.\n  if coordinate_order == 'xyz':\n    x_coordinate = locations[Ellipsis, 0]\n    y_coordinate = locations"}
{"namespace": "resample.resample_3d", "completion": "  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError('Unknown edge behavior: {}'.format(edge_behavior))\n\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError('Unknown resample method: {}'.format(method))\n\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError('Unknown coordinate order: {}'.format(coordinate_order))\n\n  if half_pixel_center:\n    offset = 0.5\n  else:\n    offset = 0.0\n\n  if method == 'TRILINEAR':\n    # Get the voxel centers.\n    locations_floor = jnp.floor(locations - 0.5)\n    locations_ceil = locations_floor + 1.0\n\n    # Compute the interpolation weights.\n    locations_frac = locations - locations_floor\n\n    # Compute the interpolation weights.\n    x_lerp = locations_frac[Ellipsis, 0]\n    y_lerp = locations_frac[Ellipsis, 1]\n    z_lerp = locations_frac[Ellipsis, 2]\n\n    # Compute the gather coordinates.\n    x_coordinate = locations_floor[Ellipsis, 0] + offset\n    y_coordinate = locations_floor[Ellipsis, 1] + offset\n    z_coordinate = locations_floor[Ellipsis, 2] + offset\n\n    # Compute the voxel centers.\n    x_lerp_coordinate = jnp.stack([x_lerp, 1.0 - x_lerp], axis=-1)\n    y_lerp_coordinate = jnp.stack([y_lerp, 1.0 - y_lerp], axis=-1)\n    z_lerp_coordinate = jnp.stack([z_lerp, 1.0 - z_lerp], axis=-1)\n\n    # Compute the trilinear weights.\n    x_lerp_coordinate"}
{"namespace": "resample.resample_3d", "completion": "  # Determine the shape of the input data\n  input_shape = data.shape\n\n  # Pad the input volume if edge_behavior is set to 'CONSTANT_OUTSIDE'\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    pad_width = ((1, 1), (1, 1), (1, 1), (0, 0))\n    data = jnp.pad(data, pad_width, mode='constant', constant_values=constant_values)\n\n  # Calculate the sample locations based on the input locations and the half_pixel_center parameter\n  if half_pixel_center:\n    sample_locations = locations + 0.5\n  else:\n    sample_locations = locations\n\n  # Calculate the lower and upper bounding indices for each sample location\n  lower_bounds = jnp.floor(sample_locations).astype(jnp.int32)\n  upper_bounds = jnp.ceil(sample_locations).astype(jnp.int32)\n\n  # Clamp the sample locations to the volume if edge_behavior is set to 'CLAMP'\n  if edge_behavior == 'CLAMP':\n    lower_bounds = jnp.clip(lower_bounds, 0, input_shape[:3] - 1)\n    upper_bounds = jnp.clip(upper_bounds, 0, input_shape[:3] - 1)\n\n  # Calculate the interpolation weights for each sample location\n  if method == 'TRILINEAR':\n    weights = sample_locations - lower_bounds\n  elif method == 'NEAREST':\n    weights = jnp.zeros_like(sample_locations)\n\n  # Calculate the interpolation weights for each sample location\n  if method == 'TRILINEAR':\n    weights = sample_locations - lower_bounds\n  elif method == 'NEAREST':\n    weights = jnp.zeros_like(sample_locations)\n\n  # Calculate the interpolation weights for each sample location\n  if method == 'TRILINEAR':"}
{"namespace": "resample.resample_3d", "completion": "  # Pad the volume by 1 voxel on each side to allow for edge behavior outside the volume.\n  padded_volume = jnp.pad(\n      array=data, pad_width=((1, 1), (1, 1), (1, 1), (0, 0)),\n      mode='constant',\n      constant_values=constant_values)\n\n  # Get the shape of the volume and the number of samples.\n  volume_shape = padded_volume.shape[:3]\n  num_samples = locations.shape[1]\n\n  # Adjust the sample locations to account for half-pixel centering if enabled.\n  if half_pixel_center:\n    locations = locations + 0.5\n\n  # Calculate the sample locations in the volume.\n  if coordinate_order == 'xyz':\n    z_coordinate = locations[Ellipsis, 0]\n    y_coordinate = locations[Ellipsis, 1]\n    x_coordinate = locations[Ellipsis, 2]\n  elif coordinate_order == 'zyx':\n    x_coordinate = locations[Ellipsis, 0]\n    y_coordinate = locations[Ellipsis, 1]\n    z_coordinate = locations[Ellipsis, 2]\n\n  # Calculate the lower and upper bounding indices for each sample location.\n  lower_z_idx = jnp.floor(z_coordinate).astype(jnp.int32)\n  lower_y_idx = jnp.floor(y_coordinate).astype(jnp.int32)\n  lower_x_idx = jnp.floor(x_coordinate).astype(jnp.int32)\n  upper_z_idx = jnp.ceil(z_coordinate).astype(jnp.int32)\n  upper_y_idx = jnp.ceil(y_coordinate).astype(jnp.int32)\n  upper_x_idx = jnp.ceil(x_coordinate).astype(jnp.int32)\n\n  # Clamp"}
{"namespace": "resample.resample_3d", "completion": "  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError(\n        f'coordinate_order must be one of \"xyz\" or \"zyx\", got {coordinate_order}'\n    )\n\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError(\n        f'method must be one of \"TRILINEAR\" or \"NEAREST\", got {method}'\n    )\n\n  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError(\n        f'edge_behavior must be one of \"CONSTANT_OUTSIDE\" or \"CLAMP\", got {edge_behavior}'\n    )\n\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    # Pad the data with constant values along the spatial dimensions.\n    pad_width = ((1, 1), (1, 1), (1, 1), (0, 0))\n    data = jnp.pad(\n        data, pad_width, mode='constant', constant_values=constant_values\n    )\n\n  # Pad the locations with zeros along the spatial dimensions.\n  pad_width = ((0, 0), (0, 0), (0, 0), (0, 1))\n  locations = jnp.pad(locations, pad_width, mode='constant', constant_values=0)\n\n  if half_pixel_center:\n    # Add 0.5 to the locations to sample from the center of the voxel.\n    locations += 0.5\n\n  if coordinate_order == 'xyz':\n    locations = jnp.flip(locations, axis=-1)\n\n  if method == 'NEAREST':\n    # Round the locations to the nearest integer.\n    locations = jnp.round(locations)\n\n  # Gather from the data at the locations.\n  resampled_values = gather_volume(data, locations, coordinate_order)\n\n  return resampled_values"}
{"namespace": "resample.resample_3d", "completion": "  # Get the shape of the input data\n  data_shape = data.shape\n  # Get the number of dimensions of the input data\n  num_dims = len(data_shape)\n  # Get the number of dimensions of the sample locations\n  num_sample_dims = len(locations.shape)\n  # Get the number of sample points\n  num_samples = locations.shape[-num_sample_dims + 1]\n  # Get the number of channels of the input data\n  num_channels = data_shape[-1]\n  # Get the shape of the sample locations\n  sample_shape = locations.shape[:-1]\n\n  # If the number of dimensions of the input data is not 4, raise an error\n  if num_dims != 4:\n    raise ValueError('data must be 4D but is {}D'.format(num_dims))\n\n  # If the number of dimensions of the sample locations is not 2 or 3, raise an error\n  if num_sample_dims not in [2, 3]:\n    raise ValueError(\n        'locations must have 2 or 3 dimensions but has {}'.format(\n            num_sample_dims))\n\n  # If the number of channels of the input data is not 1, raise an error\n  if num_channels != 1:\n    raise ValueError('data must have 1 channel but has {} channels'.format(\n        num_channels))\n\n  # If the edge behavior is not 'CONSTANT_OUTSIDE' or 'CLAMP', raise an error\n  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError(\n        'edge_behavior must be \\'CONSTANT_OUTSIDE\\' or \\'CLAMP\\' but is {}'.\n        format(edge_behavior))\n\n  # If the coordinate order is not 'xyz' or 'zyx', raise an error\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError(\n        'coordinate_order must be \\'xyz\\' or \\'zyx\\' but is {}"}
{"namespace": "resample.resample_3d", "completion": "  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError(\n        f'Invalid edge behavior: {edge_behavior}. Must be one of \"CONSTANT_OUTSIDE\", \"CLAMP\".'\n    )\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError(\n        f'Invalid resampling method: {method}. Must be one of \"TRILINEAR\", \"NEAREST\".'\n    )\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError(\n        f'Invalid coordinate order: {coordinate_order}. Must be one of \"xyz\", \"zyx\".'\n    )\n  if data.ndim != 4:\n    raise ValueError(\n        f'Invalid input data shape: {data.shape}. Must be of shape [D, H, W, C].'\n    )\n  if locations.ndim < 2:\n    raise ValueError(\n        f'Invalid locations shape: {locations.shape}. Must be at least 2D.'\n    )\n\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    # Pad the input volume with constant values outside the tensor.\n    padded_data = jnp.pad(\n        array=data,\n        pad_width=((1, 1), (1, 1), (1, 1), (0, 0)),\n        mode='constant',\n        constant_values=constant_values,\n    )\n    padded_locations = locations + 1\n  elif edge_behavior == 'CLAMP':\n    # Clamp sample points to the volume.\n    padded_data = data\n    padded_locations = jnp.clip(\n        locations, a_min=0, a_max=jnp.array(data.shape[1:4]) - 1\n    )\n\n  if half_pixel_center:\n    # Offset sample locations for half-pixel centering.\n    padded_locations += 0.5\n\n  # Compute the integer coordinates of the sample locations"}
{"namespace": "resample.resample_3d", "completion": "  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError('Unknown edge_behavior: %s' % edge_behavior)\n\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError('Unknown method: %s' % method)\n\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError('Unknown coordinate_order: %s' % coordinate_order)\n\n  if data.ndim != 4:\n    raise ValueError('data must have 4 dimensions but has %d' % data.ndim)\n\n  if locations.ndim < 2 or locations.shape[-1] != 3:\n    raise ValueError('locations must have at least 2 dimensions and '\n                     'its last dimension must be 3 but has shape %s' %\n                     (locations.shape,))\n\n  # Pad the volume by 0.5 on each side to allow for fractional indexing.\n  pad_width = jnp.array([[0, 0], [0, 0], [0, 0], [0, 0]])\n  data = jnp.pad(data, pad_width, 'constant')\n\n  # Convert to floating point and shift 0.5 to sample at voxel centers.\n  locations = locations.astype(jnp.float32)\n  if half_pixel_center:\n    locations += 0.5\n\n  # Interpolate at the sample locations.\n  if method == 'TRILINEAR':\n    # Get the voxel centers surrounding the sample locations.\n    voxels_z = jnp.floor(locations[Ellipsis, 0])\n    floor_z = voxels_z.astype(jnp.int32)\n    ceil_z = floor_z + 1\n    voxels_y = jnp.floor(locations[Ellipsis, 1])\n    floor_y = voxels_y.astype(jnp.int32)\n    ceil_y = floor_y + 1"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x), jnp.inf))\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x), jnp.inf))\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x), jnp.inf))"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x),\n                                                                jnp.inf))\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x), jnp.inf))"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x),\n                                                                jnp.inf))"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x),\n                                                                jnp.inf))"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x),\n                                                                 jnp.inf))\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x), jnp.inf))\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x),\n                                                                 jnp.inf))"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x),\n                                                                 jnp.inf))\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x),\n                                                                jnp.inf))\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x),\n                                                                 jnp.inf))\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x),\n                                                                 jnp.inf))\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x),\n                                                                 jnp.inf))\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x), jnp.inf))\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x),\n                                                                 jnp.inf))\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x),\n                                                                jnp.inf))\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x),\n                                                                jnp.inf))\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(jnp.float32(x),\n                                                                jnp.inf))\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), jnp.NINF)\n  )"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, -tiny_val,\n                   jnp.nextafter(jnp.float32(x), jnp.float32(-np.inf)))"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), jnp.float32(0))\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, -tiny_val,\n                   jnp.nextafter(jnp.float32(x), -jnp.inf))"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), jnp.NINF)\n  )"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), jnp.NINF)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), jnp.inf)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), jnp.NINF)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), jnp.float32(0))\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), jnp.NINF)\n  )\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), jnp.NINF)\n  )\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: jnp.exp(x) * x_dot,\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, _, x_dot: jnp.exp(x) * x_dot,\n      (min_val, np.log(max_val)),\n  )(x)"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, _, x_dot: jnp.exp(x) * x_dot,\n      (min_val, np.log(max_val)),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, y, x_dot: x_dot / jnp.maximum(tiny_val, x),\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, y, x_dot: x_dot / jnp.maximum(tiny_val, x),\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, y, x_dot: x_dot / jnp.maximum(tiny_val, x),\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, y, x_dot: x_dot / jnp.maximum(tiny_val, x),\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, y, x_dot: x_dot / jnp.maximum(tiny_val, x),\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, y, x_dot: x_dot / jnp.maximum(tiny_val, x),\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, y, x_dot: x_dot / jnp.maximum(tiny_val, x),\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, y, x_dot: x_dot / remove_zero(x),\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, y, x_dot: x_dot / remove_zero(x),\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / jnp.maximum(tiny_val, x),\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / jnp.maximum(tiny_val, x),\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / jnp.maximum(tiny_val, x),\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, y, x_dot: x_dot / jnp.maximum(tiny_val, jnp.abs(x)),\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, y, x_dot: x_dot / jnp.maximum(tiny_val, jnp.abs(x)),\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda _, y, x_dot: x_dot / remove_zero(x),\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / remove_zero(x),\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, y, x_dot: jnp.clip(x_dot / remove_zero(x), min_val, max_val),\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, y, x_dot: jnp.clip(x_dot / x, min_val, max_val),\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / x,\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda _, y, x_dot: x_dot / remove_zero(y),\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: 0.5 * x_dot / y,\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: 0.5 * x_dot / y,\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: 0.5 * x_dot / y,\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: 0.5 * x_dot / y,\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: 0.5 * x_dot / y,\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: 0.5 * x_dot / y,\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: 0.5 * x_dot / y,\n      (0, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: 0.5 * x_dot / y,\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: 0.5 * x_dot / y,\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: 0.5 * x_dot / y,\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda _, y, x_dot: 0.5 * x_dot / y,\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: 0.5 * x_dot / y,\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: 0.5 * x_dot / y,\n      (0, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: 0.5 * x_dot / y,\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda _, y, x_dot: 0.5 * x_dot / y,\n      (0, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, _, x_dot: 0.5 * x_dot / jnp.sqrt(x),\n      (0, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda _, y, x_dot: x_dot / (2 * y),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, _, x_dot: 0.5 * x_dot / x,\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: x_dot / (2 * y),\n      (0, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: x_dot * 0.5 / y,\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return 1\n  elif p == 0:\n    return jnp.inf\n  elif p == -jnp.inf:\n    return 0\n  elif p == jnp.inf:\n    return jnp.inf\n  else:\n    return jnp.abs(p - 1) / p"}
{"namespace": "math.power_ladder_max_output", "completion": "  return select(\n      [\n          (p == 1, jnp.inf),\n          (p == 0, 1),\n          (p == -jnp.inf, 0),\n          (p == jnp.inf, jnp.inf),\n      ],\n      jnp.nan,\n  )"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return 1\n  if p == 0:\n    return jnp.inf\n  if p == -jnp.inf:\n    return 0\n  if p == jnp.inf:\n    return jnp.inf\n  return jnp.abs(p - 1) / p"}
{"namespace": "math.power_ladder_max_output", "completion": "  return select(\n      [\n          (p == 1, 1),\n          (p == 0, jnp.inf),\n          (p == -jnp.inf, 0),\n          (p == jnp.inf, jnp.inf),\n      ],\n      jnp.nan,\n  )"}
{"namespace": "math.power_ladder_max_output", "completion": "  return select(\n      [\n          (p == 1, 1),\n          (p == 0, jnp.inf),\n          (p == -jnp.inf, 0),\n          (p == jnp.inf, 1),\n      ],\n      jnp.nan,\n  )"}
{"namespace": "math.power_ladder_max_output", "completion": "  return select(\n      [\n          (p == 1, 1),\n          (p == 0, jnp.inf),\n          (p == -jnp.inf, 0),\n          (p == jnp.inf, 1),\n      ],\n      jnp.nan,\n  )"}
{"namespace": "math.power_ladder_max_output", "completion": "  return select(\n      [\n          (p == 1, jnp.inf),\n          (p == 0, 1),\n          (p == -jnp.inf, 0),\n          (p == jnp.inf, jnp.inf),\n      ],\n      jnp.nan,\n  )"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p > 0:\n    return jnp.inf\n  elif p < 0:\n    return -jnp.inf\n  else:\n    return 1"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return jnp.inf\n  elif p == -jnp.inf:\n    return 0\n  elif p == 0:\n    return 1\n  elif p == jnp.inf:\n    return jnp.inf\n  else:\n    return jnp.abs(p - 1) / p"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return 1\n  elif p == 0:\n    return max_val\n  elif p == -jnp.inf:\n    return min_val\n  elif p == jnp.inf:\n    return max_val\n  else:\n    return max_val"}
{"namespace": "math.power_ladder_max_output", "completion": "  return select([\n      (p == 1, 1),\n      (p == 0, jnp.inf),\n      (p == -jnp.inf, 0),\n      (p == jnp.inf, 1),\n  ], jnp.inf)"}
{"namespace": "math.power_ladder_max_output", "completion": "  return select(\n      [\n          (p == 1, jnp.inf),\n          (p == 0, 1),\n          (p == -jnp.inf, 0),\n          (p == jnp.inf, jnp.inf),\n      ],\n      jnp.inf,\n  )"}
{"namespace": "math.power_ladder_max_output", "completion": "  return select(\n      [\n          (p == 1, 1),\n          (p == 0, jnp.inf),\n          (p == -jnp.inf, 0),\n          (p == jnp.inf, jnp.inf),\n      ],\n      jnp.inf,\n  )"}
{"namespace": "math.power_ladder_max_output", "completion": "  return select(\n      [\n          (p == 1, 1),\n          (p == 0, jnp.inf),\n          (p == -jnp.inf, 1),\n          (p == jnp.inf, 1),\n      ],\n      jnp.inf,\n  )"}
{"namespace": "math.power_ladder_max_output", "completion": "  return select(\n      [\n          (p == 1, 1),\n          (p == 0, max_val),\n          (p == -jnp.inf, 1),\n          (p == jnp.inf, max_val),\n      ],\n      max_val,\n  )"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return jnp.inf\n  if p == -jnp.inf:\n    return 0\n  if p == 0:\n    return 1\n  if p == jnp.inf:\n    return jnp.inf\n  return jnp.abs(p - 1) / p"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return 1\n  elif p == 0:\n    return np.inf\n  elif p == -jnp.inf:\n    return 0\n  elif p == jnp.inf:\n    return np.inf\n  else:\n    return (1 / jnp.abs(p - 1))**(1 / jnp.abs(p))"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return 1\n  elif p == 0:\n    return jnp.exp(1)\n  elif p == -jnp.inf:\n    return 0\n  elif p == jnp.inf:\n    return jnp.inf\n  else:\n    return (1 - (1 / jnp.abs(p - 1)))**(1 / p)"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 1\n  elif p == jnp.inf:\n    return jnp.inf\n  elif p == -jnp.inf:\n    return 0\n  else:\n    return jnp.abs(p - 1)"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return jnp.inf\n  elif p == 0:\n    return jnp.e\n  elif p == -jnp.inf:\n    return 0\n  elif p == jnp.inf:\n    return jnp.inf\n  else:\n    return jnp.where(\n        p > 0,\n        jnp.where(\n            p > 1,\n            jnp.inf,\n            jnp.where(\n                p > 0.5,\n                jnp.inf,\n                jnp.where(\n                    p > 0.25,\n                    jnp.inf,\n                    jnp.where(\n                        p > 0.125,\n                        jnp.inf,\n                        jnp.where(\n                            p > 0.0625,\n                            jnp.inf,\n                            jnp.where(\n                                p > 0.03125,\n                                jnp.inf,\n                                jnp.where(\n                                    p > 0.015625,\n                                    jnp.inf,\n                                    jnp.where(\n                                        p > 0.0078125,\n                                        jnp.inf,\n                                        jnp.where(\n                                            p > 0.00390625,\n                                            jnp.inf,\n                                            jnp.where(\n                                                p > 0.001953125,\n                                                jnp.inf,\n                                                jnp.where(\n                                                    p > 0.0009765625,\n                                                    jnp.inf,\n                                                    jnp.where(\n                                                        p > 0.00048828125,\n                                                        jnp.inf,\n                                                        jnp.where(\n                                                            p > 0.000244140625,\n                                                            jnp.inf,\n                                                            jnp.where(\n                                                                p > 0.0001220703125,\n                                                                jnp.inf,\n                                                                jnp.where(\n                                                                    p > 0.00006103515"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([[1, 1, 1], [-1, -1, 1], [1, -1, -1], [-1, 1, -1]])\n    base_faces = np.array([[0, 1, 2], [0, 2, 3], [0, 3, 1], [1, 3, 2]])\n  elif base_shape == 'icosahedron':\n    phi = (1 + np.sqrt(5)) / 2\n    base_verts = np.array(\n        [\n            [0, 1, phi],\n            [0, -1, phi],\n            [1, phi, 0],\n            [-1, phi, 0],\n            [phi, 0, 1],\n            [-phi, 0, -1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 2, 1],\n            [0, 3, 2],\n            [0, 4, 3],\n            [0, 1, 4],\n            [1, 2, 5],\n            [2, 3, 5],\n            [3, 4, 5],\n            [4, 1, 5],\n        ]\n    )\n  elif base_shape == 'octahedron':\n    base_verts = np.array(\n        [\n            [1, 0, 0],\n            [-1, 0, 0],\n            [0, 1, 0],\n            [0, -1, 0],\n            [0, 0, 1],\n            [0, 0, -1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 2, 4],\n            [0, 4, 3],\n            [0, 5, 2],\n            [0, 3, 5],\n            [1, 4, 2],"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array(\n        [\n            [1, 1, 1],\n            [1, -1, -1],\n            [-1, 1, -1],\n            [-1, -1, 1],\n        ]\n    )\n    base_faces = np.array([[0, 1, 2], [0, 2, 3], [0, 3, 1], [1, 3, 2]])\n  elif base_shape == 'icosahedron':\n    phi = (1 + np.sqrt(5)) / 2\n    base_verts = np.array(\n        [\n            [0, 1, phi],\n            [0, -1, -phi],\n            [phi, 0, 1],\n            [phi, 0, -1],\n            [-phi, 0, 1],\n            [-phi, 0, -1],\n            [1, phi, 0],\n            [-1, phi, 0],\n            [1, -phi, 0],\n            [-1, -phi, 0],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 2, 1],\n            [0, 3, 2],\n            [0, 4, 3],\n            [0, 1, 4],\n            [5, 1, 2],\n            [5, 2, 3],\n            [5, 3, 4],\n            [5, 4, 1],\n        ]\n    )\n  elif base_shape == 'octahedron':\n    base_verts = np.array(\n        [\n            [1, 0, 0],\n            [-1, 0, 0],\n            [0, 1, 0],\n            [0, -1, 0],\n            [0, 0, 1],\n            [0, 0, -1],\n        ]\n    )\n    base_faces = np.array"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array(\n        [\n            [1, 1, 1],\n            [-1, -1, 1],\n            [-1, 1, -1],\n            [1, -1, -1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 1, 2],\n            [0, 2, 3],\n            [0, 3, 1],\n            [1, 3, 2],\n        ]\n    )\n  elif base_shape == 'icosahedron':\n    phi = (1 + np.sqrt(5)) / 2\n    base_verts = np.array(\n        [\n            [0, 1, phi],\n            [0, -1, phi],\n            [1, phi, 0],\n            [-1, phi, 0],\n            [phi, 0, 1],\n            [-phi, 0, -1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 2, 1],\n            [0, 3, 2],\n            [0, 4, 3],\n            [0, 1, 4],\n            [1, 2, 5],\n            [2, 3, 5],\n            [3, 4, 5],\n            [4, 1, 5],\n        ]\n    )\n  elif base_shape == 'octahedron':\n    base_verts = np.array(\n        [\n            [1, 0, 0],\n            [-1, 0, 0],\n            [0, 1, 0],\n            [0, -1, 0],\n            [0, 0, 1],\n            [0, 0, -1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 2, 4],\n            [0, 4, 3],\n           "}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array(\n        [\n            [1, 1, 1],\n            [-1, -1, 1],\n            [1, -1, -1],\n            [-1, 1, -1],\n        ]\n    )\n    base_faces = np.array([[0, 1, 2], [0, 2, 3], [0, 3, 1], [1, 3, 2]])\n  elif base_shape == 'icosahedron':\n    phi = (1 + np.sqrt(5)) / 2\n    base_verts = np.array(\n        [\n            [0, 1, phi],\n            [0, -1, phi],\n            [1, phi, 0],\n            [-1, phi, 0],\n            [phi, 0, 1],\n            [-phi, 0, -1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 1, 2],\n            [0, 2, 3],\n            [0, 3, 4],\n            [0, 4, 1],\n            [1, 2, 5],\n            [2, 3, 5],\n            [3, 4, 5],\n            [4, 1, 5],\n        ]\n    )\n  elif base_shape == 'octahedron':\n    base_verts = np.array(\n        [\n            [1, 0, 0],\n            [-1, 0, 0],\n            [0, 1, 0],\n            [0, -1, 0],\n            [0, 0, 1],\n            [0, 0, -1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 1, 5],\n            [0, 2, 5],\n            [0, 3, 5],\n            [0, "}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array(\n        [\n            [1, 1, 1],\n            [-1, -1, 1],\n            [1, -1, -1],\n            [-1, 1, -1],\n        ]\n    )\n    base_faces = np.array(\n        [[0, 1, 2], [0, 2, 3], [0, 3, 1], [1, 3, 2]]\n    )\n  elif base_shape == 'icosahedron':\n    phi = (1 + np.sqrt(5)) / 2\n    base_verts = np.array(\n        [\n            [0, 1, phi],\n            [0, -1, phi],\n            [0, 1, -phi],\n            [0, -1, -phi],\n            [1, phi, 0],\n            [-1, phi, 0],\n            [1, -phi, 0],\n            [-1, -phi, 0],\n            [phi, 0, 1],\n            [phi, 0, -1],\n            [-phi, 0, 1],\n            [-phi, 0, -1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 1, 4],\n            [0, 4, 9],\n            [9, 5, 4],\n            [4, 5, 8],\n            [4, 8, 1],\n            [8, 10, 1],\n            [8, 3, 10],\n            [5, 3, 8],\n            [5, 2, 3],\n            [2, 7, 3],\n            [7, 10, 3],\n            [7, 6, 10],\n            [7, 11, 6],\n            [11, 0, 6],\n            [0, 1, 6],\n            [6"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array(\n        [\n            [1, 1, 1],\n            [1, -1, -1],\n            [-1, 1, -1],\n            [-1, -1, 1],\n        ]\n    )\n    base_faces = np.array([[0, 1, 2], [0, 2, 3], [0, 3, 1], [1, 3, 2]])\n  elif base_shape == 'icosahedron':\n    # https://en.wikipedia.org/wiki/Icosahedron#Cartesian_coordinates\n    phi = (1 + np.sqrt(5)) / 2\n    base_verts = np.array(\n        [\n            [0, 1, phi],\n            [0, -1, phi],\n            [0, 1, -phi],\n            [0, -1, -phi],\n            [1, phi, 0],\n            [-1, phi, 0],\n            [1, -phi, 0],\n            [-1, -phi, 0],\n            [phi, 0, 1],\n            [phi, 0, -1],\n            [-phi, 0, 1],\n            [-phi, 0, -1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 1, 4],\n            [0, 4, 9],\n            [9, 5, 4],\n            [4, 5, 8],\n            [4, 8, 1],\n            [8, 10, 1],\n            [8, 3, 10],\n            [5, 3, 8],\n            [5, 2, 3],\n            [2, 7, 3],\n            [7, 10, 3],\n            [7, 6, 10],\n            [7, 11, 6],\n            [11, "}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array(\n        [\n            [-1, -1, -1],\n            [1, 1, 1],\n            [-1, 1, 1],\n            [1, -1, 1],\n        ]\n    )\n    base_faces = np.array([[0, 1, 2], [0, 1, 3], [1, 2, 3], [0, 2, 3]])\n  elif base_shape == 'icosahedron':\n    # https://en.wikipedia.org/wiki/Regular_icosahedron\n    phi = (1 + np.sqrt(5)) / 2\n    base_verts = np.array(\n        [\n            [-1, phi, 0],\n            [1, phi, 0],\n            [-1, -phi, 0],\n            [1, -phi, 0],\n            [0, -1, phi],\n            [0, 1, phi],\n            [0, -1, -phi],\n            [0, 1, -phi],\n            [phi, 0, -1],\n            [phi, 0, 1],\n            [-phi, 0, -1],\n            [-phi, 0, 1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 11, 5],\n            [0, 5, 1],\n            [0, 1, 7],\n            [0, 7, 10],\n            [0, 10, 11],\n            [1, 5, 9],\n            [5, 11, 4],\n            [11, 10, 2],\n            [5, 4, 10],\n            [3, 9, 4],\n            [3, 4, 2],\n            [3, 2, 6],\n            [3, 6, 8],\n            [3, 8,"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([\n        [1, 1, 1],\n        [-1, -1, 1],\n        [1, -1, -1],\n        [-1, 1, -1],\n    ])\n    base_faces = np.array([[0, 1, 2], [0, 2, 3], [0, 3, 1], [1, 3, 2]])\n  elif base_shape == 'icosahedron':\n    phi = (1 + np.sqrt(5)) / 2\n    base_verts = np.array([\n        [0, 1, phi],\n        [0, -1, phi],\n        [1, phi, 0],\n        [-1, phi, 0],\n        [phi, 0, 1],\n        [-phi, 0, -1],\n    ])\n    base_faces = np.array([\n        [0, 1, 2],\n        [0, 1, 3],\n        [0, 1, 4],\n        [0, 1, 5],\n        [0, 2, 3],\n        [0, 2, 4],\n        [0, 2, 5],\n        [0, 3, 4],\n        [0, 3, 5],\n        [0, 4, 5],\n        [1, 2, 3],\n        [1, 2, 4],\n        [1, 2, 5],\n        [1, 3, 4],\n        [1, 3, 5],\n        [1, 4, 5],\n        [2, 3, 4],\n        [2, 3, 5],\n        [2, 4, 5],\n        [3, 4, 5],\n    ])\n  elif base_shape == 'octahedron':\n    base_verts = np.array([\n        [1, 0, 0"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([[1, 1, 1], [-1, -1, 1], [1, -1, -1], [-1, 1, -1]])\n    base_faces = np.array([[0, 1, 2], [0, 2, 3], [0, 3, 1], [1, 3, 2]])\n  elif base_shape == 'icosahedron':\n    phi = (1 + np.sqrt(5)) / 2\n    base_verts = np.array(\n        [\n            [0, 1, phi],\n            [0, -1, phi],\n            [1, phi, 0],\n            [-1, phi, 0],\n            [phi, 0, 1],\n            [-phi, 0, -1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 1, 5],\n            [0, 5, 2],\n            [0, 2, 3],\n            [0, 3, 1],\n            [1, 2, 4],\n            [2, 5, 4],\n            [3, 4, 5],\n            [1, 3, 4],\n        ]\n    )\n  elif base_shape == 'octahedron':\n    base_verts = np.array(\n        [\n            [1, 0, 0],\n            [-1, 0, 0],\n            [0, 1, 0],\n            [0, -1, 0],\n            [0, 0, 1],\n            [0, 0, -1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 2, 4],\n            [0, 4, 3],\n            [0, 5, 2],\n            [0, 3, 5],\n            [1, 4, 2],"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array(\n        [\n            [1, 1, 1],\n            [1, -1, -1],\n            [-1, 1, -1],\n            [-1, -1, 1],\n        ],\n        dtype=np.float32,\n    )\n    base_faces = np.array(\n        [\n            [0, 1, 2],\n            [0, 1, 3],\n            [0, 2, 3],\n            [1, 2, 3],\n        ],\n        dtype=np.int32,\n    )\n  elif base_shape == 'icosahedron':\n    phi = (1 + np.sqrt(5)) / 2\n    base_verts = np.array(\n        [\n            [0, 1, phi],\n            [0, -1, phi],\n            [0, 1, -phi],\n            [0, -1, -phi],\n            [1, phi, 0],\n            [-1, phi, 0],\n            [1, -phi, 0],\n            [-1, -phi, 0],\n            [phi, 0, 1],\n            [phi, 0, -1],\n            [-phi, 0, 1],\n            [-phi, 0, -1],\n        ],\n        dtype=np.float32,\n    )\n    base_faces = np.array(\n        [\n            [0, 1, 4],\n            [0, 4, 6],\n            [0, 6, 2],\n            [0, 2, 1],\n            [1, 2, 7],\n            [1, 7, 4],\n            [2, 3, 8],\n            [2, 8, 7],\n            [3, 0, 9],\n            [3, 9, 8],\n            [3, 4, 9],\n            [4, 5, "}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array(\n        [\n            [1, 1, 1],\n            [1, -1, -1],\n            [-1, 1, -1],\n            [-1, -1, 1],\n        ]\n    )\n    base_faces = np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n  elif base_shape == 'icosahedron':\n    phi = (1 + np.sqrt(5)) / 2\n    base_verts = np.array(\n        [\n            [0, 1, phi],\n            [0, -1, phi],\n            [0, 1, -phi],\n            [0, -1, -phi],\n            [1, phi, 0],\n            [-1, phi, 0],\n            [1, -phi, 0],\n            [-1, -phi, 0],\n            [phi, 0, 1],\n            [phi, 0, -1],\n            [-phi, 0, 1],\n            [-phi, 0, -1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 1, 4],\n            [0, 4, 6],\n            [0, 6, 2],\n            [0, 2, 1],\n            [1, 3, 5],\n            [1, 5, 4],\n            [1, 4, 6],\n            [1, 6, 3],\n            [2, 3, 7],\n            [2, 7, 6],\n            [2, 6, 4],\n            [2, 4, 5],\n            [3, 5, 7],\n            [3, 7, 6],\n            [3, 6, 5],\n            [3, 5, 4],\n        ]"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array(\n        [\n            [1, 1, 1],\n            [1, -1, -1],\n            [-1, 1, -1],\n            [-1, -1, 1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 1, 2],\n            [0, 1, 3],\n            [0, 2, 3],\n            [1, 2, 3],\n        ]\n    )\n  elif base_shape == 'icosahedron':\n    # https://en.wikipedia.org/wiki/Regular_icosahedron#Cartesian_coordinates\n    phi = (1 + np.sqrt(5)) / 2\n    base_verts = np.array(\n        [\n            [0, 1, phi],\n            [0, -1, phi],\n            [0, 1, -phi],\n            [0, -1, -phi],\n            [1, phi, 0],\n            [-1, phi, 0],\n            [1, -phi, 0],\n            [-1, -phi, 0],\n            [phi, 0, 1],\n            [phi, 0, -1],\n            [-phi, 0, 1],\n            [-phi, 0, -1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 1, 4],\n            [0, 4, 6],\n            [0, 6, 1],\n            [0, 1, 10],\n            [0, 10, 6],\n            [0, 6, 4],\n            [0, 4, 9],\n            [0, 9, 10],\n            [0, 10, 1],\n            [0, 1, 8],\n            [0, 8, 9],\n            [0, 9, 4],"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([[1, 1, 1], [-1, -1, 1], [1, -1, -1], [-1, 1, -1]])\n    base_faces = np.array([[0, 1, 2], [0, 2, 3], [0, 3, 1], [1, 3, 2]])\n  elif base_shape == 'icosahedron':\n    phi = (1 + np.sqrt(5)) / 2\n    base_verts = np.array(\n        [\n            [0, 1, phi],\n            [0, -1, phi],\n            [1, phi, 0],\n            [-1, phi, 0],\n            [phi, 0, 1],\n            [-phi, 0, -1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 2, 1],\n            [0, 3, 2],\n            [0, 4, 3],\n            [0, 1, 4],\n            [5, 1, 2],\n            [5, 2, 3],\n            [5, 3, 4],\n            [5, 4, 1],\n        ]\n    )\n  elif base_shape == 'octahedron':\n    base_verts = np.array([[1, 0, 0], [-1, 0, 0], [0, 1, 0], [0, -1, 0], [0, 0, 1], [0, 0, -1]])\n    base_faces = np.array([[0, 2, 4], [1, 5, 3], [0, 4, 5], [1, 3, 2]])\n  else:\n    raise ValueError(f'Unknown base shape {base_shape}')\n\n  verts = tesselate_geodesic(base_verts, base"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([[1, 1, 1], [-1, -1, 1], [1, -1, -1], [-1, 1, -1]])\n    base_faces = np.array([[0, 1, 2], [0, 2, 3], [0, 3, 1], [1, 3, 2]])\n  elif base_shape == 'icosahedron':\n    # Vertices from https://en.wikipedia.org/wiki/Icosahedron#Cartesian_coordinates\n    phi = (1 + np.sqrt(5)) / 2\n    base_verts = np.array(\n        [\n            [0, 1, phi],\n            [0, -1, phi],\n            [1, phi, 0],\n            [-1, phi, 0],\n            [phi, 0, 1],\n            [-phi, 0, 1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 2, 1],\n            [0, 3, 2],\n            [0, 4, 3],\n            [0, 1, 4],\n            [5, 1, 2],\n            [5, 2, 3],\n            [5, 3, 4],\n            [5, 4, 1],\n        ]\n    )\n  elif base_shape == 'octahedron':\n    base_verts = np.array(\n        [\n            [1, 0, 0],\n            [-1, 0, 0],\n            [0, 1, 0],\n            [0, -1, 0],\n            [0, 0, 1],\n            [0, 0, -1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 2, 4],\n            [0, 4, 3],\n            [0"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([[1, 1, 1], [-1, -1, 1], [1, -1, -1], [-1, 1, -1]])\n    base_faces = np.array([[0, 1, 2], [0, 2, 3], [0, 3, 1], [1, 3, 2]])\n  elif base_shape == 'icosahedron':\n    # Vertices are on the unit sphere.\n    phi = (1 + np.sqrt(5)) / 2\n    base_verts = np.array(\n        [\n            [0, 1, phi],\n            [0, -1, phi],\n            [1, phi, 0],\n            [-1, phi, 0],\n            [phi, 0, 1],\n            [-phi, 0, -1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 2, 1],\n            [0, 3, 2],\n            [0, 4, 3],\n            [0, 1, 4],\n            [5, 1, 2],\n            [5, 2, 3],\n            [5, 3, 4],\n            [5, 4, 1],\n        ]\n    )\n  elif base_shape == 'octahedron':\n    base_verts = np.array([[1, 0, 0], [-1, 0, 0], [0, 1, 0], [0, -1, 0], [0, 0, 1], [0, 0, -1]])\n    base_faces = np.array([[0, 2, 4], [0, 4, 3], [0, 3, 5], [0, 5, 2], [1, 4, 2], [1, 2, 5], [1, 5, 3], [1, "}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array(\n        [\n            [1, 1, 1],\n            [-1, -1, 1],\n            [-1, 1, -1],\n            [1, -1, -1],\n        ]\n    )\n    base_faces = np.array(\n        [[0, 1, 2], [0, 2, 3], [0, 3, 1], [1, 3, 2]]\n    )\n  elif base_shape == 'icosahedron':\n    base_verts = np.array(\n        [\n            [0, 1, 0.354],\n            [0.724, 0.447, 0.526],\n            [0.724, -0.447, 0.526],\n            [-0.724, 0.447, 0.526],\n            [-0.724, -0.447, 0.526],\n            [0, -1, 0.354],\n            [0.724, 0.447, -0.526],\n            [-0.724, 0.447, -0.526],\n            [0.724, -0.447, -0.526],\n            [-0.724, -0.447, -0.526],\n            [0, 0.354, 1],\n            [0, -0.354, 1],\n            [0, 0.354, -1],\n            [0, -0.354, -1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 1, 2],\n            [0, 2, 3],\n            [0, 3, 4],\n            [0, 4, 1],\n            [5, 1"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([\n        [1, 1, 1],\n        [1, -1, -1],\n        [-1, 1, -1],\n        [-1, -1, 1],\n    ]).T / np.sqrt(3)\n    base_faces = np.array([[0, 1, 2], [0, 2, 3], [0, 3, 1], [1, 3, 2]])\n  elif base_shape == 'icosahedron':\n    phi = (1 + np.sqrt(5)) / 2\n    base_verts = np.array([\n        [0, 1, phi],\n        [0, -1, -phi],\n        [phi, 0, 1],\n        [phi, 0, -1],\n        [-phi, 0, 1],\n        [-phi, 0, -1],\n        [1, phi, 0],\n        [-1, phi, 0],\n        [1, -phi, 0],\n        [-1, -phi, 0],\n    ]).T\n    base_faces = np.array([\n        [0, 2, 1],\n        [0, 3, 2],\n        [0, 4, 3],\n        [0, 1, 4],\n        [1, 2, 5],\n        [2, 3, 6],\n        [3, 4, 7],\n        [4, 1, 8],\n        [5, 6, 1],\n        [6, 7, 2],\n        [7, 8, 3],\n        [8, 5, 4],\n        [5, 7, 6],\n        [7, 5, 8],\n        [8, 6, 7],\n        [6, 8, 5],\n    ])\n  elif base_shape == 'octahedron':\n    base_verts = np.array([\n       "}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array(\n        [\n            [1, 1, 1],\n            [-1, -1, 1],\n            [1, -1, -1],\n            [-1, 1, -1],\n        ]\n    )\n    base_faces = np.array([[0, 1, 2], [0, 2, 3], [0, 3, 1], [1, 3, 2]])\n  elif base_shape == 'icosahedron':\n    # XYZ coordinates of the vertices of an icosahedron.\n    phi = (1 + np.sqrt(5)) / 2\n    base_verts = np.array(\n        [\n            [0, 1, phi],\n            [0, -1, phi],\n            [1, phi, 0],\n            [-1, phi, 0],\n            [phi, 0, 1],\n            [-phi, 0, 1],\n            [0, 1, -phi],\n            [0, -1, -phi],\n            [1, -phi, 0],\n            [-1, -phi, 0],\n            [phi, 0, -1],\n            [-phi, 0, -1],\n        ]\n    )\n    # Faces of the icosahedron as triangles.\n    base_faces = np.array(\n        [\n            [0, 2, 1],\n            [0, 3, 2],\n            [0, 4, 3],\n            [0, 1, 4],\n            [1, 2, 5],\n            [2, 3, 6],\n            [3, 4, 7],\n            [4, 1, 8],\n            [11, 6, 5],\n            [6, 7, 10],\n            [7, 8, 11],\n            [8, 5, 9],\n            [5, 2, 1],"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array(\n        [\n            [1, 1, 1],\n            [1, -1, -1],\n            [-1, 1, -1],\n            [-1, -1, 1],\n        ],\n        dtype=np.float32,\n    )\n    base_faces = np.array(\n        [[0, 1, 2], [0, 2, 3], [0, 3, 1], [1, 3, 2]], dtype=np.int32\n    )\n  elif base_shape == 'icosahedron':\n    phi = (1 + np.sqrt(5)) / 2\n    base_verts = np.array(\n        [\n            [0, 1, phi],\n            [0, -1, phi],\n            [0, 1, -phi],\n            [0, -1, -phi],\n            [1, phi, 0],\n            [-1, phi, 0],\n            [1, -phi, 0],\n            [-1, -phi, 0],\n            [phi, 0, 1],\n            [-phi, 0, 1],\n            [phi, 0, -1],\n            [-phi, 0, -1],\n        ],\n        dtype=np.float32,\n    )\n    base_faces = np.array(\n        [\n            [0, 1, 4],\n            [0, 4, 6],\n            [0, 6, 1],\n            [0, 9, 2],\n            [0, 2, 8],\n            [0, 8, 9],\n            [0, 3, 7],\n            [0, 7, 10],\n            [0, 10, 3],\n            [1, 2, 5],\n            [1, 5, 11],\n            [1, 11, 4],\n            [2, 3,"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array(\n        [\n            [-0.5, -0.5, -0.5],\n            [0.5, -0.5, 0.5],\n            [0.5, 0.5, -0.5],\n            [-0.5, 0.5, 0.5],\n        ]\n    )\n    base_faces = np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n  elif base_shape == 'icosahedron':\n    # Icosahedron coordinates from https://en.wikipedia.org/wiki/Icosahedron\n    phi = (1 + np.sqrt(5)) / 2\n    base_verts = np.array(\n        [\n            [0, -1, phi],\n            [0, 1, phi],\n            [0, -1, -phi],\n            [0, 1, -phi],\n            [1, phi, 0],\n            [-1, phi, 0],\n            [1, -phi, 0],\n            [-1, -phi, 0],\n            [phi, 0, 1],\n            [phi, 0, -1],\n            [-phi, 0, 1],\n            [-phi, 0, -1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 4, 1],\n            [0, 9, 4],\n            [9, 5, 4],\n            [4, 5, 8],\n            [4, 8, 1],\n            [8, 10, 1],\n            [8, 3, 10],\n            [5, 3, 8],\n            [5, 2, 3],\n            [2, 7, 3],\n            [7, 10, 3],\n            [7,"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + x),\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + x),\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + x),\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + x),\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + x),\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + x),\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + x),\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + x),\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + jnp.clip(x, -1, 1)),\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + jnp.clip(x, -1, 1)),\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + jnp.clip(x, -1, 1)),\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + jnp.clip(x, -1, 1)),\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (min_val, np.nextafter(np.log1p(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + jnp.clip(x, -1, 1)),\n      (min_val, np.nextafter(np.log1p(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (min_val, np.nextafter(np.log1p(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + jnp.clip(x, min_val, max_val)),\n      (min_val, np.nextafter(np.log(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  xp = jnp.abs(x)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  y_max = minus_eps(power_ladder_max_output(p))\n  y = select(\n      [\n          (p == 1, xp),\n          (p == 0, safe_expm1(xp)),\n          (p == -jnp.inf, -safe_log1p(-xp)),\n          (p == jnp.inf, safe_log1p(xp)),\n      ],\n      jnp.abs(p_safe - 1) * (jnp.abs(xp) ** p_safe - 1),\n  )\n  y = jnp.clip(y, -y_max, y_max)\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  xp = jnp.abs(x)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x_max = power_ladder_max_output(p)\n  xp = override_gradient(jnp.clip(xp, -x_max, x_max), xp)  # Clip val, not grad.\n  y = select(\n      [\n          (p == 1, xp),\n          (p == 0, safe_expm1(xp)),\n          (p == -jnp.inf, -safe_log1p(-xp)),\n          (p == jnp.inf, safe_log1p(xp)),\n      ],\n      jnp.abs(p_safe - 1)\n      * (\n          ((safe_div(p_safe, jnp.abs(p_safe - 1)) * xp + 1)) ** (1 / p_safe) - 1\n      ),\n  )\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  y_max = power_ladder_max_output(p)\n  y = select(\n      [\n          (p == 1, x),\n          (p == 0, safe_expm1(x)),\n          (p == -jnp.inf, -safe_log1p(-x)),\n          (p == jnp.inf, safe_log1p(x)),\n      ],\n      jnp.abs(p) * (jnp.abs(x) ** (1 / jnp.abs(p)) - 1),\n  )\n  y = jnp.clip(y, -y_max, y_max)\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  y = select(\n      [\n          (p == 1, x),\n          (p == 0, safe_exp(x) - 1),\n          (p == -jnp.inf, -safe_log(1 - x)),\n          (p == jnp.inf, safe_log(1 + x)),\n      ],\n      jnp.abs(p) * (jnp.abs(x) ** (1 / jnp.abs(p)) - 1),\n  )\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  xp = jnp.abs(x)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  y = select(\n      [\n          (p == 1, xp),\n          (p == 0, safe_expm1(xp)),\n          (p == -jnp.inf, -safe_log1p(-xp)),\n          (p == jnp.inf, safe_log1p(xp)),\n      ],\n      jnp.abs(p_safe - 1) * (xp**p_safe - 1),\n  )\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  xp = jnp.abs(x)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  y_max = power_ladder_max_output(p)\n  y = select(\n      [\n          (p == 1, xp),\n          (p == 0, safe_expm1(xp)),\n          (p == -jnp.inf, -safe_log1p(-xp)),\n          (p == jnp.inf, safe_log1p(xp)),\n      ],\n      jnp.abs(p_safe - 1)\n      * (\n          ((safe_div(p_safe, jnp.abs(p_safe - 1)) * xp + 1)) ** (1 / p_safe) - 1\n      ),\n  )\n  y = jnp.clip(y, -y_max, y_max)\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  p_safe = clip_finite_nograd(remove_zero(p))\n  y = select(\n      [\n          (p == 1, x),\n          (p == 0, safe_exp(x) - 1),\n          (p == -jnp.inf, -safe_log(1 - x)),\n          (p == jnp.inf, safe_log(1 + x)),\n      ],\n      safe_div(jnp.abs(p_safe - 1), p_safe) * (jnp.abs(x) ** p_safe - 1),\n  )\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  y = select(\n      [\n          (p == 1, x),\n          (p == 0, safe_expm1(x)),\n          (p == -jnp.inf, -safe_log1p(-x)),\n          (p == jnp.inf, safe_log1p(x)),\n      ],\n      jnp.abs(p) * (jnp.abs(x)**p - 1) / p,\n  )\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  p_safe = clip_finite_nograd(remove_zero(p))\n  y = select(\n      [\n          (p == 1, x),\n          (p == 0, safe_expm1(x)),\n          (p == -jnp.inf, -safe_log1p(-x)),\n          (p == jnp.inf, safe_log1p(x)),\n      ],\n      jnp.abs(p_safe - 1)\n      * (jnp.sign(x) * jnp.abs(x) + 1) ** p_safe - 1,\n  )\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  p_safe = clip_finite_nograd(remove_zero(p))\n  y = select(\n      [\n          (p == 1, x),\n          (p == 0, safe_exp(x) - 1),\n          (p == -jnp.inf, -safe_log(1 - x)),\n          (p == jnp.inf, safe_log(1 + x)),\n      ],\n      safe_div(jnp.abs(p_safe - 1) * x + 1, p_safe) ** (1 / p_safe) - 1,\n  )\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  xp = jnp.abs(x)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x_max = minus_eps(power_ladder_max_output(p))\n  xp = override_gradient(jnp.clip(xp, -x_max, x_max), xp)  # Clip val, not grad.\n  y = select(\n      [\n          (p == 1, xp),\n          (p == 0, safe_exp(xp) - 1),\n          (p == -jnp.inf, -safe_log(1 - xp)),\n          (p == jnp.inf, safe_log(1 + xp)),\n      ],\n      jnp.abs(p_safe - 1)\n      * (\n          (safe_div(p_safe, jnp.abs(p_safe - 1)) * xp + 1) ** p_safe - 1\n      ),\n  )\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  y = select(\n      [\n          (p == 1, x),\n          (p == 0, safe_exp(x) - 1),\n          (p == -jnp.inf, -safe_log(1 - x)),\n          (p == jnp.inf, safe_log(1 + x)),\n      ],\n      jnp.abs(p)\n      * (\n          (safe_div(jnp.abs(p) - 1, jnp.abs(p)) * x + 1) ** (1 / jnp.abs(p)) - 1\n      ),\n  )\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  p_safe = clip_finite_nograd(remove_zero(p))\n  y = select(\n      [\n          (p == 1, x),\n          (p == 0, safe_exp(x) - 1),\n          (p == -jnp.inf, -safe_log(1 - x)),\n          (p == jnp.inf, safe_log(1 + x)),\n      ],\n      jnp.abs(p_safe - 1) * (safe_div(x, jnp.abs(p_safe - 1)) + 1)**p_safe - 1,\n  )\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  p_safe = clip_finite_nograd(remove_zero(p))\n  y_max = minus_eps(power_ladder_max_output(p))\n  x = select(\n      [\n          (p == 1, x),\n          (p == 0, safe_expm1(x)),\n          (p == -jnp.inf, -safe_log1p(-x)),\n          (p == jnp.inf, safe_log1p(x)),\n      ],\n      jnp.abs(p_safe - 1) * jnp.sign(x) * (jnp.abs(x)**p_safe - 1),\n  )\n  if postmult is not None:\n    x *= postmult\n  return jnp.clip(x, -y_max, y_max)"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  xp = jnp.abs(x)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  y_max = minus_eps(power_ladder_max_output(p))\n  y = select(\n      [\n          (p == 1, xp),\n          (p == 0, safe_exp(xp) - 1),\n          (p == -jnp.inf, safe_log(1 + xp)),\n          (p == jnp.inf, safe_log(xp + 1)),\n      ],\n      jnp.abs(p_safe - 1)\n      * (\n          ((safe_div(p_safe, jnp.abs(p_safe - 1)) * xp + 1)) ** (1 / p_safe) - 1\n      ),\n  )\n  y = jnp.clip(y, -y_max, y_max)\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  xp = jnp.abs(x)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x_max = minus_eps(power_ladder_max_output(p))\n  xp = override_gradient(jnp.clip(xp, -x_max, x_max), xp)  # Clip val, not grad.\n  y = safe_sign(x) * select(\n      [\n          (p == 1, xp),\n          (p == 0, safe_expm1(xp)),\n          (p == -jnp.inf, -safe_log1p(-xp)),\n          (p == jnp.inf, safe_log1p(xp)),\n      ],\n      jnp.abs(p_safe - 1) * (xp**p_safe - 1),\n  )\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  p = clip_finite_nograd(remove_zero(p))\n  if premult is not None:\n    x *= premult\n  y = select(\n      [\n          (p == 1, x),\n          (p == 0, safe_exp(x) - 1),\n          (p == -jnp.inf, -safe_log(1 - x)),\n          (p == jnp.inf, safe_log(1 + x)),\n      ],\n      safe_div(jnp.abs(p - 1), p) * (jnp.abs(x)**p - 1),\n  )\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  xp = jnp.abs(x)\n  p_safe = clip_finite_nograd(remove_zero(p))\n  y_max = power_ladder_max_output(p)\n  y = select(\n      [\n          (p == 1, xp),\n          (p == 0, safe_log1p(xp)),\n          (p == -jnp.inf, safe_log(xp)),\n          (p == jnp.inf, safe_expm1(xp)),\n      ],\n      jnp.abs(p_safe - 1)\n      * (\n          jnp.sign(x)\n          * (jnp.abs(p_safe - 1) * xp + 1) ** (1 / p_safe)\n          - 1\n      ),\n  )\n  y = jnp.clip(y, -y_max, y_max)\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  p_safe = clip_finite_nograd(remove_zero(p))\n  y = select(\n      [\n          (p == 1, x),\n          (p == 0, safe_exp(x) - 1),\n          (p == -jnp.inf, -safe_log(1 - x)),\n          (p == jnp.inf, safe_log(1 + x)),\n      ],\n      jnp.abs(p_safe - 1)\n      * (jnp.abs(x)**p_safe - 1)\n      * safe_sign(x)\n      / p_safe,\n  )\n  if premult is not None:\n    y *= premult\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.power_ladder", "completion": "  p_safe = clip_finite_nograd(remove_zero(p))\n  y_max = minus_eps(power_ladder_max_output(p))\n  if premult is not None:\n    x = x / premult\n  x = jnp.clip(x, -y_max, y_max)\n  y = select(\n      [\n          (p == 1, x),\n          (p == 0, safe_exp(x) - 1),\n          (p == -jnp.inf, -safe_log(1 - x)),\n          (p == jnp.inf, safe_log(1 + x)),\n      ],\n      jnp.abs(p_safe - 1) * (jnp.abs(x)**p_safe - 1) / p_safe,\n  )\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y / premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_exp(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_exp(yp) - 1),\n          (p == -jnp.inf, safe_log(1 + yp)),\n          (p == jnp.inf, -safe_log(1 - yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** (1 / p_safe) - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  if p == 1:\n    x = y\n  elif p == 0:\n    x = safe_exp(y) - 1\n  elif p == -jnp.inf:\n    x = -safe_expm1(-y)\n  elif p == jnp.inf:\n    x = safe_expm1(y)\n  else:\n    x = jnp.abs(p - 1) / p * (jnp.abs(y) + 1)**(1 / p) - 1\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(y) * |p - 1|/p * ((|y|/|p-1| + 1)^p - 1)\n  if postmult is not None:\n    y = y * postmult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_exp(yp) - 1),\n          (p == -jnp.inf, safe_log(1 + yp)),\n          (p == jnp.inf, -safe_log(1 - yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** p_safe - 1)\n      ),\n  )\n  if premult is not None:\n    x = x * premult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(y) * |p - 1|/p * (|y|/|p-1| + 1)^(1/p) - 1\n  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** (1 / p_safe) - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  if p == 1:\n    x = y\n  elif p == 0:\n    x = safe_exp(y) - 1\n  elif p == -jnp.inf:\n    x = -safe_exp(-y)\n  elif p == jnp.inf:\n    x = safe_exp(y)\n  else:\n    x = jnp.abs(p - 1) / p * (jnp.power(jnp.abs(y) + 1, 1 / p) - 1)\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  if p == 1:\n    x = y\n  elif p == 0:\n    x = safe_exp(y) - 1\n  elif p == -jnp.inf:\n    x = -safe_exp(-y) + 1\n  elif p == jnp.inf:\n    x = safe_exp(y)\n  else:\n    p_safe = clip_finite_nograd(remove_zero(p))\n    x = safe_div(\n        jnp.abs(p_safe - 1), p_safe\n    ) * (jnp.abs(y) + 1) ** p_safe - 1\n    x = safe_sign(y) * x\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(y) * |p - 1|/p * (1 - (|y|/|p-1| + 1)^p)\n  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_exp(yp) - 1),\n          (p == -jnp.inf, safe_log(1 + yp)),\n          (p == jnp.inf, -safe_log(1 - yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * (1 - (ys + 1) ** p_safe)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y / premult\n  if p == 1:\n    x = y\n  elif p == 0:\n    x = safe_exp(y) - 1\n  elif p == -jnp.inf:\n    x = -safe_exp(-y) + 1\n  elif p == jnp.inf:\n    x = safe_exp(y) + 1\n  else:\n    x = (jnp.abs(p - 1) / p) * (jnp.power(jnp.abs(y) / (jnp.abs(p - 1) + 1), 1 / p) - 1)\n    x = jnp.where(y < 0, -x, x)\n  if postmult is not None:\n    x = x / postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  if p == -jnp.inf:\n    x = -safe_expm1(-y)\n  elif p == 0:\n    x = safe_exp(y) - 1\n  elif p == 1:\n    x = y\n  elif p == jnp.inf:\n    x = safe_expm1(y)\n  else:\n    p_safe = clip_finite_nograd(remove_zero(p))\n    x = jnp.abs(y) / jnp.maximum(tiny_val, jnp.abs(p_safe - 1))\n    x = jnp.power(x + 1, 1 / p_safe) - 1\n    x = safe_sign(y) * x\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  if p == 1:\n    x = y\n  elif p == 0:\n    x = safe_exp(y) - 1\n  elif p == -jnp.inf:\n    x = -safe_exp(-y) + 1\n  elif p == jnp.inf:\n    x = safe_exp(y) + 1\n  else:\n    p_safe = clip_finite_nograd(remove_zero(p))\n    x = jnp.abs(y)\n    x = jnp.where(\n        p_safe < 0,\n        -safe_pow(x / jnp.maximum(tiny_val, jnp.abs(p_safe - 1)),\n                  1 / jnp.maximum(tiny_val, p_safe)) + 1,\n        safe_pow(x / jnp.maximum(tiny_val, jnp.abs(p_safe - 1)) + 1,\n                 jnp.maximum(tiny_val, p_safe)),\n    )\n    x = jnp.where(y < 0, -x, x)\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  if p == 1:\n    x = y\n  elif p == 0:\n    x = safe_exp(y) - 1\n  elif p == -jnp.inf:\n    x = safe_log(1 - y)\n  elif p == jnp.inf:\n    x = -safe_log(1 - y)\n  else:\n    # Clamp p to be >= machine epsilon so that it's safe to divide by.\n    p_safe = clip_finite_nograd(remove_zero(p))\n    x = jnp.abs(y) / clip_finite_nograd(\n        jnp.abs(p_safe - 1) / p_safe\n    ) ** (1 / p_safe) - 1\n  if postmult is not None:\n    x = x * postmult\n  return x\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  if p == 1:\n    x = y\n  elif p == 0:\n    x = safe_exp(y) - 1\n  elif p == -jnp.inf:\n    x = -safe_expm1(-y)\n  elif p == jnp.inf:\n    x = safe_expm1(y)\n  else:\n    p_safe = clip_finite_nograd(remove_zero(p))\n    x = jnp.abs(y)\n    x = safe_div(x, clip_finite_nograd(jnp.abs(p_safe - 1))) + 1\n    x = safe_pow(x, clip_finite_nograd(1 / p_safe)) - 1\n    x = safe_sign(y) * x\n  if postmult is not None:\n    x = x * postmult\n  return x\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # Avoid dividing by 0 before it's needed.\n    p = jnp.clip(step / jnp.maximum(1, lr_delay_steps), 0, 1)\n    lr_mult = lr_delay_mult + (1 - lr_delay_mult) * p\n  else:\n    lr_mult = 1\n  return lr_mult * lr_init * 0.5 * (\n      1 + jnp.cos(jnp.pi * jnp.clip(step / max_steps, 0, 1))\n  )"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # Avoid dividing by 0 before it's needed.\n    p = jnp.maximum(1.0 - jnp.float32(step) / jnp.float32(lr_delay_steps), eps)\n    mult = lr_delay_mult + (1.0 - lr_delay_mult) * p\n    lr = lr_init * mult\n  else:\n    p = jnp.float32(step) / jnp.float32(max_steps - 1)\n    lr = lr_init + p * (lr_final - lr_init)\n\n  if lr_delay_steps == 0 or step < max_steps:\n    lr = jnp.maximum(lr, lr_final)\n\n  return lr"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # Avoid dividing by 0 before it's needed.\n    p = jnp.maximum(1.0 - jnp.float32(step) / jnp.float32(lr_delay_steps), eps)\n    mult = lr_delay_mult + (1.0 - lr_delay_mult) * p\n    lr = mult * lr_init\n  else:\n    p = jnp.float32(step) / jnp.float32(max_steps)\n    lr = lr_init\n  lr = jnp.clip(lr, 0.0, lr_final)\n  return lr"}
{"namespace": "math.learning_rate_decay", "completion": "  if max_steps <= lr_delay_steps:\n    raise ValueError(f'max_steps ({max_steps}) must be larger than '\n                     f'lr_delay_steps ({lr_delay_steps}).')\n  t = jnp.clip(step / max_steps, 0, 1)\n  log_lerp_t = log_lerp(t, lr_delay_mult, 1)\n  lr_frac = log_lerp_t if lr_delay_steps == 0 else log_lerp(\n      jnp.clip(step / lr_delay_steps, 0, 1), 1, log_lerp_t\n  )\n  lr = lr_frac * lr_init + (1 - lr_frac) * lr_final\n  return lr"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps:\n    # Delayed schedule for the learning rate.\n    lr_delay_rate = lr_delay_mult + (1 - lr_delay_mult) * jnp.sin(\n        0.5 * jnp.pi * jnp.clip(step / lr_delay_steps, 0, 1)\n    )\n  else:\n    lr_delay_rate = 1.0\n\n  return lr_delay_rate * log_lerp(\n      (step - lr_delay_steps) / (max_steps - lr_delay_steps), lr_init, lr_final\n  )"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # Avoid floating point issues in the induced epsilons near the boundaries\n    # by clipping the values.\n    t = jnp.clip(step / jnp.float32(max_steps - lr_delay_steps), 0, 1)\n    log_lerp_t = log_lerp(t, 1.0, 1.0 / lr_delay_mult)\n    lr_ratio = lr_delay_mult * log_lerp_t\n  else:\n    lr_ratio = log_lerp(step / jnp.float32(max_steps), 1.0, 1.0 / lr_delay_mult)\n  return lr_ratio * lr_init * (1 / lr_final)"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps:\n    # Delayed schedule for the learning rate.\n    lr_delay_steps = jnp.minimum(lr_delay_steps, max_steps)\n    progress = jnp.maximum(0.0, step - lr_delay_steps) / (\n        max_steps - lr_delay_steps\n    )\n    lr_mult = lr_delay_mult + (1.0 - lr_delay_mult) * jnp.sin(\n        0.5 * jnp.pi * jnp.clip(progress, 0.0, 1.0)\n    )\n  else:\n    # Quick schedule for the learning rate.\n    progress = step / max_steps\n    lr_mult = jnp.sin(0.5 * jnp.pi * jnp.clip(progress, 0.0, 1.0))\n  lr = lr_mult * (lr_init - lr_final) + lr_final\n  return lr"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # Avoid floating-point bugs by using integer arithmetic.\n    i = jnp.floor(\n        jnp.clip(step - lr_delay_steps, 0, max_steps - lr_delay_steps)\n        / lr_delay_steps\n    )\n    lr_mult = (0.5 * jnp.cos(jnp.pi * i) + 0.5) * lr_delay_mult\n  else:\n    lr_mult = 1.0\n\n  t = jnp.clip(step / max_steps, 0, 1)\n  lr = (1 - t) * lr_init + t * lr_final\n  return lr_mult * lr"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # Avoid floating-point operations here, for 32-bit computations\n    # 1000 values requires a lot of precision\n    progress = jnp.clip(step / max_steps, 0, 1)\n    scaled_progress = jnp.clip(\n        (step - lr_delay_steps) / max(1, max_steps - lr_delay_steps), 0, 1\n    )\n    lr = lr_delay_mult + (1 - lr_delay_mult) * progress\n    lr = lr_init * (1 - lr) + lr_final * lr\n    lr = lr_init + (lr_final - lr_init) * (\n        scaled_progress**3 * lr + (1 - scaled_progress) ** 3 * progress\n    )\n  else:\n    progress = jnp.clip(step / max_steps, 0, 1)\n    lr = lr_init + (lr_final - lr_init) * (progress**3)\n  return lr"}
{"namespace": "math.learning_rate_decay", "completion": "  # If no delay is specified, just return the linear decay.\n  if lr_delay_steps == 0:\n    return lr_init * (lr_final / lr_init) ** (step / max_steps)\n  else:\n    # Calculate the learning rate after the delay period.\n    lr_after_delay = lr_init * (lr_final / lr_init) ** (\n        (step - lr_delay_steps) / (max_steps - lr_delay_steps)\n    )\n    # Calculate the learning rate during the delay period.\n    lr_during_delay = lr_init * lr_delay_mult\n    # Return the appropriate learning rate based on the current step.\n    return jnp.where(step < lr_delay_steps, lr_during_delay, lr_after_delay)"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # Avoid floating point overflow in the case where float(step - lr_delay_steps)\n    # would become too large.\n    max_steps = jnp.minimum(max_steps - lr_delay_steps,\n                            lr_delay_steps + 1000 * lr_delay_steps)\n    delay_rate = lr_delay_mult + (1 - lr_delay_mult) * jnp.sin(\n        0.5 * jnp.pi * jnp.clip(step / lr_delay_steps, 0, 1)\n    )\n    # Delay the overall learning rate according to the delay schedule.\n    lr_init = lr_init * delay_rate\n  t = jnp.clip(step / max_steps, 0, 1)\n  lr = (1 - t) * lr_init + t * lr_final\n  return lr\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # Avoid floating point overflow in the case where lr_delay_mult * lr_init is\n    # so small that the ratio of lr_init to the delayed_lr is less than the\n    # representable number for float16 (2**-14), which may lead to instabilities\n    # in model training.\n    max_lr_ratio = 2**13\n    lr_delay_mult = jnp.minimum(\n        lr_delay_mult, max_lr_ratio / lr_init\n    )\n    lr_delay_mult = jnp.maximum(lr_delay_mult, 1 / max_lr_ratio)\n\n    # Delay the full learning rate for a few steps.\n    lr = jnp.interp(\n        jnp.minimum(step / lr_delay_steps, 1),\n        [0, 1],\n        [lr_init * lr_delay_mult, lr_init],\n    )\n  else:\n    lr = lr_init\n\n  # If we're still training, perform the scheduled decay.\n  lr = jnp.where(\n      step < max_steps,\n      jnp.interp(step / max_steps, [0, 1], [lr, lr_final]),\n      lr_final,\n  )\n  return lr"}
{"namespace": "math.learning_rate_decay", "completion": "  # If we're in a delay period, return the scaled initial learning rate.\n  if step < lr_delay_steps:\n    return lr_init * lr_delay_mult\n\n  # Calculate the progression of steps from the beginning of the decay to the end.\n  progress = (step - lr_delay_steps) / (max_steps - lr_delay_steps)\n\n  # Apply the log-linear interpolation between the initial and final learning rates.\n  return lr_init * (lr_final / lr_init)**progress"}
{"namespace": "math.learning_rate_decay", "completion": "  # If no delay is used, just use a log-linear interpolation.\n  if lr_delay_steps == 0:\n    return log_lerp(step / max_steps, lr_init, lr_final)\n  # If a delay is used, interpolate between the delayed rate and the final rate.\n  else:\n    # Calculate the delayed learning rate based on the delay parameters.\n    delayed_lr = lr_init * lr_delay_mult\n    # Calculate the progression within the delay period.\n    delay_progress = jnp.clip(step / lr_delay_steps, 0, 1)\n    # Interpolate between the delayed learning rate and the final learning rate.\n    return log_lerp(delay_progress, delayed_lr, lr_final)"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # Avoid floating-point overflow in the division below.\n    assert lr_delay_mult < 1.0\n    lr_delay_mult += jnp.finfo(jnp.float32).eps\n    pct_remaining = 1 - (step - lr_delay_steps) / (max_steps - lr_delay_steps)\n    mul = lr_delay_mult + (1.0 - lr_delay_mult) * pct_remaining\n    lr = lr_init * mul\n  else:\n    pct_remaining = 1 - step / max_steps\n    lr = lr_init + pct_remaining * (lr_final - lr_init)\n  return lr\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if max_steps is None:\n    raise ValueError('max_steps cannot be None.')\n  if lr_delay_steps > max_steps:\n    raise ValueError(\n        f'lr_delay_steps ({lr_delay_steps}) cannot be '\n        f'greater than max_steps ({max_steps}).'\n    )\n  if lr_delay_mult < 0:\n    raise ValueError(\n        f'lr_delay_mult ({lr_delay_mult}) cannot be '\n        f'negative.'\n    )\n\n  if lr_delay_steps:\n    # Delayed schedule for the learning rate.\n    m = jnp.maximum(0, step - lr_delay_steps)\n    d = jnp.float32(lr_delay_steps)\n    lr = lr_init * (lr_delay_mult + m / (d + 1e-8))\n  else:\n    # A simple linear schedule for the learning rate.\n    t = jnp.float32(step) / jnp.float32(max_steps)\n    lr = lr_init + t * (lr_final - lr_init)\n  return lr"}
{"namespace": "math.learning_rate_decay", "completion": "  # If no delay, use log-linear interpolation.\n  if lr_delay_steps == 0:\n    return log_lerp(step / max_steps, lr_init, lr_final)\n\n  # Apply the delay multiplier to the initial learning rate.\n  lr_init = lr_init * lr_delay_mult\n\n  # Calculate the learning rate for the current step.\n  step_ratio = jnp.maximum(0, step - lr_delay_steps) / max_steps\n  lr = log_lerp(step_ratio, lr_init, lr_final)\n\n  # Apply the delay multiplier to the final learning rate.\n  lr_final = lr_final * lr_delay_mult\n\n  # Calculate the learning rate for the current step with the delay applied.\n  lr_delayed = log_lerp(step_ratio, lr_init, lr_final)\n\n  # Return the learning rate with the delay applied if the current step is within the delay period.\n  return jnp.where(step < lr_delay_steps, lr, lr_delayed)"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # Avoid dividing by 0 before it's needed.\n    p = jnp.maximum(1.0 - jnp.float32(step) / jnp.float32(lr_delay_steps), 1e-8)\n    mult = lr_delay_mult + (1.0 - lr_delay_mult) * p\n  else:\n    mult = 1.0\n  t = jnp.clip(jnp.float32(step) / jnp.float32(max_steps), 0, 1)\n  lr = mult * (lr_init - lr_final) * jnp.exp(-10 * t) + lr_final\n  return lr\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if max_steps <= lr_delay_steps:\n    raise ValueError(f'max_steps ({max_steps}) must be larger than '\n                     f'lr_delay_steps ({lr_delay_steps})')\n  if lr_init <= 0:\n    raise ValueError(f'lr_init must be positive')\n  if lr_delay_mult < 1:\n    raise ValueError(f'lr_delay_mult must be no less than 1')\n  if step < 0 or step > max_steps:\n    raise ValueError(f'step must be between 0 and max_steps')\n\n  def get_mult(progress):\n    return jnp.clip(1 - progress / max_steps, 0, 1)\n\n  def get_progress(step):\n    return jnp.clip(step / max_steps, 0, 1)\n\n  def get_delay_mult(progress):\n    return jnp.clip(lr_delay_mult + (1 - lr_delay_mult) * progress, 0, 1)\n\n  if lr_delay_steps > 0:\n    # Use a log-linear interpolation schedule to the specified learning rate.\n    lr = lr_init * get_mult(step)\n    # Delay the learning rate increase until after lr_delay_steps, then continue\n    # linearly to the specified learning rate.\n    progress = get_progress(step - lr_delay_steps)\n    mult = get_mult(progress) * get_delay_mult(progress)\n    lr = jnp.clip(lr_init, lr_final, lr_init + mult * (lr_final - lr_init))\n  else:\n    # Just use a linear interpolation schedule.\n    progress = get_progress(step)\n    lr = lr_init + progress * (lr_final - lr_init)\n\n  return lr"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # Avoid dividing by 0 before it's needed.\n    p = jnp.maximum(1.0 - jnp.power(lr_delay_mult, 1 / lr_delay_steps), 1e-8)\n  else:\n    p = 1.0\n\n  # If more steps than planned, adjust the scale factor.\n  p = jnp.minimum(jnp.power(lr_delay_mult, step / lr_delay_steps), p)\n\n  # Linearly interpolate between initial and final learning rates.\n  lr = lr_init * p + lr_final * (1 - p)\n\n  # `step` starts at 0, so the first 1-step is when `p` becomes 1.0.\n  lr = jnp.where(step >= max_steps - 1, lr_final, lr)\n\n  return lr\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1,\n      origin_lo=0.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=0.0,\n      far_hi=1.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1,\n      origin_lo=0,\n      origin_hi=1,\n      radius_lo=0,\n      radius_hi=1,\n      near_lo=0,\n      near_hi=1,\n      far_lo=0,\n      far_hi=1,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1024,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=1.0,\n      far_hi=2.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1024,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=0.0,\n      far_hi=1.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  rng = random.PRNGKey(0)\n  return generate_random_rays(\n      rng,\n      1,\n      origin_lo=0.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=0.0,\n      far_hi=1.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1024,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=0.0,\n      far_hi=1.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  rng = random.PRNGKey(0)\n  random_rays = generate_random_rays(\n      rng,\n      1024,\n      origin_lo=0.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=0.0,\n      far_hi=1.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )\n  return random_rays"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1024,\n      origin_lo=-1,\n      origin_hi=1,\n      radius_lo=0,\n      radius_hi=1,\n      near_lo=0,\n      near_hi=1,\n      far_lo=1,\n      far_hi=2,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  rng = random.PRNGKey(0)\n  return generate_random_rays(\n      rng,\n      1,\n      0.0,\n      1.0,\n      0.0,\n      1.0,\n      0.0,\n      1.0,\n      0.0,\n      1.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1,\n      origin_lo=-1,\n      origin_hi=1,\n      radius_lo=0.01,\n      radius_hi=0.1,\n      near_lo=0.1,\n      near_hi=1,\n      far_lo=1,\n      far_hi=10,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  rng = random.PRNGKey(0)\n  random_rays = generate_random_rays(\n      rng,\n      1024,\n      -1.0,\n      1.0,\n      0.0,\n      1.0,\n      0.0,\n      1.0,\n      0.0,\n      1.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )\n  return random_rays"}
{"namespace": "utils.dummy_rays", "completion": "  random_rays = generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1,\n      origin_lo=-1,\n      origin_hi=1,\n      radius_lo=0.1,\n      radius_hi=0.1,\n      near_lo=0.1,\n      near_hi=0.1,\n      far_lo=1,\n      far_hi=1,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )\n  return random_rays"}
{"namespace": "utils.dummy_rays", "completion": "  rng = random.PRNGKey(0)\n  return generate_random_rays(\n      rng,\n      1,\n      -1,\n      1,\n      0.0,\n      1.0,\n      0.0,\n      1.0,\n      0.0,\n      1.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1024,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.1,\n      radius_hi=1.0,\n      near_lo=0.1,\n      near_hi=1.0,\n      far_lo=1.0,\n      far_hi=2.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      jax.random.PRNGKey(0),\n      1,\n      -1,\n      1,\n      0.01,\n      0.1,\n      0.1,\n      1,\n      0.1,\n      1,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1024,\n      origin_lo=-100,\n      origin_hi=100,\n      radius_lo=0.01,\n      radius_hi=10,\n      near_lo=0.1,\n      near_hi=1,\n      far_lo=1,\n      far_hi=10,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1,\n      origin_lo=-1,\n      origin_hi=1,\n      radius_lo=0.01,\n      radius_hi=1,\n      near_lo=0,\n      near_hi=1,\n      far_lo=1,\n      far_hi=2,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  rng = random.PRNGKey(0)\n  return generate_random_rays(\n      rng,\n      10,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=0.0,\n      far_hi=1.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "utils.dummy_rays", "completion": "  rng = random.PRNGKey(0)\n  random_rays = generate_random_rays(\n      rng,\n      n=1,\n      origin_lo=-1,\n      origin_hi=1,\n      radius_lo=0.01,\n      radius_hi=0.01,\n      near_lo=0.1,\n      near_hi=0.1,\n      far_lo=10.0,\n      far_hi=10.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )\n  return random_rays"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1024,\n      origin_lo=-1,\n      origin_hi=1,\n      radius_lo=0.01,\n      radius_hi=0.02,\n      near_lo=0.01,\n      near_hi=0.02,\n      far_lo=0.03,\n      far_hi=0.04,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )\n\n"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Must add half pixel offset to shoot rays through pixel centers.\n  def pix_to_dir(x, y):\n    return xnp.stack([x + 0.5, y + 0.5, xnp.ones_like(x)], axis=-1)\n\n  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          pix_to_dir(pix_x_int, pix_y_int),\n          pix_to_dir(pix_x_int + 1, pix_y_int),\n          pix_to_dir(pix_x_int, pix_y_int + 1),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_dirs_stacked[Ellipsis, :2]), axis=-1))\n    theta ="}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Must add half pixel offset to shoot rays through pixel centers.\n  def pix_to_dir(x, y):\n    return xnp.stack([x + 0.5, y + 0.5, xnp.ones_like(x)], axis=-1)\n\n  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          pix_to_dir(pix_x_int, pix_y_int),\n          pix_to_dir(pix_x_int + 1, pix_y_int),\n          pix_to_dir(pix_x_int, pix_y_int + 1),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_dirs_stacked[Ellipsis, :2]), axis=-1))\n    theta ="}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Must add half pixel offset to shoot rays through pixel centers.\n  def pix_to_dir(x, y):\n    return xnp.stack([x + 0.5, y + 0.5, xnp.ones_like(x)], axis=-1)\n\n  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          pix_to_dir(pix_x_int, pix_y_int),\n          pix_to_dir(pix_x_int + 1, pix_y_int),\n          pix_to_dir(pix_x_int, pix_y_int + 1),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_dirs_stacked[Ellipsis, :2]), axis=-1))\n    theta ="}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Must add half pixel offset to shoot rays through pixel centers.\n  def pix_to_dir(x, y):\n    return xnp.stack([x + 0.5, y + 0.5, xnp.ones_like(x)], axis=-1)\n\n  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          pix_to_dir(pix_x_int, pix_y_int),\n          pix_to_dir(pix_x_int + 1, pix_y_int),\n          pix_to_dir(pix_x_int, pix_y_int + 1),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_dirs_stacked[Ellipsis, :2]), axis=-1))\n    theta ="}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Must add half pixel offset to shoot rays through pixel centers.\n  def pix_to_dir(x, y):\n    return xnp.stack([x + 0.5, y + 0.5, xnp.ones_like(x)], axis=-1)\n\n  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          pix_to_dir(pix_x_int, pix_y_int),\n          pix_to_dir(pix_x_int + 1, pix_y_int),\n          pix_to_dir(pix_x_int, pix_y_int + 1),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_dirs_stacked[Ellipsis, :2]), axis=-1))\n    theta ="}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Must add half pixel offset to shoot rays through pixel centers.\n  def pix_to_dir(x, y):\n    return xnp.stack([x + 0.5, y + 0.5, xnp.ones_like(x)], axis=-1)\n\n  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          pix_to_dir(pix_x_int, pix_y_int),\n          pix_to_dir(pix_x_int + 1, pix_y_int),\n          pix_to_dir(pix_x_int, pix_y_int + 1),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_dirs_stacked[Ellipsis, :2]), axis=-1))\n    theta = x"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Must add half pixel offset to shoot rays through pixel centers.\n  def pix_to_dir(x, y):\n    return xnp.stack([x + 0.5, y + 0.5, xnp.ones_like(x)], axis=-1)\n\n  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          pix_to_dir(pix_x_int, pix_y_int),\n          pix_to_dir(pix_x_int + 1, pix_y_int),\n          pix_to_dir(pix_x_int, pix_y_int + 1),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_dirs_stacked[Ellipsis, :2]), axis=-1))\n    theta = x"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # We need the dx and dy rays to calculate ray differential radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          pix_to_dir(pix_x_int, pix_y_int),\n          pix_to_dir(pix_x_int + 1, pix_y_int),\n          pix_to_dir(pix_x_int, pix_y_int + 1),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_dirs_stacked[Ellipsis, :2]), axis=-1))\n    theta = xnp.minimum(xnp.pi, theta)\n\n    sin_theta_over_theta = xnp.sin(theta) / theta\n    camera_dirs_stacked = xnp.stack(\n        [\n            camera_dirs_stacked[E"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # We need the dx and dy rays to calculate ray differential radii.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          pix_to_dir(pix_x_int, pix_y_int),\n          pix_to_dir(pix_x_int + 1, pix_y_int),\n          pix_to_dir(pix_x_int, pix_y_int + 1),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_dirs_stacked[Ellipsis, :2]), axis=-1))\n    theta = xnp.minimum(xnp.pi, theta)\n\n    sin_theta_over_theta = xnp.sin(theta) / theta\n    camera_dirs_stacked = xnp.stack(\n        [\n            camera_dirs_stacked[Ellipsis, 0] * sin"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Flip from OpenGL to OpenCV coordinate system.\n  points = xnp.matmul(\n      points, xnp.diag(xnp.array([1.0, -1.0, -1.0]))\n  )\n\n  # Apply camera rotation matrices.\n  camera_dirs = xnp.matmul(camtoworlds[Ellipsis, :3, :3], points)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_distort(\n        camera_dirs[Ellipsis, 0],\n        camera_dirs[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_dirs[Ellipsis, :2]), axis=-1))\n    theta = xnp.minimum(xnp.pi, theta)\n\n    sin_theta_over_theta = xnp.sin(theta) / theta\n    camera_dirs = xnp.stack(\n        [\n            camera_dirs[Ellipsis, 0] * sin_theta_over_theta,\n            camera_dirs[Ellipsis, 1] * sin_theta_over_theta,\n            xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  elif camtype == ProjectionType.PANORAMIC:\n    theta = camera_dirs[Ellipsis, 0]\n    phi = camera_dirs[Ellipsis, 1]\n    # Negation on y and z components accounts for expected OpenCV convention.\n    camera_dirs = xnp.stack(\n        [\n            -xnp.sin(phi) * xnp.sin(theta"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Flip from OpenGL to OpenCV coordinate system.\n  points = xnp.matmul(\n      points, xnp.diag(xnp.array([1.0, -1.0, -1.0]))\n  )\n\n  # Apply camera rotation matrices.\n  points = xnp.matmul(\n      camtoworlds[Ellipsis, :3, :3], points[Ellipsis, :, None]\n  )[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  points = xnp.matmul(\n      pixtocams, points[Ellipsis, :, None]\n  )[Ellipsis, 0]\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(points[Ellipsis, :2]), axis=-1))\n    theta = xnp.minimum(xnp.pi, theta)\n\n    points = xnp.stack(\n        [\n            points[Ellipsis, 0] * xnp.sin(theta) / theta,\n            points[Ellipsis, 1] * xnp.sin(theta) / theta,\n            xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  elif camtype == ProjectionType.PANORAMIC:\n    theta = points[Ellipsis, 0]\n    phi = points[Ellipsis, 1]\n    # Negation on y and z components accounts for expected OpenCV convention.\n    points = xnp.stack(\n        [\n            -xnp.sin(phi) * xnp.sin(theta),\n            -xnp.cos(phi),\n            -xnp.sin(phi) * xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  # Correct for distortion.\n  if distortion_params is not None:\n    x, y = _radial_and_tangential_distort(\n       "}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # We need the dx and dy rays to calculate ray differential radii.\n  points_stacked = xnp.stack(\n      [\n          points,\n          points + xnp.array([1.0, 0.0, 0.0]),\n          points + xnp.array([0.0, 1.0, 0.0]),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply camera rotation matrices.\n  directions_stacked = mat_vec_mul(\n      camtoworlds[Ellipsis, :3, :3], points_stacked\n  )\n  # Extract the offset rays.\n  directions, dx, dy = directions_stacked\n\n  # Flip from OpenCV to OpenGL coordinate system.\n  directions = xnp.stack(\n      [\n          directions[Ellipsis, 0],\n          -directions[Ellipsis, 1],\n          -directions[Ellipsis, 2],\n      ],\n      axis=-1,\n  )\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_distort(\n        directions[Ellipsis, 0],\n        directions[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    directions = xnp.stack([x, y, directions[Ellipsis, 2]], axis=-1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(directions[Ellipsis, :2]), axis=-1))\n    theta"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Compute the 3D point coordinates in camera coordinates.\n  points_cam = xnp.matmul(camtoworlds[Ellipsis, :3, :3], points[Ellipsis, :, None]) + camtoworlds[Ellipsis, :3, 3:]\n\n  # Project the 3D points onto the image plane.\n  points_imageplane = xnp.matmul(pixtocams, points_cam)[Ellipsis, :2, 0]\n\n  # Correct for distortion.\n  if distortion_params is not None:\n    points_imageplane = _radial_and_tangential_distort(\n        points_imageplane[Ellipsis, 0],\n        points_imageplane[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n\n  # Flip from OpenCV to OpenGL coordinate system.\n  points_imageplane = xnp.stack(\n      [points_imageplane[Ellipsis, 0], -points_imageplane[Ellipsis, 1]],\n      axis=-1,\n  )\n\n  # Compute the depth values of the points in the camera coordinate system.\n  depth = points_cam[Ellipsis, 2, 0]\n\n  return points_imageplane, depth"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Convert world coordinates to camera coordinates.\n  points_cam = matmul(camtoworlds[Ellipsis, :3, :3], points[Ellipsis, None])\n  points_cam = points_cam[Ellipsis, 0] + camtoworlds[Ellipsis, :3, 3]\n\n  # Project to pixel coordinates.\n  points_pix = mat_vec_mul(pixtocams, points_cam)\n\n  # Correct principal point.\n  points_pix = points_pix[Ellipsis, :2] + pixtocams[Ellipsis, :2, 2]\n\n  # Distort with radial and tangential distortion.\n  if distortion_params is not None:\n    x, y = _radial_and_tangential_distort(\n        points_pix[Ellipsis, 0],\n        points_pix[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    points_pix = xnp.stack([x, y], axis=-1)\n\n  # Flip x and y.\n  points_pix = xnp.stack([points_pix[Ellipsis, 1], points_pix[Ellipsis, 0]], axis=-1)\n\n  # Compute depth values.\n  depth = xnp.linalg.norm(points - points_cam, axis=-1)\n\n  return points_pix, depth\n\n"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Compute the 3D point coordinates in camera space.\n  points_cam = xnp.linalg.inv(camtoworlds[Ellipsis, :3, :3]) @ (\n      points[Ellipsis, None] - camtoworlds[Ellipsis, :3, -1]\n  )\n\n  # Compute the 2D pixel coordinates in NDC space.\n  points_ndc = points_cam[Ellipsis, :2] / points_cam[Ellipsis, 2:3]\n\n  # Apply the camera intrinsic matrix.\n  points_pix = pixtocams @ points_ndc\n\n  # Correct for radial and tangential distortion.\n  if distortion_params is not None:\n    points_pix = _radial_and_tangential_distort(\n        points_pix[Ellipsis, 0],\n        points_pix[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n\n  # Compute the depth values.\n  depth = xnp.linalg.norm(points - camtoworlds[Ellipsis, :3, -1], axis=-1)\n\n  return points_pix, depth"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Add a small offset to the z-coordinate to ensure numerical stability.\n  eps = 1e-10\n  points = points + xnp.array([0, 0, eps], dtype=xnp.float32)\n\n  # Convert from world to camera coordinates.\n  points = xnp.matmul(\n      points - camtoworlds[Ellipsis, :3, -1],\n      xnp.swapaxes(camtoworlds[Ellipsis, :3, :3], -1, -2),\n  )\n\n  # Convert from camera to pixel coordinates.\n  points = xnp.matmul(points, pixtocams)\n  points = points[Ellipsis, :2] / points[Ellipsis, 2:3]\n\n  # Correct for distortion.\n  if distortion_params is not None:\n    points = _radial_and_tangential_distort(\n        points[Ellipsis, 0],\n        points[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n\n  # Convert from OpenCV to OpenGL coordinates.\n  points = xnp.stack([points[Ellipsis, 0], 1.0 - points[Ellipsis, 1]], axis=-1)\n\n  # Compute the depth values.\n  depth = xnp.linalg.norm(points - camtoworlds[Ellipsis, :3, -1], axis=-1)\n\n  return points, depth"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Compute the 3D point coordinates in camera coordinates.\n  # The camera coordinate system is assumed to be OpenGL-style, where the camera\n  # is looking down the negative Z axis, with the X axis pointing to the right\n  # and the Y axis pointing up.\n  # This is consistent with the camera coordinate system used by OpenCV and\n  # OpenGL, but is different from the camera coordinate system used by COLMAP.\n  #\n  # The camera coordinate system used by COLMAP is OpenGL-style, but with the\n  # camera looking down the positive Z axis, with the X axis pointing to the\n  # right and the Y axis pointing down.\n  #\n  # The camera coordinate system used by the NeRF code is OpenGL-style, but with\n  # the camera looking down the negative Z axis, with the X axis pointing to the\n  # right and the Y axis pointing up.\n  #\n  # The camera coordinate system used by the NeRF code is different from the\n  # camera coordinate system used by COLMAP.\n  #\n  # The camera coordinate system used by the NeRF code is different from the\n  # camera coordinate system used by OpenCV and OpenGL.\n  #\n  # The camera coordinate system used by the NeRF code is different from the\n  # camera coordinate system used by the NeRF code.\n  #\n  # The camera coordinate system used by the NeRF code is different from the\n  # camera coordinate system used by the NeRF code.\n  #\n  # The camera coordinate system used by the NeRF code is different from the\n  # camera coordinate system used by the NeRF code.\n  #\n  # The camera coordinate system used by the NeRF code is different from the\n  # camera coordinate system used by the NeRF code.\n  #\n  # The camera coordinate system used by the NeRF code is different from the\n  # camera coordinate system used by the NeRF code.\n  #\n  # The camera coordinate system used by the NeRF code is different from the\n  # camera coordinate system used by the NeRF code.\n  #\n  # The camera coordinate system used by the NeRF code is different from the\n  # camera coordinate system used by the NeRF code."}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Extract the camera intrinsics and extrinsics from the input arguments.\n  # pixtocams: Inverse of the camera intrinsic matrix.\n  # camtoworlds: Camera extrinsic matrix.\n  # distortion_params: Optional dictionary of distortion parameters.\n  # camtype: Camera projection type.\n  # xnp: Either numpy or jax.numpy.\n\n  # Define a function to convert a 3D point to a 2D pixel coordinate.\n  def point_to_pixel(point):\n    # Convert the 3D point to a homogeneous coordinate.\n    point_h = xnp.concatenate([point, xnp.ones_like(point[Ellipsis, :1])], axis=-1)\n    # Apply the camera extrinsic matrix to the point.\n    point_cam = xnp.matmul(camtoworlds, point_h[Ellipsis, None])[Ellipsis, 0]\n    # Convert the point to a 2D pixel coordinate.\n    point_pixel = xnp.matmul(pixtocams, point_cam[Ellipsis, None])[Ellipsis, 0]\n    # Normalize the pixel coordinate.\n    point_pixel = point_pixel / point_pixel[Ellipsis, 2:]\n    # Return the pixel coordinate.\n    return point_pixel\n\n  # Define a function to apply radial and tangential distortion to a 2D pixel coordinate.\n  def distort_pixel(point_pixel):\n    # Extract the x and y coordinates of the pixel.\n    x, y = point_pixel[Ellipsis, 0], point_pixel[Ellipsis, 1]\n    # Apply radial and tangential distortion to the pixel.\n    x, y = _radial_and_tangential_distort(x, y, **distortion_params)\n    # Return the distorted pixel.\n    return xnp.stack([x, y, xnp.ones_like(x)], axis=-1)\n\n  # Define a function to apply"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Compute the 2D pixel coordinates of the input 3D points.\n  # The resulting coordinates are in the OpenCV coordinate system, with the origin at the top-left corner of the image.\n  # The z coordinate of the camera coordinate system is negative, pointing towards the scene.\n  points_cam = matmul(camtoworlds[Ellipsis, :3, :3], points[Ellipsis, None])[Ellipsis, 0]\n  points_cam = points_cam + camtoworlds[Ellipsis, :3, -1]\n\n  # Compute the depth values of the input 3D points in the camera coordinate system.\n  # The depth values are positive and represent the distance from the camera to the 3D point.\n  depth = points_cam[Ellipsis, 2]\n\n  # Compute the 2D pixel coordinates of the input 3D points in the image plane.\n  # The resulting coordinates are in the OpenCV coordinate system, with the origin at the top-left corner of the image.\n  points_cam = points_cam / points_cam[Ellipsis, 2:3]\n  points_cam = points_cam[Ellipsis, :2]\n\n  # Apply camera intrinsics to transform points from camera coordinates to pixel coordinates.\n  points_pixel = matmul(pixtocams, points_cam[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply distortion correction if distortion_params are provided.\n  if distortion_params is not None:\n    x, y = _radial_and_tangential_distort(\n        points_pixel[Ellipsis, 0],\n        points_pixel[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    points_pixel = xnp.stack([x, y], axis=-1)\n\n  # Flip from OpenCV to OpenGL coordinate system.\n  points_pixel = matmul(\n      points_pixel, xnp.diag(xnp"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Shape of points: [..., 3]\n  # Shape of pixtocams: [..., 3, 3]\n  # Shape of camtoworlds: [..., 3, 4]\n\n  # Convert world coordinates to camera coordinates.\n  points_cam = xnp.matmul(\n      points, camtoworlds[Ellipsis, :3, :3].swapaxes(-1, -2)\n  ) + camtoworlds[Ellipsis, :3, -1:]\n\n  # Convert camera coordinates to pixel coordinates.\n  if camtype == ProjectionType.FISHEYE:\n    # Compute the radius of the point in the camera coordinate system.\n    r = xnp.linalg.norm(points_cam, axis=-1)\n\n    # Normalize the point and scale it by the inverse of the radius.\n    # This is equivalent to the projection onto the image plane.\n    points_cam = points_cam / r[Ellipsis, None]\n\n  # Project the point onto the image plane.\n  points_image = xnp.matmul(points_cam, pixtocams.swapaxes(-1, -2))\n\n  # Apply distortion if distortion_params is provided.\n  if distortion_params is not None:\n    x, y = _radial_and_tangential_distort(\n        points_image[Ellipsis, 0],\n        points_image[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    points_image = xnp.stack([x, y, points_image[Ellipsis, 2]], axis=-1)\n\n  # Extract the depth values from the camera coordinates.\n  depth = points_cam[Ellipsis, 2]\n\n  # Flip x and y coordinates to match the OpenCV convention.\n  points_image = points_image[Ellipsis, ::-1]\n\n  # Convert from homogeneous coordinates to 2D pixel coordinates.\n  pixels = points_image["}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[Ellipsis, :3]\n  v = screw_axis[Ellipsis, 3:]\n  theta_squared = jnp.sum(w**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  G_taylor = jnp.eye(3) + skew(w)\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta_squared > eps**2, w, 0.0)\n  theta_safe = jnp.where(theta_squared > eps**2, theta, 1.0)\n  w_safe = w_safe / theta_safe\n  G = (\n      jnp.eye(3)\n      + (1.0 - jnp.cos(theta_safe)) * skew(w_safe)\n      + (theta_safe - jnp.sin(theta_safe)) * spin_math.matmul(skew(w_safe), skew(w_safe))\n  )\n\n  R = jnp.where(theta_squared > eps**2, G, G_taylor)\n  p = jnp.where(theta_squared > eps**2, jnp.matmul(G, v[Ellipsis, jnp.newaxis]), v[Ellipsis, jnp.newaxis])\n  return rp_to_se3(R, p.squeeze(-1))"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[Ellipsis, :3]\n  v = screw_axis[Ellipsis, 3:]\n  theta_squared = jnp.sum(w**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  R_taylor = exp_so3(w, eps)\n  p_taylor = v\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta_squared > eps**2, w, 0.0)\n  theta_safe = jnp.where(theta_squared > eps**2, theta, 1.0)\n  w_safe = w_safe / theta_safe\n  W = skew(w_safe)\n  WW = spin_math.matmul(W, W)\n  G_inv = jnp.eye(3) - 0.5 * W + (1.0 / theta_safe**2) * (1.0 - 0.5 / jnp.tan(0.5 * theta_safe)) * WW\n  p_safe = jnp.where(theta_squared > eps**2, p_taylor, 0.0)\n  v_safe = jnp.where(theta_squared > eps**2, v, 0.0)\n  p = spin_math.matmul(G_inv, v_safe[Ellipsis, jnp.newaxis]).squeeze(-1)\n  R = jnp.where(theta_squared > eps**2, R_taylor, jnp.eye(3))\n  X = rp_to_se3(R, p)\n  return X"}
{"namespace": "rigid_body.exp_se3", "completion": "  w, v = screw_axis[3:], screw_axis[:3]\n  theta = _safe_sqrt(jnp.sum(w**2))\n  W = skew(w / theta)\n  G = jnp.eye(3)\n  G += jnp.sin(theta) * W\n  G += (1.0 - jnp.cos(theta)) * spin_math.matmul(W, W)\n  R = exp_so3(w, eps)\n  p = spin_math.matmul(G, v[Ellipsis, jnp.newaxis]).squeeze(-1)\n  return rp_to_se3(R, p)"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[Ellipsis, :3]\n  v = screw_axis[Ellipsis, 3:]\n\n  theta_squared = jnp.sum(w**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  R_taylor = exp_so3(w, eps)\n  p_taylor = v\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta_squared > eps**2, w, 0.0)\n  theta_safe = jnp.where(theta_squared > eps**2, theta, 1.0)\n  w_safe = w_safe / theta_safe\n  W = skew(w_safe)\n  W_W = spin_math.matmul(W, W)\n\n  R = (\n      jnp.eye(3)\n      + jnp.sin(theta_safe) * W\n      + (1.0 - jnp.cos(theta_safe)) * W_W\n  )\n  G = (\n      jnp.eye(3)\n      + (1.0 - jnp.cos(theta_safe)) * W\n      + (theta_safe - jnp.sin(theta_safe)) * W_W\n  )\n  p = spin_math.matmul(G, v[Ellipsis, jnp.newaxis]).squeeze(-1)\n\n  return jnp.where(\n      theta_squared > eps**2,\n      rp_to_se3(R, p),\n      rp_to_se3(R_taylor, p_taylor),\n  )"}
{"namespace": "rigid_body.exp_se3", "completion": "  w, v = screw_axis[3:], screw_axis[:3]\n  theta = _safe_sqrt(jnp.sum(w**2, axis=-1))\n\n  # Near zero, use first order Taylor expansion.\n  R = exp_so3(w, eps)\n  p = v\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta > eps, w, 0.0)\n  theta_safe = jnp.where(theta > eps, theta, 1.0)\n  W = skew(w_safe)\n  W_sq = spin_math.matmul(W, W)\n  R_taylor = jnp.eye(3) + W + (1.0 - jnp.cos(theta_safe)) * W_sq\n  p_taylor = jnp.where(theta > eps, v * theta_safe, 0.0)\n\n  return rp_to_se3(\n      jnp.where(theta > eps, R, R_taylor), jnp.where(theta > eps, p, p_taylor)\n  )"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[Ellipsis, :3]\n  v = screw_axis[Ellipsis, 3:]\n  theta_squared = jnp.sum(w**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  R_taylor = exp_so3(w, eps)\n  p_taylor = v\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta_squared > eps**2, w, 0.0)\n  theta_safe = jnp.where(theta_squared > eps**2, theta, 1.0)\n  w_safe = w_safe / theta_safe\n  W = skew(w_safe)\n  W_W = spin_math.matmul(W, W)\n\n  G = (\n      jnp.eye(3)\n      + (1.0 - jnp.cos(theta_safe)) * W\n      + (theta_safe - jnp.sin(theta_safe)) * W_W\n  )\n\n  p = spin_math.matmul(G, v[Ellipsis, jnp.newaxis]).squeeze(-1)\n  p_safe = jnp.where(theta_squared > eps**2, p, p_taylor)\n  R_safe = jnp.where(theta_squared > eps**2, R_taylor, jnp.eye(3))\n  return rp_to_se3(R_safe, p_safe)"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[Ellipsis, :3]\n  v = screw_axis[Ellipsis, 3:]\n  theta_squared = jnp.sum(w**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  R_taylor = exp_so3(w, eps)\n  p_taylor = v\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta_squared > eps**2, w, 0.0)\n  theta_safe = jnp.where(theta_squared > eps**2, theta, 1.0)\n  w_safe = w_safe / theta_safe\n  G = spin_math.matmul(\n      jnp.eye(3) * theta_safe + (1.0 - jnp.cos(theta_safe)) * skew(w_safe) +\n      (theta_safe - jnp.sin(theta_safe)) * spin_math.matmul(skew(w_safe),\n                                                          skew(w_safe)),\n      skew(v),\n  )\n\n  R = jnp.where(theta_squared > eps**2, R_taylor, jnp.eye(3))\n  p = jnp.where(theta_squared > eps**2, p_taylor, v)\n\n  return rp_to_se3(R, p)"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[Ellipsis, :3]\n  v = screw_axis[Ellipsis, 3:]\n\n  # Compute the rotation matrix from the exponential map.\n  R = exp_so3(w, eps)\n\n  # Compute the translation vector.\n  theta_squared = jnp.sum(w**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n  G_inv1 = jnp.eye(3)\n  G_inv2 = (1.0 - jnp.cos(theta)) * skew(w) / theta\n  G_inv3 = (theta - jnp.sin(theta)) * spin_math.matmul(skew(w), skew(w)) / theta\n  G_inv = G_inv1 + G_inv2 + G_inv3\n  p = spin_math.matmul(G_inv, v[Ellipsis, jnp.newaxis]).squeeze(-1)\n\n  # If theta = 0 then the transformation is a pure translation and v = p.\n  # This avoids using the numerically unstable G matrix when theta is near zero.\n  p = jnp.where(theta_squared > eps, p, v)\n\n  return rp_to_se3(R, p)"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[Ellipsis, :3]\n  v = screw_axis[Ellipsis, 3:]\n  theta_squared = jnp.sum(w**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  R_taylor = exp_so3(w, eps)\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta_squared > eps**2, w, 0.0)\n  theta_safe = jnp.where(theta_squared > eps**2, theta, 1.0)\n  w = w_safe / theta_safe\n  W = skew(w)\n  G_inv1 = jnp.eye(3)\n  G_inv2 = -0.5 * W\n  G_inv3 = (1.0 / theta_safe - 0.5 * jnp.cos(theta_safe) / jnp.sin(theta_safe)) * spin_math.matmul(W, W)\n  G_inv = G_inv1 + G_inv2 + G_inv3\n\n  p = spin_math.matmul(G_inv, v[Ellipsis, jnp.newaxis]).squeeze(-1)\n  # If theta = 0 then the transformation is a pure translation and p = v.\n  # This avoids using the numerically unstable G matrix when theta is near zero.\n  p = jnp.where(theta_squared > eps**2, p, v)\n  X = rp_to_se3(R_taylor, p)\n  return X"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[Ellipsis, :3]\n  v = screw_axis[Ellipsis, 3:]\n  theta_squared = jnp.sum(w**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n\n  # Near zero, use first order Taylor expansion.\n  R_taylor = exp_so3(w, eps)\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta_squared > eps**2, w, 0.0)\n  theta_safe = jnp.where(theta_squared > eps**2, theta, 1.0)\n  w_safe = w_safe / theta_safe\n  W = skew(w_safe)\n  W_W = spin_math.matmul(W, W)\n  G_theta = (\n      jnp.eye(3)\n      + (1.0 - jnp.cos(theta_safe)) * W\n      + (theta_safe - jnp.sin(theta_safe)) * W_W\n  )\n  V = jnp.where(theta_squared > eps**2, (theta_safe * jnp.eye(3) + (1.0 - jnp.cos(theta_safe)) * W + (theta_safe - jnp.sin(theta_safe)) * W_W) @ (v[Ellipsis, jnp.newaxis] / theta_safe), v[Ellipsis, jnp.newaxis])\n  T = jnp.concatenate([G_theta, V], axis=-1)\n  return jnp.where(theta_squared > eps**2, T, rp_to_se3(R_taylor, v))"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[3:]\n  v = screw_axis[:3]\n\n  theta = _safe_sqrt(jnp.sum(w**2, axis=-1))\n  theta_squared = theta**2\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  R_taylor = exp_so3(w, eps)\n  V_taylor = jnp.eye(3) + (1.0 - 0.5 * theta_squared / theta) * skew(w)\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta_squared > eps**2, w, 0.0)\n  theta_safe = jnp.where(theta_squared > eps**2, theta, 1.0)\n  w_safe = w_safe / theta_safe\n  V_safe = jnp.where(theta_squared > eps**2, V_taylor, jnp.eye(3))\n\n  R = jnp.where(theta_squared > eps**2, R_taylor, jnp.eye(3))\n  V = jnp.where(theta_squared > eps**2, V_safe, jnp.eye(3))\n\n  p = V @ v[Ellipsis, jnp.newaxis]\n  p = p.squeeze(-1)\n\n  return rp_to_se3(R, p)"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[Ellipsis, :3]\n  v = screw_axis[Ellipsis, 3:]\n  theta = _safe_sqrt(jnp.sum(w**2, axis=-1))\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  R_taylor = exp_so3(w, eps)\n  R = jnp.where(theta > eps, R_taylor, jnp.eye(3))\n  p_taylor = jnp.where(theta > eps, v * theta, 0.0)\n  p = jnp.where(theta > eps, v, 0.0)\n\n  # Prevent bad gradients from propagating back when theta is small.\n  theta_safe = jnp.where(theta > eps, theta, 1.0)\n  theta_safe_inv = jnp.where(theta > eps, 1.0 / theta, 1.0)\n  W = skew(w / theta_safe)\n  G_inv = jnp.eye(3) - 0.5 * W + theta_safe_inv * theta_safe_inv * spin_math.matmul(W, W)\n  p = p_taylor + spin_math.matmul(G_inv, v[Ellipsis, jnp.newaxis]).squeeze(-1)\n\n  X = rp_to_se3(R, p)\n  return X"}
{"namespace": "rigid_body.exp_se3", "completion": "  w, v = screw_axis[Ellipsis, :3], screw_axis[Ellipsis, 3:]\n  theta = _safe_sqrt(jnp.sum(w**2, axis=-1))\n  theta_squared = theta**2\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  R_taylor = exp_so3(w, eps)\n  w_taylor = w\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta_squared > eps**2, w, 0.0)\n  theta_safe = jnp.where(theta_squared > eps**2, theta, 1.0)\n  w_safe = w_safe / theta_safe\n\n  # Compute the translation.\n  G_inv1 = jnp.eye(3)\n  G_inv2 = (1.0 - jnp.cos(theta_safe)) * skew(w_safe)\n  G_inv3 = (theta_safe - jnp.sin(theta_safe)) * spin_math.matmul(skew(w_safe), skew(w_safe))\n  G_inv = G_inv1 + G_inv2 + G_inv3\n  p = spin_math.matmul(G_inv, v[Ellipsis, jnp.newaxis]).squeeze(-1)\n\n  # Compute the rotation.\n  R = exp_so3(w, eps)\n\n  # Assemble the output.\n  X = rp_to_se3(R, p)\n  X = jnp.where(theta_squared > eps**2, X, rp_to_se3(R_taylor, v))\n  return X"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[Ellipsis, :3]\n  v = screw_axis[Ellipsis, 3:]\n  theta_squared = jnp.sum(w**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n  W = skew(w)\n  W_inv = skew(-w)\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  G_taylor = jnp.eye(3)\n  V_taylor = jnp.eye(3)\n\n  # Prevent bad gradients from propagating back when theta is small.\n  theta_safe = jnp.where(theta_squared > eps**2, theta, 1.0)\n  W_safe = jnp.where(theta_squared > eps**2, W, jnp.zeros_like(W))\n  W_inv_safe = jnp.where(theta_squared > eps**2, W_inv, jnp.zeros_like(W_inv))\n  G_safe = jnp.where(\n      theta_squared > eps**2,\n      jnp.eye(3) + (1.0 - jnp.cos(theta_safe)) * W_safe / theta_safe,\n      jnp.zeros_like(W_safe),\n  )\n  V_safe = jnp.where(\n      theta_squared > eps**2,\n      jnp.eye(3)\n      + (\n          (1.0 - jnp.cos(theta_safe)) * W_safe / theta_safe\n          + (theta_safe - jnp.sin(theta_safe))\n          * spin_math.matmul(W_safe, W_safe) / theta_safe**2\n      ),\n      jnp.zeros_like(W_safe),\n  )\n\n  R = jnp.eye(3) + W_safe + spin_math.matmul(V_safe, W_inv_safe)\n  p = spin"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[Ellipsis, :3]\n  v = screw_axis[Ellipsis, 3:]\n  theta = _safe_sqrt(jnp.sum(w**2, axis=-1))\n  theta_squared = theta**2\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  R_taylor = exp_so3(w, eps)\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta_squared > eps**2, w, 0.0)\n  theta_safe = jnp.where(theta_squared > eps**2, theta, 1.0)\n  W = skew(w_safe / theta_safe)\n  W_2 = spin_math.matmul(W, W)\n\n  # Compute the translation part of the exponential map.\n  G_inv = jnp.eye(3)\n  G_inv = G_inv - 0.5 * W + (1.0 - theta_squared / 6.0) * W_2\n  G_inv = jnp.where(theta_squared > eps**2, G_inv, jnp.zeros_like(G_inv))\n  p = spin_math.matmul(G_inv, v[Ellipsis, jnp.newaxis]).squeeze(-1)\n\n  # Compute the rotation part of the exponential map.\n  R = exp_so3(w, eps)\n\n  return rp_to_se3(R, p)"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[Ellipsis, :3]\n  v = screw_axis[Ellipsis, 3:]\n\n  theta_squared = jnp.sum(w**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  R_taylor = exp_so3(w, eps)\n  p_taylor = v\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta_squared > eps**2, w, 0.0)\n  theta_safe = jnp.where(theta_squared > eps**2, theta, 1.0)\n  w_safe = w_safe / theta_safe\n  w_safe_skew = skew(w_safe)\n\n  W = skew(w)\n  W_squared = spin_math.matmul(W, W)\n\n  G_inv1 = jnp.eye(3)\n  G_inv2 = -0.5 * W + (1.0 - theta_safe / (2.0 * jnp.tan(theta_safe / 2.0))) * W_squared / theta_safe\n  G_inv3 = (\n      -1.0 / theta_safe\n      + (1.0 - theta_safe / (2.0 * jnp.tan(theta_safe / 2.0))) / theta_safe**2\n      + (1.0 / theta_safe - 1.0 / (2.0 * jnp.tan(theta_safe / 2.0))) * W_squared / theta_safe**2\n  ) * spin_math.matmul(W, W_squared)\n  G_inv = G_inv1 + G_inv2 + G_inv3\n\n  p = spin_math.matmul(G_inv, v[Ellipsis, jnp.newaxis]).squeeze(-1"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[Ellipsis, :3]\n  v = screw_axis[Ellipsis, 3:]\n  theta = _safe_sqrt(jnp.sum(w**2, axis=-1))\n  theta_squared = theta**2\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  W = skew(w)\n  R = exp_so3(w, eps)\n  S_taylor = jnp.eye(3) + W + (1.0 - jnp.cos(theta)) * spin_math.matmul(W, W)\n  V_taylor = jnp.eye(3) + W + (1.0 - jnp.cos(theta)) * spin_math.matmul(W, W)\n\n  # Prevent bad gradients from propagating back when theta is small.\n  W_safe = jnp.where(theta_squared > eps**2, W, 0.0)\n  R_safe = jnp.where(theta_squared > eps**2, R, jnp.eye(3))\n  S_safe = jnp.where(theta_squared > eps**2, S_taylor, jnp.eye(3))\n  V_safe = jnp.where(theta_squared > eps**2, V_taylor, jnp.eye(3))\n\n  # Compute the translation part of the homogeneous transformation matrix.\n  G = (\n      jnp.eye(3) * theta\n      + (1.0 - jnp.cos(theta)) * W_safe\n      + (theta - jnp.sin(theta)) * spin_math.matmul(W_safe, W_safe)\n  )\n  p = spin_math.matmul(G, v[Ellipsis, jnp.newaxis]).squeeze(-1)\n  return rp_to_se3(R_safe, p)"}
{"namespace": "rigid_body.exp_se3", "completion": "  w, v = screw_axis[3:], screw_axis[:3]\n  theta = _safe_sqrt(jnp.sum(w**2))\n  W = skew(w / theta)\n  WW = spin_math.matmul(W, W)\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  R_taylor = exp_so3(w, eps)\n  V_taylor = jnp.eye(3) + W + WW\n\n  # Prevent bad gradients from propagating back when theta is small.\n  W_safe = jnp.where(theta > eps, W, 0.0)\n  V_safe = jnp.where(theta > eps, V_taylor, 0.0)\n\n  R = jnp.where(theta > eps, R_taylor, jnp.eye(3))\n  V = jnp.where(theta > eps, V_safe, jnp.eye(3))\n\n  # Compute the translation part of the transform.\n  p = spin_math.matmul(V, v[Ellipsis, jnp.newaxis]).squeeze(-1)\n  return rp_to_se3(R, p)"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[Ellipsis, :3]\n  v = screw_axis[Ellipsis, 3:]\n  theta = _safe_sqrt(jnp.sum(w**2, axis=-1))\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  R_taylor = exp_so3(w, eps)\n  w_taylor = w / theta\n  w_taylor_cross = skew(w_taylor)\n  w_taylor_cross_squared = spin_math.matmul(w_taylor_cross, w_taylor_cross)\n  V_taylor = jnp.eye(3) + w_taylor_cross + (\n      (1.0 - jnp.cos(theta)) / theta**2\n  ) * w_taylor_cross_squared\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta > eps, w, 0.0)\n  theta_safe = jnp.where(theta > eps, theta, 1.0)\n  w_safe_cross = skew(w_safe / theta_safe)\n  w_safe_cross_squared = spin_math.matmul(w_safe_cross, w_safe_cross)\n  V = jnp.eye(3) + (\n      (1.0 - jnp.cos(theta_safe)) / theta_safe**2\n  ) * w_safe_cross_squared + (\n      (theta_safe - jnp.sin(theta_safe)) / theta_safe**3\n  ) * w_safe_cross_squared @ w_safe_cross\n\n  R = jnp.where(theta > eps, R_taylor, jnp.eye(3))\n  V = jnp.where(theta > eps, V, V_taylor)\n  X = rp_to_se3(R, V @ v[Ellipsis, jnp.newaxis])\n\n "}
{"namespace": "rigid_body.exp_se3", "completion": "  w, v = screw_axis[Ellipsis, :3], screw_axis[Ellipsis, 3:]\n  theta = _safe_sqrt(jnp.sum(w**2, axis=-1))\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  R_taylor = exp_so3(w, eps)\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta > eps, w, 0.0)\n  theta_safe = jnp.where(theta > eps, theta, 1.0)\n  W = skew(w_safe / theta_safe)\n  W_2 = spin_math.matmul(W, W)\n  I = jnp.eye(3)\n\n  G_inv = jnp.eye(3)\n  G_inv += (1.0 - 0.5 * theta_safe / jnp.tan(theta_safe / 2.0)) * W\n  G_inv += (\n      (theta_safe - jnp.sin(theta_safe)) / theta_safe**3\n  ) * W_2\n\n  V = I + (1.0 - jnp.cos(theta_safe)) * W + (theta_safe - jnp.sin(theta_safe)) / (\n      theta_safe**2\n  ) * W_2\n\n  # The translation is the same as the one in the Taylor expansion.\n  p = spin_math.matmul(G_inv, v[Ellipsis, jnp.newaxis]).squeeze(-1)\n\n  # If theta = 0 then the transformation is a pure translation and v = p.\n  # This avoids using the numerically unstable G matrix when theta is near zero.\n  p = jnp.where(theta > eps, p, v)\n\n  X = rp_to_se3(R_taylor, p)\n  return X"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta_squared = jnp.sum(axis_angle**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n  W = skew(axis_angle / theta)\n  # If theta = 0 then the rotation is the identity.\n  R = jnp.where(theta_squared > eps**2, jnp.eye(3) + jnp.sin(theta) * W + (1.0 - jnp.cos(theta)) * spin_math.matmul(W, W), jnp.eye(3))\n  return R"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta_squared = jnp.sum(axis_angle**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n  W = skew(axis_angle / theta)\n  # If theta = 0 then the rotation is the identity matrix.\n  R = jnp.where(\n      theta_squared > eps,\n      jnp.eye(3) + jnp.sin(theta) * W + (1.0 - jnp.cos(theta)) * spin_math.matmul(W, W),\n      jnp.eye(3),\n  )\n  return R"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta_squared = jnp.sum(axis_angle**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n  W = skew(axis_angle / theta)\n  # If theta = 0 then the rotation is the identity.\n  R = jnp.where(theta_squared > eps, jnp.eye(3) + W * jnp.sin(theta) + spin_math.matmul(W, W) * (1.0 - jnp.cos(theta)), jnp.eye(3))\n  return R"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = jnp.linalg.norm(axis_angle)\n  w = axis_angle / (theta + eps)\n  W = skew(w)\n  R = jnp.eye(3) + jnp.sin(theta) * W + (1 - jnp.cos(theta)) * W @ W\n  return R"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = _safe_sqrt(jnp.sum(axis_angle**2))\n  W = skew(axis_angle / theta)\n  # If theta is close to 0, theta_squared is close to 0.\n  theta_squared = theta**2\n  # If theta is close to 0, theta_squared is close to 0.\n  theta_squared = theta**2\n  # If theta is close to 0, theta_squared is close to 0.\n  theta_squared = theta**2\n  # If theta is close to 0, theta_squared is close to 0.\n  theta_squared = theta**2\n  # If theta is close to 0, theta_squared is close to 0.\n  theta_squared = theta**2\n  # If theta is close to 0, theta_squared is close to 0.\n  theta_squared = theta**2\n  # If theta is close to 0, theta_squared is close to 0.\n  theta_squared = theta**2\n  # If theta is close to 0, theta_squared is close to 0.\n  theta_squared = theta**2\n  # If theta is close to 0, theta_squared is close to 0.\n  theta_squared = theta**2\n  # If theta is close to 0, theta_squared is close to 0.\n  theta_squared = theta**2\n  # If theta is close to 0, theta_squared is close to 0.\n  theta_squared = theta**2\n  # If theta is close to 0, theta_squared is close to 0.\n  theta_squared = theta**2\n  # If theta is close to 0, theta_squared is close to 0.\n  theta_squared = theta**2\n  # If theta is close to"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = jnp.linalg.norm(axis_angle)\n  w = axis_angle / theta\n  w_hat = skew(w)\n  R = (\n      jnp.eye(3)\n      + jnp.sin(theta) * w_hat\n      + (1.0 - jnp.cos(theta)) * spin_math.matmul(w_hat, w_hat)\n  )\n  return R"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = _safe_sqrt(jnp.sum(axis_angle**2, axis=-1))\n  W = skew(axis_angle / theta)\n  # Note that theta = 0 means no rotation.\n  R = (\n      jnp.eye(3)\n      + jnp.sin(theta) * W\n      + (1.0 - jnp.cos(theta)) * spin_math.matmul(W, W)\n  )\n  # If theta = 0 then the rotation matrix is the identity.\n  R = jnp.where(theta > eps, R, jnp.eye(3))\n  return R"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta_squared = jnp.sum(axis_angle**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n  W = skew(axis_angle / theta)\n  # Note that theta = 0 means the axis_angle is the zero vector, so we can set\n  # R = I.\n  R = jnp.eye(3) + jnp.sin(theta) * W + (1.0 - jnp.cos(theta)) * spin_math.matmul(W, W)\n  # If theta = 0 then the rotation is the identity.\n  R = jnp.where(theta_squared > eps, R, jnp.eye(3))\n  return R"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta_squared = jnp.sum(axis_angle**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n  W = skew(axis_angle / theta)\n  # Note that theta^2 == 0.0 implies a 0-rotation.\n  R = (\n      jnp.eye(3)\n      + jnp.sin(theta) * W\n      + (1.0 - jnp.cos(theta)) * spin_math.matmul(W, W)\n  )\n  return R"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta_squared = jnp.sum(axis_angle**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n  W = skew(axis_angle / theta)\n  # Note that W is the anti-symmetric matrix of the normalized axis_angle.\n  # If theta = 0, then the rotation matrix is the identity matrix.\n  R = (\n      jnp.eye(3)\n      + jnp.sin(theta) * W\n      + (1.0 - jnp.cos(theta)) * spin_math.matmul(W, W)\n  )\n  # If theta = 0 then the rotation matrix is the identity matrix.\n  R = jnp.where(theta_squared > eps**2, R, jnp.eye(3))\n  return R"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta_squared = jnp.sum(axis_angle**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n  W = skew(axis_angle / theta)\n  # Note that W is skew symmetric so W @ W = -W.\n  # If theta = 0 then the rotation is the identity so the exponential map is\n  # just the identity matrix.\n  R = jnp.eye(3) + jnp.where(theta_squared > eps**2, W, jnp.zeros_like(W)) + jnp.where(\n      theta_squared > eps**2, spin_math.matmul(W, W), jnp.zeros_like(W)\n  )\n  return R"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = _safe_sqrt(jnp.sum(axis_angle**2, axis=-1))\n  W = skew(axis_angle / theta)\n  # Note that theta^2 = 0.0 implies that theta = pi, so theta^2 + eps^2 is\n  # non-zero.\n  return (\n      jnp.eye(3)\n      + jnp.sin(theta) * W\n      + (1.0 - jnp.cos(theta)) * spin_math.matmul(W, W)\n  )"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = _safe_sqrt(jnp.sum(axis_angle**2))\n  W = skew(axis_angle / theta)\n  # Note that theta^2 is close to zero if the axis_angle is close to a zero\n  # vector.\n  W_2 = jnp.where(theta > eps, W**2, jnp.zeros_like(W))\n  theta_2 = theta**2\n  theta_3 = theta**3\n  theta_4 = theta**4\n  theta_5 = theta**5\n  theta_6 = theta**6\n  theta_7 = theta**7\n  theta_8 = theta**8\n  theta_9 = theta**9\n  theta_10 = theta**10\n  theta_11 = theta**11\n\n  R = (\n      jnp.eye(3)\n      + W\n      + W_2\n      * (\n          1.0 / 2.0\n          - theta_2 / 24.0\n          + theta_4 / 720.0\n          - theta_6 / 40320.0\n          + theta_8 / 3628800.0\n          - theta_10 / 479001600.0\n          + theta_11 / 87178291200.0\n      )\n  )\n  return R"}
{"namespace": "rigid_body.exp_so3", "completion": "  # If the axis is not a unit vector, normalize it\n  axis_angle = axis_angle / jnp.linalg.norm(axis_angle)\n\n  # Extract the angle of rotation from the axis-angle representation\n  theta = jnp.linalg.norm(axis_angle)\n\n  # If the angle of rotation is very small, use the first-order Taylor approximation\n  if theta < eps:\n    return jnp.eye(3) + spin_math.cross_product_matrix(axis_angle)\n\n  # Otherwise, use the Rodrigues' formula\n  else:\n    k = axis_angle / theta\n    sin_theta = jnp.sin(theta)\n    cos_theta = jnp.cos(theta)\n    K = spin_math.cross_product_matrix(k)\n\n    return jnp.eye(3) + sin_theta * K + (1 - cos_theta) * K @ K"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = _safe_sqrt(jnp.sum(axis_angle**2, axis=-1))\n  theta_squared = theta**2\n  theta_cubed = theta**3\n  w = axis_angle / theta\n  w_skew = skew(w)\n\n  # Compute the exponential map using Rodrigues' formula\n  R = jnp.eye(3) + jnp.sin(theta) * w_skew + (1 - jnp.cos(theta)) * w_skew @ w_skew\n\n  # If the angle is close to zero, use a first-order approximation\n  R = jnp.where(theta_squared > eps**2, R, jnp.eye(3) + w_skew)\n\n  # If the angle is close to pi, use a second-order approximation\n  R = jnp.where(theta_squared > 10 * eps**2, R, jnp.eye(3) + w_skew + (1 - theta_squared / 6) * w_skew @ w_skew)\n\n  return R"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = _safe_sqrt(jnp.sum(axis_angle**2))\n  theta_squared = theta**2\n  theta_cubed = theta**3\n  theta_half = theta / 2.0\n  theta_half_squared = theta_half**2\n  theta_half_cubed = theta_half**3\n  theta_quarter = theta / 4.0\n  theta_quarter_squared = theta_quarter**2\n  theta_quarter_cubed = theta_quarter**3\n\n  # Compute the rotation matrix.\n  w = axis_angle / theta\n  w_squared = w**2\n  w_cubed = w**3\n  w_cross = skew(w)\n  w_cross_squared = spin_math.matmul(w_cross, w_cross)\n  w_cross_cubed = spin_math.matmul(w_cross_squared, w)\n\n  # Compute the rotation matrix.\n  R = (\n      jnp.eye(3)\n      + w_cross * jnp.sin(theta)\n      + w_cross_squared * (1.0 - jnp.cos(theta))\n  )\n\n  # If the angle is close to zero, use a first order Taylor approximation.\n  R = jnp.where(\n      theta_squared > eps**2,\n      R,\n      jnp.eye(3) + w_cross + w_cross_squared * theta_squared / 2.0,\n  )\n\n  return R"}
{"namespace": "rigid_body.exp_so3", "completion": "  w = axis_angle\n  theta = _safe_sqrt(jnp.sum(w**2))\n  W = skew(w / theta)\n  # If theta is close to 0, use first order Taylor expansion.\n  R = (\n      jnp.eye(3)\n      + jnp.where(theta < eps, 0.5 * W, (1.0 - jnp.cos(theta)) / theta**2 * W**2)\n      + jnp.where(\n          theta < eps, 1.0 / 6.0 * W**3, (theta - jnp.sin(theta)) / theta**3 * W**3\n      )\n  )\n  return R"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta_squared = jnp.sum(axis_angle**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n  W = skew(axis_angle / theta)\n  # Note that W is orthonormal by construction.\n  # The exponential map is defined by:\n  #   exp(w) = I + W * sin(theta) + W @ W * (1 - cos(theta))\n  # We use the fact that:\n  #   sin(x) / x = (1 - cos(x)) / x**2 when x is small.\n  # This avoids the division by zero when theta is close to zero.\n  W_square = spin_math.matmul(W, W)\n  theta_deflated = jnp.where(theta_squared > eps**2, theta, 1.0)\n  W_deflated = jnp.where(theta_squared > eps**2, W, jnp.zeros_like(W))\n  W_square_deflated = jnp.where(theta_squared > eps**2, W_square, jnp.zeros_like(W_square))\n  R = (\n      jnp.eye(3)\n      + W_deflated * jnp.sin(theta_deflated) / theta_deflated\n      + W_square_deflated * (1.0 - jnp.cos(theta_deflated)) / theta_squared\n  )\n  return R"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = _safe_sqrt(jnp.sum(jnp.square(axis_angle), axis=-1))\n  theta_squared = jnp.square(theta)\n  theta_cubed = theta_squared * theta\n  theta_half = theta / 2.0\n  theta_half_squared = jnp.square(theta_half)\n  theta_half_cubed = theta_half_squared * theta_half\n\n  # Compute the rotation matrix for small angles.\n  W = skew(axis_angle / theta)\n  W_squared = jnp.matmul(W, W)\n  W_cubed = jnp.matmul(W, W_squared)\n  R = (\n      jnp.eye(3)\n      + jnp.sin(theta) * W\n      + (1.0 - jnp.cos(theta)) * W_squared\n  )\n\n  # Compute the rotation matrix for small angles using a Taylor expansion.\n  # This avoids numerical instability when the angle is close to zero.\n  R_taylor = (\n      jnp.eye(3)\n      + jnp.where(\n          theta_squared > eps,\n          jnp.sin(theta) / theta,\n          1.0 - theta_squared / 6.0 + theta_cubed / 120.0,\n      )\n      * W\n      + jnp.where(\n          theta_squared > eps,\n          (1.0 - jnp.cos(theta)) / theta_squared,\n          0.5 - theta_half_squared / 24.0 + theta_half_cubed / 720.0,\n      )\n      * W_squared\n  )\n\n  # Choose between the two rotation matrices based on the angle.\n  R = jnp.where(theta_squared > eps, R, R_taylor)\n  return R"}
{"namespace": "rigid_body.exp_so3", "completion": "  # If the axis is not a unit vector, normalize it\n  axis_angle = jnp.where(\n      jnp.linalg.norm(axis_angle) > 1e-6,\n      axis_angle / jnp.linalg.norm(axis_angle),\n      axis_angle,\n  )\n\n  theta = jnp.linalg.norm(axis_angle)\n  w = axis_angle / theta\n  W = skew(w)\n\n  # Use Rodrigues' formula for the exponential map from SO(3) to SO(3)\n  # R = I + W * sin(theta) + W^2 * (1 - cos(theta))\n  # If theta is close to zero, use the first-order Taylor expansion\n  theta_sq = theta * theta\n  theta_po4 = theta_sq * theta_sq\n  theta_po6 = theta_po4 * theta_sq\n  theta_po8 = theta_po4 * theta_po4\n  R = (\n      jnp.eye(3)\n      + W * jnp.sin(theta) / (theta + eps)\n      + spin_math.matmul(W, W) * (1 - jnp.cos(theta)) / (theta_sq + eps)\n  )\n\n  # If theta is close to pi, use a second-order Taylor expansion\n  R = jnp.where(\n      theta_sq > jnp.pi * jnp.pi - eps,\n      R\n      + spin_math.matmul(W, W)\n      * (0.5 - theta_sq / 6.0 + theta_po4 / 120.0 - theta_po6 / 5040.0),\n      R,\n  )\n\n  return R"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= base_radius**2\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= base_radius**2\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= base_radius**2\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= base_radius**2\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= base_radius**2\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= base_radius**2\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= base_radius**2\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= base_radius**2\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= base_radius**2\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= base_radius**2\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= base_radius**2\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= base_radius**2\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= base_radius**2\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= base_radius**2\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= base_radius**2\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= base_radius**2\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= base_radius**2\n  t_mean = t_mean[Ellipsis, None]\n  t_var = t_var[Ellipsis, None]\n  r_var = r_var[Ellipsis, None]\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  means, covs = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  means = means + t_mean[Ellipsis, None] * d[Ellipsis, None, :]\n  return means, covs"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  means, covs = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  means = means + base_radius[Ellipsis, None] * d[Ellipsis, None, :]\n  return means, covs"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= base_radius**2\n  t0_mean, t0_var, r0_var = gaussianize_frustum(t0, t0)\n  t1_mean, t1_var, r1_var = gaussianize_frustum(t1, t1)\n\n  eps = jnp.finfo(jnp.float32).eps\n\n  if diag:\n    # This removes the covariance between the ray origin and t1, which causes\n    # the covariance matrix to no longer be positive semi-definite.\n    # This is okay, because we will never be taking the Cholesky of the\n    # covariance matrix in practice.\n    t_var = t_var - t0_var - t1_var + jnp.maximum(eps, t0_var + t1_var)\n    r_var = r_var - r0_var - r1_var\n\n  means, covs = lift_gaussian(d, t_mean, t_var, r_var, diag)\n\n  means = means + jnp.concatenate(\n      [\n          t0_mean[Ellipsis, None] * d[Ellipsis, None, :],\n          t1_mean[Ellipsis, None] * d[Ellipsis, None, :],\n      ],\n      axis=-2,\n  )\n\n  if diag:\n    cov_xx = t0_var + t1_var + jnp.maximum(eps, t0_var + t1_var)\n    cov_yy = r0_var + r1_var\n    cov_xy = (t0_var * r0_var + t1_var * r1_var) / jnp.maximum(eps, cov_xx)\n    covs = covs + jnp.concatenate(\n        [\n            cov_xy[Ellipsis, None],\n            cov_xy[Ellipsis, None],\n        ],"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean = (t0 + t1) / 2\n  r_var = radius**2 / 4\n  t_var = (t1 - t0)**2 / 12\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean = (t0 + t1) / 2\n  r_var = radius**2 / 4\n  t_var = (t1 - t0)**2 / 12\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean = (t0 + t1) / 2\n  r_var = radius**2 / 4\n  t_var = (t1 - t0)**2 / 12\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean = (t0 + t1) / 2\n  t_var = (t1 - t0)**2 / 12\n  r_var = radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean = (t0 + t1) / 2\n  t_var = (t1 - t0)**2 / 12\n  r_var = radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean = (t0 + t1) / 2\n  t_var = (t1 - t0)**2 / 12\n  r_var = radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean = (t0 + t1) / 2\n  r_var = radius**2 / 4\n  t_var = (t1 - t0)**2 / 12\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean = (t0 + t1) / 2\n  t_var = (t1 - t0)**2 / 12\n  r_var = radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean = (t0 + t1) / 2\n  t_var = (t1 - t0)**2 / 12\n  r_var = radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean = (t0 + t1) / 2\n  t_var = (t1 - t0)**2 / 12\n  r_var = radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean = (t0 + t1) / 2\n  r_var = radius**2 / 4\n  t_var = (t1 - t0)**2 / 12\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean = (t0 + t1) / 2\n  t_var = (t1 - t0)**2 / 12\n  r_var = radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean = (t0 + t1) / 2\n  t_var = (t1 - t0)**2 / 12\n  r_var = radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean = (t0 + t1) / 2\n  r_var = radius**2 / 4\n  t_var = (t1 - t0)**2 / 12\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean = (t0 + t1) / 2\n  t_var = (t1 - t0)**2 / 12\n  r_var = radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean = (t0 + t1) / 2\n  r_var = radius**2 / 4\n  t_var = (t1 - t0)**2 / 12\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean = (t0 + t1) / 2\n  t_var = (t1 - t0)**2 / 12\n  r_var = radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean = (t0 + t1) / 2\n  r_var = radius**2 / 4\n  t_var = (t1 - t0)**2 / 12\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean = (t0 + t1) / 2\n  r_var = radius**2 / 4\n  t_var = (t1 - t0)**2 / 12\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean = (t0 + t1) / 2\n  r_var = radius**2 / 4\n  t_var = (t1 - t0)**2 / 12\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # OpenGL to OpenCV coordinates.\n  pix_x_int = xnp.where(\n      xnp.isnan(pix_x_int),\n      xnp.zeros_like(pix_x_int),\n      pix_x_int,\n  )\n  pix_y_int = xnp.where(\n      xnp.isnan(pix_y_int),\n      xnp.zeros_like(pix_y_int),\n      pix_y_int,\n  )\n  pix_x_int = xnp.where(\n      xnp.isinf(pix_x_int),\n      xnp.zeros_like(pix_x_int),\n      pix_x_int,\n  )\n  pix_y_int = xnp.where(\n      xnp.isinf(pix_y_int),\n      xnp.zeros_like(pix_y_int),\n      pix_y_int,\n  )\n  pix_x_int = xnp.where(\n      xnp.isneginf(pix_x_int),\n      xnp.zeros_like(pix_x_int),\n      pix_x_int,\n  )\n  pix_y_int = xnp.where(\n      xnp.isneginf(pix_y_int),\n      xnp.zeros_like(pix_y_int),\n      pix_y_int,\n  )\n\n  pix_x = pix_x_int + 0.5\n  pix_y = pix_y_int + 0.5\n\n  # OpenGL to OpenCV coordinates.\n  pix_x = xnp.where(\n      xnp.isnan(pix_x),"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Undo OpenGL to OpenCV coordinate system convention.\n  pix_x_int, pix_y_int = xnp.stack([pix_x_int, pix_y_int], axis=-1)\n  pix_x_int, pix_y_int = xnp.moveaxis(pix_x_int, -1, 0), xnp.moveaxis(\n      pix_y_int, -1, 0\n  )\n  pixel_dirs = xnp.stack([pix_x_int, pix_y_int, xnp.ones_like(pix_x_int)], -1)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        pixel_dirs[Ellipsis, 0],\n        pixel_dirs[Ellipsis, 1],\n        **distortion_params,\n    )\n    pixel_dirs = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  # Apply intrinsics matrix.\n  pixel_dirs = mat_vec_mul(pixtocams, pixel_dirs)\n\n  # Add half pixel offset.\n  pixel_dirs = pixel_dirs + xnp.array([0.5, 0.5, 0.0])\n\n  # OpenGL to OpenCV coordinates.\n  pixel_dirs = matmul(pixel_dirs, xnp.diag(xnp.array([1.0, -1.0, -1.0])))\n\n  # Apply camera rotation.\n  rotation = camtoworlds[Ellipsis, :3, :3]"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Shape checking.\n  if pixtocams.shape[-2:] != (3, 3):\n    raise ValueError(\n        f'pixtocams must have shape [..., 3, 3], not {pixtocams.shape}.'\n    )\n  if camtoworlds.shape[-2:] != (3, 4):\n    raise ValueError(\n        f'camtoworlds must have shape [..., 3, 4], not {camtoworlds.shape}.'\n    )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Convert pixel coordinates to NDC space.\n  if pixtocam_ndc is not None:\n    pix_x_int = xnp.array(pix_x_int)\n    pix_y_int = xnp.array(pix_y_int)\n    if pixtocam_ndc.shape != (3, 3):\n      raise ValueError(\n          f'pixtocam_ndc must have shape [3, 3], not {pixtocam_ndc.shape}.'\n      )\n    pix_x_int, pix_y_int = mat_vec_mul(\n        pixtocam_ndc, xnp.stack([pix_x_int, pix_y_int, xnp.ones_like(pix_x_int)], -1)\n    )[Ellipsis, :2]\n\n  # OpenGL to OpenCV coordinates.\n  pix_x_int = xnp.array(pix_x_int)\n  pix_y_int = xnp.array(pix_y_int)\n  pix_x_int, pix_y_int = mat_vec_mul(\n      xnp.diag(xnp.array(["}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # OpenGL to OpenCV coordinates.\n  pix_x_int = xnp.where(\n      camtype == ProjectionType.PERSPECTIVE,\n      pix_x_int,\n      pix_x_int,\n  )\n  pix_y_int = xnp.where(\n      camtype == ProjectionType.PERSPECTIVE,\n      pix_y_int,\n      height - pix_y_int - 1,\n  )\n\n  # OpenGL to OpenCV coordinates.\n  pix_x_int = xnp.where(\n      camtype == ProjectionType.FISHEYE,\n      pix_x_int,\n      pix_x_int,\n  )\n  pix_y_int = xnp.where(\n      camtype == ProjectionType.FISHEYE,\n      pix_y_int,\n      height - pix_y_int - 1,\n  )\n\n  # OpenGL to OpenCV coordinates.\n  pix_x_int = xnp.where(\n      camtype == ProjectionType.PANORAMIC,\n      pix_x_int,\n      pix_x_int,\n  )\n  pix_y_int = xnp.where(\n      camtype == ProjectionType.PANORAMIC,\n      pix_y_int,\n      height - pix_y_int - 1,\n  )\n\n  # Convert from integer pixel coordinates to OpenGL camera coordinates.\n  z_inv = 1.0 / pixtocams[Ellipsis, -1, -1]\n  x = z_inv * (pix_x_int - pixtocams[Ellipsis, 0, -1])\n  y = z_inv * (pix_y_int - pi"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # OpenGL to OpenCV coordinates.\n  pix_x_int, pix_y_int = xnp.array(pix_x_int), xnp.array(pix_y_int)\n  pix_x_int = xnp.where(xnp.isnan(pix_x_int), 0, pix_x_int)\n  pix_y_int = xnp.where(xnp.isnan(pix_y_int), 0, pix_y_int)\n  pix_x_int = xnp.clip(pix_x_int, 0, xnp.array(pixtocams.shape[1]) - 1)\n  pix_y_int = xnp.clip(pix_y_int, 0, xnp.array(pixtocams.shape[0]) - 1)\n  pix_x_int = xnp.where(\n      xnp.isnan(pix_x_int), 0, xnp.where(pix_x_int < 0, 0, pix_x_int)\n  )\n  pix_y_int = xnp.where(\n      xnp.isnan(pix_y_int), 0, xnp.where(pix_y_int < 0, 0, pix_y_int)\n  )\n  pix_x_int = xnp.where(\n      xnp.isinf(pix_x_int),\n      xnp.array(pixtocams.shape[1]) - 1,\n      xnp.where(pix_x_int > xnp.array(pixtocams.shape[1]) - 1, 0, pix_x_int),\n  )\n  pix_y_int = xnp.where"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if camtype != ProjectionType.PERSPECTIVE:\n    raise ValueError(f'pixels_to_rays only supports perspective projection, '\n                     f'not {camtype} mode.')\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  rotation = camtoworlds[Ellipsis, :3, :3]\n  rotation_inv = xnp.swapaxes(rotation, -1, -2)\n  translation = camtoworlds[Ellipsis, :3, -1]\n\n  # Get the ray origin in camera space.\n  origins = -mat_vec_mul(rotation_inv, translation)\n\n  # Get the ray directions in camera space.\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Get the ray directions in camera space.\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Get the ray directions in camera space.\n  pix_ndc = xnp.stack([pix_x_int, pix_y_int, xnp.ones_like(pix_x_int)], axis=-1)\n  pix_cam = mat_vec_mul(pixtocams, pix_ndc)\n\n  if distortion_"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if camtype not in (ProjectionType.PERSPECTIVE, ProjectionType.FISHEYE):\n    raise ValueError(f'pixels_to_rays only supports perspective and fisheye '\n                     f'projection, not {camtype} mode.')\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Convert pixel coordinates to NDC space ([0, 1]) then to camera coordinates.\n  if pixtocam_ndc is not None:\n    pix_x_int = xnp.clip(pix_x_int, 0, width - 1)\n    pix_y_int = xnp.clip(pix_y_int, 0, height - 1)\n    pix_x_ndc = pix_x_int / width\n    pix_y_ndc = pix_y_int / height\n    pix_ndc = xnp.stack([pix_x_ndc, pix_y_ndc, xnp.ones_like(pix_x_int)], -1)\n    pix_camera = mat_vec_mul(pixtocam_ndc, pix_ndc)\n  else:\n    pix_camera = xnp.stack(\n        [pix_x_int - xnp.array(width) * 0.5, xnp.array(height) * 0.5 - pix_y_int,\n         xnp.ones_like(pix_x_int)],\n        -1,\n    )\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    pix_camera[Ellipsis, 0], pix_camera[Ellipsis, 1] = _radial_and_tangential_undistort(\n        pix_camera[Ellips"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Get the camera coordinates from pixel coordinates.\n  if camtype == ProjectionType.PANORAMIC:\n    # In the case of a panoramic camera, the pixel coordinates are already in\n    # camera coordinates.\n    camera_dirs = xnp.stack(\n        [\n            xnp.sin(pix_x_int / width * 2 * xnp.pi),\n            xnp.cos(pix_x_int / width * 2 * xnp.pi),\n            xnp.ones_like(pix_x_int),\n        ],\n        -1,\n    )\n  else:\n    # Convert pixel coordinates to camera frame coordinates.\n    x = (pix_x_int - xnp.array(width) * 0.5) / xnp.array(width)\n    y = (pix_y_int - xnp.array(height) * 0.5) / xnp.array(height)\n    camera_dirs = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n    if distortion_params is not None:\n      # Correct for distortion.\n      x, y = _radial_and_tangential_undistort(\n          camera_dirs[Ellipsis, 0],\n          camera_dirs[Ellipsis, 1],\n          **distortion_params,\n      )\n      camera_dirs = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n    # OpenGL convention.\n    camera_dirs = matmul(camera_dirs, xnp.diag(xnp.array([1.0, -1.0, -1.0])))\n\n  #"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if camtype == ProjectionType.PANORAMIC:\n    # OpenGL to OpenCV coordinates.\n    pixtocams = xnp.diag(xnp.array([1.0, -1.0, -1.0])) @ pixtocams\n\n  # OpenGL to OpenCV coordinates.\n  camtoworlds = xnp.diag(xnp.array([1.0, -1.0, -1.0])) @ camtoworlds\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Compute ray directions.\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        pix_x_int,\n        pix_y_int,\n        **distortion_params,\n    )\n    camera_dirs = xnp.stack([x, y, xnp.ones_like(x)], -1)\n  else:\n    camera_dirs = xnp.stack([pix_x_int, pix_y_int, xnp.ones_like(pix_x_int)], -1)\n\n  # Apply intrinsics matrix.\n  pixel_dirs = mat_vec_mul(xnp.linalg.inv(pixtocams), camera_dirs)\n\n  # Apply camera extrinsics to ray directions.\n  rotation = camtoworlds[Ellipsis, :3, :3]\n  rotation_inv = xnp.swapaxes(rotation, -1, -2)\n  translation = camtoworlds[Ellipsis, :3, -1]\n  directions = mat_vec_mul(rotation_inv, pixel_dirs)\n\n  # Compute ray origins.\n  origins ="}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  if camtype != ProjectionType.PERSPECTIVE:\n    raise ValueError(f'pixels_to_rays only supports perspective projection, '\n                     f'not {camtype} mode.')\n\n  # OpenGL to OpenCV coordinates.\n  pix_x_int = xnp.where(\n      camtype == ProjectionType.PANORAMIC,\n      pix_x_int,\n      xnp.flip(pix_x_int, axis=-1),\n  )\n\n  # Compute 3D point on image plane.\n  x_cam_int = pix_x_int\n  y_cam_int = pix_y_int\n  z_cam_int = xnp.ones_like(x_cam_int)\n  xyz_cam_int = xnp.stack([x_cam_int, y_cam_int, z_cam_int], axis=-1)\n\n  # Correct for distortion.\n  if distortion_params is not None:\n    x, y = _radial_and_tangential_undistort(\n        xyz_cam_int[Ellipsis, 0],\n        xyz_cam_int[Ellipsis, 1],\n        **distortion_params,\n    )\n    xyz_cam_int = xnp.stack([x, y, z_cam_int], axis=-1)\n\n  # Convert from OpenCV to OpenGL coordinates.\n  xyz_cam_int = xnp.where(\n      camtype == ProjectionType.PANORAMIC,\n      xyz_cam_int,\n      xnp.flip(xyz_cam_int, axis=-1),\n  )\n\n  # Comput"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if camtype == ProjectionType.PERSPECTIVE:\n    # OpenGL to OpenCV camera coordinates.\n    xnp = jnp if xnp == jnp else np\n    camtoworlds = xnp.copy(camtoworlds)\n    camtoworlds[Ellipsis, :3, 1:3] *= -1\n\n  # OpenGL to OpenCV camera coordinates.\n  camtoworlds = xnp.copy(camtoworlds)\n  camtoworlds[Ellipsis, :3, 1:3] *= -1\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Convert pixel coordinates to NDC.\n  if pixtocam_ndc is not None:\n    # OpenGL to OpenCV camera coordinates.\n    pix_x_int = xnp.copy(pix_x_int)\n    pix_y_int = xnp.copy(pix_y_int)\n    pix_x_int, pix_y_int = pix_x_int, xnp.maximum(0, pix_y_int)\n\n    # Normalized device coordinates in the range [-1, 1].\n    x = (2 * pix_x_int - pixtocam_ndc[0, 2]) / pixtocam_ndc[0, 0]\n    y = (2 * pix_y_int - pixtocam_ndc[1, 2]) / pixtocam_ndc[1, 1]\n\n    # Points in front of the camera satisfy:\n    #   z = - f * x / (z * x) = - f / (z + x * f)\n    # We assume the scene is bounded in z, so we restrict z > 0.\n    z = xnp.where(\n        xnp.abs("}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if camtype not in {ProjectionType.PERSPECTIVE, ProjectionType.FISHEYE}:\n    raise ValueError(\n        f'pixels_to_rays only supports perspective and fisheye projections, '\n        f'not {camtype} mode.'\n    )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Rotation matrix.\n  rotation = camtoworlds[Ellipsis, :3, :3]\n  rotation_inv = xnp.swapaxes(rotation, -1, -2)\n  translation = camtoworlds[Ellipsis, :3, -1]\n\n  # Convert pixel coordinates to NDC space.\n  if pixtocam_ndc is not None:\n    pix_x_int = pix_x_int - pixtocam_ndc[0, 2]\n    pix_y_int = pix_y_int - pixtocam_ndc[1, 2]\n    pix_x_int, pix_y_int = pix_x_int / pixtocam_ndc[0, 0], pix_y_int / pixtocam_ndc[1, 1]\n\n  # Compute the ray directions in camera space.\n  x, y = pix_x_int, pix_y_int\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        x,\n        y,\n        **distortion_params,\n    )\n\n  # OpenGL convention.\n  x, y = xnp.asarray(x), xnp.asarray(y)\n  x, y = xnp.where(xnp.isnan(x), 0.0, x), x"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if camtype not in (ProjectionType.PERSPECTIVE, ProjectionType.FISHEYE):\n    raise ValueError(f'pixels_to_rays only supports perspective and fisheye '\n                     f'projection, not {camtype} mode.')\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Convert pixel coordinates to NDC space.\n  if pixtocam_ndc is not None:\n    pix_x_int, pix_y_int = mat_vec_mul(pixtocam_ndc, xnp.stack([\n        pix_x_int,\n        pix_y_int,\n        xnp.ones_like(pix_x_int),\n    ], -1)).T\n\n  # Convert from OpenCV camera coordinates to OpenGL camera coordinates.\n  # x will remain the same.\n  # y will get inverted and will now represent down direction.\n  # z will get inverted and will now represent into the image plane.\n  # This only effects rays, imageplane, and directions.\n  pix_y_int = xnp.where(\n      camtype == ProjectionType.PERSPECTIVE,\n      -pix_y_int,\n      pix_y_int,\n  )\n  pix_z_int = xnp.where(\n      camtype == ProjectionType.PERSPECTIVE,\n      -xnp.ones_like(pix_x_int),\n      xnp.ones_like(pix_x_int),\n  )\n\n  # Pixel indices to camera coordinates.\n  pix_cam_int = xnp.stack([pix_x_int, pix_y_int, pix_z_int], -1)\n\n  # Correct principal point offset @ scale.\n  cam_origins = -mat_vec_m"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Get pixel coordinates in camera frame.\n  x, y = pix_x_int[Ellipsis, None], pix_y_int[Ellipsis, None]\n  if pixtocam_ndc is not None:\n    # Convert from OpenGL/Blender NDC to OpenCV pixel coordinates.\n    x, y = x * 2.0 - 1.0, y * 2.0 - 1.0\n    x, y = x * pixtocam_ndc[0, 0], y * pixtocam_ndc[1, 1]\n  x, y = x - pixtocams[Ellipsis, 2, None], y - pixtocams[Ellipsis, 2, None]\n  x, y = x / pixtocams[Ellipsis, 0, None], y / pixtocams[Ellipsis, 1, None]\n\n  if camtype == ProjectionType.FISHEYE:\n    r = xnp.sqrt(x**2 + y**2)\n    theta = r * xnp.arctan(r)\n    x_cam = theta * x / r\n    y_cam = theta * y / r\n  elif camtype == ProjectionType.PANORAMIC:\n    theta = xnp.arctan2(x, y)\n    r = xnp.sqrt(x**2 + y**2)\n    x_cam = theta * r\n    y_cam = xnp.zeros_like(r)\n  else:\n    x_cam, y_cam = x, y\n\n  if distortion_params is not None:\n    # Undistort the pixel coordinates.\n    x_cam, y_cam ="}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if camtype == ProjectionType.FISHEYE:\n    # This is a special case, because the distortion parameters are different.\n    # TODO(keunhong): add support for fisheye distortion.\n    raise NotImplementedError('Fisheye distortion not yet implemented.')\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Compute pixel center coordinates.\n  pix_x = pix_x_int + 0.5\n  pix_y = pix_y_int + 0.5\n\n  # Compute pixel camera frame coordinates.\n  if pixtocam_ndc is None:\n    # Normalized coordinates ([0, 1]) if not otherwise specified.\n    x = (pix_x - pixtocams[Ellipsis, 0, 2]) / pixtocams[Ellipsis, 0, 0]\n    y = (pix_y - pixtocams[Ellipsis, 1, 2]) / pixtocams[Ellipsis, 1, 1]\n  else:\n    x = (pix_x - pixtocam_ndc[0, 2]) / pixtocam_ndc[0, 0]\n    y = (pix_y - pixtocam_ndc[1, 2]) / pixtocam_ndc[1, 1]\n\n  x = x[Ellipsis, None]\n  y = y[Ellipsis, None]\n\n  # OpenGL convention\n  # Note that up is -y, so right is +x.\n  dirs = xnp.concatenate([x, -y, xnp.ones_like(x)], -1)\n\n  if distortion_params is not None:\n    # Correct for distortion"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to NDC.\n  if pixtocam_ndc is not None:\n    # Convert from image plane to NDC.\n    x_cam, y_cam, _ = matmul(pixtocam_ndc, xnp.stack([pix_x_int, pix_y_int, xnp.ones_like(pix_x_int)], axis=-1))\n    x_cam, y_cam = x_cam[Ellipsis, 0], y_cam[Ellipsis, 0]\n  else:\n    # Assume a unit image plane in [-1, 1] x [-1, 1].\n    x_cam, y_cam = xnp.array(pix_x_int) / (xnp.array(width) / 2.0) - 1.0, xnp.array(pix_y_int) / (xnp.array(height) / 2.0) - 1.0\n\n  if camtype == ProjectionType.PANORAMIC:\n    # Normalize to [-1, 1] x [-1, 1] x [-1, 1] cube.\n    x_cam = x_cam / xnp.maximum(xnp.abs(x_cam), 1.0)\n    y_cam = y_cam / xnp.maximum(xnp.abs(y_cam), 1.0)\n    z_cam = xnp.sqrt(xnp.maximum(1.0 - x_cam**2 - y_cam**2, 0.0))\n    camera_dirs = xnp.stack([x_cam, y_cam, z_cam], axis=-1)\n  elif camtype == ProjectionType.FISHEYE:\n    r = xnp.sqrt(x_cam**2 + y_cam**2)\n    theta = r * (1.0 + xnp.sqrt(5.0)) / 2.0\n    camera_dirs = xnp.stack([\n        xnp.sin(theta) * x_cam / r,\n        x"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if camtype == ProjectionType.FISHEYE:\n    # This is a special case for fisheye camera.\n    # The camera frame is defined as:\n    #   z: forward\n    #   y: up\n    #   x: right\n    # The projection is defined as r = f * tan(theta) / norm(p)\n    # where p is the 3D point in the camera frame, and theta is the angle\n    # between the ray and the optical axis.\n    #\n    # The projection is equivalent to the pinhole projection with the\n    # following intrinsics matrix:\n    # K = [f  0  cx]\n    #     [ 0 f  cy]\n    #     [ 0 0   1]\n    #\n    # This is a pinhole camera with the following extrinsics matrix:\n    # E = [ 1  0  0  0]\n    #     [ 0  1  0  0]\n    #     [ 0  0  1  0]\n    #\n    # The projection is equivalent to the pinhole projection with the\n    # following intrinsics matrix:\n    # K = [f  0  cx]\n    #     [ 0 f  cy]\n    #     [ 0 0   1]\n    #\n    # This is a pinhole camera with the following extrinsics matrix:\n    # E = [ 1  0  0  0]\n    #     [ 0  1  0  0]\n    #     [ 0  0  1  0]\n    #\n    # The projection is equivalent to the pinhole projection with the\n    # following intrinsics matrix:\n    # K = [f  0  cx]\n    #     [ 0 f  cy]\n    #     [ 0 0   1]\n    #\n    # This is a pinhole camera with the following extrinsics matrix:\n    # E = [ 1  0  0  0]\n    #     [ 0  1  0  0]\n    #     [ 0  0  1  0]\n    #\n    # The projection is"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert to float32 because we assume the inputs are float32 (e.g. mesh)\n  pix_x_int = pix_x_int.astype(xnp.float32)\n  pix_y_int = pix_y_int.astype(xnp.float32)\n\n  # OpenGL convention:\n  # +y is up\n  # +x is left\n  # +z is behind us\n  # See https://www.khronos.org/registry/OpenGL-Refpages/gl4/html/glReadPixels.xhtml\n  #\n  # NeRF convention:\n  # +y is up\n  # +x is right\n  # +z is in front of us\n  # See https://github.com/bmild/nerf/issues/24#issuecomment-1030282039\n  #\n  # So we do a change of basis\n  #\n  # OpenGL => NeRF by a rotation by 90 degrees around the x axis\n  #\n  # This is a change of basis, so the inverse is its transpose\n  #\n  # OpenGL => NeRF by a rotation by 90 degrees around the x axis\n  # NeRF => OpenGL by a rotation by -90 degrees around the x axis\n  #\n  # [1  0  0]\n  # [0 -1  0]\n  # [0  0 -1]\n  #\n  # [ 1  0  0]   [1  0  0]   [1  0  0]\n  # [ 0 -1  0] * [-1  0  0] = [0  1  0]\n  # [ 0  0 -1]   [ 0 -1  0]   [0  0  1]\n  #\n  # [ 1  0  0]   [1  0  0]   [1  0  0]\n  # [ 0  1  0] * [ 0 -1  0] = [0 -1  0]\n  # [ 0  0  1]   [ 0  0 -1"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        pix_x_int, pix_y_int, **distortion_params\n    )\n  else:\n    # No distortion, so we can use the raw pixel coordinates.\n    x = pix_x_int\n    y = pix_y_int\n\n  # OpenGL convention is to have the image plane at z=-1.\n  if camtype == ProjectionType.PERSPECTIVE:\n    # In OpenGL convention, the image plane is at z=-1.\n    z = xnp.ones_like(x)\n  elif camtype == ProjectionType.FISHEYE:\n    # In OpenGL convention, the image plane is at z=-1.\n    r = xnp.sqrt(x * x + y * y)\n    theta = xnp.arctan2(r, xnp.ones_like(x))\n    z = xnp.cos(theta)\n  elif camtype == ProjectionType.PANORAMIC:\n    # In OpenGL convention, the image plane is at z=-1.\n    r = xnp.sqrt(x * x + y * y)\n    theta = xnp.arctan2(r, xnp.ones_like(x))\n    z = xnp.sin(theta)\n  else:\n    raise ValueError(f'Unknown camera type: {camtype}')\n\n  # OpenGL convention is to have the camera at z=0.\n  # In OpenGL convention, the image plane is at z=-1.\n  # Therefore, we need to negate the z coordinate.\n  z = -z\n\n  # OpenGL convention is to have the camera at z=0.\n  # In OpenGL convention, the image plane is at z=-1.\n  # Therefore, we need to negate the z coordinate.\n  # Also, we need to negate the y coordinate.\n  # This is because OpenGL convention is to have the y-axis point down.\n  # In Open"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if camtype == ProjectionType.PANORAMIC:\n    # For panoramas, the rays are just the unit vectors of the pixel coordinates.\n    directions = xnp.stack(\n        [\n            xnp.sin(pix_x_int / width * 2 * xnp.pi) * xnp.cos(\n                pix_y_int / height * xnp.pi\n            ),\n            xnp.sin(pix_y_int / height * xnp.pi),\n            -xnp.cos(pix_x_int / width * 2 * xnp.pi) * xnp.cos(\n                pix_y_int / height * xnp.pi\n            ),\n        ],\n        -1,\n    )\n    origins = xnp.zeros_like(directions)\n  else:\n    # The directions are the pixel locations projected onto the camera plane.\n    if distortion_params is not None:\n      # Correct for distortion.\n      x, y = _radial_and_tangential_undistort(\n          pix_x_int, pix_y_int, **distortion_params\n      )\n    else:\n      x, y = pix_x_int, pix_y_int\n\n    # OpenGL convention is y is down, and +y is to the *left* of the camera.\n    # Flip the sign and swap the axes to obey the camera frame convention.\n    x, y = -x, -y\n    x, y = y, x\n\n    # Convert to camera coordinates by multiplying with the inverse intrinsic.\n    # This assumes that the pixels are already scaled to the view plane size.\n    if pixtocam_ndc is not None:\n      x, y = xnp.concatenate([x[Ellipsis, None], y[Ellipsis, None]], -1) @ pixtocam_ndc.T\n    else:\n      x, y = xnp.concatenate([x[Ellipsis, None], y[Ellipsis, None]], -1) @ pixtocams.T"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * jnp.linalg.norm(dirs[Ellipsis, None, :], axis=-1)\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density[Ellipsis, 1:] * jnp.linalg.norm(dirs[Ellipsis, :-1], axis=-1)\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density[Ellipsis, 1:] * jnp.linalg.norm(dirs[Ellipsis, :-1], axis=-1)\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * jnp.linalg.norm(dirs[Ellipsis, None, :], axis=-1)\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density[Ellipsis, 1:] * jnp.linalg.norm(dirs[Ellipsis, :-1], axis=-1)\n  return compute_alpha_weights_helper(density_delta)"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density[Ellipsis, 1:] * jnp.linalg.norm(dirs[Ellipsis, :-1], axis=-1)\n  return compute_alpha_weights_helper(density_delta)"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * (tdist[Ellipsis, 1:] - tdist[Ellipsis, :-1])\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * (tdist[Ellipsis, 1:] - tdist[Ellipsis, :-1])\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density[Ellipsis, 1:] * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density[Ellipsis, 1:] * jnp.linalg.norm(dirs[Ellipsis, :-1], axis=-1)\n  return compute_alpha_weights_helper(density_delta)"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Compute the product of density and the adjusted distance between points.\n  density_delta = density[Ellipsis, 1:] * jnp.linalg.norm(dirs[Ellipsis, :-1], axis=-1)\n\n  # Call the helper function to compute the alpha weights.\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density[Ellipsis, 1:] * jnp.linalg.norm(dirs[Ellipsis, 1:], axis=-1)\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density[Ellipsis, 1:] * jnp.linalg.norm(dirs[Ellipsis, :-1], axis=-1)\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density[Ellipsis, 1:] * (tdist[Ellipsis, 1:] - tdist[Ellipsis, :-1])\n  density_delta = density_delta * jnp.linalg.norm(dirs[Ellipsis, None, :], axis=-1)\n\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * (tdist[Ellipsis, 1:] - tdist[Ellipsis, :-1])\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * (tdist[Ellipsis, 1:] - tdist[Ellipsis, :-1])\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density[Ellipsis, 1:] * jnp.linalg.norm(dirs[Ellipsis, 1:], axis=-1)\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * math.l2_norm(dirs[Ellipsis, 1:, :], axis=-1)\n  return compute_alpha_weights_helper(density_delta)"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * math.l2_norm(dirs[Ellipsis, 1:, :], axis=-1)\n  return compute_alpha_weights_helper(density_delta)"}
{"namespace": "render.compute_alpha_weights", "completion": "  t_mids = 0.5 * (tdist[Ellipsis, :-1] + tdist[Ellipsis, 1:])\n  # The distance between the midpoints of adjacent rays.\n  delta = t_mids[Ellipsis, 1:] - t_mids[Ellipsis, :-1]\n\n  # The norm of each ray segment.\n  ray_d_mag = jnp.linalg.norm(dirs[Ellipsis, :-1], axis=-1)\n\n  # The normalized dot product between adjacent ray segments.\n  dot_prod = (dirs[Ellipsis, 1:] * dirs[Ellipsis, :-1]).sum(-1)\n  ray_d_mag = ray_d_mag * ray_d_mag\n  # Remove the dot product from the denominator if it's smaller than eps.\n  denom = jnp.maximum(eps, ray_d_mag * ray_d_mag - 1)\n  # This is the normalized dot product masked by the threshold.\n  ndp_masked = jnp.where(ray_d_mag < eps, jnp.zeros_like(dot_prod), dot_prod / denom)\n  # This is the normalized distance.\n  normalized_dist = delta * jnp.sqrt(2 - ndp_masked)\n\n  # This is the normalized density change between adjacent points.\n  delta_density = density[Ellipsis, 1:] - density[Ellipsis, :-1]\n\n  # This is the density change scaled by the norm of the adjacent ray segments.\n  density_delta = delta_density * ray_d_mag\n\n  # This is the density change adjusted by the change in distance.\n  density_delta_corrected = density_delta * normalized_dist\n\n  # This is the final density change to use for computing the weights.\n  density_delta_final = jnp.concatenate(\n      [density_delta_corrected[Ellipsis, :1], density_delta_corrected], axis=-1\n "}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample a set of points from the step function.\n  if rng is None:\n    # Deterministic sampling.\n    if deterministic_center:\n      # Sample from the center of each interval.\n      u = jnp.linspace(eps, 1 - eps, num_samples)\n    else:\n      # Sample from the entire PDF.\n      u = jnp.linspace(0, 1 - eps, num_samples)\n    t_samples = invert_cdf(u, t, w_logits)\n  else:\n    # Random sampling.\n    if single_jitter:\n      # Jitter every sample along each ray by the same amount.\n      u = jax.random.uniform(rng, shape=(num_samples,))\n    else:\n      # Jitter each sample independently.\n      u = jax.random.uniform(rng, shape=(num_samples,))\n    t_samples = invert_cdf(u, t, w_logits)\n\n  return t_samples"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 1:\n    raise ValueError(f'num_samples must be > 1, is {num_samples}.')\n\n  # Compute the CDF and normalize it.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n  cw0 = jnp.concatenate([jnp.zeros_like(cw[Ellipsis, :1]), cw], axis=-1)\n  cw1 = jnp.concatenate([cw, jnp.ones_like(cw[Ellipsis, :1])], axis=-1)\n\n  # Draw uniform samples.\n  if rng is None:\n    # Match the behavior of jax.random.uniform() by using \"float\" sampling.\n    shape = t.shape[:-1] + (num_samples,)\n    u = jnp.linspace(0.0, 1.0, num_samples)\n    u = jnp.broadcast_to(u, shape)\n  else:\n    # When a PRNGKey is passed in, draw uniform samples.\n    if single_jitter:\n      shape = t.shape[:-1] + (num_samples,)\n      u = jax.random.uniform(rng, shape)\n    else:\n      shape = t.shape[:-1] + (num_samples, 1)\n      u = jax.random.uniform(rng, shape)\n\n  # Invert the CDF.\n  if not deterministic_center:\n    t_new = math.sorted_interp(u, cw0, t, utils.device_is_tpu())\n  else:\n    # Figure out where each `u` lies in `cw`.\n    # Grab the index above; if we are at the left edge, choose the first bin.\n    cw_idx = jnp.clip(jnp.searchsorted(cw0, u"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 1:\n    raise ValueError(f'num_samples must be > 1, is {num_samples}.')\n\n  # Compute the CDF and sample uniformly from it.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n  if rng is None:\n    # Deterministic sampling.\n    if deterministic_center:\n      # Sample around the center of the CDF.\n      u = jnp.linspace(0.5 / num_samples, 1 - 0.5 / num_samples, num_samples)\n    else:\n      # Sample over the entire CDF.\n      u = jnp.linspace(0, 1 - 1 / num_samples, num_samples)\n  else:\n    # Random sampling.\n    if single_jitter:\n      # Apply the same jitter to every sample.\n      u = jax.random.uniform(rng, [num_samples], minval=0, maxval=1)\n    else:\n      # Apply a different jitter to every sample.\n      u = jax.random.uniform(rng, [num_samples], minval=0, maxval=1 / num_samples)\n  u = jnp.clip(u, eps, 1 - eps)\n  t_new = invert_cdf(u, t, w_logits)\n  return t_new"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 1:\n    raise ValueError(f'num_samples must be > 1, is {num_samples}.')\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Pad CDFs to make them work for testing t = 0 and t = 1 weights.\n  cw0 = jnp.concatenate([jnp.zeros(cw.shape[:-1] + (1,)), cw, jnp.ones(cw.shape[:-1] + (1,))], axis=-1)\n  t0 = jnp.concatenate([t[Ellipsis, :1] - eps, t, t[Ellipsis, -1:] + eps], axis=-1)\n\n  if rng is None:\n    # Deterministic center sampling.\n    if deterministic_center:\n      u = jnp.linspace(0.0 + eps, 1.0 - eps, num_samples)\n      u = jnp.broadcast_to(u, t.shape[:-1] + (num_samples,))\n      t_new = math.sorted_interp(u, cw0, t0, utils.device_is_tpu())\n    # Deterministic edge sampling.\n    else:\n      u = jnp.linspace(0.0, 1.0, num_samples + 2)[1:-1]\n      u = jnp.broadcast_to(u, t.shape[:-1] + (num_samples,))\n      t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n  else:\n    # Random sampling, with single or independent jitter per ray.\n    if single_jitter:\n      shape = t.shape[:-1] + (num_s"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 1:\n    raise ValueError(f'num_samples must be > 1, is {num_samples}.')\n\n  # Compute the CDF and normalize it.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Draw uniform samples.\n  if rng is None:\n    # Match the behavior of jax.random.uniform() by using linspace().\n    shape = cw.shape[:-1] + (num_samples,)\n    u = jnp.linspace(0.0, 1.0 - eps, num_samples)\n    u = jnp.broadcast_to(u, shape)\n  else:\n    # Draw uniform samples.\n    shape = cw.shape[:-1] + (num_samples,)\n    u = jax.random.uniform(rng, shape)\n\n  if single_jitter:\n    # If `single_jitter` is True, apply the same perturbation to all samples.\n    # This avoids the issue of 'staircasing', i.e. always perturbing by the same\n    # amount in the same direction.\n    u += jax.random.uniform(rng, cw.shape[:-1], minval=0, maxval=1e-7)\n    u %= 1.0 - eps\n  else:\n    # If `single_jitter` is False, draw a new random perturbation for each sample.\n    u += jax.random.uniform(rng, u.shape, minval=0, maxval=1e-7)\n    u %= 1.0 - eps\n\n  # Invert the CDF.\n  t_new = invert_cdf(u, t, w_logits)\n\n  if deterministic_center:\n    # For deterministic sampling, interpolate the value in the center of each bin.\n    t_min, t_max = t[Ellipsis,"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Draw uniform samples.\n  if rng is None:\n    # Match MATLAB's linspace.\n    u = jnp.linspace(0, 1 - eps, num_samples)\n    if not single_jitter:\n      u = jax.random.uniform(\n          jax.random.PRNGKey(0), shape=u.shape, minval=0, maxval=1 - eps\n      )\n  else:\n    u = jax.random.uniform(rng, shape=(num_samples,), minval=0, maxval=1 - eps)\n\n  # Invert the CDF.\n  t_new = invert_cdf(u, t, w_logits)\n\n  # Perturb the samples.\n  if single_jitter:\n    t_new += jax.random.uniform(\n        rng, shape=t_new.shape, minval=-eps, maxval=eps\n    )\n  else:\n    t_new += jax.random.uniform(\n        rng, shape=t_new.shape, minval=-eps, maxval=eps\n    )\n\n  # Clamp samples to the unit interval.\n  t_new = jnp.clip(t_new, 0, 1)\n\n  # Deterministic centers for each interval.\n  if deterministic_center:\n    t_new = (t_new + cw[Ellipsis, :-1]) / 2\n\n  return t_new"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Pad each CDF so that the probability of sampling the first/last bin is nonzero.\n  cw0 = jnp.concatenate([jnp.zeros_like(cw[Ellipsis, :1]), cw, jnp.ones_like(cw[Ellipsis, :1])], axis=-1)\n\n  # Draw uniform samples.\n  if rng is None:\n    # Match MATLAB `rand` here.\n    shape = t.shape[:-1] + (num_samples,)\n    u = jnp.linspace(0.0, 1.0 - eps, num_samples)\n    u = jnp.broadcast_to(u, shape)\n  else:\n    shape = t.shape[:-1] + (num_samples,)\n    if single_jitter:\n      # Apply the same jitter to all samples along each ray.\n      u = jax.random.uniform(rng, shape)\n    else:\n      # Apply a different jitter to each sample along each ray.\n      u = jax.random.uniform(rng, (shape[0], 1))\n      u = jnp.broadcast_to(u, shape)\n\n  # Invert the CDF.\n  t_new = math.sorted_interp(u, cw0, t, utils.device_is_tpu())\n\n  # Draw samples from the step function.\n  if deterministic_center:\n    # Match MATLAB `rand` here.\n    t_new = (t_new[Ellipsis, 1:] + t_new[Ellipsis, :-1]) / 2\n\n  return t_new"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 1:\n    raise ValueError(f'num_samples must be > 1, is {num_samples}.')\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  if rng is None:\n    # Match MipNeRF's (and TFNS's) behavior.\n    if deterministic_center:\n      u = jnp.linspace(0.5 / num_samples, 1.0 - 0.5 / num_samples, num_samples)\n    else:\n      u = jnp.linspace(0.0, 1.0 - 1.0 / num_samples, num_samples)\n    u = jnp.broadcast_to(u, t.shape[:-1] + (num_samples,))\n  else:\n    # Sample randomly.\n    if single_jitter:\n      # Apply the same jitter to all samples.\n      u = jax.random.uniform(rng, t.shape[:-1] + (num_samples,))\n    else:\n      # Apply a different jitter to each sample.\n      u = jax.random.uniform(rng, t.shape[:-1] + (num_samples,)) / num_samples\n\n  # Interpolate into the inverse CDF.\n  t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n\n  # Perturb the samples from the inverse CDF so they lie in the unit interval.\n  if not deterministic_center:\n    # For each interval, choose one of the two neighboring samples at uniform.\n    # This ensures samples are continuous.\n    u_perturb = jax.random.uniform(rng, t.shape[:-1] + (num_samples,))\n    t_new = math.ler"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Take midpoints of all intervals; we'll use these as new query points.\n  mids = (t[Ellipsis, 1:] + t[Ellipsis, :-1]) / 2\n\n  if rng is None:\n    # Deterministic sample along each ray.\n    if deterministic_center:\n      # Match the behavior of the original code: sample from the center of each bin.\n      pad = (t[Ellipsis, 1:] - t[Ellipsis, :-1]) / 2\n      mids_pad = jnp.stack([mids - pad, mids + pad], axis=-1)\n      mids = jnp.reshape(mids_pad, mids_pad.shape[:-2] + (-1,))\n    u = jnp.linspace(0.0 + eps, 1.0 - eps, num_samples)\n    if not single_jitter:\n      grid = jnp.broadcast_to(\n          mids[Ellipsis, None], mids.shape[:-1] + (num_samples,)\n      )\n      grid = jnp.reshape(grid, grid.shape[:-2] + (-1,))\n      u = jnp.expand_dims(u, axis=-1) + jax.random.uniform(\n          jax.random.PRNGKey(0), grid.shape\n      )\n      u = jnp.reshape(u, u.shape[:-1] + mids.shape[:-1] + (-1,))\n    u = jnp.clip(u, eps, 1.0 - eps)\n  else:\n    # Randomly sample.\n    pad = jnp.clip(t[Ellipsis, 1:] - t[Ellips"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  # Compute the CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Draw uniform samples in [0, 1).\n  if rng is None:\n    # Match the behavior of jax.random.uniform() by using linspace().\n    spacing = 1.0 / num_samples\n    u = jnp.linspace(0.0, 1.0 - spacing, num_samples)\n    u = jnp.broadcast_to(u, w.shape[:-1] + (num_samples,))\n  else:\n    # Draw uniform samples.\n    u = jax.random.uniform(rng, w.shape[:-1] + (num_samples,), minval=0.0, maxval=1.0)\n\n  if single_jitter:\n    # Apply the same jitter to every sample.\n    u += jax.random.uniform(\n        rng, w.shape[:-1] + (num_samples,), minval=0.0, maxval=1.0\n    )\n  else:\n    # Apply a different jitter to each sample.\n    u += jax.random.uniform(\n        rng, w.shape[:-1] + (num_samples,), minval=-0.5, maxval=0.5\n    )\n\n  if deterministic_center:\n    # Match the behavior of jnp.linspace().\n    # This centers the samples within each bin.\n    u = (u + jnp.arange(0.0, num_samples, dtype=w.dtype)) / num_samples\n  else:\n    # Scale up/down to the full [t0, t1] range.\n    t0, t1 = t[Ellipsis, :1], t[Ellipsis, -1:]\n    u = t0 + (t1 - t0) * u"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  if rng is None:\n    # Deterministic sampling.\n    if deterministic_center:\n      # Sample around the midpoint of each bin.\n      mid = (t[Ellipsis, 1:] + t[Ellipsis, :-1]) / 2\n      samples = mid\n    else:\n      # Sample uniformly on the interval defined by the endpoints.\n      samples = jnp.linspace(t[Ellipsis, 0], t[Ellipsis, -1], num_samples)\n  else:\n    # Random sampling.\n    if deterministic_center:\n      # Sample around the midpoint of each bin.\n      mid = (t[Ellipsis, 1:] + t[Ellipsis, :-1]) / 2\n      if single_jitter:\n        # Apply the same jitter to all samples.\n        samples = mid + jax.random.uniform(\n            rng, mid.shape[:-1] + (1,), minval=-eps, maxval=eps\n        )\n      else:\n        # Apply a different jitter to each sample.\n        samples = mid + jax.random.uniform(\n            rng, mid.shape[:-1] + (num_samples,), minval=-eps, maxval=eps\n        )\n    else:\n      # Sample uniformly on the interval defined by the endpoints.\n      samples = jax.random.uniform(\n          rng, t.shape[:-1] + (num_samples,), t[Ellipsis, 0], t[Ellipsis, -1]\n      )\n\n  # Invert the CDF.\n  t_samples = invert_cdf(samples, t, w_logits)\n  return t_samples"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 1:\n    raise ValueError(f'num_samples must be > 1, is {num_samples}.')\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Pad each CDF so that the sampled values are strictly within [0, 1).\n  cw01 = jnp.pad(cw, [(0, 0)] * (cw.ndim - 1) + [(1, 1)])\n  cw01 = jnp.clip(cw01, 0, 1)\n\n  # Draw a vector U in [0, 1) of shape [batch_size, num_samples].\n  if rng is None:\n    # Match MATLAB's linspace() behavior.\n    u = jnp.linspace(0, 1 - 1 / num_samples, num_samples)\n    u = jnp.broadcast_to(u, cw01.shape[:-1] + u.shape)\n  else:\n    if single_jitter:\n      # Apply the same jitter u to all samples in the vector.\n      u = jax.random.uniform(rng, cw01.shape[:-1] + (1,))\n    else:\n      # Draw a new jitter u for each sample.\n      u = jax.random.uniform(rng, cw01.shape[:-1] + (num_samples,))\n    u = (u + jnp.arange(num_samples, dtype=u.dtype)) / num_samples\n    u = jnp.clip(u, 0, 1 - 1 / num_samples)\n\n  # Identify the bin that each u falls into, then linearly interpolate to\n  # identify the corresponding t value.\n  t0, t1, w0"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 1:\n    raise ValueError(f'num_samples must be > 1, is {num_samples}.')\n\n  # Compute the CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Deterministic sample along the step function.\n  if rng is None:\n    if deterministic_center:\n      # Deterministic sample the center of each interval.\n      u = jnp.linspace(0.5 / num_samples, 1.0 - 0.5 / num_samples, num_samples)\n    else:\n      # Deterministic sample the entire PDF.\n      u = jnp.linspace(0.0, 1.0, num_samples)\n    u = jnp.broadcast_to(u, w.shape[:-1] + u.shape)\n\n  # Random sample the step function.\n  else:\n    # Randomly sample the step function.\n    key, rng = jax.random.split(rng)\n    shape = w.shape[:-1] + (num_samples,)\n    u = jax.random.uniform(key, shape)\n\n    # Jitter the samples.\n    if single_jitter:\n      key, rng = jax.random.split(rng)\n      u += jax.random.uniform(key, shape, minval=-eps, maxval=eps)\n    else:\n      u += jax.random.uniform(rng, shape, minval=-eps, maxval=eps)\n\n  # Invert the CDF.\n  t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n\n  return t_new"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 1:\n    raise ValueError(f'num_samples must be > 1, is {num_samples}.')\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Pad CDFs to make them work for testing the rightmost edge of the last bin.\n  cw0 = jnp.concatenate([cw, jnp.ones_like(cw[Ellipsis, :1])], axis=-1)\n  cw1 = jnp.concatenate([jnp.zeros_like(cw[Ellipsis, :1]), cw], axis=-1)\n\n  # Draw uniform samples.\n  if rng is None:\n    # Match MATLAB `rand` behavior for random sampling in the unit interval.\n    shape = cw.shape[:-1] + (num_samples,)\n    u = jnp.linspace(0, 1 - eps, num_samples)\n    u = jnp.broadcast_to(u, shape)\n  else:\n    # Draw random samples.\n    shape = cw.shape[:-1] + (num_samples,)\n    if single_jitter:\n      # If jittering a single sample along each ray, we first draw a random\n      # sample, then jitter it.\n      u = jax.random.uniform(rng, shape)\n      jitter = jax.random.uniform(rng, cw.shape[:-1] + (1,), minval=0, maxval=1)\n      u = jnp.clip(u + jitter - 0.5, 0, 1 - eps)\n    else:\n      # Otherwise, jitter the entire set of samples together, then draw.\n      jitter = jax.random.uniform(rng, cw.shape[:-1] + (1"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 1:\n    raise ValueError(f'num_samples must be > 1, is {num_samples}.')\n\n  # Sample a set of points from the step function.\n  if rng is None:\n    # Deterministic sampling.\n    if deterministic_center:\n      # Center the samples along each interval.\n      u = jnp.linspace(0.5 / num_samples, 1 - 0.5 / num_samples, num_samples)\n    else:\n      # Sample uniformly from the entire PDF.\n      u = jnp.linspace(0.5 / num_samples, 1 - 0.5 / num_samples, num_samples)\n    u = u[None, :]\n  else:\n    # Random sampling.\n    if single_jitter:\n      # Apply the same perturbation to all samples.\n      u = jax.random.uniform(rng, (num_samples,), minval=0, maxval=1)\n      u = u[None, :]\n    else:\n      # Apply a different perturbation to each sample.\n      u = jax.random.uniform(rng, (num_samples,), minval=0, maxval=1)\n      u = jnp.sort(u)\n      u = jnp.stack([u, jnp.zeros_like(u)], axis=-1)\n      u = jnp.cumsum(u, axis=-1)[:, 0]\n      u = u[None, :]\n\n  # Invert the CDF to get samples in the (t, w) domain.\n  t_samples = invert_cdf(u, t, w_logits)\n\n  # Apply a small amount of jitter, to help with stability.\n  t_samples += (eps * jax.random.uniform(rng, t_samples.shape))\n\n  return t_samples"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 1:\n    raise ValueError(f'num_samples must be > 1, is {num_samples}.')\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Take midpoints of all intervals (except the last).\n  cw_rolled = jnp.roll(cw, shift=1, axis=-1)\n  cw_rolled[Ellipsis, 0] = 0\n  cw_diff = cw - cw_rolled\n  t_mid = (t[Ellipsis, :-1] + t[Ellipsis, 1:]) / 2\n\n  # Draw uniform samples.\n  if rng is None:\n    # Match MATLAB `linspace`.\n    shape = t.shape[:-1] + (num_samples,)\n    u = jnp.linspace(0 + eps, 1 - eps, num_samples)\n    u = jnp.broadcast_to(u, shape)\n  else:\n    shape = t.shape[:-1] + (num_samples, 1)\n    if single_jitter:\n      # Apply the same jitter to every sample along each ray.\n      u = jax.random.uniform(rng, shape, dtype=jnp.float32)\n    else:\n      # Apply a different jitter to every sample along each ray.\n      u = jax.random.uniform(rng, shape)\n\n  # Identify the location in `cw` that corresponds to a random sample.\n  # Note that `cw` is one element longer than `t`, so we use `t_nxt` here.\n  t_nxt = t[Ellipsis, 1:]\n  mask = u[Ellipsis, :, None] > cw[Ellipsis, None, :]"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 1:\n    raise ValueError(f'num_samples must be > 1, is {num_samples}.')\n\n  # Compute the CDF and normalize it.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Pad cw with a 0 on the left and a 1 on the right.\n  cw0 = jnp.concatenate([jnp.zeros_like(cw[Ellipsis, :1]), cw, jnp.ones_like(cw[Ellipsis, :1])], axis=-1)\n\n  # Draw a set of samples from the step function.\n  if rng is None:\n    # Deterministic sample along each ray.\n    if deterministic_center:\n      # Sample around the center of the step function.\n      u = (jnp.linspace(0.5 - num_samples, 0.5, num_samples) + 0.5) / num_samples\n    else:\n      # Sample from the entire step function.\n      u = jnp.linspace(0, 1, num_samples + 2)[1:-1]\n  else:\n    # Random sample around each ray.\n    if single_jitter:\n      # If `single_jitter` is True, jitter all samples along each ray.\n      u = jax.random.uniform(rng, cw0.shape[:-1] + (num_samples,))\n    else:\n      # If `single_jitter` is False, jitter each sample independently.\n      u = jax.random.uniform(rng, cw0.shape[:-1] + (num_samples,)) / num_samples\n\n  # Invert the CDF to sample from the step function.\n  t_samples = invert_cdf(u, t, w_logits)\n\n  # Add a small amount of noise to the samples"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 1:\n    raise ValueError(f'num_samples must be > 1, is {num_samples}.')\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Get into a form where t defines the *centers* of each integration interval.\n  t0, t1 = t[Ellipsis, :-1], t[Ellipsis, 1:]\n  t_centers = (t0 + t1) / 2\n  dt = t1 - t0\n\n  # Draw uniform samples.\n  if rng is None:\n    # Match the behavior of jax.random.uniform() by using a fixed seed.\n    rng = jax.random.PRNGKey(0)\n  shape = w.shape[:-1] + (num_samples,)\n  u = jax.random.uniform(rng, shape)\n\n  # If single_jitter is True, jitter every sample along each ray by the same\n  # amount in the inverse CDF. Otherwise, jitter each sample independently.\n  if single_jitter:\n    u += jax.random.uniform(rng, shape)\n    u = jnp.remainder(u, 1.0)\n\n  # Invert the CDF.\n  t_new = invert_cdf(u, t_centers, w_logits)\n\n  # Pick a zeroth order hold, holding each sample at the center of its interval.\n  if deterministic_center:\n    t_new = t_new\n  # Pick a first order hold, where samples gradually flow from left to right.\n  else:\n    mask = (u > .5) & (t1 > t0)\n    t_new = jnp.where(mask, t_new + dt[Ellipsis, :-1] / 2, t_new - dt[Ell"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 1:\n    raise ValueError(f'num_samples must be > 1, is {num_samples}.')\n\n  # Compute the CDF and the inverse CDF.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n  t_new = invert_cdf(cw, t, w_logits)\n\n  # Piecewise-Constant PDF function\n  pdf = weight_to_pdf(t, w)\n\n  if rng is None:\n    # Produce deterministic samples based on a regular grid.\n    # Space the samples evenly in the inverse CDF (assumed to be monotonic).\n    if deterministic_center:\n      # Match the behavior of the original code, which drops the first and last\n      # samples to avoid corner cases.\n      spacing = (1 - 2 * eps) / (num_samples - 2)\n      offset = eps\n    else:\n      spacing = (1 - 2 * eps) / (num_samples - 1)\n      offset = eps / 2\n    u = jnp.linspace(offset, 1 - offset, num_samples)\n    u = u[None, ...]\n\n    # Invert the CDF.\n    t_samples = math.sorted_interp(u, cw, t_new, utils.device_is_tpu())\n\n  else:\n    # Sample uniformly from the CDF.\n    # Note: we could also use uniform noise, by adding U[0,1] to the samples below.\n    shape = w.shape[:-1] + (num_samples,)\n    if single_jitter:\n      pad = jnp.array([-1, 1])\n      pad = lambda x: jnp.pad(x, pad, mode='edge')\n      u = jax.random.uniform(rng, shape)\n      u = jax.lax"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Take midpoints of all intervals.\n  mid = (t[Ellipsis, 1:] + t[Ellipsis, :-1]) / 2\n\n  if rng is None:\n    # Deterministic center sampling.\n    if deterministic_center:\n      # Note: we do not add `eps` here because it would be redundant.\n      # Its purpose is just to make the result exactly equal to `mid` when\n      # `w == 1`.\n      u = jnp.linspace(0.0, 1.0, num_samples)\n      u = u[None, Ellipsis].repeat(w.shape[:-1] + (1,))\n    else:\n      # Note: we add `eps` to make the result numerically stable.\n      u = jnp.linspace(eps, 1.0 - eps, num_samples)\n      u = u[None, Ellipsis].repeat(w.shape[:-1] + (1,))\n    t_samples = invert_cdf(u, t, w_logits)\n  else:\n    # Random sampling.\n    if single_jitter:\n      # If single_jitter is enabled, we can draw a single, shared random number\n      # to jitter all samples along each ray. This avoids the consistency problem\n      # that would come with using a random number to jitter each sample\n      # independently.\n      shape = w_logits.shape[:-1] + (num_samples,)\n      u = jax.random.uniform(rng, shape)\n      t_samples = invert_cdf(u, t, w_logits)\n    else:\n      # If single_jitter is disabled, we draw a new random number for each sample\n      # along each ray. This is more consistent, but more"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  t_intervals = (t_samples[Ellipsis, 1:] + t_samples[Ellipsis, :-1]) / 2\n\n  # Adjust the first and last intervals to fit within the specified domain.\n  t_intervals = jnp.concatenate(\n      [\n          jnp.maximum(t_intervals[Ellipsis, :1], domain[0]),\n          t_intervals[Ellipsis, 1:-1],\n          jnp.minimum(t_intervals[Ellipsis, -1:], domain[1]),\n      ],\n      axis=-1,\n  )\n\n  return t_intervals"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample points from the step function\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples\n  t_midpoints = (t_samples[..., 1:] + t_samples[..., :-1]) / 2\n\n  # Adjust the first and last intervals to fit within the domain\n  t_samples = jnp.concatenate([jnp.full_like(t_samples[..., :1], domain[0]), t_midpoints, jnp.full_like(t_samples[..., :1], domain[1])], axis=-1)\n\n  return t_samples"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[..., 1:] + t_samples[..., :-1]) / 2\n\n  # Adjust the first and last intervals to fit within the specified domain.\n  t_samples = jnp.concatenate(\n      [\n          jnp.maximum(t_samples[..., :1], domain[0]),\n          t_midpoints,\n          jnp.minimum(t_samples[..., -1:], domain[1]),\n      ],\n      axis=-1,\n  )\n\n  return t_samples"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample points from the step function\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples\n  t_midpoints = (t_samples[..., 1:] + t_samples[..., :-1]) / 2\n\n  # Adjust the first and last intervals to fit within the domain\n  t_samples = jnp.concatenate(\n      [\n          jnp.full_like(t_samples[..., :1], domain[0]),\n          t_midpoints,\n          jnp.full_like(t_samples[..., :1], domain[1]),\n      ],\n      axis=-1,\n  )\n\n  return t_samples"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples\n  t_midpoints = (t_samples[..., 1:] + t_samples[..., :-1]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain\n  t_samples = jnp.concatenate([t_samples[..., :1], t_midpoints, t_samples[..., -1:]], axis=-1)\n  t_samples = jnp.clip(t_samples, domain[0], domain[1])\n\n  return t_samples"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample points from the step function\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples\n  t_mid = (t_samples[..., 1:] + t_samples[..., :-1]) / 2\n\n  # Adjust the first and last intervals to fit within the domain\n  t_samples = jnp.concatenate([jnp.full_like(t_samples[..., :1], domain[0]), t_mid, jnp.full_like(t_samples[..., :1], domain[1])], axis=-1)\n\n  return t_samples"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[..., 1:] + t_samples[..., :-1]) / 2\n\n  # Adjust the first and last intervals to fit within the specified domain.\n  t_samples = jnp.concatenate([jnp.full_like(t_samples[..., :1], domain[0]),\n                              t_midpoints,\n                              jnp.full_like(t_samples[..., :1], domain[1])], axis=-1)\n\n  return t_samples"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample points from the step function\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples\n  t_midpoints = (t_samples[..., 1:] + t_samples[..., :-1]) / 2\n\n  # Adjust the first and last intervals to fit within the specified domain\n  t_first_interval = jnp.full_like(t_midpoints[..., :1], domain[0])\n  t_last_interval = jnp.full_like(t_midpoints[..., :1], domain[1])\n  t_intervals = jnp.concatenate([t_first_interval, t_midpoints, t_last_interval], axis=-1)\n\n  return t_intervals"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample from the step function to get the points\n  u = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples\n  u_mid = (u[..., 1:] + u[..., :-1]) / 2\n\n  # Adjust the first and last intervals to fit within the domain\n  u_mid = jnp.concatenate([jnp.full_like(u[..., :1], domain[0]), u_mid, jnp.full_like(u[..., :1], domain[1])], axis=-1)\n\n  return u_mid"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample points from the step function\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples\n  midpoints = (t_samples[..., 1:] + t_samples[..., :-1]) / 2\n\n  # Adjust the first and last intervals to fit within the specified domain\n  first_interval = jnp.maximum(domain[0], 2 * t_samples[..., 0] - midpoints[..., 0])\n  last_interval = jnp.minimum(domain[1], 2 * t_samples[..., -1] - midpoints[..., -1])\n\n  # Combine the midpoints and adjusted intervals to get the final intervals\n  intervals = jnp.concatenate([first_interval[..., None], midpoints, last_interval[..., None]], axis=-1)\n\n  return intervals"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  t_samples = sample(\n      rng, t, w_logits, num_samples, single_jitter=single_jitter\n  )\n  t_samples = jnp.sort(t_samples, axis=-1)\n\n  # Compute the midpoints between adjacent samples.\n  t_mid = (t_samples[Ellipsis, 1:] + t_samples[Ellipsis, :-1]) / 2\n\n  # Include the endpoints in the output.\n  t_samples = jnp.concatenate([t_samples[Ellipsis, :1], t_mid, t_samples[Ellipsis, -1:]], axis=-1)\n\n  # Clip the output to the valid range.\n  t_samples = jnp.clip(t_samples, domain[0], domain[1])\n\n  return t_samples"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Draw uniform samples.\n  if rng is None:\n    # Match the behavior of jax.random.uniform() by spanning [0, 1-eps].\n    u = jnp.linspace(0, 1.0 - jnp.finfo(jnp.float32).eps, num_samples)\n    u = jnp.broadcast_to(u, t.shape[:-1] + (num_samples,))\n  else:\n    # `u` is in [0, 1) --- it can be zero, but it can never be 1.\n    u = jax.random.uniform(rng, t.shape[:-1] + (num_samples,))\n\n  # Invert the CDF to get the samples.\n  t_samples = invert_cdf(u, t, w_logits)\n\n  # Compute the midpoints between adjacent samples.\n  t_mid = (t_samples[Ellipsis, 1:] + t_samples[Ellipsis, :-1]) / 2\n\n  # Adjust the first and last intervals to fit within the domain.\n  t_first = jnp.full_like(t_samples[Ellipsis, :1], domain[0])\n  t_last = jnp.full_like(t_samples[Ellipsis, :1], domain[1])\n  t_mid = jnp.concatenate([t_first, t_mid, t_last], axis=-1)\n\n  return t_mid"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Draw uniform samples.\n  if rng is None:\n    # Match the behavior of jax.random.uniform() by spanning [0, 1-eps].\n    u = jnp.linspace(0, 1 - jnp.finfo(jnp.float32).eps, num_samples)\n    u = jnp.broadcast_to(u, t.shape[:-1] + (num_samples,))\n  else:\n    # `u` is in [0, 1) --- it can be zero, but it can never be 1.\n    u = jax.random.uniform(rng, t.shape[:-1] + (num_samples,))\n\n  # Invert the CDF.\n  t_samples = invert_cdf(u, t, w_logits)\n\n  # Calculate midpoints between adjacent samples.\n  t_mid = (t_samples[..., 1:] + t_samples[..., :-1]) / 2\n\n  # Adjust the first and last intervals to fit within the domain.\n  t_first = t_samples[..., :1] + (t_mid[..., :1] - t_samples[..., :1])\n  t_last = t_samples[..., -1:] + (t_mid[..., -1:] - t_samples[..., -1:])\n  t_mid = jnp.concatenate([t_first, t_mid, t_last], axis=-1)\n\n  # Clip the intervals to the valid domain.\n  t_mid = jnp.clip(t_mid, domain[0], domain[1])\n\n  return t_mid"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  # Draw uniform samples.\n  if rng is None:\n    # Match the behavior of jax.random.uniform() by spanning [0, 1-eps].\n    pad = 1 / (2 * num_samples)\n    u = jnp.linspace(pad, 1.0 - pad, num_samples)\n    u = jnp.broadcast_to(u, t.shape[:-1] + (num_samples,))\n  else:\n    # `u` is in [0, 1) --- it can be zero, but it can never be 1.\n    u_max = 1 / num_samples\n    max_jitter = (1 - u_max) / (num_samples - 1)\n    d = 1 if single_jitter else num_samples\n    u = jnp.linspace(0, 1 - u_max, num_samples) + jax.random.uniform(\n        rng, t.shape[:-1] + (d,), maxval=max_jitter\n    )\n\n  # Invert the CDF.\n  t_samples = invert_cdf(u, t, w_logits)\n\n  # Compute the midpoints between adjacent samples.\n  t_samples_mid = (t_samples[Ellipsis, 1:] + t_samples[Ellipsis, :-1]) / 2\n\n  # Ensure that the first and last intervals cover the entire domain.\n  t_samples_mid = jnp.concatenate(\n      [\n          jnp.full(t_samples_mid.shape[:-1] + (1,), domain[0]),\n          t_samples_mid,\n          jnp.full(t_samples_mid.shape[:-1] + (1,), domain[1]),\n      ],\n      axis=-1,\n  )\n\n  return t_samples_mid"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample points from the step function\n  t_samples = sample(\n      rng, t, w_logits, num_samples, single_jitter, deterministic_center=True\n  )\n\n  # Calculate midpoints between adjacent samples\n  t_intervals = (t_samples[..., 1:] + t_samples[..., :-1]) / 2\n\n  # Adjust the first and last intervals to fit within the domain\n  t_intervals = jnp.concatenate(\n      [\n          jnp.full(t_intervals.shape[:-1] + (1,), domain[0]),\n          t_intervals,\n          jnp.full(t_intervals.shape[:-1] + (1,), domain[1]),\n      ],\n      axis=-1,\n  )\n\n  return t_intervals"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample points from the step function\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples\n  midpoints = (t_samples[Ellipsis, 1:] + t_samples[Ellipsis, :-1]) / 2\n\n  # Adjust the first and last intervals to fit within the specified domain\n  left = jnp.broadcast_to(t[Ellipsis, :1], midpoints.shape[:-1] + (1,))\n  right = jnp.broadcast_to(t[Ellipsis, -1:], midpoints.shape[:-1] + (1,))\n  midpoints = jnp.concatenate([left, midpoints, right], axis=-1)\n  midpoints = jnp.clip(midpoints, domain[0], domain[1])\n\n  return midpoints"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  midpoints = (t_samples[..., 1:] + t_samples[..., :-1]) / 2\n\n  # Adjust the first and last intervals to fit within the specified domain.\n  first_interval_half_width = (midpoints[..., 0] - t[..., 0]) / 2\n  last_interval_half_width = (t[..., -1] - midpoints[..., -1]) / 2\n  t_samples = jnp.concatenate(\n      [\n          t_samples[..., :1] - first_interval_half_width,\n          midpoints,\n          t_samples[..., -1:] + last_interval_half_width,\n      ],\n      axis=-1,\n  )\n\n  # Clip the intervals to the specified domain.\n  t_samples = jnp.clip(t_samples, domain[0], domain[1])\n\n  return t_samples"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Draw uniform samples.\n  if rng is None:\n    # Match the behavior of jax.random.uniform() by spanning [0, 1-eps].\n    pad = 1 / (2 * num_samples)\n    u = jnp.linspace(pad, 1.0 - pad, num_samples)\n    u = jnp.broadcast_to(u, t.shape[:-1] + (num_samples,))\n  else:\n    # `u` is in [0, 1) --- it can be zero, but it can never be 1.\n    u_max = 1 / num_samples\n    max_jitter = (1 - u_max) / (num_samples - 1)\n    d = 1 if single_jitter else num_samples\n    u = jnp.linspace(0, 1 - u_max, num_samples) + jax.random.uniform(\n        rng, t.shape[:-1] + (d,), maxval=max_jitter\n    )\n\n  # Invert the CDF.\n  t_samples = invert_cdf(u, t, w_logits)\n\n  # Calculate midpoints between adjacent samples.\n  t_mid = (t_samples[Ellipsis, 1:] + t_samples[Ellipsis, :-1]) / 2\n\n  # Adjust the first and last intervals to fit within the specified domain.\n  t_min, t_max = domain\n  t_first = t_min + (t_samples[Ellipsis, 0] - t_min) * (t_samples[Ellipsis, 1] - t_min) / (t_samples[Ellipsis, 1] - t[Ellipsis, 0])\n  t_last = t_max - (t_max - t_samples[Ellipsis, -1]) * (t_max - t_samples[Ellipsis,"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Draw uniform samples.\n  if rng is None:\n    # Match the behavior of jax.random.uniform() by spanning [0, 1-eps].\n    u = jnp.linspace(0, 1.0 - jnp.finfo(jnp.float32).eps, num_samples)\n    u = jnp.broadcast_to(u, t.shape[:-1] + (num_samples,))\n  else:\n    # `u` is in [0, 1) --- it can be zero, but it can never be 1.\n    max_jitter = (1 - 1.0 / num_samples) / (num_samples - 1)\n    d = 1 if single_jitter else num_samples\n    u = jnp.linspace(0, 1.0 - max_jitter, num_samples) + jax.random.uniform(\n        rng, t.shape[:-1] + (d,), maxval=max_jitter\n    )\n\n  # Invert the CDF.\n  t_samples = invert_cdf(u, t, w_logits)\n\n  # Compute midpoints.\n  t_mid = (t_samples[..., 1:] + t_samples[..., :-1]) / 2\n\n  # Include a point at the center of each bin.\n  t_center = (t[..., 1:] + t[..., :-1]) / 2\n  t_all = jnp.concatenate([t[..., :1], t_center, t[..., -1:]], axis=-1)\n  w_all = jnp.concatenate([jnp.zeros_like(w_logits[..., :1]), w_logits, jnp.zeros_like(w_logits[..., :1])], axis=-1)\n  t_weights = jnp.concatenate([t_mid, t_samples"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample points from the step function.\n  u = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  u_mid = (u[Ellipsis, 1:] + u[Ellipsis, :-1]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the domain.\n  u_first = jnp.maximum(u[Ellipsis, :1], domain[0])\n  u_last = jnp.minimum(u[Ellipsis, -1:], domain[1])\n\n  # Concatenate the first, midpoints, and last intervals.\n  u_samples = jnp.concatenate([u_first, u_mid, u_last], axis=-1)\n\n  return u_samples\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  w = jnp.where(w < 0, 0, w)\n  w = w / jnp.sum(w, axis=-1, keepdims=True)\n  cw = jnp.cumsum(w, axis=-1)\n  cw = jnp.concatenate([jnp.zeros_like(cw[..., :1]), cw], axis=-1)\n  return jnp.interp(ps / 100, cw, t)"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  w = w / jnp.sum(w, axis=-1, keepdims=True)\n  cw = jnp.cumsum(w, axis=-1)\n  cw = jnp.concatenate([jnp.zeros(cw.shape[:-1] + (1,)), cw], axis=-1)\n  ps = jnp.array(ps) / 100\n  t_percentiles = jnp.interp(ps, cw, t)\n  return t_percentiles"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  cw = integrate_weights(w)\n  t_interp = math.sorted_interp(ps / 100, cw, t, utils.device_is_tpu())\n  return t_interp"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Compute the integrated weights.\n  cw = integrate_weights(w)\n  # Interpolate into the integrated weights to find the percentiles.\n  t_percentiles = math.sorted_interp(ps / 100, cw, t, utils.device_is_tpu())\n  return t_percentiles"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1.\n  w = w / jnp.sum(w, axis=-1, keepdims=True)\n\n  # Compute the integrated weights.\n  cw = jnp.cumsum(w, axis=-1)\n\n  # Interpolate the integrated weights at the given percentiles.\n  ps = jnp.array(ps) / 100\n  t_resampled = jnp.interp(ps, cw, t)\n\n  return t_resampled"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n\n  # Ensure that the weights sum to 1.\n  w = w / jnp.sum(w, axis=-1, keepdims=True)\n\n  # Compute the integrated weights.\n  cw = integrate_weights(w)\n\n  # Interpolate the integrated weights at the given percentiles.\n  ps = jnp.array(ps) / 100\n  t_percentiles = jnp.vectorize(jnp.interp, signature='(n),(m),(m)->(n)')(\n      ps, cw, t\n  )\n\n  return t_percentiles"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1.\n  w = w / jnp.sum(w, axis=-1, keepdims=True)\n\n  # Compute the integrated weights.\n  cw = jnp.cumsum(w, axis=-1)\n\n  # Interpolate the integrated weights to the given percentiles.\n  cw_interp = jnp.interp(ps, cw, t)\n\n  return cw_interp"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  w_cumsum = jnp.cumsum(w, axis=-1)\n  w_cumsum = jnp.concatenate([jnp.zeros(w_cumsum.shape[:-1] + (1,)), w_cumsum], axis=-1)\n  w_cumsum = w_cumsum / w_cumsum[..., -1:]\n  ps = jnp.array(ps) / 100\n  t_resampled = jnp.interp(ps, w_cumsum, t)\n  return t_resampled"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n\n  # Compute the integrated weights and normalize them to sum to 1.\n  cw = integrate_weights(w)\n  cw = cw / cw[Ellipsis, -1:]\n\n  # Interpolate the integrated weights at the given percentiles.\n  ps = jnp.array(ps) / 100\n  t_resampled = jnp.vectorize(jnp.interp, signature='(n),(m),(m)->(n)')(cw, ps, t)\n\n  return t_resampled"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1\n  w = w / jnp.sum(w, axis=-1, keepdims=True)\n\n  # Compute the integrated weights\n  w_int = jnp.cumsum(w, axis=-1)\n\n  # Interpolate the integrated weights to get the percentiles\n  ps_int = jnp.interp(ps, w_int, t)\n\n  return ps_int"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  w_integrated = integrate_weights(w)\n  ps = jnp.array(ps)\n  ps = jnp.reshape(ps, [-1])\n  ps_normalized = ps / 100\n  ps_normalized = jnp.clip(ps_normalized, 0, 1)\n  t_interpolated = jnp.interp(ps_normalized, w_integrated, t)\n  return t_interpolated"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Check that the weights sum to 1\n  if not jnp.allclose(jnp.sum(w, axis=-1), 1.0):\n    raise ValueError(\"The weights must sum to 1.\")\n\n  # Compute the integrated weights\n  cw = integrate_weights(w)\n\n  # Interpolate the integrated weights at the given percentiles\n  ps = jnp.array(ps) / 100.0\n  t_percentiles = jnp.interp(ps, cw, t)\n\n  return t_percentiles"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n\n  # Ensure that the weights sum to 1.\n  w = w / jnp.sum(w, axis=-1, keepdims=True)\n\n  # Compute the integrated weights.\n  cw = integrate_weights(w)\n\n  # Interpolate into the integrated weights to find the percentiles.\n  ps = jnp.array(ps)\n  ps = jnp.where(ps > 100, 100, ps)\n  ps = jnp.where(ps < 0, 0, ps)\n  ps = ps / 100\n  t_percentiles = math.sorted_interp(ps, cw, t, utils.device_is_tpu())\n\n  return t_percentiles"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  w = w / jnp.sum(w, axis=-1, keepdims=True)\n  cw = jnp.cumsum(w, axis=-1)\n  cw = jnp.concatenate([jnp.zeros_like(cw[..., :1]), cw], axis=-1)\n  ps = jnp.array(ps) / 100\n  ps = jnp.reshape(ps, ps.shape + (1,) * (cw.ndim - 1))\n  idxs = jnp.argmax(cw > ps, axis=-1)\n  idxs = jnp.clip(idxs - 1, 0, cw.shape[-1] - 2)\n  t0, t1 = t[..., idxs], t[..., idxs + 1]\n  w0, w1 = w[..., idxs], w[..., idxs + 1]\n  w0, w1 = w1 - cw[..., idxs], cw[..., idxs + 1] - w0\n  w0, w1 = w0 / (w0 + w1), w1 / (w0 + w1)\n  return t0 + w0 * (t1 - t0)"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n\n  # Ensure the weights sum to 1.\n  w = w / jnp.sum(w, axis=-1, keepdims=True)\n\n  # Compute the integrated weights.\n  acc_w = jnp.cumsum(w, axis=-1)\n\n  # Interpolate the integrated weights at the given percentiles.\n  acc_w_resampled = jnp.vectorize(jnp.interp, signature='(n),(m),(m)->(n)')(\n      ps / 100, acc_w, t\n  )\n\n  # Undo the integration to get weights.\n  w_resampled = jnp.diff(acc_w_resampled, axis=-1)\n\n  # Return the weighted percentiles.\n  return w_resampled"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  if not jnp.allclose(jnp.sum(w, axis=-1), 1.0):\n    raise ValueError(\"The weights must sum to 1.\")\n\n  # Compute the integrated weights\n  cw = jnp.cumsum(w, axis=-1)\n\n  # Normalize the percentiles to the range [0, 1]\n  ps_norm = jnp.array(ps) / 100\n\n  # Interpolate the integrated weights to get the corresponding values\n  ps_norm_resampled = jnp.interp(ps_norm, cw, t)\n\n  return ps_norm_resampled"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Compute the integrated weights.\n  w_int = integrate_weights(w)\n  # Interpolate the integrated weights to the given percentiles.\n  ps = jnp.array(ps)\n  ps = jnp.where(jnp.logical_or(ps < 0, ps > 100), jnp.nan, ps)\n  t_percentiles = jnp.interp(ps / 100, w_int, t)\n  return t_percentiles"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n\n  # Ensure the weights sum to 1.\n  w = w / jnp.sum(w, axis=-1, keepdims=True)\n\n  # Compute the cumulative sum of the weights.\n  cw = jnp.cumsum(w, axis=-1)\n\n  # Normalize the percentiles to the range [0, 1].\n  ps = jnp.asarray(ps) / 100\n\n  # Interpolate the cumulative sum of the weights at the percentile values.\n  cw_at_ps = jnp.interp(ps, cw, t)\n\n  # Compute the differences between adjacent percentile values.\n  cw_diff = jnp.diff(cw_at_ps, axis=-1)\n\n  # Compute the midpoints between adjacent percentile values.\n  cw_mid = (cw_at_ps[..., :-1] + cw_at_ps[..., 1:]) / 2\n\n  # Compute the midpoints between adjacent percentile values.\n  cw_mid = (cw_at_ps[..., :-1] + cw_at_ps[..., 1:]) / 2\n\n  # Compute the midpoints between adjacent percentile values.\n  cw_mid = (cw_at_ps[..., :-1] + cw_at_ps[..., 1:]) / 2\n\n  # Compute the midpoints between adjacent percentile values.\n  cw_mid = (cw_at_ps[..., :-1] + cw_at_ps[..., 1:]) / 2\n\n  # Compute the midpoints between adjacent percentile values.\n  cw_mid = (cw_at_ps[..., :-1] + cw_at_ps[..., 1:]) / 2\n\n  # Compute the midpoints between adjacent percentile values.\n  cw_mid = (cw_at_ps[..., :-"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1.\n  w = w / jnp.sum(w, axis=-1, keepdims=True)\n\n  # Calculate the integrated weights.\n  acc_w = jnp.cumsum(w, axis=-1)\n\n  # Add a small epsilon to the last value of the integrated weights to ensure that the maximum value is included in the interpolation.\n  acc_w = jnp.concatenate([acc_w, jnp.ones_like(acc_w[..., :1])], axis=-1)\n\n  # Interpolate the integrated weights at the given percentiles.\n  ps = jnp.array(ps) / 100\n  tp = jnp.interp(ps, acc_w, t)\n\n  return tp"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Compute the integrated weights\n  w_int = jnp.cumsum(w, axis=-1)\n\n  # Normalize the percentiles to the range [0, 1]\n  ps_norm = jnp.array(ps) / 100\n\n  # Interpolate the integrated weights at the given percentiles\n  w_int_at_ps = jnp.interp(ps_norm, w_int, t)\n\n  # Compute the difference between the interpolated weights\n  w_diff = jnp.diff(w_int_at_ps, axis=-1)\n\n  # Compute the weighted percentiles by dividing the difference by the sum of the weights\n  w_percentiles = w_diff / jnp.sum(w, axis=-1, keepdims=True)\n\n  return w_percentiles"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  utils.assert_valid_stepfun(t, w)\n  w = weight_to_pdf(t, w)\n  w = linspline.blur(t, w, blur_halfwidth)\n  w = resample(tq, t, w, use_avg=False)\n  w = pdf_to_weight(tq, w)\n  return w"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  utils.assert_valid_stepfun(t, w)\n  t_blur, w_blur = linspline.blur(t, w, blur_halfwidth)\n  w_resampled = resample(tq, t_blur, w_blur, use_avg=False)\n  return w_resampled"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  utils.assert_valid_stepfun(t, w)\n  p = weight_to_pdf(t, w)\n  p_blurred = linspline.blur(t, p, blur_halfwidth)\n  w_blurred = pdf_to_weight(t, p_blurred)\n  w_blurred_resampled = resample(tq, t, w_blurred, use_avg=False)\n  return w_blurred_resampled"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a PDF\n  p = weight_to_pdf(t, w)\n\n  # Blur the PDF\n  p_blurred = linspline.blur(t, p, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points\n  w_resampled = resample(tq, t, p_blurred, use_avg=False)\n\n  return w_resampled"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a PDF\n  pdf = weight_to_pdf(t, w)\n\n  # Blur the PDF\n  blurred_pdf = linspline.blur(t, pdf, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points\n  resampled_blurred_pdf = resample(tq, t, blurred_pdf, use_avg=True)\n\n  # Convert the resampled blurred PDF back to weights\n  resampled_blurred_weights = pdf_to_weight(tq, resampled_blurred_pdf)\n\n  return resampled_blurred_weights"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Convert weights to a PDF.\n  p = weight_to_pdf(t, w)\n  # Blur the PDF.\n  p_blurred = linspline.blur(p, blur_halfwidth)\n  # Resample the PDF.\n  p_resampled = resample(tq, t, p_blurred, use_avg=False)\n  # Convert the resampled PDF back to weights.\n  w_resampled = pdf_to_weight(tq, p_resampled)\n  return w_resampled"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  utils.assert_valid_stepfun(t, w)\n  w_pdf = weight_to_pdf(t, w)\n  w_blurred = linspline.blur(t, w_pdf, blur_halfwidth)\n  w_resampled = resample(tq, t, w_blurred, use_avg=False)\n  return w_resampled"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  utils.assert_valid_stepfun(t, w)\n  p = weight_to_pdf(t, w)\n  p_blur = linspline.blur(p, blur_halfwidth, t)\n  w_blur = pdf_to_weight(t, p_blur)\n  w_blur_resampled = resample(tq, t, w_blur, use_avg=False)\n  return w_blur_resampled"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Blur the weights to get a PDF.\n  pdf = weight_to_pdf(t, w)\n  pdf_blurred = linspline.blur(t, pdf, blur_halfwidth)\n  # Resample the PDF to get the resampled weights.\n  w_resampled = resample(tq, t, pdf_blurred, use_avg=False)\n  return w_resampled"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  utils.assert_valid_stepfun(t, w)\n  if blur_halfwidth <= 0:\n    return resample(tq, t, w)\n\n  # Convert to a PDF.\n  pdf = weight_to_pdf(t, w)\n\n  # Blur the PDF.\n  pdf_blurred = linspline.blur(t, pdf, blur_halfwidth)\n\n  # Resample the PDF.\n  pdf_resampled = resample(tq, t, pdf_blurred, use_avg=False)\n\n  # Convert back to weights.\n  w_resampled = pdf_to_weight(tq, pdf_resampled)\n\n  return w_resampled"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  utils.assert_valid_stepfun(t, w)\n\n  # Blur the weights by convolving with a Gaussian kernel.\n  t_blur, w_blur = linspline.gaussian_convolve(t, w, blur_halfwidth)\n\n  # Resample the blurred weights to the new time points.\n  w_resampled = resample(tq, t_blur, w_blur, use_avg=False)\n\n  return w_resampled"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w, axis=-1)\n  cw = integrate_weights(w)\n  # Compute the blurred PDF.\n  cw_blurred = linspline.blur(cw, t, blur_halfwidth)\n  # Interpolate into the inverse CDF.\n  t_new = math.sorted_interp(tq, cw_blurred, t, utils.device_is_tpu())\n  return t_new"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Blur the weights.\n  w_blur = linspline.convolve_with_filter1d(w, blur_halfwidth)\n  # Normalize the blurred weights.\n  w_blur = w_blur / jnp.sum(w_blur, axis=-1, keepdims=True)\n  # Resample the blurred weights.\n  w_resampled = resample(tq, t, w_blur, use_avg=False)\n  return w_resampled"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  utils.assert_valid_stepfun(t, w)\n  t_blur, w_blur = linspline.convolve_with_filter1d(\n      t, w, blur_halfwidth, padding='edge'\n  )\n  w_blur = jnp.maximum(w_blur, 0)\n  w_blur /= jnp.sum(w_blur, axis=-1, keepdims=True)\n  w_resampled = resample(tq, t_blur, w_blur, use_avg=False)\n  return w_resampled"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  utils.assert_valid_stepfun(t, w)\n  if blur_halfwidth == 0:\n    return resample(tq, t, w, use_avg=False)\n\n  # Compute the PDF.\n  wp = weight_to_pdf(t, w)\n\n  # Blur the PDF.\n  wp_blurred = linspline.blur(t, wp, blur_halfwidth)\n\n  # Resample the PDF.\n  wp_resampled = resample(tq, t, wp_blurred, use_avg=False)\n\n  # Convert the resampled PDF back to weights.\n  w_resampled = pdf_to_weight(tq, wp_resampled)\n\n  return w_resampled"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Compute the PDF and CDF for each weight vector.\n  p = weight_to_pdf(t, w)\n  cw = integrate_weights(p)\n  # Blur the PDF.\n  p_blur = linspline.blur(p, blur_halfwidth)\n  # Resample the blurred PDF.\n  cw_blur = integrate_weights(p_blur)\n  # Interpolate into the inverse CDF.\n  t_new = math.sorted_interp(cw_blur, cw, t, utils.device_is_tpu())\n  # Convert the resampled time points back to weights.\n  w_new = weight_to_pdf(t_new, jnp.ones_like(t_new))\n  return w_new"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a PDF\n  p = weight_to_pdf(t, w)\n\n  # Blur the PDF using a Gaussian filter\n  blurred_p = linspline.gaussian_filter1d(p, blur_halfwidth, 3)\n\n  # Resample the blurred PDF to match the new time points\n  resampled_p = resample(tq, t, blurred_p, use_avg=False)\n\n  # Convert the resampled PDF back to weights\n  resampled_w = pdf_to_weight(tq, resampled_p)\n\n  return resampled_w"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  utils.assert_valid_stepfun(t, w)\n  p = weight_to_pdf(t, w)\n  p_blur = linspline.blur(p, blur_halfwidth)\n  w_blur = pdf_to_weight(t, p_blur)\n  w_resampled = resample(tq, t, w_blur, use_avg=False)\n  return w_resampled\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Convert to a PDF, blur, and convert back to weights.\n  p = weight_to_pdf(t, w)\n  p_blur = linspline.blur(p, blur_halfwidth)\n  w_blur = pdf_to_weight(t, p_blur)\n  return resample(tq, t, w_blur)"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  utils.assert_valid_stepfun(t, w)\n  if blur_halfwidth > 0:\n    tp, vp = linspline.linspace_resample(t, w, blur_halfwidth, 100)\n    wp = jnp.diff(tp) * vp\n    wp = jnp.pad(wp, [(0, 0)] * (w.ndim - 1) + [(1, 1)], mode='edge')\n    wp = jnp.diff(wp, axis=-1)\n    wp /= jnp.sum(wp, axis=-1, keepdims=True)\n    wp = jnp.clip(wp, 0, jnp.inf)\n  else:\n    wp = w\n  wp = jnp.pad(wp, [(0, 0)] * (w.ndim - 1) + [(1, 1)], mode='edge')\n  wp = jnp.diff(wp, axis=-1)\n  wp = jnp.clip(wp, 0, jnp.inf)\n  wp = resample(tq, tp, wp, use_avg=False)\n  return wp\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(transform, to_homogeneous(vectors)))"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(transform, to_homogeneous(vectors)))"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(transform, to_homogeneous(vectors)))"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(transform, to_homogeneous(vectors)))"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(transform, to_homogeneous(vectors)))"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(transform, to_homogeneous(vectors)))"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(transform, to_homogeneous(vectors)))"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(transform, to_homogeneous(vectors)))"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(transform, to_homogeneous(vectors)))"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(transform, to_homogeneous(vectors)))"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(transform, to_homogeneous(vectors)))"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(transform, to_homogeneous(vectors)))"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(transform, to_homogeneous(vectors)))"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(transform, to_homogeneous(vectors)))"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(transform, to_homogeneous(vectors)))"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(transform, to_homogeneous(vectors)))"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(transform, to_homogeneous(vectors)))"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  v = to_homogeneous(vectors)\n  v = matmul(transform, v[Ellipsis, None])[Ellipsis, 0]\n  return from_homogeneous(v)"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  v = to_homogeneous(vectors)\n  v = matmul(transform, v[Ellipsis, None])[Ellipsis, 0]\n  return from_homogeneous(v)"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  v = to_homogeneous(vectors)\n  return from_homogeneous(matmul(transform, v))"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n  tp = jnp.concatenate([tp[Ellipsis, :1], tp, tp[Ellipsis, -1:]], axis=-1)\n  vp = jnp.concatenate([vp[Ellipsis, :1], vp, vp[Ellipsis, -1:]], axis=-1)\n  (i0, i1), ((vp0, vp1),) = math.sorted_lookup(t, tp, (vp,), utils.device_is_tpu())\n  if use_avg:\n    w = jnp.maximum(tp[Ellipsis, i1] - tp[Ellipsis, i0], 0)\n    v = (vp0 * w + vp1 * w) / 2\n  else:\n    v = jnp.where(i1 == i0, vp0, vp0 + vp1)\n  return v"}
{"namespace": "stepfun.resample", "completion": "  # Check if the input tensors have the same shape\n  if t.shape != tp.shape or t.shape != vp.shape:\n    raise ValueError(\"The input tensors must have the same shape.\")\n\n  # Check if the input tensors have the correct number of dimensions\n  if t.ndim != 1 or tp.ndim != 1 or vp.ndim != 1:\n    raise ValueError(\"The input tensors must have only one dimension.\")\n\n  # Check if the input tensors are sorted in ascending order\n  if not jnp.all(jnp.diff(tp) > 0):\n    raise ValueError(\"The input tensors must be sorted in ascending order.\")\n\n  # Check if the input tensors have at least two elements\n  if t.size < 2 or tp.size < 2 or vp.size < 2:\n    raise ValueError(\"The input tensors must have at least two elements.\")\n\n  # Check if the input tensors have the same dtype\n  if t.dtype != tp.dtype or t.dtype != vp.dtype:\n    raise ValueError(\"The input tensors must have the same dtype.\")\n\n  # Check if the input tensors have the same device\n  if t.device != tp.device or t.device != vp.device:\n    raise ValueError(\"The input tensors must have the same device.\")\n\n  # Check if the input tensors have the same backend\n  if t.backend != tp.backend or t.backend != vp.backend:\n    raise ValueError(\"The input tensors must have the same backend.\")\n\n  # Check if the input tensors have the same requires_grad\n  if t.requires_grad != tp.requires_grad or t.requires_grad != vp.requires_grad:\n    raise ValueError(\"The input tensors must have the same requires_grad.\")\n\n  # Check if the input tensors have the same is_contiguous\n  if t.is_contiguous != tp.is_contiguous or t"}
{"namespace": "stepfun.resample", "completion": "  # Find the indices of the original time points that are within each interval in t.\n  inds = jnp.searchsorted(tp, t[Ellipsis, :-1], side='right')\n  inds = jnp.stack([inds, inds - 1], axis=-1)\n\n  # Compute the width of each interval in t.\n  widths = jnp.diff(t, axis=-1)\n\n  # Compute the sum of values for each interval in t.\n  v = jnp.take(vp, inds, axis=-1)\n  v = jnp.where(v[Ellipsis, 0] == v[Ellipsis, 1], v[Ellipsis, 0], 0)\n  v = jnp.sum(v, axis=-1)\n\n  # If use_avg is True, compute the average value for each interval in t.\n  if use_avg:\n    v = v / widths\n\n  return v"}
{"namespace": "stepfun.resample", "completion": "  # Calculate the widths of the original intervals\n  w_orig = jnp.diff(tp, axis=-1)\n\n  # Calculate the widths of the new intervals\n  w_new = jnp.diff(t, axis=-1)\n\n  # Calculate the overlaps between the original and new intervals\n  overlap = jnp.maximum(0, jnp.minimum(tp[..., 1:], t[..., 1:, None]) - jnp.maximum(tp[..., :-1], t[..., :-1, None]))\n\n  # Calculate the weights for each overlap\n  if use_avg:\n    weights = overlap / w_new\n  else:\n    weights = overlap\n\n  # Calculate the resampled values\n  v = jnp.sum(weights * vp[..., None], axis=-1)\n\n  # If using averaging, normalize the resampled values by the widths of the new intervals\n  if use_avg:\n    v /= w_new\n\n  return v"}
{"namespace": "stepfun.resample", "completion": "  # Compute the width of each interval in `t`.\n  dt = jnp.diff(t, axis=-1)\n\n  # Compute the width of each interval in `tp`.\n  dtp = jnp.diff(tp, axis=-1)\n\n  # Compute the cumulative width of `tp`.\n  cdtp = jnp.concatenate([jnp.zeros_like(dtp[..., :1]), jnp.cumsum(dtp, axis=-1)], axis=-1)\n\n  # Compute the start and end indices of each interval in `t` in `tp`.\n  i0 = jnp.searchsorted(cdtp, t[..., :-1], side='right') - 1\n  i1 = jnp.searchsorted(cdtp, t[..., 1:], side='right') - 1\n\n  # Compute the width of each interval in `t` that overlaps with each interval in `tp`.\n  dti0 = jnp.maximum(0, t[..., :-1] - tp[..., i0])\n  dti1 = jnp.maximum(0, tp[..., i1] - t[..., 1:])\n\n  # Compute the width of each interval in `tp` that overlaps with each interval in `t`.\n  dtpi0 = jnp.maximum(0, tp[..., i0 + 1] - t[..., :-1])\n  dtpi1 = jnp.maximum(0, t[..., 1:] - tp[..., i1])\n\n  # Compute the width of each interval in `t` that is covered by each interval in `tp`.\n  dti = jnp.minimum(dt, dti0 + dti1)\n\n  # Compute the width of each interval in `tp` that is covered by each interval in `t`.\n  dtpi = jnp.minimum(dtp, dtpi0 + dtpi1)\n\n  # Compute the contribution of each interval in `tp"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n\n  if use_avg:\n    # Compute the average value for each interval in tp.\n    vp_avg = (vp[Ellipsis, 1:] + vp[Ellipsis, :-1]) / 2\n    # Compute the width of each interval in tp.\n    wp = jnp.diff(tp, axis=-1)\n    # Compute the average value for each interval in t.\n    v_avg = jnp.interp(t, tp, vp_avg)\n    # Compute the width of each interval in t.\n    w = jnp.diff(t, axis=-1)\n    # Compute the average value for each interval in t, weighted by the width of each interval in tp.\n    v = v_avg * w / jnp.interp(t, tp, wp)\n  else:\n    # Compute the value for each interval in tp.\n    vp_sum = vp[Ellipsis, 1:] + vp[Ellipsis, :-1]\n    # Compute the value for each interval in t.\n    v = jnp.interp(t, tp, vp_sum)\n\n  return v"}
{"namespace": "stepfun.resample", "completion": "  # Compute the width of each interval in `t`.\n  t_widths = jnp.diff(t, axis=-1)\n\n  # Compute the width of each interval in `tp`.\n  tp_widths = jnp.diff(tp, axis=-1)\n\n  # Compute the total width of each interval in `t`.\n  t_total_width = jnp.sum(t_widths, axis=-1)\n\n  # Compute the total width of each interval in `tp`.\n  tp_total_width = jnp.sum(tp_widths, axis=-1)\n\n  # Compute the factor by which to scale the values in `vp`.\n  scale_factor = t_total_width / tp_total_width\n\n  # Scale the values in `vp`.\n  vp_scaled = vp * scale_factor\n\n  # Compute the width of each interval in `tp`.\n  tp_widths = jnp.diff(tp, axis=-1)\n\n  # Compute the cumulative sum of the widths of each interval in `tp`.\n  tp_cum_widths = jnp.cumsum(tp_widths, axis=-1)\n\n  # Compute the cumulative sum of the scaled values in `vp`.\n  vp_cum_scaled = jnp.cumsum(vp_scaled, axis=-1)\n\n  # Compute the cumulative sum of the scaled values in `vp` divided by the cumulative sum of the widths of each interval in `tp`.\n  vp_cum_scaled_div_cum_widths = vp_cum_scaled / tp_cum_widths\n\n  # Compute the cumulative sum of the scaled values in `vp` divided by the cumulative sum of the widths of each interval in `tp`.\n  vp_cum_scaled_div_cum_widths = jnp.concatenate([jnp.zeros_like(vp_"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n  if use_avg:\n    return resample_avg(t, tp, vp)\n  else:\n    return resample_sum(t, tp, vp)\n\n"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n\n  # Compute the width of each interval in tp.\n  dt = tp[Ellipsis, 1:] - tp[Ellipsis, :-1]\n\n  # Compute the indices of the intervals in tp that overlap with each interval in t.\n  indices = jnp.searchsorted(tp, t, side='right') - 1\n  indices = jnp.clip(indices, 0, tp.shape[-1] - 2)\n\n  # Compute the start and end points of each interval in t.\n  t_start = t[Ellipsis, :-1]\n  t_end = t[Ellipsis, 1:]\n\n  # Compute the start and end points of each interval in tp that overlaps with each interval in t.\n  tp_start = tp[Ellipsis, indices]\n  tp_end = tp[Ellipsis, indices + 1]\n\n  # Compute the overlap between each interval in t and the interval in tp that overlaps with it.\n  overlap_start = jnp.maximum(t_start, tp_start)\n  overlap_end = jnp.minimum(t_end, tp_end)\n  overlap = jnp.maximum(0, overlap_end - overlap_start)\n\n  # Compute the weight of each interval in tp that overlaps with each interval in t.\n  if use_avg:\n    # Compute the width of each interval in t.\n    t_width = t_end - t_start\n    # Compute the weight of each interval in tp that overlaps with each interval in t.\n    weight = overlap / t_width\n  else:\n    # Compute the weight of each interval in tp that overlaps with each interval in t.\n    weight = overlap / dt[Ellipsis, indices]\n\n  # Compute the value of each interval in tp that overlaps with each interval in t.\n  value = vp[Ellipsis, indices]\n\n  # Comput"}
{"namespace": "stepfun.resample", "completion": "  # Compute the width of each interval in t\n  widths = jnp.diff(t, axis=-1)\n\n  # Compute the width of each interval in tp\n  widths_tp = jnp.diff(tp, axis=-1)\n\n  # Compute the cumulative sum of the widths of tp\n  cum_widths_tp = jnp.cumsum(widths_tp, axis=-1)\n\n  # Compute the total width of tp\n  total_width_tp = cum_widths_tp[..., -1]\n\n  # Compute the normalized cumulative sum of the widths of tp\n  cum_widths_tp_norm = cum_widths_tp / total_width_tp\n\n  # Compute the cumulative sum of the values of vp\n  cum_vp = jnp.cumsum(vp, axis=-1)\n\n  # Compute the total sum of vp\n  total_vp = cum_vp[..., -1]\n\n  # Compute the normalized cumulative sum of the values of vp\n  cum_vp_norm = cum_vp / total_vp\n\n  # Compute the normalized cumulative sum of the widths of t\n  cum_widths_t_norm = jnp.cumsum(widths, axis=-1) / jnp.sum(widths, axis=-1, keepdims=True)\n\n  # Compute the indices of the intervals in tp corresponding to each interval in t\n  indices = jnp.searchsorted(cum_widths_tp_norm, cum_widths_t_norm)\n\n  # Compute the values of vp corresponding to the indices\n  vp_indices = jnp.take_along_axis(vp, indices, axis=-1)\n\n  # Compute the values of the resampled step function\n  if use_avg:\n    v = vp_indices * widths / total_width_tp\n  else:\n    v = vp_indices * widths\n\n  return v"}
{"namespace": "stepfun.resample", "completion": "  # Compute the widths of each interval in the original step function\n  widths = jnp.diff(tp, axis=-1)\n\n  # Compute the widths of each interval in the new step function\n  new_widths = jnp.diff(t, axis=-1)\n\n  # Compute the indices of the intervals in the original step function that each new interval overlaps with\n  indices = jnp.searchsorted(tp, t, side='left') - 1\n\n  # Compute the widths of each overlapping interval\n  overlapping_widths = jnp.minimum(widths, new_widths)\n\n  # Compute the values of the resampled step function\n  if use_avg:\n    # Compute the average value of the step function for each interval in the new step function\n    values = jnp.sum(vp[indices] * overlapping_widths, axis=-1) / jnp.sum(overlapping_widths, axis=-1)\n  else:\n    # Compute the sum of the values of the step function for each interval in the new step function\n    values = jnp.sum(vp[indices] * overlapping_widths, axis=-1)\n\n  return values"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n\n  # Compute the width of each interval in tp.\n  tp_diff = jnp.diff(tp, axis=-1)\n\n  # Compute the width of each interval in t.\n  t_diff = jnp.diff(t, axis=-1)\n\n  # Compute the width of each interval in tp, scaled by the width of the corresponding interval in t.\n  tp_diff_scaled = tp_diff * t_diff[:, jnp.newaxis] / jnp.sum(tp_diff, axis=-1, keepdims=True)\n\n  # Compute the resampled values.\n  if use_avg:\n    # For averaging method, we simply sum the values of vp for each interval in tp,\n    # and divide by the width of the corresponding interval in t.\n    v = jnp.sum(vp[..., :, jnp.newaxis] * tp_diff_scaled[..., jnp.newaxis, :], axis=-1) / t_diff\n  else:\n    # For summation method, we simply sum the values of vp for each interval in tp.\n    v = jnp.sum(vp[..., :, jnp.newaxis] * tp_diff_scaled[..., jnp.newaxis, :], axis=-1)\n\n  return v"}
{"namespace": "stepfun.resample", "completion": "  # Compute the weights of the new intervals\n  w_new = jnp.diff(t)\n\n  # Compute the width of each original interval\n  w_old = jnp.diff(tp)\n\n  # Compute the indices of the original intervals that each new interval overlaps with\n  i = jnp.searchsorted(tp, t, side='right') - 1\n  i = jnp.clip(i, 0, len(vp) - 1)\n\n  # Compute the overlapping intervals for each new interval\n  t_overlap = jnp.minimum(tp[i + 1], t[..., 1:, None]) - jnp.maximum(tp[i], t[..., :-1, None])\n\n  # Compute the values of the resampled step function for each new interval\n  v_new = jnp.where(\n      use_avg,\n      jnp.sum(vp[i] * t_overlap / w_old[i], axis=-1),\n      jnp.sum(vp[i] * w_new, axis=-1),\n  )\n\n  return v_new"}
{"namespace": "stepfun.resample", "completion": "  # Compute the width of each interval in tp.\n  dt = tp[1:] - tp[:-1]\n\n  # Compute the width of each interval in t.\n  dt_new = t[1:] - t[:-1]\n\n  # Compute the indices of the intervals in tp that correspond to each interval in t.\n  idx_left = jnp.searchsorted(tp, t[:-1], side='right')\n  idx_right = jnp.searchsorted(tp, t[1:], side='right')\n\n  # Compute the width of each interval in tp that corresponds to each interval in t.\n  dt_intersect = jnp.clip(jnp.minimum(tp[idx_right], t[1:, None]) - jnp.maximum(tp[idx_left - 1], t[:-1, None]), 0, None)\n\n  # Compute the width of each interval in t that corresponds to each interval in tp.\n  dt_intersect_new = dt_intersect / dt_new[:, None]\n\n  # Compute the values of the step function at the endpoints of each interval in t.\n  v_left = vp[idx_left - 1]\n  v_right = vp[idx_right - 1]\n\n  # Compute the values of the step function for each interval in t.\n  v_intersect = dt_intersect_new * (v_right - v_left)\n\n  # Sum the values of the step function for each interval in t.\n  if use_avg:\n    v = jnp.sum(v_intersect, axis=-1) / dt\n  else:\n    v = jnp.sum(v_intersect, axis=-1)\n\n  return v"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n  tp = jnp.concatenate([tp[Ellipsis, :1], tp, tp[Ellipsis, -1:]], axis=-1)\n  vp = jnp.concatenate([vp[Ellipsis, :1], vp, vp[Ellipsis, -1:]], axis=-1)\n  t = jnp.concatenate([t[Ellipsis, :1], t, t[Ellipsis, -1:]], axis=-1)\n\n  # Compute the widths of each interval.\n  dt = jnp.diff(t, axis=-1)\n\n  # Compute the widths of each original step function interval.\n  dtp = jnp.diff(tp, axis=-1)\n\n  # Compute the relative widths of each original step function interval.\n  dtp_rel = dtp / dt\n\n  # Compute the relative widths of each new interval.\n  dt_rel = jnp.diff(t, axis=-1) / dt\n\n  # Compute the relative positions of each original step function interval.\n  tp_rel = jnp.concatenate([jnp.zeros_like(tp[Ellipsis, :1]), tp_rel], axis=-1)\n\n  # Compute the relative positions of each new interval.\n  t_rel = jnp.concatenate([jnp.zeros_like(t[Ellipsis, :1]), t_rel], axis=-1)\n\n  # Compute the relative positions of each new interval.\n  t_rel = jnp.concatenate([jnp.zeros_like(t[Ellipsis, :1]), t_rel], axis=-1)\n\n  # Compute the relative positions of each original step function interval.\n  tp_rel = jnp.concatenate([jnp.zeros_like(tp[Ellipsis, :1]), tp_rel], axis=-1)\n\n  # Compute"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n  utils.assert_valid_stepfun(t, jnp.zeros_like(t))\n\n  # Compute the weights of the resampled step function\n  w = jnp.zeros_like(t)\n  if use_avg:\n    # Compute the width of each interval in t\n    dt = jnp.diff(t, axis=-1)\n    # Compute the width of each interval in tp\n    dtp = jnp.diff(tp, axis=-1)\n    # Compute the weights for each interval in t by summing the weights of the intervals in tp that overlap with the interval in t\n    w = jnp.sum(jnp.where((t[..., None] > tp[..., None, :]) & (t[..., None] < tp[..., None, 1:]), dtp, 0), axis=-1) / dt\n  else:\n    # Compute the weights for each interval in t by summing the values of the intervals in vp that overlap with the interval in t\n    w = jnp.sum(jnp.where((t[..., None] > tp[..., None, :]) & (t[..., None] < tp[..., None, 1:]), vp[..., None], 0), axis=-1)\n\n  return w"}
{"namespace": "stepfun.resample", "completion": "  # Compute the widths of each interval in tp\n  widths = tp[1:] - tp[:-1]\n\n  # Compute the widths of each interval in t\n  t_widths = t[1:] - t[:-1]\n\n  # Compute the indices of the intervals in tp that overlap with each interval in t\n  tp_indices = jnp.searchsorted(tp, t)\n\n  # Compute the indices of the intervals in t that overlap with each interval in tp\n  t_indices = jnp.searchsorted(t, tp)\n\n  # Compute the indices of the intervals in tp that are completely inside each interval in t\n  tp_indices_in = tp_indices[1:] - tp_indices[:-1]\n\n  # Compute the indices of the intervals in t that are completely inside each interval in tp\n  t_indices_in = t_indices[1:] - t_indices[:-1]\n\n  # Compute the indices of the intervals in tp that are completely outside each interval in t\n  tp_indices_out = tp_indices[1:] - tp_indices[:-1] - tp_indices_in\n\n  # Compute the indices of the intervals in t that are completely outside each interval in tp\n  t_indices_out = t_indices[1:] - t_indices[:-1] - t_indices_in\n\n  # Compute the indices of the intervals in tp that are partially inside each interval in t\n  tp_indices_part = tp_indices_in - tp_indices_out\n\n  # Compute the indices of the intervals in t that are partially inside each interval in tp\n  t_indices_part = t_indices_in - t_indices_out\n\n  # Compute the indices of the intervals in tp that are partially outside each interval in t\n  tp_indices_part_out = tp_indices_part - tp_indices_in\n\n  # Compute the indices of"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n  if not use_avg:\n    # Compute the distance between each pair of consecutive time points.\n    dt = tp[Ellipsis, 1:] - tp[Ellipsis, :-1]\n    # Compute the cumulative sum of the distance.\n    cum_dt = jnp.cumsum(dt, axis=-1)\n    # Prepend a zero to the cumulative sum for the first time point.\n    cum_dt = jnp.pad(cum_dt, ((0, 0), (1, 0)), mode='constant')\n    # Find the indices of the time points where the cumulative sum is less than or equal to the new time points.\n    idx = jnp.searchsorted(cum_dt, t[Ellipsis, :, None], side='right') - 1\n    # Compute the difference between the new time points and the time points at the corresponding indices.\n    dt_new = t[Ellipsis, :, None] - tp[Ellipsis, idx, None]\n    # Compute the resampled values by summing the corresponding values of the original step function.\n    v = jnp.sum(jnp.where(dt_new > 0, vp[Ellipsis, idx] * dt_new, 0), axis=-2)\n  else:\n    # Compute the distance between each pair of consecutive time points.\n    dt = tp[Ellipsis, 1:] - tp[Ellipsis, :-1]\n    # Compute the cumulative sum of the distance.\n    cum_dt = jnp.cumsum(dt, axis=-1)\n    # Prepend a zero to the cumulative sum for the first time point.\n    cum_dt = jnp.pad(cum_dt, ((0, 0), (1, 0)), mode='constant')\n    # Find the indices of the time points where the cumulative sum is less than or equal to the new time points.\n    idx = jnp.searchsorted(c"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n\n  # Compute the weights for each interval in t.\n  w = jnp.diff(t)\n\n  # Compute the indices of the intervals in tp that overlap with each interval in t.\n  i0 = jnp.searchsorted(tp, t[Ellipsis, :-1], side='right')\n  i1 = jnp.searchsorted(tp, t[Ellipsis, 1:], side='right')\n\n  # Compute the width of each interval in t.\n  dt = jnp.diff(t, axis=-1)\n\n  # Compute the width of each interval in tp that overlaps with each interval in t.\n  dtp = jnp.take(tp, i1, axis=-1) - jnp.take(tp, i0, axis=-1)\n\n  # Compute the overlap between each interval in t and each interval in tp that overlaps with it.\n  overlap = jnp.minimum(dt, dtp)\n\n  # Compute the weights for each interval in tp that overlaps with each interval in t.\n  wp = overlap / dtp\n\n  # Compute the values of the step function at the intervals in tp that overlap with each interval in t.\n  vp_overlap = jnp.take(vp, i0, axis=-1) * wp\n\n  # Sum the values of the step function at the intervals in tp that overlap with each interval in t, weighted by the width of each interval in t.\n  v = jnp.sum(vp_overlap, axis=-1) / w\n\n  # If use_avg is True, return the average value of the step function over each interval in t, weighted by the width of each interval in t.\n  if use_avg:\n    v = v / w\n\n  return v"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n  if not jnp.all(jnp.diff(t) > 0):\n    raise ValueError('t must be sorted and increasing')\n\n  # Compute the width of each interval in the original step function.\n  tp_widths = jnp.diff(tp)\n\n  # Compute the indices of the intervals in the original step function that\n  # overlap with each interval in the new step function.\n  t_indices = jnp.searchsorted(tp, t, side='left')\n  t_indices = jnp.clip(t_indices, 0, tp.shape[-1] - 1)\n\n  # Compute the width of each interval in the new step function.\n  t_widths = jnp.diff(t)\n\n  # Compute the width of each interval in the original step function that\n  # overlaps with each interval in the new step function.\n  tp_widths_overlap = jnp.take(tp_widths, t_indices)\n\n  # Compute the width of each interval in the new step function that overlaps\n  # with each interval in the original step function.\n  t_widths_overlap = jnp.clip(t_widths, 0, tp_widths_overlap)\n\n  # Compute the value of each interval in the new step function.\n  if use_avg:\n    # Compute the weighted average of the values of the intervals in the\n    # original step function that overlap with each interval in the new step\n    # function.\n    v_overlap = jnp.take(vp, t_indices) * t_widths_overlap\n    v = jnp.cumsum(v_overlap, axis=-1)\n    v = v / jnp.clip(t_widths_overlap, a_min=1, a_max=None)\n  else:\n    # Compute the sum of the values of the intervals in the original step\n    # function that overlap with each interval in the new step function.\n    v_overlap"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance of the coordinates.\n  scaled_mean = mean / (2 ** (max_deg - 1))\n  scaled_var = var / (2 ** (2 * max_deg - 2))\n\n  # Concatenate the scaled mean and variance.\n  scaled_vars = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply a sinusoidal encoding to the scaled variables.\n  encoded_vars = jnp.sin(jnp.concatenate([scaled_vars, 2 * scaled_vars], axis=-1))\n\n  return encoded_vars"}
{"namespace": "coord.integrated_pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = mean.shape[:-1] + (-1,)\n  scaled_mean = mean[Ellipsis, None, :] * scales[:, None]\n  scaled_mean = jnp.reshape(scaled_mean, shape)\n  scaled_var = var[Ellipsis, None] * scales**2\n  scaled_var = jnp.reshape(scaled_var, shape)\n  # Note that we're not using safe_sin, unlike IPE.\n  # (..., s*c + s*c).\n  four_feat = jnp.sin(\n      jnp.concatenate([scaled_mean, scaled_mean + 0.5 * jnp.pi], axis=-1)\n  )\n  return jnp.concatenate([four_feat, scaled_var], axis=-1)"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance of the coordinates.\n  scaled_mean = mean * 2**min_deg\n  scaled_var = var * 2**(2 * min_deg)\n\n  # Concatenate the scaled mean and variance.\n  scaled_vars = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply the sinusoidal encoding to the scaled variables.\n  encoded_vars = jnp.sin(jnp.concatenate([scaled_vars, scaled_vars + 0.5 * jnp.pi], axis=-1))\n\n  # Return the encoded variables.\n  return encoded_vars"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance of the coordinates.\n  scaled_mean = mean / (2 ** (max_deg - 1))\n  scaled_var = var / (2 ** (2 * max_deg - 2))\n\n  # Concatenate the scaled mean and variance.\n  scaled_vars = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply a sinusoidal encoding to the concatenated variables.\n  encoded_vars = jnp.sin(jnp.concatenate([\n      jnp.zeros_like(scaled_vars[..., :1]),\n      jnp.cumsum(scaled_vars[..., :-1], axis=-1)\n  ], axis=-1))\n\n  return encoded_vars"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance.\n  scaled_mean = mean / (2 ** (max_deg - 1))\n  scaled_var = var / (2 ** (2 * max_deg - 2))\n\n  # Concatenate the scaled mean and variance.\n  scaled_vars = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply the sinusoidal encoding.\n  four_feat = jnp.sin(\n      jnp.concatenate([scaled_vars, scaled_vars + 0.5 * jnp.pi], axis=-1)\n  )\n\n  return four_feat"}
{"namespace": "coord.integrated_pos_enc", "completion": "  mean = mean[Ellipsis, None]\n  var = var[Ellipsis, None]\n  scales = 2**jnp.arange(min_deg, max_deg)\n  shape = mean.shape[:-1] + (-1,)\n  scaled_mean = jnp.reshape(mean * scales, shape)\n  scaled_var = jnp.maximum(0, jnp.reshape(var * scales**2, shape))\n  return jnp.concatenate([scaled_mean, scaled_var], axis=-1)"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance of the coordinates.\n  scaled_mean = mean * (2 ** min_deg)\n  scaled_var = var * (2 ** (2 * min_deg))\n\n  # Concatenate the scaled mean and variance.\n  scaled_vars = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply the sinusoidal encoding.\n  encoded_vars = jnp.sin(scaled_vars)\n\n  return encoded_vars"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance of the coordinates.\n  scale = 2 ** jnp.linspace(min_deg, max_deg, max_deg - min_deg)\n  scaled_mean = mean[Ellipsis, None] * scale[None, Ellipsis]\n  scaled_var = var[Ellipsis, None] * scale[None, Ellipsis] ** 2\n\n  # Concatenate the scaled mean and variance.\n  scaled_vars = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply a sinusoidal encoding to the concatenated variables.\n  encoded_vars = jnp.sin(scaled_vars)\n\n  return encoded_vars"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance of the coordinates to the range [0, 1].\n  mean_scaled = (mean - min_deg) / (max_deg - min_deg)\n  var_scaled = var / (max_deg - min_deg) ** 2\n\n  # Concatenate the scaled mean and variance along the last axis.\n  scaled_vars = jnp.concatenate([mean_scaled[Ellipsis, None], var_scaled[Ellipsis, None]], axis=-1)\n\n  # Apply a sinusoidal encoding to the scaled variables.\n  encoded_vars = jnp.sin(jnp.pi * scaled_vars)\n\n  return encoded_vars"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance to the range of valid values for the encoding.\n  scale = jnp.exp(\n      jnp.linspace(jnp.log(min_deg), jnp.log(max_deg), num=mean.shape[-1])\n  )\n  scaled_mean = mean * scale\n  scaled_var = var * scale**2\n\n  # Compute the encoded variables.\n  encoded_vars = jnp.concatenate(\n      [\n          jnp.sin(jnp.concatenate([scaled_mean, scaled_mean + 0.5 * jnp.pi], axis=-1)),\n          jnp.cos(jnp.concatenate([scaled_mean, scaled_mean + 0.5 * jnp.pi], axis=-1)),\n      ],\n      axis=-1,\n  )\n\n  return encoded_vars"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance to be in the range [0, 1].\n  mean_scaled = (mean - jnp.min(mean)) / (jnp.max(mean) - jnp.min(mean))\n  var_scaled = var / jnp.max(var)\n\n  # Concatenate the scaled mean and variance.\n  x = jnp.concatenate([mean_scaled, var_scaled], axis=-1)\n\n  # Apply the sinusoidal encoding.\n  for i in range(min_deg, max_deg):\n    encoding = jnp.sin(2.0**i * x)\n    x = jnp.concatenate([x, encoding], axis=-1)\n\n  return x"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance by the maximum magnitude of the mean.\n  max_mag = jnp.max(jnp.abs(mean), axis=-1, keepdims=True)\n  scaled_mean = mean / max_mag\n  scaled_var = var / max_mag**2\n\n  # Concatenate the scaled mean and variance along the last axis.\n  scaled_mean_var = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply a sinusoidal encoding to the scaled mean and variance.\n  encoded_vars = pos_enc(scaled_mean_var, min_deg, max_deg)\n\n  return encoded_vars"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance of the coordinates to the range [0, 1].\n  mean_scaled = (mean - jnp.min(mean, axis=-1, keepdims=True)) / (\n      jnp.max(mean, axis=-1, keepdims=True) - jnp.min(mean, axis=-1, keepdims=True)\n  )\n  var_scaled = var / jnp.max(var, axis=-1, keepdims=True)\n\n  # Concatenate the scaled mean and variance along the last axis.\n  x = jnp.concatenate([mean_scaled, var_scaled], axis=-1)\n\n  # Apply a sinusoidal encoding to the scaled mean and variance.\n  encoded_x = pos_enc(x, min_deg, max_deg)\n\n  return encoded_x"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance of the coordinates to be within the range of the encoding.\n  scaled_mean = mean * (2**max_deg)\n  scaled_var = var * (2**(2 * max_deg))\n\n  # Concatenate the scaled mean and variance to form the input to the encoding.\n  encoded_vars = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply the sinusoidal encoding to the input.\n  encoded_vars = jnp.sin(encoded_vars)\n\n  return encoded_vars"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance to be in the range [0, 1]\n  mean_scaled = (mean - min_deg) / (max_deg - min_deg)\n  var_scaled = var / (max_deg - min_deg) ** 2\n\n  # Concatenate the mean and variance\n  x = jnp.concatenate([mean_scaled, var_scaled], axis=-1)\n\n  # Apply the sinusoidal encoding\n  encoded = jnp.sin(jnp.pi * (x[Ellipsis, None, :] + 2 * jnp.arange(0, x.shape[-1])))\n\n  return encoded"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # The scaling factors for the sinusoidal encoding.\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = mean.shape[:-1] + (-1,)\n  scaled_mean = mean[Ellipsis, None, :] * scales[:, None]\n  scaled_mean = jnp.reshape(scaled_mean, shape)\n  scaled_var = var[Ellipsis, None, :] * scales[:, None] ** 2\n  scaled_var = jnp.reshape(scaled_var, shape)\n\n  # Compute the expected sin of the scaled mean and variance.\n  expected_sin_mean = expected_sin(scaled_mean, scaled_var)\n\n  # Concatenate the mean and variance to create the encoded variables.\n  encoded_vars = jnp.concatenate([scaled_mean, expected_sin_mean], axis=-1)\n\n  return encoded_vars"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance to be in [0, 1].\n  mean = (mean + 1) / 2\n  var = var / 4\n\n  # Compute the scaling factors for each degree.\n  scales = 2 ** jnp.arange(min_deg, max_deg)\n\n  # Compute the encoded variables using the scaled mean and variance.\n  x = mean[Ellipsis, None] * scales[:, None]  # [..., s, c]\n  x = jnp.reshape(x, x.shape[:-2] + (-1,))  # [..., s*c]\n\n  # Compute the encoded variables using the sinusoidal encoding.\n  y = jnp.sin(jnp.concatenate([x, x + 0.5 * jnp.pi], axis=-1))  # [..., s*c*2]\n\n  return y"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance of the coordinates to be within a range of\n  # [-1, 1] for the encoding to work properly.\n  scale = jnp.exp(\n      jnp.linspace(jnp.log(2.0), jnp.log(2.0**(max_deg - 1)), max_deg - min_deg)\n  )\n  shape = mean.shape + (len(scale),)\n  scaled_mean = jnp.reshape(mean[Ellipsis, None] * scale[None, :], shape)\n  scaled_var = jnp.reshape(var[Ellipsis, None] * scale[None, :] ** 2, shape)\n\n  # Compute the encoded variables by concatenating the scaled mean and variance\n  # and applying a sinusoidal encoding.\n  encoded_vars = jnp.concatenate(\n      [\n          jnp.sin(scaled_mean),\n          jnp.cos(scaled_mean),\n          jnp.sin(scaled_var),\n          jnp.cos(scaled_var),\n      ],\n      axis=-1,\n  )\n\n  return encoded_vars"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance by powers of 2, starting at min_deg and\n  # ending at max_deg.\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = mean.shape[:-1] + (-1,)\n  scaled_mean = mean[Ellipsis, None, :] * scales[:, None]  # (..., s, c).\n  scaled_mean = jnp.reshape(scaled_mean, shape)  # (..., s*c).\n  scaled_var = var[Ellipsis, None, :] * scales[:, None] ** 2\n  scaled_var = jnp.reshape(scaled_var, shape)  # (..., s*c).\n\n  # Compute the mean of sin(x) for each scaled mean and variance.\n  # Note that the variance is scaled by 2^2d, which cancels out with the\n  # variance of the sin(x) term in the KL divergence.\n  mean_sin = expected_sin(scaled_mean, scaled_var)\n\n  # Concatenate the scaled mean and mean of sin(x) to form the encoded variables.\n  return jnp.concatenate([scaled_mean, mean_sin], axis=-1)"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance to be in the range [0, 1].\n  # This is necessary because the encoding is only valid for inputs in [0, 1].\n  mean = (mean - min_deg) / (max_deg - min_deg)\n  var = var / (max_deg - min_deg) ** 2\n\n  # Concatenate the mean and variance along the last axis.\n  x = jnp.concatenate([mean, var], axis=-1)\n\n  # Apply the sinusoidal encoding to the scaled inputs.\n  # The encoding is based on the sine function, which is periodic and has a range of [-1, 1].\n  # The encoding maps the input values to the range [-1, 1] using the sine function.\n  # The resulting values are then scaled to be in the range [0, 1] by adding 1 and dividing by 2.\n  x = jnp.sin(jnp.pi * (x[Ellipsis, None, :] + jnp.arange(4)[None, :, None]))\n  x = 0.5 * (x + 1.0)\n\n  # Flatten the last two dimensions of the encoded values.\n  # This is done to reduce the dimensionality of the encoded values from 2 to 1.\n  x = jnp.reshape(x, x.shape[:-2] + (-1,))\n\n  return x"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def integrated_dir_enc_fn(xyz, kappa_inv):\n    \"\"\"Function returning integrated directional encoding (IDE).\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n      kappa_inv: [..., 1] reciprocal of the concentration parameter of the von\n        Mises-Fisher distribution.\n\n    Returns:\n      An array with the resulting IDE.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Apply att"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def integrated_dir_enc_fn(xyz, kappa_inv):\n    \"\"\"Function returning integrated directional encoding (IDE).\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n      kappa_inv: [..., 1] reciprocal of the concentration parameter of the von\n        Mises-Fisher distribution.\n\n    Returns:\n      An array with the resulting IDE.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Apply att"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def integrated_dir_enc_fn(xyz, kappa_inv):\n    \"\"\"Function returning integrated directional encoding (IDE).\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n      kappa_inv: [..., 1] reciprocal of the concentration parameter of the von\n        Mises-Fisher distribution.\n\n    Returns:\n      An array with the resulting IDE.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Apply att"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"Function returning directional encoding (IDE).\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(sph_harms), jnp.imag(sph_harms)"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def integrated_dir_enc_fn(xyz, kappa_inv):\n    \"\"\"Function returning integrated directional encoding (IDE).\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n      kappa_inv: [..., 1] reciprocal of the concentration parameter of the von\n        Mises-Fisher distribution.\n\n    Returns:\n      An array with the resulting IDE.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Apply"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def integrated_dir_enc_fn(xyz, kappa_inv):\n    \"\"\"Function returning integrated directional encoding (IDE).\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n      kappa_inv: [..., 1] reciprocal of the concentration parameter of the von\n        Mises-Fisher distribution.\n\n    Returns:\n      An array with the resulting IDE.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Apply"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz, kappa_inv):\n    \"\"\"Function returning integrated directional encoding (IDE).\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n      kappa_inv: [..., 1] reciprocal of the concentration parameter of the von\n        Mises-Fisher distribution.\n\n    Returns:\n      An array with the resulting IDE.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Apply attenuation"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def integrated_dir_enc_fn(xyz, kappa_inv):\n    \"\"\"\n    Computes the integrated directional encoding (IDE) for a given set of 3D points and inverse concentration parameter of the von Mises-Fisher distribution.\n\n    Input-Output Arguments\n    :param xyz: Array. The 3D points for which to compute the integrated directional encoding.\n    :param kappa_inv: Array. The inverse concentration parameter of the von Mises-Fisher distribution.\n    :return: Array. The computed integrated directional encoding.\n\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def integrated_dir_enc_fn(xyz, kappa_inv):\n    \"\"\"\n    Evaluates the integrated directional encoding (IDE) for the given input points and inverse concentration parameter.\n\n    Input-Output Arguments\n    :param xyz: Array. The input points for which to compute the IDE.\n    :param kappa_inv: Array. The inverse concentration parameter of the von Mises-Fisher distribution.\n    :return: Array. The computed IDE values for the input points.\n\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def integrated_dir_enc_fn(xyz, kappa_inv):\n    \"\"\"\n    Internal function that evaluates the directional encoding for given inputs.\n\n    Input-Output Arguments\n    :param xyz: Float[..., 3]. Cartesian coordinates of directions to evaluate at.\n    :param kappa_inv: Float[..., 1]. Reciprocal of the concentration parameter of the von Mises-Fisher distribution.\n    :return: Float[..., 2 * l_max + 1]. The resulting directional encoding.\n\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def integrated_dir_enc_fn(xyz, kappa_inv):\n    \"\"\"\n    Evaluates the integrated directional encoding (IDE) for a given set of 3D points and inverse concentration parameter of the von Mises-Fisher distribution.\n\n    Input-Output Arguments\n    :param xyz: Float[..., 3]. The 3D points at which to evaluate the IDE.\n    :param kappa_inv: Float[..., 1]. The inverse concentration parameter of the von Mises-Fisher distribution.\n    :return: Float[..., 2 * l_max + 1]. The evaluated IDE for the given points.\n\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def integrated_dir_enc_fn(xyz, kappa_inv):\n    \"\"\"\n    Computes the integrated directional encoding (IDE) for a given set of 3D points (xyz) and inverse concentration parameter (kappa_inv).\n\n    Input-Output Arguments\n    :param xyz: Float array of shape [..., 3]. The 3D points for which to compute the IDE.\n    :param kappa_inv: Float array of shape [..., 1]. The inverse concentration parameter of the von Mises-Fisher distribution.\n    :return: Float array of shape [..., l_max + 1]. The computed IDE values for each input point.\n\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  integrated_dir_enc_fn = generate_ide_fn(deg_view)\n\n  def dir_enc_fn(xyz, viewdirs, kappa_inv):\n    \"\"\"Function returning directional encoding (IDE).\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n      viewdirs: [..., 3] array of viewing direction of the camera.\n      kappa_inv: [..., 1] reciprocal of the concentration parameter of the von\n        Mises-Fisher distribution.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    # Compute the integrated directional encoding.\n    ide = integrated_dir_enc_fn(xyz, kappa_inv)\n\n    # Apply the view dependency.\n    ide = ide * (viewdirs[Ellipsis, None, :] * xyz).sum(axis=-1, keepdims=True)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(ide), jnp.imag(ide)], axis=-1)\n\n  return dir_enc_fn"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"\n    Evaluates the directional encoding for a given 3D point (or points) using the generated integrated directional encoding function.\n\n    Input-Output Arguments\n    :param xyz: Float[..., 3]. The 3D point (or points) to evaluate the directional encoding for.\n    :return: Float[..., 2 * l_max + 1]. The directional encoding for the input 3D point (or points).\n\n    \"\"\"\n    integrated_dir_enc_fn = generate_ide_fn(deg_view)\n    kappa_inv = jnp.ones_like(xyz[Ellipsis, 0:1])\n    return integrated_dir_enc_fn(xyz, kappa_inv)\n\n  return dir_enc_fn"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def integrated_dir_enc_fn(xyz, kappa_inv):\n    \"\"\"\n    Function that evaluates the integrated directional encoding (IDE) for the given inputs.\n\n    Input-Output Arguments\n    :param xyz: A 3D point (or points) represented as a numpy array.\n    :param kappa_inv: The inverse of the concentration parameter of the von Mises-Fisher distribution.\n    :return: A numpy array representing the directional encoding of the input point(s).\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matm"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def integrated_dir_enc_fn(xyz, kappa_inv):\n    \"\"\"\n    Evaluates the integrated directional encoding (IDE) for given inputs.\n\n    Input-Output Arguments\n    :param xyz: Float tensor of shape [..., 3]. The 3D point(s) for which to compute the directional encoding.\n    :param kappa_inv: Float tensor of shape [..., 1]. The reciprocal of the concentration parameter of the von Mises-Fisher distribution.\n    :return: Float tensor of shape [..., l_max * l_max]. The computed directional encoding for each input point.\n\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Check if the degree of spherical harmonics is valid\n  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"\n    Evaluates the directional encoding for the given 3D point(s) `xyz`.\n\n    Input-Output Arguments\n    :param xyz: Array of shape [..., 3]. The 3D point(s) to evaluate the directional encoding for.\n    :return: Array of shape [..., 2 * (deg_view + 1)^2]. The computed directional encoding(s) for the input point(s).\n\n    \"\"\"\n    # Compute z Vandermonde matrix\n    vmz = jnp.concatenate([xyz[Ellipsis, 2:3] ** i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix\n    vmxy = jnp.concatenate([(xyz[Ellipsis, 0:1] + 1j * xyz[Ellipsis, 1:2]) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n   "}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def integrated_dir_enc_fn(xyz, kappa_inv):\n    \"\"\"\n    Evaluates the integrated directional encoding (IDE) for given inputs.\n\n    Input-Output Arguments\n    :param xyz: Array of shape (..., 3). The 3D point (or points) for which the directional encoding is to be computed.\n    :param kappa_inv: Array of shape (..., 1). The reciprocal of the concentration parameter of the von Mises-Fisher distribution. It determines the shape and spread of the distribution.\n    :return: Array of shape (..., ml_array.shape[1] * 2). The computed directional encoding for the input points. The output array contains the real and imaginary parts of the spherical harmonics, multiplied by the von Mises-Fisher distribution.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  # Generate integrated directional encoding function\n  integrated_dir_enc_fn = generate_ide_fn(deg_view)\n\n  # Define a function that computes the directional encoding\n  def dir_enc_fn(xyz, v, kappa_inv):\n    \"\"\"\n    Computes the directional encoding for a given 3D point (or points) and view direction.\n\n    Input-Output Arguments\n    :param xyz: Float array. The 3D point (or points) to compute the directional encoding for.\n    :param v: Float array. The view direction(s) to compute the directional encoding for.\n    :param kappa_inv: Float array. The inverse of the concentration parameter of the von Mises-Fisher distribution.\n    :return: Float array. The directional encoding for the given inputs.\n    \"\"\"\n    # Compute the integrated directional encoding\n    ide = integrated_dir_enc_fn(xyz, kappa_inv)\n\n    # Compute the directional encoding\n    return ide * ((v[Ellipsis, None, :] * xyz[Ellipsis, :, None]).sum(axis=-1) - 1)\n\n  # Return the directional encoding function\n  return dir_enc_fn"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  # Generate the integrated directional encoding function\n  integrated_dir_enc_fn = generate_ide_fn(deg_view)\n\n  def dir_enc_fn(xyz, kappa_inv):\n    \"\"\"\n    Computes the directional encoding for a given set of 3D points and inverse concentration parameter.\n\n    Input-Output Arguments\n    :param xyz: Tensor. The 3D points for which the directional encoding is to be computed.\n    :param kappa_inv: Tensor. The inverse concentration parameter of the Von Mises-Fisher distribution.\n    :return: Tensor. The computed directional encoding.\n\n    \"\"\"\n    # Compute the integrated directional encoding\n    ide = integrated_dir_enc_fn(xyz, kappa_inv)\n\n    # Compute the length of the integrated directional encoding\n    ide_len = jnp.sqrt(jnp.sum(ide**2, axis=-1, keepdims=True))\n\n    # Normalize the integrated directional encoding\n    ide_n = ide / jnp.maximum(ide_len, 1e-10)\n\n    # Compute the dot product between the normalized integrated directional encoding and the original integrated directional encoding\n    ide_dot = (ide * ide_n).sum(axis=-1, keepdims=True)\n\n    # Compute the directional encoding by multiplying the normalized integrated directional encoding with the dot product\n    return ide_n * ide_dot\n\n  return dir_enc_fn"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    page_blocks = []\n    header_block_idx = -1\n    block_idx = 0\n    style_dict = {}\n    join_font_spacing = False\n    prev_line = None\n    text_list = []\n    prev_ents = 0\n    curr_ents = 0\n    is_incomplete = False\n    colon_rule = False\n    text_group_start = True\n    text_group_start_idx = 0\n\n    prev_line = None\n    next_line = None\n    # for idx, line in enumerate(lines[12:14]):\n    sentence_visual_end = False\n    group_id = 0\n\n    for idx, line in enumerate(lines):\n        # print(idx)\n        line_str, style_dict, text_list = (\n            line[\"text\"],\n            line[\"style\"],\n            line[\"text_list\"],\n        )\n\n        line_str = \" \".join(line_str.split())\n        if should_skip(line_str):\n            continue\n\n        curr_line = line_parser.Line(\n            line_str=line_str,\n            style_dict=style_dict,\n            text_list=text_list,\n        )\n\n        if prev_line is None:\n            # initialize memory of previous line.\n            # this will update with join decisions\n            list_char = \"\"\n            if curr_line.line_type == \"list_item\":\n                list_char = curr_line.text[0]\n                curr_line.text = curr_line.text[1:].lstrip()\n\n            if curr_line.line_type == \"header\":\n                header_block_idx = block_idx\n\n            block = {\n                \"block_idx\": block_idx,\n                \"block_text\": curr_line.text,\n                \"block_type\": curr_line.line_type,\n                \"header_block_idx\": header_block_idx,\n                \"block_group\": [curr_line.visual_line.text_list],\n                \"list_char\":"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    page_blocks = []\n    header_block_idx = -1\n    block_idx = 0\n    style_dict = {}\n    join_font_spacing = False\n    prev_line = None\n    text_list = []\n    prev_ents = 0\n    curr_ents = 0\n    is_incomplete = False\n    colon_rule = False\n    text_group_start = True\n    text_group_start_idx = 0\n\n    prev_line = None\n    next_line = None\n    # for idx, line in enumerate(lines[12:14]):\n    sentence_visual_end = False\n    group_id = 0\n\n    for idx, line in enumerate(lines):\n        # print(idx)\n        line_str, style_dict, text_list = (\n            line[\"text\"],\n            line[\"style\"],\n            line[\"text_list\"],\n        )\n\n        line_str = \" \".join(line_str.split())\n        if should_skip(line_str):\n            continue\n\n        curr_line = line_parser.Line(\n            line_str=line_str,\n            style_dict=style_dict,\n            text_list=text_list,\n        )\n\n        if prev_line is None:\n            # initialize memory of previous line.\n            # this will update with join decisions\n            list_char = \"\"\n            if curr_line.line_type == \"list_item\":\n                list_char = curr_line.text[0]\n                curr_line.text = curr_line.text[1:].lstrip()\n\n            if curr_line.line_type == \"header\":\n                header_block_idx = block_idx\n\n            block = {\n                \"block_idx\": block_idx,\n                \"block_text\": curr_line.text,\n                \"block_type\": curr_line.line_type,\n                \"header_block_idx\": header_block_idx,\n                \"block_group\": [curr_line.visual_line.text_list],\n                \"list_char\":"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    prev_line = None\n    text_list = []\n    prev_ents = 0\n    curr_ents = 0\n    is_incomplete = False\n    colon_rule = False\n    text_group_start = True\n    text_group_start_idx = 0\n\n    prev_line = None\n    next_line = None\n    # for idx, line in enumerate(lines[12:14]):\n    sentence_visual_end = False\n    group_id = 0\n\n    for idx, line in enumerate(lines):\n        # print(idx)\n        line_str, style_dict, text_list = (\n            line[\"text\"],\n            line[\"style\"],\n            line[\"text_list\"],\n        )\n\n        line_str = \" \".join(line_str.split())\n        if should_skip(line_str):\n            continue\n\n        curr_line = line_parser.Line(\n            line_str=line_str,\n            style_dict=style_dict,\n            text_list=text_list,\n        )\n\n        if prev_line is None:\n            # initialize memory of previous line.\n            # this will update with join decisions\n            list_char = \"\"\n            if curr_line.line_type == \"list_item\":\n                list_char = curr_line.text[0]\n                curr_line.text = curr_line.text[1:].lstrip()\n\n            if curr_line.line_type == \"header\":\n                header_block_idx = block_idx\n\n            block = {\n                \"block_idx\": block_idx,\n                \"block_text\": curr_line.text,\n                \"block_type\": curr_line.line_type,\n                \"header_block_idx\": header_block_idx,\n                \"block_group\": [curr_line.visual_line.text_list],\n                \"list_char\": list_char,\n                \"fs\": curr_line.visual_line.start_fs,\n                \"text_group_start_idx\": text"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    page_blocks = []\n    header_block_idx = -1\n    block_idx = 0\n    # block_idx = page_idx\n    style_dict = {}\n    join_font_spacing = False\n    prev_line = None\n    text_list = []\n    prev_ents = 0\n    curr_ents = 0\n    is_incomplete = False\n    colon_rule = False\n    text_group_start = True\n    text_group_start_idx = 0\n\n    prev_line = None\n    next_line = None\n    # for idx, line in enumerate(lines[12:14]):\n    sentence_visual_end = False\n    group_id = 0\n\n    for idx, line in enumerate(lines):\n        # print(idx)\n        line_str, style_dict, text_list = (\n            line[\"text\"],\n            line[\"style\"],\n            line[\"text_list\"],\n        )\n\n        line_str = \" \".join(line_str.split())\n        if should_skip(line_str):\n            continue\n\n        if len(line_str.split()) > 8:\n            line_set.add(line_str)\n\n        curr_line = line_parser.Line(\n            line_str=line_str,\n            style_dict=style_dict,\n            text_list=text_list,\n            page_details=page_stats,\n        )\n\n        if prev_line is None:\n            # initialize memory of previous line.\n            # this will update with join decisions\n            list_char = \"\"\n            if curr_line.line_type == \"list_item\":\n                list_char = curr_line.text[0]\n                curr_line.text = curr_line.text[1:].lstrip()\n\n            if curr_line.line_type == \"header\":\n                header_block_idx = block_idx\n\n            block = {\n                \"block_idx\": block_idx,\n                \"block_text\": curr_line.text,\n                \"block_type\": curr_line.line"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\"cleaning lines\")\n    # print(\""}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # initialize memory of previous line.\n    # this will update with join decisions\n    prev_line = None\n    text_list = []\n    prev_ents = 0\n    curr_ents = 0\n    is_incomplete = False\n    colon_rule = False\n    text_group_start = True\n    text_group_start_idx = 0\n    block_idx = 0\n    header_block_idx = -1\n    page_blocks = []\n    # for idx, line in enumerate(lines[12:14]):\n    sentence_visual_end = False\n    group_id = 0\n\n    for idx, line in enumerate(lines):\n        line_str, style_dict, text_list = (\n            line[\"text\"],\n            line[\"style\"],\n            line[\"text_list\"],\n        )\n\n        line_str = \" \".join(line_str.split())\n        if should_skip(line_str, xml):\n            continue\n\n        curr_line = line_parser.Line(\n            line_str=line_str,\n            style_dict=style_dict,\n            text_list=text_list,\n        )\n\n        if prev_line is None:\n            # initialize memory of previous line.\n            # this will update with join decisions\n            list_char = \"\"\n            if curr_line.line_type == \"list_item\":\n                list_char = curr_line.text[0]\n                curr_line.text = curr_line.text[1:].lstrip()\n\n            if curr_line.line_type == \"header\":\n                header_block_idx = block_idx\n\n            block = {\n                \"block_idx\": block_idx,\n                \"block_text\": curr_line.text,\n                \"block_type\": curr_line.line_type,\n                \"header_block_idx\": header_block_idx,\n                \"block_group\": [curr_line.text_list],\n                \"list_char\": list_char,\n                \"text_group_start_idx\": text_group_start_idx,"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # print(\"cleaning lines\")\n    # print(lines)\n    result = []\n    # print(lines)\n    # print()\n    # print(\"cleaning lines\")\n    # print(lines)\n    # print()\n    # print(\"cleaning lines\")\n    # print(lines)\n    # print()\n    # print(\"cleaning lines\")\n    # print(lines)\n    # print()\n    # print(\"cleaning lines\")\n    # print(lines)\n    # print()\n    # print(\"cleaning lines\")\n    # print(lines)\n    # print()\n    # print(\"cleaning lines\")\n    # print(lines)\n    # print()\n    # print(\"cleaning lines\")\n    # print(lines)\n    # print()\n    # print(\"cleaning lines\")\n    # print(lines)\n    # print()\n    # print(\"cleaning lines\")\n    # print(lines)\n    # print()\n    # print(\"cleaning lines\")\n    # print(lines)\n    # print()\n    # print(\"cleaning lines\")\n    # print(lines)\n    # print()\n    # print(\"cleaning lines\")\n    # print(lines)\n    # print()\n    # print(\"cleaning lines\")\n    # print(lines)\n    # print()\n    # print(\"cleaning lines\")\n    # print(lines)\n    # print()\n    # print(\"cleaning lines\")\n    # print(lines)\n    # print()\n    # print(\"cleaning lines\")\n    # print(lines)\n    # print()\n    # print(\"cleaning lines\")\n    # print(lines)\n    # print()\n    # print(\"cleaning lines\")\n    # print(lines)\n    # print()\n    # print(\"cleaning lines\")\n    # print(lines)\n    # print()\n    # print(\"cleaning lines\")\n    # print(lines)\n    # print()\n    # print(\"cleaning lines\")\n    # print(lines)\n    # print()\n    # print(\"cleaning lines\")\n    # print(lines)\n    # print()\n    # print(\""}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # TODO: make sure that there is at least one word which is greater than two characters\n    # TODO: make sure that there is at least one word which is greater than two characters\n    # TODO: make sure that there is at least one word which is greater than two characters\n\n    # TODO: make sure that there is at least one word which is greater than two characters\n    # TODO: make sure that there is at least one word which is greater than two characters\n    # TODO: make sure that there is at least one word which is greater than two characters\n\n    # TODO: make sure that there is at least one word which is greater than two characters\n    # TODO: make sure that there is at least one word which is greater than two characters\n    # TODO: make sure that there is at least one word which is greater than two characters\n\n    # TODO: make sure that there is at least one word which is greater than two characters\n    # TODO: make sure that there is at least one word which is greater than two characters\n    # TODO: make sure that there is at least one word which is greater than two characters\n\n    # TODO: make sure that there is at least one word which is greater than two characters\n    # TODO: make sure that there is at least one word which is greater than two characters\n    # TODO: make sure that there is at least one word which is greater than two characters\n\n    # TODO: make sure that there is at least one word which is greater than two characters\n    # TODO: make sure that there is at least one word which is greater than two characters\n    # TODO: make sure that there is at least one word which is greater than two characters\n\n    # TODO: make sure that there is at least one word which is greater than two characters\n    # TODO: make sure that there is at least one word which is greater than two characters\n    # TODO: make sure that there is at least one word which is greater than two characters\n\n    # TODO: make sure that there is at least one word which is greater than two characters\n    # TODO: make sure that there is at least one word which is greater than two characters\n    # TODO: make sure that there is at least one word which is greater than two characters\n\n    # TODO: make sure that there is at"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    page_blocks = []\n    block_idx = 0\n    header_block_idx = -1\n    # block_idx = page_idx\n    style_dict = {}\n    join_font_spacing = False\n    prev_line = None\n    text_list = []\n    prev_ents = 0\n    curr_ents = 0\n    is_incomplete = False\n    colon_rule = False\n    text_group_start = True\n    text_group_start_idx = 0\n\n    prev_line = None\n    next_line = None\n    # for idx, line in enumerate(lines[12:14]):\n    sentence_visual_end = False\n    group_id = 0\n\n    for idx, line in enumerate(lines):\n        # print(idx)\n        line_str, style_dict, text_list = (\n            line[\"text\"],\n            line[\"style\"],\n            line[\"text_list\"],\n        )\n\n        line_str = \" \".join(line_str.split())\n        if should_skip(line_str):\n            continue\n\n        curr_line = line_parser.Line(\n            line_str=line_str,\n            style_dict=style_dict,\n            text_list=text_list,\n            xml=xml,\n        )\n\n        if prev_line is None:\n            # initialize memory of previous line.\n            # this will update with join decisions\n            list_char = \"\"\n            if curr_line.line_type == \"list_item\":\n                list_char = curr_line.text[0]\n                curr_line.text = curr_line.text[1:].lstrip()\n\n            if curr_line.line_type == \"header\":\n                header_block_idx = block_idx\n\n            block = {\n                \"block_idx\": block_idx,\n                \"block_text\": curr_line.text,\n                \"block_type\": curr_line.line_type,\n                \"header_block_idx\": header_block_idx,\n                \"block_group\": ["}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    # line_set = set()\n    line_set = {}\n    block_idx = 0\n    header_block_idx = -1\n    style_dict = {}\n    text_list = []\n    prev_line = None\n    prev_ents = 0\n    curr_ents = 0\n    is_incomplete = False\n    colon_rule = False\n    text_group_start = True\n    text_group_start_idx = 0\n    group_id = 0\n\n    for idx, line in enumerate(lines):\n        # print(idx)\n        line_str, style_dict, text_list = (\n            line[\"text\"],\n            line[\"style\"],\n            line[\"text_list\"],\n        )\n\n        line_str = \" \".join(line_str.split())\n        if should_skip(line_str, xml):\n            continue\n\n        if line_str in line_set:\n            continue\n\n        if len(line_str.split()) > 8:\n            line_set.add(line_str)\n\n        curr_line = line_parser.Line(\n            line_str=line_str,\n            style_dict=style_dict,\n            text_list=text_list,\n        )\n\n        if prev_line is None:\n            # initialize memory of previous line.\n            # this will update with join decisions\n            list_char = \"\"\n            if curr_line.line_type == \"list_item\":\n                list_char = curr_line.text[0]\n                curr_line.text = curr_line.text[1:].lstrip()\n\n            if curr_line.line_type == \"header\":\n                header_block_idx = block_idx\n\n            block = {\n                \"block_idx\": block_idx,\n                \"block_text\": curr_line.text,\n                \"block_type\": curr_line.line_type,\n                \"header_block_idx\": header_block_idx,\n                \"block_group\": [curr_line.text_list],\n                \"list_char\": list_"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    prev_line = None\n    text_list = []\n    prev_ents = 0\n    curr_ents = 0\n    is_incomplete = False\n    colon_rule = False\n    text_group_start = True\n    text_group_start_idx = 0\n    header_block_idx = -1\n    block_idx = 0\n    group_id = 0\n    line_set = set()\n\n    for idx, line in enumerate(lines):\n        line_str = clean_line(line)\n        if should_skip(line_str, xml=xml):\n            continue\n        if line_str in line_set:\n            continue\n        if len(line_str.split()) > 8:\n            line_set.add(line_str)\n        curr_line = line_parser.Line(line_str)\n        if prev_line is None:\n            # initialize memory of previous line.\n            # this will update with join decisions\n            list_char = \"\"\n            if curr_line.line_type == \"list_item\":\n                list_char = curr_line.text[0]\n                curr_line.text = curr_line.text[1:].lstrip()\n\n            if curr_line.line_type == \"header\":\n                header_block_idx = block_idx\n\n            block = {\n                \"block_idx\": block_idx,\n                \"block_text\": curr_line.text,\n                \"block_type\": curr_line.line_type,\n                \"header_block_idx\": header_block_idx,\n                \"block_group\": [curr_line.text],\n                \"list_char\": list_char,\n                \"text_group_start_idx\": text_group_start_idx,\n                \"block_list\": curr_line.text,\n            }\n\n            prev_line = curr_line\n            block_idx += 1\n            if (idx <= 3) or (idx >= len(lines) - 3):\n                line_without_numbers = re.sub(r\"[^a-zA"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    block_idx = 0\n    header_block_idx = -1\n    text_group_start = True\n    text_group_start_idx = 0\n    group_id = 0\n    line_set = {}\n\n    for idx, line in enumerate(lines):\n        line = line.strip()\n        if should_skip(line, xml):\n            continue\n\n        if line in line_set:\n            continue\n\n        if len(line.split()) > 8:\n            line_set.add(line)\n\n        line = formatter.clean_line(line)\n\n        # if the last line was not a table, check if the next line is a table to avoid single tr\n        if idx + 1 < len(lines):\n            next_line = lines[idx + 1]\n            next_line = formatter.clean_line(next_line)\n            if prev_line.line_type != \"table_row\" and not is_table_row(next_line):\n                # check if the next line is a table and matches curr_line\n                next_line_tr = is_table_row(next_line) or should_join_table(\n                    curr_line,\n                    next_line,\n                    False,\n                )\n                if not next_line_tr and curr_line.line_type == \"table_row\":\n                    curr_line.line_type = \"para\"\n\n        # if the next line is joinable by visual stats but prev and curr are not\n        # don't join the line (only true by x-span check and y is below for prev cur)\n        # if this is not true ignore the rule\n        prev_not_above_next = (\n            next_line and prev_line.visual_line.start_y > next_line.visual_line.start_y\n        )\n        next_line_join = False\n        if next_line and check_layout(prev_line, next_line, prev_not_above_next):\n            next_line_join = check_page_spacing(\n                curr_line,\n                next_line"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    block_idx = 0\n    header_block_idx = -1\n    text_group_start = True\n    text_group_start_idx = 0\n    group_id = 0\n    line_set = set()\n    for idx, line in enumerate(lines):\n        if should_skip(line):\n            continue\n\n        if line in line_set:\n            continue\n\n        if len(line.split()) > 8:\n            line_set.add(line)\n\n        curr_line = line_parser.Line(line_str=line)\n\n        if curr_line.line_type == \"header\":\n            header_block_idx = block_idx\n\n        if text_group_start:\n            text_group_start = False\n            text_group_start_idx = block_idx\n\n        if curr_line.line_type == \"list_item\":\n            list_char = curr_line.text[0]\n            curr_line.text = curr_line.text[1:].lstrip()\n        else:\n            list_char = \"\"\n\n        block = {\n            \"block_idx\": block_idx,\n            \"block_text\": curr_line.text,\n            \"block_type\": curr_line.line_type,\n            \"header_block_idx\": header_block_idx,\n            \"block_group\": [curr_line.visual_line.text_list],\n            \"list_char\": list_char,\n            \"text_group_start_idx\": text_group_start_idx,\n            \"group_id\": group_id,\n        }\n\n        if (\n            curr_line.line_type == \"para\"\n            and idx + 1 < len(lines)\n            and not lines[idx + 1].startswith(\" \")\n        ):\n            text_group_start = True\n            group_id += 1\n\n        result.append(block)\n        block_idx += 1\n    return result"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    #     print(\"-\" * 50)\n    #     print(\"CLEANING LINES\")\n    #     print(\"-\" * 50)\n    #     print()\n    #     print(\"lines\")\n    #     print(lines)\n    #     print()\n\n    #     print(\"lines\")\n    #     print(lines)\n    #     print()\n\n    result = []\n    prev_line = None\n    text_list = []\n    prev_ents = 0\n    curr_ents = 0\n    is_incomplete = False\n    colon_rule = False\n    text_group_start = True\n    text_group_start_idx = 0\n\n    #     print(\"-\" * 50)\n    #     print(\"CLEANING LINES\")\n    #     print(\"-\" * 50)\n    #     print()\n    #     print(\"lines\")\n    #     print(lines)\n    #     print()\n\n    #     print(\"lines\")\n    #     print(lines)\n    #     print()\n\n    for idx, line in enumerate(lines):\n        # print(idx)\n        line_str, style_dict, text_list = (\n            line[\"text\"],\n            line[\"style\"],\n            line[\"text_list\"],\n        )\n\n        line_str = \" \".join(line_str.split())\n        if should_skip(line_str):\n            continue\n\n        curr_line = line_parser.Line(\n            line_str=line_str,\n            style_dict=style_dict,\n            text_list=text_list,\n        )\n\n        if prev_line is None:\n            # initialize memory of previous line.\n            # this will update with join decisions\n            list_char = \"\"\n            if curr_line.line_type == \"list_item\":\n                list_char = curr_line.text[0]\n                curr_line.text = curr_line.text[1:].lstrip()\n\n            block = {\n                \"block_idx\": idx,\n                \"block_text\": curr_"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    block_idx = 0\n    header_block_idx = -1\n    text_group_start = True\n    text_group_start_idx = 0\n    block_list = []\n    group_id = 0\n    prev_line = None\n    prev_line_type = None\n    for idx, line in enumerate(lines):\n        line = clean_lines(line)\n        if should_skip(line, xml):\n            continue\n        if len(line.split()) > 8:\n            line = \" \".join(line.split())\n            if line in line_set:\n                continue\n            line_set.add(line)\n\n        curr_line = line_parser.Line(line)\n        if prev_line is None:\n            # initialize memory of previous line.\n            # this will update with join decisions\n            if curr_line.line_type == \"list_item\":\n                curr_line.text = curr_line.text[1:].lstrip()\n\n            if curr_line.line_type == \"header\":\n                header_block_idx = block_idx\n\n            block = {\n                \"block_idx\": block_idx,\n                \"block_text\": curr_line.text,\n                \"block_type\": curr_line.line_type,\n                \"header_block_idx\": header_block_idx,\n                \"block_group\": [curr_line.text],\n                \"list_char\": \"\",\n                \"text_group_start_idx\": text_group_start_idx,\n                \"block_list\": curr_line.text,\n                \"line\": curr_line,\n                \"y\": curr_line.visual_line.start_y,\n                \"group_id\": group_id,\n            }\n\n            prev_line = curr_line\n            prev_line_type = curr_line.line_type\n            block_idx += 1\n            page_blocks.append(block)\n            continue\n\n        if (\n            len(prev_line.text) > 1\n            and len(curr_line.text"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    block_idx = 0\n    header_block_idx = -1\n    text_group_start_idx = 0\n    text_group_start = True\n    text_list = []\n    prev_line = None\n    list_char = \"\"\n    group_id = 0\n    # for idx, line in enumerate(lines):\n    for line in lines:\n        line = clean_line(line)\n        if should_skip(line):\n            continue\n\n        line = line_parser.Line(line)\n        # print(line.text)\n        if prev_line is None:\n            # initialize memory of previous line.\n            # this will update with join decisions\n            if line.line_type == \"list_item\":\n                list_char = line.text[0]\n                line.text = line.text[1:].lstrip()\n\n            if line.line_type == \"header\":\n                header_block_idx = block_idx\n\n            block = {\n                \"block_idx\": block_idx,\n                \"block_text\": line.text,\n                \"block_type\": line.line_type,\n                \"header_block_idx\": header_block_idx,\n                \"block_group\": [line.text_list],\n                \"list_char\": list_char,\n                \"text_group_start_idx\": text_group_start_idx,\n                \"block_list\": line.text_list,\n                \"line\": line,\n                \"group_id\": group_id,\n            }\n            prev_line = line\n            block_idx += 1\n            result.append(block)\n            continue\n\n        if len(line.text) > 1 and len(prev_line.text) > 1:\n            if line.text[:2] == prev_line.text[:2] and line.text[1] == \" \":\n                line.line_type = \"list_item\"\n                line.is_list_item = True\n                line.is_list_or_row = True\n\n                if result[-1][\"block_type\"] != \"list_item\":\n                    result"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    prev_line = None\n    line_set = set()\n    for line in lines:\n        line = clean_line(line)\n        if should_skip(line):\n            continue\n        if line in line_set:\n            continue\n        if len(line.split()) > 8:\n            line_set.add(line)\n\n        curr_line = line_parser.Line(line_str=line)\n        if prev_line is None:\n            # initialize memory of previous line.\n            # this will update with join decisions\n            prev_line = curr_line\n            continue\n\n        if prev_line.line_type == \"header\" and curr_line.line_type == \"header\":\n            prev_line.line_type = \"header\"\n            curr_line.line_type = \"header\"\n\n        if prev_line.line_type == \"list_item\" and curr_line.line_type == \"list_item\":\n            prev_line.line_type = \"list_item\"\n            curr_line.line_type = \"list_item\"\n\n        if prev_line.line_type == \"table_row\" and curr_line.line_type == \"table_row\":\n            prev_line.line_type = \"table_row\"\n            curr_line.line_type = \"table_row\"\n\n        if prev_line.line_type == \"list_item\" and curr_line.line_type == \"para\":\n            prev_line.line_type = \"list_item\"\n            curr_line.line_type = \"list_item\"\n\n        if prev_line.line_type == \"para\" and curr_line.line_type == \"list_item\":\n            prev_line.line_type = \"para\"\n            curr_line.line_type = \"para\"\n\n        if (\n            prev_line.line_type == \"para\"\n            and curr_line.line_type == \"para\"\n            and not check_block_join(prev_line, curr_line)\n        ):"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    line_set = set()\n    # print(lines)\n    for idx, line in enumerate(lines):\n        # print(idx, line)\n        line = clean_line(line)\n        if should_skip(line):\n            continue\n        # print(line)\n        if line in line_set:\n            continue\n        if len(line.split()) > 8:\n            line_set.add(line)\n        line_parser_obj = line_parser.Line(line)\n        # print(line_parser_obj.line_type, line_parser_obj.text)\n        # print()\n        if line_parser_obj.line_type == \"list_item\":\n            list_char = line_parser_obj.text[0]\n            line_parser_obj.text = line_parser_obj.text[1:].lstrip()\n        else:\n            list_char = \"\"\n\n        if line_parser_obj.line_type == \"header\":\n            header_block_idx = len(result)\n\n        block = {\n            \"block_text\": line_parser_obj.text,\n            \"block_type\": line_parser_obj.line_type,\n            \"header_block_idx\": header_block_idx,\n            \"list_char\": list_char,\n        }\n        if len(result) == 0:\n            result.append(block)\n            continue\n        prev_block = result[-1]\n        if check_block_join(prev_block, block):\n            result, block = join_blocks(result, [block])\n        else:\n            result.append(block)\n    return result"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    line_set = set()\n    #     print(\"CLEANING LINES\")\n    #     print()\n    for line in lines:\n        line = clean_line(line)\n        if should_skip(line, xml):\n            continue\n        if line in line_set:\n            continue\n        line_set.add(line)\n        line_obj = line_parser.Line(line)\n        if line_obj.is_header:\n            line_obj.line_type = \"header\"\n        if line_obj.is_list_item:\n            line_obj.line_type = \"list_item\"\n        if line_obj.is_table_row:\n            line_obj.line_type = \"table_row\"\n        if line_obj.is_numbered_list_item:\n            line_obj.line_type = \"numbered_list_item\"\n        if line_obj.is_table_row:\n            line_obj.line_type = \"table_row\"\n        result.append(\n            {\n                \"text\": line_obj.text,\n                \"line_type\": line_obj.line_type,\n                \"is_header\": line_obj.is_header,\n                \"is_list_item\": line_obj.is_list_item,\n                \"is_table_row\": line_obj.is_table_row,\n                \"is_numbered_list_item\": line_obj.is_numbered_list_item,\n                \"has_list_char\": line_obj.has_list_char,\n                \"hit_colon\": line_obj.hit_colon,\n                \"numbered_line\": line_obj.numbered_line,\n                \"text_list\": line_obj.text_list,\n                \"incomplete_line\": line_obj.incomplete_line,\n                \"continuing_line\": line_obj.continuing_line,\n                \"ends_with_period\": line_obj.ends_with_period,\n                \"ends_with_comma\": line_obj.ends_with_comma,\n            },"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    text_group_start = True\n    text_group_start_idx = 0\n    header_block_idx = -1\n    block_idx = 0\n    line_set = set()\n\n    for idx, line in enumerate(lines):\n        line = line.strip()\n        if should_skip(line):\n            continue\n        if line in line_set:\n            continue\n        line_set.add(line)\n\n        if len(line.split()) > 8:\n            line_set.add(line)\n\n        curr_line = line_parser.Line(line)\n        if curr_line.is_list_item:\n            curr_line.text = curr_line.text[1:].lstrip()\n\n        if curr_line.line_type == \"header\":\n            header_block_idx = block_idx\n\n        if curr_line.line_type == \"table_row\":\n            curr_line.text = curr_line.text[1:].lstrip()\n\n        block = {\n            \"block_idx\": block_idx,\n            \"block_text\": curr_line.text,\n            \"block_type\": curr_line.line_type,\n            \"header_block_idx\": header_block_idx,\n            \"block_group\": [curr_line.text],\n            \"text_group_start_idx\": text_group_start_idx,\n        }\n        if idx == 0:\n            prev_line = curr_line\n            block_idx += 1\n            result.append(block)\n            continue\n\n        if check_block_join(result[-1], block):\n            # join the blocks\n            result, block = join_blocks(result, [block])\n            result[-1][\"block_group\"].append(block[\"block_group\"])\n            continue\n\n        if curr_line.line_type == \"list_item\":\n            curr_line.text = curr_line.text[1:].lstrip()\n\n        if curr_line.line_type == \"header\":\n           "}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    if isinstance(org_texts, str):\n        org_texts = [org_texts]\n\n    tokenized_sents = []\n    for text in org_texts:\n        text = text.replace(\"\\n\", \" \")\n        text = text.replace(\"\\r\", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text = text.replace(\"  \", \" \")\n        text ="}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    # tokenize text by nltk\n    texts = nltk_tokenzier.tokenize(org_texts)\n\n    # remove any space between punctuations (.')\n    texts = [space_rule.sub(r\"\\1\", text) for text in texts]\n\n    # remove quotation marks\n    texts = [quotation_pattern.sub(\"\", text) for text in texts]\n\n    # match content inside brackets\n    texts = [bracket_rule.sub(\"\", text) for text in texts]\n\n    # apply rules\n    for rule, replaced in rules:\n        texts = [rule.sub(replaced, text) for text in texts]\n\n    return texts"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    # split text into paragraphs\n    texts = org_texts.split(\"\\n\")\n    texts = [text for text in texts if text]\n\n    # tokenize each paragraph\n    sents = []\n    for text in texts:\n        text = text.strip()\n\n        # match content inside brackets\n        bracket_texts = bracket_rule.findall(text)\n        for bracket_text in bracket_texts:\n            bracket_sents = nltk_tokenzier.tokenize(bracket_text)\n            sents.extend(bracket_sents)\n\n        # remove any space between punctuations (.')\n        text = space_rule.sub(r\"\\1\", text)\n\n        # normalize quotation marks\n        text = quotation_pattern.sub('\"', text)\n\n        # apply rules\n        for rule, replaced in rules:\n            text = rule.sub(replaced, text)\n\n        # tokenize text\n        text_sents = nltk_tokenzier.tokenize(text)\n        sents.extend(text_sents)\n\n    return sents"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    # replace newline with space\n    text = org_texts.replace(\"\\n\", \" \")\n\n    # replace space between punctuations (.')\n    text = space_rule.sub(r\"\\1\", text)\n\n    # remove quotation marks\n    text = quotation_pattern.sub('\"', text)\n\n    # replace brackets with space\n    text = bracket_rule.sub(\" \", text)\n\n    # tokenize text\n    text = nltk_tokenzier.tokenize(text)\n\n    # apply rules\n    for rule, replaced in rules:\n        text = [rule.sub(replaced, sent) for sent in text]\n\n    # remove empty sentences\n    text = [sent for sent in text if sent.strip()]\n\n    return text"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    texts = org_texts\n    # remove new lines\n    texts = texts.replace(\"\\n\", \" \")\n\n    # remove multiple spaces\n    texts = re.sub(r\"\\s+\", \" \", texts)\n\n    # remove space before punctuation\n    texts = space_rule.sub(r\"\\1\", texts)\n\n    # normalize quotation marks\n    texts = quotation_pattern.sub('\"', texts)\n\n    # split sentences\n    sentences = nltk_tokenzier.tokenize(texts)\n\n    # split sentences inside brackets\n    for i in range(len(sentences)):\n        sentences[i] = bracket_rule.sub(r\"\\1\", sentences[i])\n\n    # apply rules\n    for rule, replaced in rules:\n        sentences = [rule.sub(replaced, sent) for sent in sentences]\n\n    return sentences"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    # remove new lines\n    text = org_texts.replace(\"\\n\", \" \")\n\n    # add space before punctuation\n    text = space_rule.sub(r\"\\1\", text)\n\n    # remove quotation marks\n    text = quotation_pattern.sub('\"', text)\n\n    # replace brackets\n    text = bracket_rule.sub(r\"\\1\", text)\n\n    # split into sentences\n    sentences = nltk_tokenzier.tokenize(text)\n\n    # replace abbreviations\n    for rule, replaced in rules:\n        sentences = [rule.sub(replaced, sent) for sent in sentences]\n\n    return sentences"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return [org_texts]\n\n    # match content inside brackets\n    # (?<=\\() ==> starts with \"(\"\n    # ([^)]+) ==> repeat not \")\"\n    # (?=\\))\") ==> ends with \")\"\n    bracket_rule = re.compile(r\"(?<=\\()([^)]+)(?=\\))\")\n    space_rule = re.compile(r\"\\s([.'](?:\\s|$|\\D))\", re.IGNORECASE)  # Remove any space between punctuations (.')\n    quotation_pattern = re.compile(r'[\u201d\u201c\"\u2018\u2019\\']')\n\n    # normalize quotation marks\n    org_texts = quotation_pattern.sub('\"', org_texts)\n\n    # replace space between punctuations\n    org_texts = space_rule.sub(r\"\\1\", org_texts)\n\n    # tokenize by nltk\n    sents = nltk_tokenzier.tokenize(org_texts)\n\n    # tokenize by rules\n    for rule, replaced in rules:\n        sents = [rule.sub(replaced, sent) for sent in sents]\n\n    # handle brackets\n    sents = [bracket_rule.sub(r\"\\1\", sent) for sent in sents]\n\n    # remove empty sentences\n    sents = [sent for sent in sents if sent]\n\n    return sents"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    texts = org_texts.replace(\"\\n\", \" \")\n\n    # replace brackets with special token\n    texts = bracket_rule.sub(\"[BRACKET]\", texts)\n\n    # tokenize into sentences\n    sents = nltk_tokenzier.tokenize(texts)\n\n    # replace special token with brackets\n    sents = [sent.replace(\"[BRACKET]\", \"(BRACKET)\") for sent in sents]\n\n    # normalize quotation marks\n    sents = [quotation_pattern.sub('\"', sent) for sent in sents]\n\n    # replace space before punctuation\n    sents = [space_rule.sub(r\"\\1\", sent) for sent in sents]\n\n    # apply rules\n    for rule, replaced in rules:\n        sents = [rule.sub(replaced, sent) for sent in sents]\n\n    return sents"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    texts = org_texts\n\n    # split paragraphs\n    texts = texts.split(\"\\n\")\n    texts = [text.strip() for text in texts]\n    texts = [text for text in texts if text]\n\n    # split sentences\n    texts = [nltk_tokenzier.tokenize(text) for text in texts]\n    texts = [sent for sent_list in texts for sent in sent_list]\n\n    # split sentences\n    for rule, replaced in rules:\n        texts = [rule.sub(replaced, text) for text in texts]\n\n    # split sentences inside brackets\n    texts = [bracket_rule.sub(lambda x: x.group().replace(\" \", \"_\"), text) for text in texts]\n\n    # normalize quotation marks\n    texts = [quotation_pattern.sub('\"', text) for text in texts]\n\n    # remove space between punctuations\n    texts = [space_rule.sub(lambda x: x.group().replace(\" \", \"\"), text) for text in texts]\n\n    # remove empty strings\n    texts = [text for text in texts if text]\n\n    return texts"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    # split text into paragraphs\n    paragraphs = org_texts.split(\"\\n\")\n\n    # tokenize each paragraph\n    tokenized_texts = []\n    for paragraph in paragraphs:\n        # remove punctuation at the beginning of the paragraph\n        paragraph = paragraph.lstrip(\".,;:\")\n\n        # tokenize paragraph into sentences\n        sentences = nltk_tokenzier.tokenize(paragraph)\n\n        # remove punctuation at the end of each sentence\n        sentences = [sent.rstrip(\".,;:\") for sent in sentences]\n\n        # normalize quotation marks\n        sentences = [quotation_pattern.sub('\"', sent) for sent in sentences]\n\n        # apply rules to each sentence\n        for rule, replaced in rules:\n            sentences = [rule.sub(replaced, sent) for sent in sentences]\n\n        # split sentences that contain brackets\n        new_sentences = []\n        for sent in sentences:\n            if \"(\" in sent:\n                # split sentence into parts before and after brackets\n                parts = bracket_rule.split(sent)\n\n                # add each part to the list of new sentences\n                new_sentences.extend(parts)\n            else:\n                new_sentences.append(sent)\n\n        # remove any space between punctuations (.')\n        new_sentences = [space_rule.sub(r\"\\1\", sent) for sent in new_sentences]\n\n        # add new sentences to the list of tokenized sentences\n        tokenized_texts.extend(new_sentences)\n\n    return tokenized_texts"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    texts = org_texts\n    for rule, replaced in rules:\n        texts = rule.sub(replaced, texts)\n\n    # remove any space between punctuations (.')\n    texts = space_rule.sub(r\"\\1\", texts)\n\n    # remove any space between punctuations (.')\n    texts = quotation_pattern.sub('\"', texts)\n\n    # match content inside brackets\n    texts = bracket_rule.sub(\"\", texts)\n\n    # tokenize\n    sents = nltk_tokenzier.tokenize(texts)\n    return sents\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    # split paragraphs\n    texts = org_texts.split(\"\\n\")\n    # remove empty texts\n    texts = [text for text in texts if text]\n\n    # tokenize paragraphs\n    tokenized_texts = []\n    for text in texts:\n        # normalize quotation marks\n        text = quotation_pattern.sub('\"', text)\n        # tokenize text\n        tokenized_text = nltk_tokenzier.tokenize(text)\n        # split text into sentences\n        tokenized_texts.extend(tokenized_text)\n\n    # handle special cases\n    for rule, replaced in rules:\n        tokenized_texts = [rule.sub(replaced, text) for text in tokenized_texts]\n\n    # handle brackets\n    tokenized_texts = [bracket_rule.sub(r\"\\1\", text) for text in tokenized_texts]\n\n    # handle spaces\n    tokenized_texts = [space_rule.sub(r\"\\1\", text) for text in tokenized_texts]\n\n    return tokenized_texts"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or len(org_texts) == 0:\n        return org_texts\n\n    # split text by new line\n    texts = org_texts.split(\"\\n\")\n\n    # tokenize each text\n    sentences = []\n    for text in texts:\n        # remove space between punctuations (.')\n        text = space_rule.sub(r\"\\1\", text)\n        # normalize quotation marks\n        text = quotation_pattern.sub('\"', text)\n\n        # handle special cases\n        if text.startswith(\"(\"):\n            text = text[1:]\n        if text.endswith(\")\"):\n            text = text[:-1]\n\n        # tokenize text\n        text_sentences = nltk_tokenzier.tokenize(text)\n\n        # handle brackets\n        for i, text_sentence in enumerate(text_sentences):\n            if \"(\" in text_sentence and \")\" in text_sentence:\n                text_sentence = bracket_rule.sub(\"\", text_sentence)\n                text_sentences[i] = text_sentence\n\n        # apply rules\n        for rule, replaced in rules:\n            text_sentences = [rule.sub(replaced, text_sentence) for text_sentence in text_sentences]\n\n        # add sentences to list\n        sentences.extend(text_sentences)\n\n    return sentences"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    if isinstance(org_texts, str):\n        org_texts = [org_texts]\n\n    texts = []\n    for text in org_texts:\n        # split text into paragraphs\n        paragraphs = text.split(\"\\n\")\n        # tokenize each paragraph\n        for para in paragraphs:\n            # remove punctuation at the beginning of the text\n            para = para.strip()\n            # tokenize paragraph into sentences\n            sents = nltk_tokenzier.tokenize(para)\n            texts.extend(sents)\n\n    # apply rules\n    for rule, replaced in rules:\n        texts = [rule.sub(replaced, text) for text in texts]\n\n    # split sentences by brackets\n    texts_ = []\n    for text in texts:\n        # match content inside brackets\n        # (?<=\\() ==> starts with \"(\"\n        # ([^)]+) ==> repeat not \")\"\n        # (?=\\))\") ==> ends with \")\"\n        brackets = bracket_rule.findall(text)\n        if len(brackets) > 0:\n            # split text by brackets\n            for bracket in brackets:\n                # split text by bracket\n                texts_ += text.split(bracket)\n        else:\n            texts_.append(text)\n\n    # split sentences by space\n    texts = []\n    for text in texts_:\n        # match space between punctuations (.')\n        spaces = space_rule.findall(text)\n        if len(spaces) > 0:\n            # split text by space\n            for space in spaces:\n                # split text by space\n                texts += text.split(space)\n        else:\n            texts.append(text)\n\n    # normalize quotation marks\n    texts = [quotation_pattern.sub('\"', text) for text in texts]\n\n    return texts"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or len(org_texts) == 0:\n        return org_texts\n\n    # replace new line with space\n    text = org_texts.replace(\"\\n\", \" \")\n    # replace multiple spaces with single space\n    text = \" \".join(text.split())\n    # remove space before punctuations\n    text = space_rule.sub(r\"\\1\", text)\n\n    # match content inside brackets\n    text = bracket_rule.sub(r\"\\1\", text)\n    # normalize quotation marks\n    text = quotation_pattern.sub('\"', text)\n\n    # apply rules\n    for rule, replaced in rules:\n        text = rule.sub(replaced, text)\n\n    # tokenize\n    sents = nltk_tokenzier.tokenize(text)\n\n    # remove space at the beginning and end of each sentence\n    sents = [sent.strip() for sent in sents]\n\n    # remove empty sentences\n    sents = [sent for sent in sents if len(sent) > 0]\n\n    return sents\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    texts = org_texts.replace(\"\\n\", \" \")\n    for rule, replaced in rules:\n        texts = rule.sub(replaced, texts)\n\n    # match content inside brackets\n    texts = bracket_rule.sub(\"\", texts)\n\n    # remove any space between punctuations (.')\n    texts = space_rule.sub(r\"\\1\", texts)\n\n    # Normalize quotation marks\n    texts = quotation_pattern.sub('\"', texts)\n\n    # Tokenize into sentences\n    sents = nltk_tokenzier.tokenize(texts)\n    return sents\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    text = org_texts\n\n    # normalize quotation marks\n    text = quotation_pattern.sub('\"', text)\n\n    # split paragraphs\n    text = text.replace(\"\\n\", \" \")\n\n    # remove space before punctuation\n    text = space_rule.sub(r\"\\1\", text)\n\n    # remove space between punctuation\n    text = text.replace(\" .\", \".\")\n    text = text.replace(\" ,\", \",\")\n\n    # remove space between numbers\n    text = text.replace(\"( \", \"(\")\n    text = text.replace(\" )\", \")\")\n\n    # split brackets\n    text = bracket_rule.sub(r\"\\1\", text)\n\n    # split sentences\n    sents = nltk_tokenzier.tokenize(text)\n\n    # apply rules\n    for rule, replaced in rules:\n        sents = [rule.sub(replaced, sent) for sent in sents]\n\n    return sents\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return [org_texts]\n\n    # split into paragraphs\n    texts = org_texts.split(\"\\n\")\n\n    # split into sentences\n    sents = []\n    for text in texts:\n        text = text.strip()\n        if not text:\n            continue\n        # remove space before punctuation\n        text = space_rule.sub(r\"\\1\", text)\n        # remove quotation marks\n        text = quotation_pattern.sub(\"\", text)\n\n        # match content inside brackets\n        brackets = bracket_rule.findall(text)\n        for bracket in brackets:\n            bracket_sents = nltk_tokenzier.tokenize(bracket)\n            bracket_sents = [f\"({sent})\" for sent in bracket_sents]\n            text = text.replace(bracket, \" \".join(bracket_sents))\n        # tokenize\n        text_sents = nltk_tokenzier.tokenize(text)\n        for rule, replaced in rules:\n            text_sents = [rule.sub(replaced, sent) for sent in text_sents]\n        sents.extend(text_sents)\n    return sents\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return [org_texts]\n    # split paragraphs\n    texts = org_texts.split(\"\\n\")\n    texts = [text for text in texts if len(text) > 0]\n\n    # tokenize each sentence\n    sents = []\n    for text in texts:\n        for rule, replaced in rules:\n            text = rule.sub(replaced, text)\n\n        # match content inside brackets\n        text = bracket_rule.sub(\"\", text)\n\n        # remove space between punctuations (.')\n        text = space_rule.sub(r\"\\1\", text)\n\n        # normalize quotation marks\n        text = quotation_pattern.sub(\"\\\"\", text)\n\n        # tokenize\n        sents.extend(nltk_tokenzier.tokenize(text))\n\n    return sents\n\n"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    text = org_texts\n    text = text.replace(\"\\n\", \" \")\n    text = text.replace(\"\\r\", \" \")\n    text = text.replace(\"\\t\", \" \")\n\n    # remove quotation marks\n    text = re.sub(quotation_pattern, '\"', text)\n\n    # remove space between punctuation\n    text = re.sub(space_rule, r\"\\1\", text)\n\n    # tokenize text\n    sentences = nltk_tokenzier.tokenize(text)\n\n    # check if there are sentences within brackets\n    for sent in sentences:\n        if \"(\" in sent:\n            brackets = bracket_rule.findall(sent)\n            for bracket in brackets:\n                # remove brackets\n                sent = sent.replace(\"(\" + bracket + \")\", \"\")\n                # tokenize bracket\n                bracket_sentences = nltk_tokenzier.tokenize(bracket)\n                # add bracket sentences to sentences\n                sentences.extend(bracket_sentences)\n\n    # apply rules\n    for rule, replaced in rules:\n        sentences = [rule.sub(replaced, sent) for sent in sentences]\n\n    # remove space between punctuation\n    sentences = [re.sub(space_rule, r\"\\1\", sent) for sent in sentences]\n\n    return sentences\n\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                doc_id = key\n                if doc_id < 0:\n                    doc_id += len(self)\n                return self.posns.doc_posns(term_id, doc_id)\n            except TermMissingError:\n                return []\n        else:\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                doc_ids = self.term_mat.rows\n                return self.posns.doc_posns(term_id, doc_ids)\n            except TermMissingError:\n                return []\n\n    "}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            if not isinstance(key, numbers.Integral):\n                raise TypeError(\"Expected an integer key\")\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                return self.posns.positions(term_id, doc_id=key)\n            except TermMissingError:\n                return []\n        else:\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                return self.posns.positions(term_id)\n            except TermMissingError:\n                return []\n\n    "}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            return self.posns.doc_posns(self.term_dict.get_term_id(token))\n        else:\n            return self.posns.doc_posns(self.term_dict.get_term_id(token), key=key)\n\n    "}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            if not isinstance(key, numbers.Integral):\n                raise TypeError(\"Expected an integer key\")\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                doc_id = key\n                if doc_id < 0:\n                    doc_id += len(self)\n                return self.posns.doc_posns(term_id, doc_id)\n            except TermMissingError:\n                return []\n        else:\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                doc_ids = self.term_mat.rows\n                return self.posns.doc_posns(term_id, doc_ids)\n            except TermMissingError:\n                return []\n\n    "}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            try:\n                return self.posns.get_positions(self.term_dict.get_term_id(token), key)\n            except TermMissingError:\n                return []\n        else:\n            try:\n                return self.posns.get_positions(self.term_dict.get_term_id(token))\n            except TermMissingError:\n                return []\n\n    "}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        # Get term freqs per token\n        if key is None:\n            return self.posns.positions(token)\n        else:\n            return self.posns.positions(token, key)\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                return self.posns.get_posns(term_id, key)\n            except TermMissingError:\n                return []\n        else:\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                return self.posns.get_posns(term_id)\n            except TermMissingError:\n                return []\n\n    "}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            if isinstance(key, numbers.Integral):\n                if key < 0:\n                    key += len(self)\n                try:\n                    return self.posns.doc_posns(self.term_dict.get_term_id(token), key)\n                except TermMissingError:\n                    return []\n            else:\n                raise ValueError(\"Key must be an integer\")\n        else:\n            try:\n                return self.posns.posns(self.term_dict.get_term_id(token))\n            except TermMissingError:\n                return []\n\n    "}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            doc_ids = self.term_mat.rows\n        else:\n            doc_ids = [key]\n\n        try:\n            term_id = self.term_dict.get_term_id(token)\n            return self.posns.positions(term_id, doc_ids=doc_ids)\n        except TermMissingError:\n            return []\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if isinstance(token, list):\n            raise ValueError(\"Expected a single term, not a list of terms\")\n        if key is None:\n            return self.posns.positions(self.term_dict.get_term_id(token))\n        else:\n            return self.posns.positions(self.term_dict.get_term_id(token), key)\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        # If a specific document key is provided, retrieve the positions for that document only\n        if key is not None:\n            try:\n                doc_id = self.term_mat.rows[key]\n                return self.posns.doc_posns(self.term_dict.get_term_id(token), doc_id)\n            except TermMissingError:\n                return []\n        # If no key is provided, retrieve positions across all documents\n        else:\n            try:\n                doc_ids = self.term_mat.rows\n                return self.posns.doc_posns(self.term_dict.get_term_id(token), doc_ids)\n            except TermMissingError:\n                return []\n\n    "}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            if isinstance(key, numbers.Integral):\n                key = np.asarray([key])\n            elif isinstance(key, list):\n                key = np.asarray(key)\n            elif isinstance(key, np.ndarray):\n                pass\n            else:\n                raise ValueError(\"Invalid key type. Must be an integer, list of integers, or numpy array.\")\n\n        try:\n            term_id = self.term_dict.get_term_id(token)\n            if key is None:\n                return self.posns.positions(term_id)\n            else:\n                return self.posns.positions(term_id, key)\n        except TermMissingError:\n            return []\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if isinstance(token, str):\n            tokens = [token]\n        else:\n            tokens = token\n\n        if key is not None:\n            if not isinstance(key, numbers.Integral):\n                raise TypeError(\"Expected an integer key\")\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                return self.posns.positions(term_id, doc_id=key)\n            except TermMissingError:\n                return []\n        else:\n            posns = []\n            for token in tokens:\n                try:\n                    term_id = self.term_dict.get_term_id(token)\n                    posns.append(self.posns.positions(term_id))\n                except TermMissingError:\n                    posns.append([])\n            return posns\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is None:\n            key = np.arange(len(self))\n        else:\n            key = np.asarray([key])\n\n        if isinstance(token, str):\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                doc_ids, posns = self.posns.positions(term_id, key)\n                return posns\n            except TermMissingError:\n                return []\n        else:\n            raise ValueError(\"Expected a string\")\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            key = pd.api.indexers.check_array_indexer(self, key)\n            if isinstance(key, numbers.Integral):\n                key = [key]\n            elif isinstance(key, slice):\n                key = np.arange(len(self))[key]\n\n        try:\n            term_id = self.term_dict.get_term_id(token)\n            if self.term_mat.subset:\n                slice_of_rows = self.term_mat.rows\n                doc_ids, posns = self.posns.positions(term_id,\n                                                      doc_ids=slice_of_rows)\n                if key is not None:\n                    mask = np.isin(doc_ids, key)\n                    doc_ids = doc_ids[mask]\n                    posns = posns[mask]\n                return [np.array(posns_l) for posns_l in posns]\n            else:\n                doc_ids, posns = self.posns.positions(term_id,\n                                                      doc_ids=key)\n                return [np.array(posns_l) for posns_l in posns]\n        except TermMissingError:\n            return []\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        # Check if the token is a string or a list of strings\n        if isinstance(token, list):\n            # If it's a list of strings, raise a TypeError\n            raise TypeError(\"Expected a string, not a list of strings\")\n\n        # Get the term ID for the given token\n        term_id = self.term_dict.get_term_id(token)\n\n        # If a specific document key is provided, retrieve the positions for that document\n        if key is not None:\n            # Get the positions for the given term and document key\n            return self.posns.positions(term_id, key)\n\n        # Otherwise, retrieve the positions across all documents\n        return self.posns.positions(term_id)"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            if key < 0:\n                key += len(self)\n            try:\n                return self.posns.positions(self.term_dict.get_term_id(token), doc_id=key)\n            except TermMissingError:\n                return np.array([])\n        else:\n            try:\n                return self.posns.positions(self.term_dict.get_term_id(token))\n            except TermMissingError:\n                return np.array([])\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        # Check if token is a string\n        if not isinstance(token, str):\n            raise TypeError(\"Expected a string\")\n\n        # Check if key is a valid index\n        if key is not None and not isinstance(key, int):\n            raise TypeError(\"Expected an integer for key\")\n\n        # Check if key is within bounds\n        if key is not None and key >= len(self):\n            raise ValueError(\"Key out of bounds\")\n\n        # Get term id\n        term_id = self.term_dict.get_term_id(token)\n\n        # Get positions\n        if key is not None:\n            doc_id = self.term_mat.rows[key]\n            return self.posns.doc_posns(term_id, doc_id)\n        else:\n            doc_ids = self.term_mat.rows\n            return [self.posns.doc_posns(term_id, doc_id) for doc_id in doc_ids]\n\n    "}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            # Get term freqs per token\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                doc_id = self.term_mat.rows[key]\n                return self.posns.positions(term_id, doc_id=doc_id)\n            except TermMissingError:\n                return np.array([], dtype=int)\n        else:\n            # Get term freqs per token\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                doc_ids, positions = self.posns.positions(term_id)\n                return [positions[doc_id] for doc_id in doc_ids]\n            except TermMissingError:\n                return np.array([], dtype=int)\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            return self.posns.get_positions(token, doc_id=key)\n        else:\n            return self.posns.get_positions(token)\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith(\"%\"):\n        return int(num_clauses * float(spec[:-1]) / 100)\n    elif spec.startswith(\"<\"):\n        return num_clauses - int(spec[1:])\n    else:\n        return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith(\"%\"):\n        return int(float(spec[:-1]) * num_clauses / 100)\n    elif spec.startswith(\"<\"):\n        return int(num_clauses - float(spec[1:]))\n    else:\n        return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith(\"%\"):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n    elif spec.startswith(\"<\"):\n        return int(num_clauses - float(spec[1:]))\n    else:\n        return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith('%'):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n    elif spec.startswith('<'):\n        return num_clauses - int(spec[1:])\n    else:\n        return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith(\"%\"):\n        return int(float(spec[:-1]) * num_clauses / 100)\n    elif spec.startswith(\"<\"):\n        return int(num_clauses - float(spec[1:]))\n    else:\n        return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith('%'):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n    elif spec.startswith('<'):\n        return num_clauses - int(spec[1:])\n    else:\n        return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith(\"%\"):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n    elif spec.startswith(\"<\"):\n        return num_clauses - int(spec[1:]) + 1\n    else:\n        return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith(\"%\"):\n        return int(float(spec[:-1]) * num_clauses / 100)\n    elif \"<\" in spec:\n        min_should_match, max_should_match = map(int, spec.split(\"<\"))\n        return min_should_match\n    else:\n        return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec is None:\n        return num_clauses\n\n    if spec.endswith(\"%\"):\n        percent = float(spec[:-1])\n        return int(num_clauses * percent / 100)\n\n    if spec.startswith(\"<\"):\n        min_should_match = int(spec[1:])\n        if min_should_match > num_clauses:\n            return num_clauses\n        return min_should_match\n\n    return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Check if the spec is an absolute number\n    if spec.isdigit():\n        return int(spec)\n\n    # Check if the spec is a percentage\n    elif spec.endswith('%'):\n        percentage = float(spec[:-1])\n        return int(num_clauses * percentage / 100)\n\n    # Check if the spec is a conditional expression\n    elif '<' in spec:\n        parts = spec.split('<')\n        if len(parts) != 2:\n            raise ValueError(f\"Invalid 'mm' spec: {spec}\")\n\n        left_spec = parts[0].strip()\n        right_spec = parts[1].strip()\n\n        left_num = parse_min_should_match(num_clauses, left_spec)\n        right_num = parse_min_should_match(num_clauses, right_spec)\n\n        return max(left_num, right_num)\n\n    else:\n        raise ValueError(f\"Invalid 'mm' spec: {spec}\")"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Check if the spec is a percentage\n    if spec.endswith('%'):\n        # Parse the percentage value\n        percentage = float(spec[:-1])\n        # Calculate the minimum number of clauses that must match based on the percentage\n        return int(num_clauses * percentage / 100)\n\n    # Check if the spec is a conditional expression\n    if '<' in spec:\n        # Parse the conditional expression\n        left, right = spec.split('<')\n        # Calculate the minimum number of clauses that must match based on the conditional expression\n        return max(int(left), num_clauses - int(right))\n\n    # The spec is an absolute number\n    return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith(\"%\"):\n        # Percentage-based specification\n        percentage = int(spec[:-1])\n        return max(1, round(num_clauses * percentage / 100))\n    elif \"<\" in spec:\n        # Conditional specification\n        parts = spec.split(\"<\")\n        left_clauses = parse_min_should_match(num_clauses, parts[0])\n        right_clauses = parse_min_should_match(num_clauses, parts[1])\n        return min(left_clauses, right_clauses)\n    else:\n        # Absolute number of clauses\n        return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == \"0%\":\n        return 0\n    if spec == \"100%\":\n        return num_clauses\n    if spec.endswith(\"%\"):\n        return int(round(float(spec[:-1]) * num_clauses / 100))\n    if \"<\" in spec:\n        parts = spec.split(\"<\")\n        if len(parts) != 2:\n            raise ValueError(\"Invalid 'mm' spec: \" + spec)\n        left = parse_min_should_match(num_clauses, parts[0])\n        right = parse_min_should_match(num_clauses, parts[1])\n        return min(left, right)\n    return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == \"0%\":\n        return 0\n    if spec == \"100%\":\n        return num_clauses\n    if spec.endswith(\"%\"):\n        return int(num_clauses * float(spec[:-1]) / 100)\n    if spec.startswith(\"<\"):\n        min_should_match = int(spec[1:])\n        if min_should_match < 0 or min_should_match > num_clauses:\n            raise ValueError(f\"Invalid min should match specification: {spec}\")\n        return min_should_match\n    try:\n        min_should_match = int(spec)\n    except ValueError:\n        raise ValueError(f\"Invalid min should match specification: {spec}\")\n    if min_should_match < 0 or min_should_match > num_clauses:\n        raise ValueError(f\"Invalid min should match specification: {spec}\")\n    return min_should_match"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec is None:\n        return num_clauses\n\n    if spec.endswith(\"%\"):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n\n    if spec.startswith(\"<\"):\n        # Check if the number of clauses is less than the specified value\n        if num_clauses < int(spec[1:]):\n            return 0\n        else:\n            return num_clauses\n\n    return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith(\"%\"):\n        # Percentage-based specification\n        percentage = float(spec[:-1]) / 100\n        return max(int(num_clauses * percentage), 1)\n    elif \"<\" in spec:\n        # Conditional specification\n        parts = spec.split(\"<\")\n        threshold = int(parts[0])\n        min_clauses = int(parts[1])\n        return min(num_clauses, threshold) if num_clauses < threshold else min_clauses\n    else:\n        # Absolute number of clauses\n        return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Parse the 'mm' spec\n    if spec.endswith(\"%\"):\n        # Percentage-based specification\n        percentage = float(spec[:-1])\n        min_should_match = int(num_clauses * percentage / 100)\n    elif \"<\" in spec:\n        # Conditional specification\n        parts = spec.split(\"<\")\n        min_should_match = int(parts[0])\n        if len(parts) > 1:\n            min_should_match = max(min_should_match, int(parts[1]))\n    else:\n        # Absolute number specification\n        min_should_match = int(spec)\n\n    # Ensure that min_should_match is at least 1\n    min_should_match = max(1, min_should_match)\n\n    return min_should_match"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    spec = spec.strip()\n    if spec.endswith(\"%\"):\n        return int(num_clauses * float(spec[:-1]) / 100)\n    elif spec.startswith(\"<\"):\n        # Parse conditional expression\n        parts = spec.split()\n        if len(parts) != 3 or parts[1] != \"<\":\n            raise ValueError(f\"Invalid 'mm' spec: {spec}\")\n        try:\n            threshold = int(parts[0])\n            min_clauses = int(parts[2])\n        except ValueError:\n            raise ValueError(f\"Invalid 'mm' spec: {spec}\")\n        if threshold > num_clauses:\n            return min_clauses\n        else:\n            return num_clauses - threshold + 1\n    else:\n        try:\n            return int(spec)\n        except ValueError:\n            raise ValueError(f\"Invalid 'mm' spec: {spec}\")"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.endswith(\"%\"):\n        # Parse percentage\n        percentage = float(spec[:-1])\n        return int(num_clauses * percentage / 100)\n    elif \"<\" in spec:\n        # Parse conditional expression\n        parts = spec.split(\"<\")\n        threshold = float(parts[0])\n        num_should_match = int(parts[1])\n        return min(num_clauses, max(num_should_match, int(threshold * num_clauses)))\n    else:\n        # Parse absolute number\n        return int(spec)"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Initialize the minimum number of clauses that must match to 0\n    min_should_match = 0\n\n    # Check if the 'mm' spec is a percentage\n    if spec.endswith('%'):\n        # Calculate the percentage of clauses that must match\n        min_should_match = int(float(spec[:-1]) / 100 * num_clauses)\n\n    # Check if the 'mm' spec is a conditional expression\n    elif '<' in spec:\n        # Split the conditional expression into two parts\n        parts = spec.split('<')\n\n        # Calculate the minimum number of clauses that must match for the first part\n        min_should_match = parse_min_should_match(num_clauses, parts[0])\n\n        # Calculate the minimum number of clauses that must match for the second part\n        min_should_match += parse_min_should_match(num_clauses, parts[1])\n\n    # If the 'mm' spec is neither a percentage nor a conditional expression, it is assumed to be an absolute number\n    else:\n        # Convert the 'mm' spec to an integer and assign it to the minimum number of clauses that must match\n        min_should_match = int(spec)\n\n    # Return the minimum number of clauses that must match\n    return min_should_match"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_direct(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_scan(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_direct(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(tokens) == len(set(tokens)):\n            return self.phrase_freq_scan(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)\n\n    "}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_direct(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_direct(tokens)\n        else:\n            return self.phrase_freq_scan(tokens, slop=slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_direct(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_direct(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_direct(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        # If the slop is 1 and all tokens are unique, we can directly calculate the phrase frequency\n        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_scan(tokens)\n\n        # Otherwise, we delegate to a different method\n        return self.phrase_freq_every_diff(tokens, slop=slop)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(tokens) == len(set(tokens)):\n            return self.phrase_freq_direct(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(tokens) == len(set(tokens)):\n            return self.phrase_freq_direct(tokens)\n        else:\n            return self.phrase_freq_scan(tokens, slop=slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_positions(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if len(tokens) == 0:\n            return np.zeros(len(self), dtype=int)\n\n        if len(tokens) == 1:\n            return self.termfreqs(tokens[0])\n\n        # If slop is 1 and all tokens are unique, we can directly calculate the phrase frequencies\n        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_direct(tokens)\n\n        # If slop is not 1 or tokens are not unique, we delegate the calculation to another method\n        return self.phrase_freq_every_diff(tokens, slop=slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        # If we have a single token, just return the term freqs\n        if len(tokens) == 1:\n            return self.termfreqs(tokens[0])\n\n        # If all tokens are unique, and we have slop=1, we can do a direct lookup\n        if len(set(tokens)) == len(tokens) and slop == 1:\n            return self.phrase_freq_direct(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_direct(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_direct(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_direct(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        # If slop is 1 and all tokens are unique, we can calculate the phrase frequencies directly\n        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_direct(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        # If slop is 1 and all tokens are unique, try to calculate phrase frequencies directly\n        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_direct(tokens, mask=None)\n\n        # If slop is not 1 or tokens are not unique, delegate to another method\n        return self.phrase_freq_every_diff(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if truncate:\n            array = array[:truncate]\n\n        # Convert to a list of strings\n        if not isinstance(array, list):\n            array = list(array)\n\n        # Convert to a list of strings\n        if not isinstance(array, list):\n            array = list(array)\n\n        # Convert to a list of strings\n        if not isinstance(array, list):\n            array = list(array)\n\n        # Convert to a list of strings\n        if not isinstance(array, list):\n            array = list(array)\n\n        # Convert to a list of strings\n        if not isinstance(array, list):\n            array = list(array)\n\n        # Convert to a list of strings\n        if not isinstance(array, list):\n            array = list(array)\n\n        # Convert to a list of strings\n        if not isinstance(array, list):\n            array = list(array)\n\n        # Convert to a list of strings\n        if not isinstance(array, list):\n            array = list(array)\n\n        # Convert to a list of strings\n        if not isinstance(array, list):\n            array = list(array)\n\n        # Convert to a list of strings\n        if not isinstance(array, list):\n            array = list(array)\n\n        # Convert to a list of strings\n        if not isinstance(array, list):\n            array = list(array)\n\n        # Convert to a list of strings\n        if not isinstance(array, list):\n            array = list(array)\n\n        # Convert to a list of strings\n        if not isinstance(array, list):\n            array = list(array)\n\n        # Convert to a list of strings\n        if not isinstance(array, list):\n            array = list(array)\n\n        # Convert to a list of strings\n        if not isinstance(array, list):\n            array = list(array)\n\n        # Convert to a list of strings\n        if not isinstance(array, list):\n            array = list(array)\n\n        # Convert to a list of strings\n        if not isinstance(array, list"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if truncate:\n            array = array[:truncate]\n        if not isinstance(array, np.ndarray):\n            array = np.asarray(array, dtype=object)\n\n        if len(array) > batch_size:\n            postings = []\n            doc_lens = []\n            for i in range(0, len(array), batch_size):\n                batch = array[i:i + batch_size]\n                postings_batch, doc_lens_batch = build_index_from_tokenizer(batch, tokenizer)\n                postings.append(postings_batch)\n                doc_lens.append(doc_lens_batch)\n            postings = np.concatenate(postings)\n            doc_lens = np.concatenate(doc_lens)\n        else:\n            postings, doc_lens = build_index_from_tokenizer(array, tokenizer)\n\n        return cls(postings, tokenizer=tokenizer, avoid_copies=avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if truncate:\n            raise NotImplementedError(\"Truncate not implemented yet\")\n\n        if batch_size is None:\n            batch_size = 100000\n\n        # Build the index\n        if isinstance(array, pd.Series):\n            array = array.values\n\n        if isinstance(array, pd.DataFrame):\n            array = array.values.flatten()\n\n        if isinstance(array, np.ndarray) and array.dtype == object:\n            array = array.tolist()\n\n        if not isinstance(array, list):\n            array = list(array)\n\n        # Check if array is empty\n        if len(array) == 0:\n            return cls([], tokenizer=tokenizer)\n\n        # Check if array is a list of strings\n        if not isinstance(array[0], str):\n            raise ValueError(\"Expected a list of strings\")\n\n        # Check if array is a list of strings\n        if not isinstance(array[0], str):\n            raise ValueError(\"Expected a list of strings\")\n\n        # Check if array is a list of strings\n        if not isinstance(array[0], str):\n            raise ValueError(\"Expected a list of strings\")\n\n        # Check if array is a list of strings\n        if not isinstance(array[0], str):\n            raise ValueError(\"Expected a list of strings\")\n\n        # Check if array is a list of strings\n        if not isinstance(array[0], str):\n            raise ValueError(\"Expected a list of strings\")\n\n        # Check if array is a list of strings\n        if not isinstance(array[0], str):\n            raise ValueError(\"Expected a list of strings\")\n\n        # Check if array is a list of strings\n        if not isinstance(array[0], str):\n            raise ValueError(\"Expected a list of strings\")\n\n        # Check if array is a list of strings\n        if not isinstance(array[0], str):\n            raise ValueError(\"Expected a list of strings\")\n\n        # Check if array is a list of strings\n        if not isinstance(array[0"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if truncate:\n            # Truncate to fit within memory constraints\n            raise NotImplementedError(\"Truncation not implemented yet\")\n\n        # Build index in batches\n        if batch_size > 0:\n            batches = [array[i:i + batch_size] for i in range(0, len(array), batch_size)]\n        else:\n            batches = [array]\n\n        postings = []\n        for batch in batches:\n            postings.extend(build_index_from_tokenizer(batch, tokenizer))\n\n        return cls(postings, tokenizer=tokenizer, avoid_copies=avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if truncate:\n            # Truncate the data to fit within memory constraints\n            array = array[:batch_size]\n        else:\n            array = array\n\n        postings = []\n        doc_lens = []\n        for i in range(0, len(array), batch_size):\n            batch = array[i:i + batch_size]\n            postings_batch, term_dict_batch, doc_lens_batch = build_index_from_tokenizer(batch, tokenizer)\n            postings.extend(postings_batch)\n            doc_lens.extend(doc_lens_batch)\n\n        return cls(postings, tokenizer=tokenizer, avoid_copies=avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if truncate:\n            raise NotImplementedError(\"Truncation not implemented yet\")\n        if batch_size is None:\n            batch_size = 100000\n        if batch_size < 1:\n            raise ValueError(\"batch_size must be greater than 0\")\n\n        # Build index from array\n        postings_arr = cls.index_from_array(array, tokenizer, batch_size, avoid_copies)\n        return postings_arr\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # If we have a list of strings, tokenize them\n        if isinstance(array, list):\n            array = [tokenizer(x) for x in array]\n        # If we have a dataframe, tokenize it\n        elif isinstance(array, pd.DataFrame):\n            array = array.applymap(tokenizer)\n        # If we have a series, tokenize it\n        elif isinstance(array, pd.Series):\n            array = array.apply(tokenizer)\n        # If we have a single string, tokenize it\n        elif isinstance(array, str):\n            array = tokenizer(array)\n\n        # If we have a list of lists, tokenize it\n        elif isinstance(array, list):\n            array = [tokenizer(x) for x in array]\n        # If we have a numpy array, tokenize it\n        elif isinstance(array, np.ndarray):\n            array = np.vectorize(tokenizer)(array)\n        # If we have a pandas dataframe, tokenize it\n        elif isinstance(array, pd.DataFrame):\n            array = array.applymap(tokenizer)\n        # If we have a pandas series, tokenize it\n        elif isinstance(array, pd.Series):\n            array = array.apply(tokenizer)\n        # If we have a single string, tokenize it\n        elif isinstance(array, str):\n            array = tokenizer(array)\n        # If we have a list of lists, tokenize it\n        elif isinstance(array, list):\n            array = [tokenizer(x) for x in array]\n        # If we have a numpy array, tokenize it\n        elif isinstance(array, np.ndarray):\n            array = np.vectorize(tokenizer)(array)\n        # If we have a pandas dataframe, tokenize it\n        elif isinstance(array, pd.DataFrame):\n            array = array.applymap(tokenizer)\n        # If we have a pandas series, tokenize it\n        elif isinstance(array, pd.Series):\n            array = array.apply(tokenizer)\n        #"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if truncate:\n            raise NotImplementedError(\"Truncation not implemented yet\")\n\n        postings_list = []\n        doc_lens = []\n        for doc_id, doc in enumerate(array):\n            if doc_id % batch_size == 0:\n                term_mat, posns, \\\n                    term_dict, avg_doc_length, \\\n                    doc_lens = build_index_from_tokenizer(postings_list, tokenizer,\n                                                           Terms,\n                                                           doc_lens=doc_lens,\n                                                           avoid_copies=avoid_copies)\n                postings_list = []\n                doc_lens = []\n            postings_list.append(doc)\n            doc_lens.append(len(doc))\n\n        if len(postings_list) > 0:\n            term_mat, posns, \\\n                term_dict, avg_doc_length, \\\n                doc_lens = build_index_from_tokenizer(postings_list, tokenizer,\n                                                       Terms,\n                                                       doc_lens=doc_lens,\n                                                       avoid_copies=avoid_copies)\n\n        arr = cls([], tokenizer=tokenizer)\n        arr.term_mat = term_mat\n        arr.posns = posns\n        arr.term_dict = term_dict\n        arr.avg_doc_length = avg_doc_length\n        arr.doc_lens = np.asarray(doc_lens)\n        return arr\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if truncate:\n            array = array[:batch_size]\n        term_dict = TermDict()\n        postings = []\n        doc_lens = []\n        posns = PosnBitArray()\n        for doc_id, doc in enumerate(array):\n            tokens = tokenizer(doc)\n            doc_len = len(tokens)\n            tfs = Counter(tokens)\n            postings.append(tfs)\n            doc_lens.append(doc_len)\n            posns.add_doc(tfs, doc_id=doc_id)\n            term_dict.add_terms(tfs.keys())\n        avg_doc_length = np.mean(doc_lens)\n        term_mat = TermMatrix(postings, term_dict, avg_doc_length, doc_lens)\n        return cls(postings, tokenizer=tokenizer, term_mat=term_mat,\n                   posns=posns, term_dict=term_dict,\n                   avg_doc_length=avg_doc_length, doc_lens=doc_lens,\n                   avoid_copies=avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Check if array is a list of strings\n        if not isinstance(array, list) or not all(isinstance(item, str) for item in array):\n            raise TypeError(\"array must be a list of strings\")\n\n        # Check if tokenizer is a function\n        if not callable(tokenizer):\n            raise TypeError(\"tokenizer must be a function\")\n\n        # Check if truncate is a boolean\n        if not isinstance(truncate, bool):\n            raise TypeError(\"truncate must be a boolean\")\n\n        # Check if batch_size is an integer\n        if not isinstance(batch_size, int):\n            raise TypeError(\"batch_size must be an integer\")\n\n        # Check if avoid_copies is a boolean\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"avoid_copies must be a boolean\")\n\n        # If truncate is True, truncate the array to fit within memory constraints\n        if truncate:\n            array = array[:batch_size]\n\n        # Create an empty list to store the indexed data\n        indexed_data = []\n\n        # Process the array in batches\n        for i in range(0, len(array), batch_size):\n            # Get the current batch\n            batch = array[i:i + batch_size]\n\n            # Index the batch using the given tokenizer\n            indexed_batch = [tokenizer(item) for item in batch]\n\n            # Append the indexed batch to the list of indexed data\n            indexed_data.extend(indexed_batch)\n\n        # Create an instance of SearchArray and return it\n        return cls(indexed_data, tokenizer=tokenizer, avoid_copies=avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        if truncate:\n            array = array[:batch_size]\n\n        postings = []\n        for i in range(0, len(array), batch_size):\n            batch = array[i:i + batch_size]\n            batch_postings = build_index_from_tokenizer(batch, tokenizer=tokenizer)\n            postings.extend(batch_postings)\n\n        return cls(postings, tokenizer=tokenizer, avoid_copies=avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if truncate:\n            array = array[:batch_size]\n        term_mat, posns, term_dict, avg_doc_length, doc_lens = build_index_from_tokenizer(array,\n                                                                                          tokenizer,\n                                                                                          Terms,\n                                                                                          batch_size=batch_size)\n        return cls(postings=term_mat,\n                   tokenizer=tokenizer,\n                   avoid_copies=avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if truncate:\n            array = array[:batch_size]\n        else:\n            array = array\n\n        # Build index\n        postings = []\n        doc_lens = []\n        for doc_id, doc in enumerate(array):\n            doc_len, postings_row = build_index_from_tokenizer(doc, tokenizer)\n            postings.append(postings_row)\n            doc_lens.append(doc_len)\n\n        return cls(postings, tokenizer=tokenizer, avoid_copies=avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        # Check if we can fit the whole array into memory\n        if truncate:\n            array = array[:batch_size]\n\n        # Build the index\n        term_mat, posns, term_dict, avg_doc_length, doc_lens = build_index_from_tokenizer(\n            array, tokenizer, batch_size, avoid_copies)\n\n        # Create a new instance of SearchArray\n        arr = cls([], tokenizer=tokenizer)\n        arr.term_mat = term_mat\n        arr.posns = posns\n        arr.term_dict = term_dict\n        arr.avg_doc_length = avg_doc_length\n        arr.doc_lens = doc_lens\n        return arr\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not isinstance(array, np.ndarray):\n            array = np.asarray(array, dtype=object)\n\n        if truncate:\n            array = array[:truncate]\n\n        if batch_size is None:\n            batch_size = len(array)\n\n        postings = []\n        doc_lens = []\n        for i in range(0, len(array), batch_size):\n            batch = array[i:i + batch_size]\n            postings_batch, posns, term_dict, avg_doc_length, doc_lens_batch = build_index_from_tokenizer(batch, tokenizer)\n            postings.append(postings_batch)\n            doc_lens.append(doc_lens_batch)\n\n        postings = np.concatenate(postings)\n        doc_lens = np.concatenate(doc_lens)\n        return cls(postings, tokenizer=tokenizer, avoid_copies=avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if truncate:\n            array = array[:batch_size]\n\n        # Check if array is a list of strings\n        if isinstance(array, list) and isinstance(array[0], str):\n            # If it is, we can use the build_index_from_tokenizer function\n            term_mat, posns, term_dict, avg_doc_length, doc_lens = \\\n                build_index_from_tokenizer(array, tokenizer, Terms, batch_size=batch_size)\n        else:\n            # If not, we need to convert the array to a list of strings\n            array = list(array)\n            term_mat, posns, term_dict, avg_doc_length, doc_lens = \\\n                build_index_from_tokenizer(array, tokenizer, Terms, batch_size=batch_size)\n\n        return cls(term_mat, term_dict, posns, avg_doc_length, doc_lens, avoid_copies=avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if truncate:\n            array = array[:batch_size]\n\n        # Build index from array\n        term_mat, posns, term_dict, avg_doc_length, doc_lens = build_index_from_tokenizer(\n            array, tokenizer, batch_size=batch_size, avoid_copies=avoid_copies)\n\n        return cls(postings=term_mat, tokenizer=tokenizer, avoid_copies=avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if truncate:\n            # Truncate to fit in memory\n            array = array[:batch_size]\n        term_dict = TermDict()\n        term_mat = TermMat()\n        posns = PosnBitArray()\n        doc_lens = np.zeros(len(array), dtype=int)\n        avg_doc_length = 0\n        for doc_id, doc in enumerate(array):\n            if doc_id % 10000 == 0:\n                logger.debug(f\"Indexing {doc_id}\")\n            if doc_id % batch_size == 0 and doc_id > 0:\n                term_mat.resize((doc_id, len(term_dict)))\n                posns.resize((doc_id, len(term_dict)))\n                doc_lens = doc_lens[:doc_id]\n                avg_doc_length = np.mean(doc_lens)\n                yield SearchArray([], tokenizer=tokenizer, avoid_copies=avoid_copies)\n                term_dict = TermDict()\n                term_mat = TermMat()\n                posns = PosnBitArray()\n                doc_lens = np.zeros(len(array), dtype=int)\n                avg_doc_length = 0\n            doc_len, tfs, posns = build_index_from_tokenizer(doc, tokenizer, term_dict)\n            doc_lens[doc_id] = doc_len\n            term_mat.insert(doc_id, tfs)\n            posns.insert(doc_id, posns)\n        term_mat.resize((len(array), len(term_dict)))\n        posns.resize((len(array), len(term_dict)))\n        doc_lens = doc_lens[:len(array)]\n        avg_doc_length = np.mean(doc_lens)\n        yield SearchArray([], tokenizer=tokenizer, avoid_copies=avoid_copies)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # If we're not avoiding copies, we need to build the index from the given array\n        if not avoid_copies:\n            postings = build_index_from_tokenizer(array, tokenizer)\n            return cls(postings, tokenizer=tokenizer)\n\n        # If we're avoiding copies, we need to build the index in batches\n        else:\n            # Initialize the index with an empty list\n            index = []\n\n            # Iterate over the array in batches\n            for i in range(0, len(array), batch_size):\n                # Get the current batch\n                batch = array[i:i+batch_size]\n\n                # Build the index for the current batch\n                postings = build_index_from_tokenizer(batch, tokenizer)\n\n                # Append the index for the current batch to the overall index\n                index.extend(postings)\n\n            # Return the overall index\n            return cls(index, tokenizer=tokenizer)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # We don't want to make a copy of the array, so we need to make a copy of the array\n        # if it is a list or a numpy array\n        if isinstance(array, list):\n            array = array.copy()\n        elif isinstance(array, np.ndarray):\n            array = array.copy()\n\n        if truncate:\n            array = array[:batch_size]\n\n        # Batch process the array\n        postings = []\n        doc_lens = []\n        for i in range(0, len(array), batch_size):\n            batch = array[i:i+batch_size]\n            postings_batch, doc_lens_batch = build_index_from_tokenizer(batch,\n                                                                       tokenizer=tokenizer,\n                                                                       return_doc_lens=True)\n            postings.extend(postings_batch)\n            doc_lens.extend(doc_lens_batch)\n\n        return cls(postings, tokenizer=tokenizer, avoid_copies=avoid_copies)\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(self.config['serverHost'], self.config['serverPort'], self.config['proxyHost'], self.config['proxyPort'])\n        self.server.start()\n\n        self.connections = {}\n        self.lock = threading.Lock()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(self.config['serverHost'], self.config['serverPort'], self.config['proxyHost'], self.config['proxyPort'])\n        self.server.start()\n\n        self.connections = {}\n        self.lock = threading.Lock()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(\n            self.config['proxyHost'],\n            self.config['proxyPort'],\n            self.config['serverHost'],\n            self.config['serverPort'],\n            self.logger,\n            self.config['strategy'],\n            self.config['strategies']\n        )\n        self.server.start()\n\n        self.connections = {}\n        self.lock = threading.Lock()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(\n            self.config['proxyHost'],\n            self.config['proxyPort'],\n            self.config['serverHost'],\n            self.config['serverPort'],\n            self.config['strategy'],\n            self.config['strategies'],\n        )\n        self.server.start()\n\n        self.connections = {}\n        self.lock = threading.Lock()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(self.config['serverHost'], self.config['serverPort'], self.config['proxyHost'], self.config['proxyPort'], self.config['strategy'], self.config['strategies'])\n        self.server.start()\n\n        self.connections = {}\n        self.lock = threading.Lock()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(\n            self.config['serverHost'],\n            self.config['serverPort'],\n            self.config['proxyHost'],\n            self.config['proxyPort'],\n            self.config['strategies'][self.config['strategy']],\n            self.logger,\n        )\n        self.server.start()\n\n        self.connections = {}\n        self.lock = threading.Lock()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        # Initialize server\n        self.server = Server(\n            self.config['serverHost'],\n            self.config['serverPort'],\n            self.config['proxyHost'],\n            self.config['proxyPort'],\n            self.logger,\n            self.config['strategy'],\n            self.config['strategies']\n        )\n        self.server.start()\n\n        # Initialize connections\n        self.connections = {}\n        self.lock = threading.Lock()\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(\n            self.config['serverHost'],\n            self.config['serverPort'],\n            self.config['proxyHost'],\n            self.config['proxyPort'],\n            self._get_strategy()\n        )\n        self.server.start()\n\n        self.connections = {}\n        self.lock = threading.Lock()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        # Initialize server\n        self.server = Server(self.config['serverHost'], self.config['serverPort'], self.logger)\n        self.server.start()\n\n        # Initialize connections\n        self.connections = {}\n\n        # Initialize lock\n        self.lock = threading.Lock()\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        # Initialize server\n        self.server = Server(\n            self.config['serverHost'],\n            self.config['serverPort'],\n            self.config['proxyHost'],\n            self.config['proxyPort'],\n            self._get_strategy(self.config['strategy']),\n            self.logger\n        )\n        self.server.start()\n\n        # Initialize connections\n        self.connections = {}\n        self.lock = threading.Lock()\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(\n            host=self.config['serverHost'],\n            port=self.config['serverPort'],\n            strategy=self._get_strategy(),\n        )\n        self.server.start()\n\n        self.connections = {}\n        self.lock = threading.Lock()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        # Create server\n        self.server = Server(self.config['serverHost'], self.config['serverPort'], self.config['proxyHost'], self.config['proxyPort'], self.logger)\n        self.server.start()\n\n        # Initialize connections\n        self.connections = {}\n        self.lock = threading.Lock()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(\n            self.config['proxyHost'],\n            self.config['proxyPort'],\n            self.config['serverHost'],\n            self.config['serverPort'],\n            self._get_strategy(),\n            self.logger\n        )\n        self.server.start()\n\n        self.connections = {}\n        self.lock = threading.Lock()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        # Create server\n        self.server = Server(\n            self.config['serverHost'], \n            self.config['serverPort'], \n            self.config['proxyHost'], \n            self.config['proxyPort'], \n            self.config['strategy'], \n            self.config['strategies'],\n            self.logger\n        )\n\n        # Start server\n        self.server.start()\n\n        # Initialize connections\n        self.connections = {}\n\n        # Initialize lock\n        self.lock = threading.Lock()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        # Create a server\n        self.server = Server(self.config['proxyHost'], self.config['proxyPort'], self.config['serverHost'], self.config['serverPort'])\n        self.server.start()\n\n        # Create a dictionary for connections\n        self.connections = {}\n\n        # Create a lock\n        self.lock = threading.Lock()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        # Initialize server\n        self.server = Server(self.config['proxyHost'], self.config['proxyPort'], self.config['serverHost'], self.config['serverPort'])\n        self.server.start()\n\n        # Initialize connections\n        self.connections = {}\n        self.lock = threading.Lock()\n\n        # Initialize strategies\n        strategy = self.config['strategy']\n        if strategy not in self.config['strategies']:\n            raise Exception(f'Strategy {strategy} is not supported.')\n        strategy_config = self.config['strategies'][strategy]\n        if strategy == 'buffer':\n            self.server.strategy = BufferStrategy(strategy_config['bufferSize'])\n        elif strategy == 'suffix':\n            self.server.strategy = SuffixStrategy(strategy_config['bufferSize'], strategy_config['value'])\n        elif strategy == 'length':\n            self.server.strategy = LengthStrategy()\n        else:\n            raise Exception(f'Strategy {strategy} is not supported.')"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        # Initialize server\n        self.server = Server(\n            self.config['serverHost'],\n            self.config['serverPort'],\n            self.config['proxyHost'],\n            self.config['proxyPort'],\n            self._extract_strategy(),\n        )\n        self.server.start()\n\n        # Initialize connections dictionary\n        self.connections = {}\n\n        # Initialize lock\n        self.lock = threading.Lock()\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(\n            self.config['proxyHost'], \n            self.config['proxyPort'],\n            self.config['serverHost'],\n            self.config['serverPort'],\n            self.logger,\n            self.config['strategies'].get(self.config['strategy'], {}),\n        )\n        self.server.start()\n\n        self.connections = {}\n        self.lock = threading.Lock()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        # Create server\n        self.server = Server(self.config['serverHost'], self.config['serverPort'], self.config['proxyHost'], self.config['proxyPort'])\n        self.server.start()\n\n        # Create strategies\n        strategy_name = self.config['strategy']\n        strategy_config = self.config['strategies'][strategy_name]\n        self.server.strategy = STRATEGIES[strategy_name](**strategy_config)\n\n        # Initialize connection dictionary\n        self.connections = {}\n\n        # Initialize lock\n        self.lock = threading.Lock()\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        # Create server\n        self.server = Server(\n            self.config['serverHost'],\n            self.config['serverPort'],\n            self.config['proxyHost'],\n            self.config['proxyPort'],\n            self._get_strategy(self.config['strategy'])\n        )\n        self.server.start()\n\n        # Initialize connection dictionary\n        self.connections = {}\n\n        # Initialize lock\n        self.lock = threading.Lock()\n    "}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> _1) & s55\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr + (arr >> _4)) & s0F\n    return (arr * s01) >> all_but_one_bit"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> _1) & s55\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr + (arr >> _4)) & s0F\n    arr = (arr * s01) >> all_but_one_bit\n    return arr"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> _1) & s55\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr + (arr >> _4)) & s0F\n    arr += arr >> all_but_one_bit\n    return arr"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> _1) & s55\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr + (arr >> _4)) & s0F\n    arr += arr >> all_but_one_bit\n    return arr"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> _1) & s55\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr + (arr >> _4)) & s0F\n    arr += arr >> all_but_one_bit\n    return arr"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> _1) & s55\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr + (arr >> _4)) & s0F\n    arr = (arr * _1) >> all_but_one_bit\n    return arr"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> _1) & s55\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr + (arr >> _4)) & s0F\n    arr += arr >> all_but_one_bit\n    return arr & mask"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> 1) & s55\n    arr = (arr & s33) + ((arr >> 2) & s33)\n    arr = (arr + (arr >> 4)) & s0F\n    arr = (arr * _1) >> all_but_one_bit\n    return arr"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> 1) & s55\n    arr = (arr & s33) + ((arr >> 2) & s33)\n    arr = (arr + (arr >> 4)) & s0F\n    arr = (arr * _1) >> all_but_one_bit\n    return arr"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> _1) & s55\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr + (arr >> _4)) & s0F\n    arr = (arr * _1) >> all_but_one_bit\n\n    return arr"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> 1) & s55\n    arr = (arr & s33) + ((arr >> 2) & s33)\n    arr = (arr + (arr >> 4)) & s0F\n    arr += arr >> 8\n    arr += arr >> 16\n    arr += arr >> 32\n    return arr & 0x7F"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> _1) & s55\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr + (arr >> _4)) & s0F\n    arr = (arr * _1) >> all_but_one_bit\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> _1) & s55\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr + (arr >> _4)) & s0F\n    arr += arr >> _8\n    arr += arr >> _16\n    arr += arr >> _32\n    return (arr & _63)"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> _1) & s55\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr + (arr >> _4)) & s0F\n    arr += arr >> _8\n    arr += arr >> _16\n    arr += arr >> _32\n    return arr & all_but_one_bit\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> _1) & s55\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr + (arr >> _4)) & s0F\n    arr += arr >> all_but_one_bit\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> 1) & s55\n    arr = (arr & s33) + ((arr >> 2) & s33)\n    arr = (arr + (arr >> 4)) & s0F\n    arr += arr >> 8\n    arr += arr >> 16\n    arr += arr >> 32\n    return arr & all_but_one_bit\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> 1) & s55\n    arr = (arr & s33) + ((arr >> 2) & s33)\n    arr = (arr + (arr >> 4)) & s0F\n    arr += arr >> 8\n    arr += arr >> 16\n    return (arr + (arr >> 32)) & 0x7F\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> 1) & s55\n    arr = (arr & s33) + ((arr >> 2) & s33)\n    arr = (arr + (arr >> 4)) & s0F\n    arr += arr >> 8\n    arr += arr >> 16\n    return (arr + (arr >> 32)) & mask\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> _1) & s55\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr + (arr >> _4)) & s0F\n    arr = (arr * _1) >> all_but_one_bit\n    return arr\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> _1) & s55\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr + (arr >> _4)) & s0F\n    arr = (arr * s01) >> all_but_one_bit\n    return arr\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = \"100%\"\n\n    query_fields = parse_field_boosts(qf)\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, query_fields.keys())\n\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    return qf_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = \"100%\"\n    if pf is None:\n        pf = []\n    if pf2 is None:\n        pf2 = []\n    if pf3 is None:\n        pf3 = []\n\n    qf_boosts = parse_field_boosts(qf)\n    pf_boosts = parse_field_boosts(pf)\n    pf2_boosts = parse_field_boosts(pf2)\n    pf3_boosts = parse_field_boosts(pf3)\n\n    query_fields = {**qf_boosts, **pf_boosts, **pf2_boosts, **pf3_boosts}\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, list(query_fields.keys()))\n\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    return qf_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = \"100%\"\n\n    if q_op not in [\"OR\", \"AND\"]:\n        raise ValueError(\"q_op must be 'OR' or 'AND'\")\n\n    query_fields = parse_field_boosts(qf)\n    if pf is not None:\n        query_fields.update(parse_field_boosts(pf))\n    if pf2 is not None:\n        query_fields.update(parse_field_boosts(pf2))\n    if pf3 is not None:\n        query_fields.update(parse_field_boosts(pf3))\n\n    if not query_fields:\n        raise ValueError(\"No query fields specified\")\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, query_fields.keys())\n\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    return qf_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = \"100%\"\n\n    qf_boosts = parse_field_boosts(qf)\n    pf_boosts = parse_field_boosts(pf)\n    pf2_boosts = parse_field_boosts(pf2)\n    pf3_boosts = parse_field_boosts(pf3)\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n    if term_centric:\n        qf_scores, qf_explain = _edismax_term_centric(frame, qf_boosts, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, qf_explain = _edismax_field_centric(frame, qf_boosts, num_search_terms, search_terms, mm, similarity)\n\n    pf_scores, pf_explain = _edismax_field_centric(frame, pf_boosts, num_search_terms, search_terms, mm, similarity)\n    pf2_scores, pf2_explain = _edismax_field_centric(frame, pf2_boosts, num_search_terms, search_terms, mm, similarity)\n    pf3_scores, pf3_explain = _edismax_field_centric(frame, pf3_boosts, num_search_terms, search_terms, mm, similarity)\n\n    explain = f\"{qf_explain} {pf_explain} {pf2_explain} {pf3_explain}\"\n\n    scores = qf_scores + pf_scores + pf2_scores + pf3_scores\n    return scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = \"100%\"\n\n    qf_boosts = parse_field_boosts(qf)\n    pf_boosts = parse_field_boosts(pf) if pf is not None else {}\n    pf2_boosts = parse_field_boosts(pf2) if pf2 is not None else {}\n    pf3_boosts = parse_field_boosts(pf3) if pf3 is not None else {}\n\n    # Parse query terms\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    # Compute query field scores\n    if term_centric:\n        qf_scores, qf_explain = _edismax_term_centric(frame, qf_boosts, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, qf_explain = _edismax_field_centric(frame, qf_boosts, num_search_terms, search_terms, mm, similarity)\n\n    # Compute phrase field scores\n    if pf is not None:\n        pf_scores, pf_explain = _edismax_field_centric(frame, pf_boosts, num_search_terms, search_terms, mm, similarity)\n    else:\n        pf_scores = np.zeros(len(frame))\n        pf_explain = \"\"\n\n    # Compute bigram field scores\n    if pf2 is not None:\n        pf2_scores, pf2_explain = _edismax_field_centric(frame, pf2_boosts, num_search_terms, search_terms, mm, similarity)\n    else:\n        pf2_scores = np.zeros(len(frame))\n        pf2_explain = \"\"\n\n    # Compute trigram field scores\n    if pf3 is not None:\n        pf3_scores, pf3_explain = _ed"}
{"namespace": "searcharray.solr.edismax", "completion": "    if q_op != \"OR\":\n        raise ValueError(\"Only q_op=OR is supported\")\n\n    if mm is None:\n        mm = \"100%\"\n\n    # Parse query fields\n    query_fields = parse_field_boosts(qf)\n    if pf is not None:\n        query_fields.update(parse_field_boosts(pf))\n    if pf2 is not None:\n        query_fields.update(parse_field_boosts(pf2))\n    if pf3 is not None:\n        query_fields.update(parse_field_boosts(pf3))\n\n    # Parse query terms\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, query_fields)\n\n    # Calculate scores\n    if term_centric:\n        scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    return scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = \"100%\"\n\n    # Parse query fields\n    qf_boosts = parse_field_boosts(qf)\n    pf_boosts = parse_field_boosts(pf)\n    pf2_boosts = parse_field_boosts(pf2)\n    pf3_boosts = parse_field_boosts(pf3)\n\n    # Parse query terms\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    # Perform edismax search\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, qf_boosts, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, qf_boosts, num_search_terms, search_terms, mm, similarity)\n\n    # Compute phrase scores\n    if pf is not None:\n        pf_scores, pf_explain = _edismax_field_centric(frame, pf_boosts, num_search_terms, search_terms, mm, similarity)\n        qf_scores += pf_scores\n        explain += \" \" + pf_explain\n    if pf2 is not None:\n        pf2_scores, pf2_explain = _edismax_field_centric(frame, pf2_boosts, num_search_terms, search_terms, mm, similarity)\n        qf_scores += pf2_scores\n        explain += \" \" + pf2_explain\n    if pf3 is not None:\n        pf3_scores, pf3_explain = _edismax_field_centric(frame, pf3_boosts, num_search_terms, search_terms, mm, similarity)\n        qf_scores += pf3_scores\n        explain += \" \" + pf3_"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = \"100%\"\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n    qf_scores, qf_explain = _edismax_term_centric(frame, parse_field_boosts(qf), num_search_terms, search_terms, mm, similarity)\n    if pf is not None:\n        pf_scores, pf_explain = _edismax_term_centric(frame, parse_field_boosts(pf), num_search_terms, search_terms, mm, similarity)\n        qf_scores = qf_scores + pf_scores\n        qf_explain = qf_explain + \" \" + pf_explain\n    if pf2 is not None:\n        pf2_scores, pf2_explain = _edismax_term_centric(frame, parse_field_boosts(pf2), num_search_terms, search_terms, mm, similarity)\n        qf_scores = qf_scores + pf2_scores\n        qf_explain = qf_explain + \" \" + pf2_explain\n    if pf3 is not None:\n        pf3_scores, pf3_explain = _edismax_term_centric(frame, parse_field_boosts(pf3), num_search_terms, search_terms, mm, similarity)\n        qf_scores = qf_scores + pf3_scores\n        qf_explain = qf_explain + \" \" + pf3_explain\n    return qf_scores, qf_explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    # parse query fields\n    qf_boosts = parse_field_boosts(qf)\n    pf_boosts = parse_field_boosts(pf)\n    pf2_boosts = parse_field_boosts(pf2)\n    pf3_boosts = parse_field_boosts(pf3)\n\n    # parse query terms\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    # compute qf scores\n    if term_centric:\n        qf_scores, qf_explain = _edismax_term_centric(frame, qf_boosts, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, qf_explain = _edismax_field_centric(frame, qf_boosts, num_search_terms, search_terms, mm, similarity)\n\n    # compute pf scores\n    pf_scores, pf_explain = _edismax_field_centric(frame, pf_boosts, num_search_terms, search_terms, mm, similarity)\n\n    # compute pf2 scores\n    pf2_scores, pf2_explain = _edismax_field_centric(frame, pf2_boosts, num_search_terms, search_terms, mm, similarity)\n\n    # compute pf3 scores\n    pf3_scores, pf3_explain = _edismax_field_centric(frame, pf3_boosts, num_search_terms, search_terms, mm, similarity)\n\n    # compute final scores\n    final_scores = qf_scores + pf_scores + pf2_scores + pf3_scores\n    explain = f\"{qf_explain} {pf_explain} {pf2_explain} {pf3_explain}\"\n\n    return final_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = \"100%\"\n    if q_op not in [\"OR\", \"AND\"]:\n        raise ValueError(\"q_op must be 'OR' or 'AND'\")\n\n    query_fields = parse_field_boosts(qf)\n    phrase_fields = parse_field_boosts(pf)\n    bigram_fields = parse_field_boosts(pf2)\n    trigram_fields = parse_field_boosts(pf3)\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, query_fields.keys())\n\n    if term_centric:\n        qf_scores, qf_explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, qf_explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    explain = f\"{qf_explain}\"\n\n    if phrase_fields:\n        pf_scores, pf_explain = _edismax_field_centric(frame, phrase_fields, num_search_terms, search_terms, mm, similarity)\n        explain = f\"{explain} {pf_explain}\"\n    else:\n        pf_scores = np.zeros(len(frame))\n\n    if bigram_fields:\n        pf2_scores, pf2_explain = _edismax_field_centric(frame, bigram_fields, num_search_terms, search_terms, mm, similarity)\n        explain = f\"{explain} {pf2_explain}\"\n    else:\n        pf2_scores = np.zeros(len(frame))\n\n    if trigram_fields:\n        pf3_scores, pf3_explain = _edismax_field_centric(frame, trigram_fields, num_search_terms, search_terms"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = \"100%\"\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n    if num_search_terms == 0:\n        return np.zeros(len(frame)), \"No search terms found\"\n\n    query_fields = parse_field_boosts(qf)\n\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    explain = f\"edismax({explain})\"\n    return qf_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = \"100%\"\n\n    # parse query fields\n    query_fields = parse_field_boosts(qf)\n\n    # parse phrase fields\n    phrase_fields = parse_field_boosts(pf)\n    bigram_fields = parse_field_boosts(pf2)\n    trigram_fields = parse_field_boosts(pf3)\n\n    # parse query\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, query_fields)\n\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    # TODO: handle pf, pf2, pf3\n\n    return qf_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    mm = mm or \"100%\"\n    pf = pf or []\n    pf2 = pf2 or []\n    pf3 = pf3 or []\n\n    query_fields = parse_field_boosts(qf)\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    if pf or pf2 or pf3:\n        explain += \" \"\n        if pf:\n            explain += \"pf: \" + \" \".join(pf)\n        if pf2:\n            explain += \" pf2: \" + \" \".join(pf2)\n        if pf3:\n            explain += \" pf3: \" + \" \".join(pf3)\n\n    return qf_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Parse query fields\n    query_fields = parse_field_boosts(qf)\n    # Parse phrase, bigram, and trigram fields\n    phrase_fields = parse_field_boosts(pf)\n    bigram_fields = parse_field_boosts(pf2)\n    trigram_fields = parse_field_boosts(pf3)\n\n    # Parse query terms\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, query_fields)\n\n    # Compute term-centric or field-centric scores\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    # Compute phrase, bigram, and trigram scores\n    phrase_scores = np.zeros(len(frame))\n    for field, boost in phrase_fields.items():\n        post_arr = get_field(frame, field)\n        phrase_scores = np.maximum(phrase_scores, post_arr.score_phrase(q, similarity=similarity) * (1 if boost is None else boost))\n    bigram_scores = np.zeros(len(frame))\n    for field, boost in bigram_fields.items():\n        post_arr = get_field(frame, field)\n        bigram_scores = np.maximum(bigram_scores, post_arr.score_bigram(q, similarity=similarity) * (1 if boost is None else boost))\n    trigram_scores = np.zeros(len(frame))\n    for field, boost in trigram_fields.items():\n        post_arr = get_field(frame, field)\n        trigram_scores = np.maximum(trigram_scores, post_arr.score_trigram(q,"}
{"namespace": "searcharray.solr.edismax", "completion": "    # TODO: Support for pf, pf2, pf3\n    if mm is None:\n        mm = \"100%\"\n    if pf is not None:\n        raise NotImplementedError(\"pf is not yet supported\")\n    if pf2 is not None:\n        raise NotImplementedError(\"pf2 is not yet supported\")\n    if pf3 is not None:\n        raise NotImplementedError(\"pf3 is not yet supported\")\n    if q_op != \"OR\":\n        raise NotImplementedError(\"q_op other than 'OR' is not yet supported\")\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n    query_fields = parse_field_boosts(qf)\n\n    if term_centric:\n        return _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        return _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)"}
{"namespace": "searcharray.solr.edismax", "completion": "    if not qf:\n        raise ValueError(\"qf cannot be empty\")\n\n    if mm is None:\n        mm = \"100%\"\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n    if num_search_terms == 0:\n        return np.zeros(len(frame)), \"*:*\"\n\n    qf_scores, qf_explain = _edismax_term_centric(frame,\n                                                  parse_field_boosts(qf),\n                                                  num_search_terms,\n                                                  search_terms,\n                                                  mm,\n                                                  similarity)\n    pf_scores, pf_explain = _edismax_field_centric(frame,\n                                                  parse_field_boosts(pf),\n                                                  num_search_terms,\n                                                  search_terms,\n                                                  mm,\n                                                  similarity)\n    pf2_scores, pf2_explain = _edismax_field_centric(frame,\n                                                     parse_field_boosts(pf2),\n                                                     num_search_terms,\n                                                     search_terms,\n                                                     mm,\n                                                     similarity)\n    pf3_scores, pf3_explain = _edismax_field_centric(frame,\n                                                     parse_field_boosts(pf3),\n                                                     num_search_terms,\n                                                     search_terms,\n                                                     mm,\n                                                     similarity)\n    scores = qf_scores + pf_scores + pf2_scores + pf3_scores\n    explain = f\"{qf_explain} {pf_explain} {pf2_explain} {pf3_explain}\"\n\n    return scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = \"100%\"\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n    query_fields = parse_field_boosts(qf)\n    if pf is not None:\n        query_fields.update(parse_field_boosts(pf))\n    if pf2 is not None:\n        query_fields.update(parse_field_boosts(pf2))\n    if pf3 is not None:\n        query_fields.update(parse_field_boosts(pf3))\n\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    if q_op == \"OR\":\n        qf_scores[qf_scores == 0] = -1\n        qf_scores = np.maximum(qf_scores, 0)\n    elif q_op == \"AND\":\n        qf_scores[qf_scores == 0] = 1\n        qf_scores = np.minimum(qf_scores, 0)\n    else:\n        raise ValueError(\"Invalid q_op. Expecting 'OR' or 'AND'\")\n\n    return qf_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = \"100%\"\n\n    if q_op == \"AND\":\n        raise ValueError(\"AND operator not supported yet\")\n\n    query_fields = parse_field_boosts(qf)\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, query_fields.keys())\n\n    if term_centric:\n        qf_scores, qf_explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, qf_explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    explain = f\"{qf_explain} #combine(qf:{mm})\"\n    if pf:\n        explain += f\" #combine(pf:{mm})\"\n    if pf2:\n        explain += f\" #combine(pf2:{mm})\"\n    if pf3:\n        explain += f\" #combine(pf3:{mm})\"\n\n    return qf_scores, explain"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = \"100%\"\n\n    if q_op == \"OR\":\n        q_op_str = \"OR\"\n    elif q_op == \"AND\":\n        q_op_str = \"AND\"\n    else:\n        raise ValueError(\"q_op must be OR or AND\")\n\n    if pf is None:\n        pf = []\n    if pf2 is None:\n        pf2 = []\n    if pf3 is None:\n        pf3 = []\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    query_fields = parse_field_boosts(qf)\n    phrase_fields = parse_field_boosts(pf)\n    bigram_fields = parse_field_boosts(pf2)\n    trigram_fields = parse_field_boosts(pf3)\n\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    for field, boost in phrase_fields.items():\n        post_arr = get_field(frame, field)\n        phrase_scores = post_arr.score_phrase(search_terms[field], similarity=similarity) * (1 if boost is None else boost)\n        qf_scores = np.maximum(qf_scores, phrase_scores)\n\n    for field, boost in bigram_fields.items():\n        post_arr = get_field(frame, field)\n        bigram_scores = post_arr.score_phrase(search_terms[field], ngrams=2, similarity=similarity) * (1 if boost is None else boost)\n        qf_scores = np.maximum(qf_scores, bigram_scores)"}
{"namespace": "searcharray.solr.edismax", "completion": "    # TODO: support other q_op\n    if q_op != \"OR\":\n        raise ValueError(\"q_op must be OR\")\n\n    if mm is None:\n        mm = \"100%\"\n\n    if not qf:\n        raise ValueError(\"qf must be specified\")\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, query=q, query_fields=qf)\n\n    qf_scores, qf_explain = _edismax_term_centric(frame,\n                                                  query_fields=parse_field_boosts(qf),\n                                                  num_search_terms=num_search_terms,\n                                                  search_terms=search_terms,\n                                                  mm=mm,\n                                                  similarity=similarity)\n\n    pf_scores, pf_explain = _edismax_field_centric(frame,\n                                                   query_fields=parse_field_boosts(pf),\n                                                   num_search_terms=num_search_terms,\n                                                   search_terms=search_terms,\n                                                   mm=mm,\n                                                   similarity=similarity)\n\n    pf2_scores, pf2_explain = _edismax_field_centric(frame,\n                                                     query_fields=parse_field_boosts(pf2),\n                                                     num_search_terms=num_search_terms,\n                                                     search_terms=search_terms,\n                                                     mm=mm,\n                                                     similarity=similarity)\n\n    pf3_scores, pf3_explain = _edismax_field_centric(frame,\n                                                     query_fields=parse_field_boosts(pf3),\n                                                     num_search_terms=num_search_terms,\n                                                     search_terms=search_terms,\n                                                     mm=mm,\n                                                     similarity=similarity)\n\n    qf_scores = qf_scores * 1.0\n    pf_scores = pf_scores * 0.8\n    pf2_scores = pf2"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)\n\n        return message"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)\n\n        return message"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)\n        else:\n            raise Exception('Unknown message type')"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            self._get_connection(message).c2s(message)\n        elif isinstance(message, RecvMessage):\n            self._get_connection(message).s2c(message)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)\n        else:\n            raise Exception('Unknown message type: %s' % type(message))"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)\n        else:\n            raise NotImplementedError('Message type is not supported: %s' % type(message))"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)\n        else:\n            raise ValueError('Unsupported message type')\n\n    "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)\n\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        for connection in self.connections.values():\n            connection.stop()\n\n        # Stop server\n        self.server.stop()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        for connection in self.connections.values():\n            connection.stop()\n\n        # Stop the server\n        self.server.stop()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            for connection in self.connections.values():\n                connection.stop()\n            self.connections = {}\n\n        if self.server is not None:\n            self.server.stop()\n            self.server = None"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            for connection in self.connections.values():\n                connection.stop()\n            self.connections = {}\n\n        if self.server is not None:\n            self.server.stop()\n            self.server = None"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection in self.connections.values():\n            connection.stop()\n        self.server.stop()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            for connection in self.connections.values():\n                connection.stop()\n            self.connections.clear()\n\n        if self.server is not None:\n            self.server.stop()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            for connection in self.connections.values():\n                connection.stop()\n            self.connections.clear()\n\n        if self.server is not None:\n            self.server.stop()\n            self.server = None"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            for connection in self.connections.values():\n                connection.stop()\n            self.connections.clear()\n\n        if self.server is not None:\n            self.server.stop()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        for connection in self.connections.values():\n            connection.stop()\n\n        # Stop the server\n        self.server.stop()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            for connection in self.connections.values():\n                connection.stop()\n\n        if self.server is not None:\n            self.server.stop()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        for connection in self.connections.values():\n            connection.stop()\n\n        # Stop server\n        self.server.stop()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        for connection in self.connections.values():\n            connection.stop()\n\n        # Stop the server\n        self.server.stop()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        for connection in self.connections.values():\n            connection.stop()\n        self.connections = {}\n\n        # Stop server\n        self.server.stop()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        self.logger.info('Stopping all connections...')\n        for connection in self.connections.values():\n            connection.stop()\n        self.connections.clear()\n\n        self.logger.info('Stopping server...')\n        self.server.stop()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            for connection_id, connection in self.connections.items():\n                self.logger.info('Connection %s is being closed due to interceptor shutdown.', connection.id)\n                connection.stop()\n\n        if self.server is not None:\n            self.server.stop()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Stop all connections\n        for connection_id, connection in self.connections.items():\n            self.logger.info('Connection %s is being closed due to interceptor destruction.', connection.id)\n            connection.stop()\n        \n        # Stop server\n        self.server.stop()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            for connection in self.connections.values():\n                connection.stop()\n\n            self.connections = {}\n\n        if self.server is not None:\n            self.server.stop()\n            self.server = None"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            for connection in self.connections.values():\n                connection.stop()\n            self.connections = {}\n\n        if self.server is not None:\n            self.server.stop()\n            self.server = None\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        for connection_id in list(self.connections.keys()):\n            self._handle_connection_close_message(CloseMessage(metadata={\n                MetadataType.CONNECTION_ID: connection_id,\n            }))\n        self.server.stop()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            # Close all connections\n            for connection_id, connection in self.connections.items():\n                self.logger.info('Connection %s is being closed due to ProxifierMessageInterceptor is being destroyed.', connection_id)\n                connection.stop()\n\n            # Stop server\n            self.server.stop()"}
