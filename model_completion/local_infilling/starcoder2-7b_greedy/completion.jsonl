{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "coord.inv_contract", "completion": "t_to_s(z)\n  return z\n\n\ndef contract(x):\n  \"\"\"Contract a vector towards the origin (Eq 10 arxiv.org/abs/2111.12077).\"\"\"\n  x_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  # Clamp to 1 to avoid inf gradients\n  x_mag_sq = jnp.clip(x_mag_sq, 1e-3, 1.0)\n  z = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  return z\n\n\ndef contract3(x):\n  \"\"\"Contract a vector towards the origin (Eq 10 arxiv.org/abs/2111.12077).\"\"\"\n  x_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  # Clamp to 1 to avoid inf gradients\n  x_mag_sq = jnp.clip(x_mag_sq, 1e-3, 1.0)\n  z = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  return z\n\n\ndef contract3_isoscale(x):\n  \"\"\"Contract a vector towards the origin (Eq 10 arxiv.org/abs/2111.12077).\"\"\"\n  x_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  # Clamp to 1 to avoid inf gradients\n  x_mag_sq = jnp.clip(x_mag_sq, 1e-3, 1.0)\n  z = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  return z\n\n\ndef contract3_isoscale_fast(x):\n  \"\"\"Contract a vector towards the origin (Eq 10 arxiv.org/abs/2111.12077).\"\"\"\n  x_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  # Clamp to "}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            with SQLiteMemoization(filename) as memo:\n                return memo.fetch_or_compute(func, func_name, *args, **kwargs)\n\n        return wrapper\n\n    return decorator\n\n"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max. Received {values}\")\n\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max. Received {values}\")\n\n    return values\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  sq_dist = np.sum(mat0**2, 1, keepdims=True) + np.sum(mat1**2, 1, keepdims=True)\n  sq_dist -= 2 * np.matmul(mat0, mat1.T)\n  sq_dist[sq_dist < 0] = 0\n  return sq_dist"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"file://\") or path.startswith(\"http://\") or path.startswith(\"https://\"):\n        return True\n    return False\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"The dictionary '{name}' must have the same keys as the assets \"\n                f\"names, got {list(items.keys())}.\"\n            )\n        if len(items)!= len(assets_names):\n            raise ValueError(\n                f\"The dictionary '{name}' must have the same number of keys as the \"\n                f\"assets names, got {len(items)}!= {len(assets_names)}.\"\n            )\n        items = np.array([items.get(a, fill_value) for a in assets_names])\n    elif isinstance(items, np.ndarray):\n        if items.ndim == 1:\n            if items.shape[0]!= n_assets:\n                raise ValueError(\n                    f\"The array '{name}' must have the same number of rows as the \"\n                    f\"number of assets, got {items.shape[0]}!= {n_assets}.\"\n                )\n        elif items.ndim == 2:\n            if items.shape[1]!= n_assets:\n                raise ValueError(\n                    f\"The array '{name}' must have the same number of columns as the \"\n                    f\"number of assets, got {items.shape[1]}!= {n_assets}.\"\n                )\n            if dim == 1:\n                raise ValueError(\n                    f\"The array '{name}' must have a single dimension, got {items.ndim}.\"\n                )\n        else:\n            raise ValueError(\n                f\"The array '{name}' must have a single dimension, got {items.ndim}.\"\n            )\n    else:\n        raise TypeError(\n            f\"The array '{name}' must be a dictionary, a numpy array, or any array-like \"\n            f\"structure, got {type(items)}.\"\n        )\n    return items\n\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data[\"purpose_embedding\"]\n        if isinstance(purpose_embedding, list):\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        return MicroAgent(\n            dynamic_prompt=data[\"dynamic_prompt\"],\n            purpose=data[\"purpose\"],\n            purpose_embedding=purpose_embedding,\n            depth=data[\"depth\"],\n            max_depth=data[\"max_depth\"],\n            usage_count=data[\"usage_count\"],\n            id=data[\"id\"],\n            parent_id=data[\"parent_id\"],\n            working_agent=data[\"working_agent\"],\n            is_prime=data[\"is_prime\"],\n            evolve_count=data[\"evolve_count\"],\n            number_of_code_executions=data[\"number_of_code_executions\"],\n            last_input=data[\"last_input\"],\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  srgb0 = 0.04045 * srgb\n  srgb1 = ((srgb + 0.055) / 1.055) ** 2.4\n  return xnp.where(srgb <= 0.04045, srgb0, srgb1)\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  \"\"\"\n  if spline_degree < 1:\n    raise ValueError('spline_degree must be at least 1.')\n  if spline_degree > len(x):\n    raise ValueError('spline_degree must be at most the number of points in x.')\n\n  # Interpolate the spline.\n  k = min(spline_degree, len(x) - 1)\n  t = scipy.interpolate.splrep(x, y, k=k, s=smoothness)\n  new_points = scipy.interpolate.splev(t_output, t)\n\n  return new_points\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    else:\n        if word[0].isupper() and word[1].islower():\n            return word[0].upper() + word[1:].lower()\n        elif word[0].islower() and word[1].isupper():\n            return word[0].lower() + word[1:].upper()\n        else:\n            return word.lower()\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must contain only boolean values. Received {v.dtype}\"\n        )\n\n    return v\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.sum(x**2, axis=-1)\n  x_mag = jnp.sqrt(x_mag_sq)\n  x_mag_sq_clamped = jnp.clip(x_mag_sq, 0, 1)\n  x_mag_clamped = jnp.sqrt(x_mag_sq_clamped)\n  x_mag_clamped_inv = 1 / x_mag_clamped\n  x_mag_clamped_inv_sq = x_mag_clamped_inv ** 2\n  x_mag_clamped_inv_sq_sq = x_mag_clamped_inv_sq ** 2\n  x_mag_clamped_inv_sq_sq_sq = x_mag_clamped_inv_sq_sq ** 2\n  x_mag_clamped_inv_sq_sq_sq_sq = x_mag_clamped_inv_sq_sq_sq ** 2\n  x_mag_clamped_inv_sq_sq_sq_sq_sq = x_mag_clamped_inv_sq_sq_sq_sq ** 4\n  x_mag_clamped_inv_sq_sq_sq_sq_sq_sq = x_mag_clamped_inv_sq_sq_sq_sq_sq ** 2\n  x_mag_clamped_inv_sq_sq_sq_sq_sq_sq_sq = x_mag_clamped_inv_sq_sq_sq_sq_sq_sq ** 2\n  x_mag_clamped_inv_sq_sq_sq_sq_sq_sq_sq_sq = x_mag_clamped_inv_sq_sq_sq_sq_sq_sq_sq ** 2\n  x_mag_clamped_inv_sq_sq_sq_sq_sq_sq_sq_sq_sq = x_mag_clamped_inv_sq_sq_sq_sq_sq_sq_sq_sq ** 2\n  x_mag_clamped_inv_sq_sq_sq_sq_sq_sq_sq_sq_sq_"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    summary_df = pd.read_csv(summary_path)\n    if dict_columns is None:\n        dict_columns = ['module_params']\n    for column_name in dict_columns:\n        summary_df[column_name] = summary_df[column_name].apply(lambda x: ast.literal_eval(x))\n    return summary_df\n\n"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # Compute the determinant of the covariance matrix.\n    det = jnp.linalg.det(mat)\n\n    # Check for invalid determinants.\n    if jnp.any(det <= 0):\n      raise ValueError('Invalid determinant.')\n\n    # Compute the square root of the determinant.\n    sqrt_det = jnp.sqrt(det)\n\n    # Compute the inverse of the square root of the determinant.\n    inv_sqrt_det = 1 / sqrt_det\n\n    # Compute the square root of the covariance matrix.\n    sqrt_mat = jnp.sqrt(mat)\n\n    # Compute the inverse of the square root of the covariance matrix.\n    inv_sqrt_mat = 1 / sqrt_mat\n\n    # Compute the isotropic covariance matrix.\n    isotropic_cov = inv_sqrt_mat @ sqrt_mat\n\n    # Return the isotropic covariance matrix.\n    return isotropic_cov\n\n  elif mode == 'accurate':\n    # Compute the logarithm of the determinant of the covariance matrix.\n    log_det = jnp.linalg.slogdet(mat)[1]\n\n    # Check for invalid logarithms.\n    if jnp.any(log_det <= 0):\n      raise ValueError('Invalid logarithm.')\n\n    # Compute the square root of the logarithm of the determinant.\n    sqrt_log_det = jnp.sqrt(log_det)\n\n    # Compute the inverse of the square root of the logarithm of the determinant.\n    inv_sqrt_log_det = 1 / sqrt_log_det\n\n    # Compute the square root of the covariance matrix.\n    sqrt_mat = jnp.sqrt(mat)\n\n    # Compute the inverse of the square root of the covariance matrix.\n    inv_sqrt_mat = 1 / sqrt_mat\n\n    # Compute the isotropic covariance matrix.\n    isotropic_cov = inv_sqrt_mat @ sqrt_mat\n\n    # Return the isotropic covariance matrix.\n    return isotropic_cov\n\n  else:\n    raise ValueError(f'Invalid mode: {mode}.')\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"XAgent Command Line Interface\")\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"+\", help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", help=\"Operational mode, which can be 'auto' or'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts, specifying how many times a task can be retried upon failure.\")\n    parser.add_argument(\"--config-file\", type=str, default=os.environ.get(\"CONFIG_FILE\", \"assets/config.yml\"), help=\"Path to the configuration file, specifying where to find the configuration settings.\")\n\n    return parser.parse_args()"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape!= (_, 2):\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        current_index = indexes[worker_idx]\n        chunk_index = 0\n        for interval in intervals:\n            if current_index >= interval[0] and current_index < interval[1]:\n                break\n            chunk_index += 1\n        chunks_index[worker_idx] = chunk_index\n\n    return chunks_index, indexes"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure not in ['grid', 'hash']:\n    raise ValueError(\n        f'Invalid datastructure {datastructure}. '\n        + 'Only \"grid\" and \"hash\" are supported.'\n    )\n\n  if datastructure == 'grid':\n    # Resample the values to the specified coordinates.\n    values = resample.resample(values, coordinates)\n    # Interpolate the resampled values to get the final values.\n    values = trilerp(values, coordinates, datastructure)\n  elif datastructure == 'hash':\n    # Resample the values to the specified coordinates.\n    values = hash_resample.resample(values, coordinates)\n    # Interpolate the resampled values to get the final values.\n    values = trilerp(values, coordinates, datastructure)\n\n  return values"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if not isinstance(v, int):\n    raise ValueError(f'v {v} must an integer')\n  if v < 1:\n    raise ValueError(f'v {v} must be greater than or equal to 1')\n\n  # Generate the weights for the vertices of the triangle.\n  weights = np.array([\n      [1, 1, 1],\n      [1, 1, 0],\n      [1, 0, 1],\n      [1, 0, 0],\n      [0, 1, 1],\n      [0, 1, 0],\n      [0, 0, 1],\n      [0, 0, 0],\n  ])\n  for _ in range(v - 1):\n    weights = np.kron(weights, weights)\n\n  # Normalize the weights to get the barycentric coordinates.\n  weights = weights / np.sum(weights, 1, keepdims=True)\n  return weights"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip the query points to the range of the spline.\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the spline values corresponding to each query point.\n  idx0, idx1 = sorted_lookup(tq, t)\n  t0 = jnp.take_along_axis(t, idx0, axis=-1)\n  t1 = jnp.take_along_axis(t, idx1, axis=-1)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Interpolate the spline values.\n  td = tq - t0\n  v = (v1 - v0) * td / (t1 - t0) + v0\n  return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if not np.all(v > 0):\n        raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "s,\n    xnp = np,\n):\n  \"\"\"Converts rays from world space to NDC space.\n\n  Args:\n    origins: float array, [..., 3], ray origins in world space.\n    directions: float array, [..., 3], ray directions in world space.\n    pixtocam: float array, [3, 3], inverse intrinsic matrix.\n    xnp: either numpy or jax.numpy.\n\n  Returns:\n    origins: float array, [..., 3], ray origins in NDC space.\n    directions: float array, [..., 3], ray directions in NDC space.\n  \"\"\"\n  # Convert ray origins and directions to projective NDC space.\n  ndc_fn = functools.partial(convert_to_ndc, pixtocam=pixtocam, xnp=xnp)\n  origins_ndc, directions_ndc = ndc_fn(origins, directions)\n\n  # Normalize the rays.\n  directions_ndc = directions_ndc / xnp.linalg.norm(directions_ndc, axis=-1, keepdims=True)\n\n  return origins_ndc, directions_ndc\n\n\ndef convert_to_ndc(\n    origins,\n    directions,\n    pixtocam,\n    xnp = np,\n):\n  \"\"\"Converts rays from world space to NDC space.\n\n  Args:\n    origins: float array, [..., 3], ray origins in world space.\n    directions: float array, [..., 3], ray directions in world space.\n    pixtocam: float array, [3, 3], inverse intrinsic matrix.\n    xnp: either numpy or jax.numpy.\n\n  Returns:\n    origins: float array, [..., 3], ray origins in NDC space.\n    directions: float array, [..., 3], ray directions in NDC space.\n  \"\"\"\n  # Convert ray origins and directions to projective NDC space.\n  origins_ndc = xnp.matmul(origins, pixtocam)\n  directions_ndc = xnp.matmul(directions, pixtocam)\n\n  # Normalize the rays.\n  directions_ndc = directions_ndc / xnp.linalg.norm(directions_ndc, axis=-1, keepdims="}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors.\n  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n\n  # Compute the dot product of the normalized direction vectors.\n  dot_product = jnp.dot(dir1, dir2)\n\n  # Check if the dot product is close to 1.0 (considering a small epsilon).\n  is_parallel = jnp.isclose(dot_product, 1.0, atol=1e-6)\n\n  return is_parallel\n\n"}
{"namespace": "common.bleu4_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f, weights=[0.25, 0.25, 0.25, 0.25])\n    score = results['bleu']\n    if with_penalty:\n        score = score * (10 ** 4) / (len(continuation.split()) ** 0.25)\n    return score"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x > eps, jnp.sqrt(x), jnp.sqrt(value_at_zero))\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Compute the PDF by dividing the weights by the difference between consecutive elements in a given vector t.\n  p = w / jnp.diff(t)\n  return p\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n\n    for dirpath, _, filenames in os.walk(path):\n        for filename in filenames:\n            try:\n                total_size += os.path.getsize(os.path.join(dirpath, filename))\n            except FileNotFoundError:\n                pass\n\n    return total_size"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    if isinstance(val, torch.Tensor):\n        val = val.view(-1)\n    elif isinstance(val, np.ndarray):\n        val = val.ravel()\n\n    val = val.astype(np.float32)\n    val = val - offset * period\n    val = val % period\n    val = val + offset * period\n\n    if isinstance(val, torch.Tensor):\n        val = val.view(val.shape[0], -1)\n    elif isinstance(val, np.ndarray):\n        val = val.reshape(val.shape[0], -1)\n\n    return val\n\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        purpose_embedding = agent.purpose_embedding.tolist() if agent.purpose_embedding is not None else None\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Create a dictionary to store the items in each bin\n    bins = defaultdict(list)\n\n    # Create a dictionary to store the total weight of items in each bin\n    bin_weights = defaultdict(int)\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Iterate over the sorted items and place them into the bins\n    for item, weight in sorted_items:\n        # Find the bin with the lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Place the item in the bin with the lowest total weight\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name,) + args + tuple(sorted(kwargs.items()))\n        return hashlib.sha256(json.dumps(data).encode(\"utf-8\")).hexdigest()"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the distances between consecutive points in the polygon\n    distances = np.linalg.norm(np.diff(polygon, axis=0), axis=1)\n\n    # Initialize the total length to zero\n    total_length = 0\n\n    # Iterate over the distances and add them to the total length if they are below the maximum distance\n    for distance in distances:\n        if distance <= max_point_distance:\n            total_length += distance\n\n    return total_length"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return []\n\n    largest_area = max(area(polygon) for polygon in polygons)\n\n    return [\n        polygon\n        for polygon in polygons\n        if area(polygon) > rel_tr * largest_area or area(polygon) > abs_tr\n    ]"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker should process\n    samples_per_worker = num_samples_yielded // num_workers\n    # Calculate the number of samples that will be left over after distributing the samples evenly\n    remaining_samples = num_samples_yielded % num_workers\n    # Calculate the number of samples each worker should process, taking into account the remaining samples\n    samples_per_worker_with_remaining = samples_per_worker + (remaining_samples // num_workers)\n\n    # Create a dictionary to store the number of samples each worker has processed\n    samples_per_worker_dict = {}\n    for worker_idx in range(num_workers):\n        samples_per_worker_dict[worker_idx] = samples_per_worker_with_remaining if worker_idx < remaining_samples else samples_per_worker\n\n    # Distribute any remaining samples evenly among the workers\n    for worker_idx in range(remaining_samples):\n        samples_per_worker_dict[worker_idx] += 1\n\n    # Calculate the number of samples each worker should process, taking into account the batch size\n    samples_per_worker_with_batch_size = {}\n    for worker_idx, samples in samples_per_worker_dict.items():\n        samples_per_worker_with_batch_size[worker_idx] = samples // batch_size\n\n    # Distribute any remaining samples evenly among the workers\n    for worker_idx in range(remaining_samples):\n        samples_per_worker_with_batch_size[worker_idx] += 1\n\n    # Calculate the number of samples each worker should process, taking into account the batch size and the remaining samples\n    samples_per_worker_with_batch_size_and_remaining = {}\n    for worker_idx, samples in samples_per_worker_with_batch_size.items():\n        samples_per_worker_with_batch_size_and_remaining[worker_idx] = samples + (remaining_samples // num_workers)\n\n    # Distribute any remaining samples evenly among the workers\n    for worker_idx in range(remaining_samples):\n        samples_per_worker_with_batch_size_and_remaining[worker_idx] += 1\n\n    # Calculate the number of samples each worker should process, taking into account the batch"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n    assert len(results) == len(metadatas), \"results and module_filename must have the same length.\"\n    assert all([isinstance(result, list) for result in results]), \\\n        \"results must be list.\"\n    assert all([isinstance(value, list) for value in value]), \\\n        \"value must be list.\"\n    assert len(results) == len(value), \\\n        \"results and value must have the same length.\"\n    assert all([isinstance(metadata, list) for metadata in metadatas]), \\\n        \"metadatas must be list.\"\n    assert len(results) == len(metadatas), \\\n        \"results and metadatas must have the same length.\"\n    filtered_results = []\n    filtered_metadatas = []\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n    return filtered_results, filtered_metadatas"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1]!= 2:\n        raise ValueError(\n            f\"The input array must have shape (_, 2), indicating it represents a list of points. Got {array.shape}\"\n        )\n\n    # Shoelace formula\n    return 0.5 * np.abs(np.dot(array[:, 0], np.roll(array[:, 1], 1)) - np.dot(array[:, 1], np.roll(array[:, 0], 1)))\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Find the indices where elements of v should be inserted into a.\n    # This is done by finding the indices where the elements of v should be\n    # inserted into a.\n    #\n    # The following code is adapted from\n    # https://github.com/google/jax/blob/master/jax/_src/lax/lax.py#L1012\n    #\n    # The basic idea is to use binary search to find the indices where the\n    # elements of v should be inserted into a.\n    #\n    # The binary search is done in two steps. First, we find the indices where\n    # the elements of v should be inserted into a[0:mid]. Then, we find the\n    # indices where the elements of v should be inserted into a[mid:].\n    #\n    # The binary search is done in two steps. First, we find the indices where\n    # the elements of v should be inserted into a[0:mid]. Then, we find the\n    # indices where the elements of v should be inserted into a[mid:].\n    #\n    # The binary search is done in two steps. First, we find the indices where\n    # the elements of v should be inserted into a[0:mid]. Then, we find the\n    # indices where the elements of v should be inserted into a[mid:].\n    #\n    # The binary search is done in two steps. First, we find the indices where\n    # the elements of v should be inserted into a[0:mid]. Then, we find the\n    # indices where the elements of v should be inserted into a[mid:].\n    #\n    # The binary search is done in two steps. First, we find the indices where\n    # the elements of v should be inserted into a[0:mid]. Then, we find the\n    # indices where the elements of v should be inserted into a[mid:].\n    #\n    # The binary search is done in two steps. First, we find the indices where\n    # the elements of v should be inserted into a[0:mid]. Then, we find the\n    # indices where the elements of v should be inserted into a[mid:].\n    #\n    # The binary search is done in two steps. First, we find the indices where\n    # the elements of v should be inserted into a[0:mid]. Then, we find the\n    #"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "ntrinsic matrix.\n  \"\"\"\n  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ])\n\n\ndef convert_to_ndc(\n    points,\n    pixtocam,\n    xnp = np,\n):\n  \"\"\"Converts points from pixel space to NDC space.\n\n  Args:\n    points: float array, [..., 3], 3D points in pixel space.\n    pixtocam: float array, [3, 3], inverse intrinsics.\n    xnp: either numpy or jax.numpy.\n\n  Returns:\n    points: float array, [..., 3], 3D points in NDC space.\n  \"\"\"\n  points_ndc = xnp.matmul(points, pixtocam)\n  points_ndc = points_ndc / points_ndc[..., -1:]\n  return points_ndc\n\n\ndef convert_from_ndc(\n    points_ndc,\n    pixtocam,\n    xnp = np,\n):\n  \"\"\"Converts points from NDC space to pixel space.\n\n  Args:\n    points_ndc: float array, [..., 3], 3D points in NDC space.\n    pixtocam: float array, [3, 3], inverse intrinsics.\n    xnp: either numpy or jax.numpy.\n\n  Returns:\n    points: float array, [..., 3], 3D points in pixel space.\n  \"\"\"\n  points = xnp.matmul(points_ndc, xnp.linalg.inv(pixtocam))\n  return points\n\n\ndef get_pixtocam(\n    focal,\n    width,\n    height,\n    xnp = np,\n):\n  \"\"\"Generates the inverse intrinsic matrix for a pinhole camera.\n\n  Args:\n    focal: Numeric. The focal length of the camera along the x-axis.\n    width: Numeric. The width of the image.\n    height: Numeric. The height of the image.\n    xnp: Module, the numpy or jax.numpy module used for matrix operations, defaulting to numpy.\n\n  Returns:\n    The inverse intrinsic matrix.\n  \"\"\"\n  return xnp.array([\n      [focal, 0, width * 0.5],\n      [0, focal, height *"}
{"namespace": "coord.contract", "completion": "t of the points after being scaled.\n  \"\"\"\n  # Compute the magnitude of the points.\n  mag = jnp.sqrt(jnp.sum(x**2, axis=-1, keepdims=True))\n  # Scale the points to be within the unit sphere.\n  x = x / mag\n  # Scale the points to be within the unit sphere.\n  x = x / jnp.sqrt(jnp.sum(x**2, axis=-1, keepdims=True))\n  return x\n\n\ndef inv_contract(x):\n  \"\"\"\n  Inverts the contract() function.\n\n  This function is used to scale the points back to their original magnitudes.\n\n  Input-Output Arguments\n  :param x: Array. The input array of points to be scaled back to their original magnitudes. It is used to calculate the magnitude of the points and then scale them back to their original magnitudes.\n  :return: Array. The array of points after being scaled back to their original magnitudes.\n  \"\"\"\n  # Compute the magnitude of the points.\n  mag = jnp.sqrt(jnp.sum(x**2, axis=-1, keepdims=True))\n  # Scale the points back to their original magnitudes.\n  x = x * mag\n  return x\n\n\ndef contract3(x):\n  \"\"\"\n  Contracts points towards the origin based on a specific mathematical formula (Equation 10 from the paper available at arxiv.org/abs/2112.11207).\n\n  Input-Output Arguments\n  :param x: Array. The input array of points to be contracted towards the origin. It is used to calculate the magnitude of the points and then scale them towards the origin based on a specific mathematical formula.\n  :return: Array. The array of points after being contracted towards the origin.\n  \"\"\"\n  # Compute the magnitude of the points.\n  mag = jnp.sqrt(jnp.sum(x**2, axis=-1, keepdims=True))\n  # Scale the points towards the origin based on a specific mathematical formula.\n  x = x / (2 * jnp.sqrt(mag - 1))\n  return x\n\n\ndef contract3_isoscale(x):\n  \"\"\"\n  Contracts points towards the origin based on a specific mathematical formula (Equation 10 from the paper available at arxiv."}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes < 1000:\n        return f\"{num_bytes} B\"\n    for suffix in _FORMAT_TO_RATIO:\n        if num_bytes < _FORMAT_TO_RATIO[suffix]:\n            return f\"{num_bytes / _FORMAT_TO_RATIO[suffix]} {suffix}\"\n        num_bytes /= _FORMAT_TO_RATIO[suffix]\n    return f\"{num_bytes} PB\""}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check if array has nb_dimensions dimensions.\"\"\"\n        if len(v.shape)!= nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. got {len(v.shape)} dimensions.\"\n            )\n\n        return v\n\n    return __validator\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Calculate the radius (r) of the point using the Pythagorean theorem.\n  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n\n  # Calculate the inclination (theta) of the point using the inverse sine function.\n  theta = jnp.arcsin(jnp.clip(cartesian_vector[..., 2] / r, -1 + eps, 1 - eps))\n\n  # Calculate the azimuth (phi) of the point using the inverse cosine function.\n  phi = jnp.arccos(jnp.clip(cartesian_vector[..., 0] / (r * jnp.cos(theta)), -1 + eps, 1 - eps))\n\n  # Convert the azimuth from radians to degrees.\n  phi = jnp.degrees(phi)\n\n  # Convert the inclination from radians to degrees.\n  theta = jnp.degrees(theta)\n\n  return r, theta, phi\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['rougeL']['f']\n    return score\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # First, try to locate the object using the standard method.\n    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    # If the object cannot be located using the standard method, try to locate the object using a fallback method.\n    try:\n        return pydoc.locate(name.replace(\".\", \"_\"))\n    except ImportError:\n        pass\n\n    # If the object cannot be located by either method, raise an exception.\n    raise ImportError(f\"Could not locate object with name {name}\")"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the module to an in-memory buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Load the module from the buffer\n    buffer.seek(0)\n    reloaded_module = torch.jit.load(buffer)\n\n    return reloaded_module"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    if len(ids)!= len(scores):\n        raise ValueError('The length of the ids and scores tuples must match.')\n\n    if len(ids)!= len(weights):\n        raise ValueError('The length of the ids, scores, and weights tuples must match.')\n\n    if sum(weights)!= 1:\n        raise ValueError('The sum of the weights must equal 1.')\n\n    fused_ids = []\n    fused_scores = []\n\n    for _id, score, weight in zip(ids, scores, weights):\n        fused_ids.append(_id[:top_k])\n        fused_scores.append([s * weight for s in score[:top_k]])\n\n    fused_ids = [item for sublist in fused_ids for item in sublist]\n    fused_scores = [item for sublist in fused_scores for item in sublist]\n\n    return fused_ids[:top_k], fused_scores[:top_k]"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    if percent:\n        x *= 100\n        s = \"%\"\n    else:\n        s = \"\"\n    return f\"{x:.{np.abs(np.floor(np.log10(np.abs(x))))}f}{s}\"\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "   while True:\n        try:\n            disk_usage = shutil.disk_usage(path)\n            if disk_usage.free > threshold_in_gb * 1024 ** 3:\n                break\n            time.sleep(sleep_time)\n        except Exception as e:\n            print(e)\n            time.sleep(sleep_time)\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = \"\".join(line_text.split())\n    # Segment the modified text into smaller parts or tokens\n    tokens = line_text.split()\n    return tokens\n\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(f\"The number of zeros must not exceed the number of weights ({n})\")\n\n    weights = np.random.rand(n)\n    weights /= np.sum(weights)\n\n    if zeros > 0:\n        weights[np.random.choice(n, zeros, replace=False)] = 0\n\n    return weights\n\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        return cls(module_type, module_dict)\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the bounding box of the instance\n    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n\n    # Convert the bounding box to the desired mode\n    if bbox_mode == BoxMode.XYXY_ABS:\n        bbox = BoxMode.convert(bbox, BoxMode.XYXY_ABS, BoxMode.XYWH_ABS)\n    elif bbox_mode == BoxMode.XYWH_ABS:\n        bbox = BoxMode.convert(bbox, BoxMode.XYWH_ABS, BoxMode.XYXY_ABS)\n    else:\n        raise ValueError(\"Bounding box mode {} is not supported.\".format(bbox_mode))\n\n    # Calculate the center of the bounding box\n    center_x = (bbox[0] + bbox[2]) / 2\n    center_y = (bbox[1] + bbox[3]) / 2\n\n    # Calculate the dimensions of the bounding box\n    width = bbox[2] - bbox[0]\n    height = bbox[3] - bbox[1]\n\n    # Calculate the aspect ratio of the bounding box\n    aspect_ratio = width / height\n\n    # Calculate the desired width and height of the crop\n    desired_width = crop_size[0]\n    desired_height = crop_size[1]\n\n    # Calculate the desired aspect ratio of the crop\n    desired_aspect_ratio = desired_width / desired_height\n\n    # Calculate the desired width and height of the crop based on the aspect ratio\n    if aspect_ratio > desired_aspect_ratio:\n        desired_width = desired_height * aspect_ratio\n    else:\n        desired_height = desired_width / aspect_ratio\n\n    # Calculate the desired center of the crop based on the desired width and height\n    desired_center_x = center_x - desired_width / 2\n    desired_center_y = center_y - desired_height / 2\n\n    # Calculate the top-left corner of the crop\n    top_left_x = int(desired_center_x)\n    top_left_y = int(desired_center_y)\n\n    # Calculate the dimensions of the crop\n    crop_width = int(desired_width)\n    crop_height = int(desired_height)\n\n    # Calculate the bottom-right corner of the crop\n    bottom_right_x = top_left_x + crop_width\n    bottom_right_y"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Compute the squared norm of x along the last axis.\n  norm_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n\n  # Clamp the norm to a minimum value during the forward pass.\n  norm_sq = jnp.maximum(norm_sq, grad_eps)\n\n  # Compute the normalized array.\n  return x / jnp.sqrt(norm_sq)\n\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name = agent_info.split(\":\")[0]\n        input_text = agent_info.split(\":\")[1] if len(agent_info.split(\":\")) > 1 else \"\"\n        return agent_name, input_text"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    # Create an empty Instances object\n    target = Instances(image_size)\n\n    # Process bounding boxes\n    if \"bbox\" in annos[0]:\n        boxes = [obj[\"bbox\"] for obj in annos]\n        boxes = target.gt_boxes = Boxes(boxes)\n        boxes.clip(image_size)\n\n    # Process classes\n    if \"category_id\" in annos[0]:\n        classes = [obj[\"category_id\"] for obj in annos]\n        classes = torch.tensor(classes, dtype=torch.int64)\n        target.gt_classes = classes\n\n    # Process masks\n    if \"segmentation\" in annos[0]:\n        if mask_format == \"polygon\":\n            masks = [obj[\"segmentation\"] for obj in annos]\n            masks = PolygonMasks(masks)\n        elif mask_format == \"bitmask\":\n            masks = [obj[\"segmentation\"] for obj in annos]\n            masks = [polygons_to_bitmask(p, image_size) for p in masks]\n            masks = BitMasks(masks)\n        else:\n            raise ValueError(\n                \"Unknown mask format: {}\".format(mask_format)\n            )\n        target.gt_masks = masks\n\n    # Process keypoints\n    if \"keypoints\" in annos[0]:\n        keypoints = [obj[\"keypoints\"] for obj in annos]\n        keypoints = Keypoints(keypoints)\n        target.gt_keypoints = keypoints\n\n    return target"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", None)\n        if data_home is None:\n            data_home = os.path.join(os.path.expanduser(\"~\"), \"skfolio_data\")\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    if cov.ndim!= 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    corr = cov / np.sqrt(np.outer(np.diag(cov), np.diag(cov)))\n    std = np.sqrt(np.diag(cov))\n    return corr, std"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # Create a copy of the model to be used for freezing the training mode.\n    frozen_model = deepcopy(model)\n\n    # Iterate through all submodules in the model and freeze their \"training\" attribute.\n    for module in frozen_model.modules():\n        if hasattr(module, \"training\"):\n            module.training = torch.jit.Final[bool](False)\n\n    # Use the context manager to temporarily set the model to the frozen state.\n    with torch.no_grad():\n        yield frozen_model"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if len(field1) equals len(field2) and if every element have the same shape.\"\"\"\n        shapes_field_1 = [element.shape for element in values[field1]]\n        shapes_field_2 = [element.shape for element in values[field2]]\n\n        if len(values[field1]) != len(values[field2]) or shapes_field_1 != shapes_field_2:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {shapes_field_1} and {shapes_field_2}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list):\n        if isinstance(metrics[0], str):\n            return metrics, [{}] * len(metrics)\n        elif isinstance(metrics[0], dict):\n            return [metric['name'] for metric in metrics], metrics\n        else:\n            raise ValueError('Metrics must be a list of strings or dictionaries.')\n    else:\n        raise ValueError('Metrics must be a list of strings or dictionaries.')\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    # Attempt to automatically determine the inverse of `fn` based on a predefined mapping of functions to their inverses.\n    if fn in [math.safe_sqrt, math.safe_log, math.safe_exp]:\n      fn_inv = lambda x: jnp.where(x > 0, fn(x), 0)\n    elif fn in [math.safe_sin, math.safe_cos]:\n      fn_inv = lambda x: fn(x)\n    elif fn in [math.safe_tan, math.safe_atan]:\n      fn_inv = lambda x: fn(x)\n    elif fn in [math.safe_asin, math.safe_acos]:\n      fn_inv = lambda x: fn(x)\n    elif fn in [math.safe_atan2]:\n      fn_inv = lambda x, y: fn(x, y)\n    elif fn in [math.safe_atan2_2d]:\n      fn_inv = lambda x, y: fn(x, y)\n    elif fn in [math.safe_atan2_3d]:\n      fn_inv = lambda x, y, z: fn(x, y, z)\n    elif fn in [math.safe_atan2_4d]:\n      fn_inv = lambda x, y, z, w: fn(x, y, z, w)\n    elif fn in [math.safe_atan2_5d]:\n      fn_inv = lambda x, y, z, w, v: fn(x, y, z, w, v)\n    elif fn in [math.safe_atan2_6d]:\n      fn_inv = lambda x, y, z, w, v, u: fn(x, y, z, w, v, u)\n    elif fn in [math.safe_atan2_7d]:\n      fn_inv = lambda x, y, z, w, v, u, t: fn(x, y, z, w, v, u, t)\n    elif fn in [math.safe_atan2_8d]:\n      fn_inv = lambda x, y, z, w, v, u, t, s: fn(x, y, z, w, v, u, t, s)\n    elif fn"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  check_zero_endpoints(w)\n\n  # Compute the integral using the trapezoid rule.\n  dt = jnp.diff(t)\n  return jnp.cumsum(dt * (w[Ellipsis, :-1] + w[Ellipsis, 1:]), axis=-1)"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    id_df = pd.DataFrame({f'id_{i}': id_list for i, id_list in enumerate(ids)})\n    score_df = pd.DataFrame({f'score_{i}': score_list for i, score_list in enumerate(scores)})\n    df = pd.concat([id_df, score_df], axis=1)\n\n    def cc_pure_apply(row):\n        ids_tuple = tuple(row[[f'id_{i}' for i in range(len(ids))]].values)\n        scores_tuple = tuple(row[[f'score_{i}' for i in range(len(scores))]].values)\n        return cc_pure(ids_tuple, scores_tuple, weights, top_k)\n\n    df[['cc_id', 'cc_score']] = df.apply(cc_pure_apply, axis=1)\n    return df['cc_id'].tolist(), df['cc_score'].tolist()"}
{"namespace": "coord.track_linearize", "completion": "s, covs):\n  \"\"\"\n  This function applies a given function to a set of means and covariances, using a linearization technique similar to that used in a Kalman filter. It linearizes the given function around the mean and then applies the function to the linearized function.\n\n  Input Arguments:\n  fn: Function. A function that can be applied to the means and covariances. It is used to transform the Gaussian means and covariances.\n  mean: Tensor. A tensor of Gaussian means, where the last dimension represents the means. It is used as the point around which the function is linearized.\n  cov: Tensor. A tensor of covariances, where the last two dimensions represent the dimensions of the covariances. It is used as the point around which the function is linearized.\n\n  Returns: Tuple (Tensor, Tensor).\n  fn_mean: The transformed means.\n  fn_cov: The transformed covariances.\n  \"\"\"\n  d = mean.shape[-1]\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n  fn_cov = jnp.zeros_like(cov)\n  for i in range(d):\n    for j in range(d):\n      fn_cov[..., i, j] = jnp.trace(\n          jnp.matmul(lin_fn(mean[..., i:i + 1]), jnp.matmul(cov[..., i:i + 1, j:j + 1], lin_fn(mean[..., j:j + 1]).T))\n      )\n  return fn_mean, fn_cov\n\n\ndef track_linearize(fn, mean, cov):\n  \"\"\"\n  This function applies a given function to a set of means and covariances, using a linearization technique similar to that used in a Kalman filter. It linearizes the given function around the mean and then applies the function to the linearized function.\n\n  Input Arguments:\n  fn: Function. A function that can be applied to the means and covariances. It is used to transform the Gaussian means and covariances.\n  mean: Tensor. A tensor of Gaussian means, where the last dimension represents the means. It is used as the point around which the function is linearized.\n  cov: Tensor. A tensor of covariances, where the last two"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for arr in x:\n        if len(arr) > 1:\n            yield arr[: len(arr) // 2], arr[len(arr) // 2 :]\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim!= 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0]!= x.shape[1]:\n        raise ValueError(\"The matrix must be square\")"}
{"namespace": "coord.pos_enc", "completion": "  # Generate the scales for the positional encoding.\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n\n  # Reshape the input array to add a new dimension for the scaling.\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n\n  # Apply the sine function to the scaled input.\n  encoded = jnp.sin(scaled_x)\n\n  # Optionally append the original input to the encoded features.\n  if append_identity:\n    encoded = jnp.concatenate([encoded, x], axis=-1)\n\n  return encoded\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Check if field1.shape equals field2.shape.\"\"\"\n        if len(values[field1])!= len(values[field2]):\n            raise ValueError(f\"{cls.__name__}: {field1} and {field2} length mismatch.\")\n\n        for i in range(len(values[field1])):\n            if values[field1][i].shape!= values[field2][i].shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n\n        return values\n\n    return __root_validator"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "eglContext.makeCurrent(eglContext)  # resize the context\n        self.resize(w, h)\n\n    def render(self, camera: Camera):\n        \"\"\"\n        Render the mesh using the given camera configuration.\n\n        Input-Output Arguments:\n        eglContext: eglContext.renderable_context, the rendering context used for rendering.\n        camera: Camera, the camera configuration used for rendering.\n        Returns: No return values.\n        \"\"\"\n        if not self.visible: return\n\n        # Render the mesh\n        self.upload_gl_uniforms(camera)\n        glBindVertexArray(self.vao)\n        if self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawElements(gl.GL_LINES, self.n_faces_byte, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of indices\n        elif self.render_type == Mesh.RenderType.TRIANGLES:\n            gl.glDrawElements(gl.GL_TRIANGLES, self.n_faces_byte, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of indices\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawElements(gl.GL_TRIANGLE_STRIP, self.n_faces_byte, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))  # number of indices\n        else:\n            raise NotImplementedError\n\n        # House keepings\n        glBindVertexArray(0)\n\n    def render_imgui(self, mesh, viewer: 'VolumetricVideoViewer', batch: dotdict):\n        from imgui_bundle import imgui\n        from easyvolcap.utils.imgui_utils import push_button_color, pop_button_color\n\n        i = batch.i\n        imgui.push_item_width(mesh.slider_width)\n        if mesh.render_type == Mesh.RenderType.LINES:\n            push_button_color(0x55cc33ff)\n            if imgui.button(f'Lines##{i}'):\n                mesh.render_type = Mesh.RenderType.LINES\n            pop_button_color()\n        elif mesh.render_type == Mesh.RenderType.TRIANGLES:\n            push_button_color("}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # The following attributes are not present in the original BertConfig, but are required for the Nomic model architecture.\n    # These attributes are set to default values to ensure compatibility with the Nomic model.\n    n_positions = 512\n    n_inner = 3072\n    n_layer = 12\n    n_head = 12\n    type_vocab_size = 1\n    initializer_range = 0.02\n    layer_norm_epsilon = 1e-5\n    pad_vocab_size_multiple = 1\n    last_layer_subset = False\n\n    # The following attributes are inherited from the original BertConfig, but are modified to be compatible with the Nomic model architecture.\n    # These attributes are set to default values to ensure compatibility with the Nomic model.\n    vocab_size = bert_config.vocab_size\n    n_embd = bert_config.hidden_size\n    n_positions = bert_config.max_position_embeddings\n    n_layer = bert_config.num_hidden_layers\n    n_head = bert_config.num_attention_heads\n    n_inner = bert_config.intermediate_size\n    activation_function = bert_config.hidden_act\n    resid_pdrop = bert_config.hidden_dropout_prob\n    attn_pdrop = bert_config.attention_probs_dropout_prob\n    n_positions = bert_config.max_position_embeddings\n    type_vocab_size = bert_config.type_vocab_size\n    initializer_range = bert_config.initializer_range\n    layer_norm_epsilon = bert_config.layer_norm_eps\n\n    # The following attributes are not present in the original BertConfig, but are required for the Nomic model architecture.\n    # These attributes are set to default values to ensure compatibility with the Nomic model.\n    n_positions = 512\n    n_inner = 3072\n    n_layer = 12\n    n_head = 12\n    type_vocab_size = 1\n    initializer_range = 0.02\n    layer_norm_epsilon = 1e-5\n    pad_vocab_size_multiple = 1\n    last_layer_subset = False\n\n    return N"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "s.GL_TRIANGLES)  # number of vertices\n        gl.glDrawArrays(gl.GL_TRIANGLES, 0, self.n_faces * 3)  # number of vertices\n        gl.glBindVertexArray(0)\n\n    def render_imgui(self, viewer: 'VolumetricVideoViewer', batch: dotdict):\n        super().render_imgui(viewer, batch)\n\n        i = batch.i\n        from imgui_bundle import imgui\n        imgui.same_line()\n        push_button_color(0x55cc33ff if not self.shade_flat else 0x8855aaff)\n        if imgui.button(f'Smooth##{i}' if not self.shade_flat else f' Flat##{i}'):\n            self.shade_flat = not self.shade_flat\n        pop_button_color()\n\n        imgui.same_line()\n        push_button_color(0x55cc33ff if not self.render_type == Mesh.RenderType.TRIANGLES else 0x8855aaff)\n        if imgui.button(f'Triangles##{i}' if not self.render_type == Mesh.RenderType.TRIANGLES else f' Quads##{i}'):\n            self.render_type = Mesh.RenderType.TRIANGLES  # construct_faces will be called\n        pop_button_color()\n\n        imgui.same_line()\n        push_button_color(0x55cc33ff if not self.render_type == Mesh.RenderType.STRIPS else 0x8855aaff)\n        if imgui.button(f'Strips##{i}' if not self.render_type == Mesh.RenderType.STRIPS else f' Fans##{i}'):\n            self.render_type = Mesh.RenderType.STRIPS  # construct_faces will be called\n        pop_button_color()\n\n        imgui.same_line()\n        push_button_color(0x55cc33ff if not self.render_type == Mesh.RenderType.LINES else 0x8855aaff)\n        if imgui.button(f'Lines##{i}' if not self.render_type"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "    def upload_to_texture(self, ptr: np.ndarray, x: int = 0, y: int = 0, w: int = 0, h: int = 0):\n        \"\"\"\n        This function uploads a portion or the entirety of a numpy array or a PyTorch tensor to the texture.\n        It respects the OpenGL convention of lower left corners\n        \"\"\"\n        w = w or self.W\n        h = h or self.H\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)  # only render in this small region of the viewport\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        # Upload the texture\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n\n        # Restore the state\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n\n    def render(self, x: int = 0, y: int = 0, w: int = 0, h: int = 0):\n        \"\"\"\n        This function renders the texture to the screen.\n        It respects the OpenGL convention of lower left corners\n        \"\"\"\n        w = w or self.W\n        h = h or self.H\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)  # only render in this small region of the viewport\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        # Render the texture\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)  # number of vertices\n\n        # Restore the state\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0,"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    \"\"\"\n    This function calculates and returns the camera parameters for Pulsar, including camera position, rotation, intrinsic parameters such as focal length and sensor width, and the image size, based on the input rotation matrix, translation vector, camera intrinsic matrix, and image size. It ensures that all inputs are batched and validates their shapes and values before computing the camera parameters.\n\n    Input-Output Arguments:\n    :param R: torch.Tensor. A batch of rotation matrices of the camera, used to compute the camera's rotation in a different representation.\n    :param tvec: torch.Tensor. A batch of translation vectors of the camera, used to compute the camera's position.\n    :param camera_matrix: torch.Tensor. A batch of camera intrinsic matrices, used to derive focal lengths, principal points, and sensor width.\n    :param image_size: torch.Tensor. A batch of image sizes, used to adjust the principal point offsets and normalize the focal length.\n    :param znear: float, optional (default=0.1). The near clipping plane distance, used to adjust the focal length.\n    :return: torch.Tensor. A tensor containing the computed camera parameters for each instance in the batch, including camera position, rotation, intrinsic parameters, and image size.\n\n    \"\"\"\n    # Ensure that all inputs are batched and have the correct shapes\n    R = R.reshape(-1, 3, 3)\n    tvec = tvec.reshape(-1, 3)\n    camera_matrix = camera_matrix.reshape(-1, 3, 3)\n    image_size = image_size.reshape(-1, 2)\n\n    # Calculate the camera position\n    C = -R.mT @ tvec\n\n    # Calculate the camera rotation\n    R = R.clone()\n    R[..., 0, :] *= -1  # flip x row\n    R[..., 1, :] *= -1  # flip y row\n    R[..., 2, :] *= -1  # flip z row\n    R = R.mT  # applied left (left multiply to right multiply, god knows why...)\n\n    # Calculate the focal length\n    f = camera_matrix[..., 0, 0] / 2.0  # fx\n    f = f.reshape(-1, 1)\n\n    # Calculate the principal point\n    pp = camera_matrix"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n        gl.glDrawArrays(gl.GL_TRIANGLES, 0, self.max_verts)  # number of vertices\n        gl.glDisable(gl.GL_SCISSOR_TEST)\n        gl.glDisable(gl.GL_DEPTH_TEST)\n        gl.glDisable(gl.GL_BLEND)\n        gl.glUseProgram(0)\n        gl.glBindVertexArray(0)\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, 0)\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    :param batch: dotdict. A batch of data containing camera parameters (R, T, K, metadata) and metadata (H, W) in a specific format. The rotation (R) and translation (T) matrices are applied on the right (R.T @ T) to align with PyTorch3D's coordinate system and conventions. The intrinsic matrix (K) is recalculated for normalized device coordinates (NDC).\n    :return: Tuple containing the height (H), width (W), intrinsic matrix (K), adjusted rotation matrix (R), adjusted translation vector (T), and camera center (C).\n    \"\"\"\n    # Extract pytorc3d camera parameters from batch input\n    H = batch.meta.H[0].item()  #!: BATCH\n    W = batch.meta.W[0].item()  #!: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -R.mT @ T  # B, 3, 1\n    # Adjust rotation matrix to align with PyTorch3D's coordinate system and conventions.\n    R = R.permute(0, 2, 1)\n    # Adjust translation vector to align with PyTorch3D's coordinate system and conventions.\n    T = -R @ T\n    # Recalculate intrinsic matrix for normalized device coordinates (NDC).\n    K = get_pytorch3d_ndc_K(K, H, W)\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        gl.glReadBuffer(gl.GL_COLOR_ATTACHMENT0)\n        gl.glBlitFramebuffer(x, y, w, h, x, y, w, h, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, 0)\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "nner and outer measures.\n    Input-Output Arguments:\n    t: Tensor. The target time (given as a function of t).\n    t_env: Tensor. The source time (given as a function of t).\n    y1: Tensor. The values corresponding to the given t.\n    y2: Tensor. The values corresponding to the given t_env.\n    eps: Float. A small positive number to avoid division by zero.\n    Returns:\n    Tuple of Tensors. The first tensor is the inner measure, and the second tensor is the outer measure.\n    \"\"\"\n    # The inner measure is the integral of the difference between the target and the source.\n    inner = torch.sum(torch.abs(y1 - y2))\n\n    # The outer measure is the integral of the difference between the target and the source.\n    outer = torch.sum(torch.abs(y1 - y2))\n\n    return inner, outer\n\n\ndef lossfun_zip_outer(t, t_env, y1, y2, eps=1e-6):\n    \"\"\"\n    Constructs the outer measure of the zipper loss. This is the integral of the difference between the target and the source.\n    Input-Output Arguments:\n    t: Tensor. The target time (given as a function of t).\n    t_env: Tensor. The source time (given as a function of t).\n    y1: Tensor. The values corresponding to the given t.\n    y2: Tensor. The values corresponding to the given t_env.\n    eps: Float. A small positive number to avoid division by zero.\n    Returns:\n    Tensor. The outer measure.\n    \"\"\"\n    # The outer measure is the integral of the difference between the target and the source.\n    outer = torch.sum(torch.abs(y1 - y2))\n\n    return outer\n\n\ndef lossfun_zip_outer_dpt(t, t_env, y1, y2, eps=1e-6):\n    \"\"\"\n    Constructs the outer measure of the zipper loss. This is the integral of the difference between the target and the source.\n    Input-Output Arguments:\n    t: Tensor. The target time (given as a function of t).\n    t_env: Tensor. The source time (given as a function of t).\n    y1: Tensor. The values corresponding to the given t.\n    y"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "_env, t_env_w, eps=eps)\n    return loss_outer(t, t_env, t_env_w, eps=eps)\n\n\ndef lossfun_zip_outer(t: torch.Tensor, t_env: torch.Tensor, t_env_w: torch.Tensor, pulse_width: float, eps=1e-6):\n    \"\"\"\n    This function calculates a scaled half-quadratic loss based on the outer envelope positions and weights.\n    The loss is calculated using the following formula:\n    loss = 0.5 * (t - t_env)^2 / (t_env_w * t_env_w)\n    The loss is scaled by the pulse_width to account for the width of the pulse envelope.\n    The eps argument is used to prevent division by zero.\n    \"\"\"\n    t_normalize = t / torch.clamp_min(t_env_w, eps)\n\n    t_normalize, t_env_w = blur_stepfun(t_normalize, t_env_w, pulse_width)\n    t_normalize = torch.clamp(t_normalize, 0., 1.)\n    assert (t_normalize >= 0.0).all().item()\n\n    # piecewise linear pdf to piecewise quadratic cdf\n    area = 0.5 * (t_normalize[1:] + t_normalize[:-1]) * (torch.cumsum(t_env_w, dim=-1) - t_env_w)\n\n    cdf = torch.cat([torch.zeros_like(area[..., :1]), area], dim=-1)\n\n    # query piecewise quadratic cdf\n    ret = x_to_y(t_normalize, cdf, cdf)\n\n    return ret\n\n\ndef lossfun_zip_outer_v2(t: torch.Tensor, t_env: torch.Tensor, t_env_w: torch.Tensor, pulse_width: float, eps=1e-6):\n    \"\"\"\n    This function calculates a scaled half-quadratic loss based on the outer envelope positions and weights.\n    The loss is calculated using the following formula:\n    loss = 0.5 * (t - t_env)^2 / (t_env_w * t_env_w)\n    The loss is scaled by the pulse_"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "eights.\n    The loss is computed as the sum of the intra-interval and inter-interval losses.\n\n    Input-Output Arguments:\n    - t: The torch.Tensor for the time values. It is expected that the last dimension of 't' is one more than that of 'w'.\n    - w: The torch.Tensor for the weights. It is expected that the last dimension of 'w' is one less than that of 't'.\n    - return_loss: A boolean value indicating whether to return the total loss or the individual losses.\n\n    Returns:\n    - The torch.Tensor for the total loss. If'return_loss' is set to False, the function returns None.\n    \"\"\"\n    t, w = matchup_channels(t, w)\n\n    # Compute the intra- and inter-interval losses.\n    loss_intra = lossfun_outer(t, w)\n    loss_inter = lossfun_distortion(t, w)\n\n    # Compute the total loss.\n    loss = loss_intra + loss_inter\n\n    # Return the total loss if requested.\n    if return_loss:\n        return loss\n    else:\n        return None\n\n\ndef lossfun_outer(t: torch.Tensor, w: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Compute the outer loss for the given time and weight tensors.\n\n    Input-Output Arguments:\n    - t: The torch.Tensor for the time values. It is expected that the last dimension of 't' is one more than that of 'w'.\n    - w: The torch.Tensor for the weights. It is expected that the last dimension of 'w' is one less than that of 't'.\n\n    Returns:\n    - The torch.Tensor for the outer loss.\n    \"\"\"\n    t, w = matchup_channels(t, w)\n\n    # Compute the outer loss.\n    loss_outer = lossfun_outer4d(t, w)\n\n    return loss_outer\n\n\ndef lossfun_outer4d(t: torch.Tensor, w: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Compute the outer loss for the given time and weight tensors.\n\n    Input-Output Arguments:\n    - t: The torch.Tensor for the time values. It is expected that the last dimension of 't' is one more than that of 'w'.\n    - w: The torch.Tensor"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # match up the channels of the tensors\n    t, w = matchup_channels(t, w)\n\n    # integrate the weights\n    cw = integrate_weights(w)\n\n    # interpolate the integrated weights to find the weighted percentiles\n    ps = torch.tensor(ps, dtype=t.dtype, device=t.device)\n    return interpolate(ps, cw, t)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Compute the PDF and CDF for each weight vector.\n    p = weight_to_pdf(t, w)\n    cdf = integrate_weights(p)\n\n    # Generate samples from the CDF.\n    u = torch.rand(num_samples, device=t.device)\n    t_sample = invert_cdf(u, cdf, p)\n\n    # Optionally perturb the samples.\n    if perturb:\n        t_sample = perturb_samples(t_sample, t, p)\n\n    # Optionally jitter the samples.\n    if single_jitter:\n        t_sample = jitter_samples(t_sample, t, p)\n    else:\n        t_sample = jitter_samples(t_sample, t, p, single_jitter=False)\n\n    return t_sample\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Dilate the time steps.\n    t_dilate = torch.nn.functional.max_pool1d(t.unsqueeze(-1), kernel_size=dilation + 1, stride=dilation, padding=dilation // 2).squeeze(-1)\n    # Clip the dilated time steps to the specified domain.\n    t_dilate = torch.clamp(t_dilate, min=domain[0], max=domain[1])\n    # Adjust the weights accordingly.\n    p_dilate = torch.nn.functional.max_pool1d(w.unsqueeze(-1), kernel_size=dilation + 1, stride=dilation, padding=dilation // 2).squeeze(-1)\n    return t_dilate, p_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Find the indices of the query times that match a step change time.\n    idx_match = torch.where(torch.isclose(tq, t))[0]\n\n    # Interpolate the values at the query times that don't match a step change time.\n    idx_interp = torch.where(~torch.isclose(tq, t))[0]\n    y_interp = interpolate(tq[idx_interp], t, y)\n\n    # Return the interpolated values and the outside values.\n    y_out = torch.zeros_like(tq)\n    y_out[idx_match] = outside_value\n    y_out[idx_interp] = y_interp\n    return y_out\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # Check if the weights tensor is empty or has only one element\n    if w.numel() == 0 or w.numel() == 1:\n        return w\n\n    # Check if the training fraction is greater than 1 or less than 0\n    if train_frac > 1 or train_frac < 0:\n        raise ValueError(\"Training fraction must be between 0 and 1.\")\n\n    # Check if the annealing slope is greater than 0\n    if anneal_slope <= 0:\n        raise ValueError(\"Annealing slope must be greater than 0.\")\n\n    # Check if the weights tensor has the same number of elements as the time tensor\n    if w.numel()!= t.numel():\n        raise ValueError(\"The weights tensor must have the same number of elements as the time tensor.\")\n\n    # Check if the time tensor has the same number of elements as the weights tensor\n    if t.numel()!= w.numel():\n        raise ValueError(\"The time tensor must have the same number of elements as the weights tensor.\")\n\n    # Check if the time tensor is sorted in ascending order\n    if not torch.all(t[:-1] <= t[1:]):\n        raise ValueError(\"The time tensor must be sorted in ascending order.\")\n\n    # Check if the time tensor is not empty\n    if t.numel() == 0:\n        raise ValueError(\"The time tensor must not be empty.\")\n\n    # Check if the time tensor has at least two elements\n    if t.numel() < 2:\n        raise ValueError(\"The time tensor must have at least two elements.\")\n\n    # Check if the time tensor has at least two adjacent elements\n    if t.numel() < 3:\n        raise ValueError(\"The time tensor must have at least two adjacent elements.\")\n\n    # Check if the time tensor has at least two adjacent elements with zero distance\n    if torch.all(t[1:] - t[:-1] == 0):\n        raise ValueError(\"The time tensor must have at least two adjacent elements with non-zero distance.\")\n\n    # Check if the time tensor has at least two adjacent elements with zero distance\n    if torch.all(t[1:] - t[:-1] == 0):\n        raise ValueError(\"The time tensor must have at least two adjacent elements with non-zero distance.\")\n\n    # Check if the time tensor has"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "e input batch to the specified device. It supports handling nested tuples, lists, and dictionaries, as well as handling dictionary keys named \"meta\". It ensures that the data is moved to the specified device for GPU processing.\n\n    Input-Output Arguments\n    - batch (tuple, list, dict, or numpy.ndarray): The input batch that needs to be converted to a CUDA tensor.\n    - device (str, default \"cuda\"): The target device to which the data needs to be moved.\n    - ignore_list (bool, default False): A flag indicating whether to ignore certain elements, though it is not directly applicable in the function's usage.\n\n    Returns:\n    - batch (tuple, list, dict, or numpy.ndarray): The input batch converted to a CUDA tensor, ready for GPU processing.\n    \"\"\"\n    if isinstance(batch, (tuple, list)):\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    elif isinstance(batch, np.ndarray) and batch.dtype == np.object:\n        batch = batch.astype(object)\n    else:\n        pass  # do nothing here, used for typed in to_cuda for methods\n        # FIXME: Inconsistent behavior here, might lead to undebuggable bugs\n    return batch\n\n\ndef to_x(batch, x: str) -> Union[torch.Tensor, dotdict[str, torch.Tensor]]:\n    \"\"\"\n    Converts the input batch to a specific tensor, specified by the x argument. It supports handling nested tuples, lists, and dictionaries, as well as handling dictionary keys named \"meta\". It ensures that the data is moved to the specified device for GPU processing.\n\n    Input-Output Arguments\n    - batch (tuple, list, dict, or numpy.ndarray): The input batch that needs to be converted to a CUDA tensor.\n    - x (str): The name of the tensor to be converted.\n\n    Returns:\n    - batch (tuple, list, dict, or numpy.ndarray): The input batch converted to a CUDA tensor, ready for GPU processing.\n    \"\"\"\n    if isinstance(batch, (tuple"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # Adjust the dimensions of the faces tensor to match the batch dimension of the vertices tensor\n    if f.ndim < v.ndim:\n        f = f.expand(v.shape[0], *f.shape)\n\n    # Reshape the result to maintain the original faces tensor structure with additional dimensions for batch processing\n    return v[f].reshape(v.shape[0], *f.shape, v.shape[1])\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "s a new tensor with the same data type as the input tensor, but with an additional dimension of size 1 at the zeroth position. This function is used to implement the indexing behavior of the dotdict object, allowing it to be treated like a regular dictionary.\n    \"\"\"\n    if isinstance(batch, (list, tuple)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, np.ndarray):\n        batch = torch.from_numpy(batch)\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.unsqueeze(0)\n    else:\n        batch = torch.tensor(batch)\n    return batch\n\n\ndef add_scalar(batch, name, value) -> Union[torch.Tensor, np.ndarray]:\n    batch = add_batch(batch)\n    batch[name] = torch.tensor(value)\n    batch['meta'][name] = torch.tensor(value)\n    return batch\n\n\ndef add_iter(batch, iter, total) -> Union[torch.Tensor, np.ndarray]:\n    batch = add_batch(batch)\n    batch['iter'] = torch.tensor(iter)\n    batch['meta']['iter'] = torch.tensor(iter)\n    batch['total'] = torch.tensor(total)\n    batch['meta']['total'] = torch.tensor(total)\n    return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H = self.H\n        batch.W = self.W\n        batch.K = to_tensor(self.K)\n        batch.R = to_tensor(self.R)\n        batch.T = to_tensor(self.T)\n        batch.n = self.n\n        batch.f = self.f\n        batch.t = self.t\n        batch.v = self.v\n        batch.bounds = to_tensor(self.bounds)\n\n        # Camera update related parameters\n        batch.origin = to_tensor(self.origin)\n        batch.world_up = to_tensor(self.world_up)\n        batch.movement_speed = self.movement_speed\n        batch.front = to_tensor(self.front)\n        batch.down = to_tensor(self.down)\n        batch.right = to_tensor(self.right)\n        batch.center = to_tensor(self.center)\n        batch.drag_start = to_tensor(self.drag_start)\n        batch.drag_start_front = to_tensor(self.drag_start_front)\n        batch.drag_start_down = to_tensor(self.drag_start_down)\n        batch.drag_start_right = to_tensor(self.drag_start_right)\n        batch.drag_start_center = to_tensor(self.drag_start_center)\n        batch.drag_start_origin = to_tensor(self.drag_start_origin)\n        batch.drag_start_world_up = to_tensor(self.drag_start_world_up)\n        batch.drag_ymin = self.drag_ymin\n        batch.drag_ymax = self.drag_ymax\n        batch.is_dragging = self.is_dragging\n        batch.is_panning = self.is_panning\n        batch.about_origin = self.about_origin\n        batch.lock_fx_fy = self.lock_fx_fy\n\n        # Internal states to facilitate camera position change\n        batch.speed = to_tensor(self.speed)\n        batch.acc = to_tensor(self.acc)\n        batch.drag_coeff_mult = self.drag_coeff_mult\n        batch.constant_drag = self.constant_drag\n        batch.pause"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            similarity_threshold = self.calculate_similarity_threshold()\n            similarities = [cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0] for agent in self.agents]\n            if not similarities:\n                return None, -np.inf\n            max_similarity = max(similarities)\n            if max_similarity < similarity_threshold:\n                return None, -np.inf\n            return self.agents[np.argmax(similarities)], max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper)\n        prime_agent.prime = True\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.working_agent = True\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        serialized_agent = self.persistence.load_agent(agent.purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        agents = []\n        for serialized_agent in self.persistence.fetch_all_agents():\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = PROMPT_ENGINEERING_SYSTEM_PROMPT.format(goal=goal, sample_input=sample_input)\n            return self.openai_wrapper.get_completion(prompt)\n        except Exception as e:\n            logger.exception(f\"Error in generating prompt: {e}\")\n            return \"\""}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"REPLACE INTO agents (id, purpose, data) VALUES (?,?,?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE id =?\", (purpose,))\n            row = cursor.fetchone()\n            if row is None:\n                return None\n            return json.loads(row[0])"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            purposes = [row[0] for row in cursor.fetchall()]\n            return purposes"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"SELECT result FROM cache WHERE hash =?\",\n            (arg_hash,)\n        )\n        result = cursor.fetchone()\n        if result is None:\n            return None\n        return json.loads(result[0])"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"INSERT INTO cache VALUES (?,?)\", (arg_hash, json.dumps(result)))\n        self.connection.commit()"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update(args)\n\n    # If quiet mode is enabled, redirect the standard output to a file instead of displaying it in the terminal\n    if quiet_mode:\n        with open(os.path.join(CONFIG.get('record_dir'), 'output.log'), 'w') as f:\n            with redirect_stdout(f):\n                start_command_line(deepcopy(args.__dict__))\n    else:\n        start_command_line(deepcopy(args.__dict__))\n"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n                chatcompletion_kwargs.update(kwargs)\n                chatcompletion_kwargs.pop(\"schema_error_retry\", None)\n\n                response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n                response = json.loads(str(response))\n            else:\n                raise e\n\n        return response"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (self._last_time is not None and time() - self._last_time > self._refetch_interval):\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        state = {\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader\": self.item_loader.state_dict() if self.item_loader else None,\n            \"shuffle\": self.shuffle,\n            \"seed\": self.seed,\n            \"drop_last\": self.drop_last,\n            \"world_size\": self.distributed_env.world_size,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"num_samples_yielded\": num_samples_yielded,\n        }\n\n        return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict\n        self._validate_state_dict()\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        # Check if the state dictionary is empty\n        if not self._state_dict:\n            raise ValueError(\"The state dictionary is empty. Please ensure that the state dictionary is not empty before calling the load_state_dict method.\")\n\n        # Check if the current state of the StreamingDataset instance matches the state dictionary\n        if self._state_dict[\"current_epoch\"]!= self.current_epoch:\n            raise ValueError(\n                f\"The current epoch of the StreamingDataset instance ({self.current_epoch}) does not match the current epoch in the state dictionary ({self._state_dict['current_epoch']}). Please ensure that the current epoch of the StreamingDataset instance matches the current epoch in the state dictionary before calling the load_state_dict method.\"\n            )\n\n        if self._state_dict[\"input_dir_path\"]!= self.input_dir.path:\n            raise ValueError(\n                f\"The input directory path of the StreamingDataset instance ({self.input_dir.path}) does not match the input directory path in the state dictionary ({self._state_dict['input_dir_path']}). Please ensure that the input directory path of the StreamingDataset instance matches the input directory path in the state dictionary before calling the load_state_dict method.\"\n            )\n\n        if self._state_dict[\"input_dir_url\"]!= self.input_dir.url:\n            raise ValueError(\n                f\"The input directory URL of the StreamingDataset instance ({self.input_dir.url}) does not match the input directory URL in the state dictionary ({self._state_dict['input_dir_url']}). Please ensure that the input directory URL of the StreamingDataset instance matches the input directory URL in the state dictionary before calling the load_state_dict method.\"\n            )\n\n        if self._state_dict[\"item_loader\"]!= self.item_loader.state_dict():\n            raise ValueError(\n                f\"The item loader state of the StreamingDataset instance ({self.item_loader.state_dict()}) does not match the item loader state in the state dictionary ({self._state_dict['item_loader']}). Please ensure that the item loader state of the StreamingDataset instance matches the item loader state in the state dictionary before calling the load_state_dict method.\"\n            )\n\n        if self._state_dict[\"drop_last\"]!= self.drop_last:\n            raise ValueError(\n               "}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        return None\n\n    if os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"):\n        cache_dir = os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\")\n    else:\n        cache_dir = _DEFAULT_CACHE_DIR\n\n    cache_dir = os.path.join(cache_dir, hashlib.sha256(input_dir.encode(\"utf-8\")).hexdigest())\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    return cache_dir"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not self._s5cmd_available:\n            self._client.download_file(remote_filepath, local_filepath)\n            return\n\n        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"The provided `remote_filepath` {remote_filepath} doesn't have a s3:// scheme.\")\n\n        parsed_url = parse.urlparse(remote_filepath)\n        bucket_name = parsed_url.netloc\n        key = parsed_url.path.lstrip(\"/\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        with FileLock(f\"{local_filepath}.lock\", timeout=10):\n            if not os.path.exists(local_filepath):\n                cmd = f\"s5cmd cp s3://{bucket_name}/{key} {local_filepath}\"\n                subprocess.run(cmd, shell=True)\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Create a dictionary to store the chunks and intervals assigned to each worker.\n    workers_chunks = {}\n    workers_intervals = {}\n\n    # Calculate the number of chunks and intervals to be assigned to each worker.\n    num_chunks = len(chunks_replica)\n    num_intervals = len(intervals_replica)\n    num_chunks_per_worker = num_chunks // num_workers\n    num_intervals_per_worker = num_intervals // num_workers\n\n    # Distribute the chunks and intervals to the workers.\n    for worker_idx in range(num_workers):\n        # Calculate the start and end indices for the chunks and intervals assigned to this worker.\n        start_chunk_idx = worker_idx * num_chunks_per_worker\n        end_chunk_idx = (worker_idx + 1) * num_chunks_per_worker\n        start_interval_idx = worker_idx * num_intervals_per_worker\n        end_interval_idx = (worker_idx + 1) * num_intervals_per_worker\n\n        # Assign the chunks and intervals to this worker.\n        workers_chunks[worker_idx] = chunks_replica[start_chunk_idx:end_chunk_idx]\n        workers_intervals[worker_idx] = intervals_replica[start_interval_idx:end_interval_idx]\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[6:]\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            if not hasattr(item, \"filename\"):\n                raise ValueError(\n                    \"The JPEG Image's filename isn't defined. HINT: Open the image in your Dataset __getitem__ method.\"\n                )\n            if item.filename and os.path.isfile(item.filename):\n                # read the content of the file directly\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                item_bytes = io.BytesIO()\n                item.save(item_bytes, format=\"JPEG\")\n                item_bytes = item_bytes.getvalue()\n                return item_bytes, None\n\n        if isinstance(item, (PngImageFile, WebPImageFile, GifImageFile, Image.Image)):\n            buff = io.BytesIO()\n            item.convert(\"RGB\").save(buff, quality=100, format=\"JPEG\")\n            buff.seek(0)\n            return buff.read(), None\n\n        raise TypeError(f\"The provided item should be of type {JpegImageFile}. Found {item}.\")"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if _PIL_AVAILABLE and isinstance(item, JpegImageFile):\n            if item.filename:\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                with io.BytesIO() as f:\n                    item.save(f, format=\"jpeg\")\n                    return f.getvalue(), None\n        elif _PIL_AVAILABLE and isinstance(item, Image.Image):\n            with io.BytesIO() as f:\n                item.save(f, format=\"jpeg\")\n                return f.getvalue(), None\n        else:\n            raise TypeError(\"Unsupported image type\")\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[0:12], np.uint32)\n        mode = data[12:12 + ints[2]].decode(\"utf-8\")\n        raw = data[12 + ints[2] :]\n        return Image.frombytes(mode, (ints[0], ints[1]), raw)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the tensor bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return torch.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        return torch.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype).reshape(shape)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        if _PIL_AVAILABLE:\n            try:\n                return Image.open(io.BytesIO(data))\n            except OSError:\n                pass\n\n        raise TypeError(f\"The provided item should be of type {JpegImageFile}. Found {item}.\")"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        \"\"\"\n        Serializes a PyTorch tensor into a bytes object and a string representing the tensor's data type. The serialization process converts the tensor to a NumPy array and then to bytes. The data type of the tensor is mapped to an index, which is included in the returned string for identification.\n\n        Input-Output Arguments\n        :param self: NoHeaderTensorSerializer. An instance of the NoHeaderTensorSerializer class, which is responsible for serializing PyTorch tensors without including header information.\n        :param item: torch.Tensor, The PyTorch tensor to be serialized. It is used to extract the tensor's data and data type for serialization.\n        :return: Tuple[bytes, Optional[str]], A tuple where the first element is the serialized tensor data as bytes, and the second element is a string representing the tensor's data type through an index. The string is prefixed with \"no_header_tensor:\" followed by the data type index.\n        \"\"\"\n\n        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.numpy().tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return tensor.reshape(shape)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state_dict = {\n            \"dataset\": self.dataset.state_dict(),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_streaming if isinstance(self.dataset, StreamingDataset) else self._num_samples_yielded_combined,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n\n        return state_dict\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        # Check if the required libraries are installed\n        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\"The torchvision library is required to deserialize videos. Please install it using 'pip install torchvision'.\")\n        if not _AV_AVAILABLE:\n            raise ImportError(\"The av library is required to deserialize videos. Please install it using 'pip install av'.\")\n\n        # Write the data to a temporary file\n        with tempfile.NamedTemporaryFile(mode=\"wb\") as temp_file:\n            temp_file.write(data)\n            temp_file.flush()\n\n            # Use torchvision's read_video function to deserialize the video file into a video object\n            video = torchvision.io.read_video(temp_file.name)\n\n        return video"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return []\n\n        # Write the remaining chunks\n        while self._should_write():\n            self.write_chunk(on_done=True)\n\n        # Write the index file\n        self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        return [os.path.join(self._cache_dir, f) for f in os.listdir(self._cache_dir) if f.endswith(\".bin\")]"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"], self._num_samples_yielded_streaming)\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"], self._num_samples_yielded_combined)\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.current_epoch = obj[\"current_epoch\"]\n        self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n        self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        self.restore = True\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            else:\n                return {\n                    \"num_samples_yielded\": num_samples_yielded,\n                }\n\n        return self._iterator.state_dict(num_workers, batch_size)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if state_dict is None:\n            return\n\n        # Load the state of each dataset within the CombinedStreamingDataset\n        for dataset_idx, dataset in enumerate(self._datasets):\n            dataset.load_state_dict(state_dict[str(dataset_idx)])\n\n        # Update the number of samples yielded by the streaming dataloader to avoid repeating samples\n        if self._use_streaming_dataloader:\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if isinstance(dir_path, str):\n        if dir_path.startswith(\"s3://\"):\n            return _resolve_s3_connections(dir_path)\n\n        if dir_path.startswith(\"lightning://\"):\n            return _resolve_studio(dir_path, None, None)\n\n        if dir_path.startswith(\"lightning://datasets/\"):\n            return _resolve_datasets(dir_path)\n\n        if dir_path.startswith(\"lightning://studio/\"):\n            return _resolve_studio(dir_path, None, None)\n\n        if dir_path.startswith(\"lightning://studio/\"):\n            return _resolve_studio(dir_path, None, None)\n\n        if dir_path.startswith(\"lightning://studio/\"):\n            return _resolve_studio(dir_path, None, None)\n\n        if dir_path.startswith(\"lightning://studio/\"):\n            return _resolve_studio(dir_path, None, None)\n\n        if dir_path.startswith(\"lightning://studio/\"):\n            return _resolve_studio(dir_path, None, None)\n\n        if dir_path.startswith(\"lightning://studio/\"):\n            return _resolve_studio(dir_path, None, None)\n\n        if dir_path.startswith(\"lightning://studio/\"):\n            return _resolve_studio(dir_path, None, None)\n\n        if dir_path.startswith(\"lightning://studio/\"):\n            return _resolve_studio(dir_path, None, None)\n\n        if dir_path.startswith(\"lightning://studio/\"):\n            return _resolve_studio(dir_path, None, None)\n\n        if dir_path.startswith(\"lightning://studio/\"):\n            return _resolve_studio(dir_path, None, None)\n\n        if dir_path.startswith(\"lightning://studio/\"):\n            return _resolve_studio(dir_path, None, None)\n\n        if dir_path.startswith(\"lightning://studio/\"):\n            return _resolve_studio(dir_path, None, None)\n\n        if dir_path.startswith(\"lightning://studio/\"):\n            return _resolve_studio(dir_path, None, None)\n\n        if dir_path.startswith(\"lightning://studio/\"):\n            return _resolve_studio(dir_path"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    # No files are found in this folder\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    # Check the index file exists\n    try:\n        s3.head_object(Bucket=obj.netloc, Key=os.path.join(prefix, \"index.json\"))\n        has_index_file = True\n    except botocore.exceptions.ClientError:\n        has_index_file = False\n\n    if has_index_file:\n        if overwrite:\n            bucket_name = obj.netloc\n            s3 = boto3.resource(\"s3\")\n            for obj in s3.Bucket(bucket_name).objects.filter(Prefix=prefix):\n                s3.Object(bucket_name, obj.key).delete()\n        else:\n            raise RuntimeError(\n                f\"The provided output_dir `{output_dir.path}` already contains an optimized immutable datasets.\"\n                \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n            )\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme!= \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if objects[\"KeyCount\"] > 0:\n        for obj in objects[\"Contents\"]:\n            if obj[\"Key\"].endswith(\"index.json\"):\n                raise RuntimeError(\n                    f\"The provided output_dir `{output_dir.path}` already contains an index file named 'index.json'.\"\n                    \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n                )\n\n    # Delete all objects within the specified prefix in the bucket.\n    if objects[\"KeyCount\"] > 0:\n        s3.delete_objects(\n            Bucket=obj.netloc,\n            Delete={\n                \"Objects\": [\n                    {\"Key\": obj[\"Key\"]} for obj in objects[\"Contents\"] if not obj[\"Key\"].endswith(\"index.json\")\n                ],\n                \"Quiet\": True,\n            },\n        )\n\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if self._distributed_env == _DistributedEnv.NONE:\n            self._merge_no_wait()\n            return\n\n        if node_rank is None:\n            node_rank = self._distributed_env.global_rank\n\n        if node_rank == 0:\n            # Wait for all the workers to write their own index\n            while len(os.listdir(self._cache_dir)) < num_workers:\n                sleep(0.1)\n\n            # Merge the index files\n            self._merge_no_wait(node_rank)\n        else:\n            # Wait for the master node to write the index\n            while not os.path.exists(os.path.join(self._cache_dir, f\"{node_rank}-{_INDEX_FILENAME}\")):\n                sleep(0.1)"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ImportError(\n            \"The `lightning_sdk` package is required to use the `execute` method. \"\n            \"Please install it with `pip install lightning_sdk`.\"\n        )\n\n    if machine is None:\n        machine = Machine(num_nodes=num_nodes)\n\n    if command is None:\n        command = f\"cd {os.getcwd()} && env\"\n\n    job = machine.execute(name=name, command=command)\n\n    print(f\"Job started: {job.url}\")\n\n    while True:\n        job.refresh()\n        if job.status == \"running\":\n            print(f\"Job running: {job.url}\")\n        elif job.status == \"failed\":\n            raise RuntimeError(f\"Job failed: {job.url}\")\n        elif job.status == \"succeeded\":\n            print(f\"Job succeeded: {job.url}\")\n            break\n        sleep(10)\n\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Check if the config is already loaded\n        if self._config is not None:\n            return self._config\n\n        # Check if the config is already loaded\n        if self._config is not None:\n            return self._config\n\n        # Check if the config is already loaded\n        if self._config is not None:\n            return self._config\n\n        # Check if the config is already loaded\n        if self._config is not None:\n            return self._config\n\n        # Check if the config is already loaded\n        if self._config is not None:\n            return self._config\n\n        # Check if the config is already loaded\n        if self._config is not None:\n            return self._config\n\n        # Check if the config is already loaded\n        if self._config is not None:\n            return self._config\n\n        # Check if the config is already loaded\n        if self._config is not None:\n            return self._config\n\n        # Check if the config is already loaded\n        if self._config is not None:\n            return self._config\n\n        # Check if the config is already loaded\n        if self._config is not None:\n            return self._config\n\n        # Check if the config is already loaded\n        if self._config is not None:\n            return self._config\n\n        # Check if the config is already loaded\n        if self._config is not None:\n            return self._config\n\n        # Check if the config is already loaded\n        if self._config is not None:\n            return self._config\n\n        # Check if the config is already loaded\n        if self._config is not None:\n            return self._config\n\n        # Check if the config is already loaded\n        if self._config is not None:\n            return self._config\n\n        # Check if the config is already loaded\n        if self._config is not None:\n            return self._config\n\n        # Check if the config is already loaded\n        if self._config is not None:\n            return self._config\n\n        # Check if the config is already loaded\n        if self._config is not None:\n            return self._config\n\n        # Check if the config is already loaded\n        if self._config is not None:\n            return self._config\n\n        # Check if the config is already loaded\n        if self._config is not None:\n            return self._config\n\n        # Check if the config is already loaded\n        if self._config is not None:\n            return self._config\n\n        # Check if the config is already loaded\n        if self._config is not None"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The index should be an instance of ChunkedIndex, got {type(index)} instead.\")\n\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Check whether the chunk is already loaded\n        if self._config.is_chunk_loaded(index):\n            return self._item_loader.get_item(index)\n\n        # Check whether the chunk is already downloaded\n        if self._config.is_chunk_downloaded(index):\n            self._item_loader.pre_load_chunk(index.chunk_index, self._config[index])\n            return self._item_loader.get_item(index)\n\n        # Check whether the chunk is already downloaded\n        if self._config.is_chunk_downloaded(index):\n            self._item_loader.pre_load_chunk(index.chunk_index, self._config[index])\n            return self._item_loader.get_item(index)\n\n        # Check whether the chunk is already downloaded\n        if self._config.is_chunk_downloaded(index):\n            self._item_loader.pre_load_chunk(index.chunk_index, self._config[index])\n            return self._item_loader.get_item(index)\n\n        # Check whether the chunk is already downloaded\n        if self._config.is_chunk_downloaded(index):\n            self._item_loader.pre_load_chunk(index.chunk_index, self._config[index])\n            return self._item_loader.get_item(index)\n\n        # Check whether the chunk is already downloaded\n        if self._config.is_chunk_downloaded(index):\n            self._item_loader.pre_load_chunk(index.chunk_index, self._config[index])\n            return self._item_loader.get_item(index)\n\n        # Check whether the chunk is already downloaded\n        if self._config.is_chunk_downloaded(index):\n            self._item_loader.pre_load_chunk(index.chunk_index, self._config[index])\n            return self._item_loader.get_item(index)\n\n        # Check whether the chunk is already downloaded\n        if self._config.is_chunk_downloaded(index):\n            self"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_CLOUD_URL\") is None:\n        return obj\n\n    try:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    except Exception as e:\n        logger.warning(f\"Failed to broadcast {key=} {obj=}. {e}\")\n        return obj"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Get the number of chunks per rank\n    num_chunks_per_rank = [len(chunks) for chunks in chunks_per_ranks]\n\n    # 2. Get the total number of chunks\n    total_num_chunks = sum(num_chunks_per_rank)\n\n    # 3. Create a list of chunk indexes\n    chunk_indexes = list(range(total_num_chunks))\n\n    # 4. Shuffle the chunk indexes using the provided seed and current epoch\n    np.random.seed(seed + current_epoch)\n    np.random.shuffle(chunk_indexes)\n\n    # 5. Split the shuffled chunk indexes into chunks per rank\n    chunks_per_ranks = [\n        chunk_indexes[i : i + num_chunks_per_rank[i]]\n        for i in range(0, total_num_chunks, num_chunks_per_rank[0])\n    ]\n\n    return chunks_per_ranks\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if len(inputs) < 2:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not isinstance(inputs[0], str) or not isinstance(inputs[1], str):\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not os.path.exists(inputs[0]) or not os.path.exists(inputs[1]):\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not os.path.commonpath([inputs[0], inputs[1]]):\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    return os.path.commonpath([inputs[0], inputs[1]])\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if not _IS_IN_STUDIO:\n        return\n\n    with open(\"/etc/resolv.conf\") as f:\n        lines = f.readlines()\n\n    if (enable and any(\"127.0.0.53\" in line for line in lines)) or (\n        not enable and any(\"127.0.0.1\" in line for line in lines)\n    ):\n        return\n\n    cmd = (\n        f\"sudo /home/zeus/miniconda3/envs/cloudspace/bin/python\"\n        f\" -c 'from litdata.processing.utilities import _optimize_dns; _optimize_dns({enable})'\"\n    )\n    Popen(cmd, shell=True, stdout=DEVNULL, stderr=DEVNULL).wait()  # E501\n\n    try:\n        yield\n    finally:\n        _optimize_dns(not enable)"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items each rank should process\n    num_items_per_rank = distributed_env.world_size // distributed_env.num_nodes\n    if drop_last:\n        num_items_per_rank -= distributed_env.world_size % distributed_env.num_nodes\n\n    # distribute the chunks and their intervals across the ranks\n    chunks_per_ranks = [[] for _ in range(distributed_env.num_nodes)]\n    intervals_per_ranks = [[] for _ in range(distributed_env.num_nodes)]\n    for i in range(len(indexes)):\n        rank = i % distributed_env.num_nodes\n        chunks_per_ranks[rank].append(indexes[i])\n        intervals_per_ranks[rank].append(chunk_intervals[i])\n\n    return chunks_per_ranks, intervals_per_ranks"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            self._find_device()\n\n        if self._contains_is_last:\n            yield self._fn(item_metadata, output_dir, is_last=is_last)\n        else:\n            yield self._fn(item_metadata, output_dir)"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": " the file exists. If the file does not exist, the function will wait for the file to exist before returning. If the file exists, the function will return the response from the head_object method. If any other error occurs, the function will raise an error.\n        Input-Output Arguments:\n            s3: The S3Client object.\n            obj: The parsed result of the S3 object, containing the bucket name and key.\n            sleep_time: The amount of time in seconds to wait before retrying if the file does not exist. Defaults to 2 seconds.\n        Returns:\n            Any: The response from the head_object method if the file exists. This includes metadata about the S3 object.\n        \"\"\"\n        while True:\n            try:\n                return s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            except botocore.exceptions.ClientError as e:\n                if e.response[\"Error\"][\"Code\"] == \"404\":\n                    sleep(sleep_time)\n                else:\n                    raise e\n\n    def _wait_for_file_to_exist(s3: S3Client, obj: parse.ParseResult, sleep_time: int = 2) -> None:\n        \"\"\"This function waits for a specified file to exist in an S3 bucket.\n\n        This function will attempt to retrieve the metadata for a specified file in an S3 bucket using the head_object method. If the file does not exist, the function will wait for the file to exist before returning. If the file exists, the function will return the response from the head_object method. If any other error occurs, the function will raise an error.\n        Input-Output Arguments:\n            s3: The S3Client object.\n            obj: The parsed result of the S3 object, containing the bucket name and key.\n            sleep_time: The amount of time in seconds to wait before retrying if the file does not exist. Defaults to 2 seconds.\n        Returns:\n            Any: The response from the head_object method if the file exists. This includes metadata about the S3 object.\n        \"\"\"\n        while True:\n            try:\n                return s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            except botocore.exceptions.ClientError as e:\n                if e."}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to optimize your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n           "}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "e_queue, queue_out)\n\n        # 2. Check if the file is already downloaded\n        if os.path.exists(local_filepath):\n            # 2.1. Check if the file is already downloaded\n            if os.path.getsize(local_filepath) == file_bytes:\n                # 2.2. If the file is already downloaded, we can skip it\n                queue_out.put(None)\n                return\n            else:\n                # 2.3. If the file is not the same size, we need to delete it and download it again\n                os.remove(local_filepath)\n\n        # 3. Download the file\n        if obj.scheme == \"s3\":\n            s3 = S3Client()\n            s3.client.download_file(obj.netloc, obj.path.lstrip(\"/\"), local_filepath)\n        elif input_dir.path and os.path.isdir(input_dir.path):\n            shutil.copyfile(os.path.join(input_dir.path, os.path.basename(remote_filepath)), local_filepath)\n\n        # 4. Wait for the file to be downloaded\n        _wait_for_file_to_exist(local_filepath, sleep_time=2)\n\n        # 5. Check if the file is the same size as the file we downloaded\n        if os.path.getsize(local_filepath) == file_bytes:\n            # 5.1. If the file is the same size, we can skip it\n            queue_out.put(None)\n        else:\n            # 5.2. If the file is not the same size, we need to delete it and download it again\n            os.remove(local_filepath)\n            _download_data_target(input_dir, obj, local_filepath, queue_in, queue_out)\n\n    # Terminate the process if we received a termination signal\n    if index is None:\n        return\n\n    # 6. If the file is the same size, we can skip it\n    queue_out.put(None)\n\ne_dir, \"data\", \"test\", \"test.txt\")\n    with open(test_file, \"w\") as f:\n        f.write(\"test\")\n\n    # 2. Create a dataset\n    dataset = Dataset(\n        input_dir="}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "   # 2. Remove the file from the cache dir.\n        # 3. Send a signal to the uploader to remove the file from the remote dir.\n        # 4. Send a signal to the remover to remove the file from the local dir.\n        # 5. Send a signal to the downloader to remove the file from the local dir.\n        # 6. Send a signal to the downloader to remove the file from the remote dir.\n        # 7. Send a signal to the remover to remove the file from the remote dir.\n        # 8. Send a signal to the remover to remove the file from the cache dir.\n        # 9. Send a signal to the remover to remove the file from the cache data dir.\n        # 10. Send a signal to the remover to remove the file from the cache data dir.\n        # 11. Send a signal to the remover to remove the file from the cache data dir.\n        # 12. Send a signal to the remover to remove the file from the cache data dir.\n        # 13. Send a signal to the remover to remove the file from the cache data dir.\n        # 14. Send a signal to the remover to remove the file from the cache data dir.\n        # 15. Send a signal to the remover to remove the file from the cache data dir.\n        # 16. Send a signal to the remover to remove the file from the cache data dir.\n        # 17. Send a signal to the remover to remove the file from the cache data dir.\n        # 18. Send a signal to the remover to remove the file from the cache data dir.\n        # 19. Send a signal to the remover to remove the file from the cache data dir.\n        # 20. Send a signal to the remover to remove the file from the cache data dir.\n        # 21. Send a signal to the remover to remove the file from the cache data dir.\n        # 22. Send a signal to the remover to remove the file from the cache data dir.\n        # 23. Send a signal to the remover to remove the file from the cache data dir.\n        # 24. Send a signal to the remover to remove the file"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "s.\n    \"\"\"\n    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    # If the number of items is less than the number of workers, we need to add some extra workers.\n    extra_workers = num_items % num_workers\n    if extra_workers:\n        num_workers += 1\n\n    # Distribute the items among the workers.\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n\n    # Distribute the items among the workers.\n    worker_items_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n    # Shuffle the items among the workers.\n    worker_items = [worker_items[i] for i in worker_items_ids_this_node]\n    worker_weights = [worker_weights[i] for i in worker_items_ids_this_node]\n\n    return worker_items, worker_weights\n\n\ndef _pack_greedily(items: List[Any], weights: Optional[List[int]], num_bins: int) -> Tuple[List[Any], List[int]]:\n    \"\"\"This function distributes a list of items among workers in a greedy manner, optionally considering file sizes.\n\n    Arguments:\n        items: The list of items to be distributed among workers.\n        weights: The list of weights for each item, used for distribution. If not provided, all items are considered to\n            have equal weight.\n        num_bins: The number of bins to distribute the items into.\n\n    Returns:\n        A tuple of lists, where the first list contains the items distributed among workers, and the second list contains\n        the weights of each item.\n    \"\"\"\n    if weights is None:\n        weights = [1] * len(items)\n\n    # Sort the items by weight.\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Distribute the items among the workers.\n    worker_items = [[] for _ in range(num_bins)]\n    worker_weights = [[] for _ in range(num_bins)]\n   "}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # 2. If the remainder is not 0, add the remainder to the last worker.\n    # 3. Ensure the output list has the same length as the number of workers; otherwise, raise an error.\n    # 4. This function relies on two other functions: `_get_num_nodes()` to get the total number of nodes and\n    # `_get_node_rank()` to identify the current node's rank within the environment.\n\n    # Get the total number of nodes\n    num_nodes = _get_num_nodes()\n\n    # Get the current node's rank within the environment\n    node_rank = _get_node_rank()\n\n    # Get the number of items to process\n    num_items = len(user_items)\n\n    # Get the number of items to process per worker\n    num_items_per_worker = num_items // num_nodes\n\n    # Get the remainder of the division\n    remainder = num_items % num_nodes\n\n    # Create an empty list to store the worker indices\n    worker_indices = []\n\n    # Iterate over the number of nodes\n    for i in range(num_nodes):\n        # Get the start index for the current worker\n        start_index = i * num_items_per_worker\n\n        # Get the end index for the current worker\n        end_index = start_index + num_items_per_worker\n\n        # If the remainder is not 0 and the current worker is the last worker, add the remainder to the end index\n        if remainder > 0 and i == num_nodes - 1:\n            end_index += remainder\n\n        # Add the worker indices to the list\n        worker_indices.append(list(range(start_index, end_index)))\n\n    # Adjust the worker indices to account for the remainder\n    if remainder > 0:\n        worker_indices[-1].append(worker_indices[-1][-1] + remainder)\n\n    # Ensure the output list has the same length as the number of workers; otherwise, raise an error.\n    if len(worker_indices)!= num_nodes:\n        raise RuntimeError(\n            f\"The number of workers ({num_nodes}) does not match the number of items ({num_items}) to process.\"\n        )\n\n    # Return the worker indices\n    return worker_indices\n\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "e_dir, \"data\")\n        os.makedirs(data_dir, exist_ok=True)\n\n        # Create a dummy file\n        with open(os.path.join(data_dir, \"dummy.txt\"), \"w\") as f:\n            f.write(\"dummy\")\n\n        # Create a dummy file\n        with open(os.path.join(data_dir, \"dummy2.txt\"), \"w\") as f:\n            f.write(\"dummy\")\n\n        # Create a dummy file\n        with open(os.path.join(data_dir, \"dummy3.txt\"), \"w\") as f:\n            f.write(\"dummy\")\n\n        # Create a dummy file\n        with open(os.path.join(data_dir, \"dummy4.txt\"), \"w\") as f:\n            f.write(\"dummy\")\n\n        # Create a dummy file\n        with open(os.path.join(data_dir, \"dummy5.txt\"), \"w\") as f:\n            f.write(\"dummy\")\n\n        # Create a dummy file\n        with open(os.path.join(data_dir, \"dummy6.txt\"), \"w\") as f:\n            f.write(\"dummy\")\n\n        # Create a dummy file\n        with open(os.path.join(data_dir, \"dummy7.txt\"), \"w\") as f:\n            f.write(\"dummy\")\n\n        # Create a dummy file\n        with open(os.path.join(data_dir, \"dummy8.txt\"), \"w\") as f:\n            f.write(\"dummy\")\n\n        # Create a dummy file\n        with open(os.path.join(data_dir, \"dummy9.txt\"), \"w\") as f:\n            f.write(\"dummy\")\n\n        # Create a dummy file\n        with open(os.path.join(data_dir, \"dummy10.txt\"), \"w\") as f:\n            f.write(\"dummy\")\n\n        # Create a dummy file\n        with open(os.path.join(data_dir, \"dummy11.txt\"), \"w\") as f:\n            f.write(\"dummy\")\n\n        # Create a dummy file\n        with open(os.path.join(data_dir, \"dummy12.txt\"), \"w\") as f:\n            f.write(\"dummy\")\n\n        # Create a"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    \"\"\"\n    item_sizes = []\n\n    cpu_count = os.cpu_count() or 1\n\n    # Parallelize to accelerate retrieving the number of file bytes to read for each item\n    with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count * 2 if cpu_count > 4 else cpu_count) as executor:\n\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n\n    \"\"\"\n    Computes and returns a list of file sizes for each item in the given list of items by parallelizing the file size retrieval process to improve performance.\n    Input-Output Arguments:\n    items: List[Any]. A list of items for which to compute the file sizes. The nature of these items is not specified, but they are used to determine file sizes in some way.\n    base_path: str, optional. A base path to be prepended to each item's path before computing its size. Defaults to an empty string, indicating no base path is used.\n    return: List[int]. A list of file sizes corresponding to each item in the input list. Each size is an integer representing the number of bytes.\n    \"\"\"\n    for future in futures:\n        item_sizes.append(future.result())\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    return False\n\n    return True\n\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"n_layers must be greater than 0\"\n        assert n_neurons > 0, \"n_neurons must be greater than 0\"\n\n        if self.tcnn:\n            if n_neurons > 1000:\n                return self.get_network_with_skip_layers(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    n_layers=n_layers,\n                    n_neurons=n_neurons,\n                    activation=activation,\n                    output_activation=output_activation,\n                    skips=[n_neurons],\n                )\n            else:\n                return self.get_network_with_skip_layers(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    n_layers=n_layers,\n                    n_neurons=n_neurons,\n                    activation=activation,\n                    output_activation=output_activation,\n                    skips=[],\n                )\n        else:\n            return self.get_pytorch_network(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                n_layers=n_layers,\n                n_neurons=n_neurons,\n                activation=activation,\n                output_activation=output_activation,\n            )\n"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Compute the rolling median of the signal by shifting it by a range defined by the kernel offset, and then computing the median of these shifted signals.\n        shifted_signals = np.array([np.roll(signal, i) for i in range(-kernel_offset, kernel_offset + 1)])\n        rolling_median = np.median(shifted_signals, axis=0)\n\n        # Trim the resulting median array to account for edge effects introduced by the shifting process.\n        return rolling_median[kernel_offset:-kernel_offset]"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if not isinstance(template_probe, IrisTemplate):\n        raise MatcherError(\"template_probe must be an IrisTemplate\")\n    if not isinstance(template_gallery, IrisTemplate):\n        raise MatcherError(\"template_gallery must be an IrisTemplate\")\n\n    if template_probe.width!= template_gallery.width:\n        raise MatcherError(\"template_probe and template_gallery must have the same width\")\n\n    if template_probe.height!= template_gallery.height:\n        raise MatcherError(\"template_probe and template_gallery must have the same height\")\n\n    if template_probe.width % 2!= 1:\n        raise MatcherError(\"template_probe and template_gallery must have an odd width\")\n\n    if template_probe.height % 2!= 1:\n        raise MatcherError(\"template_probe and template_gallery must have an odd height\")\n\n    if template_probe.width < 3:\n        raise MatcherError(\"template_probe and template_gallery must have a width of at least 3\")\n\n    if template_probe.height < 3:\n        raise MatcherError(\"template_probe and template_gallery must have a height of at least 3\")\n\n    if rotation_shift < 0:\n        raise MatcherError(\"rotation_shift must be a non-negative integer\")\n\n    if nm_dist is not None and nm_dist < 0:\n        raise MatcherError(\"nm_dist must be a non-negative float\")\n\n    if weights is not None:\n        if not isinstance(weights, list):\n            raise MatcherError(\"weights must be a list of numpy arrays\")\n        if len(weights)!= template_probe.width:\n            raise MatcherError(\"weights must be a list of numpy arrays with the same length as the template width\")\n        for w in weights:\n            if not isinstance(w, np.ndarray):\n                raise MatcherError(\"weights must be a list of numpy arrays\")\n            if w.shape!= (template_probe.height, template_probe.width):\n                raise MatcherError(\"weights must be a list of numpy arrays with the same shape as the template\")\n\n    half_width = [w // 2 for w in template_probe.width]\n\n    irisbits_probe = template_probe.irisbits\n    maskbits_probe = template_probe.maskbits\n    irisbits_gallery = template_gallery.irisbits\n   "}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Generate a random permutation of the polygon's vertices\n        random_permutation = np.random.permutation(polygon)\n\n        # Initialize the first and second bisectors' points\n        first_bisectors_point = np.zeros_like(polygon)\n        second_bisectors_point = np.zeros_like(polygon)\n\n        # Iterate over the random permutation and calculate the perpendicular bisectors\n        for i in range(self.params.num_bisectors):\n            # Get the first and second points\n            first_point = random_permutation[i]\n            second_point = random_permutation[(i + 1) % len(random_permutation)]\n\n            # Calculate the perpendicular bisector\n            bisector_point = (first_point + second_point) / 2\n\n            # Calculate the distance between the bisector point and the first point\n            distance_to_first_point = np.linalg.norm(first_point - bisector_point)\n\n            # Check if the distance is greater than the minimum distance\n            if distance_to_first_point >= min_distance_between_sector_points_in_px:\n                # If the distance is greater than the minimum distance, set the bisector point as the first bisector point\n                first_bisectors_point[i] = bisector_point\n            else:\n                # If the distance is less than the minimum distance, set the first point as the first bisector point\n                first_bisectors_point[i] = first_point\n\n            # Calculate the distance between the bisector point and the second point\n            distance_to_second_point = np.linalg.norm(second_point - bisector_point)\n\n            # Check if the distance is greater than the minimum distance\n            if distance_to_second_point >= min_distance_between_sector_points_in_px:\n                # If the distance is greater than the minimum distance, set the bisector point as the second bisector point\n                second_bisectors_point[i] = bisector_point\n            else:\n                # If the distance is less than the minimum distance, set the second point as the second bisector point\n                second_bisectors_point[i] = second_point\n\n        # Return the first and second bisectors'"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(self, *args, **kwargs)\n\n        return result\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        \"\"\"\n        # Deserialize the output string into a Python object\n        try:\n            output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        # Validate the output against the type definition\n        return self.check_type(output, type_definition)"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        # Get the function's signature and type hints\n        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        # Get the function's name and docstring\n        func_name = func_object.__name__\n        func_docstring = func_object.__doc__\n\n        # Initialize the function description\n        function_description = FunctionDescription(func_name, func_docstring)\n\n        # Get the function's input and output type hints\n        input_type_hints = {}\n        output_type_hints = {}\n        for name, hint in type_hints.items():\n            if name =='return':\n                output_type_hints[name] = hint\n            else:\n                input_type_hints[name] = hint\n\n        # Get the function's input and output class definitions\n        input_class_definitions = {}\n        output_class_definitions = {}\n        for name, hint in input_type_hints.items():\n            input_class_definitions[name] = get_class_definition(hint)\n        for name, hint in output_type_hints.items():\n            output_class_definitions[name] = get_class_definition(hint)\n\n        # Set the function's input and output class definitions\n        function_description.input_class_definitions = input_class_definitions\n        function_description.output_class_definitions = output_class_definitions\n\n        # Set the function's input and output type hints\n        function_description.input_type_hints = input_type_hints\n        function_description.output_type_hints = output_type_hints\n\n        # Set the function's signature\n        function_description.signature = signature\n\n        # Set the function's source code\n        function_description.source = get_source(func_object)\n\n        # Determine the function type\n        function_type = FunctionType.SYMBOLIC\n        if issubclass(output_type_hints['return'], Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        elif get_origin(output_type_hints['return']) == Union:\n            function_type = FunctionType.EMBEDDABLE\n\n        # Set the function type\n        function_description.type = function_type\n\n        return function_description\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] = seed"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n\n        if len(loaded_bit_array)!= self.size:\n            logging.warning(f\"Loaded bit array length {len(loaded_bit_array)} does not match expected length {self.size}. Reinitializing bit array and indices.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_arraye_name, self.file_extension)\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.create_model_config(json_dict[\"distilled_model\"])\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        self.teacher_models = [config_factory.create_model_config(model_config) for model_config in json_dict[\"teacher_models\"]]\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        # Check if the API key is set\n        if not self.api_key:\n            raise ValueError(\"OpenAI API key is not set\")\n\n        # Check if the model is valid\n        if not model.model_name:\n            raise ValueError(\"Model name is not set\")\n\n        # Check if the system message is valid\n        if not system_message:\n            raise ValueError(\"System message is not set\")\n\n        # Check if the prompt is valid\n        if not prompt:\n            raise ValueError(\"Prompt is not set\")\n\n        # Set the default parameters\n        parameters = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n        }\n\n        # Add any additional parameters\n        parameters.update(kwargs)\n\n        # Set the default headers\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.api_key}\"\n        }\n\n        # Set the default timeout\n        timeout = 10\n\n        # Set the default retries\n        retries = 5\n\n        # Set the default backoff factor\n        backoff_factor = 1.5\n\n        # Set the default backoff delay\n        backoff_delay = 1\n\n        # Set the default backoff max delay\n        backoff_max_delay = 30\n\n        # Set the default backoff method\n        backoff_method = \"exponential\"\n\n        # Set the default backoff exceptions\n        backoff_exceptions = (requests.exceptions.ConnectionError, requests.exceptions.ReadTimeout)\n\n        # Set the default backoff callback\n        def backoff_callback(details):\n            logging.warning(f\"Retrying request after {details['total_time']}.\")\n\n        # Set the default backoff options\n        backoff_options = {\n            \"backoff_factor\": backoff_factor,\n            \"backoff_delay\": backoff_delay,\n            \"backoff_max_delay\": backoff_max_delay,\n            \"method\": backoff_method,\n            \"exceptions\": backoff_exceptions,\n            \"callback\": backoff_callback\n        }\n\n        # Set the default retry options\n        retry_options = {\n            \"total\": retries,\n            \"connect\": retries,\n            \"read\": retries,\n            \"status\": retries,\n            \"method_whitelist\": backoff_exceptions,\n            \"backoff\": backoff_options\n        }\n\n        # Set the default retry object\n        retry = requests.adapters"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.ndim!= 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0]!= x.shape[1]:\n        raise ValueError(f\"`x` must be a square matrix, got a {x.shape[0]}x{x.shape[1]} matrix\")\n    if not np.allclose(x, x.T):\n        raise ValueError(f\"`x` must be a symmetric matrix, got a non-symmetric matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    if x.ndim!= 2 or x.shape[0]!= x.shape[1]:\n        raise ValueError(\"The matrix must be square\")\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n    if not np.allclose(np.diag(x), np.zeros(x.shape[0])):\n        raise ValueError(\"The matrix must have zero diagonal elements\")\n\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the teacher models\n        teacher_models = self.function_modeler.get_models(function_description)[1]\n        # get the distilled models\n        distilled_models = self.function_modeler.get_models(function_description)[0]\n        # get the alignments\n        aligns = self.function_modeler.get_symbolic_alignments(func_hash, max=5)\n        # get the examples\n        examples = [f\"Inputs:\\nArgs: {align['args']}\\nKwargs: {align['kwargs']}\\nOutput: {align['output']}\" for align in\n                 aligns]\n        # get the token counts\n        examples_token_count = sum([approximate_token_count(example) for example in examples])\n        input_prompt_token_count = approximate_token_count(f\"Function: {function_description}\\n---\\nInputs:\\nArgs: {args}\\nKwargs: {kwargs}\\nOutput:\")\n        generation_tokens = llm_parameters.get(\"max_new_tokens\", self.default_generation_length)\n        # check if the input is suitable for finetuning\n        suitable_for_finetune, input_prompt_token_count = self.suitable_for_finetuning_token_check(args, kwargs, function_description, distilled_models[0])\n        # check if the input is suitable for distillation\n        suitable_for_distillation = self.suitable_for_distillation_token_check(input_prompt_token_count, examples_token_count, generation_tokens)\n        # check if the function is already initialized\n        function_initialized = self.initialized_functions.get(func_hash, None)\n        if function_initialized:\n            model = function_initialized[\"model\"]\n            save_to_finetune = function_initialized[\"save_to_finetune\"]\n            is_distilled_model = function_initialized[\"is_distilled_model\"]\n        else:\n            # choose the model\n            model = self.choose_model_from_tokens(teacher_models, \n                                                  input_prompt_token_count+examples_token_count+generation_tokens,\n                                                  len(examples))\n            if model:\n                # if the model is not None, then"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if cov.ndim!= 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n\n    if higham:\n        return _higham_nearest(cov, max_iteration=higham_max_iteration)\n    else:\n        return _clip_nearest(cov)\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home=data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, (list, tuple)):\n        return ListSchema.flatten(obj)\n    elif isinstance(obj, dict):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, (Instances, Boxes, ROIMasks)):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, (torch.Tensor, torch.nn.Parameter)):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, (torch.nn.Module, torch.nn.ModuleList)):\n        raise ValueError(\n            \"TracingAdapter does not support tracing nn.Module or nn.ModuleList.\"\n        )\n    elif isinstance(obj, (torch.nn.ModuleDict, torch.nn.ParameterDict)):\n        raise ValueError(\n            \"TracingAdapter does not support tracing nn.ModuleDict or nn.ParameterDict.\"\n        )\n    elif isinstance(obj, (torch.nn.Sequential, torch.nn.ModuleList)):\n        raise ValueError(\n            \"TracingAdapter does not support tracing nn.Sequential or nn.ModuleList.\"\n        )\n    elif isinstance(obj, (torch.nn.ModuleDict, torch.nn.ParameterDict)):\n        raise ValueError(\n            \"TracingAdapter does not support tracing nn.ModuleDict or nn.ParameterDict.\"\n        )\n    elif isinstance(obj, (torch.nn.ModuleDict, torch.nn.ParameterDict)):\n        raise ValueError(\n            \"TracingAdapter does not support tracing nn.ModuleDict or nn.ParameterDict.\"\n        )\n    elif isinstance(obj, (torch.nn.ModuleDict, torch.nn.ParameterDict)):\n        raise ValueError(\n            \"TracingAdapter does not support tracing nn.ModuleDict or nn.ParameterDict.\"\n        )\n    elif isinstance(obj, (torch.nn.ModuleDict, torch.nn.ParameterDict)):\n        raise ValueError(\n            \"TracingAdapter does not support tracing nn.ModuleDict or nn.ParameterDict.\"\n        )\n    elif isinstance(obj, (torch.nn.ModuleDict, torch.nn.ParameterDict)):\n        raise ValueError(\n            \"TracingAdapter does not support tracing nn.ModuleDict or nn.ParameterDict.\"\n        )\n    elif isinstance(obj, (torch.nn.ModuleDict, torch.nn.ParameterDict)):\n        raise ValueError(\n            \"TracingAdapter does not support tracing nn.ModuleDict"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    if not isinstance(groups, np.ndarray):\n        raise TypeError(f\"{names[0]} must be a numpy array, not {type(groups)}\")\n    if not isinstance(equations, np.ndarray):\n        raise TypeError(f\"{names[1]} must be a numpy array, not {type(equations)}\")\n    if not isinstance(sum_to_one, bool):\n        raise TypeError(f\"sum_to_one must be a boolean, not {type(sum_to_one)}\")\n    if not isinstance(raise_if_group_missing, bool):\n        raise TypeError(\n            f\"raise_if_group_missing must be a boolean, not {type(raise_if_group_missing)}\"\n        )\n    if not isinstance(names, tuple):\n        raise TypeError(f\"names must be a tuple, not {type(names)}\")\n    if len(names)!= 2:\n        raise ValueError(f\"names must be a tuple of length 2, not {len(names)}\")\n    if not isinstance(names[0], str):\n        raise TypeError(f\"names[0] must be a string, not {type(names[0])}\")\n    if not isinstance(names[1], str):\n        raise TypeError(f\"names[1] must be a string, not {type(names[1])}\")\n\n    n_groups = groups.shape[0]\n    n_assets = groups.shape[1]\n    n_equations = len(equations)\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n\n    for i, equation in enumerate(equations):\n        try:\n            left[i], right[i] = _string_to_equation(\n                groups=groups, string=equation, sum_to_one=sum_to_one\n            )\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            else:\n                warnings.warn(str(e))\n\n    return left, right"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    with ExitStack() as stack:\n        # create a temporary file\n        with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as f:\n            # write the class definition to the file\n            cls_name, cls_def = _gen_instance_module(fields)\n            f.write(cls_def)\n            # import the class\n            newInstances = _import(f.name)\n            # add from_instances method\n            _add_instances_conversion_methods(newInstances)\n            # add to context manager\n            stack.enter_context(mock.patch(\"detectron2.structures.Instances\", newInstances))\n            # clear jit cache\n            _clear_jit_cache()\n            yield newInstances\n        # remove the temporary file\n        os.remove(f.name)"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    Args:\n        file_name (str): the path to the image file that needs to be read.\n        format (str): optional, specifies the desired output format for the image. It can be one of the supported image modes in PIL (PIL.Image.Image.mode), \"BGR\", or \"YUV-BT.601\". If not specified, the image's original format is used.\n\n    Returns:\n        np.ndarray: the image after being read, possibly oriented correctly, and converted to the specified format. The data type of the array is uint8 for 0-255 ranges in supported PIL modes or \"BGR\" and \"YUV-BT.601\", and float32 for other modes.\n    \"\"\"\n    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n    return image\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # Transform bounding box\n    if \"bbox\" in annotation:\n        annotation[\"bbox\"] = transforms.apply_box(\n            BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n        )\n        annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform segmentation\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            annotation[\"segmentation\"] = transforms.apply_coords(\n                np.asarray(annotation[\"segmentation\"], dtype=\"float64\")\n            )\n        elif isinstance(annotation[\"segmentation\"], dict):\n            # COCO RLE\n            annotation[\"segmentation\"] = mask_util.decode(annotation[\"segmentation\"])\n            annotation[\"segmentation\"] = transforms.apply_segmentation(\n                annotation[\"segmentation\"]\n            )\n        elif isinstance(annotation[\"segmentation\"], np.ndarray):\n            # mask array\n            annotation[\"segmentation\"] = transforms.apply_segmentation(\n                annotation[\"segmentation\"]\n            )\n        else:\n            raise ValueError(\n                \"Cannot convert segmentation of type '{}' to BitMasks!\"\n                \"Supported types are: polygons as list[list[float] or ndarray],\"\n                \" COCO-style RLE as a dict, or a binary segmentation mask \"\n                \" in a 2D numpy array of shape HxW.\".format(type(annotation[\"segmentation\"]))\n            )\n\n    # Transform keypoints\n    if \"keypoints\" in annotation:\n        annotation[\"keypoints\"] = transform_keypoint_annotations(\n            annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n        )\n\n    return annotation"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        coords = coords.astype(np.float32)\n        coords = np.dot(coords, self.rm_coords.T)\n        return coords\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if len(img) == 0 or self.angle % 360 == 0:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        # Apply the rotation transformation to the image using OpenCV's warpAffine function.\n        # The rotation matrix (rm_image) is used to transform the image coordinates, and the interpolation method (interp) is used to determine how the pixels are interpolated.\n        # The image is rotated around the center of the image (center) by the specified angle (angle).\n        # The bounding dimensions (bound_w, bound_h) are used to determine the size of the output image after rotation.\n        # The image is cropped to the specified dimensions (w, h) to ensure that the output image has the correct size.\n        # The image is converted to a float32 data type to ensure accurate interpolation.\n        # The image is then converted back to the original data type (uint8) and returned.\n        img = cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n        img = img[\n            (self.bound_h - self.h) // 2 : (self.bound_h + self.h) // 2,\n            (self.bound_w - self.w) // 2 : (self.bound_w + self.w) // 2,\n        ]\n        return img.astype(img.dtype)\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "s:\n            predictions (Instances): The output of an instance detection/segmentation/keypoint\n                prediction algorithm.\n        Returns:\n            output (VisImage): image object with visualizations.\n        \"\"\"\n        if self._instance_mode == ColorMode.IMAGE_BW:\n            self.output.reset_image(self._create_grayscale_image(predictions.pred_masks))\n\n        if self._instance_mode == ColorMode.SEGMENTATION:\n            self.overlay_instances(\n                labels=predictions.pred_classes, masks=predictions.pred_masks, alpha=0.7\n            )\n\n        if self._instance_mode == ColorMode.IMAGE_BW:\n            self.overlay_instances(\n                labels=predictions.pred_classes, masks=predictions.pred_masks, alpha=0.7\n            )\n\n        if self._instance_mode == ColorMode.IMAGE_COLOR:\n            self.overlay_instances(\n                labels=predictions.pred_classes,\n                masks=predictions.pred_masks,\n                keypoints=predictions.pred_keypoints,\n                assigned_colors=predictions.pred_classes_colors,\n            )\n\n        if self._instance_mode == ColorMode.SEGMENTATION:\n            self.overlay_instances(\n                labels=predictions.pred_classes,\n                masks=predictions.pred_masks,\n                keypoints=predictions.pred_keypoints,\n                assigned_colors=predictions.pred_classes_colors,\n            )\n\n        return self.output\n\n    def draw_instance_predictions(self, predictions):\n        \"\"\"\n        Draw instance-level prediction results on an image.\n\n        Args:\n            predictions (Instances): the output of an instance detection/segmentation/keypoint\n                prediction algorithm.\n        Returns:\n            output (VisImage): image object with visualizations.\n        \"\"\"\n        if self._instance_mode == ColorMode.IMAGE_BW:\n            self.output.reset_image(self._create_grayscale_image(predictions.pred_masks))\n\n        if self._instance_mode == ColorMode.SEGMENTATION:\n            self.overlay_instances(\n                labels=predictions.pred_classes, masks=predictions.pred_masks, alpha=0.7\n            )\n\n        if self._instance_mode == ColorMode.IMAGE_BW:\n            self.overlay_instances(\n                labels=predictions.pred_classes, masks=predictions.pred_masks, alpha=0.7\n            )"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "    \"\"\"\n        Returns:\n            ndarray. The visualized image in RGB format as a numpy ndarray of uint8 type.\n        \"\"\"\n        self.canvas.draw()\n        return np.asarray(self.canvas.tostring_rgb())\n\n    def reset(self, img):\n        \"\"\"\n        Args:\n            img: same as in __init__\n        \"\"\"\n        self.img = img\n        self.output = VisImage(img, self.scale)\n        self._setup_figure(img)\n\n    def _setup_figure(self, img):\n        \"\"\"\n        Args:\n            img: same as in __init__\n\n        Returns:\n            fig (matplotlib.pyplot.figure): top level container for the plot elements.\n            ax (matplotlib.pyplot.axes): container for the plot elements.\n        \"\"\"\n        self.output.reset_image(img)\n        self.canvas.print_figure(self.output.fig, dpi=self.output.dpi)\n        self.output.ax.set_axis_off()\n        self.output.ax.set_xlim(0, self.output.width)\n        self.output.ax.set_ylim(self.output.height, 0)\n        self.output.ax.set_aspect(\"equal\")\n        self.output.ax.set_position([0, 0, 1, 1])\n        return self.output.fig, self.output.ax\n\n    def _convert_boxes(self, boxes):\n        \"\"\"\n        Convert different format of boxes to an NxB array, where B = 4 or 5 is the box dimension.\n        \"\"\"\n        if isinstance(boxes, Boxes) or isinstance(boxes, RotatedBoxes):\n            return boxes.tensor.detach().numpy()\n        else:\n            return np.asarray(boxes)\n\n    def _convert_masks(self, masks_or_polygons):\n        \"\"\"\n        Convert different format of masks or polygons to a tuple of masks and polygons.\n\n        Returns:\n            list[GenericMask]:\n        \"\"\"\n\n        m = masks_or_polygons\n        if isinstance(m, PolygonMasks):\n            m = m.polygons\n        if isinstance(m, BitMasks):\n            m = m.tensor.numpy()\n        if isinstance(m, torch.Tensor):\n            m = m.numpy()\n        ret = []\n        for x in m:\n            if isinstance(x, GenericMask"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # Draw the image\n        self.output.reset_image(self.img)\n\n        # Draw the annotations\n        if \"annotations\" in dic:\n            if \"segmentation\" in dic[\"annotations\"]:\n                if \"segmentation\" in dic[\"annotations\"][\"segmentation\"]:\n                    if \"counts\" in dic[\"annotations\"][\"segmentation\"][\"counts\"]:\n                        # RLE\n                        rle = dic[\"annotations\"][\"segmentation\"][\"counts\"]\n                        mask = mask_util.decode(rle)\n                        self.draw_binary_mask(mask)\n                    else:\n                        # COCO-style\n                        mask = dic[\"annotations\"][\"segmentation\"]\n                        self.draw_binary_mask(mask)\n                else:\n                    # Polygon\n                    polygons = dic[\"annotations\"][\"segmentation\"]\n                    self.draw_polygon(polygons)\n            if \"keypoints\" in dic[\"annotations\"]:\n                keypoints = dic[\"annotations\"][\"keypoints\"]\n                self.draw_keypoints(keypoints)\n            if \"bbox\" in dic[\"annotations\"]:\n                bbox = dic[\"annotations\"][\"bbox\"]\n                self.draw_box(bbox)\n            if \"bbox_mode\" in dic[\"annotations\"]:\n                bbox_mode = dic[\"annotations\"][\"bbox_mode\"]\n                if bbox_mode == BoxMode.XYXY_ABS:\n                    self.draw_box(bbox)\n                elif bbox_mode == BoxMode.XYWH_ABS:\n                    self.draw_box(bbox)\n                elif bbox_mode == BoxMode.XYXY_REL:\n                    self.draw_box(bbox)\n                elif bbox_mode == BoxMode.XYWH_REL:\n                    self.draw_box(bbox)\n                elif bbox_mode == BoxMode.XYWHA_ABS:\n                    self.draw_rotated_box(bbox)\n                elif bbox_mode == BoxMode.XYWHA_REL:\n                    self.draw_rotated_box(bbox)\n                else:\n                    raise ValueError(f\"Unknown bbox_mode {bbox_mode}\")\n            if \"segmentation\" in dic[\"annotations\"]:\n                if \"counts\" in dic[\"annotations\"][\"segmentation\"]:\n                    # RLE\n                    rle = dic[\"annotations\"][\"segmentation\"][\"counts\"]\n                    mask = mask_util.decode(rle)\n                    self.draw_binary_mask(mask)\n                else:\n                    # COCO-style\n                    mask = dic[\"annotations\"][\"segmentation\"]\n                    self.draw_binary_mask(mask"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        shape2d = (binary_mask.shape[0], binary_mask.shape[1])\n        rgba = np.zeros(shape2d + (4,), dtype=\"float32\")\n        rgba[:, :, :3] = color\n        rgba[:, :, 3] = binary_mask * alpha\n        self.output.ax.imshow(rgba, extent=(0, self.output.width, self.output.height, 0))\n\n        if text is not None:\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            self._draw_text_in_mask(binary_mask, text, lighter_color)\n\n        if area_threshold is not None:\n            # find connected components\n            _num_cc, cc_labels, stats, centroids = cv2.connectedComponentsWithStats(\n                binary_mask, 8\n            )\n            if stats[1:, -1].size == 0:\n                return self.output\n            largest_component_id = np.argmax(stats[1:, -1]) + 1\n\n            # draw text on the largest component, as well as other very large components.\n            for cid in range(1, _num_cc):\n                if cid == largest_component_id or stats[cid, -1] > area_threshold:\n                    # median is more stable than centroid\n                    # center = centroids[largest_component_id]\n                    center = np.median((cc_labels == cid).nonzero(), axis=1)[::-1]\n                    self.draw_text(text, center, color=lighter_color)\n\n        return self.output"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    # Check if the image sizes are the same\n    if size_as_tensor:\n        assert input.image_size.equal(other.image_size), f\"{msg}Image sizes do not match: {input.image_size}!= {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg}Image sizes do not match: {input.image_size}!= {other.image_size}\"\n\n    # Check if the fields are the same\n    for name in input._field_names:\n        val1 = getattr(input, \"_\" + name, None)\n        val2 = getattr(other, \"_\" + name, None)\n        if val1 is None:\n            assert val2 is None, f\"{msg}Field {name} is missing in the first instance but present in the second\"\n        elif val2 is None:\n            assert val1 is None, f\"{msg}Field {name} is missing in the second instance but present in the first\"\n        else:\n            if isinstance(val1, Boxes):\n                assert_boxes_allclose(val1, val2, rtol=rtol, msg=msg)\n            elif isinstance(val1, ROIMasks):\n                assert_roimasks_allclose(val1, val2, rtol=rtol, msg=msg)\n            elif isinstance(val1, torch.Tensor):\n                assert_tensors_allclose(val1, val2, rtol=rtol, msg=msg)\n            else:\n                assert val1 == val2, f\"{msg}Field {name} does not match: {val1}!= {val2}\"\n\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.tensor[:, 2] * self.tensor[:, 3]\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        proposals = proposals[0]\n        gt_classes = proposals.gt_classes\n        gt_boxes = proposals.gt_boxes\n        proposal_boxes = proposals.proposal_boxes\n\n        # Classification loss\n        loss_cls = cross_entropy(\n            scores, gt_classes, reduction=\"mean\", ignore_index=self.num_classes\n        )\n\n        # Box regression loss\n        loss_box_reg = self.box_reg_loss(proposal_boxes, gt_boxes, proposal_deltas, gt_classes)\n\n        # Weighted sum of classification and box regression losses\n        loss_dict = {\n            \"loss_cls\": loss_cls * self.loss_weight[\"loss_cls\"],\n            \"loss_box_reg\": loss_box_reg * self.loss_weight[\"loss_box_reg\"],\n        }\n\n        # Log classification metrics to EventStorage\n        _log_classification_stats(scores, gt_classes, prefix=\"fast_rcnn\")\n\n        return loss_dict"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.NAME\n    tracker_class = TRACKER_HEADS_REGISTRY.get(tracker_name)\n    if tracker_class is None:\n        raise KeyError(\"Unknown tracker: {}\".format(tracker_name))\n    return tracker_class(cfg)"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[1] % 4 == 0 and boxes.shape[1] == 4\n\n        boxes = boxes.to(deltas.dtype).unsqueeze(2)\n\n        ctr_x = boxes[:, 0]\n        ctr_y = boxes[:, 1]\n        widths = boxes[:, 2]\n        heights = boxes[:, 3]\n\n        wx, wy, ww, wh = self.weights\n\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = dx * widths + ctr_x  # x_ctr\n        pred_boxes[:, 1::4] = dy * heights + ctr_y  # y_ctr\n        pred_boxes[:, 2::4] = torch.exp(dw) * widths  # width\n        pred_boxes[:, 3::4] = torch.exp(dh) * heights  # height\n\n        return pred_boxes"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            anno_type = self.anno_type_map.keys()\n        elif isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        output = self.general_ins(image)\n        output_dict = {}\n        for tp in anno_type:\n            output_dict[tp] = output[tp]\n        return output_dict"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = normalize_string(query)\n        keywords = query.split(\" \")\n        result = {}\n        for keyword in keywords:\n            result = update_url_scores(result, self.bm25(keyword))\n        return result"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize the angles to be within the range (-180, 180] degrees.\n        self.normalize_angles()\n\n        # Identify the indices of the boxes that are nearly horizontal based on the clip_angle_threshold.\n        clip_angle_threshold_rad = clip_angle_threshold * math.pi / 180.0\n        horizontal_boxes = torch.abs(self.tensor[:, 4]) < clip_angle_threshold_rad\n\n        # For these identified boxes, convert their representation from (center x, center y, width, height, angle) to (x1, y1, x2, y2), where (x1, y1) and (x2, y2) are the coordinates of the top-left and bottom-right corners, respectively.\n        x1 = self.tensor[:, 0] - self.tensor[:, 2] / 2.0\n        y1 = self.tensor[:, 1] - self.tensor[:, 3] / 2.0\n        x2 = self.tensor[:, 0] + self.tensor[:, 2] / 2.0\n        y2 = self.tensor[:, 1] + self.tensor[:, 3] / 2.0\n\n        # Clip the x and y coordinates to ensure they do not exceed the specified box_size limits.\n        x1 = torch.clamp(x1, min=0, max=box_size[1])\n        y1 = torch.clamp(y1, min=0, max=box_size[0])\n        x2 = torch.clamp(x2, min=0, max=box_size[1])\n        y2 = torch.clamp(y2, min=0, max=box_size[0])\n\n        # Convert the boxes back to their original representation, ensuring that any numerical errors do not increase their sizes.\n        self.tensor[:, 0] = (x1 + x2) / 2.0\n        self.tensor[:, 1] = (y1 + y2) / 2.0\n        self.tensor[:, 2] = x2 - x1\n        self.tensor[:, 3] = y2 - y1\n\n        # Restrict the angles to the range (-180, 180) degrees.\n        self.normalize_angles()\n\n       "}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        stats = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n        for item in self.data:\n            stats[item['type']] += 1\n        return stats"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMSEG_LOSSES.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMSEG_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    # Initialize the dictionary to store the evaluation results\n    eval_results = {}\n\n    # Initialize the dictionary to store the class-wise AP and AR for each IoU threshold\n    class_wise_results = {}\n\n    # Initialize the dictionary to store the overall mAP and mAR\n    overall_results = {}\n\n    # Loop over the IoU thresholds\n    for iou_threshold in metric:\n        # Initialize the dictionary to store the class-wise AP and AR for the current IoU threshold\n        class_wise_results[iou_threshold] = {}\n\n        # Loop over the classes\n        for class_id in label2cat:\n            # Initialize the dictionary to store the AP and AR for the current class and IoU threshold\n            class_wise_results[iou_threshold][class_id] = {}\n\n            # Filter the ground truth annotations for the current class\n            gt_class_annos = [anno for anno in gt_annos if anno['name'] == label2cat[class_id]]\n\n            # Filter the detection annotations for the current class\n            dt_class_annos = [anno for anno in dt_annos if anno['name'] == label2cat[class_id]]\n\n            # Compute the average precision and recall for the current class and IoU threshold\n            ap, ar = compute_ap_and_ar(gt_class_annos, dt_class_annos, iou_threshold, box_type_3d, box_mode_3d)\n\n            # Store the AP and AR for the current class and IoU threshold in the dictionary\n            class_wise_results[iou_threshold][class_id]['AP'] = ap\n            class_wise_results[iou_threshold][class_id]['AR'] = ar\n\n        # Compute the overall mAP and mAR for the current IoU threshold\n        overall_results[iou_threshold] = compute_overall_map_and_ar(class_wise_results[iou_threshold], metric)\n\n    # Store the evaluation results in the dictionary\n    eval_results['class_wise_results'] = class_wise_results\n    eval_results['overall_results'] = overall_results\n\n    # Print the evaluation results\n    if logger is not None:\n        print_log(eval_results, logger=logger)\n\n    # Return the evaluation results\n    return eval_results\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type.upper() == 'LIDAR':\n        return LidarInstance3DBoxes, 'LIDAR'\n    elif box_type.upper() == 'CAMERA':\n        return CameraInstance3DBoxes, 'CAMERA'\n    elif box_type.upper() == 'DEPTH':\n        return DepthInstance3DBoxes, 'DEPTH'\n    else:\n        raise ValueError(f'Unknown box type {box_type}.')\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "/ollama/client.py\nimport io\nimport json\nimport os\nimport binascii\nimport platform\nimport urllib.parse\nfrom hashlib import sha256\nfrom base64 import b64encode, b64decode\nfrom typing import Any, AnyStr, Optional, Sequence, Mapping, Literal, Union\n\nimport sys\n\nif sys.version_info < (3, 9):\n  from typing import Iterator, AsyncIterator\nelse:\n  from collections.abc import Iterator, AsyncIterator\n\nfrom pathlib import Path\nfrom httpx import Client as httpx_Client\n\nfrom ollama._types import Message, Options, RequestError, ResponseError\n\n\ndef _as_path(s: Optional[Union[str, PathLike]]) -> Union[Path, None]:\n  if isinstance(s, str) or isinstance(s, Path):\n    try:\n      if (p := Path(s)).exists():\n        return p\n    except Exception:\n      ...\n  return None\n\n\ndef _as_bytesio(s: Any) -> Union[io.BytesIO, None]:\n  if isinstance(s, io.BytesIO):\n    return s\n  elif isinstance(s, bytes):\n    return io.BytesIO(s)\n  return None\n\n\ndef _encode_image(image) -> str:\n  \"\"\"\n  >>> _encode_image(b'ollama')\n  'b2xsYW1h'\n  >>> _encode_image(io.BytesIO(b'ollama'))\n  'b2xsYW1h'\n  >>> _encode_image('LICENSE')\n  'TUlUIExpY2Vuc2UKCkNvcHlyaWdodCAoYykgT2xsYW1hCgpQZXJtaXNzaW9uIGlzIGhlcmVieSBncmFudGVkLCBmcmVlIG9mIGNoYXJnZSwgdG8gYW55IHBlcnNvbiBvYnRhaW5pbmcgYSBjb3B5Cm9mIHRoaXMgc29mdHdhcmUgYW5kIGFzc29jaWF0ZWQgZG9jdW1lbnRhdGlvbiBmaWxlcyAodGhlICJTb2Z0d2FyZSIpLCB0byBkZWFsCmluIHR"}
{"namespace": "ollama._client.Client.pull", "completion": "/ollama/client.py\nimport io\nimport os\nimport re\nimport sys\nimport time\nimport json\nimport httpx\nimport binascii\nimport platform\nimport urllib.parse\nfrom os import PathLike\nfrom pathlib import Path\nfrom hashlib import sha256\nfrom base64 import b64encode, b64decode\n\nfrom typing import Any, AnyStr, Union, Optional, Sequence, Mapping, Literal, Iterator\n\nfrom ollama._types import Message, Options, RequestError, ResponseError\n\n\nclass BaseClient:\n  def __init__(\n    self,\n    client,\n    host: Optional[str] = None,\n    follow_redirects: bool = True,\n    timeout: Any = None,\n    **kwargs,\n  ) -> None:\n    \"\"\"\n    Creates a httpx client. Default parameters are the same as those defined in httpx except for the following:\n    - `follow_redirects`: True\n    - `timeout`: None\n    `kwargs` are passed to the httpx client.\n    \"\"\"\n\n    headers = kwargs.pop('headers', {})\n    headers['Content-Type'] = 'application/json'\n    headers['Accept'] = 'application/json'\n    headers['User-Agent'] = f'ollama-python/{__version__} ({platform.machine()} {platform.system().lower()}) Python/{platform.python_version()}'\n\n    self._client = client(\n      base_url=_parse_host(host or os.getenv('OLLAMA_HOST')),\n      follow_redirects=follow_redirects,\n      timeout=timeout,\n      headers=headers,\n      **kwargs,\n    )\n\n  def _request(self, method: str, url: str, **kwargs) -> httpx.Response:\n    response = self._client.request(method, url, **kwargs)\n\n    try:\n      response.raise_for_status()\n    except httpx.HTTPStatusError as e:\n      raise ResponseError(e.response.text, e.response.status_code) from None\n\n    return response\n\n  def _stream(self, method: str, url: str, **kwargs) -> Iterator[Mapping[str, Any]]:\n    with self._client.stream(method, url, **kwargs) as r:\n      try:\n        r.raise_for_status()\n      except httpx.HTTPStatusError as"}
{"namespace": "ollama._client.Client.generate", "completion": "import io\nimport json\nimport os\nimport sys\nimport urllib.parse\nfrom base64 import b64decode, b64encode\nfrom hashlib import sha256\nfrom httpx import Response\nfrom pathlib import Path\nfrom typing import Any, AnyStr, Optional, Sequence, Union, Literal, Mapping, Iterator\n\nimport sys\n\nif sys.version_info < (3, 9):\n  from typing import Iterator, AsyncIterator\nelse:\n  from collections.abc import Iterator, AsyncIterator\n\nfrom importlib import metadata\n\nfrom ollama._types import Message, Options, RequestError, ResponseError\n\n\n__version__ = '0.0.0'\n\nif sys.version_info < (3, 9):\n  from typing import Literal\nelse:\n  from typing import Literal as Literal\n\n\nclass Client:\n  def __init__(self, host: Optional[str] = None, **kwargs) -> None:\n    \"\"\"\n    Creates a httpx client. Default parameters are the same as those defined in httpx except for the following:\n    - `follow_redirects`: True\n    - `timeout`: None\n    `kwargs` are passed to the httpx client.\n    \"\"\"\n\n    headers = kwargs.pop('headers', {})\n    headers['Content-Type'] = 'application/json'\n    headers['Accept'] = 'application/json'\n    headers['User-Agent'] = f'ollama-python/{__version__} ({platform.machine()} {platform.system().lower()}) Python/{platform.python_version()}'\n\n    self._client = httpx.Client(\n      base_url=_parse_host(host or os.getenv('OLLAMA_HOST')),\n      follow_redirects=True,\n      timeout=None,\n      headers=headers,\n      **kwargs,\n    )\n\n  def _request(self, method: str, url: str, **kwargs) -> httpx.Response:\n    response = self._client.request(method, url, **kwargs)\n\n    try:\n      response.raise_for_status()\n    except httpx.HTTPStatusError as e:\n      raise ResponseError(e.response.text, e.response.status_code) from None\n\n    return response\n\n  def _stream(self, method: str, url: str, **kwargs) -> Iterator[Mapping[str, Any"}
{"namespace": "ollama._client.Client.push", "completion": "  def push(\n    self,\n    model: str,\n    insecure: bool = False,\n    stream: bool = False,\n  ) -> Union[Mapping[str, Any], Iterator[Mapping[str, Any]]]:\n    \"\"\"\n    Raises `ResponseError` if the request could not be fulfilled.\n\n    Returns `ProgressResponse` if `stream` is `False`, otherwise returns a `ProgressResponse` generator.\n    \"\"\"\n    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n       'stream': stream,\n      },\n      stream=stream,\n    )\n\n  async def push(\n    self,\n    model: str,\n    insecure: bool = False,\n    stream: bool = False,\n  ) -> Union[Mapping[str, Any], AsyncIterator[Mapping[str, Any]]]:\n    \"\"\"\n    Raises `ResponseError` if the request could not be fulfilled.\n\n    Returns `ProgressResponse` if `stream` is `False`, otherwise returns a `ProgressResponse` generator.\n    \"\"\"\n    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n       'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n       'modelfile': modelfile,\n       'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "   \"\"\"\n    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code!= 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": ">\n    Input-Output Arguments\n    :param model: str, default ''. The model identifier used for generating the response. It is required and an error is raised if not provided.\n    :param prompt: str, default ''. The prompt or input text for the model to process.\n    :param system: str, default ''. An identifier for the system or environment where the model is run.\n    :param template: str, default ''. A template identifier to format the model's response.\n    :param context: Optional[Sequence[int]], default None. A sequence of integers representing the context or additional data for the model.\n    :param stream: bool, default False. Determines if the response should be streamed asynchronously. If True, the function returns an asynchronous generator.\n    :param raw: bool, default False. Indicates if the response should be returned in raw format.\n    :param format: Literal['', 'json'], default ''. Specifies the format of the response; can be empty or 'json'.\n    :param images: Optional[Sequence[AnyStr]], default None. A sequence of images to be included in the request, encoded as strings.\n    :param options: Optional[Options], default None. Additional options for the request.\n    :param keep_alive: Optional[Union[float, str]], default None. Specifies the keep-alive parameter for the connection.\n    :return: Union[Mapping[str, Any], AsyncIterator[Mapping[str, Any]]]. If 'stream' is False, returns a single response object as a dictionary. If 'stream' is True, returns an asynchronous `ChatResponse` generator.\n    \"\"\"\n    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise RequestError('messages must be a list of strings')\n      if 'role' not in message or not message['role'] in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n       'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if not messages:\n      messages = []\n\n    if not isinstance(messages, list):\n      raise RequestError('messages must be a list')\n\n    for message in messages:\n      if not isinstance(message, dict):\n        raise RequestError('messages must be a list of dict-like objects')\n\n      if not (role := message.get('role')) or not isinstance(role, str):\n        raise RequestError('messages must be a list of dict-like objects with a role')\n\n      if not (content := message.get('content')) or not isinstance(content, str):\n        raise RequestError('messages must be a list of dict-like objects with a content')\n\n      if not (images := message.get('images', [])) or not isinstance(images, list):\n        raise RequestError('messages must be a list of dict-like objects with a list of images')\n\n      for image in images:\n        if not isinstance(image, str):\n          raise RequestError('messages must be a list of dict-like objects with a list of images')\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n       'model': model,\n       'messages': messages,\n       'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n       'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code!= 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary directory to store the user code and test code\n        with tempfile.TemporaryDirectory() as temp_dir:\n            # Create a file to store the user code\n            user_code_file = Path(temp_dir) / \"user_code.py\"\n            user_code_file.write_text(user_code, encoding=\"utf-8\")\n\n            # Create a file to store the test code\n            test_code_file = Path(temp_dir) / \"test_code.py\"\n            test_code_file.write_text(test_code, encoding=\"utf-8\")\n\n            # Run Pyright to type check the user code and test code\n            pyright_command = [\"pyright\", \"--no-config\", \"--output-format\", \"json\", user_code_file, test_code_file]\n            pyright_output = subprocess.run(pyright_command, capture_output=True, text=True)\n\n            # Parse the output of Pyright\n            pyright_output_lines = pyright_output.stdout.splitlines()\n            error_lines = []\n            for line in pyright_output_lines:\n                match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n                if match:\n                    error_lines.append(match.group(1))\n\n            # Check if there are any expected type errors\n            expected_error_lines = [line for line in error_lines if cls.EXPECT_ERROR_COMMENT in line]\n            if expected_error_lines:\n                # If there are expected type errors, return a TypeCheckResult object with a message indicating the failure\n                message = f\"Expected type errors on lines {', '.join(expected_error_lines)}\"\n                return TypeCheckResult(message=message, passed=False, debug_info={\"error_lines\": error_lines})\n\n            # If there are no expected type errors, return a TypeCheckResult object with a message indicating the success\n            message = \"Type check passed\"\n            return TypeCheckResult(message=message, passed=True, debug_info={\"error_lines\": error_lines})\n\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if not path and not modelfile:\n      raise RequestError('must provide path or modelfile')\n\n    if path:\n      if (realpath := _as_path(path)) and realpath.exists():\n        modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n      elif modelfile:\n        modelfile = self._parse_modelfile(modelfile)\n      else:\n        raise RequestError('must provide path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n       'model': model,\n       'modelfile': modelfile,\n       'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, get_compiler_fn(\"AOT Module\"))\n    else:\n        return aot_function(fn, get_compiler_fn(\"AOT Function\"))"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    config_dict = yaml.safe_load(open(os.path.join(trial_path, 'config.yaml')))\n    config_dict = summary_df_to_yaml(summary_df, config_dict)\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError(\"The output_path must end with.yaml or.yml\")\n        with open(output_path, 'w') as f:\n            yaml.dump(config_dict, f)\n    return config_dict\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    if not callable(func):\n        raise TypeError(f'func must be callable, but got {type(func)}')\n\n    if not isinstance(func, torch.nn.Module):\n        func = to_module(func)\n\n    if not hasattr(func, 'forward'):\n        raise TypeError(f'func must have a forward method, but got {func}')\n\n    if not callable(func.forward):\n        raise TypeError(f'func.forward must be callable, but got {type(func.forward)}')\n\n    if not hasattr(func, 'training'):\n        raise TypeError(f'func must have a training attribute, but got {func}')\n\n    if not callable(func.training):\n        raise TypeError(f'func.training must be callable, but got {type(func.training)}')\n\n    if not hasattr(func, '__name__'):\n        raise TypeError(f'func must have a __name__ attribute, but got {func}')\n\n    if not callable(func.__name__):\n        raise TypeError(f'func.__name__ must be callable, but got {type(func.__name__)}')\n\n    if not hasattr(func, '__self__'):\n        raise TypeError(f'func must have a __self__ attribute, but got {func}')\n\n    if not callable(func.__self__):\n        raise TypeError(f'func.__self__ must be callable, but got {type(func.__self__)}')\n\n    if not hasattr(func, '__qualname__'):\n        raise TypeError(f'func must have a __qualname__ attribute, but got {func}')\n\n    if not callable(func.__qualname__):\n        raise TypeError(f'func.__qualname__ must be callable, but got {type(func.__qualname__)}')\n\n    if not hasattr(func, '__module__'):\n        raise TypeError(f'func must have a __module__ attribute, but got {func}')\n\n    if not callable(func.__module__):\n        raise TypeError(f'func.__module__ must be callable, but got {type(func.__module__)}')\n\n    if not hasattr(func, '__doc__'):\n        raise TypeError(f'func must have a __doc__ attribute, but got {func}')\n\n    if not callable(func.__doc__):\n        raise TypeError(f'func.__doc__ must be callable, but got {type(func.__doc"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        return cls(config, project_dir=os.path.dirname(trial_path))"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create a directory for this node line\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Run each module with the given parameters\n    results = []\n    for module, module_params in zip(modules, module_params):\n        result = module(previous_result, **module_params)\n        results.append(result)\n\n    # Measure the execution time of each module\n    execution_times = measure_speed(modules, module_params)\n\n    # Evaluate each module's result\n    evaluation_results = []\n    for result, module, module_params, execution_time in zip(results, modules, module_params, execution_times):\n        evaluation_result = evaluate_retrieval_node(result, previous_result, strategies['metrics'])\n        evaluation_result['module_name'] = module.__name__\n        evaluation_result['module_params'] = module_params\n        evaluation_result['execution_time'] = execution_time\n        evaluation_results.append(evaluation_result)\n\n    # Select the best result\n    best_result = select_best_average(evaluation_results, strategies['metrics'])\n\n    # Save the results and summaries to disk\n    best_result.to_parquet(os.path.join(node_line_dir, \"result.parquet\"))\n    pd.DataFrame(execution_times, columns=['module_name', 'execution_time']).to_csv(os.path.join(node_line_dir, \"execution_times.csv\"))\n    pd.DataFrame(evaluation_results).to_csv(os.path.join(node_line_dir, \"evaluation_results.csv\"))\n    pd.DataFrame(execution_times, columns=['module_name', 'execution_time']).to_csv(os.path.join(node_line_dir, \"execution_times.csv\"))\n    pd.DataFrame(evaluation_results).to_csv(os.path.join(node_line_dir, \"evaluation_results.csv\"))\n\n    # Filter the results by the speed threshold\n    filtered_results = filter_by_threshold(execution_times, strategies['speed_threshold'])\n\n    # Save the filtered results and summaries to disk\n    pd.DataFrame(filtered_results, columns=['module_name', 'execution_time']).to_csv(os.path."}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # make all possible combinations of modules and parameters\n    module_combinations = make_combinations(modules, module_params)\n\n    # run all combinations\n    results = list(map(lambda x: x[0](previous_result=previous_result, **x[1]), module_combinations))\n\n    # evaluate all combinations\n    evaluation_results = list(map(lambda x: evaluate_retrieval_node(x, strategies['metrics']), results))\n\n    # select the best result\n    best_result, _ = select_best_average(evaluation_results, strategies['metrics'])\n\n    # save the results\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'), index=False)\n    pd.DataFrame(evaluation_results).to_csv(os.path.join(node_line_dir, 'evaluation_results.csv'), index=False)\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create the node's output directory\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Run the prompt maker modules\n    prompt_maker_results = list(map(lambda x: x[0](previous_result=previous_result, **x[1]),\n                                    zip(modules, module_params)))\n\n    # Evaluate the prompt maker modules\n    evaluation_results = list(map(lambda x: evaluate_prompt_maker_result(x[0], strategies['metrics']),\n                                  zip(prompt_maker_results, modules)))\n\n    # Select the best prompt maker module based on the specified strategies\n    best_result, _ = select_best_average(evaluation_results, strategies['metrics'])\n\n    # Save the results and a summary to the node's output directory\n    best_result.to_csv(os.path.join(node_line_dir, 'best_result.csv'), index=False)\n    summary = pd.DataFrame(strategies)\n    summary.to_csv(os.path.join(node_line_dir,'summary.csv'), index=False)\n\n    # Combine the results from the previous operation and the best prompt maker's output\n    combined_result = pd.concat([previous_result, best_result], axis=1)\n\n    return combined_result\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model('all-mpnet-base-v2')\n\n    gt_embeddings = embedding_model.embed_documents(generation_gt)\n    pred_embedding = embedding_model.embed_query(pred)\n    return max(list(map(lambda x: calculate_cosine_similarity(x, pred_embedding), gt_embeddings)))"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer not set up\")\n        return np_image\n\n    try:\n        return gfpgan_face_restorer.restore(np_image)\n    except Exception:\n        errors.report(\"Error restoring faces with GFPGAN\", exc_info=True)\n        return np_image"}
{"namespace": "codeformer_model.setup_model", "completion": "    global codeformer\n    try:\n        codeformer = FaceRestorerCodeFormer(dirname)\n        logger.info(f\"Initialized CodeFormer face restorer with model from {dirname}\")\n    except errors.ModelLoadingError as e:\n        logger.error(f\"Failed to initialize CodeFormer face restorer with model from {dirname}: {e}\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n\n    try:\n        # Patch the facexlib with the given directory\n        shared.patch_facexlib(dirname)\n\n        # Initialize the GFPGAN face restorer with the model located in the specified directory\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n\n    except Exception as e:\n        # Report any exceptions that occur during the setup process\n        logger.error(f\"Error occurred while setting up GFPGAN face restorer: {e}\")\n        raise errors.FaceRestorerError(f\"Error occurred while setting up GFPGAN face restorer: {e}\") from e"}
{"namespace": "quaternion.rotate", "completion": "  \"\"\"\n  Rotates a vector in 3D space using a quaternion to represent the rotation. The function performs the rotation by converting the vector into a quaternion format, applying the rotation, and then converting it back.\n\n  Input-Output Arguments\n  :param q: Array-like. The quaternion representing the rotation, used to rotate the vector.\n  :param v: Array-like. The vector to be rotated, represented as an array or similar data structure.\n  :return: Array-like. The rotated vector, after applying the quaternion rotation.\n  \"\"\"\n\n  # Convert the vector to a quaternion\n  v_quat = jnp.concatenate([v, jnp.zeros_like(v)], axis=-1)\n\n  # Multiply the quaternion by the vector\n  rotated_quat = multiply(q, v_quat)\n\n  # Convert the rotated quaternion back to a vector\n  rotated_vector = im(rotated_quat)\n\n  return rotated_vector\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.asarray(axis_angle)\n  axis = axis_angle / linalg.norm(axis_angle, axis=-1, keepdims=True)\n  angle = linalg.norm(axis_angle, axis=-1, keepdims=True)\n\n  # Ensure that the angle is in the range [-pi, pi].\n  angle = jnp.where(angle > jnp.pi, angle - 2.0 * jnp.pi, angle)\n  angle = jnp.where(angle < -jnp.pi, angle + 2.0 * jnp.pi, angle)\n\n  # Compute the sine and cosine of the angle.\n  sin_angle = jnp.sin(angle / 2.0)\n  cos_angle = jnp.cos(angle / 2.0)\n\n  # Compute the quaternion.\n  return jnp.concatenate(\n      [axis * sin_angle, cos_angle], axis=-1\n  )"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # check if idx is the argmax\n    if model.argmax(prefix) == idx:\n        return 0, k\n\n    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias)!= idx:\n        logit_bias[idx] *= 2\n        k += 1\n    high = logit_bias[idx]\n\n    # improve estimate\n    mid = (high + 1) / 2\n    while high >= 1 + eps:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + 1) / 2\n        k += 1\n    return -mid, k"}
{"namespace": "resample.resample_3d", "completion": "  if method == 'TRILINEAR':\n    return trilinear_resample_3d(\n        data,\n        locations,\n        edge_behavior=edge_behavior,\n        constant_values=constant_values,\n        coordinate_order=coordinate_order,\n        half_pixel_center=half_pixel_center)\n  elif method == 'NEAREST':\n    return nearest_neighbor_resample_3d(\n        data,\n        locations,\n        edge_behavior=edge_behavior,\n        constant_values=constant_values,\n        coordinate_order=coordinate_order,\n        half_pixel_center=half_pixel_center)\n  else:\n    raise ValueError(f'Invalid method: {method}')\n\n"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(x, jnp.float32(0)))\n"}
{"namespace": "math.minus_eps", "completion": "  \"\"\"\n  if jnp.abs(x) < tiny_val:\n    return tiny_val\n  else:\n    return jnp.nextafter(jnp.float32(x), jnp.float32(0))\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, _, x_dot: jnp.exp(x) * x_dot,\n      (min_val, np.nextafter(np.log1p(max_val), np.float32(0))),\n  )(x)\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / jnp.maximum(tiny_val, x),\n      (0, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  @jax.custom_jvp\n  def safe_sqrt(x):\n    \"\"\"\n    This function creates a safe version of the square root function that can handle edge cases more gracefully. It uses a helper function to generate this safe square root function, which includes a custom gradient for backpropagation in automatic differentiation contexts. The safe version ensures that the input is clamped between 0 and a maximum value to avoid invalid inputs like negative numbers.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe square root will be computed. It is used as the argument for the square root function and its custom gradient computation.\n    :return: The result of applying the safe square root function to the input x. This includes the application of a custom gradient function for automatic differentiation purposes.\n    \"\"\"\n    return jnp.sqrt(jnp.clip(x, 0, max_val))\n\n  @safe_sqrt.defjvp\n  def safe_sqrt_jvp(primals, tangents):\n    \"\"\"\n    This function creates a safe version of the square root function that can handle edge cases more gracefully. It uses a helper function to generate this safe square root function, which includes a custom gradient for backpropagation in automatic differentiation contexts. The safe version ensures that the input is clamped between 0 and a maximum value to avoid invalid inputs like negative numbers.\n\n    Input-Output Arguments\n    :param x: The input value for which the safe square root will be computed. It is used as the argument for the square root function and its custom gradient computation.\n    :return: The result of applying the safe square root function to the input x. This includes the application of a custom gradient function for automatic differentiation purposes.\n    \"\"\"\n    (x,), (x_dot,) = primals\n    y = safe_sqrt(x)\n    y_dot = x_dot / (2 * y)\n    return y, y_dot\n\n  return safe_sqrt\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return xp\n  if p == 0:\n    return safe_log1p(xp)\n  if p == -jnp.inf:\n    return -safe_expm1(-xp)\n  if p == jnp.inf:\n    return safe_expm1(xp)\n  return jnp.abs(p - 1) * ((safe_div(p, jnp.abs(p - 1)) * xp + 1) ** p - 1)"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [-1, -1, -1]])\n    base_faces = np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array(\n        [[0.5257311121191336, 0.0, 0.8506508083520399],\n         [0.0, 0.8506508083520399, 0.5257311121191336],\n         [-0.5257311121191336, 0.0, 0.8506508083520399],\n         [0.5257311121191336, 0.0, -0.8506508083520399],\n         [0.0, 0.8506508083520399, -0.5257311121191336],\n         [-0.5257311121191336, 0.0, -0.8506508083520399],\n         [0.5257311121191336, -0.8506508083520399, 0.0],\n         [-0.5257311121191336, -0.8506508083520399, 0.0],\n         [0.0, -0.8506"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + x),\n      (min_val, np.nextafter(np.log1p(max_val), np.float32(0))),\n  )(x)"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  if postmult is not None:\n    x *= postmult\n\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x_max = minus_eps(power_ladder_max_output(p))\n  x = override_gradient(jnp.clip(x, -x_max, x_max), x)  # Clip val, not grad.\n  y = select(\n      [\n          (p == 1, x),\n          (p == 0, safe_expm1(x)),\n          (p == -jnp.inf, -safe_log1p(-x)),\n          (p == jnp.inf, safe_log1p(x)),\n      ],\n      jnp.abs(p_safe - 1)\n      * (\n          ((safe_div(p_safe, jnp.abs(p_safe - 1)) * x + 1)) ** (1 / p_safe) - 1\n      ),\n  )\n  return y"}
{"namespace": "math.inv_power_ladder", "completion": "  \"\"\"\n  Performs the inverse operation of a power ladder transformation on the input `y` using a specified power `p`, with optional pre-multiplication and post-multiplication adjustments. It applies various mathematical operations based on the value of `p` to compute the inverse transformation.\n\n  Input-Output Arguments\n  :param y: The input value(s) to be transformed. It is used as the base for the inverse power ladder operation.\n  :param p: The power value used in the transformation. It determines the specific operation to be applied to `y`.\n  :param premult: Optional. A scalar value to pre-multiply the result after the inverse transformation is applied. It is used to adjust the scale of the output before the transformation.\n  :param postmult: Optional. A scalar value to post-multiply the input `y` before the inverse transformation is applied. It is used to adjust the scale of the input before the transformation.\n  :return: The result of the inverse power ladder transformation applied to `y`, adjusted by optional pre-multiplication and post-multiplication factors.\n  \"\"\"\n\n  if premult is not None:\n    y = y * premult\n  if postmult is not None:\n    y = y * postmult\n  if p == 1:\n    return y\n  elif p == 0:\n    return jnp.exp(y) - 1\n  elif p == -jnp.inf:\n    return jnp.expm1(-y)\n  elif p == jnp.inf:\n    return jnp.expm1(y)\n  else:\n    return jnp.sign(y) * jnp.abs(p - 1) / p * ((jnp.abs(y) / jnp.abs(p - 1) + 1)**p - 1)\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # If a delay is specified, the initial learning rate is scaled down by a multiplier and gradually returns to the normal rate after the delay period.\n    if step < lr_delay_steps:\n      lr = lr_init * lr_delay_mult\n    else:\n      lr = lr_init * lr_delay_mult + (lr_final - lr_init) * (step - lr_delay_steps) / (max_steps - lr_delay_steps)\n  else:\n    # If no delay is specified, the learning rate is linearly interpolated between the initial and final learning rates over the specified number of steps.\n    lr = lr_init + (lr_final - lr_init) * step / max_steps\n\n  return lr\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=random.PRNGKey(0),\n      n=1,\n      origin_lo=0.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=0.0,\n      far_hi=1.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )\n\n"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  \"\"\"\n  # Convert to camera coordinates.\n  points_cam = matmul(points, pixtocams)\n\n  # Correct for distortion.\n  if distortion_params is not None:\n    points_cam = _radial_and_tangential_undistort(\n        points_cam,\n        **distortion_params,\n        xnp=xnp,\n    )\n\n  # Convert to pixel coordinates.\n  points_pix = matmul(points_cam, camtoworlds)\n\n  # Convert to image coordinates.\n  points_pix = points_pix[:, :2] / points_pix[:, 2:3]\n\n  # Convert to depth values.\n  points_depth = points_cam[:, 2:3]\n\n  return points_pix, points_depth\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  \"\"\"\n  This function computes the exponential map from the Lie algebra se3 to the Lie group SE3, which is used to represent the motion in 3D space. It calculates the homogeneous transformation matrix that represents the motion of a body given a screw axis and a small motion magnitude.\n\n  Input-Output Arguments\n  :param screw_axis: A 6-vector (numpy array). It encodes a screw axis of motion, which can be divided into [w, v] where w is an angle-axis rotation and v represents a translation. The magnitude of w corresponds to the magnitude of motion.\n  :param eps: Float. An epsilon value for numerical stability, used to avoid division by zero or other numerical issues. It defaults to the smallest positive representable number in float32.\n  :return: A (4, 4) numpy array. The homogeneous transformation matrix that represents the motion of a body for one second about the screw axis S with magnitude theta.\n\n  \"\"\"\n\n  theta_squared = jnp.sum(screw_axis**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  a_X_b_taylor = jnp.eye(4) + screw_axis\n\n  # Prevent bad gradients from propagating back when theta is small.\n  screw_axis_safe = jnp.where(theta_squared > eps**2, screw_axis, 0.0)\n  theta_safe = jnp.where(theta_squared > eps**2, theta, 1.0)\n  w = screw_axis_safe / theta_safe\n  W = skew(w)\n\n  a_X_b1 = jnp.eye(4)\n  a_X_b2 = jnp.eye(4)\n  a_X_b3 = jnp.eye(4)\n  a_X_b4 = jnp.eye(4)\n  a_X_b5 = jnp.eye(4)\n  a_X_b6 = jnp.eye(4)\n\n  a_X_b1 = a_X_b1.at[:3, :3].set(exp_so3(w, eps))\n  a_X_b"}
{"namespace": "rigid_body.exp_so3", "completion": "  axis_angle = jnp.reshape(axis_angle, (3,))\n  theta_squared = jnp.sum(axis_angle**2, axis=-1)\n  theta = _safe_sqrt(theta_squared)\n  W = skew(axis_angle / theta)\n  # Note that p = 0 when theta = 0.\n  p = spin_math.matmul(\n      (\n          theta * jnp.eye(3)\n          + (1.0 - jnp.cos(theta)) * W\n          + (theta - jnp.sin(theta)) * spin_math.matmul(W, W)\n      ),\n      axis_angle / theta,\n  )\n  # If theta^2 is close to 0 it means this is a pure rotation so p = 0.\n  p = jnp.where(theta_squared > eps**2, p, jnp.zeros_like(p))\n  return rp_to_se3(quat_lib.from_axis_angle(axis_angle, eps), p)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # The axis of the cone is normalized.\n  d = d / jnp.linalg.norm(d)\n\n  # The mean of the Gaussian is the origin.\n  mean = jnp.zeros_like(d)\n\n  # The covariance of the Gaussian is calculated using the starting and ending distances of the conical frustum, the scale of the radius as a function of distance, and whether the Gaussian should have a diagonal or full-covariance.\n  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  t_mean = t_mean[Ellipsis, None]\n  t_var = t_var[Ellipsis, None]\n  r_var = r_var[Ellipsis, None]\n\n  # The covariance of the Gaussian is calculated using the starting and ending distances of the conical frustum, the scale of the radius as a function of distance, and whether the Gaussian should have a diagonal or full-covariance.\n  if diag:\n    cov = t_var * d**2 + r_var\n  else:\n    cov = t_var * d[Ellipsis, :, None] * d[Ellipsis, None, :] + r_var * jnp.eye(3)\n\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  # Calculate the mean and covariance of the Gaussian distribution.\n  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  \"\"\"\n  # Compute ray origins and directions.\n  origins = mat_vec_mul(pixtocams, xnp.stack([pix_x_int, pix_y_int, xnp.ones_like(pix_x_int)], axis=-1))\n  directions = mat_vec_mul(pixtocams, xnp.stack([xnp.zeros_like(pix_x_int), xnp.zeros_like(pix_y_int), -xnp.ones_like(pix_x_int)], axis=-1))\n\n  # Compute ray view directions.\n  viewdirs = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Compute ray differential radii.\n  radii = xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Compute ray imageplane coordinates.\n  if camtype == ProjectionType.PERSPECTIVE:\n    imageplane = origins / -directions[..., -1:]\n  elif camtype == ProjectionType.FISHEYE:\n    imageplane = origins / -directions[..., -1:]\n    imageplane = mat_vec_mul(pixtocam_ndc, imageplane)\n  elif camtype == ProjectionType.PANORAMIC:\n    imageplane = origins / -directions[..., -1:]\n    imageplane = mat_vec_mul(pixtocam_ndc, imageplane)\n  else:\n    raise ValueError(f'Unknown camera type {camtype}.')\n\n  # Apply distortion correction.\n  if distortion_params is not None:\n    x, y = _radial_and_tangential_distort(\n        imageplane[Ellipsis, 0],\n        imageplane[Ellipsis, 1],\n        **distortion_params,\n    )\n    imageplane = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  # Apply inverse intrinsics.\n  if pixtocam_ndc is not None:\n    imageplane = mat_vec_mul(pixtocam_ndc, imageplane)\n\n  return origins, directions, viewdirs, radii, imageplane\n\n"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Compute the product of density and the adjusted distance between points.\n  density_delta = density * tdist\n\n  # Call the helper function to compute the alpha weights.\n  weights = compute_alpha_weights_helper(density_delta)\n\n  return weights\n"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 1:\n    raise ValueError(f'num_samples must be > 1, is {num_samples}.')\n\n  # Sample a set of points from the step function.\n  centers = sample(\n      rng, t, w_logits, num_samples, single_jitter, deterministic_center=True\n  )\n\n  # The intervals we return will span the midpoints of each adjacent sample.\n  mid = (centers[Ellipsis, 1:] + centers[Ellipsis, :-1]) / 2\n\n  # Each first/last fencepost is the reflection of the first/last midpoint\n  # around the first/last sampled center.\n  first = 2 * centers[Ellipsis, :1] - mid[Ellipsis, :1]\n  last = 2 * centers[Ellipsis, -1:] - mid[Ellipsis, -1:]\n  samples = jnp.concatenate([first, mid, last], axis=-1)\n\n  # We clamp to the limits of the input domain, provided by the caller.\n  samples = jnp.clip(samples, *domain)\n  return samples"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter=single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[Ellipsis, 1:] + t_samples[Ellipsis, :-1]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_midpoints = jnp.concatenate([jnp.array([domain[0]]), t_midpoints, jnp.array([domain[1]])], axis=-1)\n\n  return t_midpoints\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1.\n  w = w / jnp.sum(w, axis=-1, keepdims=True)\n\n  # Compute the integrated weights.\n  acc_w = jnp.cumsum(w, axis=-1)\n\n  # Interpolate the integrated weights to get the percentiles.\n  ps = ps / 100\n  ps_interp = jnp.vectorize(jnp.interp, signature='(n),(m),(m)->(n)')(ps, acc_w, t)\n\n  return ps_interp\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a PDF\n  pdf = weight_to_pdf(t, w)\n\n  # Blur the PDF\n  pdf_blurred = linspline.blur_pdf(t, pdf, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points\n  w_blurred = pdf_to_weight(t, pdf_blurred)\n  w_blurred_resampled = resample(tq, t, w_blurred)\n\n  return w_blurred_resampled"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Add a homogeneous coordinate to the input vectors.\n  vectors = to_homogeneous(vectors)\n\n  # Apply the transformation to the homogeneous vectors.\n  transformed_vectors = matmul(transform, vectors)\n\n  # Remove the homogeneous coordinate from the transformed vectors.\n  return from_homogeneous(transformed_vectors)\n\n"}
{"namespace": "stepfun.resample", "completion": "  # Check if the input arguments are valid.\n  utils.assert_valid_stepfun(t, tp, vp)\n\n  # Check if the resampling method is valid.\n  if not use_avg and not use_sum:\n    raise ValueError('Invalid resampling method. Must be either use_avg or use_sum.')\n\n  # Check if the resampling method is valid.\n  if not use_avg and not use_sum:\n    raise ValueError('Invalid resampling method. Must be either use_avg or use_sum.')\n\n  # Check if the resampling method is valid.\n  if not use_avg and not use_sum:\n    raise ValueError('Invalid resampling method. Must be either use_avg or use_sum.')\n\n  # Check if the resampling method is valid.\n  if not use_avg and not use_sum:\n    raise ValueError('Invalid resampling method. Must be either use_avg or use_sum.')\n\n  # Check if the resampling method is valid.\n  if not use_avg and not use_sum:\n    raise ValueError('Invalid resampling method. Must be either use_avg or use_sum.')\n\n  # Check if the resampling method is valid.\n  if not use_avg and not use_sum:\n    raise ValueError('Invalid resampling method. Must be either use_avg or use_sum.')\n\n  # Check if the resampling method is valid.\n  if not use_avg and not use_sum:\n    raise ValueError('Invalid resampling method. Must be either use_avg or use_sum.')\n\n  # Check if the resampling method is valid.\n  if not use_avg and not use_sum:\n    raise ValueError('Invalid resampling method. Must be either use_avg or use_sum.')\n\n  # Check if the resampling method is valid.\n  if not use_avg and not use_sum:\n    raise ValueError('Invalid resampling method. Must be either use_avg or use_sum.')\n\n  # Check if the resampling method is valid.\n  if not use_avg and not use_sum:\n    raise ValueError('Invalid resampling method. Must be either use_avg or use_sum.')\n\n  # Check if the resampling method is valid.\n  if not use_avg and not use_sum:\n    raise ValueError('Invalid resampling method. Must be either use_avg or use_sum.')\n\n "}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Compute the mean and variance of the input coordinates.\n  mean_sq = jnp.sum(mean**2, axis=-1, keepdims=True)\n  var_sq = jnp.sum(var**2, axis=-1, keepdims=True)\n\n  # Compute the scaling factor for each degree.\n  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n\n  # Scale the mean and variance of the input coordinates.\n  scaled_mean = mean[Ellipsis, None, :] * scales[:, None]\n  scaled_var = var[Ellipsis, None, :] * scales[:, None]\n\n  # Concatenate the scaled mean and variance.\n  concat_mean_var = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply the sinusoidal encoding to the concatenated mean and variance.\n  four_feat = jnp.sin(\n      jnp.concatenate([concat_mean_var, concat_mean_var + 0.5 * jnp.pi], axis=-1)\n  )\n\n  # Return the encoded variables.\n  return four_feat\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz, kappa_inv):\n    \"\"\"Function returning directional encoding (DE).\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n      kappa_inv: [..., 1] reciprocal of the concentration parameter of the von\n        Mises-Fisher distribution.\n\n    Returns:\n      An array with the resulting DE.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Apply attenuation function using the von Mises-Fisher distribution\n    # concentration parameter, kappa.\n    sigma = 0.5 * ml_array[1, :] * (ml_array[1, :] + 1)\n    de = sph_harms * jnp.exp(-sigma * kappa_inv)\n\n    # Split"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "/nlp_ingestor/ingestor_utils/line_parser.py\nimport re\nimport logging\nimport nltk\nfrom nltk.tokenize import sent_tokenize\nfrom. import line_parser\nfrom. import formatter\nfrom. import spell_utils as su\nfrom. import line_parser\nfrom. import page_parser\nfrom. import page_stats\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils\nfrom. import page_stats_utils"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    # if the input text is empty or None, return the input as is\n    if not org_texts:\n        return org_texts\n\n    # tokenize the text into sentences using the nltk tokenizer\n    sentences = nltk_tokenzier.tokenize(org_texts)\n\n    # normalize quotation marks\n    sentences = [quotation_pattern.sub(\"\", sentence) for sentence in sentences]\n\n    # apply the normalization rules\n    for rule, replaced in rules:\n        sentences = [rule.sub(replaced, sentence) for sentence in sentences]\n\n    # remove any space between punctuations ('.')\n    sentences = [space_rule.sub(r\"\\1\", sentence) for sentence in sentences]\n\n    # remove any space between punctuations ('.')\n    sentences = [space_rule.sub(r\"\\1\", sentence) for sentence in sentences]\n\n    # remove any space between punctuations ('.')\n    sentences = [space_rule.sub(r\"\\1\", sentence) for sentence in sentences]\n\n    # remove any space between punctuations ('.')\n    sentences = [space_rule.sub(r\"\\1\", sentence) for sentence in sentences]\n\n    # remove any space between punctuations ('.')\n    sentences = [space_rule.sub(r\"\\1\", sentence) for sentence in sentences]\n\n    # remove any space between punctuations ('.')\n    sentences = [space_rule.sub(r\"\\1\", sentence) for sentence in sentences]\n\n    # remove any space between punctuations ('.')\n    sentences = [space_rule.sub(r\"\\1\", sentence) for sentence in sentences]\n\n    # remove any space between punctuations ('.')\n    sentences = [space_rule.sub(r\"\\1\", sentence) for sentence in sentences]\n\n    # remove any space between punctuations ('.')\n    sentences = [space_rule.sub(r\"\\1\", sentence) for sentence in sentences]\n\n    # remove any space between punctuations ('.')\n    sentences = [space_rule.sub(r\"\\1\", sentence) for sentence in sentences]\n\n    # remove any space between punctuations ('.')\n    sentences = [space_rule.sub(r\"\\1\", sentence) for sentence in sentences]\n\n    # remove any space between punctuations ('.')\n    sentences = [space"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if not isinstance(token, str):\n            raise TypeError(\"Expected a string\")\n\n        try:\n            term_id = self.term_dict.get_term_id(token)\n            if key is None:\n                return self.posns.positions(term_id)\n            else:\n                return self.posns.positions(term_id, doc_id=key)\n        except TermMissingError:\n            return []\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == \"100%\":\n        return num_clauses\n\n    if spec == \"0%\":\n        return 0\n\n    if spec.endswith(\"%\"):\n        return int(num_clauses * float(spec[:-1]) / 100)\n\n    if spec.startswith(\"<\"):\n        return int(spec[1:])\n\n    return int(spec)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1:\n            return self.phrase_freq_every_diff(tokens)\n        else:\n            return self.phrase_freq_scan(tokens)"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # If we have a list of strings, we need to tokenize them\n        if isinstance(postings, list):\n            postings = [self.tokenizer(string) for string in postings]\n\n        # If we have a list of dicts, we need to tokenize them\n        if isinstance(postings, list) and isinstance(postings[0], dict):\n            postings = [self.tokenizer(string) for string in postings]\n\n        # If we have a list of Terms, we need to tokenize them\n        if isinstance(postings, list) and isinstance(postings[0], Terms):\n            postings = [self.tokenizer(string) for string in postings]\n\n        # If we have a list of Terms, we need to tokenize them\n        if isinstance(postings, list) and isinstance(postings[0], Terms):\n            postings = [self.tokenizer(string) for string in postings]\n\n        # If we have a list of Terms, we need to tokenize them\n        if isinstance(postings, list) and isinstance(postings[0], Terms):\n            postings = [self.tokenizer(string) for string in postings]\n\n        # If we have a list of Terms, we need to tokenize them\n        if isinstance(postings, list) and isinstance(postings[0], Terms):\n            postings = [self.tokenizer(string) for string in postings]\n\n        # If we have a list of Terms, we need to tokenize them\n        if isinstance(postings, list) and isinstance(postings[0], Terms):\n            postings = [self.tokenizer(string) for string in postings]\n\n        # If we have a list of Terms, we need to tokenize them\n        if isinstance(postings, list) and isinstance(postings[0], Terms):\n            postings = [self.tokenizer(string) for string in postings]\n\n        # If we have a list of Terms, we need to tokenize them\n        if isinstance(postings, list) and isinstance(postings[0], Terms):\n            postings = [self.tokenizer(string) for string in postings]\n\n        # If we have a list of Terms, we need to tokenize them\n        if isinstance(postings, list) and isinstance(postings[0], Terms):\n            postings = [self.tokenizer(string) for string in postings"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(self.config['proxyHost'], self.config['proxyPort'])\n        self.server.start()\n        self.connections = {}\n        self.lock = threading.Lock()\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = np.bitwise_and(arr, s55)\n    arr = np.right_shift(arr, 1)\n    arr = np.bitwise_or(arr, np.bitwise_and(arr, s33))\n    arr = np.right_shift(arr, 2)\n    arr = np.bitwise_or(arr, np.bitwise_and(arr, s0F))\n    arr = np.right_shift(arr, 4)\n    arr = np.bitwise_or(arr, np.bitwise_and(arr, s01))\n    arr = np.right_shift(arr, 8)\n    arr = np.bitwise_or(arr, np.bitwise_and(arr, s01))\n    arr = np.right_shift(arr, 16)\n    arr = np.bitwise_or(arr, np.bitwise_and(arr, s01))\n    arr = np.right_shift(arr, 32)\n    arr = np.bitwise_or(arr, np.bitwise_and(arr, s01))\n    return arr\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = \"100%\"\n\n    query_fields = parse_field_boosts(qf)\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    if term_centric:\n        return _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        return _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            for connection in self.connections.values():\n                connection.stop()\n            self.server.stop()\n            self.server.join()\n            self.server = None\n            self.connections = None\n            self.lock = None"}
