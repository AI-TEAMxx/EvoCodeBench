{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    @functools.wraps(func_name)\n    def wrapper(*args, **kwargs):\n        with SQLiteMemoization(filename) as memo:\n            return memo.fetch_or_compute(func_name, func_name.__name__, *args, **kwargs)\n\n    return wrapper"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max.\")\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max.\")\n    return values"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  mat0 = np.reshape(mat0, [-1, 1])\n  mat1 = np.reshape(mat1, [1, -1])\n  dist = np.sum(mat0**2, 0, keepdims=True) + np.sum(mat1**2, 0, keepdims=True)\n  dist -= 2 * np.matmul(mat0, mat1)\n  dist = np.maximum(dist, 0)\n  return dist"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if not path:\n        return False\n    if path.startswith(\"s3://\") or path.startswith(\"gs://\"):\n        return True\n    if path.startswith(\"hdfs://\"):\n        return True\n    if path.startswith(\"file://\"):\n        return True\n    if path.startswith(\"http://\") or path.startswith(\"https://\"):\n        return True\n    if path.startswith(\"ftp://\"):\n        return True\n    if path.startswith(\"sftp://\"):\n        return True\n    if path.startswith(\"smb://\"):\n        return True\n    if path.startswith(\"nfs://\"):\n        return True\n    if path.startswith(\"cifs://\"):\n        return True\n    if path.startswith(\"ssh://\"):\n        return True\n    if path.startswith(\"s3a://\"):\n        return True\n    if path.startswith(\"s3n://\"):\n        return True\n    if path.startswith(\"s3p://\"):\n        return True\n    if path.startswith(\"s3c://\"):\n        return True\n    if path.startswith(\"s3d://\"):\n        return True\n    if path.startswith(\"s3v://\"):\n        return True\n    if path.startswith(\"s3y://\"):\n        return True\n    if path.startswith(\"s3z://\"):\n        return True\n    if path.startswith(\"s3r://\"):\n        return True\n    if path.startswith(\"s3q://\"):\n        return True\n    if path.startswith(\"s3w://\"):\n        return True\n    if path.startswith(\"s3x://\"):\n        return True\n    if path.startswith(\"s3m://\"):\n        return True\n    if path.startswith(\"s3u://\"):\n        return True\n    if path.startswith(\"s3t://\"):\n        return True\n    if path."}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"The '{name}' argument must be a dictionary when 'assets_names' is \"\n                \"None\"\n            )\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"The length of 'assets_names' must be equal to 'n_assets' ({n_assets})\"\n            )\n        if len(items) != n_assets:\n            raise ValueError(\n                f\"The length of '{name}' must be equal to 'n_assets' ({n_assets})\"\n            )\n        if not all(isinstance(v, (int, float)) for v in items.values()):\n            raise ValueError(\n                f\"The values of '{name}' must be numeric, got {type(items.values())}\"\n            )\n        if not all(isinstance(k, str) for k in items.keys()):\n            raise ValueError(\n                f\"The keys of '{name}' must be strings, got {type(items.keys())}\"\n            )\n        if not all(k in assets_names for k in items.keys()):\n            raise ValueError(\n                f\"The keys of '{name}' must be in 'assets_names' ({assets_names})\"\n            )\n        if not all(k in assets_names for k in items.keys()):\n            raise ValueError(\n                f\"The keys of '{name}' must be in 'assets_names' ({assets_names})\"\n            )\n        if not all(k in assets_names for k in items.keys()):\n            raise ValueError(\n                f\"The keys of '{name}' must be in 'assets_names' ({assets_names})\"\n            )\n        if not all(k in assets_names for k in items.keys()):\n            raise ValueError(\n                f\"The keys of '{name}' must be in 'assets_names' ({assets_names})\"\n            )\n        if not all(k in assets_names for k in"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\")\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        return MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\"),\n            purpose=data.get(\"purpose\"),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\"),\n            max_depth=data.get(\"max_depth\"),\n            usage_count=data.get(\"usage_count\"),\n            id=data.get(\"id\"),\n            parent_id=data.get(\"parent_id\"),\n            working_agent=data.get(\"working_agent\"),\n            is_prime=data.get(\"is_prime\"),\n            evolve_count=data.get(\"evolve_count\"),\n            number_of_code_executions=data.get(\"number_of_code_executions\"),\n            last_input=data.get(\"last_input\"),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = 0.055 * xnp.exp(1.07 * srgb - 0.07)\n  linear1 = (srgb + 0.055) / 1.055\n  linear1 = xnp.maximum(eps, linear1) ** (12 / 11)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  tck, u_keyframes = scipy.interpolate.splprep(x.T, k=spline_degree, s=smoothness, per=False)\n  new_x = np.array(scipy.interpolate.splev(t_output, tck))\n  return new_x\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n    if word[0].isupper() and word[1].isupper():\n        return word\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    if word[0].islower() and word[1].isupper():\n        return word.capitalize()\n    return word"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a binary array, found {v.dtype} instead.\"\n        )\n\n    return v"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n    return summary_df"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # Compute the determinant of the input covariance matrix.\n    det = jnp.linalg.det(cov)\n    # Compute the square root of the determinant.\n    sqrt_det = jnp.sqrt(det)\n    # Compute the inverse square root of the determinant.\n    inv_sqrt_det = 1.0 / sqrt_det\n    # Compute the isotropic covariance matrix by multiplying the input covariance matrix with the inverse square root of the determinant.\n    isotropic_cov = inv_sqrt_det * cov * inv_sqrt_det\n    return isotropic_cov\n  elif mode == 'accurate':\n    # Compute the logarithm of the determinant of the input covariance matrix.\n    log_det = jnp.linalg.slogdet(cov)[1]\n    # Compute the square root of the logarithm of the determinant.\n    sqrt_log_det = jnp.sqrt(log_det)\n    # Compute the inverse square root of the logarithm of the determinant.\n    inv_sqrt_log_det = 1.0 / sqrt_log_det\n    # Compute the isotropic covariance matrix by multiplying the input covariance matrix with the inverse square root of the logarithm of the determinant.\n    isotropic_cov = inv_sqrt_log_det * cov * inv_sqrt_log_det\n    return isotropic_cov\n  else:\n    raise ValueError(f'Invalid mode: {mode}')"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='XAgent Command Line Interface')\n    parser.add_argument('--task', type=str, required=True, help='The task description, specifying what task should be performed.')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload, allowing multiple files to be specified.')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use.')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs.')\n    parser.add_argument('--mode', type=str, default=\"auto\", help='Operational mode, which can be \\'auto\\' or \\'manual\\', specifying how the task should be executed.')\n    parser.add_argument('--quiet', action='store_true', help='If set, the program runs in quiet mode with minimal output.')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be.')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution.')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans.')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be.')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points.\")\n\n    return v"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + \"_\"\n    return charset[n]"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunk_index = 0\n        for interval in intervals:\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                chunk_index += 1\n                indexes[worker_idx] -= interval[1] - interval[0]\n        chunks_index[worker_idx] = chunk_index\n    return chunks_index, indexes"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # The coordinates are assumed to be in the range [0, 1].\n    # We need to scale them to the range [0, N-1] where N is the size of the\n    # grid.\n    # We also need to round the coordinates to the nearest integer.\n    # This is because the trilinear interpolation requires the coordinates to be\n    # integers.\n    # We also need to clip the coordinates to the range [0, N-1] to avoid\n    # out-of-bounds errors.\n    # We also need to transpose the coordinates to the shape (..., 3, 1) to\n    # allow for broadcasting.\n    # We also need to transpose the values to the shape (D, H, W, C) to allow\n    # for broadcasting.\n    # We also need to transpose the coordinates to the shape (..., 3) to allow\n    # for broadcasting.\n    # We also need to transpose the values to the shape (D, H, W, C) to allow\n    # for broadcasting.\n    # We also need to transpose the coordinates to the shape (..., 3, 1) to\n    # allow for broadcasting.\n    # We also need to transpose the values to the shape (D, H, W, C) to allow\n    # for broadcasting.\n    # We also need to transpose the coordinates to the shape (..., 3, 1) to\n    # allow for broadcasting.\n    # We also need to transpose the values to the shape (D, H, W, C) to allow\n    # for broadcasting.\n    # We also need to transpose the coordinates to the shape (..., 3, 1) to\n    # allow for broadcasting.\n    # We also need to transpose the values to the shape (D, H, W, C) to allow\n    # for broadcasting.\n    # We also need to transpose the coordinates to the shape (..., 3, 1) to\n    # allow for broadcasting.\n    # We also need to transpose the values to the shape (D, H, W, C)"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  if v < 1:\n    raise ValueError(f'v {v} must be >= 1')\n  if v == 1:\n    return np.ones((1, 3))\n\n  weights = []\n  for i in range(v + 1):\n    for j in range(v + 1 - i):\n      weights.append([i, j, v - i - j])\n  weights = np.array(weights)\n  weights = weights / np.sum(weights, 1, keepdims=True)\n  return weights"}
{"namespace": "linspline.query", "completion": "  utils.assert_valid_linspline(t, v)\n  check_zero_endpoints(v)\n\n  # Clip to valid inputs (assumes repeating boundaries).\n  tq = jnp.clip(tq, t[Ellipsis, :1], math.minus_eps(t[Ellipsis, -1:]))\n\n  # Lookup the value corresponding to each input query.\n  idx0, idx1 = sorted_lookup(tq, t)\n  v0 = jnp.take_along_axis(v, idx0, axis=-1)\n  v1 = jnp.take_along_axis(v, idx1, axis=-1)\n\n  # Interpolate between the two values.\n  dt = t[Ellipsis, idx1] - t[Ellipsis, idx0]\n  vq = v0 + (v1 - v0) * (tq - t[Ellipsis, idx0]) / dt\n  return vq"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(isinstance(value, (int, float)) and value > 0 for value in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if not isinstance(v, (int, float)) or v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Convert to homogeneous coordinates.\n  rays_o = xnp.concatenate([origins, xnp.ones_like(origins[Ellipsis, :1])], -1)\n  rays_d = xnp.concatenate([directions, xnp.ones_like(directions[Ellipsis, :1])], -1)\n\n  # Convert to NDC.\n  rays_o = xnp.matmul(pixtocam, rays_o.transpose([0, 2, 1])).transpose([0, 2, 1])\n  rays_d = xnp.matmul(pixtocam, rays_d.transpose([0, 2, 1])).transpose([0, 2, 1])\n\n  # Normalize to [-1, 1]^2.\n  rays_o = rays_o[..., :2] / rays_o[..., 2:]\n  rays_d = rays_d[..., :2] / rays_d[..., 2:]\n\n  # Adjust ray origins to the near plane.\n  rays_o = rays_o * near\n\n  # Normalize ray directions.\n  rays_d = rays_d / xnp.linalg.norm(rays_d, axis=-1, keepdims=True)\n\n  return rays_o, rays_d\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.abs(jnp.dot(dir1, dir2)) > 1 - 1e-6\n\n"}
{"namespace": "common.bleu4_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    bleu = evaluate.load('uhgeval/.cache/huggingface/bleu')\n    results = bleu.compute(predictions=[continuation], references=[[reference]], tokenizer=f, smooth=with_penalty)\n    score = results['bleu']\n    return score\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x > eps, jnp.sqrt(x), value_at_zero)\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Compute the PDF by dividing the weights by the difference between consecutive elements in the input vector t.\n  pdf = w / jnp.diff(t)\n  return pdf"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    with contextlib.suppress(FileNotFoundError):\n        for dirpath, _, filenames in os.walk(path):\n            for f in filenames:\n                fp = os.path.join(dirpath, f)\n                total_size += os.path.getsize(fp)\n    return total_size"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val = val - offset * period\n    val = torch.fmod(val, period)\n    val = val + offset * period\n    return val\n\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        data = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding.tolist() if agent.purpose_embedding is not None else None,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n        return data"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    "}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        return hashlib.sha256(json.dumps(data).encode(\"utf-8\")).hexdigest()"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the distances between consecutive points in the polygon\n    distances = np.linalg.norm(polygon[1:] - polygon[:-1], axis=1)\n\n    # Exclude distances that exceed the maximum distance threshold\n    distances = distances[distances <= max_point_distance]\n\n    # Sum the distances to obtain the total length of the polygon\n    total_length = np.sum(distances)\n\n    return total_length"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if not polygons:\n        return polygons\n\n    largest_polygon_area = area(polygons[0])\n    polygons = [polygon for polygon in polygons if area(polygon) > largest_polygon_area * rel_tr]\n    polygons = [polygon for polygon in polygons if area(polygon) > abs_tr]\n\n    return polygons"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    indexes = {}\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] = num_samples_yielded % num_workers\n\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] += worker_idx * (num_samples_yielded // num_workers)\n\n    for worker_idx in range(num_workers):\n        indexes[worker_idx] = indexes[worker_idx] % batch_size\n\n    return indexes"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n    for result, value, metadata in zip(results, value, metadatas):\n        if value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n    return filtered_results, filtered_metadatas"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f\"This function expects a polygon, i.e. an array of shape (_, 2). Got {array.shape}\")\n\n    xs = array[:, 0]\n    ys = array[:, 1]\n\n    return 0.5 * np.abs(np.dot(xs, np.roll(ys, 1)) - np.dot(ys, np.roll(xs, 1)))\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Find the indices where elements of v should be inserted into a.\n    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n    return idx_lo, idx_hi"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ])\n\n"}
{"namespace": "coord.contract", "completion": "  z_mag_sq = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = 2 * jnp.sqrt(z_mag_sq) - z_mag_sq\n  z = x / scale\n  return z"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0 B\"\n    for suffix in _FORMAT_TO_RATIO:\n        if num_bytes >= _FORMAT_TO_RATIO[suffix]:\n            return f\"{num_bytes / _FORMAT_TO_RATIO[suffix]:.2f} {suffix}\"\n    return f\"{num_bytes} B\""}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def __validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check that the array has the specified number of dimensions.\"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. \"\n                f\"Received {len(v.shape)} dimensions.\"\n            )\n\n        return v\n\n    return __validator"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(cartesian_vector[..., 2] / (r + eps))\n  phi = jnp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n  return r, theta, phi"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    return results['rougeL']"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # First, try to locate the object using the standard method.\n    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    # If the object cannot be located using the standard method, try to locate it using the fallback method.\n    try:\n        module_name, class_name = name.rsplit(\".\", 1)\n        module = pydoc.locate(module_name)\n        return getattr(module, class_name)\n    except (ImportError, AttributeError):\n        pass\n\n    # If the object cannot be located using either method, raise an exception.\n    raise ImportError(f\"Failed to locate object with name '{name}'\")"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    ids = [list(i) for i in ids]\n    scores = [list(i) for i in scores]\n    weights = list(weights)\n\n    if len(ids) != len(scores):\n        raise ValueError(\"The length of the ids tuple must match the length of the scores tuple.\")\n    if len(ids) != len(weights):\n        raise ValueError(\"The length of the ids tuple must match the length of the weights tuple.\")\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of the weights must equal 1.\")\n\n    ids, scores = cc_pure(ids, scores, weights, top_k)\n\n    return ids, scores"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    if percent:\n        x *= 100\n    if x < 0.0001:\n        return f\"{x:.2e}\"\n    if x < 0.001:\n        return f\"{x:.3f}\"\n    if x < 0.01:\n        return f\"{x:.2f}\"\n    if x < 0.1:\n        return f\"{x:.1f}\"\n    return f\"{x:.0f}\""}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        # 1. Get the free space in the directory\n        free_space = shutil.disk_usage(input_dir).free\n\n        # 2. Check if the free space is lower than the threshold\n        if free_space > threshold_in_gb * 1024 * 1024 * 1024:\n            return\n\n        # 3. Sleep for a while before checking again\n        sleep(sleep_time)"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  w = jnp.where(td < np.finfo(np.float32).tiny, 0, p * td)\n  return w\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = nlm_tokenize(line_text)\n\n    # Join the tokens back into a single string\n    line_text = \" \".join(tokens)\n\n    return line_text"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zeros cannot exceed the number of weights\")\n    weights = np.random.uniform(size=n)\n    weights = weights / np.sum(weights)\n    if zeros > 0:\n        zero_idx = np.random.choice(n, zeros, replace=False)\n        weights[zero_idx] = 0\n    return weights"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n    bbox_center = (bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2\n    crop_center = bbox_center\n    crop_size = crop_size\n    crop_size = crop_size[0], crop_size[1]\n    image_size = image_size[0], image_size[1]\n    crop_size = min(crop_size[0], image_size[0]), min(crop_size[1], image_size[1])\n    crop_center = min(max(crop_center[0], crop_size[0] / 2), image_size[0] - crop_size[0] / 2), min(\n        max(crop_center[1], crop_size[1] / 2), image_size[1] - crop_size[1] / 2\n    )\n    crop_top_left = (\n        crop_center[0] - crop_size[0] / 2,\n        crop_center[1] - crop_size[1] / 2,\n    )\n    crop_transform = T.CropTransform(crop_size, crop_top_left)\n    return crop_transform"}
{"namespace": "ref_utils.l2_normalize", "completion": "  norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  norm_clamped = jnp.maximum(norm, grad_eps)\n  return x / norm_clamped\n\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name = agent_info.split(\":\")[0]\n        input_text = agent_info.split(\":\")[1] if len(agent_info.split(\":\")) > 1 else \"\"\n        return agent_name, input_text"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [obj[\"bbox\"] for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if mask_format == \"polygon\":\n        segms = [obj.get(\"segmentation\", None) for obj in annos]\n        masks = PolygonMasks(segms)\n        masks = masks.to(dtype=torch.uint8)\n        target.gt_masks = masks\n    elif mask_format == \"bitmask\":\n        segms = [obj.get(\"segmentation\", None) for obj in annos]\n        masks = polygons_to_bitmask(segms, *image_size)\n        masks = BitMasks(masks)\n        target.gt_masks = masks\n    else:\n        raise ValueError(\"Unknown mask format: {}\".format(mask_format))\n\n    keypoints = [obj.get(\"keypoints\", None) for obj in annos]\n    keypoints = Keypoints(keypoints)\n    target.gt_keypoints = keypoints\n\n    return target"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\"))\n    data_home = os.path.expanduser(data_home)\n    os.makedirs(data_home, exist_ok=True)\n    return data_home"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert_is_square(cov)\n    assert_is_symmetric(cov)\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    return corr, std"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # Iterate through all submodules of the model\n    for name, module in model.named_modules():\n        # Check if the module has a \"training\" attribute\n        if hasattr(module, \"training\"):\n            # Get the original value of the \"training\" attribute\n            original_training_value = getattr(module, \"training\")\n            # Create a new class definition for the module with the \"training\" attribute annotated as a constant\n            new_module_class = type(module.__class__.__name__, (module.__class__,), {})\n            new_module_class.__annotations__[\"training\"] = torch.jit.Final[bool]\n            # Replace the module's class definition with the new one\n            setattr(model, name, new_module_class())\n\n    # Yield control to the context manager\n    yield\n\n    # Revert the changes made to the model's submodules\n    for name, module in model.named_modules():\n        if hasattr(module, \"training\"):\n            setattr(module, \"training\", original_training_value)\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Check if the shapes of the two fields match.\n\n        This function is a validator for a Pydantic model. It takes in a dictionary of values, where the keys are the names of the fields being compared, and the values are the corresponding NumPy arrays. The function checks if the shapes of the two fields match. If the shapes do not match, it raises a ValueError indicating the mismatch.\n\n        :param cls: The class of the Pydantic model being validated.\n        :param values: A dictionary of values, where the keys are the names of the fields being compared, and the values are the corresponding NumPy arrays.\n        :return: The same dictionary of values that was passed in.\n        :raises ValueError: If the shapes of the two fields do not match.\n        \"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, resp. {values[field1].shape} and {values[field2].shape}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list):\n        if isinstance(metrics[0], str):\n            return metrics, [{} for _ in metrics]\n        elif isinstance(metrics[0], dict):\n            return [metric['name'] for metric in metrics], metrics\n        else:\n            raise ValueError('Invalid metric format.')\n    else:\n        raise ValueError('Invalid metric format.')"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    # Try to automatically determine the inverse of the function.\n    if fn == contract:\n      fn_inv = inv_contract\n    else:\n      raise ValueError(f'fn_inv not provided and fn={fn} not supported.')\n\n  def t_to_s(t):\n    \"\"\"\n    Maps metric distances to normalized distances in the range [0, 1].\n\n    Args:\n      t: Tensor. The input metric distances.\n\n    Returns:\n      s: Tensor. The corresponding normalized distances in the range [0, 1].\n    \"\"\"\n    # Clamp the input distances to the specified near and far planes.\n    t = jnp.clip(t, t_near, t_far)\n    # Compute the scaling factor based on the function `fn`.\n    scale = fn_inv(t_far) - fn_inv(t_near)\n    # Map the input distances to normalized distances.\n    s = (fn_inv(t) - fn_inv(t_near)) / scale\n    return s\n\n  def s_to_t(s):\n    \"\"\"\n    Maps normalized distances back to metric distances.\n\n    Args:\n      s: Tensor. The input normalized distances in the range [0, 1].\n\n    Returns:\n      t: Tensor. The corresponding metric distances.\n    \"\"\"\n    # Clamp the input normalized distances to the range [0, 1].\n    s = jnp.clip(s, 0, 1)\n    # Compute the scaling factor based on the function `fn`.\n    scale = fn_inv(t_far) - fn_inv(t_near)\n    # Map the normalized distances back to metric distances.\n    t = fn(s * scale + fn_inv(t_near))\n    return t\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)  # pytype: disable=bad-return-type  # jax-ndarray"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  check_zero_endpoints(w)\n  dt = jnp.diff(t)\n  dw = jnp.diff(w)\n  return jnp.sum(dt * (w[Ellipsis, :-1] + dw / 2), axis=-1)"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    id_df = pd.DataFrame({f'id_{i}': id_list for i, id_list in enumerate(ids)})\n    score_df = pd.DataFrame({f'score_{i}': score_list for i, score_list in enumerate(scores)})\n    df = pd.concat([id_df, score_df], axis=1)\n\n    def cc_pure_apply(row):\n        ids_tuple = tuple(row[[f'id_{i}' for i in range(len(ids))]].values)\n        scores_tuple = tuple(row[[f'score_{i}' for i in range(len(scores))]].values)\n        return pd.Series(cc_pure(ids_tuple, scores_tuple, weights, top_k))\n\n    df[['cc_id', 'cc_score']] = df.apply(cc_pure_apply, axis=1)\n    return df['cc_id'].tolist(), df['cc_score'].tolist()\n\n"}
{"namespace": "coord.track_linearize", "completion": "  if mean.shape[:-1] != cov.shape[:-2]:\n    raise ValueError(\n        f'mean.shape[:-1] {mean.shape[:-1]} != cov.shape[:-2] {cov.shape[:-2]}.'\n    )\n  d = mean.shape[-1]\n  fn_mean, lin_fn = jax.linearize(fn, mean)\n\n  # Compute the Jacobian of fn function at the locations of each mean.\n  jac = jax.vmap(lin_fn, in_axes=-1, out_axes=-1)(\n      jnp.broadcast_to(jnp.eye(d), mean.shape + (d,))\n  )\n\n  # The cube root of the determinant of the Jacobian is the geometric mean\n  # of the eigenvalues of the Jacobian, which gives us the isotropic scaling\n  # implied by `fn` at each mean that `cov` should be multiplied by.\n  eps = jnp.finfo(jnp.float32).tiny  # Guard against an inf gradient at 0.\n  abs_det = jnp.maximum(eps, jnp.abs(jnp.linalg.det(jac)))\n  # Special case d == 3 for speed's sake.\n  fn_cov = cov * (jnp.cbrt(abs_det) if d == 3 else abs_det ** (1 / d))\n  return fn_mean, fn_cov"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in range(len(x)):\n        if len(x[i]) > 1:\n            yield x[i][: len(x[i]) // 2], x[i][len(x[i]) // 2 :]"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(f\"`x` must be a 2D array, got a {x.ndim}D array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  encoded = jnp.concatenate(\n      [jnp.sin(scaled_x), jnp.cos(scaled_x)], axis=-1\n  )\n  if append_identity:\n    encoded = jnp.concatenate([x[Ellipsis, None, :], encoded], axis=-1)\n  return encoded"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n\n        \"\"\"\n        This function is a Pydantic model validator that checks if two lists of numpy arrays (specified by field names) have the same length and if each corresponding pair of arrays within these lists has the same shape.\n\n        Input-Output Arguments\n        :param cls: type. The class type of the Pydantic model being validated.\n        :param values: Dict[str, List[np.ndarray]]. A dictionary of values to be validated. The keys are the field names specified in the input arguments, and the values are the corresponding lists of numpy arrays.\n        :return: Dict[str, List[np.ndarray]]. The validated values if the check passes.\n\n        \"\"\"\n\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(\n                    f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                    f\"resp. {values[field1][i].shape} and {values[field2][i].shape}\"\n                )\n\n        return values\n\n    return __root_validator"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context\n        eglctx.resize(camera.W, camera.H)\n\n        # Render the mesh\n        self.render(camera)\n\n        # Read the rendered image from the framebuffer\n        image = eglctx.read_framebuffer(camera.W, camera.H)\n        return image\n"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    return NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        # The following attributes do not have a direct equivalent in BertConfig\n        # and are set to commonly used defaults for GPT2Config\n        n_positions=bert_config.max_position_embeddings,\n        n_embd=bert_config.hidden_size,\n        n_layer=bert_config.num_hidden_layers,\n        n_head=bert_config.num_attention_heads,\n        n_inner=bert_config.intermediate_size,\n        activation_function=bert_config.hidden_act,\n        resid_pdrop=bert_config.hidden_dropout_prob,\n        attn_pdrop=bert_config.attention_probs_dropout_prob,\n        layer_norm_epsilon=bert_config.layer_norm_eps,\n        initializer_range=bert_config.initializer_range,\n        summary_type=\"cls_index\",\n        summary_use_proj=True,\n        summary_activation=\"tanh\",\n        summary_last_dropout=0.1,\n        scale_attn_weights=True,\n        scale_embedding=True,\n        use_cache"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            self.use_gl_program(self.point_program)\n        else:\n            self.use_gl_program(self.mesh_program)\n\n        self.upload_gl_uniforms(camera)\n        gl.glBindVertexArray(self.vao)\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        elif self.render_type == Mesh.RenderType.QUADS:\n            gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n        else:\n            raise NotImplementedError\n\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if not hasattr(self, 'cu_tex'):\n            self.init_texture()\n\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n        if isinstance(ptr, np.ndarray):\n            ptr = np.ascontiguousarray(ptr)\n\n        w = w or self.W\n        h = h or self.H\n        if ptr.shape[-1] == 3:\n            ptr = np.concatenate([ptr, ptr[..., -1:] * 255], axis=-1)  # add alpha channel\n\n        from cuda import cudart\n        kind = cudart.cudaMemcpyKind.cudaMemcpyHostToDevice\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsMapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n        cu_tex_arr = CHECK_CUDART_ERROR(cudart.cudaGraphicsSubResourceGetMappedArray(self.cu_tex, 0, 0))\n        CHECK_CUDART_ERROR(cudart.cudaMemcpy2DToArrayAsync(cu_tex_arr,\n                                                           x * 4 * ptr.element_size(),\n                                                           y,\n                                                           ptr.data_ptr(),\n                                                           w * 4 * ptr.element_size(),  # differently sized\n                                                           w * 4 * ptr.element_size(),  # rgba, should do a composition first\n                                                           h,\n                                                           kind,\n                                                           torch.cuda.current_stream().cuda_stream))\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsUnmapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure inputs are batched\n    batch_dim = R.shape[:-2]\n    R = R.view(-1, 3, 3)\n    tvec = tvec.view(-1, 3, 1)\n    camera_matrix = camera_matrix.view(-1, 3, 3)\n    image_size = image_size.view(-1, 2)\n\n    # Validate shapes and values\n    assert R.shape == (batch_dim + (3, 3)), \"R must be a batch of rotation matrices of size (*, 3, 3)\"\n    assert tvec.shape == (batch_dim + (3, 1)), \"tvec must be a batch of translation vectors of size (*, 3, 1)\"\n    assert camera_matrix.shape == (batch_dim + (3, 3)), \"camera_matrix must be a batch of camera intrinsic matrices of size (*, 3, 3)\"\n    assert image_size.shape == (batch_dim + (2,)), \"image_size must be a batch of image sizes of size (*, 2)\"\n    assert znear > 0, \"znear must be a positive number\"\n\n    # Extract focal lengths from camera matrix\n    fx = camera_matrix[..., 0, 0]\n    fy = camera_matrix[..., 1, 1]\n\n    # Calculate sensor width\n    sensor_width = fx * image_size[..., 0] / camera_matrix[..., 0, 2]\n\n    # Calculate principal point offsets\n    cx = camera_matrix[..., 0, 2] - image_size[..., 0] / 2\n    cy = camera_matrix[..., 1, 2] - image_size[..., 1] / 2\n\n    # Calculate focal length\n    focal_length = torch.sqrt(fx * fy)\n\n    # Calculate camera position\n    camera_position = -R.transpose(-1, -2) @ tvec\n\n    # Calculate camera rotation\n    camera_rotation = matrix_to_rot"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        w = w or self.W\n        h = h or self.H\n        old_prog = gl.glGetIntegerv(gl.GL_CURRENT_PROGRAM)\n        old_vao = gl.glGetIntegerv(gl.GL_VERTEX_ARRAY_BINDING)\n        gl.glUseProgram(self.quad_program)\n        self.upload_gl_uniforms(camera)\n\n        # Prepare to render to textures\n        gl.glBindVertexArray(self.vao)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))  # number of vertices\n        gl.glBindVertexArray(old_vao)\n        gl.glUseProgram(old_prog)\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    K = get_ndc_perspective_matrix(K, H, W, 1, 1)  # B, 4, 4\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        # Get the current read framebuffer\n        old_read_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n\n        # Bind the read framebuffer to the Quad instance's FBO\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n\n        # Blit the contents of the read framebuffer to the destination framebuffer\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n\n        # Restore the original read framebuffer\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_read_fbo)\n\n        \"\"\"\n        This function temporarily binds the Quad instance's framebuffer object (FBO) as the read framebuffer, performs the pixel copy operation, and then restores the previously bound read framebuffer. This ensures that the function's operations do not interfere with the current OpenGL state beyond the scope of the function.\n        \"\"\"\n\n        \"\"\"\n        The function blit copies a block of pixels from the source framebuffer to the destination framebuffer. It respects the OpenGL convention of using the lower left corner as the origin. This is typically used for operations like copying a texture, rendering to a texture, or performing a fast pixel transfer operation.\n\n        Input-Output Arguments\n        :param self: Quad. An instance of the Quad class, which presumably holds information about a framebuffer object (FBO) and its dimensions.\n        :param x: int, optional. The x-coordinate of the lower left corner where the copy starts. Defaults to 0.\n        :param y: int, optional. The y-coordinate of the lower left corner where the copy starts. Defaults to 0.\n        :param w: int, optional. The width of the pixel block to be copied. If 0, the width of the Quad instance (self.W) is used. Defaults to 0.\n        :param h: int, optional. The height of the pixel block to be copied."}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    t0, t1, y1 = matchup_channels(t0, t1, y1)\n    t0, t1 = matchup_channels(t0, t1)\n    t0, t1 = t0.view(-1, 1), t1.view(-1, 1)\n    y1 = y1.view(-1, 1)\n    t0_idx = searchsorted(t1, t0)\n    t1_idx = searchsorted(t1, t0, side='right')\n    t0_idx = torch.clamp(t0_idx, 0, t1.shape[0] - 1)\n    t1_idx = torch.clamp(t1_idx, 0, t1.shape[0] - 1)\n    t0_idx = t0_idx.view(-1)\n    t1_idx = t1_idx.view(-1)\n    y0 = y1[t0_idx]\n    y1 = y1[t1_idx]\n    inner = (t1 - t0) * (y0 + y1) / 2\n    outer = (t1 - t0) * y1\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    w_env, w_env = matchup_channels(w_env, w_env)\n    t_env, w_env = matchup_channels(t_env, w_env)\n\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], eps)\n\n    t_, w_ = blur_stepfun(t, w_normalize, 1.)\n    w_ = torch.clip(w_, min=0.)\n    assert (w_ >= 0.0).all()\n\n    # piecewise linear pdf to piecewise quadratic cdf\n    area = 0.5 * (w_[..., 1:] + w_[..., :-1]) * (t_[..., 1:] - t_[..., :-1])\n\n    cdf = torch.cat([torch.zeros_like(area[..., :1]), torch.cumsum(area, dim=-1)], dim=-1)\n\n    # query piecewise quadratic interpolation\n    cdf_interp = sorted_interp_quad(t_env, t_, w_, cdf)\n    # difference between adjacent interpolated values\n    w_s = torch.diff(cdf_interp, dim=-1)\n\n    return ((w_s - w_env).clip(0.).pow(2) / (w_env + eps)).mean()\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n    # t.shape[-1] = w.shape[-1] + 1\n   "}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, device=t.device), cw, t)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    t, w = matchup_channels(t, w)\n    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n    # Sample uniformly from [0, 1)\n    u = torch.rand(num_samples, device=t.device)\n    # Invert the CDF to get a sample from the PDF.\n    t_new = invert_cdf(u, t, w)\n    # Optionally perturb the samples to avoid sample clustering at bin boundaries.\n    if perturb:\n        # Get the PDF value at the newly-sampled points.\n        pdf_new = query(t_new, t, w)\n        # Compute the CDF for the PDF (used to sample the truncated exponential below).\n        cdf_new = integrate_weights(pdf_new)\n        # Sample an offset from a truncated exponential distribution.\n        if single_jitter:\n            offset = -torch.log(torch.rand(num_samples, device=t.device))\n        else:\n            offset = -torch.log(torch.rand(num_samples, t_new.shape[-1], device=t.device))\n        # Offset the newly-sampled points to avoid sample clustering.\n        t_new = t_new + query(offset, cdf_new, pdf_new)\n    return t_new"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, p = matchup_channels(t, w)\n    # Dilate the time steps by the specified dilation parameter.\n    t_dilate = t * dilation\n\n    # Clip the dilated time steps to the specified domain.\n    t_dilate = torch.clamp(t_dilate, min=domain[0], max=domain[1])\n\n    # Compute the dilated weights by taking the maximum value within each interval.\n    p_dilate = torch.max_pool1d(p[..., None], kernel_size=2, stride=1, padding=0)[..., 0]\n\n    return t_dilate, p_dilate"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check if the input tensors have the same shape\n    if t.shape != y.shape:\n        raise ValueError(\"Input tensors 't' and 'y' must have the same shape.\")\n\n    # Check if the query times are sorted\n    if not torch.all(torch.diff(tq) >= 0):\n        raise ValueError(\"Query times 'tq' must be sorted in ascending order.\")\n\n    # Check if the step function times are sorted\n    if not torch.all(torch.diff(t) >= 0):\n        raise ValueError(\"Step function times 't' must be sorted in ascending order.\")\n\n    # Check if the step function values are non-negative\n    if torch.any(y < 0):\n        raise ValueError(\"Step function values 'y' must be non-negative.\")\n\n    # Check if the step function times are strictly increasing\n    if not torch.all(torch.diff(t) > 0):\n        raise ValueError(\"Step function times 't' must be strictly increasing.\")\n\n    # Check if the step function values are strictly increasing\n    if not torch.all(torch.diff(y) > 0):\n        raise ValueError(\"Step function values 'y' must be strictly increasing.\")\n\n    # Check if the step function times and values are finite\n    if not torch.all(torch.isfinite(t)) or not torch.all(torch.isfinite(y)):\n        raise ValueError(\"Step function times 't' and values 'y' must be finite.\")\n\n    # Check if the query times are finite\n    if not torch.all(torch.isfinite(tq)):\n        raise ValueError(\"Query times 'tq' must be finite.\")\n\n    # Check if the query times are within the range of the step function times\n    if torch.any(tq < t[0]) or torch.any(tq > t[-1]):\n        raise ValueError(\"Query times 'tq' must be within the range of the step function times 't'.\")\n\n    # Check if the query times are within the range of the step function times"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    # Calculate the bias based on the training fraction and annealing slope\n    bias = torch.sigmoid(anneal_slope * (1 - train_frac))\n\n    # Adjust the weights using the bias\n    w_adjusted = w * bias\n\n    # Handle cases where adjacent intervals have zero distance\n    w_adjusted = torch.where(torch.abs(t[..., 1:] - t[..., :-1]) < eps, 0, w_adjusted)\n\n    # Prevent NaN values by using a softmax operation on the adjusted weights\n    w_adjusted = torch.softmax(w_adjusted, dim=-1)\n\n    return w_adjusted"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: to_cuda(v, device, ignore_list) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.to(device, non_blocking=True)\n    else:  # numpy and others\n        batch = torch.as_tensor(batch).to(device, non_blocking=True)\n    return batch"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # f: B, N, 3\n    # v: B, P, 3\n    # return: B, N, 3, 3\n    # f = f.expand(-1, -1, 3)\n    # f = f.reshape(f.shape[0], -1, 3)\n    # v = v.expand(f.shape[0], -1, -1)\n    # v = v.reshape(f.shape[0], -1, 3)\n    # return torch.stack([v[..., f[..., 0]], v[..., f[..., 1]], v[..., f[..., 2]]], dim=-1)\n\n    # f: B, N, 3\n    # v: B, P, 3\n    # return: B, N, 3, 3\n    # f = f.expand(-1, -1, 3)\n    # f = f.reshape(f.shape[0], -1, 3)\n    # v = v.expand(f.shape[0], -1, -1)\n    # v = v.reshape(f.shape[0], -1, 3)\n    # return torch.stack([v[..., f[..., 0]], v[..., f[..., 1]], v[..., f[..., 2]]], dim=-1)\n\n    # f: B, N, 3\n    # v: B, P, 3\n    # return: B, N, 3, 3\n    # f = f.expand(-1, -1, 3)\n    # f = f.reshape(f.shape[0], -1, 3)\n    # v = v.expand(f.shape[0], -1, -1)\n    # v = v.reshape(f.shape[0], -1, 3)\n    # return torch.stack([v[..., f[..., 0]], v[..., f[..., 1]], v[..., f[..., 2]]"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch[None]\n    else:  # numpy and others\n        batch = np.array([batch])\n    return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H = self.H\n        batch.W = self.W\n        batch.K = self.K\n        batch.R = self.R\n        batch.T = self.T\n        batch.n = self.n\n        batch.f = self.f\n        batch.t = self.t\n        batch.v = self.v\n        batch.bounds = self.bounds\n\n        batch.meta = dotdict()\n        batch.meta.H = self.H\n        batch.meta.W = self.W\n        batch.meta.K = self.K\n        batch.meta.R = self.R\n        batch.meta.T = self.T\n        batch.meta.n = self.n\n        batch.meta.f = self.f\n        batch.meta.t = self.t\n        batch.meta.v = self.v\n        batch.meta.bounds = self.bounds\n\n        batch.meta.origin = self.origin\n        batch.meta.world_up = self.world_up\n        batch.meta.movement_speed = self.movement_speed\n        batch.meta.movement_force = self.movement_force\n        batch.meta.drag_coeff_mult = self.drag_coeff_mult\n        batch.meta.constant_drag = self.constant_drag\n        batch.meta.mass = self.mass\n        batch.meta.moment_of_inertia = self.moment_of_inertia\n        batch.meta.movement_torque = self.movement_torque\n        batch.meta.angular_friction = self.angular_friction\n        batch.meta.constant_torque = self.constant_torque\n        batch.meta.min_interval = self.min_interval\n        batch.meta.pause_physics = self.pause_physics\n\n        return batch"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.to_dict(agent)"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if not isinstance(purpose_embedding, np.ndarray):\n                raise ValueError(\"Purpose embedding must be a numpy array\")\n\n            if len(self.agents) == 0:\n                return None, -np.inf\n\n            if len(self.agents) == 1:\n                return self.agents[0], 1.0\n\n            similarities = [cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0] for agent in self.agents]\n            max_similarity = max(similarities)\n            max_similarity_index = similarities.index(max_similarity)\n            return self.agents[max_similarity_index], max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")\n\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(PRIME_PROMPT, PRIME_NAME, 0, self, self.openai_wrapper)\n        prime_agent.weight = PRIME_AGENT_WEIGHT\n        prime_agent.prime = True\n        prime_agent.unspecified = True\n        self.agents.append(prime_agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error in saving agent: {e}\")\n            raise"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agent_lifecycle.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n            goal=goal,\n            examples=EXAMPLES,\n            system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n            sample_input=sample_input\n        )\n\n        try:\n            return self.openai_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"REPLACE INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n        return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"SELECT result FROM cache WHERE hash = ?\",\n            (arg_hash,)\n        )\n        result = cursor.fetchone()\n        if result is not None:\n            return json.loads(result[0])\n        return None"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT OR REPLACE INTO cache VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_config(args)\n\n    # If quiet mode is enabled, redirect the standard output to a file\n    if quiet_mode:\n        with open(\"output.txt\", \"w\") as f:\n            with redirect_stdout(f):\n                start_command_line(vars(args))\n    else:\n        start_command_line(vars(args))"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._last_time is None or time() - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n\n        return self._client"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise ValueError(\"The `state_dict` method should not be called from a DataLoader worker process.\")\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"shuffle\": self.shuffle,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n        }\n\n        if self.item_loader:\n            state[\"item_loader\"] = self.item_loader.state_dict()\n\n        return state"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        if self._state_dict is not None:\n            raise ValueError(\"The method `load_state_dict` should only be called once.\")\n\n        self._validate_state_dict(state_dict)\n\n        self._state_dict = state_dict\n\n        self.current_epoch = state_dict[\"current_epoch\"]\n        self.input_dir.path = state_dict[\"input_dir_path\"]\n        self.input_dir.url = state_dict[\"input_dir_url\"]\n        self.shuffle = state_dict[\"shuffle\"]\n        self.seed = state_dict[\"seed\"]\n        self.drop_last = state_dict[\"drop_last\"]\n        self.distributed_env.world_size = state_dict[\"world_size\"]\n\n        if self.item_loader:\n            self.item_loader.load_state_dict(state_dict[\"item_loader\"])\n\n        self.worker_env = _WorkerEnv.detect()\n        self.cache = self._create_cache(worker_env=self.worker_env)\n        self.shuffler = self._create_shuffler(self.cache)\n\n        self.num_chunks = state_dict[\"num_workers\"]\n        self.chunk_index = state_dict[\"num_samples_yielded\"] // state_dict[\"batch_size\"]\n\n        workers_chunks, workers_intervals = _associate_chunks_to_workers(\n            num_workers=state_dict[\"num_workers\"],\n            worker_env=self.worker_env,\n            chunks_replica=self.cache._reader.chunks_indexes,\n            intervals_replica=self.cache._reader.chunks_intervals,\n        )\n\n        self.worker_chunks = workers_chunks[self.worker_env.rank]\n        self.worker_intervals = workers_intervals[self.worker_"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            raise ValueError(\"The state_dict is not set.\")\n\n        state: Dict[str, Any] = self._state_dict\n\n        if self.worker_env is None:\n            self.worker_env = _WorkerEnv.detect()\n\n        if self.cache is None:\n            self.cache = self._create_cache(worker_env=self.worker_env)\n\n        if self.shuffler is None:\n            self.shuffler = self._create_shuffler(self.cache)\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The input_dir_path in the state_dict is different from the current input_dir_path. \"\n                f\"Expected: {self.input_dir.path}, found: {state['input_dir_path']}\"\n            )\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The input_dir_url in the state_dict is different from the current input_dir_url. \"\n                f\"Expected: {self.input_dir.url}, found: {state['input_dir_url']}\"\n            )\n\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\n                f\"The num_workers in the state_dict is different from the current num_workers. \"\n                f\"Expected: {self.worker_env.world_size}, found: {state['num_workers']}\"\n            )\n\n        if state[\"batch_size\"] != self.worker_env.batch_size:\n            raise ValueError(\n                f\"The batch_size in the state_dict is different from the current batch_size. \"\n                f\"Expected: {self.worker_env.batch_size}, found: {state['batch_size']}\"\n            )\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(\n                f\"The seed in the state"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hashlib.md5(input_dir.encode()).hexdigest())\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    return cache_dir"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"The provided remote_filepath doesn't use the 's3' scheme: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        parsed_url = parse.urlparse(remote_filepath)\n        bucket = parsed_url.netloc\n        key = parsed_url.path.lstrip(\"/\")\n\n        if self._s5cmd_available:\n            self._download_file_with_s5cmd(bucket, key, local_filepath)\n        else:\n            self._download_file_with_boto3(bucket, key, local_filepath)\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {}\n    workers_intervals = {}\n\n    for worker_idx in range(num_workers):\n        workers_chunks[worker_idx] = []\n        workers_intervals[worker_idx] = []\n\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_idx = i % worker_env.world_size\n        workers_chunks[worker_idx].append(chunk_index)\n        workers_intervals[worker_idx].append(chunk_interval)\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if not remote_filepath.startswith(\"local:\"):\n            super().download_file(remote_filepath, local_filepath)\n            return\n\n        remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        idx = 3 * 4\n        width, height, mode_size = np.uint32(item.width), np.uint32(item.height), np.uint32(len(item.mode))\n        mode = item.mode.encode(\"utf-8\")\n        size = width, height\n        raw = item.tobytes()\n        return (\n            width.tobytes()\n            + height.tobytes()\n            + mode_size.tobytes()\n            + mode\n            + raw[idx:],\n            None,\n        )"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            if os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                raise FileNotFoundError(f\"{item.filename} does not exist.\")\n        elif isinstance(item, Image.Image):\n            with io.BytesIO() as f:\n                item.save(f, format=\"JPEG\")\n                return f.getvalue(), None\n        else:\n            raise TypeError(f\"{type(item)} is not supported.\")\n\n    "}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        width, height, mode_size = np.frombuffer(data[0:12], np.uint32)\n        mode = data[12:12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size:]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        # deserialize the shape header\n        # Note: The start position of the shape value: 8 (dtype + shape length) + 4 * shape_idx\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n\n        # deserialize the numpy array bytes\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        try:\n            return pil_to_tensor(Image.open(io.BytesIO(data)))\n        except Exception:\n            raise TypeError(f\"The provided item should be of type {JpegImageFile}. Found {data}.\")"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice: int = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_tensor:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        assert self._dtype\n        return torch.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n\n    "}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\"torchvision is not installed.\")\n        if not _AV_AVAILABLE:\n            raise ImportError(\"av is not installed.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.seek(0)\n            video, _, _ = torchvision.io.read_video(f.name)\n        return video"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return []\n\n        # Write the remaining chunks\n        while self._should_write():\n            self.write_chunk()\n\n        # Write the index file\n        self.write_chunks_index()\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        # Return the list of written chunk file paths\n        return [os.path.join(self._cache_dir, self.get_chunk_filename())]"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.restore = True\n\n    "}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            return {}\n\n        if num_samples_yielded is not None:\n            return self._iterator.state_dict(num_workers, batch_size)\n\n        return {\n            \"dataset\": _state_dict(self._datasets, self._num_samples_yielded, num_workers, batch_size),\n            \"num_samples_yielded\": self._num_samples_yielded,\n        }"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        for dataset_idx, dataset in enumerate(self._datasets):\n            dataset.load_state_dict(state_dict[str(dataset_idx)])\n\n        if self._use_streaming_dataloader:\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if os.path.isabs(dir_path):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"studio://\"):\n        return _resolve_studio(dir_path, None, None)\n\n    if dir_path.startswith(\"s3-connections://\"):\n        return _resolve_s3_connections(dir_path)\n\n    if dir_path.startswith(\"datasets://\"):\n        return _resolve_datasets(dir_path)\n\n    if dir_path.startswith(\"project://\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"~\"):\n        return Dir(path=os.path.expanduser(dir_path))\n\n    return Dir(path=dir_path)"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    prefix = obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\"\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=prefix,\n    )\n\n    # No files are found in this folder\n    if objects[\"KeyCount\"] == 0:\n        return\n\n    # Check the index file exists\n    try:\n        s3.head_object(Bucket=obj.netloc, Key=os.path.join(prefix, \"index.json\"))\n        has_index_file = True\n    except botocore.exceptions.ClientError:\n        has_index_file = False\n\n    if has_index_file:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an optimized immutable datasets.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    if append or overwrite:\n        raise NotImplementedError(\n            \"The append and overwrite options are not implemented yet. Please use the `output_dir` with your own versioning as a suffix.\"\n        )\n\n    bucket_name = obj.netloc\n    s3 = boto3.resource(\"s3\")\n    for obj in s3.Bucket(bucket_name).objects.filter(Prefix=prefix):\n        s3.Object(bucket_name, obj.key).delete()"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if \"Contents\" not in objects:\n        return\n\n    for obj in objects[\"Contents\"]:\n        if obj[\"Key\"].endswith(\"index.json\"):\n            raise RuntimeError(\n                f\"The provided output_dir `{output_dir.path}` already contains an index file named 'index.json'.\"\n                \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n            )\n\n    # Delete all objects within the specified prefix in the bucket.\n    for obj in objects[\"Contents\"]:\n        s3.delete_object(Bucket=obj[\"Bucket\"], Key=obj[\"Key\"])"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = self.rank\n\n        if node_rank != 0:\n            while True:\n                files = os.listdir(self._cache_dir)\n                index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n                if len(index_files) == num_workers:\n                    break\n                sleep(0.1)\n\n            self._merge_no_wait(node_rank)\n\n        else:\n            while True:\n                files = os.listdir(self._cache_dir)\n                index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n                if len(index_files) == num_workers:\n                    break\n                sleep(0.1)\n\n            self._merge_no_wait()"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\n            \"The Lightning SDK is not installed. Please install it with `pip install lightning-sdk`.\"\n        )\n\n    if machine is None:\n        machine = Machine(cpu=1, gpu=0, memory=\"1Gi\")\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run app {os.getcwd()} --env-vars {os.environ}\"\n\n    job = Studio.execute(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n    )\n\n    job_url = job.job_url\n\n    print(f\"Job URL: {job_url}\")\n\n    while job.status != \"succeeded\":\n        if job.status == \"failed\":\n            raise RuntimeError(f\"Job failed with error: {job.error}\")\n        sleep(1)\n        job = Studio.get_job(job_url)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n\n    "}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config is not None:\n            return self._config\n\n        # Load the config containing the index\n        index_filepath = os.path.join(self._cache_dir, \"index.json\")\n        if not os.path.exists(index_filepath):\n            return None\n\n        self._config = ChunksConfig.load(\n            index_filepath,\n            self._cache_dir,\n            self._remote_input_dir,\n            self._compression,\n            self._serializers,\n            self._item_loader,\n        )\n        return self._config"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index should be an instance of ChunkedIndex.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index from the index\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Get the chunk filepath\n        chunk_filepath, _, _ = self.config[index]\n\n        # Check if the chunk is already loaded\n        if chunk_index not in self._item_loader.loaded_chunks:\n            # Load the chunk\n            self._item_loader.load_chunk(chunk_index, chunk_filepath)\n\n        # Get the item from the chunk\n        item = self._item_loader.get_item(chunk_index, index.index)\n\n        # Delete the chunk if it is the last one\n        if chunk_index == self._last_chunk_index:\n            self._last_chunk_index = None\n            self._item_loader.delete(chunk_index, chunk_filepath)\n\n        return item"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is None:\n        return obj\n\n    distributed_map = _ImmutableDistributedMap()\n    return distributed_map.set_and_get(key, obj)"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # 1. Get the number of chunks per rank\n    num_chunks_per_rank = [len(chunks) for chunks in chunks_per_ranks]\n\n    # 2. Calculate the number of chunks per rank for each node\n    num_chunks_per_rank_per_node = [\n        num_chunks_per_rank[i] // distributed_env.world_size\n        + num_chunks_per_rank[i] % distributed_env.world_size\n        if i == distributed_env.world_size - 1\n        else num_chunks_per_rank[i] // distributed_env.world_size\n        for i in range(distributed_env.world_size)\n    ]\n\n    # 3. Generate random permutations for each node\n    np.random.seed(seed + current_epoch)\n    permutations = [\n        np.random.permutation(num_chunks_per_rank_per_node[i])\n        for i in range(distributed_env.world_size)\n    ]\n\n    # 4. Flatten the permutations and return the result\n    return [\n        chunks_per_ranks[i][permutations[i][j]]\n        for i in range(distributed_env.world_size)\n        for j in range(num_chunks_per_rank_per_node[i])\n    ]"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) == 1:\n        return os.path.dirname(list(indexed_paths.values())[0])\n\n    if len(indexed_paths) == 2:\n        first_path = list(indexed_paths.values())[0]\n        second_path = list(indexed_paths.values())[1]\n\n        if os.path.dirname(first_path) == os.path.dirname(second_path):\n            return os.path.dirname(first_path)\n\n    raise ValueError(\n        f\"The provided inputs should contain at least a valid file path. Found {inputs}.\"\n        \" HINT: You can either provide a single file path or a sequence of file paths.\"\n    )"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if not _IS_IN_STUDIO:\n        yield\n        return\n\n    with open(\"/etc/resolv.conf\") as f:\n        lines = f.readlines()\n\n    write_lines = []\n    for line in lines:\n        if \"nameserver 127\" in line:\n            if enable:\n                write_lines.append(\"nameserver 127.0.0.1\\n\")\n            else:\n                write_lines.append(\"nameserver 127.0.0.53\\n\")\n        else:\n            write_lines.append(line)\n\n    with open(\"/etc/resolv.conf\", \"w\") as f:\n        for line in write_lines:\n            f.write(line)\n\n    try:\n        yield\n    finally:\n        with open(\"/etc/resolv.conf\") as f:\n            lines = f.readlines()\n\n        write_lines = []\n        for line in lines:\n            if \"nameserver 127\" in line:\n                if enable:\n                    write_lines.append(\"nameserver 127.0.0.53\\n\")\n                else:\n                    write_lines.append(\"nameserver 127.0.0.1\\n\")\n            else:\n                write_lines.append(line)\n\n        with open(\"/etc/resolv.conf\", \"w\") as f:\n            for line in write_lines:\n                f.write(line)"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    \"\"\"\n    This function distributes chunks and their corresponding intervals across different ranks in a distributed environment. It calculates the number of items each rank should process based on the total items and the world size of the distributed environment, taking into account whether to drop the last items or not. Then, it assigns chunks and their intervals to each rank accordingly.\n\n    Input-Output Arguments\n    :param distributed_env: _DistributedEnv. The distributed environment configuration, including the world size.\n    :param indexes: Any. A list or array of chunk indexes that need to be distributed among the ranks.\n    :param chunk_intervals: Any. A list or array of tuples, where each tuple represents the start and end of a chunk interval.\n    :param drop_last: Bool. A flag indicating whether to drop the last items to make the distribution even across all ranks.\n    :return: Tuple[List[List[int]], List[Any]]. A tuple containing two elements. The first element is a list of lists, where each sublist contains the chunk indexes assigned to each rank. The second element is a list of lists of lists, where each sublist contains the intervals of chunks assigned to each rank.\n    \"\"\"\n\n    \"\"\"\n    This function distributes chunks and their corresponding intervals across different ranks in a distributed environment. It calculates the number of items each rank should process based on the total items and the world size of the distributed environment, taking into account whether to drop the last items or not. Then, it assigns chunks and their intervals to each rank accordingly.\n\n    Input-Output Arguments\n    :param distributed_env: _DistributedEnv. The distributed environment configuration, including the world size.\n    :param indexes: Any. A list or array of chunk indexes that need to be distributed among the ranks.\n    :param chunk_intervals: Any. A list or array of tuples, where each tuple represents the start and end of a chunk interval.\n    :param drop_last: Bool. A flag indicating whether to drop the last items to make the distribution even across all ranks.\n    :return: Tuple[List[List[int]], List[Any]]. A tuple containing two elements. The first element is a list of lists, where each"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            self._find_device()\n\n        if self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last, device=self._device)\n        else:\n            self._fn(item_metadata, output_dir, device=self._device)"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            s3.client.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n                continue\n            raise e"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir="}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \"Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to process your data faster \"\n            \"using multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url is not None and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    while True:\n        # 1. Collect the task\n        index, paths = queue_in.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and download them sequentially.\n        for path in paths:\n            if input_dir:\n                if not path.startswith(input_dir.path) and input_dir.path is not None:\n                    path = path.replace(input_dir.path, cache_dir)\n\n                if os.path.exists(path):\n                    continue\n\n                if input_dir.url:\n                    obj = parse.urlparse(path)\n                    if obj.scheme == \"s3\":\n                        s3 = S3Client()\n                        _wait_for_file_to_exist(s3, obj)\n                        s3.client.download_file(obj.netloc, obj.path.lstrip(\"/\"), path)\n                    elif input_dir.path and os.path.isdir(input_dir.path):\n                        shutil.copyfile(path, path.replace(input_dir.path, cache_dir))\n\n            elif os.path.exists(path) and \"s3_connections\" not in path:\n                os.makedirs(os.path.dirname(path), exist_ok=True)\n                shutil.copyfile(path, path.replace(input_dir.path, cache_dir))\n\n        # 4. Signal the completion of the task\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    while True:\n        # 1. Fetch from the queue\n        r: Optional[Union[str, Tuple[str, str]]] = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if r is None:\n            remove_queue.put(None)\n            return\n\n        # 3. Unpack\n        if isinstance(r, str):\n            local_path = r\n            if input_dir.path:\n                local_path = local_path.replace(input_dir.path, cache_dir)\n\n            if input_dir.url and input_dir.path:\n                local_path = local_path.replace(input_dir.path, input_dir.url)\n\n            obj = parse.urlparse(local_path)\n\n            if obj.scheme == \"s3\":\n                dirpath = os.path.dirname(local_path)\n\n                os.makedirs(dirpath, exist_ok=True)\n\n                with open(local_path, \"wb\") as f:\n                    s3.client.download_fileobj(obj.netloc, obj.path.lstrip(\"/\"), f)\n\n            elif os.path.isfile(local_path):\n                if not local_path.startswith(\"/teamspace/studios/this_studio\"):\n                    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n                    shutil.copyfile(local_path, local_path)\n            else:\n                raise ValueError(f\"The provided {input_dir.url} isn't supported.\")\n\n        elif isinstance(r, tuple):\n            local_dir, local_path = r\n            if input_dir.path:\n                local_path = local_path.replace(input_dir.path, cache_dir)\n\n            if input_dir.url and input_dir.path:\n                local_path = local_path.replace(input_dir.path, input_dir.url)\n\n            obj = parse.urlparse(local_path)\n\n            if obj."}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if file_size:\n        print(f\"Worker {str(node_rank)} is distributing {len(worker_items)} items to {num_workers} workers.\")\n        for i, (worker_id, worker_item) in enumerate(zip(worker_ids_this_node, worker_items)):\n            print(f\"Worker {str(worker_id)} is assigned {len(worker_item)} items.\")\n    else:\n        print(f\"Worker {str(node_rank)} is distributing {len(worker_items)} items to {num_workers} workers.\")\n        for i, (worker_id, worker_item) in enumerate(zip(worker_ids_this_node, worker_items)):\n            print(f\"Worker {str(worker_id)} is assigned {worker_weights[i]} items.\")\n\n    # Shuffle the items to avoid having the same items on the same worker.\n    for worker_id, worker_item in zip(worker_ids_this_node, worker_items):\n        random.Random(worker_id).shuffle(worker_item)\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n    world_size = num_nodes * num_workers\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // world_size\n    num_items_per_worker_remainder = len(user_items) % world_size\n\n    # Distribute the remainder among the last workers\n    num_items_per_worker_remainder_per_worker = num_items_per_worker_remainder // num_workers\n    num_items_per_worker_remainder_per_worker_remainder = num_items_per_worker_remainder % num_workers\n\n    # Calculate the start and end indices for each worker's items\n    worker_start_indices = np.cumsum([num_items_per_worker] * num_workers)\n    worker_end_indices = np.cumsum([num_items_per_worker] * num_workers) + num_items_per_worker_remainder_per_worker\n\n    # Distribute the remainder among the last workers\n    worker_start_indices[-num_items_per_worker_remainder_per_worker_remainder:] += (\n        num_items_per_worker_remainder_per_worker + 1\n    )\n    worker_end_indices[-num_items_per_worker_remainder_per_worker_remainder:] += (\n        num_items_per_worker_remainder_per_worker + 1\n    )\n\n    # Ensure the output list has a length equal to the number of workers\n    if len(worker_start_indices) != num_workers:\n        raise RuntimeError(\n            f\"The output list has a length of {len(worker_start_indices)} which is not equal to the number of workers {num_workers}.\"\n        )\n\n    # Create a list of lists, where each sublist contains the items assigned to a worker\n    worker_items"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        cache_dir = _get_cache_dir()\n\n        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir, ignore_errors=True)\n\n        os.makedirs(cache_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    return [future.result() for future in futures]\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "class BaseWorker:\n    def __init__(\n        self,\n        worker_index: int,\n        num_workers: int,\n        node_rank: int,\n        "}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons > 1024:\n                raise ValueError(\"tcnn does not support more than 1024 neurons\")\n            return tcnn.Network(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                n_neurons=n_neurons,\n                n_hidden_layers=n_layers,\n                activation=activation,\n                output_activation=output_activation,\n                seed=self._get_seed(),\n            )\n        else:\n            model_list = []\n            in_features = n_input_dims\n            for i in range(n_layers):\n                model_list.extend(self._get_torch_layer(\n                    in_features=in_features,\n                    out_features=n_neurons,\n                    activation_name=activation,\n                ))\n                in_features = n_neurons\n            model_list.extend(self._get_torch_layer(\n                in_features=in_features,\n                out_features=n_output_dims,\n                activation_name=output_activation,\n            ))\n            return nn.Sequential(*model_list)"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the length of the signal and the total number of shifts\n        signal_length = len(signal)\n        num_shifts = 2 * kernel_offset + 1\n\n        # Initialize an empty array to store the shifted signals\n        shifted_signals = np.zeros((num_shifts, signal_length))\n\n        # Shift the signal by the specified kernel offset and store the shifted signals in the shifted_signals array\n        for i in range(num_shifts):\n            shifted_signals[i] = np.roll(signal, i - kernel_offset)\n\n        # Calculate the rolling median by taking the median of each row of the shifted_signals array\n        rolling_median = np.median(shifted_signals, axis=0)\n\n        # Trim the rolling median to remove edge effects introduced by the shifting process\n        trimmed_median = rolling_median[kernel_offset:-kernel_offset]\n\n        return trimmed_median"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Check if the input templates have the same size\n    if template_probe.iris_size != template_gallery.iris_size:\n        raise MatcherError(\"Templates have different sizes\")\n\n    # Check if the input templates have the same number of iris codes\n    if template_probe.iris_codes != template_gallery.iris_codes:\n        raise MatcherError(\"Templates have different number of iris codes\")\n\n    # Check if the input templates have the same number of mask codes\n    if template_probe.mask_codes != template_gallery.mask_codes:\n        raise MatcherError(\"Templates have different number of mask codes\")\n\n    # Check if the input templates have the same number of iris codes\n    if template_probe.iris_codes != template_gallery.iris_codes:\n        raise MatcherError(\"Templates have different number of iris codes\")\n\n    # Check if the input templates have the same number of mask codes\n    if template_probe.mask_codes != template_gallery.mask_codes:\n        raise MatcherError(\"Templates have different number of mask codes\")\n\n    # Check if the input templates have the same number of iris codes\n    if template_probe.iris_codes != template_gallery.iris_codes:\n        raise MatcherError(\"Templates have different number of iris codes\")\n\n    # Check if the input templates have the same number of mask codes\n    if template_probe.mask_codes != template_gallery.mask_codes:\n        raise MatcherError(\"Templates have different number of mask codes\")\n\n    # Check if the input templates have the same number of iris codes\n    if template_probe.iris_codes != template_gallery.iris_codes:\n        raise MatcherError(\"Templates have different number of iris codes\")\n\n    # Check if the input templates have the same number of mask codes\n    if template_probe.mask_codes != template_gallery.mask_codes:\n        raise MatcherError(\"Templates have different"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        # Initialize the arrays to store the bisectors\n        first_bisectors_point = np.zeros((num_bisectors, 2))\n        second_bisectors_point = np.zeros((num_bisectors, 2))\n\n        # Iterate over the number of bisectors\n        for i in range(num_bisectors):\n            # Initialize the iteration counter\n            iteration = 0\n\n            # Generate random indices for the polygon vertices\n            indices = np.random.choice(len(polygon), size=2, replace=False)\n\n            # Calculate the perpendicular bisector of the randomly chosen points\n            bisector = self._calculate_perpendicular_bisector(polygon[indices[0]], polygon[indices[1]])\n\n            # Check if the distance between the chosen points is greater than the minimum distance\n            while np.linalg.norm(polygon[indices[0]] - polygon[indices[1]]) < min_distance_between_sector_points_in_px:\n                # If the distance is less than the minimum, generate new random indices\n                indices = np.random.choice(len(polygon), size=2, replace=False)\n\n                # Calculate the perpendicular bisector of the new points\n                bisector = self._calculate_perpendicular_bisector(polygon[indices[0]], polygon[indices[1]])\n\n                # Increment the iteration counter\n                iteration += 1\n\n                # Check if the maximum number of iterations has been reached\n                if iteration > max_iterations:\n                    raise EyeCentersEstimationError(\n                        f\"Failed to find a sufficient number of point pairs that meet the distance criterion within {max_iterations} iterations.\"\n                    )\n\n            # Store the bisector points\n            first_bisectors_point[i] = bisector[0]\n            second_bisectors_point[i] = bisector[1]\n\n        return first_bisectors_point, second"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.post_execute(self, *args, **kwargs)\n\n        return result"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n            return self.check_type(deserialized_output, type_definition)\n        except json.JSONDecodeError:\n            return False\n\n    "}
{"namespace": "tanuki.register.Register.load_function_description", "completion": ""}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        if bit_array is None:\n            logging.warning(\"BloomFilter: No bit array found in persistence, reinitializing\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        elif len(bit_array) != self.size:\n            logging.warning(\"BloomFilter: Bit array length mismatch, reinitializing\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        for key, value in json_dict.items():\n            if key == \"distilled_model\":\n                self.distilled_model = config_factory.create_model_config(value)\n            elif key == \"current_model_stats\":\n                self.current_model_stats = value\n            elif key == \"last_training_run\":\n                self.last_training_run = value\n            elif key == \"current_training_run\":\n                self.current_training_run = value\n            elif key == \"nr_of_training_runs\":\n                self.nr_of_training_runs = value\n            elif key == \"teacher_models\":\n                self.teacher_models = [config_factory.create_model_config(teacher_model) for teacher_model in value]\n        return self\n    "}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # validate the parameters\n        for param in LLM_GENERATION_PARAMETERS:\n            if param in kwargs:\n                if not isinstance(kwargs[param], (int, float)):\n                    raise ValueError(f\"{param} must be a number\")\n\n        # create the request body\n        request_body = {\n            \"model\": model.model_name,\n            \"messages\": [\n                {\"role\": \"system\", \"content\": system_message},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n            **kwargs\n        }\n\n        # retry up to 5 times with exponential backoff\n        for i in range(5):\n            try:\n                response = requests.post(OPENAI_URL, json=request_body, headers={\"Authorization\": f\"Bearer {self.api_key}\"})\n                response.raise_for_status()\n                response_json = response.json()\n                if \"error\" in response_json:\n                    raise ValueError(response_json[\"error\"])\n                return self.process_response(response_json, model)\n            except requests.exceptions.RequestException as e:\n                logging.error(f\"Request failed: {e}\")\n                time.sleep(2 ** i)\n\n        raise ValueError(\"Failed to generate response after 5 retries\")\n"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal must be zero\")"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the model\n        model = self.choose_model(args, kwargs, function_description, llm_parameters)\n        # get the prompt\n        prompt = self.construct_prompt(function_description, args, kwargs, model)\n        # get the save to finetune\n        save_to_finetune = self.suitable_for_finetuning(args, kwargs, function_description, model)\n        # get the is distilled model\n        is_distilled_model = self.is_distilled_model(args, kwargs, function_description, model)\n        return prompt, model, save_to_finetune, is_distilled_model\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Higham & Nick (2002) algorithm\n        cov_nearest = cov.copy()\n        for _ in range(higham_max_iteration):\n            try:\n                np.linalg.cholesky(cov_nearest)\n                break\n            except np.linalg.LinAlgError:\n                # Compute the eigenvalues and eigenvectors of the covariance matrix\n                eigvals, eigvecs = np.linalg.eigh(cov_nearest)\n                # Clip the eigenvalues to ensure they are positive\n                eigvals = np.maximum(eigvals, _CLIPPING_VALUE)\n                # Reconstruct the covariance matrix from the clipped eigenvalues and eigenvectors\n                cov_nearest = eigvecs @ np.diag(eigvals) @ eigvecs.T\n        return cov_nearest\n    else:\n        # Clip the eigenvalues to ensure they are positive\n        eigvals, eigvecs = np.linalg.eigh(cov)\n        eigvals = np.maximum(eigvals, _CLIPPING_VALUE)\n        # Reconstruct the covariance matrix from the clipped eigenvalues and eigenvectors\n        cov_nearest = eigvecs @ np.diag(eigvals) @ eigvecs.T\n        return cov_nearest"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    elif isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    elif isinstance(obj, dict):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, (Boxes, ROIMasks)):\n        return TensorWrapSchema.flatten(obj)\n    else:\n        raise ValueError(f\"Unsupported type {type(obj)}\")"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    if groups.ndim != 2:\n        raise EquationToMatrixError(\n            f\"{names[0]} should be a 2D array, but it is a {groups.ndim}D array\"\n        )\n    if equations.ndim != 1:\n        raise EquationToMatrixError(\n            f\"{names[1]} should be a 1D array, but it is a {equations.ndim}D array\"\n        )\n    if groups.shape[1] != equations.shape[0]:\n        raise EquationToMatrixError(\n            f\"{names[0]} and {names[1]} should have the same number of assets, but\"\n            f\" {groups.shape[1]} != {equations.shape[0]}\"\n        )\n    if not isinstance(sum_to_one, bool):\n        raise EquationToMatrixError(\n            f\"sum_to_one should be a boolean, but it is a {type(sum_to_one)}\"\n        )\n\n    n_equations = equations.shape[0]\n    n_assets = groups.shape[1]\n    left = np.zeros((n_equations, n_assets))\n    right = np.zeros(n_equations)\n    for i, equation in enumerate(equations):\n        try:\n            l, r = _string_to_equation(groups, equation, sum_to_one)\n        except GroupNotFoundError as e:\n            if raise_if_group_missing:\n                raise e\n            warnings.warn(str(e))\n            continue\n        left[i, :] = l\n        right[i] = r\n\n    return left, right"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n\n    cls_name, cls_def = _gen_instance_module(fields)\n\n    with tempfile.NamedTemporaryFile(suffix=\".py\", delete=False) as f:\n        f.write(cls_def.encode(\"utf-8\"))\n        f.flush()\n        path = f.name\n\n    with ExitStack() as stack:\n        stack.enter_context(patch_builtin_len())\n        stack.enter_context(mock.patch(\"detectron2.structures.Instances\", cls_name))\n        stack.enter_context(mock.patch(\"detectron2.structances.Instances\", cls_name))\n        stack.enter_context(mock.patch(\"detectron2.modeling.roi_heads.Instances\", cls_name))\n        stack.enter_context(mock.patch(\"detectron2.modeling.roi_heads.fast_rcnn.Instances\", cls_name))\n        stack.enter_context(mock.patch(\"detectron2.modeling.roi_heads.mask_head.Instances\", cls_name))\n        stack.enter_context(mock.patch(\"detectron2.modeling.roi_heads.keypoint_head.Instances\", cls_name))\n        stack.enter_context(mock.patch(\"detectron2.modeling.proposal_generator.Instances\", cls_name))\n        stack.enter_context(mock.patch(\"detectron2.modeling.proposal_generator.rpn.Instances\", cls_name))\n        stack.enter_context(mock.patch(\"detectron2.modeling.proposal_generator.rcnn.Instances\", cls_name))\n        stack.enter_context(mock.patch(\"detectron2.modeling.proposal_generator.fast_rcnn.Instances\", cls_name))\n        stack.enter_context(mock.patch(\"detectron2.modeling.proposal_generator.mask_rcnn.Instances"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n    return image"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # Transform bounding box\n    bbox = annotation[\"bbox\"]\n    bbox = transforms.apply_box([bbox])[0]\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform segmentation\n    if \"segmentation\" in annotation:\n        segm = annotation[\"segmentation\"]\n        if isinstance(segm, list):\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            if isinstance(segm[0], list):\n                # polygon\n                mask = PolygonMasks(segm)\n            else:\n                # mask\n                mask = mask_util.frPyObjects(segm, *image_size)\n            mask = mask.crop_and_resize(transforms, *image_size)\n            # We expand the mask because cropping might cause too many\n            # small regions to merge back together.\n            mask = mask.resize((image_size[0], image_size[1]))\n            # Some heuristic for removing too small regions\n            mask = mask > 0.5\n            # Use RLE to encode the binary mask\n            annotation[\"segmentation\"] = mask_util.encode(np.asfortranarray(mask))\n        elif isinstance(segm, dict):\n            # RLE\n            segm = transforms.apply_segmentation(segm)\n            annotation[\"segmentation\"] = segm\n        else:\n            raise ValueError(\"Unknown segmentation format: {}\".format(segm))\n\n    # Transform keypoints\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        keypoints = transform_keypoint_annotations(\n            keypoints, transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        return cv2.transform(coords, self.rm_coords)"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    return _wrapper_count_operators(model=model, inputs=inputs, mode=FLOPS_MODE)"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if len(img.shape) > 2 and img.shape[2] == 1:\n            img = img[:, :, 0]\n        if len(img.shape) > 2:\n            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n        if self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        if isinstance(predictions, torch.Tensor):\n            predictions = predictions.to(self.cpu_device)\n        if isinstance(predictions, dict):\n            predictions = [predictions]\n\n        if self._instance_mode == ColorMode.IMAGE_BW:\n            self.output.reset_image(self._create_grayscale_image(predictions[0][\"pred_masks\"]))\n\n        for pred in predictions:\n            if \"pred_masks\" in pred:\n                pred_masks = pred[\"pred_masks\"]\n                pred_masks = pred_masks.numpy()\n                pred_masks = [GenericMask(x, self.output.height, self.output.width) for x in pred_masks]\n            else:\n                pred_masks = None\n\n            if \"pred_boxes\" in pred:\n                boxes = pred[\"pred_boxes\"].tensor.numpy()\n            else:\n                boxes = None\n\n            if \"pred_classes\" in pred:\n                classes = pred[\"pred_classes\"].numpy()\n            else:\n                classes = None\n\n            if \"scores\" in pred:\n                scores = pred[\"scores\"].numpy()\n            else:\n                scores = None\n\n            if \"pred_keypoints\" in pred:\n                keypoints = pred[\"pred_keypoints\"].tensor.numpy()\n            else:\n                keypoints = None\n\n            labels = _create_text_labels(classes, scores, self.metadata.thing_classes)\n            self.overlay_instances(\n                boxes=boxes,\n                masks=pred_masks,\n                labels=labels,\n                keypoints=keypoints,\n            )\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert the image from RGBA to RGB format\n        img = self.canvas.buffer_rgba()\n        img = np.array(img)\n        img = img[:, :, :3]\n        return img"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            annotations = dic[\"annotations\"]\n            if \"segmentation\" in annotations:\n                self.draw_sem_seg(annotations[\"segmentation\"])\n            if \"keypoints\" in annotations:\n                self.draw_keypoints(annotations[\"keypoints\"])\n            if \"bbox\" in annotations:\n                self.draw_box(annotations[\"bbox\"])\n\n        if \"sem_seg_file_name\" in dic:\n            with PathManager.open(dic[\"sem_seg_file_name\"], \"rb\") as f:\n                sem_seg = Image.open(f)\n                sem_seg = np.asarray(sem_seg, dtype=\"uint8\")\n            self.draw_sem_seg(sem_seg)\n\n        if \"panoptic_seg_file_name\" in dic:\n            with PathManager.open(dic[\"panoptic_seg_file_name\"], \"rb\") as f:\n                panoptic_seg = Image.open(f)\n                panoptic_seg = np.asarray(panoptic_seg, dtype=\"uint32\")\n            segments_info = dic.get(\"segments_info\", None)\n            self.draw_panoptic_seg(panoptic_seg, segments_info)\n\n        if \"instances\" in dic:\n            instances = dic[\"instances\"]\n            if \"pred_boxes\" in instances:\n                self.draw_instance_predictions(instances)\n            if \"pred_masks\" in instances:\n                self.draw_instance_predictions(instances)\n            if \"pred_keypoints\" in instances:\n                self.draw_instance_predictions(instances)\n\n        if \"pred_boxes\" in dic:\n            self.draw_instance_predictions(dic)\n\n        if \"pred_masks\" in dic:\n            self.draw_instance_predictions(dic)\n\n        if \"pred_keypoints\" in dic:\n            self.draw_instance_predictions(dic"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        color = mplc.to_rgb(color)\n\n        if isinstance(binary_mask, torch.Tensor):\n            binary_mask = binary_mask.numpy()\n        if isinstance(binary_mask, np.ndarray):\n            binary_mask = binary_mask.astype(\"uint8\")\n\n        if edge_color is None:\n            edge_color = self._change_color_brightness(color, brightness_factor=-0.7)\n        edge_color = mplc.to_rgb(edge_color) + (1,)\n\n        # use cv2 to find contours\n        contours, hierarchy = cv2.findContours(\n            binary_mask, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE\n        )\n        # filter out small contours\n        contours = [c for c in contours if cv2.contourArea(c) > area_threshold]\n        for c in contours:\n            self.output.ax.add_patch(\n                mpl.patches.Polygon(\n                    c.reshape(-1, 2),\n                    fill=True,\n                    facecolor=color + (alpha,),\n                    edgecolor=edge_color,\n                    linewidth=max(self._default_font_size // 15 * self.output.scale, 1),\n                )\n            )\n\n        if text is not None:\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            self._draw_text_in_mask(binary_mask, text, lighter_color)\n        return self.output"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), f\"{msg}Expect an Instances object, but got {type(input)}!\"\n    assert isinstance(other, Instances), f\"{msg}Expect an Instances object, but got {type(other)}!\"\n    assert input.image_size == other.image_size, f\"{msg}Expect image_size to be equal, but got {input.image_size} vs {other.image_size}\"\n    if size_as_tensor:\n        assert torch.equal(\n            torch.tensor(input.image_size), torch.tensor(other.image_size)\n        ), f\"{msg}Expect image_size to be equal, but got {input.image_size} vs {other.image_size}\"\n\n    for name in input._field_names:\n        val1 = getattr(input, \"_\" + name, None)\n        val2 = getattr(other, \"_\" + name, None)\n        if val1 is None and val2 is None:\n            continue\n        if val1 is None or val2 is None:\n            raise AssertionError(f\"{msg}Expect {name} to be equal, but got {val1} vs {val2}\")\n        if isinstance(val1, Boxes):\n            assert val1.tensor.shape == val2.tensor.shape, f\"{msg}Expect {name}.tensor to be equal, but got {val1.tensor.shape} vs {val2.tensor.shape}\"\n            assert torch.allclose(val1.tensor, val2.tensor), f\"{msg}Expect {name}.tensor to be equal, but got {val1.tensor} vs {val2.tensor}\"\n        elif isinstance(val1, ROIMasks):\n            assert val1.tensor.shape == val2.tensor.shape, f\"{msg}Expect {name}.tensor to be equal, but got {val1.tensor.shape} vs {val2.tensor.shape}\"\n            assert torch.allclose(val1.tensor, val2.tensor), f\"{msg}Expect {name}.tensor to be equal"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        widths = self.tensor[:, 2]\n        heights = self.tensor[:, 3]\n        return widths * heights"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    else:\n        return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        proposals = [p for p in proposals if p.has(\"gt_boxes\")]\n        if not len(proposals):\n            return {}\n\n        gt_boxes = cat([p.gt_boxes.tensor for p in proposals], dim=0)\n        gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n        loss_box_reg = self.box_reg_loss(\n            cat([p.proposal_boxes.tensor for p in proposals], dim=0),\n            gt_boxes,\n            proposal_deltas,\n            gt_classes,\n        )\n        loss_cls = cross_entropy(\n            scores,\n            gt_classes,\n            self.loss_weight.get(\"loss_cls\", 1.0),\n        )\n        _log_classification_stats(scores, gt_classes)\n        return {\n            \"loss_cls\": loss_cls,\n            \"loss_box_reg\": loss_box_reg,\n        }\n\n    "}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER.NAME\n    tracker_head = TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)\n    return tracker_head"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert deltas.shape[-1] % 4 == 0 and boxes.shape[-1] == 4\n\n        boxes = boxes.to(deltas.dtype).unsqueeze(2)\n\n        ctr_x = boxes[:, 0]\n        ctr_y = boxes[:, 1]\n        widths = boxes[:, 2]\n        heights = boxes[:, 3]\n\n        wx, wy, ww, wh = self.weights\n\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = dx * widths + ctr_x  # x_ctr\n        pred_boxes[:, 1::4] = dy * heights + ctr_y  # y_ctr\n        pred_boxes[:, 2::4] = torch.exp(dw) * widths  # width\n        pred_boxes[:, 3::4] = torch.exp(dh) * heights  # height\n\n        return pred_boxes"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            return self.general_ins(image)\n        else:\n            if isinstance(anno_type, str):\n                anno_type = [anno_type]\n            elif isinstance(anno_type, (list, tuple)):\n                pass\n            else:\n                raise Exception(f'Error anno_type: {anno_type}')\n\n        output = self.general_ins(image)\n        output_dict = {}\n        for tp in anno_type:\n            if tp in output.keys():\n                output_dict[tp] = output[tp]\n        if len(output_dict) == 0:\n            return output\n        elif len(output_dict) == 1:\n            return list(output_dict.values())[0]\n        else:\n            return output_dict"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        result = {}\n        for kw in normalize_string(query).split(\" \"):\n            result = update_url_scores(result, self.bm25(kw))\n        return result"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        box = self.tensor\n        x1 = box[:, 0] - box[:, 2] / 2.0\n        y1 = box[:, 1] - box[:, 3] / 2.0\n        x2 = box[:, 0] + box[:, 2] / 2.0\n        y2 = box[:, 1] + box[:, 3] / 2.0\n        x1, y1, x2, y2 = self._clip_boxes(x1, y1, x2, y2, box_size)\n        box[:, 0] = (x1 + x2) / 2.0\n        box[:, 1] = (y1 + y2) / 2.0\n        box[:, 2] = x2 - x1\n        box[:, 3] = y2 - y1\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        stats = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n        for item in self.data:\n            stats[item['type']] += 1\n        return stats"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return DETECTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    # "}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        box_cls = LiDARInstance3DBoxes\n        mode = 'lidar'\n    elif box_type == 'Camera':\n        box_cls = CameraInstance3DBoxes\n        mode = 'camera'\n    elif box_type == 'Depth':\n        box_cls = DepthInstance3DBoxes\n        mode = 'depth'\n    else:\n        raise ValueError(f'box_type: {box_type} is not supported.')\n\n    return box_cls, mode"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of strings')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n  "}
{"namespace": "ollama._client.Client._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      def upload_bytes():\n        with open(path, 'rb') as r:\n          while True:\n            chunk = r.read(32 * 1024)\n            if not chunk:\n              break\n            yield chunk\n\n      self._request('POST', f'/api/blobs/{digest}', content=upload_bytes())\n\n    return digest"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n            break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n        await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n        if e.status_code != 404:\n            raise\n\n        with open(path, 'rb') as r:\n            await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(user_code)\n            f.write(test_code)\n            f.flush()\n            combined_file = f.name\n\n        # Run Pyright on the combined file\n        result = subprocess.run(\n            [\"pyright\", combined_file],\n            capture_output=True,\n            text=True,\n        )\n\n        # Parse the Pyright output to extract the expected error messages and line numbers\n        expected_errors = []\n        expected_error_lines = set()\n        for line in result.stdout.splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_no = int(match.group(1))\n                message = match.group(2)\n                if cls.EXPECT_ERROR_COMMENT in message:\n                    expected_errors.append(message)\n                    expected_error_lines.add(line_no)\n\n        # Check if the type check passed\n        passed = result.returncode == 0 and not expected_errors\n\n        # Return the result\n        return TypeCheckResult(\n            message=\"\\n\".join(expected_errors),\n            passed=passed,\n            debug_info={\"pyright_output\": result.stdout},\n        )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if not (path or modelfile):\n      raise RequestError('must provide either path or modelfile')\n\n    if path:\n      modelfile = await self._parse_modelfile(await _as_path(path).read_text(), base=await _as_path(path).parent)\n    elif modelfile:\n      modelfile = await self._parse_modelfile(modelfile)\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_fn=get_compiler_fn(title=\"Module\"))\n    else:\n        return aot_function(fn, compiler_fn=get_compiler_fn(title=\"Function\"))"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    with open(os.path.join(trial_path, 'config.yaml'), 'r') as f:\n        try:\n            config_dict = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            logger.error(exc)\n            raise exc\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n    if output_path is not None:\n        with open(output_path, 'w') as f:\n            yaml.safe_dump(best_config, f)\n    return best_config"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    # Create a lock to ensure thread safety\n    lock = threading.Lock()\n\n    # Define a cache to store the traced modules\n    cache = {}\n\n    # Define a function to get the key for the cache\n    def get_cache_key(func, inputs, kwargs):\n        return (func, inputs, kwargs)\n\n    # Define a function to get the key for the cache\n    def get_cache_key(func, inputs, kwargs):\n        return (func, inputs, kwargs)\n\n    # Define a function to get the key for the cache\n    def get_cache_key(func, inputs, kwargs):\n        return (func, inputs, kwargs)\n\n    # Define a function to get the key for the cache\n    def get_cache_key(func, inputs, kwargs):\n        return (func, inputs, kwargs)\n\n    # Define a function to get the key for the cache\n    def get_cache_key(func, inputs, kwargs):\n        return (func, inputs, kwargs)\n\n    # Define a function to get the key for the cache\n    def get_cache_key(func, inputs, kwargs):\n        return (func, inputs, kwargs)\n\n    # Define a function to get the key for the cache\n    def get_cache_key(func, inputs, kwargs):\n        return (func, inputs, kwargs)\n\n    # Define a function to get the key for the cache\n    def get_cache_key(func, inputs, kwargs):\n        return (func, inputs, kwargs)\n\n    # Define a function to get the key for the cache\n    def get_cache_key(func, inputs, kwargs):\n        return (func, inputs, kwargs)\n\n    # Define a function to get the key for the cache\n    def get_cache_key(func, inputs, kwargs):\n        return (func, inputs, kwargs)\n\n    # Define a function to get the key for the cache\n    def get_cache_key(func, inputs, kwargs):"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config_yaml_path = os.path.join(trial_path, 'config.yaml')\n        with open(config_yaml_path, 'r') as f:\n            try:\n                config = yaml.safe_load(f)\n            except yaml.YAMLError as exc:\n                logger.error(exc)\n                raise exc\n        return cls(config, project_dir=os.path.dirname(trial_path))\n\n    "}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the directory for this node line if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Evaluate and select the best retrieval node result\n    result_df = evaluate_and_select_best_retrieval_node(modules, module_params, previous_result, node_line_dir, strategies)\n\n    return result_df\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    logger.info(\"Running query expansion node\")\n    os.makedirs(node_line_dir, exist_ok=True)\n    query_expansion_results = []\n    for module, params in zip(modules, module_params):\n        logger.info(f\"Running query expansion module {module.__name__} with params {params}\")\n        result = module(previous_result, **params)\n        query_expansion_results.append(result)\n    query_expansion_results = pd.concat(query_expansion_results)\n    query_expansion_results.to_csv(os.path.join(node_line_dir, \"query_expansion_results.csv\"), index=False)\n    query_expansion_results = query_expansion_results.groupby('queries').agg(list)\n    query_expansion_results = query_expansion_results.reset_index()\n    query_expansion_results['queries'] = query_expansion_results['queries'].apply(lambda x: x[0])\n    query_expansion_results = query_expansion_results.explode('queries')\n    query_expansion_results = query_expansion_results.reset_index(drop=True)\n    query_expansion_results.to_csv(os.path.join(node_line_dir, \"query_expansion_results.csv\"), index=False)\n    query_expansion_results = query_expansion_results.groupby('queries').agg(list)\n    query_expansion_results = query_expansion_results.reset_index()\n    query_expansion_results['queries'] = query_expansion_results['queries'].apply(lambda x: x[0])\n    query_expansion_results = query_expansion_results.explode('queries')\n    query_expansion_results = query_expansion_results.reset_index(drop=True)\n    query_expansion_results.to_csv(os.path.join(node_line_dir, \"query_expansion_results.csv\"), index=False)\n    query_expansion_results"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    validate_qa_dataset(previous_result)\n    os.makedirs(node_line_dir, exist_ok=True)\n    generator_funcs, generator_params = make_generator_callable_params(strategies)\n    prompts = previous_result['prompts'].tolist()\n    generation_gt = previous_result['generation_gt'].tolist()\n    metrics = strategies['metrics']\n    metrics = cast_metrics(metrics)\n    results = list(map(lambda x: evaluate_one_prompt_maker_node(generator_funcs, generator_params, prompts, generation_gt, metrics, node_line_dir),\n                       zip(modules, module_params)))\n    best_result, _ = select_best_average(results, strategies['metrics'])\n    best_result = pd.concat([previous_result, best_result], axis=1)\n    return best_result"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda node: extract_values(node, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model()\n\n    # Convert ground truth strings to embeddings\n    gt_embeddings = embedding_model.get_embeddings(generation_gt)\n\n    # Convert predicted string to embedding\n    pred_embedding = embedding_model.get_embeddings([pred])[0]\n\n    # Calculate cosine similarity between predicted embedding and ground truth embeddings\n    cosine_similarities = calculate_cosine_similarity(pred_embedding, gt_embeddings)\n\n    # Return the maximum cosine similarity as the semantic similarity score\n    return max(cosine_similarities)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer not set up\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        face_restorers.append(FaceRestorerCodeFormer(dirname))\n    except errors.ModelNotFoundError as e:\n        logger.warning(f\"{e}\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        face_restoration.patch_facexlib(dirname)\n        global gfpgan_face_restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except Exception as e:\n        logger.warning(f\"GFPGAN face restorer setup failed: {e}\")"}
{"namespace": "quaternion.rotate", "completion": "  q_v = jnp.concatenate([jnp.zeros_like(v[Ellipsis, :1]), v], axis=-1)\n  return multiply(multiply(q, q_v), inverse(q))[Ellipsis, 1:]"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  axis_angle = axis_angle / (jnp.linalg.norm(axis_angle) + eps)\n  angle = jnp.linalg.norm(axis_angle)\n  axis = axis_angle / angle\n  return jnp.concatenate([axis * jnp.sin(angle / 2.0), [jnp.cos(angle / 2.0)]])"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    logit_bias = {idx: high}\n    topk_words = model.topk(prefix, logit_bias)\n    if idx not in topk_words:\n        raise TypeError(\n            f\"Tokens {idx} not in top-k with bias {high}.\"\n            \"Either increase bias or provide top unbiased logprob (top_logprob)\"\n        )\n    logprob = topk_words[idx]\n    return logprob, 1"}
{"namespace": "resample.resample_3d", "completion": "  # Check input arguments\n  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError('Invalid edge_behavior. Must be either '\n                     '\"CONSTANT_OUTSIDE\" or \"CLAMP\".')\n  if method not in ['TRILINEAR', 'NEAREST']:\n    raise ValueError('Invalid method. Must be either \"TRILINEAR\" or '\n                     '\"NEAREST\".')\n  if coordinate_order not in ['xyz', 'zyx']:\n    raise ValueError('Invalid coordinate_order. Must be either \"xyz\" or '\n                     '\"zyx\".')\n\n  # Pad the input data if necessary\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    data = jnp.pad(data, ((1, 1), (1, 1), (1, 1), (0, 0)),\n                   mode='constant',\n                   constant_values=constant_values)\n\n  # Calculate sample locations\n  if half_pixel_center:\n    locations = locations + 0.5\n\n  # Gather data at sample locations\n  gathered_data = gather_volume(data, locations, coordinate_order)\n\n  # Return the gathered data\n  return gathered_data"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(x, +jnp.inf))"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -np.inf)\n  )"}
{"namespace": "math.safe_exp", "completion": "safe_exp.defvjp(safe_exp_fwd, safe_exp_bwd)\n\n\ndef safe_exp_fwd(x):\n  \"\"\"\n  The function computes the forward pass of the safe exponential function. It clips the input to a specified range and computes the exponential of the clipped input.\n\n  Input-Output Arguments\n  :param x: The input value for which the safe exponential function is computed. It is used as the argument for the exponential function and its custom gradient.\n  :return: The result of applying the safe exponential function to the input x. This ensures that the output is within a specified range to prevent overflow errors.\n  \"\"\"\n  x_safe = jnp.clip(x, min_val, np.nextafter(np.log(max_val), np.float32(0)))\n  return jnp.exp(x_safe), (x_safe,)\n\n\ndef safe_exp_bwd(res, g):\n  \"\"\"\n  The function computes the backward pass of the safe exponential function. It computes the gradient of the output with respect to the input using the gradient of the exponential function and the clipped input.\n\n  Input-Output Arguments\n  :param res: The result of applying the safe exponential function to the input x. This ensures that the output is within a specified range to prevent overflow errors.\n  :param g: The gradient of the output with respect to the input x.\n  :return: The gradient of the input x with respect to the output.\n  \"\"\"\n  x_safe, = res\n  return g * jnp.exp(x_safe)"}
{"namespace": "math.safe_log", "completion": "def safe_log(x):\n  return generate_safe_fn(\n      jnp.log,\n      lambda x, _, x_dot: x_dot / x,\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.safe_sqrt", "completion": "def safe_sqrt(x):\n  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: safe_div(x_dot, 2 * y),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return jnp.inf\n  elif p == 0:\n    return jnp.inf\n  elif p == -jnp.inf:\n    return 0\n  elif p == jnp.inf:\n    return 0\n  else:\n    return jnp.abs(p - 1) / p * (jnp.exp(jnp.log(jnp.abs(p - 1)) / p) - 1)"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([\n        [1, 1, 1],\n        [-1, 1, -1],\n        [-1, -1, 1],\n        [1, -1, -1],\n    ])\n    base_faces = np.array([\n        [0, 1, 2],\n        [0, 1, 3],\n        [0, 2, 3],\n        [1, 2, 3],\n    ])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array([\n        [0, 0, 1],\n        [0, 0.942809, 0.333333],\n        [0, -0.471405, -0.816497],\n        [0, -0.471405, 0.816497],\n        [0.816497, 0, 0.57735],\n        [-0.408249, 0.707107, 0.57735],\n        [-0.408249, -0.707107, 0.57735],\n        [0.408249, 0.707107, -0.57735],\n        [0.408249, -0.707107, -0.57735],\n        [0.816497, 0, -0.57735],\n        [0.471405, 0.816497, 0],\n        [-0.471405, 0.816497, 0],\n        [-0.471405, -0.816497, 0],\n        [0.471405"}
{"namespace": "math.safe_log1p", "completion": "def safe_log1p(x):\n  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x *= premult\n  if p == 1:\n    y = x\n  elif p == 0:\n    y = safe_log1p(x)\n  elif p == -jnp.inf:\n    y = -safe_log1p(-x)\n  elif p == jnp.inf:\n    y = safe_log1p(x)\n  else:\n    y = safe_div(p - 1, p) * (((safe_div(p, jnp.abs(p - 1)) * x + 1))**(1 / p) - 1)\n  if postmult is not None:\n    y *= postmult\n  return y"}
{"namespace": "math.inv_power_ladder", "completion": "  # Compute sign(y) * |p - 1|/p * ((|y|/|p-1| + 1)^p - 1)\n  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = safe_sign(y) * select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_log1p(yp)),\n          (p == -jnp.inf, -safe_expm1(-yp)),\n          (p == jnp.inf, safe_expm1(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((ys + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_init <= 0 or lr_final <= 0:\n    raise ValueError(f'Learning rates must be positive, but are {lr_init} and {lr_final}.')\n  if max_steps <= 0:\n    raise ValueError(f'Max steps must be positive, but is {max_steps}.')\n  if lr_delay_steps < 0:\n    raise ValueError(f'Delay steps must be non-negative, but is {lr_delay_steps}.')\n  if lr_delay_mult <= 0:\n    raise ValueError(f'Delay multiplier must be positive, but is {lr_delay_mult}.')\n\n  if lr_delay_steps > 0:\n    lr_init *= lr_delay_mult\n    lr_delay_mult = 1\n\n  lr_delay_mult = jnp.maximum(lr_delay_mult, 1)\n  lr_delay_mult = jnp.minimum(lr_delay_mult, 100)\n\n  # The learning rate is a log-linear interpolation between the initial and final\n  # learning rates.\n  lr = jnp.exp(\n      jnp.clip(\n          jnp.log(lr_init) + (jnp.log(lr_final) - jnp.log(lr_init)) * (\n              (step - lr_delay_steps) / (max_steps - lr_delay_steps)),\n          jnp.log(lr_init / lr_delay_mult),\n          jnp.log(lr_final),\n      )\n  )\n\n  return lr"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      rng=jax.random.PRNGKey(0),\n      n=1,\n      origin_lo=-1.0,\n      origin_hi=1.0,\n      radius_lo=0.0,\n      radius_hi=1.0,\n      near_lo=0.0,\n      near_hi=1.0,\n      far_lo=1.0,\n      far_hi=2.0,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Convert points to homogeneous coordinates.\n  points_homogeneous = xnp.concatenate(\n      [points, xnp.ones_like(points[Ellipsis, :1])], axis=-1\n  )\n\n  # Apply camera rotation matrices.\n  points_camera = mat_vec_mul(camtoworlds[Ellipsis, :3, :3], points_homogeneous)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_distort(\n        points_camera[Ellipsis, 0],\n        points_camera[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    points_camera = xnp.stack([x, y, points_camera[Ellipsis, 2]], axis=-1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(points_camera[Ellipsis, :2]), axis=-1))\n    theta = xnp.minimum(xnp.pi, theta)\n\n    sin_theta_over_theta = xnp.sin(theta) / theta\n    points_camera = xnp.stack(\n        [\n            points_camera[Ellipsis, 0] * sin_theta_over_theta,\n            points_camera[Ellipsis, 1] * sin_theta_over_theta,\n            xnp.cos(theta),\n        ],\n        axis=-1,\n    )\n\n  # Apply inverse intrinsic matrices.\n  coordinates = mat_vec_mul(pixtocams, points_camera)\n\n  # Extract the depth values.\n  depth = points_camera[Ellipsis, 2]\n\n  return coordinates, depth"}
{"namespace": "rigid_body.exp_se3", "completion": "  w, v = screw_axis[Ellipsis, :3], screw_axis[Ellipsis, 3:]\n  theta = spin_math.safe_norm(w, min_norm=eps)\n  W = skew(w / theta)\n\n  R_taylor = jnp.eye(3) + W\n  R = jnp.eye(3) + jnp.sin(theta) * W + (1.0 - jnp.cos(theta)) * spin_math.matmul(W, W)\n\n  X = jnp.eye(4)\n  X = X.at[:3, :3].set(R)\n  X = X.at[:3, 3].set(v)\n\n  return X\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  axis_angle = jnp.reshape(axis_angle, (3))\n  theta = jnp.linalg.norm(axis_angle)\n  axis = axis_angle / (theta + eps)\n  theta = jnp.where(theta > eps, theta, 1.0)\n  W = skew(axis)\n  R = jnp.eye(3) + jnp.sin(theta) * W + (1.0 - jnp.cos(theta)) * spin_math.matmul(\n      W, W\n  )\n  return R"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, base_radius * r_var, diag)"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  if camtype == ProjectionType.PERSPECTIVE:\n    # Convert pixel coordinates to camera coordinates.\n    x, y = _radial_and_tangential_undistort(\n        pix_x_int,\n        pix_y_int,\n        **distortion_params,\n    )\n    x, y = x[Ellipsis, None], y[Ellipsis, None]\n    if pixtocam_ndc is not None:\n      # Convert camera coordinates to NDC coordinates.\n      x, y = matmul(pixtocam_ndc, xy_stack([x, y]))\n    else:\n      # Convert camera coordinates to NDC coordinates.\n      x, y = xy_stack([x, y])\n  elif camtype == ProjectionType.FISHEYE:\n    # Convert pixel coordinates to camera coordinates.\n    x, y = _radial_and_tangential_undistort(\n        pix_x_int,\n        pix_y_int,\n        **distortion_params,\n    )\n    x, y = x[Ellipsis, None], y[Ellipsis, None]\n    # Convert camera coordinates to NDC coordinates.\n    x, y = xy_stack([x, y])\n  elif camtype == ProjectionType.PANORAMIC:\n    # Convert pixel coordinates to camera coordinates.\n    x, y = pix_x_int, pix_y_int\n    x, y = x[Ellipsis, None], y[Ellipsis, None]\n    # Convert camera coordinates to NDC coordinates.\n    x, y = xy_stack([x, y])\n  else:\n    raise ValueError(f'Unknown projection type: {camtype}')\n\n  # Convert camera coordinates to world coordinates.\n  origins = matmul(camtoworlds[Ellipsis, :3, :3], x) + camtoworlds[Ellipsis, :3, -1]\n  directions = matmul(camtoworlds[Ellipsis, :3, :3], pixtocams["}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  if num_samples <= 1:\n    raise ValueError(f'num_samples must be > 1, is {num_samples}.')\n\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Sample from the CDF.\n  if rng is None:\n    # If rng is None, we use linspace to generate the samples.\n    u = jnp.linspace(eps, 1 - eps, num_samples, endpoint=False)\n  else:\n    # If rng is not None, we use random sampling.\n    if single_jitter:\n      # If single_jitter is True, we jitter each sample by the same amount.\n      u = jax.random.uniform(rng, (num_samples,))\n    else:\n      # If single_jitter is False, we jitter each sample independently.\n      u = jax.random.uniform(rng, (num_samples,))\n\n  # If deterministic_center is True, we center the samples in each interval.\n  if deterministic_center:\n    u = u + jnp.arange(num_samples) / num_samples\n\n  # Interpolate into the CDF to get the corresponding samples.\n  samples = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n\n  return samples"}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[..., :-1] + t_samples[..., 1:]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples = jnp.concatenate(\n      [\n          jnp.full(t_samples.shape[:-1] + (1,), domain[0]),\n          t_samples,\n          jnp.full(t_samples.shape[:-1] + (1,), domain[1]),\n      ],\n      axis=-1,\n  )\n  t_midpoints = jnp.concatenate(\n      [\n          jnp.full(t_midpoints.shape[:-1] + (1,), domain[0]),\n          t_midpoints,\n          jnp.full(t_midpoints.shape[:-1] + (1,), domain[1]),\n      ],\n      axis=-1,\n  )\n\n  return t_midpoints"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n\n  # Integrate the weights to get the CDF.\n  cdf = integrate_weights(w)\n\n  # Interpolate the CDF to get the weighted percentiles.\n  return jnp.interp(ps, cdf, t)"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a PDF\n  p = pdf_to_weight(t, w)\n\n  # Blur the PDF using a Gaussian kernel\n  p_blurred = linspline.gaussian_kernel_1d(p, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points\n  w_resampled = resample(tq, t, p_blurred)\n\n  return w_resampled"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(to_homogeneous(vectors), transform))"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n\n  # Compute the width of each interval in `t`.\n  dt = jnp.diff(t)\n\n  # Compute the width of each interval in `tp`.\n  dtp = jnp.diff(tp)\n\n  # Compute the indices of the intervals in `t` that each interval in `tp` belongs to.\n  i0, i1 = math.sorted_lookup(t, tp, (), utils.device_is_tpu())\n\n  # Compute the weights for each interval in `t`.\n  w = jnp.zeros_like(t)\n  w = w.at[Ellipsis, i0].add(dtp / dt)\n  w = w.at[Ellipsis, i1].add(-dtp / dt)\n\n  # Compute the values for each interval in `t`.\n  v = jnp.zeros_like(t)\n  v = v.at[Ellipsis, i0].add(vp * dtp / dt)\n  v = v.at[Ellipsis, i1].add(-vp * dtp / dt)\n\n  # Sum or average the values for each interval in `t`.\n  if use_avg:\n    v = v / jnp.maximum(dt, jnp.finfo(jnp.float32).tiny)\n  else:\n    v = jnp.cumsum(v, axis=-1)\n\n  return v"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Compute the scaling factor for the encoding.\n  scale = jnp.sqrt(var)\n\n  # Concatenate the mean and variance along the last dimension.\n  x = jnp.concatenate([mean, scale], axis=-1)\n\n  # Apply the sinusoidal encoding.\n  x = pos_enc(x, min_deg, max_deg)\n\n  # Return the encoded variables.\n  return x"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(sph_harms), jnp.imag(sph_harms)], axis"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # print(\"*\" * 50)\n    # print(\"CLEANING LINES\")\n    # print(\"*\" * 50)\n    # print()\n    # print(\"-\" * 50)\n    # print(\"INPUT\")\n    # print(\"-\" * 50)\n    # print()\n    # print(lines)\n    # print()\n    # print(\"-\" * 50)\n    # print(\"OUTPUT\")\n    # print(\"-\" * 50)\n    # print()\n    # print(\"-\" * 50)\n    # print(\"INPUT\")\n    # print(\"-\" * 50)\n    # print()\n    # print(lines)\n    # print()\n    # print(\"-\" * 50)\n    # print(\"OUTPUT\")\n    # print(\"-\" * 50)\n    # print()\n    # print(\"-\" * 50)\n    # print(\"INPUT\")\n    # print(\"-\" * 50)\n    # print()\n    # print(lines)\n    # print()\n    # print(\"-\" * 50)\n    # print(\"OUTPUT\")\n    # print(\"-\" * 50)\n    # print()\n    # print(\"-\" * 50)\n    # print(\"INPUT\")\n    # print(\"-\" * 50)\n    # print()\n    # print(lines)\n    # print()\n    # print(\"-\" * 50)\n    # print(\"OUTPUT\")\n    # print(\"-\" * 50)\n    # print()\n    # print(\"-\" * 50)\n    # print(\"INPUT\")\n    # print(\"-\" * 50)\n    # print()\n    # print(lines)\n    # print()\n    # print(\"-\" * 50)\n    # print(\"OUTPUT\")\n    # print(\"-\" * 50)\n    # print()\n    # print(\"-\" * 50)\n    # print(\"INPUT\")\n    # print(\"-\" * 50)\n    # print()\n    # print(lines)\n    # print()\n   "}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    # normalize quotation marks\n    text = quotation_pattern.sub('\"', org_texts)\n\n    # split paragraphs\n    paragraphs = text.split(\"\\n\")\n\n    # tokenize sentences\n    sentences = []\n    for paragraph in paragraphs:\n        if paragraph:\n            sentences.extend(nltk_tokenzier.tokenize(paragraph))\n\n    # remove spaces between punctuations\n    sentences = [space_rule.sub(r\"\\1\", sent) for sent in sentences]\n\n    # remove brackets\n    sentences = [bracket_rule.sub(\"\", sent) for sent in sentences]\n\n    # apply rules\n    for rule, replaced in rules:\n        sentences = [rule.sub(replaced, sent) for sent in sentences]\n\n    # remove empty sentences\n    sentences = [sent for sent in sentences if sent.strip()]\n\n    return sentences"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if not isinstance(token, str):\n            raise TypeError(\"Expected a string\")\n\n        if key is not None:\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                return self.posns.doc_encoded_posns(term_id, doc_id=key)\n            except TermMissingError:\n                return []\n        else:\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                return self.posns.term_encoded_posns(term_id)\n            except TermMissingError:\n                return []"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec == \"100%\":\n        return num_clauses\n    if spec.endswith(\"%\"):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n    return int(spec)"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return self.phrase_freq_scan(tokens)\n        else:\n            return self.phrase_freq_every_diff(tokens, slop=slop)"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        if truncate:\n            array = array[:batch_size]\n\n        term_mat, posns, term_dict, avg_doc_length, doc_lens = build_index_from_terms_list(array, Terms)\n        arr = SearchArray([], tokenizer=tokenizer)\n        arr.term_mat = term_mat\n        arr.posns = posns\n        arr.term_dict = term_dict\n        arr.avg_doc_length = avg_doc_length\n        arr.doc_lens = doc_lens\n        arr.avoid_copies = avoid_copies\n        return arr\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.logger.info('Starting server...')\n        self.server = Server(self.config['serverHost'], self.config['serverPort'])\n        self.server.start()\n        self.logger.info('Server started.')\n\n        self.connections = {}\n        self.lock = threading.Lock()\n\n    "}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = arr & mask\n    arr = (arr >> _1) & (s55 + (arr & s55))\n    arr = (arr >> _2) & (s33 + (arr & s33))\n    arr = (arr >> _4) & (s0F + (arr & s0F))\n    arr = (arr >> _8) & (s01 + (arr & s01))\n    arr = arr.sum(axis=1)\n    return arr"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Parse query fields and boosts\n    query_fields = parse_field_boosts(qf)\n    if pf:\n        query_fields.update(parse_field_boosts(pf))\n    if pf2:\n        query_fields.update(parse_field_boosts(pf2))\n    if pf3:\n        query_fields.update(parse_field_boosts(pf3))\n\n    # Parse query terms\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, query_fields.keys())\n\n    if term_centric:\n        return _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        return _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            connection = self._get_connection(message)\n            message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            connection = self._get_connection(message)\n            message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            self._handle_connection_close_message(message)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            for connection in self.connections.values():\n                connection.stop()\n\n        self.server.stop()"}
