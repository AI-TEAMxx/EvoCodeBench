{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Executing command: {cmd}\")\n    return os.system(cmd)"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (1 / (2 * jnp.sqrt(z_mag_sq) - 1)) / z_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Connect to the SQLite database\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n\n            # Create the cache table if it doesn't exist\n            c.execute(\n                \"\"\"CREATE TABLE IF NOT EXISTS cache\n                            (func_name TEXT, args TEXT, kwargs TEXT, result TEXT, PRIMARY KEY (func_name, args, kwargs))\"\"\"\n            )\n\n            # Serialize the arguments and keyword arguments\n            args_serialized = json.dumps(args)\n            kwargs_serialized = json.dumps(kwargs)\n\n            # Compute the hash of the function name, arguments, and keyword arguments\n            hash_input = f\"{func_name}{args_serialized}{kwargs_serialized}\".encode(\"utf-8\")\n            hash_output = hashlib.sha256(hash_input).hexdigest()\n\n            # Check if the result is already in the cache\n            c.execute(\n                \"SELECT result FROM cache WHERE func_name=? AND args=? AND kwargs=?\",\n                (func_name, args_serialized, kwargs_serialized),\n            )\n            result = c.fetchone()\n\n            if result:\n                # If the result is in the cache, return it\n                return json.loads(result[0])\n            else:\n                # If the result is not in the cache, compute it and store it\n                result = func(*args, **kwargs)\n                c.execute(\n                    \"INSERT INTO cache (func_name, args, kwargs, result) VALUES (?, ?, ?, ?)\",\n                    (func_name, args_serialized, kwargs_serialized, json.dumps(result)),\n                )\n                conn.commit()\n                return result\n\n        return wrapper\n\n    return decorator"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max.\")\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max.\")\n\n    return values\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  return np.maximum(0, np.sum(mat0 ** 2, axis=0)[:, np.newaxis] +\n                    np.sum(mat1 ** 2, axis=0) - 2 * mat0.T.dot(mat1))\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\":\n        return False\n    return path.startswith(\"s3://\") or path.startswith(\"gs://\") or path.startswith(\"hdfs://\")\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"{name} is a dictionary, but no assets names were provided.\"\n            )\n        if dim == 1:\n            items = np.array([items.get(name, fill_value) for name in assets_names])\n        elif dim == 2:\n            items = np.array(\n                [\n                    [items.get(name, fill_value) for name in assets_names]\n                    for _ in range(n_assets)\n                ]\n            )\n        else:\n            raise ValueError(f\"{name} is a dictionary, but dim must be 1 or 2.\")\n    elif isinstance(items, np.ndarray):\n        if dim == 1:\n            if items.ndim != 1:\n                raise ValueError(f\"{name} must be a 1D array.\")\n            if items.shape[0] != n_assets:\n                raise ValueError(\n                    f\"{name} must have {n_assets} elements, but has {items.shape[0]}.\"\n                )\n        elif dim == 2:\n            if items.ndim != 2:\n                raise ValueError(f\"{name} must be a 2D array.\")\n            if items.shape[0] != n_assets or items.shape[1] != n_assets:\n                raise ValueError(\n                    f\"{name} must have shape ({n_assets}, {n_assets}), but has shape {items.shape}.\"\n                )\n        else:\n            raise ValueError(f\"{name} is a numpy array, but dim must be 1 or 2.\")\n    else:\n        raise TypeError(f\"{name} must be a dictionary or a numpy array.\")\n\n    return items\n\n"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        agent = MicroAgent(agent_lifecycle, openai_wrapper)\n        agent.dynamic_prompt = data.get(\"dynamic_prompt\", \"\")\n        agent.purpose = data.get(\"purpose\", \"\")\n        agent.purpose_embedding = data.get(\"purpose_embedding\", [])\n        agent.depth = data.get(\"depth\", 0)\n        agent.max_depth = data.get(\"max_depth\", 0)\n        agent.usage_count = data.get(\"usage_count\", 0)\n        agent.id = data.get(\"id\", \"\")\n        agent.parent_id = data.get(\"parent_id\", \"\")\n        agent.working_agent = data.get(\"working_agent\", False)\n        agent.is_prime = data.get(\"is_prime\", False)\n        agent.evolve_count = data.get(\"evolve_count\", 0)\n        agent.number_of_code_executions = data.get(\"number_of_code_executions\", 0)\n        agent.last_input = data.get(\"last_input\", \"\")\n        return agent\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  srgb_linear0 = srgb / 12.92\n  srgb_linear1 = ((srgb + 0.055) / 1.055) ** 2.4\n  return xnp.where(srgb <= 0.04045, srgb_linear0, srgb_linear1)"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Adjust the spline degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Fit a spline to the input signal\n  tck = scipy.interpolate.splrep(t_input, x, k=spline_degree, s=smoothness)\n\n  # Evaluate the spline at the output times\n  x_output = scipy.interpolate.splev(t_output, tck)\n\n  return x_output\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n    if word[0].isupper() and word[1].isupper():\n        return word.upper()\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    if word[0].islower() and word[1].isupper():\n        return word.lower()\n    return word"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.array_equal(v, v.astype(bool)):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must contain only boolean values. Found {v.dtype}.\"\n        )\n\n    return v\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # This is the fast mode, which uses the determinant directly.\n    det = jnp.linalg.det(cov)\n    if jnp.any(det <= 0):\n      raise ValueError('Invalid determinant.')\n    return jnp.eye(cov.shape[-1]) * jnp.sqrt(det)\n  elif mode == 'accurate':\n    # This is the accurate mode, which uses the logarithm of the determinant for stability.\n    log_det = jnp.linalg.slogdet(cov)[1]\n    if jnp.any(jnp.isinf(log_det)):\n      raise ValueError('Invalid log determinant.')\n    return jnp.eye(cov.shape[-1]) * jnp.exp(0.5 * log_det)\n  else:\n    raise ValueError('Invalid mode.')\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='XAgent command line interface')\n    parser.add_argument('--task', type=str, required=True, help='Task description')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload')\n    parser.add_argument('--model', type=str, help='Model identifier')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs')\n    parser.add_argument('--mode', type=str, default='auto', help='Operational mode')\n    parser.add_argument('--quiet', action='store_true', help='Run in quiet mode')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Enable asking for human assistance during task execution')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts')\n    parser.add_argument('--config-file', type=str, default=os.environ.get('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file')\n    args = parser.parse_args()\n\n    return args\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points.\")\n\n    return v"}
{"namespace": "tanuki.utils.encode_int", "completion": "    character_set = string.ascii_lowercase + string.digits + '_'\n    return character_set[n]\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunks_index[worker_idx] = 0\n        for interval in intervals:\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                indexes[worker_idx] -= interval[1] - interval[0]\n                chunks_index[worker_idx] += 1\n            else:\n                break\n    return chunks_index, indexes"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return resample.trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return hash_resample.trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(f'Invalid datastructure: {datastructure}')\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Generate integer weights for each vertex of the triangle\n  weights = np.array(list(itertools.product(range(v + 1), repeat=3)))\n\n  # Normalize the weights to get the barycentric coordinates\n  weights = weights / v\n\n  # Remove any weights that do not sum to 1\n  weights = weights[np.sum(weights, axis=1) == 1]\n\n  return weights"}
{"namespace": "linspline.query", "completion": "  # Check that the spline is valid\n  check_zero_endpoints(v)\n\n  # Interpolate the spline at the query points\n  vq = math.interp(tq, t, v)\n\n  # Set the extrapolated values to 0\n  vq = jnp.where((tq < t[0]) | (tq > t[-1]), 0, vq)\n\n  return vq\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(x > 0 for x in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane.\n  origins = origins / directions[..., 2:3] * near\n\n  # Calculate the directions in NDC.\n  directions = xnp.matmul(directions, pixtocam[:3, :3].T)\n  origins = xnp.concatenate([origins, near * xnp.ones_like(origins[..., :1])], -1)\n  return origins, directions\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n\n  # Compute the dot product of the normalized direction vectors\n  dot_product = jnp.dot(dir1, dir2)\n\n  # Check if the dot product is close to 1 or -1, within a small epsilon\n  epsilon = 1e-6\n  return jnp.abs(dot_product - 1) < epsilon or jnp.abs(dot_product + 1) < epsilon"}
{"namespace": "common.bleu4_score", "completion": "    tokenizer = lambda x: list(jieba.cut(x))\n    bleu = evaluate.load(\"bleu\")\n    results = bleu.compute(\n        predictions=[continuation],\n        references=[[reference]],\n        tokenizer=tokenizer,\n        max_order=4,\n        smooth=True\n    )\n    if with_penalty:\n        return results[\"bleu\"]\n    else:\n        return results[\"bleu\"] * 100\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  safe_x = jnp.where(x < eps, eps, x)\n  return jnp.sqrt(safe_x)\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  return w / (t[Ellipsis, 1:] - t[Ellipsis, :-1])\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for f in filenames:\n            try:\n                fp = os.path.join(dirpath, f)\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val, is_numpy = val, False\n    if isinstance(val, np.ndarray):\n        is_numpy = True\n    elif not isinstance(val, torch.Tensor):\n        raise ValueError(f'The value should be a tensor or numpy array, '\n                         f'but got {type(val)}')\n    if isinstance(period, np.ndarray):\n        period = period.tolist()\n    if isinstance(period, torch.Tensor):\n        period = period.item()\n    if not isinstance(period, int) and not isinstance(period, float):\n        raise TypeError(f'The period should be an int or a float number, '\n                        f'but got {type(period)}')\n    if isinstance(offset, np.ndarray):\n        offset = offset.tolist()\n    if isinstance(offset, torch.Tensor):\n        offset = offset.item()\n    if not isinstance(offset, int) and not isinstance(offset, float):\n        raise TypeError(f'The offset should be an int or a float number, '\n                        f'but got {type(offset)}')\n\n    val = val - torch.floor(val / period + offset) * period\n    if is_numpy:\n        return val.numpy()\n\n    return val\n\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent.purpose_embedding, np.ndarray):\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of items and weights are equal\n    assert len(items) == len(weights)\n\n    # Check that all weights are positive\n    assert all(w >= 0 for w in weights)\n\n    # Check that the number of bins is positive\n    assert num_bins > 0\n\n    # Create a dictionary to store the items in each bin\n    bins = defaultdict(list)\n\n    # Create a dictionary to store the total weight of each bin\n    bin_weights = defaultdict(int)\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Iterate over the sorted items\n    for item, weight in sorted_items:\n\n        # Find the bin with the current lowest total weight\n        min_bin = min(bin_weights, key=bin_weights.get)\n\n        # Add the item to the bin with the current lowest total weight\n        bins[min_bin].append(item)\n\n        # Update the total weight of the bin\n        bin_weights[min_bin] += weight\n\n    # Return the bins and bin weights\n    return bins, bin_weights\n\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        data_str = json.dumps(data)\n        data_bytes = data_str.encode(\"utf-8\")\n        return hashlib.sha256(data_bytes).hexdigest()\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the pairwise distances between all points in the polygon\n    distances = np.linalg.norm(polygon[:-1] - polygon[1:], axis=1)\n\n    # Filter out distances that exceed the maximum point distance\n    filtered_distances = distances[distances <= max_point_distance]\n\n    # Compute the total length of the polygon\n    total_length = np.sum(filtered_distances)\n\n    return total_length"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    areas = [area(polygon) for polygon in polygons]\n    max_area = max(areas)\n    rel_threshold = max_area * rel_tr\n    abs_threshold = abs_tr\n\n    filtered_polygons = [\n        polygon\n        for polygon, area in zip(polygons, areas)\n        if area >= rel_threshold and area >= abs_threshold\n    ]\n\n    return filtered_polygons\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples each worker has processed\n    num_samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of batches each worker has processed\n    num_batches_per_worker = num_samples_per_worker // batch_size\n\n    # Calculate the number of samples remaining after distributing the batches evenly among workers\n    remaining_samples = num_samples_yielded - num_batches_per_worker * batch_size * num_workers\n\n    # Distribute the remaining samples among workers\n    num_samples_per_worker += remaining_samples // num_workers\n\n    # Calculate the number of samples each worker has processed\n    num_samples_per_worker = [num_samples_per_worker] * num_workers\n\n    # Distribute the remaining samples among workers\n    for i in range(remaining_samples % num_workers):\n        num_samples_per_worker[i] += 1\n\n    # Calculate the number of samples each worker has processed\n    num_samples_per_worker = [num_samples_per_worker[i] * batch_size for i in range(num_workers)]\n\n    # Calculate the number of samples each worker has processed\n    num_samples_per_worker = [num_samples_per_worker[i] - batch_size for i in range(num_workers)]\n\n    # Calculate the number of samples each worker has processed\n    num_samples_per_worker = [num_samples_per_worker[i] + batch_size for i in range(num_workers)]\n\n    # Calculate the number of samples each worker has processed\n    num_samples_per_worker = [num_samples_per_worker[i] - num_samples_yielded for i in range(num_workers)]\n\n    # Calculate the number of samples each worker has processed\n    num_samples_per_worker = [num_samples_per_worker[i] + num_samples_yielded"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n    return [result for result, value, metadata in zip(results, value, metadatas) if value <= threshold], [\n        metadata for result, value, metadata in zip(results, value, metadatas) if value <= threshold]\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2)\")\n\n    x = array[:, 0]\n    y = array[:, 1]\n\n    # Implementation of Shoelace formula\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    v = v.reshape(-1)\n    idx = torch.searchsorted(a, v)\n    idx_lo = torch.max(idx - 1, torch.zeros_like(idx))\n    idx_hi = torch.min(idx, torch.full_like(idx, a.shape[-1] - 1))\n    return idx_lo, idx_hi\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n\n"}
{"namespace": "coord.contract", "completion": "  return x / (1 + jnp.linalg.norm(x, axis=-1, keepdims=True) ** 2)\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n        if num_bytes < 1000.0:\n            return f\"{num_bytes:3.1f} {unit}\"\n        num_bytes /= 1000.0\n    return f\"{num_bytes:.1f} PB\""}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def is_array_n_dimensions_validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions.\")\n\n        return v\n\n    return is_array_n_dimensions_validator"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = jnp.split(cartesian_vector, 3, axis=-1)\n\n  r = jnp.linalg.norm(cartesian_vector, axis=-1, keepdims=True)\n  theta = jnp.arccos(jnp.clip(z / r, -1, 1))\n  phi = jnp.arctan2(y, x)\n\n  return r, theta, phi\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    return results['rougeL']\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        return pydoc.locate(name)\n    except ImportError:\n        # Fallback to a manual scan.\n        obj = pydoc.locate(name.split(\".\")[-1])\n        if obj is None:\n            raise ImportError(f\"Cannot locate object: {name}\")\n        return obj"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check if the length of the ids and scores tuples match\n    if len(ids) != len(scores):\n        raise ValueError(\"The length of the ids and scores tuples must match.\")\n\n    # Check if the length of the weights tuple matches the length of the ids and scores tuples\n    if len(weights) != len(ids):\n        raise ValueError(\"The length of the weights tuple must match the length of the ids and scores tuples.\")\n\n    # Check if the sum of the weights equals 1\n    if sum(weights) != 1:\n        raise ValueError(\"The sum of the weights must equal 1.\")\n\n    # Normalize the scores\n    normalized_scores = [pd.Series(score) / sum(score) for score in scores]\n\n    # Combine the scores using the convex combination method\n    combined_scores = sum([weight * normalized_score for weight, normalized_score in zip(weights, normalized_scores)])\n\n    # Sort the combined scores in descending order\n    sorted_indices = combined_scores.sort_values(ascending=False).index\n\n    # Select the top_k results\n    top_k_indices = sorted_indices[:top_k]\n\n    # Get the ids and scores of the top_k results\n    top_k_ids = [ids[i][j] for i, j in enumerate(top_k_indices)]\n    top_k_scores = [scores[i][j] for i, j in enumerate(top_k_indices)]\n\n    return top_k_ids, top_k_scores"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if x == 0:\n        return \"0\"\n    if x < 1e-3:\n        return f\"{x:.2e}\"\n    if x < 1e-1:\n        return f\"{x:.3f}\"\n    if x < 1e0:\n        return f\"{x:.2f}\"\n    if x < 1e1:\n        return f\"{x:.1f}\"\n    return f\"{x:.0f}\"\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        total, used, free = shutil.disk_usage(input_dir)\n        free_in_gb = free // (2**30)\n        if free_in_gb < threshold_in_gb:\n            sleep(sleep_time)\n        else:\n            break\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    line_text = \" \".join(line_text)\n\n    return line_text"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zeros cannot be greater than the total number of weights.\")\n\n    if zeros == 0:\n        return rand_weights_dirichlet(n)\n\n    weights = np.random.dirichlet(np.ones(n - zeros))\n    weights = np.append(weights, np.zeros(zeros))\n    np.random.shuffle(weights)\n    return weights\n\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    center_y = 0.5 * (bbox[1] + bbox[3])\n    center_x = 0.5 * (bbox[0] + bbox[2])\n\n    assert (\n        image_size[0] >= center_y >= crop_size[0]\n    ), \"Crop must be inside image and not larger than image\"\n    assert (\n        image_size[1] >= center_x >= crop_size[1]\n    ), \"Crop must be inside image and not larger than image\"\n\n    min_y = center_y - crop_size[0]\n    min_x = center_x - crop_size[1]\n\n    if min_y < 0:\n        min_y = 0\n    if min_x < 0:\n        min_x = 0\n\n    max_y = min_y + crop_size[0]\n    max_x = min_x + crop_size[1]\n\n    crop = T.CropTransform(min_x, min_y, max_x, max_y)\n    return crop\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Clamp the squared norm to a minimum value to prevent exploding gradients in the backward pass.\n  # The clamping value is chosen to be approximately equal to the default value of grad_eps in jax.\n  # This ensures that the gradient of the sqrt function is well-defined and does not explode during the backward pass.\n  # The clamping value is chosen to be approximately equal to the default value of grad_eps in jax.\n  # This ensures that the gradient of the sqrt function is well-defined and does not explode during the backward pass.\n  # The clamping value is chosen to be approximately equal to the default value of grad_eps in jax.\n  # This ensures that the gradient of the sqrt function is well-defined and does not explode during the backward pass.\n  # The clamping value is chosen to be approximately equal to the default value of grad_eps in jax.\n  # This ensures that the gradient of the sqrt function is well-defined and does not explode during the backward pass.\n  # The clamping value is chosen to be approximately equal to the default value of grad_eps in jax.\n  # This ensures that the gradient of the sqrt function is well-defined and does not explode during the backward pass.\n  # The clamping value is chosen to be approximately equal to the default value of grad_eps in jax.\n  # This ensures that the gradient of the sqrt function is well-defined and does not explode during the backward pass.\n  # The clamping value is chosen to be approximately equal to the default value of grad_eps in jax.\n  # This ensures that the gradient of the sqrt function is well-defined and does not explode during the backward pass.\n  # The clamping value is chosen to be approximately equal to the default value of grad_eps in jax.\n  # This ensures that the gradient of the sqrt function is well-defined and does not explode during the backward pass.\n  # The clamping value is chosen to be approximately equal to the default value of grad_eps in jax.\n  # This ensures that the gradient of the sqrt function is well-defined and does not explode during the backward pass"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name, input_text = agent_info.split(\":\")\n        return agent_name, input_text\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    boxes = target.gt_boxes = Boxes(boxes)\n    boxes.clip(image_size)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(polygons_to_bitmask(segm, *image_size))\n                elif isinstance(segm, dict):\n                    # COCO RLE\n                    masks.append(mask_util.decode(segm))\n                elif isinstance(segm, np.ndarray):\n                    assert segm.ndim == 3, f\"Expects a 3-dimensional mask, got {segm.ndim}.\"\n                    # mask array\n                    masks.append(segm)\n                else:\n                    raise ValueError(\n                        \"Cannot process segmentation of type '{}'!\"\n                        \"Supported types are: polygons as list[list[float] or ndarray],\"\n                        \" COCO-style RLE as a dict.\".format(type(segm))\n                    )\n            masks = BitMasks(masks)\n        target.gt_masks = masks\n\n    if len(annos) and \"keypoints\" in annos[0]:\n        kpts = [obj.get(\"keypoints\", []) for obj in annos]\n        target.gt_keypoint"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\n            \"SKFOLIO_DATA\", os.path.join(\"~\", \"skfolio_data\")\n        )\n    data_home = str(Path(data_home).expanduser())\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert_is_symmetric(cov)\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n    corr[corr < -1], corr[corr > 1] = -1, 1\n    return corr, std\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO: find a more automatic way to enable import of other classes\n    # TODO"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if the shapes of the two fields are equal.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, str):\n        metrics = [metrics]\n\n    metric_names = []\n    metric_params = []\n\n    for metric in metrics:\n        if isinstance(metric, str):\n            metric_names.append(metric)\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            metric_names.append(metric[\"name\"])\n            metric_params.append(metric.get(\"params\", {}))\n        else:\n            raise ValueError(\n                f\"Invalid metric format: {metric}. Metrics must be strings or dictionaries.\"\n            )\n\n    return metric_names, metric_params\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    fn_inv = {\n        contract: inv_contract,\n        contract3_isoscale: lambda x: inv_contract(x) / contract3_isoscale(x),\n    }[fn]\n\n  def t_to_s(t):\n    t_clipped = jnp.clip(t, t_near, t_far)\n    s = (fn(t_clipped) - fn(t_near)) / (fn(t_far) - fn(t_near))\n    return s\n\n  def s_to_t(s):\n    t = fn_inv(s * (fn(t_far) - fn(t_near)) + fn(t_near))\n    return t\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.stack([x, y, z], axis=-1)\n\n"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  return jnp.trapz(w, t)\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    # Create a dictionary to store the weighted sum of scores for each ID\n    weighted_sums = {}\n\n    # Iterate over the IDs and scores for each category or group\n    for i in range(len(ids)):\n        for j in range(len(ids[i])):\n            # Get the ID and score for the current category or group\n            id = ids[i][j]\n            score = scores[i][j]\n\n            # If the ID is not in the dictionary, add it with the weighted score\n            if id not in weighted_sums:\n                weighted_sums[id] = score * weights[i]\n            # If the ID is already in the dictionary, add the weighted score to the existing value\n            else:\n                weighted_sums[id] += score * weights[i]\n\n    # Sort the dictionary by the weighted sum of scores in descending order\n    sorted_weighted_sums = sorted(weighted_sums.items(), key=lambda x: x[1], reverse=True)\n\n    # Get the top K IDs and their corresponding scores\n    top_ids = [x[0] for x in sorted_weighted_sums[:top_k]]\n    top_scores = [x[1] for x in sorted_weighted_sums[:top_k]]\n\n    # Normalize the scores\n    top_scores = [score / sum(top_scores) for score in top_scores]\n\n    return top_ids, top_scores"}
{"namespace": "coord.track_linearize", "completion": "  # Compute the Jacobian of the function at the mean\n  jacobian = jax.jacfwd(fn)(mean)\n\n  # Compute the transformed means\n  fn_mean = fn(mean)\n\n  # Compute the transformed covariances\n  fn_cov = jnp.einsum('...ij,...jk,...kl->...il', jacobian, cov, jacobian)\n\n  return fn_mean, fn_cov\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if i.size > 1:\n            yield [i[: i.size // 2], i[i.size // 2 :]]\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(\"The matrix must be 2-dimensional.\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix must be square.\")\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  x = jnp.sin(jnp.concatenate([x, x + 0.5 * jnp.pi], axis=-1))\n  if append_identity:\n    x = jnp.concatenate([x, x[Ellipsis, :1]], axis=-1)\n  return x"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Check if len(field1) equals len(field2) and if each pair of arrays have the same shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n\n        for array1, array2 in zip(values[field1], values[field2]):\n            if array1.shape != array2.shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n\n        return values\n\n    return __root_validator"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(camera)\n"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        classifier_dropout=bert_config.classifier_dropout,\n        pad_token_id=bert_config.pad_token_id,\n        bos_token_id=bert_config.bos_token_id,\n        eos_token_id=bert_config.eos_token_id,\n        sep_token_id=bert_config.sep_token_id,\n        attention_window=bert_config.attention_window,\n        num_labels=bert_config.num_labels,\n        summary_type=bert_config.summary_type,\n        summary_use_proj=bert_config.summary_use_proj,\n        summary_activation=bert_config.summary_activation,\n        summary_last_dropout=bert_config.summary_last_dropout,\n        summary_proj_to_labels=bert_config.summary_proj_to_labels,\n        summary_first_dropout=bert_config.summary_first_dropout,\n        scale_"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible: return\n\n        # Select the appropriate shader program based on the render type\n        if self.render_type == Mesh.RenderType.POINTS:\n            use_gl_program(self.point_program)\n        else:\n            use_gl_program(self.mesh_program)\n\n        # Upload necessary uniforms to the GPU\n        self.upload_gl_uniforms(camera)\n\n        # Bind the vertex array object of the mesh\n        gl.glBindVertexArray(self.vao)\n\n        # Issue the appropriate OpenGL draw call based on the render type\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, self.n_verts)\n        elif self.render_type == Mesh.RenderType.LINES:\n            if self.faces.dtype == torch.int32:\n                gl.glDrawElements(gl.GL_LINES, self.n_faces, gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawElements(gl.GL_LINES, self.n_faces, gl.GL_UNSIGNED_SHORT, None)\n        elif self.render_type == Mesh.RenderType.TRIS:\n            if self.faces.dtype == torch.int32:\n                gl.glDrawElements(gl.GL_TRIANGLES, self.n_faces, gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawElements(gl.GL_TRIANGLES, self.n_faces, gl.GL_UNSIGNED_SHORT, None)\n        elif self.render_type == Mesh.RenderType.QUADS:\n            if self.faces.dtype == torch.int32:\n                gl.glDrawElements(gl.GL_QUADS, self.n_faces, gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawElements(gl.GL_"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        w = w or self.W\n        h = h or self.H\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.cpu().numpy()\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Validate inputs\n    assert R.ndim == 3 and R.shape[-2:] == (3, 3)\n    assert tvec.ndim == 2 and tvec.shape[-1] == 3\n    assert camera_matrix.ndim == 3 and camera_matrix.shape[-2:] == (3, 3)\n    assert image_size.ndim == 2 and image_size.shape[-1] == 2\n\n    # Extract focal lengths and principal points from camera matrix\n    fx = camera_matrix[..., 0, 0]\n    fy = camera_matrix[..., 1, 1]\n    cx = camera_matrix[..., 0, 2]\n    cy = camera_matrix[..., 1, 2]\n\n    # Compute focal length and sensor width\n    f = (fx + fy) / 2\n    sensor_width = max(image_size[..., 0] / f, image_size[..., 1] / f)\n\n    # Adjust principal point offsets and normalize focal length\n    cx = cx - image_size[..., 0] / 2\n    cy = cy - image_size[..., 1] / 2\n    f = f / sensor_width\n\n    # Compute camera position and rotation\n    cam_pos = -R.transpose(-2, -1) @ tvec[..., None]\n    cam_rot = matrix_to_rotation_6d(R)\n\n    # Concatenate camera parameters\n    cam_params = torch.cat([cam_pos, cam_rot, f[..., None], cx[..., None], cy[..., None], znear * torch.ones_like(f[..., None])], dim=-1)\n\n    return cam_params"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit()\n            return\n\n        w = w or self.W\n        h = h or self.H\n\n        # Set up the viewport and scissor box\n        old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        old_scissor = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Activate the shader program\n        gl.glUseProgram(self.quad_program)\n\n        # Bind the texture\n        gl.glActiveTexture(gl.GL_TEXTURE0)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Draw the quadrilateral\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n        gl.glBindVertexArray(0)\n\n        # Restore the viewport and scissor box\n        gl.glViewport(*old_viewport)\n        gl.glScissor(*old_scissor)\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    # R = R.mT\n    # T = -R @ T\n    # K = K.clone()\n    # K[..., 0, 2] = W / 2\n    # K[..., 1, 2] = H / 2\n    return H, W, K, R, T, C"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        w = w or self.W\n        h = h or self.H\n        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBlitFramebuffer(x, y, w, h,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Compute the cumulative sum of y1\n    y1_cum = torch.cumsum(y1, dim=-1)\n\n    # Compute the inner measure\n    y0_inner = torch.gather(y1_cum, -1, searchsorted(t1, t0))\n\n    # Compute the outer measure\n    y0_outer = torch.gather(y1_cum, -1, searchsorted(t1, t0) - 1)\n\n    return y0_inner, y0_outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    w_outer = w[..., 1:] - w[..., :-1]\n    w_outer = torch.cat([w_outer, torch.zeros_like(w_outer[..., :1])], dim=-1)\n\n    w_env_outer = w_env[..., 1:] - w_env[..., :-1]\n    w_env_outer = torch.cat([w_env_outer, torch.zeros_like(w_env_outer[..., :1])], dim=-1)\n\n    w_env_outer_max = torch.maximum(w_env_outer, torch.zeros_like(w_env_outer))\n    w_env_outer_min = torch.minimum(w_env_outer, torch.zeros_like(w_env_outer))\n\n    w_outer_max = torch.maximum(w_outer, torch.zeros_like(w_outer))\n    w_outer_min = torch.minimum(w_outer, torch.zeros_like(w_outer))\n\n    w_outer_max_max = torch.maximum(w_outer_max, w_env_outer_max)\n    w_outer_min_min = torch.minimum(w_outer_min, w_env_outer_min)\n\n    w_outer_max_max_max = torch.maximum(w_outer_max_max, torch.zeros_like(w_outer_max_max))\n    w_outer_min_min_min = torch.minimum(w_outer_min_min, torch.zeros_like(w_outer_min_min))\n\n    w_outer_max_max_max_max = torch.maximum(w_outer_max_max_max, torch.zeros_like(w_outer_max_max_max))\n    w_outer_min_min_min_min = torch.minimum(w_outer_min_min_min, torch.zeros_like(w_outer_"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n\n    # Compute the inter-interval loss\n    w_outer = inner_outer(t, t, w)[1]\n    inter_interval_loss = (w - w_outer).clip(0.).pow(2) / (w + 1e-6)\n\n    # Compute the intra-interval loss\n    w_inner = inner_outer(t, t, w)[0]\n    intra_interval_loss = (w_inner - w).clip(0.).pow(2) / (w + 1e-6)\n\n    # Combine the inter-interval and intra-interval losses\n    distortion_loss = inter_interval_loss + intra_interval_loss\n\n    return distortion_loss.mean()"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw0 = integrate_weights(w)\n    cw1 = cw0[..., 1:]\n    cw0 = cw0[..., :-1]\n    cw0, cw1 = torch.broadcast_tensors(cw0, cw1)\n    ps = torch.tensor(ps, device=cw0.device, dtype=cw0.dtype)\n    return interpolate(ps[None, None], cw0, t)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Get the CDF and PDF.\n    cw = integrate_weights(w)\n    pdf = cw[..., 1:] - cw[..., :-1]\n\n    # Take mid-points of all intervals; we'll use these as new samples.\n    mid_points = 0.5 * (t[..., :-1] + t[..., 1:])\n\n    # Invert the CDF.\n    u = torch.linspace(0. + 1e-5, 1. - 1e-5, steps=num_samples, device=t.device)\n    t_new = invert_cdf(u, t, w)\n\n    # Perturb and apply the inverse CDF.\n    if perturb:\n        # Get the CDF and PDF for the t_new.\n        cw_new = integrate_weights(pdf)\n        cdf_new = cw_new[..., 1:] - cw_new[..., :-1]\n\n        # Perturb the t_new.\n        u_new = u + (torch.rand(list(cdf_new.shape[:-1]) + [num_samples], device=t.device) - 0.5) * (1 / num_samples)\n        t_new = invert_cdf(u_new, t_new, pdf)\n\n    # Hack: append a value at the end to act as the CDF-value of a point past\n    # the last interval endpoint.\n    cdf_new = torch.cat([cdf_new, torch.zeros_like(cdf_new[..., -1:])], dim=-1)\n\n    # Draw uniform samples.\n    if not single_jitter:\n        shape = list(cdf_new.shape[:-1]) + [num_samples]\n        delta = torch.zeros_like(t_new)\n        if t.ndim > 1:\n            delta = delta[..., None, :]\n        delta[..., 1:] = torch.randint_like(delta[..."}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n    t_dilated = t[..., None, :] + dilation\n    t_dilated = torch.cat([t_dilated, t_dilated[..., -1:]], dim=-1)\n    t_dilated = torch.clamp(t_dilated, domain[0], domain[1])\n    w_dilated = torch.max(w, dim=-1, keepdim=True)[0]\n    return t_dilated, w_dilated\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # tq: [..., num_query_times]\n    # t: [..., num_steps]\n    # y: [..., num_steps]\n\n    # Find the indices of the step changes that occur before each query time.\n    indices = torch.searchsorted(t, tq, side='right') - 1\n\n    # Clamp the indices to the valid range of step changes.\n    indices = torch.clamp(indices, 0, t.shape[-1] - 1)\n\n    # Compute the values of the step function at the query times.\n    yq = torch.where(\n        (tq[..., None] == t[..., indices]) & (indices < t.shape[-1] - 1),\n        y[..., indices],\n        outside_value,\n    )\n\n    return yq"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n    # Compute the distance between adjacent intervals.\n    delta = t[..., 1:] - t[..., :-1]\n    # Handle cases where adjacent intervals have zero distance.\n    delta = torch.where(delta == 0, torch.zeros_like(delta), delta)\n    # Compute the annealing effect using Schlick's bias function.\n    bias = torch.exp(-delta * anneal_slope)\n    # Compute the adjusted weights.\n    w_adj = w * (1 - bias)\n    # Handle cases where adjacent intervals have zero distance.\n    w_adj = torch.where(delta == 0, torch.zeros_like(w_adj), w_adj)\n    # Compute the weight adjustment based on the training fraction.\n    w_adj = w_adj * (1 - train_frac)\n    # Ensure stability in the computation by handling cases where adjacent intervals have zero distance.\n    w_adj = torch.where(delta == 0, torch.zeros_like(w_adj), w_adj)\n    # Prevent NaN values by using a softmax operation on the adjusted weights.\n    w_adj = torch.softmax(w_adj, dim=-1)\n    # Ensure stability in the computation by handling cases where adjacent intervals have zero distance.\n    w_adj = torch.where(delta == 0, torch.zeros_like(w_adj), w_adj)\n    return w_adj"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device, non_blocking=True)\n    elif isinstance(batch, list):\n        if ignore_list:\n            return batch\n        else:\n            return [to_cuda(b, device) for b in batch]\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(b, device) for b in batch)\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device) for k, v in batch.items() if k != \"meta\"}\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust the dimensions of the faces tensor to match the batch dimension of the vertices tensor\n    if f.ndim == v.ndim - 1:\n        f = f.unsqueeze(0).expand(v.shape[0], *f.shape)\n    # gather the vertices for each face\n    v0 = multi_gather(v, f[:, :, 0], dim)\n    v1 = multi_gather(v, f[:, :, 1], dim)\n    v2 = multi_gather(v, f[:, :, 2], dim)\n    # compute the face normals\n    n = torch.cross(v1 - v0, v2 - v0, dim=-1)\n    # reshape the result to maintain the original faces tensor structure with additional dimensions for batch processing\n    return n.view(*f.shape, 3)"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        batch = batch[None]\n    else:\n        batch = torch.as_tensor(batch)[None]\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        batch.meta = dotdict()\n        batch.meta.H, batch.meta.W, batch.meta.K, batch.meta.R, batch.meta.T, batch.meta.n, batch.meta.f, batch.meta.t, batch.meta.v, batch.meta.bounds = self.H, self.W, self.K, self.R, self.T, self.n, self.f, self.t, self.v, self.bounds\n        return batch\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.serialize(agent)\n            self.persistence.save_agent(serialized_agent)"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            max_similarity = -float('inf')\n            closest_agent = None\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n                if similarity > max_similarity:\n                    max_similarity = similarity\n                    closest_agent = agent\n\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")\n\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(\n            prompt=PRIME_PROMPT,\n            name=PRIME_NAME,\n            weight=PRIME_AGENT_WEIGHT,\n            prime=True,\n            unspecified=True,\n        )\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent:\n            return AgentSerializer.deserialize(\n                serialized_agent, agent_lifecycle, openai_wrapper\n            )\n        return None"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": ""}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"Error saving agent: {e}\")\n            raise e\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = self.openai_wrapper.get_chat_completion(\n                PROMPT_ENGINEERING_SYSTEM_PROMPT,\n                PROMPT_ENGINEERING_TEMPLATE.format(goal=goal, sample_input=sample_input, examples=EXAMPLES)\n            )\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": ""}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.execute(\n            \"SELECT result FROM cache WHERE hash = ?\", (arg_hash,)\n        )\n        result = cursor.fetchone()\n        if result is None:\n            return None\n        return json.loads(result[0])\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update(vars(args))\n\n    # If quiet mode is enabled, redirect standard output to a file\n    if quiet_mode:\n        with open(os.devnull, 'w') as devnull:\n            with redirect_stdout(devnull):\n                # Execute the command line process\n                execute_command_line_process(args)\n    else:\n        # Execute the command line process\n        execute_command_line_process(args)\n\n"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apicon"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (self._last_time is not None and time() - self._last_time > self._refetch_interval):\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\n                \"The `state_dict` method should not be called from a DataLoader worker process. \"\n                \"Please call it from the main process or from a worker process that is not used for loading data.\"\n            )\n\n        return {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path,\n            \"input_dir_url\": self.input_dir.url,\n            \"item_loader_state\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._state_dict = state_dict\n\n        self.current_epoch = state_dict[\"current_epoch\"]\n        self.input_dir.path = state_dict[\"input_dir_path\"]\n        self.input_dir.url = state_dict[\"input_dir_url\"]\n        self.item_loader = BaseItemLoader.load_state_dict(state_dict[\"item_loader\"]) if state_dict[\"item_loader\"] else None\n        self.drop_last = state_dict[\"drop_last\"]\n        self.seed = state_dict[\"seed\"]\n        self.distributed_env.world_size = state_dict[\"world_size\"]\n        self.shuffle = state_dict[\"shuffle\"]\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        assert self._state_dict\n        assert self.worker_env\n        assert self.cache\n\n        state: Dict[str, Any] = self._state_dict\n\n        if self.shuffle != state[\"shuffle\"]:\n            raise ValueError(\n                f\"The shuffle parameter of the StreamingDataset instance is {self.shuffle} but the state dictionary has {state['shuffle']}.\"\n            )\n\n        if self.worker_env.world_size != state[\"world_size\"]:\n            raise ValueError(\n                f\"The world_size of the StreamingDataset instance is {self.worker_env.world_size} but the state dictionary has {state['world_size']}.\"\n            )\n\n        if self.cache.input_dir.path != state[\"input_dir_path\"]:\n            raise ValueError(\n                f\"The input_dir_path of the StreamingDataset instance is {self.cache.input_dir.path} but the state dictionary has {state['input_dir_path']}.\"\n            )\n\n        if self.cache.input_dir.url != state[\"input_dir_url\"]:\n            raise ValueError(\n                f\"The input_dir_url of the StreamingDataset instance is {self.cache.input_dir.url} but the state dictionary has {state['input_dir_url']}.\"\n            )\n\n        if self.seed != state[\"seed\"]:\n            raise ValueError(\n                f\"The seed of the StreamingDataset instance is {self.seed} but the state dictionary has {state['seed']}.\"\n            )\n\n        if self.item_loader is not None and self.item_loader.state_dict() != state[\"item_loader\"]:\n            raise ValueError(\n                f\"The item_loader state of the StreamingDataset instance is {self.item_loader.state_dict()} but the state dictionary has {state['item_loader']}.\"\n            )\n\n        if self.drop_last != state[\"drop_last\"]:\n            raise ValueError(\n                f\"The drop_last parameter"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Generate a unique directory name based on the input directory\n    cache_dir_name = hashlib.sha256(input_dir.encode(\"utf-8\")).hexdigest()\n\n    # Check if the environment variables are set\n    if os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"):\n        cache_dir = os.path.join(os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"), cache_dir_name)\n    else:\n        cache_dir = os.path.join(_DEFAULT_CACHE_DIR, cache_dir_name)\n\n    # Create the cache directory if it doesn't exist\n    if not os.path.exists(cache_dir):\n        try:\n            os.makedirs(cache_dir)\n        except Exception as e:\n            logger.error(f\"Failed to create cache directory: {e}\")\n            return None\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        parsed_url = parse.urlparse(remote_filepath)\n        if parsed_url.scheme != \"s3\":\n            raise ValueError(f\"Invalid S3 URL: {remote_filepath}\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        lock_filepath = f\"{local_filepath}.lock\"\n        with FileLock(lock_filepath, timeout=10):\n            if self._s5cmd_available:\n                subprocess.run(\n                    f\"s5cmd --no-sign-request cp {remote_filepath} {local_filepath}\",\n                    shell=True,\n                    check=True,\n                )\n            else:\n                self._client.download_file(\n                    parsed_url.netloc, parsed_url.path[1:], local_filepath\n                )\n\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Initialize empty lists to store the chunks and intervals assigned to each worker\n    workers_chunks = [[] for _ in range(num_workers)]\n    workers_intervals = [[] for _ in range(num_workers)]\n\n    # Iterate over the chunks and intervals\n    for chunk_index, interval in zip(chunks_replica, intervals_replica):\n        # Determine the worker index based on the chunk index and the world size\n        worker_index = chunk_index % worker_env.world_size\n\n        # Assign the chunk and its interval to the corresponding worker\n        workers_chunks[worker_index].append(chunk_index)\n        workers_intervals[worker_index].append(interval)\n\n    return workers_chunks, workers_intervals\n\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[6:]\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        if not isinstance(item, Image):\n            raise ValueError(f\"Expected PIL Image, got {type(item)}\")\n\n        with io.BytesIO() as f:\n            item.save(f, format=\"PNG\")\n            data = f.getvalue()\n\n        return data, None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                return item.tobytes(), None\n        elif isinstance(item, Image):\n            return item.convert(\"RGB\").tobytes(), None\n        else:\n            raise TypeError(f\"Unsupported image type: {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[:12], np.uint32)\n        width, height, mode_size = ints\n        mode = data[12 : 12 + mode_size].decode(\"utf-8\")\n        raw = data[12 + mode_size :]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[:4], np.uint32)[0]\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        ndim = np.frombuffer(data[4:8], np.uint32)[0]\n        shape = np.frombuffer(data[8 : 8 + ndim * 4], np.uint32)\n        data = data[8 + ndim * 4 :]\n        return torch.from_numpy(np.frombuffer(data, dtype=dtype).reshape(shape))\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype = item.dtype\n        shape = item.shape\n        raw = item.numpy().tobytes()\n        ints = np.array([self._dtype_to_indices[dtype], *shape], np.uint32)\n        return ints.tobytes() + raw, None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        return PILSerializer().deserialize(data)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        if self._dtype is None:\n            raise ValueError(\"The data type is not defined. HINT: Use the setup method to define the data type.\")\n        if item.dtype != self._dtype:\n            raise ValueError(f\"The provided item should be of type {self._dtype}. Found {item.dtype}.\")\n        return item.numpy().tobytes(order=\"C\"), None\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        if self._dtype is None:\n            raise ValueError(\"The data type is not defined. HINT: Use the setup method to define the data type.\")\n        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        array = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if array.shape == shape:\n            return array\n        return np.reshape(array, shape)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise ImportError(\n                \"The torchvision.io.read_video function requires the av library. Please install it with `pip install av`.\"\n            )\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.flush()\n            return torchvision.io.read_video(f.name)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return []\n        while self._should_write():\n            self.write_chunk()\n        self.write_chunk(on_done=True)\n        self._is_done = True\n        return [self.write_chunks_index()]\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self.restore = True\n        self._latest_worker_idx = obj[\"latest_worker_idx\"]\n\n        if isinstance(self.dataset, StreamingDataset):\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            else:\n                return {\n                    __NUM_SAMPLES_YIELDED_KEY__: num_samples_yielded,\n                }\n\n        if self._iterator._state_dict is None:\n            return {\n                __SAMPLES_KEY__: [\n                    dataset.state_dict(num_workers, batch_size, num_samples_yielded)\n                    for dataset in self._datasets\n                ],\n            }\n        else:\n            return {\n                __SAMPLES_KEY__: self._iterator._state_dict,\n            }\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if self._iterator is None:\n            self._iterator = _CombinedDatasetIterator(self._datasets, self._seed, self._weights, self._use_streaming_dataloader)\n        self._iterator.load_state_dict(state_dict)\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"gs://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"dbfs:/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"file://\"):\n        return Dir(path=dir_path.replace(\"file://\", \"\"))\n\n    if dir_path.startswith(\"dbfs://\"):\n        return Dir(path=dir_path.replace(\"dbfs://\", \"dbfs:/\"))\n\n    if dir_path.startswith(\"s3a://\"):\n        return Dir(path=dir_path.replace(\"s3a://\", \"s3://\"))\n\n    if dir_path.startswith(\"s3n://\"):\n        return Dir(path=dir_path.replace(\"s3n://\", \"s3://\"))\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"gs://\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"wasbs://\"):\n        return Dir(path=dir_path.replace(\"wasbs://\", \"wasb://\"))\n\n    if dir_path.startswith(\"wasb://\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"wasbs://\"):\n        return Dir(path=dir_path.replace(\"wasbs://\", \"wasb://\"))\n\n    if dir_path.startswith(\"abfss://\"):\n        return Dir(path=dir_path.replace(\"abfss://\", \"abfs://\"))\n\n    if dir_path.startswith(\"abfs://\"):\n        return Dir("}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"`output_dir` must be a `Dir`, got: {output_dir}\")\n\n    if output_dir.url is None:\n        raise ValueError(f\"`output_dir` must be a `Dir` with a `url`, got: {output_dir}\")\n\n    if not output_dir.url.startswith(\"s3://\"):\n        raise ValueError(f\"`output_dir` must be a `Dir` with a `url` starting with 's3://', got: {output_dir}\")\n\n    if append:\n        raise NotImplementedError(\"`append` is not implemented yet.\")\n\n    if overwrite:\n        raise NotImplementedError(\"`overwrite` is not implemented yet.\")\n\n    if _BOTO3_AVAILABLE:\n        parsed_url = parse.urlparse(output_dir.url)\n        bucket_name = parsed_url.netloc\n        prefix = parsed_url.path.lstrip(\"/\")\n\n        s3 = boto3.resource(\"s3\")\n        bucket = s3.Bucket(bucket_name)\n\n        if bucket.objects.filter(Prefix=prefix).count() > 0:\n            raise ValueError(f\"The directory {output_dir.url} is not empty.\")\n\n    else:\n        raise NotImplementedError(\"`_assert_dir_is_empty` is not implemented yet.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    try:\n        s3.get_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/index.json\")\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n            return\n        else:\n            raise e\n\n    # Delete all objects in the bucket\n    s3.delete_objects(\n        Bucket=obj.netloc,\n        Delete={\n            \"Objects\": [{\"Key\": obj[\"Key\"]} for obj in s3.list_objects_v2(Bucket=obj.netloc, Prefix=obj.path.lstrip(\"/\").rstrip(\"/\"))[\"Contents\"]]\n        },\n    )\n\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if node_rank is None:\n            node_rank = get_worker_rank()\n\n        if node_rank is not None and node_rank != 0:\n            while not os.path.exists(os.path.join(self._cache_dir, f\"{node_rank}.{_INDEX_FILENAME}\")):\n                sleep(0.1)\n            return\n\n        while len(os.listdir(self._cache_dir)) < num_workers:\n            sleep(0.1)\n\n        chunks_info: List[Dict[str, Any]] = []\n        for i in range(num_workers):\n            with open(os.path.join(self._cache_dir, f\"{i}.{_INDEX_FILENAME}\")) as f:\n                chunks_info.extend(json.load(f)[\"chunks\"])\n\n        chunks_info = sorted(chunks_info, key=lambda x: x[\"chunk_index\"])\n\n        with open(os.path.join(self._cache_dir, _INDEX_FILENAME), \"w\") as f:\n            json.dump({\"chunks\": chunks_info}, f, sort_keys=True)\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\n            \"The `lightning_sdk` package is required to execute this operator remotely. \"\n            \"Please install it with `pip install lightning-sdk`.\"\n        )\n\n    if not _BOTO3_AVAILABLE:\n        raise RuntimeError(\n            \"The `boto3` package is required to execute this operator remotely. \"\n            \"Please install it with `pip install boto3`.\"\n        )\n\n    if machine is None:\n        machine = Machine(\n            name=\"default\",\n            accelerators=\"auto\",\n            cpu_count=1,\n            memory=1000,\n            disk_size=1000,\n            image=\"lightning/base:latest\",\n        )\n\n    if command is None:\n        command = \" \".join(\n            [\n                f\"cd {os.getcwd()}\",\n                \"&&\",\n                \" \".join([f\"{k}={v}\" for k, v in os.environ.items()]),\n                \"&&\",\n                \" \".join(sys.argv),\n            ]\n        )\n\n    # Get the ids from env variables\n    cluster_id = os.getenv(\"LIGHTNING_CLUSTER_ID\", None)\n    project_id = os.getenv(\"LIGHTNING_CLOUD_PROJECT_ID\", None)\n    cloud_space_id = os.getenv(\"LIGHTNING_CLOUD_SPACE_ID\", None)\n\n    if cluster_id is None:\n        raise RuntimeError(\"The `cluster_id` couldn't be found from the environement variables.\")\n\n    if project_id is None:\n        raise RuntimeError(\"The `project_id` couldn't be found from the environement variables.\")\n\n    if cloud_space_id is None:\n        raise RuntimeError(\"The `cloud_space_id` couldn't be found from the environement variables.\")\n\n    client = LightningClient(max_tries=2)"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        if self._config is None:\n            try:\n                self._config = ChunksConfig.from_cache_dir(\n                    self._cache_dir,\n                    self._serializers,\n                    self._remote_input_dir,\n                    self._item_loader,\n                )\n            except FileNotFoundError:\n                return None\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The config should be defined before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The index should be an instance of ChunkedIndex. Got {type(index)}.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index from the index\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Check whether the chunk is available locally\n        if not self._config.is_chunk_available(chunk_index):\n            # Download the chunk\n            self._config.download_chunk_from_index(chunk_index)\n\n        # Check whether the chunk is available in memory\n        if not self._config.is_chunk_in_memory(chunk_index):\n            # Load the chunk in memory\n            self._config.load_chunk_from_index(chunk_index)\n\n        # Prefetch the next chunk\n        self._prefetch_next_chunk(chunk_index)\n\n        # Get the item from the chunk\n        item = self._config.get_item_from_chunk(chunk_index, index.item_index)\n\n        # Delete the chunk if it's the last one\n        if self._last_chunk_index == chunk_index:\n            self._delete_chunk(chunk_index)\n\n        return item\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Set the seed for the current epoch\n    np.random.seed(seed + current_epoch)\n\n    # Shuffle the chunk indexes for each rank\n    shuffled_chunks_per_ranks = [\n        np.random.permutation(chunks).tolist() for chunks in chunks_per_ranks\n    ]\n\n    # Flatten the shuffled chunk indexes\n    shuffled_chunks = [\n        chunk for chunks in shuffled_chunks_per_ranks for chunk in chunks\n    ]\n\n    return shuffled_chunks\n\n"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    if len(inputs) == 0:\n        return None\n\n    if len(inputs) == 1:\n        if isinstance(inputs[0], str):\n            return os.path.abspath(inputs[0])\n        else:\n            return None\n\n    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) > 1:\n        raise ValueError(\n            \"More than one file path found in the inputs. Please provide a single file path.\"\n        )\n\n    indexed_path = list(indexed_paths.values())[0]\n\n    if not os.path.exists(indexed_path):\n        raise ValueError(f\"File path {indexed_path} does not exist.\")\n\n    if _IS_IN_STUDIO:\n        return os.path.abspath(indexed_path)\n\n    return os.path.abspath(indexed_path)\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if _IS_IN_STUDIO:\n        import lightning_app\n\n        if enable:\n            lightning_app.utilities.network.enable_dns_optimization()\n        else:\n            lightning_app.utilities.network.disable_dns_optimization()\n\n    try:\n        yield\n    finally:\n        if _IS_IN_STUDIO:\n            import lightning_app\n\n            lightning_app.utilities.network.disable_dns_optimization()\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # Calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    if drop_last:\n        num_items_per_rank = num_items_per_rank * distributed_env.world_size\n    else:\n        num_items_per_rank = num_items_per_rank + len(indexes) % distributed_env.world_size\n\n    # Assign chunks and their intervals to each rank\n    chunks_per_ranks = []\n    intervals_per_ranks = []\n    for rank in range(distributed_env.world_size):\n        start_index = rank * num_items_per_rank\n        end_index = start_index + num_items_per_rank\n        chunks_per_ranks.append(indexes[start_index:end_index])\n        intervals_per_ranks.append(chunk_intervals[start_index:end_index])\n\n    return chunks_per_ranks, intervals_per_ranks"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_device:\n            kwargs[\"device\"] = self._device\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(input"}
{"namespace": "litdata.processing.functions.map", "completion": "    if _IS_IN_STUDIO:\n        if num_nodes is None:\n            num_nodes = int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", \"1\"))\n\n        if machine is None:\n            machine = os.getenv(\"DATA_OPTIMIZER_MACHINE\", \"\")\n\n        if num_downloaders is None:\n            num_downloaders = int(os.getenv(\"DATA_OPTIMIZER_NUM_DOWNLOADERS\", \"1\"))\n\n        if num_uploaders is None:\n            num_uploaders = int(os.getenv(\"DATA_OPTIMIZER_NUM_UPLOADERS\", \"1\"))\n\n    if num_workers is None:\n        num_workers = _get_default_num_workers()\n\n    if num_workers < 1:\n        raise ValueError(f\"The provided num_workers {num_workers} is invalid.\")\n\n    if fast_dev_run is True:\n        fast_dev_run = 1\n\n    if isinstance(fast_dev_run, int) and fast_dev_run < 1:\n        raise ValueError(f\"The provided fast_dev_run {fast_dev_run} is invalid.\")\n\n    if fast_dev_run is not False and fast_dev_run is not None:\n        if not isinstance(fast_dev_run, int):\n            raise ValueError(f\"The provided fast_dev_run {fast_dev_run} is invalid.\")\n\n        if fast_dev_run > len(inputs):\n            raise ValueError(f\"The provided fast_dev_run {fast_dev_run} is invalid.\")\n\n    if num_nodes is not None and num_nodes < 1:\n        raise ValueError(f\"The provided num_nodes {num_nodes} is invalid.\")\n\n    if num_downloaders is not None and num_downloaders < 1:\n        raise ValueError(f\"The provided num_downloaders {num_downloaders} is invalid.\")\n\n    if num_uploaders is not None and num_uploaders < 1:\n        raise Value"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    if input_dir.is_local:\n        return\n\n    s3 = S3Client(input_dir.url)\n\n    while True:\n        try:\n            task = queue_in.get(timeout=1)\n        except Empty:\n            continue\n\n        if task is None:\n            break\n\n        index, files = task\n\n        for file in files:\n            obj = parse.urlparse(file)\n            file_path = os.path.join(cache_dir, obj.path.lstrip(\"/\"))\n            if not os.path.exists(file_path):\n                os.makedirs(os.path.dirname(file_path), exist_ok=True)\n                s3.client.download_file(obj.netloc, obj.path.lstrip(\"/\"), file_path)\n\n        queue_out.put(index)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Collect paths\n        paths = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and delete them sequentially.\n        for path in paths:\n            if isinstance(path, tuple):\n                tmp_dir, path = path\n\n            if output_dir.path and not path.startswith(output_dir.path):\n                path = path.replace(cache_dir, output_dir.path)\n\n            if output_dir.url and not path.startswith(output_dir.url):\n                path = path.replace(cache_dir, output_dir.url)\n\n            obj = parse.urlparse(path)\n\n            if obj.scheme == \"s3\":\n                dirpath = os.path.dirname(path)\n\n                if not dirpath.startswith(output_dir.url):\n                    dirpath = dirpath.replace(cache_dir, output_dir.url)\n\n                s3.client.put_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"), Body=open(path, \"rb\"))\n\n                # 4. Wait for the file to be available\n                _wait_for_file_to_exist(s3, obj)\n\n                # 5. Remove the file\n                remove_queue.put(path)\n\n            elif os.path.isfile(path):\n                if not path.startswith(\"/teamspace/studios/this_studio\"):\n                    os.makedirs(os.path.dirname(path), exist_ok=True)\n                    shutil.copyfile(path, path)\n\n            else:\n                raise ValueError(f\"The provided {output_dir.url} isn't supported.\")\n\n            # 6. Remove the file\n            remove_queue.put(path)\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Print the distribution details for workers on the current node\n    for worker_id in worker_ids_this_node:\n        worker_items_this_worker = worker_items[worker_id]\n        worker_weights_this_worker = worker_weights[worker_id]\n        worker_size_this_worker = sum(worker_weights_this_worker)\n        if file_size:\n            worker_size_this_worker = worker_size_this_worker / 1024 / 1024\n        print(f\"Worker {worker_id} has {len(worker_items_this_worker)} items with a total size of {worker_size_this_worker:.2f} MB\")\n\n    # Return a list of items for each worker, with the items shuffled randomly\n    return [list(np.random.permutation(worker_items[worker_id])) for worker_id in worker_ids_this_node]\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    num_nodes = _get_num_nodes()\n    node_rank = _get_node_rank()\n\n    num_workers_per_node = num_workers\n    num_workers = num_workers_per_node * num_nodes\n\n    num_items = len(user_items)\n    num_items_per_worker = num_items // num_workers\n    num_items_remainder = num_items % num_workers\n\n    # Calculate the start and end indices for each worker's items\n    start_indices = np.cumsum([0] + [num_items_per_worker] * (num_workers - 1))\n    end_indices = np.cumsum([num_items_per_worker] * num_workers)\n\n    # Adjust for any remainder by adding extra items to the workers starting from the end of the list\n    for i in range(num_items_remainder):\n        end_indices[-(i + 1)] += 1\n\n    # Assign items to workers\n    items_per_worker = [user_items[start:end] for start, end in zip(start_indices, end_indices)]\n\n    # Ensure the output list has a length equal to the number of workers\n    if len(items_per_worker) != num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers\")\n\n    return items_per_worker"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": ""}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    for future in concurrent.futures.as_completed(futures):\n        item_sizes.append(future.result())\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": ""}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            import tinycudann as tcnn\n\n            if n_neurons < 16:\n                network_type = tcnn.NetworkWithInputEncoding\n            elif n_neurons < 64:\n                network_type = tcnn.NetworkWithInputSkips\n            else:\n                network_type = tcnn.Network\n\n            return network_type(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                encoding_config={\n                    \"otype\": \"HashGrid\",\n                    \"n_levels\": 16,\n                    \"n_features_per_level\": 2,\n                    \"log2_hashmap_size\": 19,\n                    \"base_resolution\": 16,\n                    \"per_level_scale\": 1.3819,\n                },\n                network_config={\n                    \"otype\": \"FullyFusedMLP\",\n                    \"activation\": activation,\n                    \"output_activation\": output_activation,\n                    \"n_neurons\": n_neurons,\n                    \"n_hidden_layers\": n_layers - 1,\n                },\n            )\n        else:\n            layers = [\n                nn.Linear(n_input_dims, n_neurons),\n                nn.ReLU(),\n            ]\n            for i in range(n_layers - 2):\n                layers.append(nn.Linear(n_neurons, n_neurons))\n                layers.append(nn.ReLU())\n            layers.append(nn.Linear(n_neurons, n_output_dims))\n            if output_activation == \"ReLU\":\n                layers.append(nn.ReLU())\n            elif output_activation == \"Sigmoid\":\n                layers.append(nn.Sigmoid())\n            return nn.Sequential(*layers)"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        shifted_signals = [np.roll(signal, i) for i in range(-kernel_offset, kernel_offset + 1)]\n        median_array = np.median(shifted_signals, axis=0)\n        return median_array[kernel_offset:-kernel_offset]"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if template_probe.codesize != template_gallery.codesize:\n        raise MatcherError(\"Codesize of probe and gallery are not equal.\")\n\n    if template_probe.codesize % 2 != 0:\n        raise MatcherError(\"Codesize of probe and gallery must be even.\")\n\n    if rotation_shift < 0:\n        raise MatcherError(\"Rotation shift must be non-negative.\")\n\n    if rotation_shift > template_probe.codesize // 2:\n        raise MatcherError(\"Rotation shift must be less than half of the codesize.\")\n\n    if nm_dist is not None and nm_dist < 0:\n        raise MatcherError(\"Nonmatch distance must be non-negative.\")\n\n    if weights is not None and len(weights) != template_probe.codesize:\n        raise MatcherError(\"Weights must be a list of arrays with the same length as the codesize.\")\n\n    if weights is not None and any(w.shape != (template_probe.codesize, template_probe.codesize) for w in weights):\n        raise MatcherError(\"Weights must be a list of arrays with shape (codesize, codesize).\")\n\n    if weights is not None and any(not np.all(np.isclose(w, w.T)) for w in weights):\n        raise MatcherError(\"Weights must be a list of symmetric arrays.\")\n\n    if weights is not None and any(not np.all(np.isclose(w, 0) | np.isclose(w, 1)) for w in weights):\n        raise MatcherError(\"Weights must be a list of arrays with values 0 or 1.\")\n\n    if weights is not None and any(not np.all(np.isclose(np.sum(w), 1)) for w in weights):\n        raise MatcherError(\"Weights must be a list of arrays with sum 1.\")\n\n    if weights is not None and any(not np.all(np.isclose(np.sum(w, axis=0), 1)) for w in weights):\n        raise MatcherError"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        for _ in range(max_iterations):\n            first_bisectors_point = []\n            second_bisectors_point = []\n            for _ in range(num_bisectors):\n                first_point, second_point = self._find_random_points_pair(polygon, min_distance_between_sector_points_in_px)\n                first_bisectors_point.append(first_point)\n                second_bisectors_point.append(second_point)\n\n            if len(first_bisectors_point) == num_bisectors:\n                return np.array(first_bisectors_point), np.array(second_bisectors_point)\n\n        raise EyeCentersEstimationError(\"Failed to find a sufficient number of point pairs that meet the distance criterion.\")\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": ""}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n        docstring = inspect.getdoc(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n\n        for param_name, param in signature.parameters.items():\n            if param.kind == param.POSITIONAL_OR_KEYWORD:\n                input_type_hints[param_name] = type_hints.get(param_name, None)\n\n        output_type_hint = type_hints.get('return', None)\n\n        if output_type_hint:\n            if inspect.isclass(output_type_hint) or issubclass(output_type_hint, Union):\n                output_class_definition = get_class_definition(output_type_hint)\n                if issubclass(output_class_definition, Embedding):\n                    function_type = FunctionType.EMBEDDABLE\n                else:\n                    function_type = FunctionType.SYMBOLIC\n            else:\n                output_class_definition = get_class_definition(output_type_hint)\n                function_type = FunctionType.SYMBOLIC\n        else:\n            output_class_definition = None\n            function_type = FunctionType.SYMBOLIC\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=docstring,\n            input_type_hints=input_type_hints,\n            output_type_hints=output_type_hints,\n            input_class_definitions={param_name: get_class_definition(type_hint) for param_name, type_hint in\n                                     input_type_hints.items()},\n            output_class_definition=output_class_definition,\n            function_type=function_type,\n            source=get_source(func_object)\n        )\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] += 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        expected_length = self.size\n        if len(loaded_bit_array) != expected_length:\n            logging.warning(f\"Loaded bit array length ({len(loaded_bit_array)}) does not match expected length ({expected_length}). Reinitializing and saving.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array\n            self.indices = np.zeros(self.size, dtype=np.int32)\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for i in range(self.hash_count):\n            index = (hash1 + i * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.get_model_config(json_dict[DISTILLED_MODEL])\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if \"teacher_models\" in json_dict:\n            self.teacher_models = [config_factory.get_model_config(teacher_model) for teacher_model in json_dict[\"teacher_models\"]]\n        return self\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # Validate the parameters\n        for parameter in LLM_GENERATION_PARAMETERS:\n            if parameter in kwargs:\n                assert isinstance(kwargs[parameter], (int, float)), f\"{parameter} must be a number\"\n\n        # Set the default values for the parameters\n        temperature = kwargs.get(\"temperature\", 0.7)\n        top_p = kwargs.get(\"top_p\", 1)\n        frequency_penalty = kwargs.get(\"frequency_penalty\", 0)\n        presence_penalty = kwargs.get(\"presence_penalty\", 0)\n        max_new_tokens = kwargs.get(\"max_new_tokens\", 100)\n\n        # Set the model name and any parsing helper tokens\n        model_name = model.model_name\n        parsing_helper_tokens = model.parsing_helper_tokens\n\n        # Set the retry parameters\n        max_retries = 5\n        retry_delay = 1\n\n        # Set the initial response to None\n        response = None\n\n        # Loop until a response is received or the maximum number of retries is reached\n        for i in range(max_retries):\n            try:\n                # Send the request to the OpenAI API\n                response = self.client.chat.completions.create(\n                    model=model_name,\n                    messages=[\n                        {\"role\": \"system\", \"content\": system_message},\n                        {\"role\": \"user\", \"content\": prompt},\n                    ],\n                    temperature=temperature,\n                    top_p=top_p,\n                    frequency_penalty=frequency_penalty,\n                    presence_penalty=presence_penalty,\n                    max_tokens=max_new_tokens,\n                )\n\n                # If a response is received, break out of the loop\n                break\n            except Exception as e:\n                # If an error occurs, log the error and wait for the next retry\n                logging.error(f\"Error occurred while generating text: {"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal elements of the matrix must be close to zero\")\n\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Check if the function is already initialized\n        if func_hash in self.initialized_functions:\n            # If the function is already initialized, retrieve the model and examples\n            model = self.initialized_functions[func_hash][\"model\"]\n            examples = self.initialized_functions[func_hash][\"examples\"]\n        else:\n            # If the function is not initialized, initialize it with an empty model and examples\n            model = \"\"\n            examples = []\n\n        # Check if the function is suitable for distillation\n        if function_description.suitable_for_distillation:\n            # If the function is suitable for distillation, use the distilled model\n            model = self.api_provider.distilled_model\n            # Set the flag indicating that the model is suitable for distillation\n            is_distilled_model = True\n        else:\n            # If the function is not suitable for distillation, use the teacher model\n            model = self.api_provider.teacher_model\n            # Set the flag indicating that the model is not suitable for distillation\n            is_distilled_model = False\n\n        # Check if the function is already initialized and does not require saving examples for fine-tuning\n        if func_hash in self.initialized_functions and not is_distilled_model:\n            # If the function is already initialized and does not require saving examples for fine-tuning, set the flag to False\n            save_to_finetune = False\n        else:\n            # If the function is not initialized or requires saving examples for fine-tuning, set the flag to True\n            save_to_finetune = True\n\n        # Construct the prompt for generation\n        prompt = self.function_modeler.construct_prompt(args, kwargs, function_description, examples)\n\n        # Return the constructed prompt, the selected model, the flag indicating if the model is suitable for distillation, and the flag indicating if the function is already initialized and does not require saving examples for fine-tuning\n        return prompt, model, save_to_finetune, is_distilled_model\n"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if not is_positive_definite(cov):\n        if higham:\n            cov = _cov_nearest_higham(cov, max_iteration=higham_max_iteration)\n        else:\n            cov = _cov_nearest_clipped(cov)\n    return cov\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, bytes):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    elif isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    elif isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    elif isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    else:\n        raise ValueError(f\"Unsupported type {type(obj)}\")\n\n"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Convert groups and equations to numpy arrays\n    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n\n    # Check if groups and equations have the correct dimensions\n    if groups.ndim != 2:\n        raise EquationToMatrixError(\n            f\"{names[0]} must be a 2D array, but it has {groups.ndim} dimensions.\"\n        )\n    if equations.ndim != 1:\n        raise EquationToMatrixError(\n            f\"{names[1]} must be a 1D array, but it has {equations.ndim} dimensions.\"\n        )\n\n    # Check if groups and equations have the same number of columns\n    if groups.shape[1] != equations.shape[0]:\n        raise EquationToMatrixError(\n            f\"{names[0]} and {names[1]} must have the same number of columns.\"\n        )\n\n    # Check if groups and equations have the same number of rows\n    if groups.shape[0] != equations.shape[0]:\n        raise EquationToMatrixError(\n            f\"{names[0]} and {names[1]} must have the same number of rows.\"\n        )\n\n    # Check if groups and equations have the same number of columns\n    if groups.shape[1] != equations.shape[0]:\n        raise EquationToMatrixError(\n            f\"{names[0]} and {names[1]} must have the same number of columns.\"\n        )\n\n    # Check if groups and equations have the same number of rows\n    if groups.shape[0] != equations.shape[0]:\n        raise EquationToMatrixError(\n            f\"{names[0]} and {names[1]} must have the same number of rows.\"\n        )\n\n    # Check if groups and equations have the same number of columns\n    if groups.shape[1] != equations.shape[0]:\n        raise EquationToMatrixError(\n            f\"{names[0]} and {names[1]} must have the same number of columns.\"\n        )\n\n    # Check if groups and equations have the same number of rows\n    if groups.shape[0"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n    temp_dir = tempfile.TemporaryDirectory(prefix=\"detectron2_jit_\")\n    new_module_path = os.path.join(temp_dir.name, f\"instances_{_counter}.py\")\n    with open(new_module_path, \"w\") as f:\n        f.write(\n            f\"\"\""}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n\n        # capture and ignore this bug: https://github.com/python-pillow/Pillow/issues/3973\n        try:\n            image = _apply_exif_orientation(image)\n        except Exception:\n            pass\n\n        image = convert_PIL_to_numpy(image, format)\n\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (list, tuple)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box([bbox])[0].clip(image_size)\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform polygon or uncompressed RLE to tight XYXY_ABS format\n    # for segmentation.\n    segmentation = annotation[\"segmentation\"]\n    if isinstance(segmentation, list):\n        # Polygon\n        segmentation = [transforms.apply_segmentation(obj) for obj in segmentation]\n    else:\n        # uncompressed RLE\n        segmentation = transforms.apply_segmentation(segmentation)\n    annotation[\"segmentation\"] = segmentation\n\n    # Transform keypoints\n    keypoints = utils.transform_keypoint_annotations(\n        annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n    )\n    annotation[\"keypoints\"] = keypoints\n\n    return annotation\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        coords = coords[:, [1, 0]]  # to xy\n        # Transform image center to coordinate center\n        coords = coords - self.center\n        coords = np.dot(coords, self.rm_coords.T)\n        coords = coords + self.center\n        coords = coords[:, [1, 0]]  # to yx\n        return coords\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Create a TracingAdapter to handle the model and inputs\n    tracing_adapter = TracingAdapter(model, inputs, allow_non_tensor=True)\n\n    # Create a FlopCountAnalysis object to perform the flops computation\n    flop_counter = fvcore.nn.FlopCountAnalysis(tracing_adapter, tracing_adapter.flattened_inputs)\n\n    # Set the ignored operations\n    flop_counter.set_op_handle(**{k: None for k in _IGNORED_OPS})\n\n    # Return the flops count as a dictionary\n    return flop_counter.by_operator()\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        return cv2.warpAffine(img, self.rm_image, (self.bound_w, self.bound_h), flags=interp)\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        boxes = predictions.pred_boxes.tensor.numpy() if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes.numpy() if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        keypoints = predictions.pred_keypoints[0].numpy() if predictions.has(\"pred_keypoints\") else None\n\n        if predictions.has(\"pred_masks\"):\n            masks = np.asarray(predictions.pred_masks)\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n        else:\n            masks = None\n\n        if self._instance_mode == ColorMode.IMAGE_BW:  # change color of text to white\n            self.output.ax.text(\n                x=self._default_font_size,\n                y=self._default_font_size,\n                s=\"Score: {:.2f}\".format(scores[0]),\n                color=\"white\",\n                horizontalalignment=\"left\",\n                verticalalignment=\"top\",\n                fontsize=self._default_font_size,\n            )\n            self.output.ax.text(\n                x=self._default_font_size,\n                y=self._default_font_size * 2,\n                s=\"Class: {}\".format(labels[0]),\n                color=\"white\",\n                horizontalalignment=\"left\",\n                verticalalignment=\"top\",\n                fontsize=self._default_font_size,\n            )\n\n        if self._instance_mode == ColorMode.IMAGE_BW and predictions.has(\"pred_masks\"):\n            # use grayscale image for mask visualization\n            self.output.reset_image(self.img[:, :, ::-1])\n            alpha = 0.5\n            for m, s in"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        canvas = self.canvas\n        s, (width, height) = canvas.print_to_buffer()\n        buffer = np.frombuffer(s, dtype=\"uint8\")\n\n        img_rgba = buffer.reshape(height, width, 4)\n        rgb, alpha = np.split(img_rgba, [3], axis=2)\n        return rgb.astype(\"uint8\")"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        annos = dic.get(\"annotations\", None)\n        if annos:\n            if \"segmentation\" in annos[0]:\n                masks = [x[\"segmentation\"] for x in annos]\n            else:\n                masks = None\n            if \"keypoints\" in annos[0]:\n                keypts = [x[\"keypoints\"] for x in annos]\n                keypts = np.array(keypts).reshape(len(keypts), -1, 3)\n            else:\n                keypts = None\n\n            boxes = [BoxMode.convert(x[\"bbox\"], x[\"bbox_mode\"], BoxMode.XYXY_ABS) for x in annos]\n            boxes = np.array(boxes)\n            labels = [x[\"category_id\"] for x in annos]\n            names = self.metadata.get(\"thing_classes\", None)\n            labels = [names[i] for i in labels] if names else labels\n            colors = [x[\"color\"] for x in annos] if \"color\" in annos[0] else None\n            area = [x[\"area\"] for x in annos] if \"area\" in annos[0] else None\n            self.overlay_instances(\n                labels=labels,\n                boxes=boxes,\n                masks=masks,\n                keypoints=keypts,\n                assigned_colors=colors,\n                alpha=0.3,\n                area=area,\n            )\n\n        sem_seg = dic.get(\"sem_seg\", None)\n        if sem_seg is None and \"sem_seg_file_name\" in dic:\n            sem_seg = PathManager.get_local_path(dic[\"sem_seg_file_name\"])\n        if sem_seg is not None:\n            self.draw_sem_seg(sem_seg, area_threshold=0, alpha=0.5)\n\n        pan_seg = dic.get(\"pan_seg\", None)\n        segments_info = dic.get(\"segments_info\", None)\n        if pan_"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        if edge_color is None:\n            edge_color = color\n\n        binary_mask = np.asarray(binary_mask)\n        if binary_mask.shape[0] == 0 or binary_mask.shape[1] == 0:\n            return self.output\n\n        # If the mask is not binary, convert it to binary\n        if binary_mask.dtype != bool:\n            binary_mask = binary_mask > 0\n\n        # Find connected components\n        mask = np.asarray(binary_mask, dtype=np.uint8, order=\"F\")\n        cv2.connectedComponents(mask, connectivity=4)\n        components = cv2.findContours(mask, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n        components = components[0] if len(components) == 2 else components[1]\n\n        # Draw polygons for each component\n        for component in components:\n            if component.shape[0] < 3:\n                continue\n            if cv2.contourArea(component) < area_threshold:\n                continue\n            component = component.astype(np.int32)\n            p = component.reshape(-1, 2)\n            self.draw_polygon(p, color, edge_color=edge_color, alpha=alpha)\n\n        # Draw text on the mask\n        if text is not None:\n            # Compute the center of the mask\n            x, y = np.mean(p, axis=0).astype(np.int32)\n            # Draw the text on the mask\n            self.draw_text(text, (x, y), color=edge_color, horizontal_alignment=\"center\")\n\n        return self.output\n"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances) and isinstance(\n        other, Instances\n    ), f\"Expect input and other to be Instances, but got {type(input)} and {type(other)}!\"\n    if size_as_tensor:\n        assert (\n            input.image_size == other.image_size\n        ), f\"image_size mismatch: {input.image_size} vs {other.image_size}\"\n    else:\n        assert (\n            input.image_size == other.image_size\n        ), f\"image_size mismatch: {tuple(input.image_size)} vs {tuple(other.image_size)}\"\n\n    for name in input._field_names:\n        val = getattr(input, name)\n        other_val = getattr(other, name)\n        if isinstance(val, Boxes):\n            assert torch.allclose(val.tensor, other_val.tensor), f\"{msg} {name} mismatch\"\n        elif isinstance(val, ROIMasks):\n            assert val.tensor.allclose(other_val.tensor), f\"{msg} {name} mismatch\"\n        elif isinstance(val, torch.Tensor):\n            assert torch.allclose(val, other_val, rtol=rtol), f\"{msg} {name} mismatch\"\n        elif isinstance(val, (list, tuple)):\n            assert len(val) == len(\n                other_val\n            ), f\"{msg} {name} length mismatch: {len(val)} vs {len(other_val)}\"\n            for v, ov in zip(val, other_val):\n                assert torch.allclose(v, ov, rtol=rtol), f\"{msg} {name} mismatch\"\n        elif isinstance(val, dict):\n            assert len(val) == len(\n                other_val\n            ), f\"{msg} {name} length mismatch: {len(val)} vs {len(other_val)}\"\n            for k in val:\n                assert k in other_val, f\"{msg} {"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        box = self.tensor\n        area = (box[:, 2] * box[:, 3]).to(box.dtype)\n        return area\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n\n        # parse classification outputs\n        gt_classes = (\n            cat([p.gt_classes for p in proposals], dim=0) if len(proposals) else torch.empty(0)\n        )\n        _log_classification_stats(scores, gt_classes)\n\n        if len(proposals):\n            proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)  # Nx4\n            assert not proposal_boxes.requires_grad, \"Proposals do not need gradients!\"\n            # If \"gt_boxes\" does not exist, the proposals must be all negative and\n            # should not be included in regression loss computation and proposal_boxes\n            # should be an empty tensor.\n            if len(proposals) and \"gt_boxes\" in proposals[0]:\n                gt_boxes = cat([p.gt_boxes.tensor for p in proposals], dim=0)\n            else:\n                gt_boxes = torch.empty((0, 4))\n\n            # box regression\n            box_dim = proposal_deltas.shape[1] // self.num_classes\n            cls_agnostic_bbox_reg = self.box2box_transform.weights is None\n            box_deltas = proposal_deltas.view(-1, self.num_classes, box_dim)\n            if cls_agnostic_bbox_reg:\n                box_deltas = box_deltas[:, 0]\n\n            if self.box_reg_loss_type == \"smooth_l1\":\n                box_loss = smooth_l1_loss(\n                    box_deltas,\n                    gt_boxes,\n                    proposal_boxes,\n                    self.smooth_l1_beta,\n                    reduction=\"none\",\n                )\n            elif self.box_reg_loss_type == \"giou\":\n                box_loss = giou_loss(\n                    Boxes(box_deltas),\n                    Boxes(gt"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.TRACKER.NAME\n    return TRACKER_HEADS_REGISTRY.get(name)(cfg)"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        deltas = deltas.float()  # ensure fp32 for decoding precision\n        boxes = boxes.to(deltas.dtype)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w  # x1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h  # y1\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w  # x2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h  # y2\n\n        return pred_boxes"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            return self.general_ins(image)\n        elif isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            assert all(tp in self.anno_type_map.keys() for tp in anno_type)\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        output = self.general_ins(image)\n        if len(anno_type) == 1:\n            return output[anno_type[0]]\n        else:\n            return {tp: output[tp] for tp in anno_type}"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = normalize_string(query)\n        keywords = query.split()\n        scores = defaultdict(float)\n        for kw in keywords:\n            scores = update_url_scores(scores, self.bm25(kw))\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        h, w = box_size\n        boxes = self.tensor\n        x_ctr, y_ctr, width, height, angle = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3], boxes[:, 4]\n        # Step 1: Convert rotated boxes to horizontal boxes\n        # Normalize the angle to be within the range (-180, 180] degrees\n        angle = (angle + 180.0) % 360.0 - 180.0\n        # Find the indices of boxes that are nearly horizontal\n        nearly_horizontal_indices = torch.abs(angle) < clip_angle_threshold\n        # Convert the nearly horizontal boxes to horizontal boxes\n        if nearly_horizontal_indices.any():\n            boxes_horizontal = boxes[nearly_horizontal_indices].clone()\n            x1 = boxes_horizontal[:, 0] - boxes_horizontal[:, 2] / 2.0\n            y1 = boxes_horizontal[:, 1] - boxes_horizontal[:, 3] / 2.0\n            x2 = boxes_horizontal[:, 0] + boxes_horizontal[:, 2] / 2.0\n            y2 = boxes_horizontal[:, 1] + boxes_horizontal[:, 3] / 2.0\n            # Step 2: Clip the horizontal boxes\n            x1 = torch.clamp(x1, min=0, max=w)\n            y1 = torch.clamp(y1, min=0, max=h)\n            x2 = torch.clamp(x2, min=0, max=w)\n            y2 = torch.clamp(y2, min=0, max=h)\n            # Step 3: Convert the horizontal boxes back to rotated boxes\n            boxes_horizontal[:, 0] = (x1 + x2) / 2.0\n            boxes_horizontal[:, 1] = (y1 + y2) / 2.0\n            boxes_horizontal[:, 2] = x2 - x1\n            boxes"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0,\n        }\n        for item in self.data:\n            if item['type'] in statistics:\n                statistics[item['type']] += 1\n        return statistics"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMDET_LOSSES._module_dict.keys():\n        return MMDET_LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        raise NotImplementedError\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    return SEGMENTORS.build(\n        cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if 'train_cfg' in cfg:\n        warnings.warn('train_cfg is deprecated, please specify them in model')\n    if 'test_cfg' in cfg:\n        warnings.warn('test_cfg is deprecated, please specify them in model')\n\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(dt_annos) == len(gt_annos)\n    if box_type_3d == 'Depth' or box_type_3d == 'BEV':\n        gt_annos = ind_eval_utils.depth_box3d_to_bev_box3d(gt_annos)\n        dt_annos = ind_eval_utils.depth_box3d_to_bev_box3d(dt_annos)\n    elif box_type_3d != 'Camera' and box_type_3d is not None:\n        raise NotImplementedError(\n            f'Unsupported box_type_3d {box_type_3d} for indoor eval')\n\n    if box_mode_3d != 'lidar' and box_mode_3d is not None:\n        raise NotImplementedError(\n            f'Unsupported box_mode_3d {box_mode_3d} for indoor eval')\n\n    metric = [0.2, 0.5] if metric is None else metric\n    ap_result_str, ar_result_str = '', ''\n    ap_dict = {}\n    for i, thresh in enumerate(metric):\n        recall_dict, precision_dict, ap_dict_ = eval_map_recall(\n            gt_annos, dt_annos, [thresh], label2cat)\n        recall = list(recall_dict.values())[0]\n        precision = list(precision_dict.values())[0]\n        for j, name in enumerate(label2cat.values()):\n            if name in ap_dict:\n                ap_dict[name][i] = ap_dict_[name]\n            else:\n                ap_dict[name] = [0] * len(metric)\n                ap_dict[name][i] = ap_dict_[name]\n        ar = np.mean(recall)\n        ap = np.mean(precision)\n        recall_list = list(np.around(np.array(recall), decimals=2))\n        precision_"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    from .box_3d_mode import (Box3DMode, CameraInstance3DBoxes,\n                              DepthInstance3DBoxes, LiDARInstance3DBoxes)\n    box_type_lower = box_type.lower()\n    if box_type_lower == 'lidar':\n        box_type_3d = LiDARInstance3DBoxes\n        box_mode_3d = Box3DMode.LIDAR\n    elif box_type_lower == 'camera':\n        box_type_3d = CameraInstance3DBoxes\n        box_mode_3d = Box3DMode.CAM\n    elif box_type_lower == 'depth':\n        box_type_3d = DepthInstance3DBoxes\n        box_mode_3d = Box3DMode.DEPTH\n    else:\n        raise ValueError('Only \"Lidar\", \"Camera\", or \"Depth\" boxes are supported')\n\n    return box_type_3d, box_mode_3d"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if not messages:\n      raise RequestError('must provide at least one message')\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    for message in messages:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n\n      if 'role' not in message:\n        raise RequestError('messages must have a role')\n\n      if 'content' not in message:\n        raise RequestError('messages must have content')\n\n      if 'images' in message:\n        message['images'] = [_encode_image(image) for image in message['images']]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'model': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('model is required')\n\n    if images:\n      images = [\n        b64encode(image.read() if isinstance(image, PathLike) else image).decode('utf-8')\n        for image in images\n      ]\n\n    if context is None:\n      context = []\n\n    if options is None:\n      options = {}\n\n    if keep_alive is not None:\n      options['keep_alive'] = keep_alive\n\n    return self._request_stream(\n      'POST',\n      '/generate',\n      stream=stream,\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context,\n        'raw': raw,\n        'format': format,\n        'images': images,\n        'options': options,\n      },\n    )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not path and not modelfile:\n      raise RequestError('must provide a path or modelfile')\n\n    if path:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': b64encode(modelfile).decode('utf-8'),\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, 'rb') as f:\n      digest = sha256(f.read()).hexdigest()\n\n    try:\n      self._client.head(f'/api/blobs/{digest}')\n    except httpx.HTTPStatusError as e:\n      if e.response.status_code != 404:\n        raise ResponseError(e.response.text, e.response.status_code) from None\n\n      with open(path, 'rb') as f:\n        self._client.post(f'/api/blobs/{digest}', content=f.read())\n\n    return f'sha256:{digest}'\n\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine the user code and test code\n        code = user_code + test_code\n\n        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(code)\n            f.close()\n\n        # Run Pyright on the temporary file\n        result = subprocess.run(\n            [\"pyright\", f.name], capture_output=True, text=True\n        )\n\n        # Delete the temporary file\n        Path(f.name).unlink()\n\n        # Parse the Pyright output to extract error messages and line numbers\n        error_messages = []\n        error_lines = set()\n        for line in result.stdout.splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_no, message = match.groups()\n                error_messages.append(message)\n                error_lines.add(int(line_no))\n\n        # Identify lines with expected type errors\n        expected_error_lines = set()\n        for token in tokenize.generate_tokens(io.StringIO(test_code).readline):\n            if token.type == tokenize.COMMENT and token.string.strip() == cls.EXPECT_ERROR_COMMENT:\n                expected_error_lines.add(token.start[0])\n\n        # Check if the type check passed or failed\n        passed = error_lines == expected_error_lines\n\n        # Generate a message detailing the outcome of the type check\n        if passed:\n            message = \"Type check passed!\"\n        else:\n            message = \"Type check failed. \"\n            if error_messages:\n                message += \" \".join(error_messages)\n            else:\n                message += \"No errors found.\"\n\n        # Return the result of the type check\n        return TypeCheckResult(message=message, passed=passed)"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, fw_compiler=get_compiler_fn(\"Forward\"),\n                          bw_compiler=get_compiler_fn(\"Backward\"))\n    else:\n        return aot_function(fn, fw_compiler=get_compiler_fn(\"Forward\"),\n                            bw_compiler=get_compiler_fn(\"Backward\"))"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    config_dict = yaml.safe_load(open(os.path.join(trial_path, 'config.yaml'), 'r'))\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError('The output file path must have a .yaml or .yml extension.')\n        with open(output_path, 'w') as f:\n            yaml.dump(best_config, f)\n\n    return best_config\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    if ts_compiler is None:\n        ts_compiler = lambda m, **kwargs: m\n\n    def wrapper(*args, **kwargs):\n        nonlocal func\n        nonlocal ts_compiler\n        nonlocal kwargs_\n        if isinstance(func, torch.nn.Module):\n            func = func.forward\n        if not callable(func):\n            raise TypeError(f\"{func} is not callable\")\n        if not isinstance(args, tuple):\n            args = (args,)\n        args = flat_tensors.flattern(args)\n        kwargs = flat_tensors.flattern(kwargs)\n        key = (func, args, tuple(kwargs.items()))\n        if key in _cache:\n            return _cache[key]\n        with _lock:\n            if key not in _cache:\n                traced_module, call_helper = trace_with_kwargs(\n                    func,\n                    example_inputs=args,\n                    example_kwarg_inputs=kwargs,\n                    **kwargs_)\n                traced_module = ts_compiler(traced_module, **kwargs_)\n                _cache[key] = call_helper(traced_module)\n        return _cache[key]\n\n    return wrapper\n\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        project_dir = os.path.dirname(trial_path)\n        return cls(config, project_dir=project_dir)\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the node line directory if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Initialize the best result dataframe with the previous result\n    best_result = previous_result\n\n    # Initialize the best result summary with the previous result summary\n    best_result_summary = load_summary_file(os.path.join(node_line_dir, \"summary.csv\"))\n\n    # Iterate over each module and its parameters\n    for module, params in zip(modules, module_params):\n        # Get the module name\n        module_name = module.__name__\n\n        # Create the module directory if it doesn't exist\n        module_dir = os.path.join(node_line_dir, module_name)\n        pathlib.Path(module_dir).mkdir(parents=True, exist_ok=True)\n\n        # Run the module with the given parameters\n        result = module(**params)\n\n        # Measure the execution time of the module\n        execution_time = measure_speed(module, params)\n\n        # Evaluate the result using the previous result and the execution time\n        evaluation_result = evaluate_retrieval(result, previous_result, execution_time)\n\n        # Save the result and evaluation result to disk\n        result.to_csv(os.path.join(module_dir, \"result.csv\"), index=False)\n        evaluation_result.to_csv(os.path.join(module_dir, \"evaluation_result.csv\"), index=False)\n\n        # Check if the execution time is within the specified threshold\n        if \"speed_threshold\" in strategies and execution_time > strategies[\"speed_threshold\"]:\n            logger.info(f\"Module {module_name} execution time is too slow ({execution_time:.2f} seconds), skipping.\")\n            continue\n\n        # Check if the evaluation result is better than the current best result\n        if best_result is None or evaluation_result[\"score\"].mean() > best_result_summary[\"score\"].mean():\n            best_result = result\n            best_"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create a directory for the query expansion node\n    node_dir = os.path.join(node_line_dir, \"query_expansion\")\n    pathlib.Path(node_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the query expansion node results\n    node_results_dir = os.path.join(node_dir, \"results\")\n    pathlib.Path(node_results_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the query expansion node summaries\n    node_summaries_dir = os.path.join(node_dir, \"summaries\")\n    pathlib.Path(node_summaries_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the query expansion node best results\n    node_best_results_dir = os.path.join(node_dir, \"best_results\")\n    pathlib.Path(node_best_results_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the query expansion node best summaries\n    node_best_summaries_dir = os.path.join(node_dir, \"best_summaries\")\n    pathlib.Path(node_best_summaries_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the query expansion node best results\n    node_best_results_dir = os.path.join(node_dir, \"best_results\")\n    pathlib.Path(node_best_results_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the query expansion node best summaries\n    node_best_summaries_dir = os.path.join(node_dir, \"best_summaries\")\n    pathlib.Path(node_best_summaries_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the query expansion node best results\n    node_best_results_dir = os.path.join(node"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create the node's directory\n    node_dir = os.path.join(node_line_dir, 'prompt_maker')\n    pathlib.Path(node_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create the node's output directory\n    node_output_dir = os.path.join(node_dir, 'output')\n    pathlib.Path(node_output_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create the node's summary directory\n    node_summary_dir = os.path.join(node_dir, 'summary')\n    pathlib.Path(node_summary_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create the node's summary directory\n    node_summary_dir = os.path.join(node_dir, 'summary')\n    pathlib.Path(node_summary_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create the node's summary directory\n    node_summary_dir = os.path.join(node_dir, 'summary')\n    pathlib.Path(node_summary_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create the node's summary directory\n    node_summary_dir = os.path.join(node_dir, 'summary')\n    pathlib.Path(node_summary_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create the node's summary directory\n    node_summary_dir = os.path.join(node_dir, 'summary')\n    pathlib.Path(node_summary_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create the node's summary directory\n    node_summary_dir = os.path.join(node_dir, 'summary')\n    pathlib.Path(node_summary_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create the node's summary directory\n    node_summary_dir = os.path.join(node_dir, 'summary')\n    pathlib"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda x: extract_values(x, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_default_embedding_model()\n\n    gt_embeddings = embedding_model.get_text_embedding(generation_gt)\n    pred_embedding = embedding_model.get_text_embedding([pred])\n    return calculate_cosine_similarity(gt_embeddings, pred_embedding)\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\n            \"GFPGAN face restorer is not set up. Skipping face restoration.\"\n        )\n        return np_image\n\n    try:\n        return gfpgan_face_restorer.restore(np_image)\n    except errors.RestorationError as e:\n        logger.warning(f\"GFPGAN face restoration failed: {e}\")\n        return np_image\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global codeformer\n        codeformer = FaceRestorerCodeFormer(dirname)\n        face_restoration.face_restorers.append(codeformer)\n    except Exception as e:\n        errors.report(f\"Error setting up CodeFormer: {e}\", exc_info=True)"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        global gfpgan_face_restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except Exception as e:\n        logger.error(f\"Error setting up GFPGAN: {e}\")\n        errors.display(e, f\"Error setting up GFPGAN: {e}\")\n        shared.state.sd_model = None\n        shared.state.sd_model_hash = None\n        shared.state.sd_model_type = None\n        shared.state.sd_model_name = None\n        shared.state.sd_model_config = None\n        shared.state.sd_model_vram_usage = None\n        shared.state.sd_model_vram_usage_level = None\n        shared.state.sd_model_vae_name = None\n        shared.state.sd_model_vae_hash = None\n        shared.state.sd_model_vae_config = None\n        shared.state.sd_model_vae_vram_usage = None\n        shared.state.sd_model_vae_vram_usage_level = None\n        shared.state.sd_model_vae_train_dataset_name = None\n        shared.state.sd_model_vae_train_dataset_hash = None\n        shared.state.sd_model_vae_train_dataset_url = None\n        shared.state.sd_model_vae_train_dataset_path = None\n        shared.state.sd_model_vae_train_dataset_size = None\n        shared.state.sd_model_vae_train_dataset_vram_usage = None\n        shared.state.sd_model_vae_train_dataset_vram_usage_level = None\n        shared.state.sd_model_vae_train_dataset_vram_usage_level_name = None\n        shared.state.sd_model_vae_train_dataset_vram_usage_level_name_short = None\n        shared.state.sd_model_vae_train_dataset_vram_usage_level_name"}
{"namespace": "quaternion.rotate", "completion": "  v = jnp.concatenate([v, jnp.zeros_like(re(q))], axis=-1)\n  return im(multiply(multiply(q, v), conjugate(q)))\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.asarray(axis_angle, dtype=jnp.float32)\n  axis_angle = axis_angle / jnp.linalg.norm(axis_angle, axis=-1, keepdims=True)\n  angle = jnp.linalg.norm(axis_angle, axis=-1, keepdims=True)\n  q = jnp.where(\n      angle > eps,\n      jnp.concatenate(\n          [\n              axis_angle * jnp.sin(angle / 2) / angle,\n              jnp.cos(angle / 2),\n          ],\n          axis=-1,\n      ),\n      jnp.concatenate(\n          [\n              axis_angle * 0.5,\n              jnp.ones_like(axis_angle[Ellipsis, 0:1]),\n          ],\n          axis=-1,\n      ),\n  )\n  return q"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    topk_words = model.topk(prefix, k=k)\n    topk_idxs = [i for i, _ in topk_words]\n    if idx in topk_idxs:\n        return 0, k\n\n    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n        k += 1\n    high = logit_bias[idx]\n\n    # improve estimate\n    mid = (high + low) / 2\n    while high >= low + eps:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n        k += 1\n    return -mid, k\n\n"}
{"namespace": "resample.resample_3d", "completion": "  # Check if the input data is a JAX array.\n  if not isinstance(data, jnp.ndarray):\n    raise ValueError('data must be a JAX array.')\n\n  # Check if the input locations is a JAX array.\n  if not isinstance(locations, jnp.ndarray):\n    raise ValueError('locations must be a JAX array.')\n\n  # Check if the input data has four dimensions.\n  if data.ndim != 4:\n    raise ValueError('data must have four dimensions.')\n\n  # Check if the input locations has at least two dimensions.\n  if locations.ndim < 2:\n    raise ValueError('locations must have at least two dimensions.')\n\n  # Check if the input locations has the same number of dimensions as the input data.\n  if locations.ndim != data.ndim:\n    raise ValueError('locations must have the same number of dimensions as data.')\n\n  # Check if the input locations has the same batch size as the input data.\n  if locations.shape[0] != data.shape[0]:\n    raise ValueError('locations must have the same batch size as data.')\n\n  # Check if the input locations has the same spatial dimensions as the input data.\n  if locations.shape[1:-1] != data.shape[1:-1]:\n    raise ValueError('locations must have the same spatial dimensions as data.')\n\n  # Check if the input locations has three channels.\n  if locations.shape[-1] != 3:\n    raise ValueError('locations must have three channels.')\n\n  # Check if the input edge_behavior is valid.\n  if edge_behavior not in ['CONSTANT_OUTSIDE', 'CLAMP']:\n    raise ValueError('edge_behavior must be either CONSTANT_OUTSIDE or CLAMP.')\n\n  # Check if the input constant_values is a float.\n  if not isinstance(constant_values, float):\n    raise ValueError('constant_values must be a float.')\n\n  # Check if the input coordinate_order is valid.\n  if coordinate_"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(jnp.abs(x) < tiny_val, tiny_val, jnp.nextafter(x, jnp.inf))\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, 0),\n  )(x)\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, lambda x, y, x_dot: x_dot / remove_zero(x),\n                          (tiny_val, max_val))(x)\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda _, y, x_dot: 0.5 * x_dot / y,\n      (0, max_val),\n  )(x)"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return 1\n  elif p == 2:\n    return 2\n  elif p == 3:\n    return 3\n  elif p == 4:\n    return 4\n  elif p == 5:\n    return 5\n  elif p == 6:\n    return 6\n  elif p == 7:\n    return 7\n  elif p == 8:\n    return 8\n  elif p == 9:\n    return 9\n  elif p == 10:\n    return 10\n  elif p == 11:\n    return 11\n  elif p == 12:\n    return 12\n  elif p == 13:\n    return 13\n  elif p == 14:\n    return 14\n  elif p == 15:\n    return 15\n  elif p == 16:\n    return 16\n  elif p == 17:\n    return 17\n  elif p == 18:\n    return 18\n  elif p == 19:\n    return 19\n  elif p == 20:\n    return 20\n  elif p == 21:\n    return 21\n  elif p == 22:\n    return 22\n  elif p == 23:\n    return 23\n  elif p == 24:\n    return 24\n  elif p == 25:\n    return 25\n  elif p == 26:\n    return 26\n  elif p == 27:\n    return 27\n  elif p == 28:\n    return 28\n  elif p == 29:\n    return 29\n  elif p == 30:\n    return 30\n  elif p == 31:\n    return 31\n  elif p == 32:\n    return 32\n  elif p == 33:\n    return 33\n  elif p == 34:\n    return 34\n  elif p == 35:\n    return 35\n  elif p == 36:\n    return 36\n  elif p == 37:\n    return 37"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array(\n        [\n            [1, 1, 1],\n            [-1, -1, 1],\n            [1, -1, -1],\n            [-1, 1, -1],\n        ]\n    )\n    base_faces = np.array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]])\n  elif base_shape == 'icosahedron':\n    phi = (1 + np.sqrt(5)) / 2\n    base_verts = np.array(\n        [\n            [0, 1, phi],\n            [0, -1, phi],\n            [0, 1, -phi],\n            [0, -1, -phi],\n            [1, phi, 0],\n            [-1, phi, 0],\n            [1, -phi, 0],\n            [-1, -phi, 0],\n            [phi, 0, 1],\n            [-phi, 0, 1],\n            [phi, 0, -1],\n            [-phi, 0, -1],\n        ]\n    )\n    base_faces = np.array(\n        [\n            [0, 1, 4],\n            [0, 4, 9],\n            [0, 9, 5],\n            [0, 5, 1],\n            [1, 5, 11],\n            [1, 11, 10],\n            [1, 10, 4],\n            [2, 3, 7],\n            [2, 7, 8],\n            [2, 8, 6],\n            [2, 6, 3],\n            [3, 6, 10],\n            [3, 10, 11],\n            [3, 11, 7],\n            [4, 10, 8],\n            [4,"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda _, y, x_dot: x_dot / (1 + y),\n      (min_val, max_val),\n  )(x)"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n  if p == 1:\n    y = jnp.sign(x) * jnp.log1p(jnp.abs(x))\n  elif p == 0:\n    y = jnp.log(jnp.abs(x) + 1)\n  elif p == -jnp.inf:\n    y = jnp.sign(x)\n  elif p == jnp.inf:\n    y = jnp.sign(x) * jnp.log(jnp.abs(x) + 1)\n  else:\n    y = jnp.sign(x) * jnp.abs(x) ** (p - 1)\n  if postmult is not None:\n    y = y * postmult\n  return y\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = select(\n      [\n          (p == 1, y),\n          (p == 0, safe_expm1(y)),\n          (p == -jnp.inf, -safe_log1p(-y)),\n          (p == jnp.inf, safe_log1p(y)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((jnp.abs(y) + 1) ** (1 / p_safe) - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    # Avoid nan for steps < lr_delay_steps\n    lr_step_ratio = jnp.maximum(\n        jnp.float32(step - lr_delay_steps) / jnp.float32(max_steps - lr_delay_steps),\n        0.0,\n    )\n    lr = lr_multiplier(\n        lr_step_ratio, lr_init * lr_delay_mult, lr_final * lr_delay_mult\n    )\n  else:\n    lr_step_ratio = jnp.float32(step) / jnp.float32(max_steps)\n    lr = lr_multiplier(lr_step_ratio, lr_init, lr_final)\n  return lr\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  rng = random.PRNGKey(0)\n  return generate_random_rays(\n      rng,\n      1,\n      origin_lo=-1,\n      origin_hi=1,\n      radius_lo=0.01,\n      radius_hi=0.1,\n      near_lo=0.01,\n      near_hi=1,\n      far_lo=1,\n      far_hi=10,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )\n\n"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Must add half pixel offset to shoot rays through pixel centers.\n  def dir_to_pix(x, y):\n    return xnp.stack([x - 0.5, y - 0.5, xnp.ones_like(x)], axis=-1)\n\n  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          dir_to_pix(points[Ellipsis, 0], points[Ellipsis, 1]),\n          dir_to_pix(points[Ellipsis, 0] + 1, points[Ellipsis, 1]),\n          dir_to_pix(points[Ellipsis, 0], points[Ellipsis, 1] + 1),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp"}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[3:]\n  v = screw_axis[:3]\n  theta = _safe_sqrt(jnp.sum(w**2, axis=-1))\n\n  # Near zero, we switch to using the first order Taylor expansion.\n  R_taylor = exp_so3(w, eps)\n  p_taylor = jnp.where(theta > eps, v * theta, v)\n\n  # Prevent bad gradients from propagating back when theta is small.\n  w_safe = jnp.where(theta > eps, w, 0.0)\n  theta_safe = jnp.where(theta > eps, theta, 1.0)\n  w_hat = skew(w_safe)\n  w_hat_2 = jnp.matmul(w_hat, w_hat)\n  w_hat_3 = jnp.matmul(w_hat_2, w_hat)\n  R = (\n      jnp.eye(3)\n      + jnp.sin(theta_safe) / theta_safe * w_hat\n      + (1.0 - jnp.cos(theta_safe)) / theta_safe**2 * w_hat_2\n  )\n  V = (\n      jnp.eye(3)\n      + (1.0 - jnp.cos(theta_safe)) / theta_safe**2 * w_hat\n      + (theta_safe - jnp.sin(theta_safe)) / theta_safe**3 * w_hat_2\n  )\n  p = jnp.matmul(V, v)\n\n  return jnp.where(\n      theta > eps,\n      rp_to_se3(R, p),\n      rp_to_se3(R_taylor, p_taylor),\n  )\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = jnp.linalg.norm(axis_angle)\n  skew_axis_angle = skew(axis_angle)\n  theta_is_nonzero = theta > eps\n  theta_is_zero = theta <= eps\n\n  safe_theta = jnp.where(theta_is_nonzero, theta, 1.0)\n  safe_skew_axis_angle = jnp.where(\n      theta_is_nonzero[..., jnp.newaxis, jnp.newaxis],\n      skew_axis_angle,\n      jnp.eye(3),\n  )\n\n  theta_is_pi = jnp.isclose(theta, jnp.pi)\n  theta_is_pi_or_small = theta_is_pi | (theta <= eps)\n  safe_theta = jnp.where(theta_is_pi_or_small, 1.0, safe_theta)\n\n  axis_angle_is_zero = jnp.all(axis_angle == 0.0)\n  safe_skew_axis_angle = jnp.where(\n      axis_angle_is_zero[..., jnp.newaxis, jnp.newaxis],\n      jnp.zeros((3, 3)),\n      safe_skew_axis_angle,\n  )\n\n  safe_skew_axis_angle_sq = jnp.matmul(\n      safe_skew_axis_angle, safe_skew_axis_angle\n  )\n  I = jnp.eye(3)\n  theta_is_pi_or_small = theta_is_pi | (theta <= eps)\n  safe_skew_axis_angle_sq = jnp.where(\n      theta_is_pi_or_small[..., jnp.newaxis, jnp.newaxis],\n      I,\n      safe_skew_axis_angle_sq,\n  )\n\n  R = (\n      I\n      + (jnp.sin(safe_theta) /"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= base_radius**2\n  t0_mean, t0_var = gaussianize_frustum(t0, jnp.zeros_like(t0))\n  p_mean, p_var = lift_gaussian(d, t0_mean, t0_var, r_var, diag)\n  return p_mean, p_var\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean = (t0 + t1) / 2\n  r_var = radius**2 / 4\n  t_var = (t1 - t0)**2 / 12\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Shape of the input pixel coordinates.\n  sh = pix_x_int.shape\n\n  # Convert pixel coordinates to NDC space.\n  if pixtocam_ndc is not None:\n    pix_x_int, pix_y_int = xnp.split(\n        pixtocam_ndc @ xnp.stack(\n            [pix_x_int, pix_y_int, xnp.ones_like(pix_x_int)], axis=0\n        ),\n        2,\n    )\n    pix_x_int = pix_x_int / pix_y_int\n\n  # Convert pixel coordinates to camera coordinates.\n  if camtype == ProjectionType.PERSPECTIVE:\n    # Perspective camera projection.\n    x = (pix_x_int - pixtocams[Ellipsis, 0, 2]) / pixtocams[Ellipsis, 0, 0]\n    y = (pix_y_int - pixtocams[Ellipsis, 1, 2]) / pixtocams[Ellipsis, 1, 1]\n    z = xnp.ones_like(pix_x_int)\n  elif camtype == ProjectionType.FISHEYE:\n    # Fisheye camera projection.\n    r = xnp.sqrt(pix_x_int ** 2 + pix_y_int ** 2)\n    theta = r * xnp.arctan2(r, 1) / r\n    x = theta * xnp.sin(theta)\n    y = theta * xnp.cos(theta)\n    z = xnp.ones_like(pix_x_int)\n  elif camtype == ProjectionType.PANORAMIC:\n    # Panoramic camera projection.\n    theta = xnp.pi * pix_x_int / pixtocams[Ellipsis, 0, 2]\n    phi = xnp.pi * pix_y_int / pixtocams[Ellipsis, 0, 2]\n   "}
{"namespace": "render.compute_alpha_weights", "completion": "  adjusted_density = density * jnp.linalg.norm(dirs[Ellipsis, None, :], axis=-1)\n  density_delta = adjusted_density * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Invert the CDF.\n  if rng is None:\n    # Match Mip-NeRF's sampling, which does argmax over the bins.\n    # This is not the same as inverse transform sampling.\n    u = jnp.linspace(0., 1., num_samples)\n    t_new = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n  else:\n    # Sample uniformly from the CDF.\n    # Note: we found that for the foreground (object) bins, uniform sampling was\n    # insufficient, so we draw two samples from a tent distribution for each bin\n    # and use the min. One can achieve this by using inverse transform sampling.\n    if single_jitter:\n      # Apply the same jitter to all samples.\n      shape = w.shape[:-1] + (num_samples,)\n      u = jax.random.uniform(rng, shape)\n    else:\n      # Apply a different jitter to each sample.\n      u_shape = w.shape[:-1] + (num_samples, 2)\n      u = jax.random.uniform(rng, u_shape)\n      u = jnp.min(u, axis=-1)\n    if deterministic_center:\n      # Match the behavior of the original NeRF code.\n      t0 = jnp.clip(t[Ellipsis, :-1] + eps, a_min=0, a_max=1)\n      t1 = jnp.clip(t[Ellipsis, 1:] - eps, a_min=0, a_max=1)\n      t_new = math.sorted_interp(u, cw, jnp.stack([t0, t1], axis=-1),\n                                "}
{"namespace": "stepfun.sample_intervals", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n\n  # Draw uniform samples.\n  if rng is None:\n    # Match the behavior of jax.random.uniform() by spanning [0, 1-eps].\n    u = jnp.linspace(0, 1.0 - jnp.finfo(jnp.float32).eps, num_samples)\n    u = jnp.broadcast_to(u, t.shape[:-1] + (num_samples,))\n  else:\n    # `u` is in [0, 1) --- it can be zero, but it can never be 1.\n    u_max = jnp.finfo(jnp.float32).eps + (1 - jnp.finfo(jnp.float32).eps) / num_samples\n    max_jitter = (1 - u_max) / (num_samples - 1) - jnp.finfo(jnp.float32).eps\n    d = 1 if single_jitter else num_samples\n    u = jnp.linspace(0, 1 - u_max, num_samples) + jax.random.uniform(\n        rng, t.shape[:-1] + (d,), maxval=max_jitter\n    )\n\n  t_samples = invert_cdf(u, t, w_logits)\n\n  # Compute the midpoints between adjacent samples.\n  t_mid = (t_samples[Ellipsis, 1:] + t_samples[Ellipsis, :-1]) / 2\n\n  # Include a sample that is at the left-hand boundary.\n  t_left = t[Ellipsis, :1]\n  t_mid = jnp.concatenate([t_left, t_mid], axis=-1)\n\n  # Include a sample that is at the right-hand boundary.\n  t_right = t[Ellipsis, -1:]\n  t_mid = jnp.concatenate([t_mid, t"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  w = jnp.where(w == jnp.inf, 0, w)\n  w = jnp.where(jnp.isnan(w), 0, w)\n  w = jnp.where(w < 0, 0, w)\n  w = jnp.where(w > 1, 1, w)\n  w = w / jnp.sum(w, axis=-1, keepdims=True)\n  cw = integrate_weights(w)\n  return linspline.interpolate(cw, t, ps / 100)"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a PDF\n  pdf = weight_to_pdf(t, w)\n\n  # Blur the PDF\n  blurred_pdf = linspline.blur(t, pdf, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points\n  resampled_weights = resample(tq, t, blurred_pdf, use_avg=False)\n\n  return resampled_weights"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(transform, to_homogeneous(vectors)))"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(tp, vp)\n  # Compute the weights for each original interval.\n  w = jnp.diff(tp, axis=-1)\n  # Compute the width of each new interval.\n  wp = jnp.diff(t, axis=-1)\n  # Compute the total width of each original interval.\n  w_total = jnp.sum(w, axis=-1)\n  # Compute the total width of each new interval.\n  wp_total = jnp.sum(wp, axis=-1)\n  # Compute the ratio of the new interval width to the original interval width.\n  wp_ratio = wp_total / w_total\n  # Compute the ratio of the original interval width to the new interval width.\n  w_ratio = w_total / wp_total\n  # Compute the ratio of the original interval width to the total width of all original intervals.\n  w_ratio_total = w_total / jnp.sum(w_total)\n  # Compute the ratio of the new interval width to the total width of all new intervals.\n  wp_ratio_total = wp_total / jnp.sum(wp_total)\n\n  # Compute the resampled values for each new interval.\n  if use_avg:\n    # If use_avg is True, compute the average value of the step function for each new interval.\n    v = jnp.sum(w * vp[Ellipsis, :-1], axis=-1) / wp_total\n  else:\n    # If use_avg is False, compute the sum of the values of the step function for each new interval.\n    v = jnp.sum(w * vp[Ellipsis, :-1], axis=-1)\n\n  # Return the resampled values.\n  return v"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scaling the mean and variance by powers of 2.\n  scaled_mean = mean[..., None] * (2.0 ** jnp.arange(min_deg, max_deg))[:, None]\n  scaled_var = var[..., None] * (2.0 ** jnp.arange(min_deg, max_deg))[:, None]\n\n  # Concatenating the scaled mean and variance along the last axis.\n  scaled_mean_var = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Applying the sinusoidal encoding to the scaled mean and variance.\n  encoded_vars = jnp.sin(\n      jnp.concatenate(\n          [scaled_mean_var, scaled_mean_var + 0.5 * jnp.pi], axis=-1\n      )\n  )\n\n  return encoded_vars\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def integrated_dir_enc_fn(xyz, kappa_inv):\n    \"\"\"\n    Evaluates the integrated directional encoding (IDE) for a given set of 3D points and inverse concentration parameter.\n\n    Input-Output Arguments\n    :param xyz: Array. A 3D point (or points) to evaluate the IDE at.\n    :param kappa_inv: Array. The inverse concentration parameter of the von Mises-Fisher distribution.\n    :return: Array. The evaluated IDE for the given inputs.\n\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    text_group_index = 0\n    text_group_start_index = 0\n    text_group_type = \"paragraph\"\n    text_group_list = []\n    text_group_header_index = -1\n    text_group_indentation = 0\n    text_group_list_level = 0\n    text_group_text = \"\"\n    text_group_text_list = []\n    text_group_text_list_level = 0\n    text_group_text_list_index = 0\n    text_group_text_list_start_index = 0\n    text_group_text_list_end_index = 0\n    text_group_text_list_type = \"list\"\n    text_group_text_list_list = []\n    text_group_text_list_list_level = 0\n    text_group_text_list_list_index = 0\n    text_group_text_list_list_start_index = 0\n    text_group_text_list_list_end_index = 0\n    text_group_text_list_list_type = \"list\"\n    text_group_text_list_list_list = []\n    text_group_text_list_list_list_level = 0\n    text_group_text_list_list_list_index = 0\n    text_group_text_list_list_list_start_index = 0\n    text_group_text_list_list_list_end_index = 0\n    text_group_text_list_list_list_type = \"list\"\n    text_group_text_list_list_list_list = []\n    text_group_text_list_list_list_list_level = 0\n    text_group_text_list_list_list_list_index = 0\n    text_group_text_list_list_list_list_start_index = 0\n    text_group_text_list_list_list_list_end_index = 0\n    text_group_text_list_list_list_list_type = \"list\"\n    text_group_text_list_list_"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    # remove new line\n    texts = org_texts.replace(\"\\n\", \" \")\n\n    # remove space before punctuation\n    texts = space_rule.sub(r\"\\1\", texts)\n\n    # normalize quotation marks\n    texts = quotation_pattern.sub('\"', texts)\n\n    # tokenize\n    sents = nltk_tokenzier.tokenize(texts)\n\n    # handle special case\n    for i, sent in enumerate(sents):\n        if sent.startswith(\".\"):\n            sents[i] = sent[1:]\n\n    # handle bracket\n    for i, sent in enumerate(sents):\n        if \"(\" in sent:\n            brackets = bracket_rule.findall(sent)\n            for bracket in brackets:\n                sent = sent.replace(bracket, bracket.replace(\" \", \"_\"))\n            sents[i] = sent\n\n    # apply rules\n    for i, sent in enumerate(sents):\n        for rule, replaced in rules:\n            sent = rule.sub(replaced, sent)\n        sents[i] = sent\n\n    return sents"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            if not isinstance(key, int):\n                raise TypeError(\"Expected an integer for key\")\n            if key < 0:\n                key += len(self)\n            if key >= len(self):\n                raise IndexError(\"Index out of bounds\")\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                return self.posns.doc_posns(term_id, doc_id=key)\n            except TermMissingError:\n                return []\n        else:\n            try:\n                term_id = self.term_dict.get_term_id(token)\n                return self.posns.all_posns(term_id)\n            except TermMissingError:\n                return []\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Check if the spec is a percentage\n    if spec.endswith('%'):\n        # Parse the percentage value\n        percentage = float(spec[:-1])\n        # Calculate the minimum number of clauses based on the percentage\n        min_clauses = int(num_clauses * percentage / 100)\n    # Check if the spec is a conditional expression\n    elif '<' in spec:\n        # Split the expression into two parts\n        left, right = spec.split('<')\n        # Parse the left and right values\n        left_value = int(left)\n        right_value = int(right)\n        # Calculate the minimum number of clauses based on the conditional expression\n        min_clauses = min(left_value, right_value)\n    # Otherwise, assume the spec is an absolute number\n    else:\n        # Parse the absolute value\n        min_clauses = int(spec)\n\n    return min_clauses\n\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            # If the slop is 1 and all tokens are unique, we can directly calculate the phrase frequencies\n            # using the positions of terms\n            term_ids = [self.term_dict.get_term_id(token) for token in tokens]\n            posns = self.positions(tokens[0])\n            if len(posns) == 0:\n                return np.zeros(len(self), dtype=int)\n            else:\n                return scan_merge_ins(posns, term_ids, slop)\n        else:\n            # If the slop is not 1 or tokens are not unique, we delegate the calculation to another method\n            return self._phrase_freq_non_unique(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        if truncate:\n            array = array[:batch_size]\n\n        # Build the index\n        term_mat, posns, term_dict, avg_doc_length, doc_lens = build_index_from_tokenizer(\n            array, tokenizer, batch_size=batch_size)\n\n        # Create a new instance of SearchArray\n        return cls(term_mat, term_dict, posns, avg_doc_length, doc_lens,\n                   tokenizer=tokenizer, avoid_copies=avoid_copies)\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(\n            self.config['serverHost'],\n            self.config['serverPort'],\n            self.config['proxyHost'],\n            self.config['proxyPort'],\n            self.config['strategy'],\n            self.config['strategies'],\n            self.config['autoCloseConnections'],\n            self.config['multipleConnections'],\n            self.logger,\n        )\n        self.server.start()\n\n        self.connections = {}\n        self.lock = threading.Lock()\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> _1) & s55\n    arr = (arr & s33) + ((arr >> _2) & s33)\n    arr = (arr + (arr >> _4)) & s0F\n    arr = (arr * _1) >> all_but_one_bit\n    return arr\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = \"100%\"\n\n    if q_op not in [\"OR\", \"AND\"]:\n        raise ValueError(\"q_op must be either 'OR' or 'AND'\")\n\n    if q_op == \"AND\":\n        raise NotImplementedError(\"AND operator not implemented yet\")\n\n    query_fields = parse_field_boosts(qf)\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, query_fields.keys())\n\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    return qf_scores, explain"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            self.logger.debug(f'Intercepting SendMessage: {message}')\n            self.intercept_send(process, message)\n        elif isinstance(message, RecvMessage):\n            self.logger.debug(f'Intercepting RecvMessage: {message}')\n            self.intercept_recv(process, message)\n        elif isinstance(message, CloseMessage):\n            self.logger.debug(f'Intercepting CloseMessage: {message}')\n            self.intercept_close(process, message)\n        else:\n            self.logger.debug(f'Intercepting Message: {message}')\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            for connection in self.connections.values():\n                connection.close()\n            self.connections.clear()\n        if self.server:\n            self.server.stop()\n"}
