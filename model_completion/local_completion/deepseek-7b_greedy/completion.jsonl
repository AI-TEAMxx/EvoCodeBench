{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "coord.inv_contract", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(z_mag_sq) - 1) / z_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create a connection to the SQLite database\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n\n            # Create a table to store the function outputs if it doesn't exist\n            c.execute(\n                \"\"\"CREATE TABLE IF NOT EXISTS cache (\n                    func_name TEXT,\n                    args TEXT,\n                    kwargs TEXT,\n                    result TEXT\n                )\"\"\"\n            )\n\n            # Compute the hash of the function arguments\n            args_hash = hashlib.sha256(\n                json.dumps(args, sort_keys=True).encode(\"utf-8\")\n            ).hexdigest()\n            kwargs_hash = hashlib.sha256(\n                json.dumps(kwargs, sort_keys=True).encode(\"utf-8\")\n            ).hexdigest()\n\n            # Check if the function output is already stored in the database\n            c.execute(\n                \"SELECT result FROM cache WHERE func_name = ? AND args = ? AND kwargs = ?\",\n                (func_name, args_hash, kwargs_hash),\n            )\n            result = c.fetchone()\n\n            # If the function output is not stored, compute it and store it in the database\n            if result is None:\n                result = func(*args, **kwargs)\n                c.execute(\n                    \"INSERT INTO cache (func_name, args, kwargs, result) VALUES (?, ?, ?, ?)\",\n                    (func_name, args_hash, kwargs_hash, json.dumps(result)),\n                )\n                conn.commit()\n            else:\n                result = json.loads(result[0])\n\n            # Close the database connection\n            conn.close()\n\n            return result\n\n        return wrapper\n\n    return decorator"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max.\")\n\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max.\")\n\n    return values\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  mat0_norm = np.sum(mat0**2, axis=0)\n  mat1_norm = np.sum(mat1**2, axis=0)\n  mat01_dot = np.matmul(mat0.T, mat1)\n\n  dist_sq = mat0_norm[:, None] + mat1_norm[None, :] - 2 * mat01_dot\n  dist_sq[dist_sq < 0] = 0\n\n  return dist_sq\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"http://\") or path.startswith(\"https://\"):\n        return True\n    return False\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                f\"{name} must be a dictionary when assets_names is None\"\n            )\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"{name} must have {n_assets} elements when assets_names is not None\"\n            )\n        if not all(isinstance(k, str) for k in items.keys()):\n            raise ValueError(f\"{name} keys must be strings\")\n        if not all(isinstance(v, (int, float)) for v in items.values()):\n            raise ValueError(f\"{name} values must be numeric\")\n        if not all(k in assets_names for k in items.keys()):\n            raise ValueError(\n                f\"{name} keys must be in assets_names: {assets_names}\"\n            )\n        if dim == 1:\n            return np.array([items.get(k, fill_value) for k in assets_names])\n        if dim == 2:\n            return np.array([[items.get(k, fill_value) for k in assets_names]])\n        raise ValueError(f\"dim must be 1 or 2, got {dim}\")\n\n    if isinstance(items, np.ndarray):\n        if dim == 1:\n            if items.ndim != 1:\n                raise ValueError(f\"{name} must be 1D array\")\n            if items.shape[0] != n_assets:\n                raise ValueError(\n                    f\"{name} must have {n_assets} elements, got {items.shape[0]}\"\n                )\n            return items\n        if dim == 2:\n            if items.ndim != 2:\n                raise ValueError(f\"{name} must be 2D array\")\n            if items.shape[1] != n_assets:\n                raise ValueError(\n                    f\"{name} must have {n_assets} columns, got {items.shape[1]}\"\n                )\n            return items\n       "}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\", None)\n        if purpose_embedding is not None:\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        return MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\", None),\n            purpose=data.get(\"purpose\", None),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\", 0),\n            max_depth=data.get(\"max_depth\", 0),\n            usage_count=data.get(\"usage_count\", 0),\n            id=data.get(\"id\", None),\n            parent_id=data.get(\"parent_id\", None),\n            working_agent=data.get(\"working_agent\", None),\n            is_prime=data.get(\"is_prime\", False),\n            evolve_count=data.get(\"evolve_count\", 0),\n            number_of_code_executions=data.get(\"number_of_code_executions\", 0),\n            last_input=data.get(\"last_input\", None),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper,\n        )\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb / 12.92\n  linear1 = xnp.power((srgb + 0.055) / 1.055, 2.4)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Adjust spline degree to be at most one less than the number of points in x\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Fit a spline to the input signal\n  tck, u_keyframes = scipy.interpolate.splprep([x], k=spline_degree, s=smoothness)\n\n  # Evaluate the spline at the output times\n  y = scipy.interpolate.splev(t_output, tck)\n\n  return y\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    if word.islower():\n        return word\n    if word[0].isupper() and word[1].isupper():\n        return word\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    if word[0].islower() and word[1].isupper():\n        return word.capitalize()\n    return word\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be binary (bool). Found {v.dtype}.\"\n        )\n\n    return v\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(lambda x: ast.literal_eval(x))\n\n    return summary_df\n\n"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # Compute the isotropic covariance matrix using the determinant directly.\n    det = jnp.linalg.det(cov)\n    isotropic_cov = det * jnp.eye(cov.shape[-1])\n  elif mode == 'accurate':\n    # Compute the isotropic covariance matrix using the logarithm of the determinant for stability.\n    log_det = jnp.linalg.slogdet(cov)[1]\n    isotropic_cov = jnp.exp(log_det) * jnp.eye(cov.shape[-1])\n  else:\n    raise ValueError(f'Invalid mode: {mode}')\n\n  return isotropic_cov\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"XAgent: A tool for executing tasks and managing their execution.\")\n\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"+\", help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", choices=[\"auto\", \"manual\"], help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    char_set = string.ascii_lowercase + string.digits + \"_\"\n\n    # Calculate the number of characters in the character set\n    char_set_len = len(char_set)\n\n    # Initialize the encoded string\n    encoded_str = \"\"\n\n    # Encode the integer into a single character\n    while n > 0:\n        # Calculate the remainder of the integer division by the number of characters in the character set\n        remainder = n % char_set_len\n\n        # Append the character corresponding to the remainder to the encoded string\n        encoded_str = char_set[remainder] + encoded_str\n\n        # Divide the integer by the number of characters in the character set\n        n = n // char_set_len\n\n    # Return the encoded string\n    return encoded_str\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx, intervals in workers_intervals.items():\n        chunk_index = 0\n        for interval in intervals:\n            if indexes[worker_idx] >= interval[1] - interval[0]:\n                chunk_index += 1\n                indexes[worker_idx] -= interval[1] - interval[0]\n            else:\n                break\n        chunks_index[worker_idx] = chunk_index\n\n    return chunks_index, indexes"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # Adjust coordinates to be within the bounds of the voxel grid.\n    coordinates = jnp.clip(coordinates, 0, 1)\n\n    # Perform trilinear interpolation on the voxel grid.\n    return resample.trilerp(values, coordinates)\n\n  elif datastructure == 'hash':\n    # Adjust coordinates to be within the bounds of the hash data structure.\n    coordinates = jnp.clip(coordinates, 0, 1)\n\n    # Perform trilinear interpolation on the hash data structure.\n    return hash_resample.trilerp(values, coordinates)\n\n  else:\n    raise ValueError(f'Invalid datastructure: {datastructure}')\n\n\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Generate the integer weights for each vertex of the triangle\n  weights = np.array(list(itertools.product(range(v + 1), repeat=3)))\n\n  # Normalize the weights to get the barycentric coordinates\n  weights = weights / np.sum(weights, axis=1)[:, None]\n\n  return weights\n\n"}
{"namespace": "linspline.query", "completion": "  checkify.check(jnp.all(tq >= t[0]), 'Query points must be >= t[0].')\n  checkify.check(jnp.all(tq <= t[-1]), 'Query points must be <= t[-1].')\n  checkify.check(jnp.all(t[1:] > t[:-1]), 'Time points must be strictly increasing.')\n  checkify.check(jnp.all(v[1:] >= v[:-1]), 'Values must be non-decreasing.')\n\n  check_zero_endpoints(v)\n\n  tq = jnp.clip(tq, t[0], t[-1])\n\n  tq_indices = jnp.searchsorted(t, tq)\n  tq_indices = jnp.clip(tq_indices, 1, t.shape[0] - 1)\n\n  tq_indices_m1 = tq_indices - 1\n  tq_indices_p1 = tq_indices + 1\n\n  tq_indices_m1 = jnp.clip(tq_indices_m1, 0, t.shape[0] - 2)\n  tq_indices_p1 = jnp.clip(tq_indices_p1, 1, t.shape[0] - 1)\n\n  t_m1 = t[tq_indices_m1]\n  t_p1 = t[tq_indices_p1]\n\n  v_m1 = v[tq_indices_m1]\n  v_p1 = v[tq_indices_p1]\n\n  tq_m1 = tq - t_m1\n  tq_p1 = tq - t_p1\n\n  t_m1_p1 = t_p1 - t_m1\n\n  v_m1_p1 = v_p1 - v_m1\n\n  v_tq = v_m1 + tq_m1 * v_m1_"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(x > 0 for x in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Adjust ray origins to the near plane.\n  origins = origins + near * directions\n\n  # Calculate ray directions in NDC.\n  directions = xnp.einsum('ij,...j->...i', pixtocam, directions)\n  directions = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  return origins, directions\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n  return jnp.isclose(jnp.dot(dir1, dir2), 1.0)\n\n"}
{"namespace": "common.bleu4_score", "completion": "    bleu = evaluate.load(\"bleu\")\n    tokenizer = lambda x: jieba.lcut(x)\n    score = bleu.compute(\n        predictions=[continuation],\n        references=[[reference]],\n        tokenizer=tokenizer,\n        smooth=False,\n        max_order=4,\n        force=True\n    )\n    if with_penalty:\n        score = bleu.compute(\n            predictions=[continuation],\n            references=[[reference]],\n            tokenizer=tokenizer,\n            smooth=False,\n            max_order=4,\n            force=True,\n            use_effective_order=True\n        )\n    return score\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, value_at_zero, jnp.sqrt(x))\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  utils.assert_valid_stepfun(t, w)\n  w = jnp.asarray(w)\n  t = jnp.asarray(t)\n  return w / jnp.diff(t)\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(path):\n        for filename in filenames:\n            file_path = os.path.join(dirpath, filename)\n            try:\n                total_size += os.path.getsize(file_path)\n            except FileNotFoundError:\n                pass\n    return total_size\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    if isinstance(val, torch.Tensor):\n        val = val.detach().cpu().numpy()\n    if isinstance(val, np.ndarray):\n        val = val.copy()\n    val = val - offset * period\n    val = val - period * (val // period)\n    val = val + offset * period\n    return val\n\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent.purpose_embedding, np.ndarray):\n            agent.purpose_embedding = agent.purpose_embedding.tolist()\n\n        return {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check that the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"The number of bins must be positive.\")\n\n    # Check that the number of items and weights are equal\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items and weights must be equal.\")\n\n    # Check that all weights are positive\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"All weights must be positive.\")\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the bins and their weights\n    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        min_bin = min(bin_weights.items(), key=lambda x: x[1])[0]\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights\n\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        return hashlib.sha256(json.dumps(data).encode(\"utf-8\")).hexdigest()\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Initialize the total length to zero\n    total_length = 0.0\n\n    # Iterate over the points in the polygon\n    for i in range(len(polygon)):\n        # Get the current point and the next point\n        current_point = polygon[i]\n        next_point = polygon[(i + 1) % len(polygon)]\n\n        # Compute the distance between the current point and the next point\n        distance = np.linalg.norm(current_point - next_point)\n\n        # If the distance is below the maximum distance, add it to the total length\n        if distance <= max_point_distance:\n            total_length += distance\n\n    # Return the total length\n    return total_length\n\n"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return polygons\n\n    if rel_tr < 0 or abs_tr < 0:\n        raise ValueError(\"Relative and absolute thresholds must be non-negative.\")\n\n    if rel_tr == 0 and abs_tr == 0:\n        raise ValueError(\"Either relative or absolute threshold must be non-zero.\")\n\n    largest_area = max(area(polygon) for polygon in polygons)\n    rel_threshold = largest_area * rel_tr\n    abs_threshold = abs_tr\n\n    filtered_polygons = [polygon for polygon in polygons if area(polygon) >= max(rel_threshold, abs_threshold)]\n\n    return filtered_polygons\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples per worker\n    samples_per_worker = num_samples_yielded // num_workers\n\n    # Calculate the number of samples that will be left over after distributing the samples evenly\n    remaining_samples = num_samples_yielded % num_workers\n\n    # Distribute the remaining samples evenly among the workers\n    samples_per_worker += remaining_samples // num_workers\n\n    # Create a dictionary to store the number of samples processed by each worker\n    samples_processed = {}\n\n    # Initialize the number of samples processed by each worker to 0\n    for worker_idx in range(num_workers):\n        samples_processed[worker_idx] = 0\n\n    # Distribute the samples evenly among the workers\n    for worker_idx in range(num_workers):\n        # Calculate the number of samples to be processed by the current worker\n        samples_to_process = samples_per_worker\n\n        # If the current worker is responsible for processing the remaining samples, distribute them evenly\n        if worker_idx < remaining_samples % num_workers:\n            samples_to_process += 1\n\n        # Update the number of samples processed by the current worker\n        samples_processed[worker_idx] = samples_to_process\n\n    return samples_processed\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    filtered_results = []\n    filtered_metadatas = []\n\n    for result, result_value, metadata in zip(results, value, metadatas):\n        if result_value <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f\"Input array must have shape (_, 2), but has shape {array.shape}\")\n\n    n = array.shape[0]\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i, 0] * array[j, 1]\n        area -= array[j, 0] * array[i, 1]\n    area = abs(area) / 2.0\n    return area\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n\n    return idx_lo, idx_hi\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1],\n  ])\n\n"}
{"namespace": "coord.contract", "completion": "  return x * (1.0 - jnp.linalg.norm(x, axis=-1, keepdims=True)**2)**0.5\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes == 0:\n        return \"0B\"\n    if num_bytes < 0:\n        raise ValueError(\"Bytes must be a positive number.\")\n\n    for unit in _FORMAT_TO_RATIO:\n        if num_bytes < _FORMAT_TO_RATIO[unit]:\n            return f\"{num_bytes / _FORMAT_TO_RATIO[unit]:.2f}{unit}\"\n    return f\"{num_bytes / _FORMAT_TO_RATIO['tb']:.2f}tb\"\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. Got {len(v.shape)} dimensions.\"\n            )\n\n        return v\n\n    return validator\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = jnp.sqrt(x**2 + y**2 + z**2)\n  theta = jnp.arccos(z / (r + eps))\n  phi = jnp.arctan2(y, x)\n  return r, theta, phi\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    return results['rougeL']\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # First, try to locate the object using the standard method.\n    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    # If the object cannot be located using the standard method, try to locate it using a fallback method.\n    try:\n        module_name, class_name = name.rsplit(\".\", 1)\n        module = __import__(module_name, fromlist=[class_name])\n        return getattr(module, class_name)\n    except (AttributeError, ValueError):\n        raise ImportError(f\"Failed to locate object '{name}'\")\n\n"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the module to an in-memory buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n\n    # Load the module from the in-memory buffer\n    buffer.seek(0)\n    reloaded_module = torch.jit.load(buffer)\n\n    return reloaded_module"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Normalize the scores\n    scores = [pd.Series(s).apply(lambda x: x / sum(s)).tolist() for s in scores]\n\n    # Combine the scores\n    combined_scores = [sum([s[i] * w for s, w in zip(scores, weights)]) for i in range(len(scores[0]))]\n\n    # Sort the combined scores and retrieve the top_k results\n    top_k_indices = sorted(range(len(combined_scores)), key=lambda i: combined_scores[i], reverse=True)[:top_k]\n\n    # Retrieve the ids and scores for the top_k results\n    top_k_ids = [ids[i][j] for i in range(len(ids)) for j in top_k_indices if j < len(ids[i])]\n    top_k_scores = [scores[i][j] for i in range(len(scores)) for j in top_k_indices if j < len(scores[i])]\n\n    return top_k_ids, top_k_scores"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n    if percent:\n        x *= 100\n    if x < 1e-3:\n        return f\"{x:.2e}\"\n    if x < 1e-1:\n        return f\"{x:.3f}\"\n    if x < 1:\n        return f\"{x:.2f}\"\n    if x < 10:\n        return f\"{x:.1f}\"\n    return f\"{x:.0f}\"\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        free_space = shutil.disk_usage(input_dir).free / (1024 * 1024 * 1024)\n        if free_space > threshold_in_gb:\n            break\n        sleep(sleep_time)\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, math.safe_div(p, td))\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = line_text.replace(\" \", \"\")\n\n    # Segment the modified text into smaller parts or tokens\n    tokens = nlm_tokenize(line_text)\n\n    # Join the tokens back into a single string\n    line_text = \" \".join(tokens)\n\n    return line_text\n"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zeros cannot exceed the total number of weights.\")\n\n    weights = np.random.dirichlet(np.ones(n - zeros))\n    weights = np.append(weights, np.zeros(zeros))\n    return weights / np.sum(weights)\n\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        module_param = deepcopy(module_dict)\n        return cls(module_type, module_param)\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Convert the bounding box to XYXY_ABS mode\n    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n\n    # Calculate the center of the bounding box\n    bbox_center = [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2]\n\n    # Calculate the crop size based on the aspect ratio of the bounding box\n    crop_aspect_ratio = crop_size[0] / crop_size[1]\n    bbox_aspect_ratio = (bbox[2] - bbox[0]) / (bbox[3] - bbox[1])\n    if crop_aspect_ratio > bbox_aspect_ratio:\n        crop_height = crop_size[0]\n        crop_width = int(crop_height / bbox_aspect_ratio)\n    else:\n        crop_width = crop_size[1]\n        crop_height = int(crop_width * bbox_aspect_ratio)\n\n    # Calculate the top-left corner of the crop region\n    crop_x = int(bbox_center[0] - crop_width / 2)\n    crop_y = int(bbox_center[1] - crop_height / 2)\n\n    # Adjust the crop region to ensure it fits within the image boundaries\n    crop_x = max(0, crop_x)\n    crop_y = max(0, crop_y)\n    crop_x = min(crop_x, image_size[1] - crop_width)\n    crop_y = min(crop_y, image_size[0] - crop_height)\n\n    # Create the CropTransform object\n    crop_transform = T.CropTransform(x=crop_x, y=crop_y, w=crop_width, h=crop_height)\n\n    return crop_transform\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  norm = jnp.sqrt(jnp.sum(x**2, axis=-1, keepdims=True))\n  norm = jnp.maximum(norm, grad_eps)\n  return x / norm\n\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name = agent_info.split(\":\")[0]\n        input_text = agent_info.split(\":\")[1] if len(agent_info.split(\":\")) > 1 else \"\"\n        return agent_name, input_text\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(anno[\"bbox\"], anno[\"bbox_mode\"], BoxMode.XYXY_ABS) for anno in annos]\n    target = Instances(image_size)\n    target.gt_boxes = Boxes(boxes)\n    target.gt_classes = [anno[\"category_id\"] for anno in annos]\n    if \"segmentation\" in annos[0]:\n        masks = [\n            mask_util.decode(anno[\"segmentation\"])\n            for anno in annos\n            if anno.get(\"iscrowd\", 0) == 0\n        ]\n        if mask_format == \"polygon\":\n            target.gt_masks = PolygonMasks(masks)\n        elif mask_format == \"bitmask\":\n            target.gt_masks = BitMasks(polygons_to_bitmask(masks, *image_size))\n        else:\n            raise ValueError(\"Unknown mask format: {}\".format(mask_format))\n    if \"keypoints\" in annos[0]:\n        keypoints = [anno[\"keypoints\"] for anno in annos]\n        keypoints = np.asarray(keypoints, dtype=\"float32\").reshape(-1, 3)\n        target.gt_keypoints = Keypoints(keypoints)\n    return target\n\n"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home)\n    data_home.mkdir(parents=True, exist_ok=True)\n    return data_home\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Ensure input is a 2D array\n    if cov.ndim != 2:\n        raise ValueError(\"Input must be a 2D array\")\n\n    # Calculate the standard deviation for each variable\n    std = np.sqrt(np.diag(cov))\n\n    # Calculate the correlation matrix\n    corr = cov / np.outer(std, std)\n\n    return corr, std\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # This function temporarily sets the \"training\" attribute of every submodule in a given model to a constant value, allowing for optimization by meta-compilation.\n    # It uses a context manager to ensure that these changes are reverted back to their original state after the context manager exits.\n    # Input-Output Arguments\n    # :param model: The model whose submodules' \"training\" attributes are to be temporarily annotated as constants. It is used to iterate through all submodules and modify their class definitions.\n    # :return: No return values. This function operates by side effects, modifying the class definitions of the model's submodules within the context.\n    # Example Usage\n    # with freeze_training_mode(model):\n    #     model.eval()\n    #     model.forward(input)\n    #     model.train()\n    #     model.forward(input)\n    #     model.eval()\n    #     model.forward(input)\n    #     model.train()\n    #     model.forward(input)\n    #     model.eval()\n    #     model.forward(input)\n    #     model.train()\n    #     model.forward(input)\n    #     model.eval()\n    #     model.forward(input)\n    #     model.train()\n    #     model.forward(input)\n    #     model.eval()\n    #     model.forward(input)\n    #     model.train()\n    #     model.forward(input)\n    #     model.eval()\n    #     model.forward(input)\n    #     model.train()\n    #     model.forward(input)\n    #     model.eval()\n    #     model.forward(input)\n    #     model.train()\n    #     model.forward(input)\n    #     model.eval()\n    #     model.forward(input)\n    #     model.train()\n    #     model.forward(input)\n    #     model.eval()\n    #     model.forward(input)\n    #     model.train()\n    #     model.forward(input)\n    #     model.eval()\n    #     model"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if the shapes of field1 and field2 are equal.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shapes mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list):\n        if isinstance(metrics[0], str):\n            return metrics, [{} for _ in range(len(metrics))]\n        elif isinstance(metrics[0], dict):\n            return [metric[\"name\"] for metric in metrics], metrics\n        else:\n            raise ValueError(\"Metrics must be either a list of strings or a list of dictionaries.\")\n    else:\n        raise ValueError(\"Metrics must be a list.\")\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    # Try to automatically determine the inverse of the function.\n    if fn == contract:\n      fn_inv = inv_contract\n    else:\n      raise ValueError(f'Cannot automatically determine inverse of {fn}.')\n\n  def t_to_s(t):\n    \"\"\"\n    Maps metric distances to normalized distances in the range [0, 1].\n\n    Input-Output Arguments\n    :param t: Tensor. Represents the metric distances to be mapped.\n    :return: Tensor. The corresponding normalized distances in the range [0, 1].\n    \"\"\"\n    t = jnp.clip(t, t_near, t_far)\n    s = (t - t_near) / (t_far - t_near)\n    s = fn(s)\n    return s\n\n  def s_to_t(s):\n    \"\"\"\n    Maps normalized distances back to metric distances.\n\n    Input-Output Arguments\n    :param s: Tensor. Represents the normalized distances to be mapped back to metric distances.\n    :return: Tensor. The corresponding metric distances.\n    \"\"\"\n    s = jnp.clip(s, 0, 1)\n    s = fn_inv(s)\n    t = s * (t_far - t_near) + t_near\n    return t\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.array([x, y, z])\n\n"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  return jnp.trapz(w, t)\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    assert len(ids) == len(scores), \"The length of ids and scores must be the same.\"\n    assert len(ids) == len(weights), \"The length of weights must be the same as the length of ids.\"\n    assert len(ids) > 1, \"You must input more than one retrieval results.\"\n    assert top_k > 0, \"top_k must be greater than 0.\"\n    assert sum(weights) == 1, \"The sum of weights must be 1.\"\n\n    # Calculate the weighted sum of scores for each ID\n    weighted_sums = []\n    for i in range(len(ids)):\n        id_list = ids[i]\n        score_list = scores[i]\n        weight = weights[i]\n        weighted_sum = {id_: score * weight for id_, score in zip(id_list, score_list)}\n        weighted_sums.append(weighted_sum)\n\n    # Normalize the scores\n    max_score = max(max(sum_.values()) for sum_ in weighted_sums)\n    normalized_scores = []\n    for i in range(len(ids)):\n        id_list = ids[i]\n        score_list = scores[i]\n        weight = weights[i]\n        normalized_score = {id_: score / max_score * weight for id_, score in zip(id_list, score_list)}\n        normalized_scores.append(normalized_score)\n\n    # Combine the normalized scores\n    combined_scores = {}\n    for i in range(len(ids)):\n        id_list = ids[i]\n        score_list = scores[i]\n        weight = weights[i]\n        for id_, score in zip(id_list, score_list):\n            if id_ in combined_scores:\n                combined_scores[id_] += score * weight\n            else:\n                combined_scores[id_] = score * weight\n\n    # Sort the IDs by their combined scores in descending order\n    sorted_ids = sorted(combined_scores.keys(), key=lambda"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean\n  fn_mean = fn(mean)\n  jac = jax.jacfwd(fn)(mean)\n\n  # Transform the covariances using the Jacobian\n  fn_cov = jac @ cov @ jac.transpose(\n      *range(jac.ndim - 2), jac.ndim - 1, jac.ndim - 2)\n\n  return fn_mean, fn_cov\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if i.size > 1:\n            yield [i[: i.size // 2], i[i.size // 2 :]]\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(\"`x` must be a 2d-array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"`x` must be a square matrix\")\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  x_enc = jnp.concatenate([jnp.sin(scaled_x), jnp.cos(scaled_x)], axis=-1)\n  if append_identity:\n    x_enc = jnp.concatenate([x, x_enc], axis=-1)\n  return x_enc\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if len(field1) equals len(field2).\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                f\"resp. {len(values[field1])} and {len(values[field2])}\"\n            )\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context to match the camera's width and height\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(eglctx, camera)\n"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new NomicBertConfig object with the same attributes as the original BertConfig\n    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        pad_token_id=bert_config.pad_token_id,\n        gradient_checkpointing=bert_config.gradient_checkpointing,\n        position_embedding_type=bert_config.position_embedding_type,\n        use_cache=bert_config.use_cache,\n        classifier_dropout=bert_config.classifier_dropout,\n        num_labels=bert_config.num_labels,\n        num_special_tokens=bert_config.num_special_tokens,\n        num_special_tokens_to_add=bert_config.num_special_tokens_to_add,\n        num_special_tokens_to_add_to_head=bert_config.num_special_tokens_to_add_to_head,\n        num_special_tokens_to_add_to_tail=bert_config.num_special_tokens_to_add_to_tail,\n        num_special_tokens_to_add_to_head_and_tail=bert"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        # Select shader program\n        if self.render_type == Mesh.RenderType.POINTS:\n            program = self.point_program\n        else:\n            program = self.mesh_program\n\n        # Upload uniforms\n        self.upload_gl_uniforms(camera)\n\n        # Bind VAO\n        gl.glBindVertexArray(self.vao)\n\n        # Draw\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.TRIS:\n            if len(self.faces):\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(gl.GL_TRIANGLES, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.QUADS:\n            if len(self.faces):\n                gl.glDrawElements(gl.GL_QUADS, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(gl.GL_QUADS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            if len(self.faces):\n                gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, None)\n            else:\n                gl.glDrawArrays(gl.GL_TRIANGLE_STR"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if not self.use_quad_cuda:\n            w = w or self.W\n            h = h or self.H\n            if ptr.shape[-1] == 3:\n                ptr = np.concatenate([ptr, ptr.new_ones(ptr.shape[:-1] + (1,)) * 255], axis=-1)  # add alpha channel\n            gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n            gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n            return\n\n        if not hasattr(self, 'cu_tex'):\n            self.init_texture()\n\n        # assert self.use_quad_cuda, \"Need to enable cuda-opengl interop to copy from device to device, check creation of this Quad\"\n        w = w or self.W\n        h = h or self.H\n        if ptr.shape[-1] == 3:\n            ptr = np.concatenate([ptr, ptr.new_ones(ptr.shape[:-1] + (1,)) * 255], axis=-1)  # add alpha channel\n\n        from cuda import cudart\n        kind = cudart.cudaMemcpyKind.cudaMemcpyDeviceToDevice\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsMapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n        cu_tex_arr = CHECK_CUDART_ERROR(cudart.cudaGraphicsSubResourceGetMappedArray(self.cu_tex, 0, 0))\n\n        if self.compose:\n            \"\"\"\n            Blit current framebuffer to this texture (self.tex)\n            Read content of this texture into a cuda buffer\n            Perform alpha blending based on the frame's alpha channel\n            Copy the blended image back into the"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Ensure all inputs are batched\n    R = R.unsqueeze(0) if R.ndim == 2 else R\n    tvec = tvec.unsqueeze(0) if tvec.ndim == 1 else tvec\n    camera_matrix = camera_matrix.unsqueeze(0) if camera_matrix.ndim == 2 else camera_matrix\n    image_size = image_size.unsqueeze(0) if image_size.ndim == 1 else image_size\n\n    # Validate input shapes and values\n    assert R.ndim == 3 and R.shape[-2:] == (3, 3), \"R must be a batch of 3x3 rotation matrices\"\n    assert tvec.ndim == 2 and tvec.shape[-1] == 3, \"tvec must be a batch of 3D translation vectors\"\n    assert camera_matrix.ndim == 3 and camera_matrix.shape[-2:] == (3, 3), \"camera_matrix must be a batch of 3x3 camera matrices\"\n    assert image_size.ndim == 2 and image_size.shape[-1] == 2, \"image_size must be a batch of 2D image sizes\"\n\n    # Compute focal lengths and principal points\n    fx, fy = camera_matrix[..., 0, 0], camera_matrix[..., 1, 1]\n    cx, cy = camera_matrix[..., 0, 2], camera_matrix[..., 1, 2]\n    fx_avg, fy_avg = (fx + fy) / 2, (fx + fy) / 2\n    cx_avg, cy_avg = cx, cy\n\n    # Adjust focal lengths and principal points for aspect ratio\n    aspect_ratio = image_size[..., 1] / image_size[..., 0]\n    fx_avg *= aspect_ratio\n    cx_avg *= aspect_ratio\n\n    # Normalize focal lengths\n    fx_avg /= image_size[..., 0]\n    fy"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.blit(x, y, w, h)\n            return\n\n        # Set up the viewport and scissor box\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Activate the shader program and bind the texture\n        gl.glUseProgram(self.quad_program)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Draw the quadrilateral\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n\n        # Restore the viewport and scissor box\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        w = w or self.W\n        h = h or self.H\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBlitFramebuffer(0, 0, self.W, self.H,\n                             x, y, w, h,\n                             gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, 0)\n\n        # Some house keepings\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Find the indices of the source times that are less than or equal to the target times\n    idx = searchsorted(t1, t0)\n\n    # Compute the inner measure as the cumulative sum of the values at the corresponding source times\n    inner = torch.cumsum(y1[idx], dim=0)\n\n    # Compute the outer measure as the cumulative sum of the values at the corresponding source times,\n    # minus the inner measure\n    outer = torch.cumsum(y1[idx], dim=0) - inner\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n\n    # t_env = torch.cat([t_env[..., :1], t_env[..., 1:] - t_env[..., :-1]], dim=-1)\n    # w_env = torch.cat([w_env[..., :1], w_env[..., 1:] - w_env[..., :-1]], dim=-1)\n\n    # t_env = torch.cat([t_env[..., :1], t_env[..., 1:] - t_env[..., :-1]], dim=-1)\n    # w_env = torch.cat([w_env[..., :1], w_env[..., 1:] - w_env[..., :-1]], dim=-1)\n\n    # t_env = torch.cat([t_env[..., :1], t_env[..., 1:] - t_env[..., :-1]], dim=-1)\n    # w_env = torch.cat([w_env[..., :1], w_env[..., 1:] - w_env[..., :-1]], dim=-1)\n\n    # t_env = torch.cat([t_env[..., :1], t_env[..., 1:] - t_env[..., :-1]], dim=-1)\n    # w_env = torch.cat([w_env[..., :1], w_env[..., 1:] - w_env[..., :-1]], dim=-1)\n\n    # t_env = torch.cat([t_env[..., :1], t_env[..., 1:] - t_env[..., :-1]], dim=-1)\n    # w_env = torch.cat([w_env[..., :1], w_env[..., 1:] -"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    t, w = matchup_channels(t, w)\n    w_normalize = w / torch.clamp_min(t[..., 1:] - t[..., :-1], torch.finfo(torch.float32).eps)\n    w_normalize = torch.cat([torch.zeros_like(w_normalize[..., :1]), w_normalize], dim=-1)\n\n    # inter-interval loss\n    w_inter = torch.diff(w_normalize, dim=-1)\n    w_inter = torch.cat([w_inter, torch.zeros_like(w_inter[..., :1])], dim=-1)\n    w_inter = torch.abs(w_inter)\n    w_inter = torch.where(w_inter > 0, w_inter, torch.zeros_like(w_inter))\n    w_inter = torch.sum(w_inter, dim=-1)\n\n    # intra-interval loss\n    w_intra = torch.diff(w_normalize, dim=-1)\n    w_intra = torch.abs(w_intra)\n    w_intra = torch.where(w_intra > 0, w_intra, torch.zeros_like(w_intra))\n    w_intra = torch.sum(w_intra, dim=-1)\n\n    # total loss\n    loss = w_inter + w_intra\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, device=t.device), t, cw)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Ensure that the weights sum to 1.\n    w = w / torch.sum(w, dim=-1, keepdim=True)\n\n    # Compute the PDF and CDF for each weight vector.\n    cw = integrate_weights(w)\n\n    # Sample uniformly from [0, 1) according to the PDF.\n    u = torch.rand(num_samples, device=t.device)\n    t_new = invert_cdf(u, t, w)\n\n    # Apply perturbation to the samples.\n    if perturb:\n        if single_jitter:\n            jitter = torch.rand_like(t_new)\n        else:\n            jitter = torch.rand_like(t_new)\n        t_new = t_new + jitter\n\n    # Clamp the samples to the valid range.\n    t_new = torch.clamp(t_new, t[..., 0], t[..., -1])\n\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    t, w = matchup_channels(t, w)\n\n    # Compute the dilated time steps\n    dilated_t = t * dilation\n\n    # Clip the dilated time steps to the specified domain\n    dilated_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n\n    # Compute the dilated weights\n    dilated_w = w / dilation\n\n    return dilated_t, dilated_w\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check if the input tensors have the same shape\n    if t.shape != y.shape:\n        raise ValueError(\"Input tensors 't' and 'y' must have the same shape.\")\n\n    # Check if the query times are sorted\n    if not torch.all(torch.diff(tq) >= 0):\n        raise ValueError(\"Query times 'tq' must be sorted in ascending order.\")\n\n    # Check if the step function times are sorted\n    if not torch.all(torch.diff(t) >= 0):\n        raise ValueError(\"Step function times 't' must be sorted in ascending order.\")\n\n    # Check if the step function values are non-negative\n    if torch.any(y < 0):\n        raise ValueError(\"Step function values 'y' must be non-negative.\")\n\n    # Check if the step function times are unique\n    if torch.any(torch.diff(t) == 0):\n        raise ValueError(\"Step function times 't' must be unique.\")\n\n    # Check if the query times are within the range of the step function times\n    if torch.any(tq < t[0]) or torch.any(tq > t[-1]):\n        raise ValueError(\"Query times 'tq' must be within the range of the step function times 't'.\")\n\n    # Check if the step function times are within the range of the query times\n    if torch.any(t < tq[0]) or torch.any(t > tq[-1]):\n        raise ValueError(\"Step function times 't' must be within the range of the query times 'tq'.\")\n\n    # Check if the step function values are non-negative\n    if torch.any(y < 0):\n        raise ValueError(\"Step function values 'y' must be non-negative.\")\n\n    # Check if the step function times are unique\n    if torch.any(torch.diff(t) == 0):\n        raise ValueError(\"Step function times 't' must be unique.\")\n\n    # Check if the query times are within the range of the step function times\n    if torch."}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    t, w = matchup_channels(t, w)\n\n    # Calculate the annealing factor based on the training fraction and slope\n    anneal_factor = torch.sigmoid(anneal_slope * (train_frac - 0.5))\n\n    # Calculate the annealed weights by multiplying the original weights with the annealing factor\n    w_annealed = w * anneal_factor\n\n    # Handle cases where adjacent intervals have zero distance\n    w_annealed = torch.where(\n        (t[..., 1:] - t[..., :-1]) == 0,\n        torch.zeros_like(w_annealed),\n        w_annealed,\n    )\n\n    # Prevent NaN values by using a softmax operation on the adjusted weights\n    w_annealed = torch.softmax(w_annealed, dim=-1)\n\n    # Normalize the annealed weights to sum to 1\n    w_annealed /= torch.sum(w_annealed, dim=-1, keepdim=True).clip(eps)\n\n    return w_annealed\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device, non_blocking=True)\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(b, device, ignore_list) for b in batch)\n    elif isinstance(batch, list):\n        return [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device, ignore_list) for k, v in batch.items() if k != \"meta\"}\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # adjust the faces tensor to match the batch dimension of the vertices tensor\n    if f.shape[dim] != v.shape[0]:\n        f = f.expand(v.shape[0], *f.shape[1:])\n\n    # gather the vertices corresponding to the faces\n    v1 = multi_gather(v, f[:, 0], dim)\n    v2 = multi_gather(v, f[:, 1], dim)\n    v3 = multi_gather(v, f[:, 2], dim)\n\n    # compute the normals of the faces\n    n = torch.cross(v2 - v1, v3 - v1, dim=dim)\n    n = n / torch.norm(n, dim=dim, keepdim=True)\n\n    # reshape the result to maintain the original faces tensor structure\n    return n.reshape(*f.shape[:dim], *f.shape[dim + 1:], 3)\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.unsqueeze(0)\n    else:  # numpy and others\n        batch = np.expand_dims(batch, axis=0)\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Direct mapping\n        batch = dotdict()\n        batch.H = self.H\n        batch.W = self.W\n        batch.K = self.K\n        batch.R = self.R\n        batch.T = self.T\n        batch.n = self.n\n        batch.f = self.f\n        batch.t = self.t\n        batch.v = self.v\n        batch.bounds = self.bounds\n\n        # Nested 'meta' dictionary\n        batch.meta = dotdict()\n        batch.meta.origin = self.origin\n        batch.meta.world_up = self.world_up\n        batch.meta.movement_speed = self.movement_speed\n        batch.meta.movement_force = self.movement_force\n        batch.meta.drag_coeff_mult = self.drag_coeff_mult\n        batch.meta.constant_drag = self.constant_drag\n        batch.meta.mass = self.mass\n        batch.meta.moment_of_inertia = self.moment_of_inertia\n        batch.meta.movement_torque = self.movement_torque\n        batch.meta.angular_friction = self.angular_friction\n        batch.meta.constant_torque = self.constant_torque\n        batch.meta.min_interval = self.min_interval\n        batch.meta.pause_physics = self.pause_physics\n\n        return batch\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.serialize(agent)\n            self.persistence.save_agent(serialized_agent)\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if len(self.agents) < 250:\n                return None, -np.inf\n\n            similarities = [cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0] for agent in self.agents]\n            max_similarity = max(similarities)\n            if max_similarity > 0.999:\n                return None, -np.inf\n\n            closest_agent_index = similarities.index(max_similarity)\n            return self.agents[closest_agent_index], max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")\n\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(\n            prompt=PRIME_PROMPT,\n            name=PRIME_NAME,\n            weight=PRIME_AGENT_WEIGHT,\n            prime=True,\n            unspecified=True,\n            openai_wrapper=self.openai_wrapper\n        )\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent is not None:\n            agent = AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n            return agent\n        else:\n            return None\n\n    def get_all_agents(self):\n        \"\"\"\n        Retrieves all agents from the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :return: A list of all agents in the database.\n        \"\"\"\n        return self.persistence.get_all_agents()\n\n    def get_agent_by_id(self, agent_id):\n        \"\"\"\n        Retrieves an agent with a specified ID from the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent_id: str. The ID of the agent to be retrieved.\n        :return: An instance of the agent with the specified ID if found, otherwise None.\n        \"\"\"\n        return self.persistence.get_agent_by_id(agent_id)\n\n    def get_agent_by_purpose(self, purpose):\n        \"\"\"\n        Retrieves an agent with a specified purpose from the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str. The purpose of the agent to be retrieved.\n        :return: An instance of the agent with the specified purpose if found, otherwise None.\n        \"\"\"\n        return self.persistence.get_agent_by_purpose(purpose)\n\n    def get_agent_by_name(self, name):\n        \"\"\"\n        Retrieves an agent with a specified name from the database.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the Agent"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    @memoize_to_sqlite\n    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        serialized_agents = self.persistence.fetch_all_agents()\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        return agents\n\n    def clear_all_agents(self):\n        \"\"\"\n        Clear all agents from the database.\n        \"\"\"\n        self.persistence.clear_all_agents()"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.error(f\"Error saving agent: {e}\")\n            raise e\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.agent_lifecycle.cleanup_agents()\n        return self.agent_lifecycle.agents\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n                goal=goal,\n                examples=EXAMPLES,\n                system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n                sample_input=sample_input\n            )\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\"\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (?, ?, ?)\n            \"\"\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row:\n                return json.loads(row[0])\n            else:\n                return None\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.execute(\n            \"SELECT result FROM cache WHERE hash = ?\", (arg_hash,)\n        )\n        result = cursor.fetchone()\n        if result is not None:\n            return json.loads(result[0])\n        return None\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT OR REPLACE INTO cache VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_config_params(args)\n\n    # If quiet mode is enabled, redirect the standard output to a file\n    if quiet_mode:\n        with open(os.path.join(CONFIG.record_dir, \"output.txt\"), \"w\") as f:\n            with redirect_stdout(f):\n                CommandLine(CONFIG).run()\n    else:\n        CommandLine(CONFIG).run()\n\n"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_ap"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or self._last_time is None or time() - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\n                \"The `state_dict` method should not be called from a DataLoader worker process. \"\n                \"Please use the `state_dict` method from the StreamingDataLoader instead.\"\n            )\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir\": self.input_dir.path,\n            \"input_url\": self.input_dir.url,\n            \"item_loader_state\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._state_dict = state_dict\n\n        self._validate_state_dict()\n\n        state: Dict[str, Any] = self._state_dict\n\n        self.current_epoch = state[\"current_epoch\"]\n        self.seed = state[\"seed\"]\n        self.drop_last = state[\"drop_last\"]\n        self.shuffle = state[\"shuffle\"]\n\n        self.input_dir = Dir(state[\"input_dir_path\"], state[\"input_dir_url\"])\n\n        if state[\"item_loader\"] is not None:\n            self.item_loader = BaseItemLoader.from_state_dict(state[\"item_loader\"])\n\n        self.distributed_env = _DistributedEnv(state[\"world_size\"])\n\n        self.cache = self._create_cache(worker_env=_WorkerEnv.detect())\n        self.shuffler = self._create_shuffler(self.cache)\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            raise ValueError(\"The state_dict is not set.\")\n\n        state: Dict[str, Any] = self._state_dict\n\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(f\"The shuffle state is not consistent. Expected {self.shuffle}, found {state['shuffle']}\")\n\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(\n                f\"The num_workers state is not consistent. Expected {self.worker_env.world_size}, found {state['num_workers']}\"\n            )\n\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The input_dir_path state is not consistent. Expected {self.input_dir.path}, found {state['input_dir_path']}\"\n            )\n\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The input_dir_url state is not consistent. Expected {self.input_dir.url}, found {state['input_dir_url']}\"\n            )\n\n        if state[\"item_loader\"] is not None and self.item_loader is not None:\n            if state[\"item_loader\"] != self.item_loader.state_dict():\n                raise ValueError(\n                    f\"The item_loader state is not consistent. Expected {self.item_loader.state_dict()}, found {state['item_loader']}\"\n                )\n\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                f\"The drop_last state is not consistent. Expected {self.drop_last}, found {state['drop_last']}\"\n            )\n\n        if state[\"seed\"] != self.seed:\n            raise ValueError(f\"The seed state is not consistent. Expected {self.seed}, found {state['seed']}\")\n\n"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    cache_dir = os.path.join(_DEFAULT_CACHE_DIR, hashlib.md5(input_dir.encode()).hexdigest())\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        if not remote_filepath.startswith(\"s3://\"):\n            raise ValueError(f\"Remote filepath {remote_filepath} is not an S3 URL\")\n\n        if os.path.exists(local_filepath):\n            return\n\n        parsed_url = parse.urlparse(remote_filepath)\n        bucket = parsed_url.netloc\n        key = parsed_url.path.lstrip(\"/\")\n\n        if self._s5cmd_available:\n            self._download_file_with_s5cmd(bucket, key, local_filepath)\n        else:\n            self._download_file_with_boto3(bucket, key, local_filepath)\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    workers_chunks = {}\n    workers_intervals = {}\n\n    for i in range(num_workers):\n        workers_chunks[i] = []\n        workers_intervals[i] = []\n\n    for i, (chunk_index, chunk_interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        worker_index = i % worker_env.world_size\n        workers_chunks[worker_index].append(chunk_index)\n        workers_intervals[worker_index].append(chunk_interval)\n\n    return workers_chunks, workers_intervals\n\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        # Convert the PIL Image object to a bytes object\n        img_bytes = io.BytesIO()\n        item.save(img_bytes, format=item.format)\n        img_bytes = img_bytes.getvalue()\n\n        # Serialize the image's dimensions, mode, and raw pixel data\n        width, height = item.size\n        mode_bytes = item.mode.encode(\"utf-8\")\n        mode_length = len(mode_bytes)\n        serialized_data = width.to_bytes(4, \"little\") + height.to_bytes(4, \"little\") + mode_length.to_bytes(4, \"little\") + mode_bytes + img_bytes\n\n        return serialized_data, None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                raise TypeError(f\"JPEG image {item.filename} does not exist.\")\n        elif isinstance(item, Image.Image):\n            with io.BytesIO() as f:\n                item.save(f, format=\"JPEG\")\n                return f.getvalue(), None\n        else:\n            raise TypeError(f\"Unsupported image type: {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[:12], np.uint32)\n        width, height, mode_size = ints\n        mode = data[12 : 12 + mode_size]\n        raw = data[12 + mode_size :]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[:4], np.uint32)[0]\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32)[0]\n        shape = np.frombuffer(data[8 : 8 + shape_size * 4], np.uint32)\n        tensor = torch.frombuffer(data[8 + shape_size * 4 :], dtype=dtype)\n        return tensor.reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        # Convert the tensor to a NumPy array\n        array = item.numpy()\n\n        # Get the dtype and shape of the tensor\n        dtype = array.dtype\n        shape = array.shape\n\n        # Convert the dtype to a string representation\n        dtype_str = str(dtype)\n\n        # Convert the shape to a string representation\n        shape_str = str(shape)\n\n        # Serialize the NumPy array to a bytes object\n        serialized_array = pickle.dumps(array)\n\n        # Combine the dtype, shape, and serialized array into a single bytes object\n        serialized_tensor = dtype_str.encode() + b\"\\n\" + shape_str.encode() + b\"\\n\" + serialized_array\n\n        # Return the serialized tensor and None as the second element of the tuple\n        return serialized_tensor, None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        if _PIL_AVAILABLE:\n            image = Image.open(io.BytesIO(data))\n            if _TORCH_VISION_AVAILABLE:\n                return pil_to_tensor(image)\n            return image\n\n        raise ValueError(\"No available serializer to deserialize the data.\")\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), f\"no_header_tensor:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        if self._dtype is None:\n            raise ValueError(\"The data type is not defined. Please call setup() method before deserializing.\")\n        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if tensor.shape == shape:\n            return tensor\n        return np.reshape(tensor, shape)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"dataset\": deepcopy(self.dataset),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_streaming\n            if isinstance(self.dataset, StreamingDataset)\n            else self._num_samples_yielded_combined,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n\n        return state\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\"torchvision is not installed. Please install it to use this function.\")\n        if not _AV_AVAILABLE:\n            raise ImportError(\"av is not installed. Please install it to use this function.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.seek(0)\n            video, _, _ = torchvision.io.read_video(f.name)\n        return video\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self.filled:\n            return []\n\n        while self._should_write():\n            self.write_chunk()\n\n        if self._serialized_items:\n            self.write_chunk(on_done=True)\n\n        filepath = self.write_chunks_index()\n        self._is_done = True\n        return [filepath]\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        self.restore = True\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        if self._iterator is not None:\n            return self._iterator.state_dict(num_workers, batch_size)\n\n        assert num_samples_yielded is not None\n\n        state_dict = {}\n        for i, dataset in enumerate(self._datasets):\n            state_dict[f\"dataset_{i}\"] = dataset.state_dict(num_workers, batch_size, num_samples_yielded[i])\n\n        return state_dict\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if state_dict is None:\n            return\n\n        for dataset in self._datasets:\n            dataset.load_state_dict(state_dict.get(dataset.__class__.__name__, {}))\n\n        if __NUM_SAMPLES_YIELDED_KEY__ in state_dict:\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir()\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"http://\") or dir_path.startswith(\"https://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"gs://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"file://\"):\n        return Dir(path=dir_path[7:])\n\n    if dir_path.startswith(\"projects/\"):\n        return Dir(url=f\"s3://{dir_path}\")\n\n    return Dir(path=dir_path)\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"`output_dir` must be a `Dir`, got: {output_dir}\")\n\n    if output_dir.url is None:\n        raise ValueError(f\"`output_dir` must be an S3 URL, got: {output_dir}\")\n\n    if not output_dir.url.startswith(\"s3://\"):\n        raise ValueError(f\"`output_dir` must be an S3 URL, got: {output_dir}\")\n\n    if append or overwrite:\n        raise NotImplementedError(\"Append and overwrite are not implemented yet.\")\n\n    if not _BOTO3_AVAILABLE:\n        raise ImportError(\"boto3 is not installed.\")\n\n    s3_client = boto3.client(\"s3\")\n\n    bucket_name, key = parse.urlparse(output_dir.url).netloc, parse.urlparse(output_dir.url).path.lstrip(\"/\")\n\n    try:\n        response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=key)\n    except botocore.exceptions.ClientError as e:\n        if e.response[\"Error\"][\"Code\"] == \"NoSuchBucket\":\n            raise ValueError(f\"Bucket {bucket_name} does not exist.\")\n        else:\n            raise e\n\n    if \"Contents\" in response:\n        raise ValueError(f\"Directory {output_dir.url} is not empty.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if \"Contents\" in objects:\n        for content in objects[\"Contents\"]:\n            if content[\"Key\"].endswith(\"index.json\"):\n                raise RuntimeError(\n                    f\"The provided output_dir `{output_dir.path}` already contains an index file named 'index.json'. \"\n                    \"HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n                )\n\n    if \"CommonPrefixes\" in objects:\n        for prefix in objects[\"CommonPrefixes\"]:\n            if prefix[\"Prefix\"].endswith(\"index.json\"):\n                raise RuntimeError(\n                    f\"The provided output_dir `{output_dir.path}` already contains an index file named 'index.json'. \"\n                    \"HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n                )\n\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait for all index files to be available\n        while True:\n            index_files = [f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n            if len(index_files) == num_workers:\n                break\n            sleep(0.1)\n\n        # Only the master node (rank 0) performs the merge\n        if node_rank is None:\n            node_rank = self.rank\n        if node_rank != 0:\n            while True:\n                merged_index_file = os.path.join(self._cache_dir, f\"{node_rank}.{_INDEX_FILENAME}\")\n                if os.path.exists(merged_index_file):\n                    break\n                sleep(0.1)\n            return\n\n        # Merge index files\n        index_files = [f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)]\n        index_files.sort()\n        merged_index = {}\n        for index_file in index_files:\n            with open(os.path.join(self._cache_dir, index_file), \"r\") as f:\n                index = json.load(f)\n                merged_index.update(index)\n\n        # Write merged index to file\n        merged_index_file = os.path.join(self._cache_dir, f\"{node_rank}.{_INDEX_FILENAME}\")\n        with open(merged_index_file, \"w\") as f:\n            json.dump(merged_index, f, sort_keys=True)\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\n            \"The Lightning SDK is not installed. Please install it with `pip install lightning-sdk`.\"\n        )\n\n    if machine is None:\n        machine = Machine(cpu=1, gpu=0, gpu_type=\"A100\", memory=\"16Gi\")\n\n    if command is None:\n        command = f\"python -m lightning.app.cli.lightning run app {os.getcwd()} --env-vars {os.environ}\"\n\n    studio = Studio(\n        cloud_compute=machine,\n        cloud_build_config={\"build_steps\": [{\"name\": \"lightning-app\", \"command\": command}]},\n    )\n\n    job = studio.start_job(name=name, num_nodes=num_nodes)\n\n    print(f\"Job URL: {job.url}\")\n\n    while job.status not in [\"SUCCEEDED\", \"FAILED\", \"CANCELLED\"]:\n        sleep(1)\n        job = studio.get_job(job.id)\n\n    if job.status == \"FAILED\":\n        raise RuntimeError(f\"Job failed with error: {job.error}\")\n\n    if job.status == \"CANCELLED\":\n        raise RuntimeError(f\"Job was cancelled with error: {job.error}\")\n\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Load the config containing the index\n        if self._config is None:\n            self._config = ChunksConfig(\n                cache_dir=self._cache_dir,\n                remote_input_dir=self._remote_input_dir,\n                compression=self._compression,\n                serializers=self._serializers,\n                item_loader=self._item_loader,\n            )\n\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration should be defined before accessing it.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index should be an instance of ChunkedIndex.\")\n\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index from the index\n        chunk_index = index.chunk_index\n\n        # Check if the chunk is already loaded\n        if chunk_index not in self.config.loaded_chunks:\n            # Load the chunk\n            self.config.load_chunk(chunk_index)\n\n        # Get the chunk filepath\n        chunk_filepath, _, _ = self.config[index]\n\n        # Get the item from the chunk\n        item = self._item_loader.load(chunk_filepath, index.index)\n\n        # Delete the chunk if it is fully consumed\n        if self._last_chunk_index != chunk_index:\n            self._last_chunk_index = chunk_index\n            self._prepare_thread.delete([chunk_index])\n\n        return item\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    if os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\") is not None:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    return obj\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Get the number of nodes and the world size\n    num_nodes = distributed_env.num_nodes\n    world_size = distributed_env.world_size\n\n    # Calculate the chunk size for each node\n    chunk_size = len(chunks_per_ranks[0])\n\n    # Calculate the number of chunks per node\n    num_chunks_per_node = chunk_size * num_nodes\n\n    # Calculate the number of chunks per rank\n    num_chunks_per_rank = chunk_size * world_size\n\n    # Calculate the number of chunks per rank for each node\n    num_chunks_per_rank_per_node = chunk_size\n\n    # Calculate the number of chunks per rank for each node\n    num_chunks_per_rank_per_node = chunk_size\n\n    # Calculate the number of chunks per rank for each node\n    num_chunks_per_rank_per_node = chunk_size\n\n    # Calculate the number of chunks per rank for each node\n    num_chunks_per_rank_per_node = chunk_size\n\n    # Calculate the number of chunks per rank for each node\n    num_chunks_per_rank_per_node = chunk_size\n\n    # Calculate the number of chunks per rank for each node\n    num_chunks_per_rank_per_node = chunk_size\n\n    # Calculate the number of chunks per rank for each node\n    num_chunks_per_rank_per_node = chunk_size\n\n    # Calculate the number of chunks per rank for each node\n    num_chunks_per_rank_per_node = chunk_size\n\n    # Calculate the number of chunks per rank for each node\n    num_chunks_per_rank_per_node = chunk_size\n\n    # Calculate the number of chunks per rank for each node\n    num_chunks_per_rank_per_node = chunk_size\n\n    # Calculate the number of chunks per rank for each node\n    num_chunks_per_rank_per_"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) == 1:\n        return _resolve_dir(indexed_paths[0])\n\n    if len(indexed_paths) == 2:\n        if indexed_paths[0] == indexed_paths[1]:\n            return _resolve_dir(indexed_paths[0])\n        else:\n            raise ValueError(\n                f\"Inconsistent file paths found in inputs: {indexed_paths[0]} and {indexed_paths[1]}\"\n            )\n\n    raise ValueError(\n        f\"Too many file paths found in inputs: {indexed_paths.values()}\"\n    )\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if _IS_IN_STUDIO:\n        yield\n        return\n\n    if enable:\n        # Enable DNS optimization\n        Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet.ip.ttl=1\"], stdout=DEVNULL, stderr=DEVNULL)\n    else:\n        # Disable DNS optimization\n        Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet.ip.ttl=64\"], stdout=DEVNULL, stderr=DEVNULL)\n\n    try:\n        yield\n    finally:\n        # Disable DNS optimization\n        Popen([\"sudo\", \"sysctl\", \"-w\", \"net.inet.ip.ttl=64\"], stdout=DEVNULL, stderr=DEVNULL)"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items each rank should process\n    num_items_per_rank = len(indexes) // distributed_env.world_size\n    if drop_last:\n        num_items_per_rank = num_items_per_rank - len(indexes) % distributed_env.world_size\n\n    # distribute the chunks and their intervals to the ranks\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    chunk_intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    for i in range(len(indexes)):\n        rank = i % distributed_env.world_size\n        chunks_per_ranks[rank].append(indexes[i])\n        chunk_intervals_per_ranks[rank].append(chunk_intervals[i])\n\n    return chunks_per_ranks, chunk_intervals_per_ranks\n\n"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            device = self._device or \"cpu\"\n            if isinstance(device, torch.device):\n                device = device.type\n            kwargs = {\"device\": device}\n        else:\n            kwargs = {}\n\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, **kwargs)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(obj.bucket_name, obj.key)\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                logger.info(f\"File {obj.key} not found in bucket {obj.bucket_name}. Waiting for {sleep_time} seconds...\")\n                sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if isinstance(inputs, StreamingDataLoader) and weights is not None:\n        raise ValueError(\"When providing a streaming dataloader, weights isn't supported.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir"}
{"namespace": "litdata.processing.functions.map", "completion": "    if isinstance(output_dir, str):\n        output_dir = Dir(output_dir)\n\n    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"The provided output_dir {output_dir} isn't supported.\")\n\n    if not isinstance(inputs, (list, tuple)):\n        raise ValueError(f\"The provided inputs {inputs} isn't supported.\")\n\n    if not isinstance(fn, (FunctionType, partial)):\n        raise ValueError(f\"The provided fn {fn} isn't supported.\")\n\n    if num_workers is None:\n        num_workers = _get_default_num_workers()\n\n    if num_workers < 1:\n        raise ValueError(f\"The provided num_workers {num_workers} isn't supported.\")\n\n    if num_nodes is not None and num_nodes < 1:\n        raise ValueError(f\"The provided num_nodes {num_nodes} isn't supported.\")\n\n    if num_downloaders is not None and num_downloaders < 1:\n        raise ValueError(f\"The provided num_downloaders {num_downloaders} isn't supported.\")\n\n    if num_uploaders is not None and num_uploaders < 1:\n        raise ValueError(f\"The provided num_uploaders {num_uploaders} isn't supported.\")\n\n    if batch_size is not None and batch_size < 1:\n        raise ValueError(f\"The provided batch_size {batch_size} isn't supported.\")\n\n    if fast_dev_run and not isinstance(fast_dev_run, bool):\n        fast_dev_run = fast_dev_run > 0\n\n    if fast_dev_run and len(inputs) == 0:\n        raise ValueError(\"The provided inputs are empty.\")\n\n    if fast_dev_run and len(inputs) > 0:\n        inputs = inputs[:1]\n\n    if error_when_not_empty:\n        _assert_dir_is_empty(output_dir)\n\n    if num_nodes is not None and num_"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client(input_dir.url)\n\n    while True:\n        try:\n            task = queue_in.get(timeout=10)\n            index, files = task\n            for file in files:\n                obj = parse.urlparse(file)\n                if not os.path.exists(os.path.join(cache_dir, obj.path.lstrip(\"/\"))):\n                    _wait_for_file_to_exist(s3, obj)\n                    s3.download(obj, os.path.join(cache_dir, obj.path.lstrip(\"/\")))\n            queue_out.put(index)\n        except Empty:\n            pass\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Collect paths\n        paths = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if paths is None:\n            return\n\n        # 3. Iterate through the paths and upload them sequentially.\n        for path in paths:\n            if isinstance(path, tuple):\n                dirpath, path = path\n                path = os.path.join(dirpath, path)\n\n            if output_dir.url:\n                if output_dir.path:\n                    path = path.replace(cache_dir, output_dir.path)\n\n                obj = parse.urlparse(path)\n\n                if obj.scheme == \"s3\":\n                    s3.client.upload_file(path, obj.netloc, obj.path.lstrip(\"/\"))\n\n                elif obj.scheme == \"file\":\n                    shutil.copyfile(path, obj.path)\n\n                else:\n                    raise ValueError(f\"The provided {output_dir.url} isn't supported.\")\n\n            elif output_dir.path:\n                if not path.startswith(output_dir.path):\n                    path = path.replace(cache_dir, output_dir.path)\n\n                if os.path.isfile(path):\n                    shutil.copyfile(path, path)\n\n            else:\n                raise ValueError(f\"The provided {output_dir.url} isn't supported.\")\n\n            # 4. Send the path to the remove queue\n            remove_queue.put(path)\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Calculate the total number of workers across all nodes.\n    total_workers = num_nodes * num_workers\n\n    # Distribute items to the workers based on provided weights.\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=total_workers)\n\n    # Print the distribution details for workers on the current node.\n    print(f\"Distributing items to workers on node {node_rank}\")\n    for worker_id in worker_ids_this_node:\n        worker_items_this_node = worker_items[worker_id]\n        worker_weights_this_node = worker_weights[worker_id]\n        if file_size:\n            print(\n                f\"Worker {worker_id} on node {node_rank} has {len(worker_items_this_node)} items with total size {sum(worker_weights_this_node) / 1000000:.2f} MB\"\n            )\n        else:\n            print(\n                f\"Worker {worker_id} on node {node_rank} has {len(worker_items_this_node)} items with total weight {sum(worker_weights_this_node)}\"\n            )\n\n    # Shuffle the items for each worker and return the result.\n    result = []\n    for worker_id in worker_ids_this_node:\n        worker_items_this_node = worker_items[worker_id]\n        random.shuffle(worker_items_this_node)\n        result.append(worker_items_this_node)\n\n    return result\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Calculate the total number of workers across all nodes\n    total_workers = _get_num_nodes() * num_workers\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // total_workers\n\n    # Calculate the number of items that will be assigned to the last workers\n    remainder = len(user_items) % total_workers\n\n    # Calculate the start and end indices for each worker's items\n    start_indices = np.cumsum([num_items_per_worker + 1] * (total_workers - remainder) + [num_items_per_worker] * remainder)\n    end_indices = np.cumsum([num_items_per_worker] * (total_workers - remainder) + [num_items_per_worker + 1] * remainder)\n\n    # Create a list of lists, where each sublist contains the items assigned to a worker\n    worker_items = [user_items[start:end] for start, end in zip(start_indices, end_indices)]\n\n    # Ensure that the output list has a length equal to the number of workers\n    if len(worker_items) != total_workers:\n        raise RuntimeError(\"Improper assignment of items to workers.\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "#litdata/streaming/dataloader.py\nimport os\nfrom typing import Any, List, Optional, Union\n\nfrom litdata.streaming.resolver import _resolve_dir\nfrom litdata.utilities.packing import _pack_greedily\n\n\nclass StreamingDataLoader:\n    def __init__(\n        self,\n        items: List[Any],\n        num_workers: Optional[int] = None,\n        weights: Optional[List[int]] = None,\n        input_dir: Optional[Union[str, \"Dir\"]] = None,\n    ) -> None:\n        \"\"\"The StreamingDataLoader is responsible to process the user data.\"\"\"\n        self.items = items\n        self.num_workers = num_workers\n        self.weights = weights\n        self.input_dir = _resolve_dir(input_dir)\n\n    def remap_items(self, items: List[Any], num_workers: int) -> List[List[Any]]:\n        \"\"\"The remap_items method is responsible to remap the items to the workers.\"\"\"\n        if self.weights is not None:\n            worker_items, worker_weights = _pack_greedily(items=self.items, weights=self.weights, num_bins=num_workers)\n            worker_ids_this_node = range(num_workers)\n\n            for worker_id, size in worker_weights.items():\n                if worker_id not in worker_ids_this_node:\n                    continue\n\n                print(f\"Worker {worker_id} gets ({len(worker_items[worker_id])}) items for a total weight of {size}.\")\n\n            return [worker_items[worker_id] for worker_id in worker_ids_this_node]\n\n        items_per_worker = len(self.items) // num_workers\n        extra_items = len(self.items) % num_workers\n\n        start = 0\n        result = []\n        for i in range(num_workers):\n            worker_items = items_per_worker + 1 if"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    for future in concurrent.futures.as_completed(futures):\n        item_sizes.append(future.result())\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if not isinstance(element, str):\n        return False\n\n    if input_dir is not None:\n        if element.startswith(input_dir):\n            return True\n\n        element = str(Path(element).absolute())\n\n    return os.path.exists(element)\n\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            from tinycudann.layers import Dense, Activation\n\n            if n_neurons > 1024:\n                n_neurons = 1024\n\n            layers = [Dense(n_input_dims, n_neurons, seed=self._get_seed())]\n            for _ in range(n_layers - 1):\n                layers.append(Activation(\"ReLU\"))\n                layers.append(Dense(n_neurons, n_neurons, seed=self._get_seed()))\n\n            layers.append(Activation(output_activation))\n            layers.append(Dense(n_neurons, n_output_dims, seed=self._get_seed()))\n\n            return NetworkWithSkipLayers(layers[:-1], layers[-1])\n\n        else:\n            layers = []\n            if activation == \"ReLU\":\n                layers.append(nn.ReLU())\n            elif activation == \"None\":\n                pass\n            else:\n                raise ValueError(f\"Unknown activation function: {activation}\")\n\n            layers.append(nn.Linear(n_input_dims, n_neurons))\n            for _ in range(n_layers - 1):\n                layers.append(nn.ReLU())\n                layers.append(nn.Linear(n_neurons, n_neurons))\n\n            if output_activation == \"ReLU\":\n                layers.append(nn.ReLU())\n            elif output_activation == \"Sigmoid\":\n                layers.append(nn.Sigmoid())\n            elif output_activation == \"None\":\n                pass\n            else:\n                raise ValueError(f\"Unknown activation function: {output_activation}\")\n\n            layers.append(nn.Linear(n_neurons, n_output_dims))\n\n            return nn.Sequential(*layers)"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Calculate the length of the signal and the total number of shifts\n        signal_length = len(signal)\n        num_shifts = 2 * kernel_offset + 1\n\n        # Initialize an empty array to store the shifted signals\n        shifted_signals = np.zeros((num_shifts, signal_length))\n\n        # Shift the signal by the specified kernel offset and store the shifted signals in the shifted_signals array\n        for i in range(num_shifts):\n            shifted_signals[i] = np.roll(signal, i - kernel_offset)\n\n        # Calculate the rolling median by taking the median of the shifted signals along the first axis (axis=0)\n        rolling_median = np.median(shifted_signals, axis=0)\n\n        # Trim the rolling median to remove edge effects introduced by the shifting process\n        trimmed_rolling_median = rolling_median[kernel_offset:-kernel_offset]\n\n        return trimmed_rolling_median"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    if template_probe.iriscodes.shape != template_gallery.iriscodes.shape:\n        raise MatcherError(\"Iris codes have different shape\")\n\n    if template_probe.maskcodes.shape != template_gallery.maskcodes.shape:\n        raise MatcherError(\"Mask codes have different shape\")\n\n    if template_probe.iriscodes.shape[0] != template_gallery.iriscodes.shape[0]:\n        raise MatcherError(\"Iris codes have different number of iris\")\n\n    if template_probe.iriscodes.shape[1] != template_gallery.iriscodes.shape[1]:\n        raise MatcherError(\"Iris codes have different number of iris\")\n\n    if template_probe.iriscodes.shape[2] != template_gallery.iriscodes.shape[2]:\n        raise MatcherError(\"Iris codes have different number of iris\")\n\n    if template_probe.maskcodes.shape[0] != template_gallery.maskcodes.shape[0]:\n        raise MatcherError(\"Mask codes have different number of iris\")\n\n    if template_probe.maskcodes.shape[1] != template_gallery.maskcodes.shape[1]:\n        raise MatcherError(\"Mask codes have different number of iris\")\n\n    if template_probe.maskcodes.shape[2] != template_gallery.maskcodes.shape[2]:\n        raise MatcherError(\"Mask codes have different number of iris\")\n\n    if template_probe.iriscodes.shape[3] != template_gallery.iriscodes.shape[3]:\n        raise MatcherError(\"Iris codes have different number of iris\")\n\n    if template_probe.maskcodes.shape[3] != template_gallery.maskcodes.shape[3]:\n        raise MatcherError(\"Mask codes have different number of iris\")\n\n    if template_probe.iriscodes.shape[4] != template_gallery.iriscodes.shape[4]:\n        raise MatcherError(\"Iris codes"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        num_bisectors = self.params.num_bisectors\n        max_iterations = self.params.max_iterations\n\n        first_bisectors_point = np.zeros((num_bisectors, 2))\n        second_bisectors_point = np.zeros((num_bisectors, 2))\n\n        for i in range(num_bisectors):\n            for j in range(max_iterations):\n                first_point_index = np.random.randint(0, len(polygon))\n                second_point_index = np.random.randint(0, len(polygon))\n\n                if first_point_index == second_point_index:\n                    continue\n\n                first_point = polygon[first_point_index]\n                second_point = polygon[second_point_index]\n\n                distance = np.linalg.norm(first_point - second_point)\n\n                if distance > min_distance_between_sector_points_in_px:\n                    first_bisectors_point[i] = first_point\n                    second_bisectors_point[i] = second_point\n                    break\n\n            if np.linalg.norm(first_bisectors_point[i] - second_bisectors_point[i]) < min_distance_between_sector_points_in_px:\n                raise EyeCentersEstimationError(\n                    f\"Failed to find a sufficient number of point pairs that meet the distance criterion within {max_iterations} iterations.\"\n                )\n\n        return first_bisectors_point, second_bisectors_point\n"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.before_execute(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.after_execute(self, result, *args, **kwargs)\n\n        return result\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        def get_class_definition(type_hint):\n            if type_hint.__module__ == 'builtins':\n                return None\n            else:\n                return type_hint\n\n        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        input_type_hints = {}\n        output_type_hints = {}\n\n        for param_name, param in signature.parameters.items():\n            if param.kind == param.VAR_KEYWORD:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR_POSITIONAL:\n                continue\n            if param.kind == param.VAR"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] += 1\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        if loaded_bit_array is None:\n            logging.warning(\"No bit array found in persistence. Initializing new bit array.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n            return\n\n        if len(loaded_bit_array) != self.size:\n            logging.warning(\"Loaded bit array length does not match expected length. Initializing new bit array.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n            return\n\n        self.bit_array = loaded_bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for i in range(self.hash_count):\n            index = (hash1 + i * hash2) % self.size\n            if self.bit_array[index] == 0:\n                return False\n        return True\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        if \"distilled_model\" in json_dict:\n            self.distilled_model = config_factory.create_model_config(json_dict[\"distilled_model\"])\n        if \"current_model_stats\" in json_dict:\n            self.current_model_stats = json_dict[\"current_model_stats\"]\n        if \"last_training_run\" in json_dict:\n            self.last_training_run = json_dict[\"last_training_run\"]\n        if \"current_training_run\" in json_dict:\n            self.current_training_run = json_dict[\"current_training_run\"]\n        if \"nr_of_training_runs\" in json_dict:\n            self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if \"teacher_models\" in json_dict:\n            self.teacher_models = [config_factory.create_model_config(teacher_model) for teacher_model in json_dict[\"teacher_models\"]]\n\n        return self\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # Validate the input parameters\n        for param in LLM_GENERATION_PARAMETERS:\n            if param in kwargs:\n                if not isinstance(kwargs[param], (int, float)):\n                    raise ValueError(f\"Invalid value for parameter '{param}': {kwargs[param]}\")\n\n        # Set the default model name if not provided\n        if not model.model_name:\n            model.model_name = DEFAULT_DISTILLED_MODEL_NAME\n\n        # Set the default temperature if not provided\n        if \"temperature\" not in kwargs:\n            kwargs[\"temperature\"] = 0.7\n\n        # Set the default max_new_tokens if not provided\n        if \"max_new_tokens\" not in kwargs:\n            kwargs[\"max_new_tokens\"] = 100\n\n        # Set the default frequency_penalty if not provided\n        if \"frequency_penalty\" not in kwargs:\n            kwargs[\"frequency_penalty\"] = 0\n\n        # Set the default presence_penalty if not provided\n        if \"presence_penalty\" not in kwargs:\n            kwargs[\"presence_penalty\"] = 0\n\n        # Set the default top_p if not provided\n        if \"top_p\" not in kwargs:\n            kwargs[\"top_p\"] = 1\n\n        # Set the default stop if not provided\n        if \"stop\" not in kwargs:\n            kwargs[\"stop\"] = model.parsing_helper_tokens\n\n        # Set the default model if not provided\n        if \"model\" not in kwargs:\n            kwargs[\"model\"] = model.model_name\n\n        # Set the default stream if not provided\n        if \"stream\" not in kwargs:\n            kwargs[\"stream\"] = False\n\n        # Set the default logprobs if not provided\n        if \"logprobs\" not in kwargs:\n            kwargs[\"logprobs\"] = None\n\n        # Set the default echo"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_symmetric(x)\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The matrix must be a distance matrix\")\n\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize it\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize it\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize it\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize it\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize it\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize it\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize it\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize it\n            self.initialized_functions[func_hash] = {\"model\": \"\", \"examples\": []}\n\n        # check if the function is already initialized\n        if func_hash not in self.initialized_functions:\n            # if not, initialize it\n            self.initialized_functions[func"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        # Use the Higham & Nick (2002) algorithm to find the nearest positive definite matrix\n        cov_nearest = cov.copy()\n        for _ in range(higham_max_iteration):\n            try:\n                np.linalg.cholesky(cov_nearest)\n                break\n            except np.linalg.LinAlgError:\n                # Compute the spectral decomposition of the matrix\n                eigvals, eigvecs = np.linalg.eigh(cov_nearest)\n                # Clip the eigenvalues to ensure they are positive\n                eigvals = np.maximum(eigvals, _CLIPPING_VALUE)\n                # Reconstruct the matrix from the spectral decomposition\n                cov_nearest = eigvecs @ np.diag(eigvals) @ eigvecs.T\n        return cov_nearest\n\n    # Clip the eigenvalues to ensure they are positive\n    eigvals, eigvecs = np.linalg.eigh(cov)\n    eigvals = np.maximum(eigvals, _CLIPPING_VALUE)\n    # Reconstruct the matrix from the spectral decomposition\n    cov_nearest = eigvecs @ np.diag(eigvals) @ eigvecs.T\n    return cov_nearest\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, bytes):\n        return (obj,), IdentitySchema()\n    if isinstance(obj, list):\n        return ListSchema.flatten(obj)\n    if isinstance(obj, tuple):\n        return TupleSchema.flatten(obj)\n    if isinstance(obj, dict):\n        return DictSchema.flatten(obj)\n    if isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    if isinstance(obj, Boxes):\n        return TensorWrapSchema.flatten(obj)\n    if isinstance(obj, ROIMasks):\n        return TensorWrapSchema.flatten(obj)\n    raise TypeError(f\"Unsupported type {type(obj)}\")\n\n"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Convert input to numpy arrays\n    groups = np.array(groups)\n    equations = np.array(equations)\n\n    # Check if any group is mentioned in the equations\n    if not any(group in equation for equation in equations for group in groups):\n        if raise_if_group_missing:\n            raise GroupNotFoundError(\n                f\"None of the groups in {names[0]} are mentioned in the {names[1]}.\"\n            )\n        else:\n            warnings.warn(\n                f\"None of the groups in {names[0]} are mentioned in the {names[1]}.\"\n            )\n            return None\n\n    # Initialize the left and right matrices\n    left = np.zeros((len(equations), groups.shape[1]))\n    right = np.zeros(len(equations))\n\n    # Iterate over each equation\n    for i, equation in enumerate(equations):\n\n        # Split the equation into left and right sides\n        left_side, right_side = equation.split(\"<=\")\n\n        # Split the left side into groups and coefficients\n        groups_coeffs = re.findall(r\"([^\\s]+)\", left_side)\n\n        # Iterate over each group and coefficient\n        for group_coeff in groups_coeffs:\n\n            # Split the group and coefficient\n            group, coeff = group_coeff.split(\"*\")\n\n            # Find the index of the group in the groups array\n            group_index = np.where(groups == group)[0][0]\n\n            # Add the coefficient to the left matrix\n            left[i, group_index] = float(coeff)\n\n        # Add the right side to the right matrix\n        right[i] = float(right_side)\n\n    # Check if all elements in a group should sum to one\n    if sum_to_one:\n        left = np.hstack((left, np.ones((left.shape[0], 1))))\n\n    # Return the left and right matrices\n    return left, right\n\n"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n    with ExitStack() as stack:\n        # create a new module for the class\n        module = stack.enter_context(tempfile.TemporaryDirectory())\n        module_name = f\"instances_module_{_counter}\"\n        module_path = os.path.join(module, f\"{module_name}.py\")\n\n        # write the class definition to the module\n        with open(module_path, \"w\") as f:\n            f.write(f\"class {module_name}(Instances):\\n\")\n            for field in fields:\n                f.write(f\"    {field[0]}: {field[1]}\\n\")\n\n        # import the module and get the class\n        sys.path.insert(0, module)\n        newInstances = _import_file(module_name)\n        stack.callback(lambda: sys.path.remove(module))\n\n        # add from_instances method to the class\n        _add_instances_conversion_methods(newInstances)\n\n        # set up the environment\n        with mock.patch(\"detectron2.structures.Instances\", newInstances):\n            yield newInstances\n\n        # clean up\n        _clear_jit_cache()\n\n"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    bbox = annotation[\"bbox\"]\n    bbox = BoxMode.convert(bbox, annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox = transforms.apply_box([bbox])[0]\n    annotation[\"bbox\"] = BoxMode.convert(bbox, BoxMode.XYXY_ABS, annotation[\"bbox_mode\"])\n\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # polygon\n            for i, seg in enumerate(annotation[\"segmentation\"]):\n                annotation[\"segmentation\"][i] = transforms.apply_segmentation(seg)[0]\n        else:\n            # RLE\n            annotation[\"segmentation\"] = transforms.apply_segmentation(\n                annotation[\"segmentation\"]\n            )[0]\n\n    if \"keypoints\" in annotation:\n        keypoints = annotation[\"keypoints\"]\n        if len(keypoints) == 0:\n            return annotation\n        keypoints = keypoints.reshape(-1, 3)\n        keypoints[:, :2] = transforms.apply_coords(keypoints[:, :2]).reshape(-1, 2)\n        if keypoint_hflip_indices is not None:\n            keypoints[keypoint_hflip_indices] = keypoints[keypoint_hflip_indices][:, ::-1]\n        annotation[\"keypoints\"] = keypoints.flatten()\n\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n    return annotation\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        coords = np.array(coords)\n        coords = np.concatenate((coords, np.ones((coords.shape[0], 1))), axis=1)\n        coords = np.dot(coords, self.rm_coords.T)\n        return coords[:, :2]\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Run the model with the provided inputs\n    model.eval()\n    with torch.no_grad():\n        _ = model(inputs)\n\n    # Compute the flops using jit compilation\n    flops = flop_count(model, inputs)\n\n    # Convert the flops to Gflop\n    flops_gflop = flops / 1e9\n\n    # Convert the flops to a dictionary\n    flops_dict = flops_gflop.to_dict()\n\n    return flops_dict\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img.shape[:2] != (self.h, self.w):\n            raise ValueError(\"Input image shape must match initial image shape.\")\n        if self.angle % 360 == 0:\n            return img\n        if interp is None:\n            interp = self.interp\n        if len(img.shape) > 2 and img.shape[2] == 1:\n            pil_image = Image.fromarray(img[:, :, 0], mode=\"L\")\n        else:\n            pil_image = Image.fromarray(img)\n        pil_image = pil_image.rotate(self.angle, resample=interp, expand=True)\n        ret = np.asarray(pil_image)\n        if len(img.shape) > 2 and img.shape[2] == 1:\n            ret = np.expand_dims(ret, -1)\n        return ret\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        # Draw predictions\n        if \"pred_boxes\" in predictions:\n            boxes = predictions[\"pred_boxes\"]\n            boxes = boxes.tensor.cpu()\n            boxes = BoxMode.convert(boxes, BoxMode.XYXY_ABS, BoxMode.XYWH_ABS)\n            boxes = boxes.numpy()\n            boxes[:, 2] = np.maximum(boxes[:, 2], 1)\n            boxes[:, 3] = np.maximum(boxes[:, 3], 1)\n            self.draw_box(boxes, thickness=2)\n\n        if \"pred_classes\" in predictions:\n            classes = predictions[\"pred_classes\"].tolist()\n            scores = predictions[\"scores\"].tolist()\n            labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n            self.draw_text(labels)\n\n        if \"pred_masks\" in predictions:\n            masks = predictions[\"pred_masks\"]\n            masks = masks.cpu()\n            masks = masks.numpy()\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n            self.draw_mask(masks)\n\n        if \"pred_keypoints\" in predictions:\n            keypoints = predictions[\"pred_keypoints\"]\n            keypoints = keypoints.cpu()\n            keypoints = keypoints.numpy()\n            keypoints = [GenericMask(x, self.output.height, self.output.width) for x in keypoints]\n            self.draw_keypoints(keypoints)\n\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        # Convert the canvas to an RGBA image\n        canvas = self.canvas\n        canvas.draw()\n        rgba_image = np.frombuffer(canvas.tostring_rgba(), dtype=np.uint8).reshape(canvas.get_width_height()[::-1] + (4,))\n\n        # Convert the RGBA image to RGB format\n        rgb_image = rgba_image[:, :, :3].copy()\n\n        # Return the RGB image as a numpy ndarray of uint8 type\n        return rgb_image\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            self.draw_instance_predictions(dic[\"annotations\"])\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"])\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(dic[\"panoptic_seg\"], dic.get(\"segments_info\", None))\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        if edge_color is None:\n            edge_color = _OFF_WHITE\n\n        # convert to mask\n        mask = binary_mask.astype(np.uint8)\n        mask = mask.astype(np.bool)\n        mask = mask.astype(np.uint8)\n        mask = GenericMask(mask, self.output.height, self.output.width)\n\n        # draw mask\n        for segment in mask.polygons:\n            self.draw_polygon(segment.reshape(-1, 2), color, edge_color, alpha=alpha)\n\n        # draw text\n        if text is not None:\n            # first get a box\n            x0, y0, x1, y1 = mask.bbox()\n            # for small objects, draw text at the side to avoid occlusion\n            instance_area = (y1 - y0) * (x1 - x0)\n            if (\n                instance_area < _SMALL_OBJECT_AREA_THRESH * self.output.scale\n                or y1 - y0 < 40 * self.output.scale\n            ):\n                if y1 >= self.output.height - 5:\n                    text_pos = (x1, y0)\n                else:\n                    text_pos = (x0, y1)\n            else:\n                text_pos = np.median(mask.mask.nonzero(), axis=1)[::-1]\n            height_ratio = (y1 - y0) / np.sqrt(self.output.height * self.output.width)\n            lighter_color = self._change_color_brightness(color, brightness_factor=0.7)\n            font_size = (\n                np.clip((height_ratio - 0.02) / 0.08 + 1, 1.2, 2)\n                * 0.5\n                * self._default_font_size\n            )\n            self.draw_text(\n                text,\n                text_pos,\n               "}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    # Check if the image sizes are the same\n    if size_as_tensor:\n        assert torch.allclose(\n            torch.tensor(input.image_size), torch.tensor(other.image_size)\n        ), f\"{msg}Image sizes do not match: {input.image_size} vs {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg}Image sizes do not match: {input.image_size} vs {other.image_size}\"\n\n    # Check if all fields are equal or close\n    for field in input._fields:\n        input_val = getattr(input, field)\n        other_val = getattr(other, field)\n\n        if isinstance(input_val, Boxes):\n            assert input_val.tensor.shape == other_val.tensor.shape, f\"{msg}Boxes tensor shapes do not match: {input_val.tensor.shape} vs {other_val.tensor.shape}\"\n            assert torch.allclose(input_val.tensor, other_val.tensor), f\"{msg}Boxes tensors do not match: {input_val.tensor} vs {other_val.tensor}\"\n        elif isinstance(input_val, ROIMasks):\n            assert input_val.tensor.shape == other_val.tensor.shape, f\"{msg}ROIMasks tensor shapes do not match: {input_val.tensor.shape} vs {other_val.tensor.shape}\"\n            assert torch.allclose(input_val.tensor, other_val.tensor), f\"{msg}ROIMasks tensors do not match: {input_val.tensor} vs {other_val.tensor}\"\n        elif isinstance(input_val, torch.Tensor):\n            assert input_val.shape == other_val.shape, f\"{msg}Tensor shapes do not match: {input_val.shape} vs {other_val.shape}\"\n            assert torch.allclose(input_val, other_val, rtol=rtol), f\"{msg}Tensors do not match: {"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        return self.tensor[:, 2] * self.tensor[:, 3]\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        # In training, the proposals are the ground-truth, so the network can be\n        # trained with gradients from the proposals.\n        # In inference, the proposals are the predicted boxes from the R-CNN, so\n        # we only train the network with gradients from the feature maps.\n        # Faster R-CNN only supports training on single images.\n        if len(proposals) != 1:\n            raise ValueError(\"No support for multiple images\")\n\n        proposals = proposals[0]\n        if proposals.is_empty():\n            # During training, the proposals are empty for an image, so we ignore\n            # the loss.\n            return {}\n\n        scores, proposal_deltas = predictions\n        gt_classes = proposals.gt_classes\n        gt_boxes = proposals.gt_boxes\n        if self.box_reg_loss_type == \"giou\":\n            from detectron2.layers import generalized_box_iou\n\n            box_loss_func = generalized_box_iou\n        elif self.box_reg_loss_type == \"diou\":\n            from detectron2.layers import distance_box_iou\n\n            box_loss_func = distance_box_iou\n        elif self.box_reg_loss_type == \"ciou\":\n            from detectron2.layers import complete_box_iou\n\n            box_loss_func = complete_box_iou\n        else:\n            box_loss_func = None\n\n        # Classification loss\n        # We use F.cross_entropy instead of F.softmax + F.cross_entropy so that\n        # we don't need to convert scores into probabilities.\n        # The latter requires the gradient computation, which is redundant as\n        # we have already computed the gradients for the scores in the forward\n        # pass.\n        loss_cls = cross_entropy(\n            scores,\n            gt_classes,\n            self.loss_weight.get(\"loss_cls\", 1.0),\n        )\n        _log_classification_stats(scores, gt_classes)\n\n        # Box regression loss\n        if"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.MODEL.TRACKER.NAME\n    tracker_head = TRACKER_HEADS_REGISTRY.get(tracker_name)(cfg)\n    return tracker_head"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert isinstance(deltas, torch.Tensor), type(deltas)\n        assert isinstance(boxes, torch.Tensor), type(boxes)\n\n        # Apply deltas to each corner of each box\n        # x1, y1, x2, y2 = torch.split(boxes, 1, dim=1)\n        # dx1, dy1, dx2, dy2 = torch.split(deltas, 1, dim=1)\n        # x1_pred = dx1 + x1\n        # y1_pred = dy1 + y1\n        # x2_pred = dx2 + x2\n        # y2_pred = dy2 + y2\n        # pred_boxes = torch.cat((x1_pred, y1_pred, x2_pred, y2_pred), dim=1)\n\n        # Apply deltas to each corner of each box\n        # x1, y1, x2, y2 = torch.split(boxes, 1, dim=1)\n        # dx1, dy1, dx2, dy2 = torch.split(deltas, 1, dim=1)\n        # x1_pred = dx1 + x1\n        # y1_pred = dy1 + y1\n        # x2_pred = dx2 + x2\n        # y2_pred = dy2 + y2\n        # pred_boxes = torch.cat((x1_pred, y1_pred, x2_pred, y2_pred), dim=1)\n\n        # Apply deltas to each corner of each box\n        x1, y1, x2, y2 = torch.split(boxes, 1, dim=1)\n        dx1, dy1, dx2, dy2 = torch.split(deltas, 1, dim=1)\n        x1_pred = dx1 + x1\n        y1_pred = dy1 + y1\n        x2_pred = dx2 + x2\n        y2_pred = dy2 + y2\n        pred_boxes = torch"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if isinstance(anno_type, str):\n            anno_type = [anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            pass\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')\n\n        output = self.general_ins(image)\n        if anno_type is None:\n            return output\n        else:\n            anno_dict = {}\n            for tp in anno_type:\n                if tp in output.keys():\n                    anno_dict[tp] = output[tp]\n            return anno_dict"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        query = normalize_string(query)\n        keywords = query.split()\n        scores = {}\n        for kw in keywords:\n            scores = update_url_scores(scores, self.bm25(kw))\n        return scores\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize angles to [-180, 180]\n        self.normalize_angles()\n\n        # Identify indices of nearly horizontal boxes\n        angle_threshold = clip_angle_threshold * math.pi / 180.0\n        angle_mask = torch.abs(self.tensor[:, 4]) < angle_threshold\n\n        # Convert to (x1, y1, x2, y2) representation\n        x1 = self.tensor[:, 0] - self.tensor[:, 2] / 2\n        y1 = self.tensor[:, 1] - self.tensor[:, 3] / 2\n        x2 = self.tensor[:, 0] + self.tensor[:, 2] / 2\n        y2 = self.tensor[:, 1] + self.tensor[:, 3] / 2\n\n        # Clamp x and y coordinates\n        x1 = torch.clamp(x1, min=0, max=box_size[1])\n        y1 = torch.clamp(y1, min=0, max=box_size[0])\n        x2 = torch.clamp(x2, min=0, max=box_size[1])\n        y2 = torch.clamp(y2, min=0, max=box_size[0])\n\n        # Convert back to (center x, center y, width, height, angle) representation\n        self.tensor[:, 0] = (x1 + x2) / 2\n        self.tensor[:, 1] = (y1 + y2) / 2\n        self.tensor[:, 2] = x2 - x1\n        self.tensor[:, 3] = y2 - y1\n\n        # Convert angles back to [-180, 180]\n        self.normalize_angles()\n"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n        for item in self.data:\n            statistics[item['type']] += 1\n        return statistics\n\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMSEG_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg are deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and ' \\\n        'model field is deprecated, please check'\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and ' \\\n        'model field is deprecated, please check'\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(cfg, train_cfg, test_cfg)\n    else:\n        return MMDET_DETECTORS.build(cfg, train_cfg, test_cfg)\n\n"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    if box_type_3d is not None and box_mode_3d is not None:\n        from mmdet3d.core.bbox import box_np_ops\n        from mmdet3d.core.bbox.structures import get_box_type\n        box_type_3d = get_box_type(box_type_3d)\n        gt_annos = box_type_3d(gt_annos, box_dim=-1, box_mode_3d=box_mode_3d)\n        dt_annos = box_type_3d(dt_annos, box_dim=-1, box_mode_3d=box_mode_3d)\n\n    if isinstance(metric, float):\n        metric = [metric]\n    recalls, precisions, ap = eval_map_recall(dt_annos, gt_annos, metric)\n\n    eval_results = {}\n    print_log(f'Evaluation results of metric {metric}', logger=logger)\n    for i, iou_thr in enumerate(metric):\n        print_log(f'IoU threshold: {iou_thr}', logger=logger)\n        for j, cls in enumerate(label2cat):\n            print_log(f'{cls}: ap {ap[i][j]:.4f}, recall {recalls[i][j]:.4f}, '\n                      f'precision {precisions[i][j]:.4f}',\n                      logger=logger)\n            eval_results[f'AP/{cls}/{iou_thr}'] = ap[i][j]\n            eval_results[f'Recall/{cls}/{iou_thr}'] = recalls[i][j]\n            eval_results[f'Precision/{cls}/{iou_thr}'] = precisions[i][j]\n        print_log(f'mAP: {ap[i].mean():.4f}', logger=logger)\n        print_log(f'mAR: {recalls[i].mean():.4f}', logger="}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        box_cls = LiDARInstance3DBoxes\n        mode = 'lidar'\n    elif box_type == 'Camera':\n        box_cls = CameraInstance3DBoxes\n        mode = 'camera'\n    elif box_type == 'Depth':\n        box_cls = DepthInstance3DBoxes\n        mode = 'depth'\n    else:\n        raise ValueError(f'box type {box_type} is not supported')\n\n    return box_cls, mode\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if messages is None:\n      messages = []\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    for message in messages:\n      if not isinstance(message, Message):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': [\n          {\n            'role': message.role,\n            'content': message.content,\n            'images': [_encode_image(image) for image in message.images or []],\n          }\n          for message in messages\n        ],\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'model': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('model is required')\n\n    if not prompt and not system:\n      raise RequestError('prompt or system is required')\n\n    if images:\n      images = [b64encode(image).decode() for image in images]\n\n    if options:\n      options = options.dict()\n\n    if keep_alive:\n      if isinstance(keep_alive, str):\n        keep_alive = float(keep_alive)\n      options = options or {}\n      options['keep_alive'] = keep_alive\n\n    return self._request_stream(\n      'POST',\n      '/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context,\n        'raw': raw,\n        'format': format,\n        'images': images,\n        'options': options,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not path and not modelfile:\n      raise RequestError('must provide a path or modelfile')\n\n    if path:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    path = Path(path)\n    checksum = sha256(path.read_bytes()).hexdigest()\n\n    response = self._client.head(f'/api/blobs/{checksum}')\n    if response.status_code == 404:\n      response = self._client.post(\n        '/api/blobs',\n        files={'file': path.open('rb')},\n      )\n      response.raise_for_status()\n\n    return response.json()['digest']\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Combine user code and test code\n        code = user_code + cls.CODE_SPLITTER + test_code\n\n        # Create a temporary file to store the combined code\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n            f.write(code)\n            f.flush()\n            file_path = f.name\n\n        # Run Pyright to perform type checking\n        try:\n            output = subprocess.check_output(\n                [\"pyright\", file_path],\n                stderr=subprocess.STDOUT,\n                universal_newlines=True,\n            )\n        except subprocess.CalledProcessError as e:\n            output = e.output\n\n        # Parse Pyright output to extract expected error messages and line numbers\n        expected_errors = []\n        for line in output.splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_no = int(match.group(1))\n                message = match.group(2)\n                if cls.EXPECT_ERROR_COMMENT in message:\n                    expected_errors.append((line_no, message))\n\n        # Remove the temporary file\n        Path(file_path).unlink()\n\n        # Return the result of the type check\n        return TypeCheckResult(\n            message=\"Type check passed\" if not expected_errors else \"Type check failed\",\n            passed=not expected_errors,\n            debug_info={\"expected_errors\": expected_errors},\n        )\n\n"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, compiler_fn=get_compiler_fn(title=\"Module\"))\n    else:\n        return aot_function(fn, compiler_fn=get_compiler_fn(title=\"Function\"))\n\n"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    config_dict = yaml.safe_load(open(os.path.join(trial_path, 'config.yaml')))\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            output_path += '.yaml'\n        with open(output_path, 'w') as f:\n            yaml.dump(best_config, f)\n\n    return best_config\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        key = (args, tuple(kwargs.items()))\n        with lock:\n            if key not in cache:\n                traced_module, traced_call = trace_with_kwargs(\n                    func, *args, **kwargs)\n                if ts_compiler is not None:\n                    traced_module = ts_compiler(traced_module)\n                cache[key] = traced_module, traced_call\n            traced_module, traced_call = cache[key]\n        return traced_call(traced_module)\n\n    cache = {}\n    lock = threading.Lock()\n    return wrapper\n\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        project_dir = os.path.dirname(trial_path)\n        return cls(config, project_dir=project_dir)\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the directory for this node line if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Evaluate and select the best retrieval node result\n    best_result = select_best_average(modules, module_params, previous_result, strategies)\n\n    # Save the best result to disk\n    best_result.to_csv(os.path.join(node_line_dir, \"best_result.csv\"), index=False)\n\n    # Save the summary of the execution times and evaluation metrics to disk\n    summary = pd.DataFrame(columns=[\"module\", \"execution_time\", \"evaluation_metrics\"])\n    for module, module_param, result in zip(modules, module_params, best_result):\n        summary = summary.append({\n            \"module\": module.__name__,\n            \"execution_time\": result[\"execution_time\"],\n            \"evaluation_metrics\": result[\"evaluation_metrics\"]\n        }, ignore_index=True)\n    summary.to_csv(os.path.join(node_line_dir, \"summary.csv\"), index=False)\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create the directory for the node line if it doesn't exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a list of dictionaries to store the results of each module\n    results = []\n\n    # Iterate over the modules and their parameters\n    for module, params in zip(modules, module_params):\n        # Create a directory for the module if it doesn't exist\n        module_dir = os.path.join(node_line_dir, module.__name__)\n        pathlib.Path(module_dir).mkdir(parents=True, exist_ok=True)\n\n        # Run the module with the given parameters\n        result = module(previous_result, **params)\n\n        # Save the result to a file\n        result_file = os.path.join(module_dir, \"result.csv\")\n        result.to_csv(result_file, index=False)\n\n        # Measure the execution time of the module\n        execution_time = measure_speed(module, previous_result, **params)\n\n        # Evaluate the performance of the module using the specified strategies\n        evaluation = evaluate_query_expansion_node(result, strategies)\n\n        # Add the module's results to the list of results\n        results.append({\n            \"module\": module.__name__,\n            \"params\": params,\n            \"result\": result,\n            \"execution_time\": execution_time,\n            \"evaluation\": evaluation\n        })\n\n    # Save the results to a file\n    results_file = os.path.join(node_line_dir, \"results.json\")\n    with open(results_file, \"w\") as f:\n        json.dump(results, f)\n\n    # Create a dataframe from the results\n    results_df = pd.DataFrame(results)\n\n    # Save the results dataframe to a file\n    results_df_file = os.path.join(node_line_dir, \"results_df.csv\")\n    results_df.to_csv(results_df_file, index=False)\n\n    #"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Get the generator module\n    generator_module = strategies.get('generator_module', 'default')\n\n    # Get the generator module parameters\n    generator_module_params = strategies.get('generator_module_params', {})\n\n    # Get the evaluation metrics\n    evaluation_metrics = strategies.get('evaluation_metrics', ['accuracy', 'f1'])\n\n    # Get the speed thresholds\n    speed_thresholds = strategies.get('speed_thresholds', [0.5, 1.0, 2.0])\n\n    # Get the evaluation metrics\n    evaluation_metrics = strategies.get('evaluation_metrics', ['accuracy', 'f1'])\n\n    # Get the speed thresholds\n    speed_thresholds = strategies.get('speed_thresholds', [0.5, 1.0, 2.0])\n\n    # Get the evaluation metrics\n    evaluation_metrics = strategies.get('evaluation_metrics', ['accuracy', 'f1'])\n\n    # Get the speed thresholds\n    speed_thresholds = strategies.get('speed_thresholds', [0.5, 1.0, 2.0])\n\n    # Get the evaluation metrics\n    evaluation_metrics = strategies.get('evaluation_metrics', ['accuracy', 'f1'])\n\n    # Get the speed thresholds\n    speed_thresholds = strategies.get('speed_thresholds', [0.5, 1.0, 2.0])\n\n    # Get the evaluation metrics\n    evaluation_metrics = strategies.get('evaluation_metrics', ['accuracy', 'f1'])\n\n    # Get the speed thresholds\n    speed_thresholds = strategies.get('speed_thresholds', [0.5, 1.0, 2.0])\n\n    # Get the evaluation metrics\n    evaluation_metrics = strategies.get('evaluation_metrics', ['accuracy', 'f1'])\n\n    # Get the"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(map(lambda x: extract_values(x, key), nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model()\n\n    # Convert the input strings into embeddings\n    pred_embedding = embedding_model.get_query_embedding(pred)\n    gt_embeddings = [embedding_model.get_query_embedding(gt) for gt in generation_gt]\n\n    # Calculate the cosine similarity between the predicted string and each ground truth string\n    cosine_similarities = [calculate_cosine_similarity(pred_embedding, gt_embedding) for gt_embedding in gt_embeddings]\n\n    # Return the maximum cosine similarity as the semantic similarity score\n    return max(cosine_similarities)\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer not set up\")\n        return np_image\n\n    return gfpgan_face_restorer.restore(np_image)\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global codeformer\n        codeformer = FaceRestorerCodeFormer(dirname)\n        face_restoration.face_restorers.append(codeformer)\n    except Exception as e:\n        logger.error(f\"Error setting up CodeFormer: {e}\")\n\n"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        global gfpgan_face_restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except Exception as e:\n        logger.warning(f\"GFPGAN face restorer setup failed: {e}\")\n\n"}
{"namespace": "quaternion.rotate", "completion": "  q_v = jnp.concatenate([jnp.zeros_like(v[..., :1]), v], axis=-1)\n  return multiply(multiply(q, q_v), conjugate(q))[..., 1:]\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis_angle = jnp.array(axis_angle)\n  axis = axis_angle[:3]\n  angle = axis_angle[3]\n  axis_norm = linalg.norm(axis, axis=-1, keepdims=True)\n  axis_norm = jnp.maximum(axis_norm, eps * jnp.ones_like(axis_norm))\n  axis = axis / axis_norm\n  angle = angle / 2.0\n  return jnp.concatenate([axis * jnp.sin(angle), jnp.cos(angle)], axis=-1)\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # check if idx is the argmax\n    num_calls = k\n    if model.argmax(prefix) == idx:\n        return 0, num_calls\n\n    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n        num_calls += k\n    high = logit_bias[idx]\n\n    # improve estimate\n    low = high / 2\n    while high >= low:\n        logit_bias[idx] = (high + low) / 2\n        if model.argmax(prefix, logit_bias) == idx:\n            high = logit_bias[idx]\n        else:\n            low = logit_bias[idx]\n        num_calls += k\n    return -logit_bias[idx], num_calls\n\n"}
{"namespace": "resample.resample_3d", "completion": "  if half_pixel_center:\n    locations = locations + 0.5\n\n  if coordinate_order == 'xyz':\n    x_coordinate = locations[Ellipsis, 0]\n    y_coordinate = locations[Ellipsis, 1]\n    z_coordinate = locations[Ellipsis, 2]\n  elif coordinate_order == 'zyx':\n    z_coordinate = locations[Ellipsis, 0]\n    y_coordinate = locations[Ellipsis, 1]\n    x_coordinate = locations[Ellipsis, 2]\n\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    # Pad the input data with a constant value.\n    data = jnp.pad(data, ((1, 1), (1, 1), (1, 1), (0, 0)),\n                   mode='constant',\n                   constant_values=constant_values)\n\n    # Calculate the new coordinates after padding.\n    x_coordinate = x_coordinate + 1\n    y_coordinate = y_coordinate + 1\n    z_coordinate = z_coordinate + 1\n\n  elif edge_behavior == 'CLAMP':\n    # Clamp the coordinates to the valid range.\n    x_coordinate = jnp.clip(x_coordinate, 0, data.shape[2] - 1)\n    y_coordinate = jnp.clip(y_coordinate, 0, data.shape[1] - 1)\n    z_coordinate = jnp.clip(z_coordinate, 0, data.shape[0] - 1)\n\n  if method == 'TRILINEAR':\n    # Calculate the fractional coordinates.\n    x_fraction = x_coordinate - jnp.floor(x_coordinate)\n    y_fraction = y_coordinate - jnp.floor(y_coordinate)\n    z_fraction = z_coordinate - jnp.floor(z_coordinate)\n\n    # Calculate the indices of the 8 surrounding voxels.\n   "}
{"namespace": "math.plus_eps", "completion": "  return jnp.nextafter(x, jnp.inf)\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, lambda x, y, dy: dy / y, (tiny_val, max_val))(x)\n\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: x_dot / (2 * y),\n      (tiny_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 1:\n    return np.inf\n  elif p == 2:\n    return np.inf\n  elif p == 3:\n    return np.inf\n  elif p == 4:\n    return np.inf\n  elif p == 5:\n    return np.inf\n  elif p == 6:\n    return np.inf\n  elif p == 7:\n    return np.inf\n  elif p == 8:\n    return np.inf\n  elif p == 9:\n    return np.inf\n  elif p == 10:\n    return np.inf\n  elif p == 11:\n    return np.inf\n  elif p == 12:\n    return np.inf\n  elif p == 13:\n    return np.inf\n  elif p == 14:\n    return np.inf\n  elif p == 15:\n    return np.inf\n  elif p == 16:\n    return np.inf\n  elif p == 17:\n    return np.inf\n  elif p == 18:\n    return np.inf\n  elif p == 19:\n    return np.inf\n  elif p == 20:\n    return np.inf\n  elif p == 21:\n    return np.inf\n  elif p == 22:\n    return np.inf\n  elif p == 23:\n    return np.inf\n  elif p == 24:\n    return np.inf\n  elif p == 25:\n    return np.inf\n  elif p == 26:\n    return np.inf\n  elif p == 27:\n    return np.inf\n  elif p == 28:\n    return np.inf\n  elif p == 29:\n    return np.inf\n  elif p == 30:\n    return np.inf\n  elif p == 31:\n    return np.inf\n  elif p == 32:\n    return np.inf\n  elif p == 33:\n    return np.inf\n  elif p == 34:\n    return np.inf\n  elif p == 35:\n    return np.inf\n  elif p == 36:\n    return np.inf\n  elif p"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([\n        [1, 1, 1],\n        [-1, 1, -1],\n        [-1, -1, 1],\n        [1, -1, -1],\n    ])\n    base_faces = np.array([\n        [0, 1, 2],\n        [0, 1, 3],\n        [0, 2, 3],\n        [1, 2, 3],\n    ])\n  elif base_shape == 'icosahedron':\n    base_verts = np.array([\n        [0, 0, 1],\n        [0, 0.942809, 0.333333],\n        [0, -0.471405, 0.881921],\n        [0, -0.942809, -0.333333],\n        [0.816497, 0, 0.57735],\n        [0.408248, 0.707107, 0.57735],\n        [-0.408248, 0.707107, 0.57735],\n        [-0.816497, 0, 0.57735],\n        [-0.408248, -0.707107, 0.57735],\n        [0.408248, -0.707107, 0.57735],\n        [0.92388, 0, -0.382683],\n        [0.361508, 0.92388, -0.117317],\n        [-0.361508, 0.92388, -0.117317],"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (x + 1),\n      (tiny_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    y = x\n  elif p == 0:\n    y = jnp.sign(x)\n  elif p == -jnp.inf:\n    y = jnp.abs(x)\n  elif p == jnp.inf:\n    y = jnp.abs(x) ** 2\n  else:\n    y = jnp.abs(x) ** p\n\n  if postmult is not None:\n    y = y * postmult\n\n  return y\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y * premult\n  yp = jnp.abs(y)\n  ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = select(\n      [\n          (p == 1, yp),\n          (p == 0, safe_expm1(yp)),\n          (p == -jnp.inf, -safe_log1p(-yp)),\n          (p == jnp.inf, safe_log1p(yp)),\n      ],\n      clip_finite_nograd(\n          jnp.sign(y) * (jnp.abs(p_safe - 1) / p_safe * (1 + ys) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  if lr_delay_steps > 0:\n    lr_init *= lr_delay_mult\n    lr_delay_mult = 1\n\n  if step < lr_delay_steps:\n    return lr_init * (step / lr_delay_steps) ** lr_delay_mult\n  else:\n    return lr_final * (1 - (step - lr_delay_steps) / (max_steps - lr_delay_steps))\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  rng = random.PRNGKey(0)\n  n = 1\n  origin_lo = -1.0\n  origin_hi = 1.0\n  radius_lo = 0.0\n  radius_hi = 1.0\n  near_lo = 0.0\n  near_hi = 1.0\n  far_lo = 1.0\n  far_hi = 2.0\n\n  return generate_random_rays(\n      rng,\n      n,\n      origin_lo,\n      origin_hi,\n      radius_lo,\n      radius_hi,\n      near_lo,\n      near_hi,\n      far_lo,\n      far_hi,\n      include_exposure_idx,\n      include_exposure_values,\n      include_device_idx,\n  )\n\n"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Must add half pixel offset to shoot rays through pixel centers.\n  def pix_to_dir(x, y):\n    return xnp.stack([x + 0.5, y + 0.5, xnp.ones_like(x)], axis=-1)\n\n  # We need the dx and dy rays to calculate ray radii for mip-NeRF cones.\n  pixel_dirs_stacked = xnp.stack(\n      [\n          pix_to_dir(pix_x_int, pix_y_int),\n          pix_to_dir(pix_x_int + 1, pix_y_int),\n          pix_to_dir(pix_x_int, pix_y_int + 1),\n      ],\n      axis=0,\n  )\n\n  # For jax, need to specify high-precision matmul.\n  matmul = math.matmul if xnp == jnp else xnp.matmul\n  mat_vec_mul = lambda A, b: matmul(A, b[Ellipsis, None])[Ellipsis, 0]\n\n  # Apply inverse intrinsic matrices.\n  camera_dirs_stacked = mat_vec_mul(pixtocams, pixel_dirs_stacked)\n\n  if distortion_params is not None:\n    # Correct for distortion.\n    x, y = _radial_and_tangential_undistort(\n        camera_dirs_stacked[Ellipsis, 0],\n        camera_dirs_stacked[Ellipsis, 1],\n        **distortion_params,\n        xnp=xnp,\n    )\n    camera_dirs_stacked = xnp.stack([x, y, xnp.ones_like(x)], -1)\n\n  if camtype == ProjectionType.FISHEYE:\n    theta = xnp.sqrt(xnp.sum(xnp.square(camera_dirs_stacked[Ellipsis, :2]), axis=-1))\n    theta ="}
{"namespace": "rigid_body.exp_se3", "completion": "  w = screw_axis[0:3]\n  v = screw_axis[3:6]\n  theta = jnp.linalg.norm(w)\n  if theta < eps:\n    return jnp.eye(4)\n  else:\n    w_hat = skew(w)\n    R = jnp.eye(3) + jnp.sin(theta) / theta * w_hat + (1 - jnp.cos(theta)) / theta**2 * jnp.matmul(w_hat, w_hat)\n    p = jnp.matmul(jnp.eye(3) - R, v)\n    return jnp.block([[R, p], [jnp.array([[0.0, 0.0, 0.0, 1.0]])]])\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Extract the axis and angle of rotation from the input axis-angle vector\n  axis = axis_angle / (jnp.linalg.norm(axis_angle) + eps)\n  angle = jnp.linalg.norm(axis_angle)\n\n  # Compute the rotation matrix using Rodrigues' formula\n  R = jnp.eye(3) + jnp.sin(angle) * skew(axis) + (1 - jnp.cos(angle)) * jnp.matmul(skew(axis), skew(axis))\n\n  return R\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= base_radius**2\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to camera coordinates.\n  pix_x_cam, pix_y_cam, pix_z_cam = (\n      pixtocams[Ellipsis, 0, 0] * pix_x_int + pixtocams[Ellipsis, 0, 1] * pix_y_int + pixtocams[Ellipsis, 0, 2],\n      pixtocams[Ellipsis, 1, 1] * pix_y_int + pixtocams[Ellipsis, 1, 1] * pix_y_int + pixtocams[Ellipsis, 1, 2],\n      pixtocams[Ellipsis, 2, 2],\n  )\n\n  # Apply lens distortion correction if distortion_params are provided.\n  if distortion_params is not None:\n    k1, k2, k3, k4, p1, p2 = (\n        distortion_params['k1'],\n        distortion_params['k2'],\n        distortion_params['k3'],\n        distortion_params['k4'],\n        distortion_params['p1'],\n        distortion_params['p2'],\n    )\n    pix_x_cam, pix_y_cam = _radial_and_tangential_undistort(\n        pix_x_cam, pix_y_cam, k1, k2, k3, k4, p1, p2, xnp=xnp\n    )\n\n  # Compute ray origins and directions.\n  origins = camtoworlds[Ellipsis, :3, 3]\n  directions = xnp.stack(\n      [\n          pix_x_cam,\n          pix_y_cam,\n          pix_z_cam,\n      ],\n      axis=-1,\n  )\n  directions = camtoworlds[Ellipsis, :3, :3] @ directions[..., None]\n  directions = directions[..., 0]\n\n  # Normalize ray directions.\n  viewdirs = directions / xnp"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n"}
{"namespace": "stepfun.sample", "completion": "  # Check input shapes and types\n  assert isinstance(t, jnp.ndarray), \"t must be a jnp.ndarray\"\n  assert isinstance(w_logits, jnp.ndarray), \"w_logits must be a jnp.ndarray\"\n  assert t.ndim == 1, \"t must be a 1D array\"\n  assert w_logits.ndim == 1, \"w_logits must be a 1D array\"\n  assert t.shape == w_logits.shape, \"t and w_logits must have the same shape\"\n  assert t.dtype == jnp.float32, \"t must be of dtype float32\"\n  assert w_logits.dtype == jnp.float32, \"w_logits must be of dtype float32\"\n  assert num_samples > 0, \"num_samples must be a positive integer\"\n  assert isinstance(single_jitter, bool), \"single_jitter must be a boolean\"\n  assert isinstance(deterministic_center, bool), \"deterministic_center must be a boolean\"\n  assert isinstance(eps, float), \"eps must be a float\"\n\n  # Compute the PDF and CDF from the weights\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n\n  # Compute the bin widths and the total probability mass\n  bin_widths = jnp.diff(t)\n  total_mass = jnp.sum(w * bin_widths)\n\n  # Compute the bin probabilities and the cumulative bin probabilities\n  bin_probs = w * bin_widths / total_mass\n  cum_bin_probs = jnp.cumsum(bin_probs)\n\n  # Compute the bin indices for each sample\n  if rng is None:\n    # Deterministic sampling\n    if deterministic_center:\n      # Sample from the center of each bin\n      bin_indices = jnp.searchsorted(cum_bin_probs, jnp."}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[..., :-1] + t_samples[..., 1:]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples = jnp.concatenate([t_samples[..., :1], t_midpoints, t_samples[..., -1:]], axis=-1)\n\n  # Ensure the first and last intervals are within the specified domain.\n  t_samples = jnp.clip(t_samples, domain[0], domain[1])\n\n  return t_samples\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1.\n  w = jnp.asarray(w)\n  w = w / jnp.sum(w, axis=-1, keepdims=True)\n\n  # Integrate the weights to obtain the cumulative distribution function (CDF).\n  cdf = jnp.cumsum(w, axis=-1)\n\n  # Interpolate the CDF to obtain the weighted percentiles.\n  return linspline.interpolate(t, cdf, ps)\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a probability density function (PDF)\n  p = pdf_to_weight(t, w)\n\n  # Blur the PDF using a Gaussian kernel with the specified half-width\n  p_blurred = linspline.gaussian_kernel_blur(p, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points `tq`\n  w_resampled = resample(tq, t, p_blurred)\n\n  return w_resampled\n\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  return from_homogeneous(matmul(to_homogeneous(vectors), transform))\n\n"}
{"namespace": "stepfun.resample", "completion": "  # Check if the input tensors have the same shape\n  if t.shape != tp.shape:\n    raise ValueError(\"Input tensors must have the same shape.\")\n\n  # Check if the input tensors are sorted\n  if not jnp.all(jnp.diff(tp) >= 0):\n    raise ValueError(\"Input tensors must be sorted.\")\n\n  # Check if the input tensors have the same number of elements\n  if t.size != tp.size:\n    raise ValueError(\"Input tensors must have the same number of elements.\")\n\n  # Check if the input tensors have the same number of elements\n  if t.size != vp.size:\n    raise ValueError(\"Input tensors must have the same number of elements.\")\n\n  # Check if the input tensors have the same number of elements\n  if t.ndim != tp.ndim:\n    raise ValueError(\"Input tensors must have the same number of dimensions.\")\n\n  # Check if the input tensors have the same number of elements\n  if t.ndim != vp.ndim:\n    raise ValueError(\"Input tensors must have the same number of dimensions.\")\n\n  # Check if the input tensors have the same number of elements\n  if t.ndim != 1:\n    raise ValueError(\"Input tensors must have 1 dimension.\")\n\n  # Check if the input tensors have the same number of elements\n  if tp.ndim != 1:\n    raise ValueError(\"Input tensors must have 1 dimension.\")\n\n  # Check if the input tensors have the same number of elements\n  if vp.ndim != 1:\n    raise ValueError(\"Input tensors must have 1 dimension.\")\n\n  # Check if the input tensors have the same number of elements\n  if t.shape != vp.shape:\n    raise ValueError(\"Input tensors must have the same shape.\")\n\n  # Check if the input tensors have the same number of elements\n  if t.shape != tp.shape:\n    raise ValueError(\"Input tensors must have the same shape.\")\n\n  # Check if the input tensors"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance\n  scale = 2**jnp.arange(min_deg, max_deg)\n  mean_scaled = mean * scale\n  var_scaled = var * scale**2\n\n  # Concatenate the scaled mean and variance\n  x = jnp.concatenate([mean_scaled, var_scaled], axis=-1)\n\n  # Apply sinusoidal encoding\n  encoding = jnp.sin(x[..., None] * jnp.arange(1, x.shape[-1] + 1))\n\n  return encoding\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz):\n    \"\"\"Function returning directional encoding.\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(sph_harms), jnp.imag(sph_harms)],"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    result = []\n    blocks = []\n    block_index = 0\n    header_index = 0\n    list_index = 0\n    list_level = 0\n    list_items = []\n    list_item_index = 0\n    list_item_level = 0\n    list_item_start_index = 0\n    list_item_end_index = 0\n    list_item_text = \"\"\n    list_item_text_start_index = 0\n    list_item_text_end_index = 0\n    list_item_text_start_line_index = 0\n    list_item_text_end_line_index = 0\n    list_item_text_start_line_index_offset = 0\n    list_item_text_end_line_index_offset = 0\n    list_item_text_start_line_index_offset_2 = 0\n    list_item_text_end_line_index_offset_2 = 0\n    list_item_text_start_line_index_offset_3 = 0\n    list_item_text_end_line_index_offset_3 = 0\n    list_item_text_start_line_index_offset_4 = 0\n    list_item_text_end_line_index_offset_4 = 0\n    list_item_text_start_line_index_offset_5 = 0\n    list_item_text_end_line_index_offset_5 = 0\n    list_item_text_start_line_index_offset_6 = 0\n    list_item_text_end_line_index_offset_6 = 0\n    list_item_text_start_line_index_offset_7 = 0\n    list_item_text_end_line_index_offset_7 = 0\n    list_item_text_start_line_index_offset_8 = 0\n    list_item_text_end_line_index_offset_8 = 0\n    list_item_text_start_line_index_offset_9 = 0\n    list_item_text_end_line_index_offset_9 = 0\n    list_item_text_start_line"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return org_texts\n\n    # remove quotation marks\n    text = quotation_pattern.sub(\"\", org_texts)\n\n    # remove space between punctuations\n    text = space_rule.sub(r\"\\1\", text)\n\n    # tokenize sentences\n    sentences = nltk_tokenzier.tokenize(text)\n\n    # remove sentences within brackets\n    sentences = [sentence for sentence in sentences if not bracket_rule.search(sentence)]\n\n    # apply rules\n    for rule, replaced in rules:\n        sentences = [rule.sub(replaced, sentence) for sentence in sentences]\n\n    return sentences\n\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if not isinstance(token, str):\n            raise TypeError(\"Expected a string\")\n        try:\n            term_id = self.term_dict.get_term_id(token)\n            if key is None:\n                doc_ids, term_posns = self.posns.term_posns(term_id)\n                return term_posns\n            else:\n                term_posns = self.posns.doc_encoded_posns(term_id, doc_id=key)\n                return [term_posns]\n        except TermMissingError:\n            return []\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Split the spec into parts\n    parts = spec.split()\n\n    # Initialize the minimum number of clauses to match\n    min_should_match = num_clauses\n\n    # Iterate over the parts of the spec\n    for part in parts:\n        # Check if the part is a percentage\n        if part.endswith('%'):\n            # Calculate the minimum number of clauses to match based on the percentage\n            min_should_match = int(num_clauses * float(part[:-1]) / 100)\n        # Check if the part is a conditional expression\n        elif '<' in part:\n            # Split the part into the condition and the value\n            condition, value = part.split('<')\n            # Calculate the minimum number of clauses to match based on the condition\n            if condition == 'min':\n                min_should_match = int(value)\n            elif condition == 'max':\n                min_should_match = num_clauses - int(value)\n        # Otherwise, the part is an absolute number\n        else:\n            # Set the minimum number of clauses to match to the absolute value\n            min_should_match = int(part)\n\n    # Return the minimum number of clauses to match\n    return min_should_match\n\n"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        # If slop is 1 and all tokens are unique, we can directly calculate the phrase frequencies using the positions of terms\n        if slop == 1 and len(set(tokens)) == len(tokens):\n            return compute_phrase_freqs(self, tokens)\n        else:\n            # If slop is not 1 or tokens are not unique, we delegate the calculation to another method that handles different slops or non-unique tokens\n            return self.phrase_freq_with_slop(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        if truncate:\n            array = array[:batch_size]\n\n        term_mat, posns, term_dict, avg_doc_length, doc_lens = build_index_from_terms_list(array, Terms,\n                                                                                           batch_size=batch_size,\n                                                                                           avoid_copies=avoid_copies)\n\n        return cls(term_mat, tokenizer=tokenizer, avoid_copies=avoid_copies)\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(\n            self.config['proxyHost'],\n            self.config['proxyPort'],\n            self.config['serverHost'],\n            self.config['serverPort'],\n            self.config['strategy'],\n            self.config['strategies'],\n            self.config['autoCloseConnections'],\n            self.config['multipleConnections'],\n        )\n        self.server.start()\n\n        self.connections = {}\n        self.lock = threading.Lock()\n\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = arr & mask\n    arr = (arr >> _1) & (s55 + (arr & s55))\n    arr = (arr >> _2) & (s33 + (arr & s33))\n    arr = (arr >> _4) & (s0F + (arr & s0F))\n    arr = (arr >> _8) & (s01 + (arr & s01))\n    return arr\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    if not qf:\n        raise ValueError(\"Must specify at least one field to search\")\n\n    if not q:\n        return np.zeros(len(frame)), \"No query\"\n\n    if q_op not in [\"OR\", \"AND\"]:\n        raise ValueError(\"q_op must be 'OR' or 'AND'\")\n\n    if mm is None:\n        mm = \"1\"\n\n    query_fields = parse_field_boosts(qf)\n    if pf:\n        query_fields.update(parse_field_boosts(pf))\n    if pf2:\n        query_fields.update(parse_field_boosts(pf2))\n    if pf3:\n        query_fields.update(parse_field_boosts(pf3))\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    return qf_scores, explain"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            with self.lock:\n                connection = self.connections.get(process.id, None)\n                if connection is None:\n                    connection = Connection(\n                        process=process,\n                        server=self.server,\n                        auto_close=self.config['autoCloseConnections'],\n                        logger=self.logger\n                    )\n                    self.connections[process.id] = connection\n                message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            with self.lock:\n                connection = self.connections.get(process.id, None)\n                if connection is None:\n                    connection = Connection(\n                        process=process,\n                        server=self.server,\n                        auto_close=self.config['autoCloseConnections'],\n                        logger=self.logger\n                    )\n                    self.connections[process.id] = connection\n                message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            with self.lock:\n                connection = self.connections.get(process.id, None)\n                if connection is not None:\n                    connection.close()\n                    if not self.config['multipleConnections']:\n                        del self.connections[process.id]\n        else:\n            raise Exception(f'Unsupported message type: {type(message)}')\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        self.logger.info('Destroying ProxifierMessageInterceptor')\n        self.logger.info('Closing all connections')\n        with self.lock:\n            for connection in self.connections.values():\n                connection.close()\n        self.logger.info('Stopping server')\n        self.server.stop()\n"}
