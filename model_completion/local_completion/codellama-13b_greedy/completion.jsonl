{"namespace": "chat.utils.run_cmd", "completion": "    print(cmd)\n    return os.system(cmd)\n\n"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "coord.inv_contract", "completion": "  x_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (1 - jnp.sqrt(x_mag_sq)) / (2 * jnp.sqrt(x_mag_sq) - 1)\n  x = scale * z\n  return x\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            conn = sqlite3.connect(filename)\n            cur = conn.cursor()\n            cur.execute(f\"CREATE TABLE IF NOT EXISTS {func_name} (key TEXT PRIMARY KEY, value TEXT)\")\n            key = hashlib.md5(json.dumps({\"args\": args, \"kwargs\": kwargs}).encode(\"utf-8\")).hexdigest()\n            cur.execute(f\"SELECT value FROM {func_name} WHERE key=?\", (key,))\n            result = cur.fetchone()\n            if result:\n                result = json.loads(result[0])\n            else:\n                result = func(*args, **kwargs)\n                cur.execute(f\"INSERT INTO {func_name} (key, value) VALUES (?, ?)\", (key, json.dumps(result)))\n                conn.commit()\n            conn.close()\n            return result\n\n        return wrapper\n\n    return decorator"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    if values[\"x_min\"] >= values[\"x_max\"]:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max. Received x_min: {values['x_min']} and x_max: {values['x_max']}\")\n    if values[\"y_min\"] >= values[\"y_max\"]:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max. Received y_min: {values['y_min']} and y_max: {values['y_max']}\")\n\n    return values\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the norms of each column of each matrix\n  norms0 = np.sum(mat0**2, axis=0)\n  norms1 = np.sum(mat1**2, axis=0)\n\n  # Compute the dot products of each column of each matrix\n  dot_products = np.dot(mat0.T, mat1)\n\n  # Compute the squared distances\n  sq_dist = norms0[:, np.newaxis] + norms1 - 2 * dot_products\n\n  # Ensure that negative distances are set to zero\n  sq_dist[sq_dist < 0] = 0\n\n  return sq_dist\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\":\n        return True\n    if path.startswith(\"gs://\") or path.startswith(\"s3://\") or path.startswith(\"hdfs://\"):\n        return True\n    return False\n\n"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\n                \"You must specify the 'assets_names' parameter when 'items' is a dictionary.\"\n            )\n        if not isinstance(assets_names, np.ndarray):\n            raise TypeError(\n                \"The 'assets_names' parameter must be a numpy array when 'items' is a dictionary.\"\n            )\n        if len(assets_names) != n_assets:\n            raise ValueError(\n                f\"The number of assets in 'assets_names' must be equal to the number of assets in 'items'.\"\n            )\n        if dim == 1:\n            array = np.full(n_assets, fill_value)\n        else:\n            array = np.full((n_assets, n_assets), fill_value)\n        for asset_name, value in items.items():\n            if asset_name not in assets_names:\n                raise ValueError(\n                    f\"The asset '{asset_name}' is not present in the 'assets_names' parameter.\"\n                )\n            if dim == 1:\n                array[assets_names == asset_name] = value\n            else:\n                array[assets_names == asset_name, assets_names == asset_name] = value\n    else:\n        array = np.asarray(items)\n        if dim == 1:\n            if array.ndim != 1:\n                raise ValueError(\n                    f\"The number of dimensions of the '{name}' array must be 1, got {array.ndim}.\"\n                )\n            if array.shape[0] != n_assets:\n                raise ValueError(\n                    f\"The number of assets in the '{name}' array must be equal to the number of assets in 'assets_names'.\"\n                )\n        else:\n            if array.ndim != 2:\n                raise ValueError(\n                    f\"The number of dimensions of the '{name}' array must be 2, got {array.ndim}.\"\n                )\n            if array.shape[0] != array.shape[1]:\n                raise ValueError("}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        agent = MicroAgent(agent_lifecycle, openai_wrapper)\n        agent.dynamic_prompt = data[\"dynamic_prompt\"]\n        agent.purpose = data[\"purpose\"]\n        agent.purpose_embedding = data[\"purpose_embedding\"]\n        agent.depth = data[\"depth\"]\n        agent.max_depth = data[\"max_depth\"]\n        agent.usage_count = data[\"usage_count\"]\n        agent.id = data[\"id\"]\n        agent.parent_id = data[\"parent_id\"]\n        agent.working_agent = data[\"working_agent\"]\n        agent.is_prime = data[\"is_prime\"]\n        agent.evolve_count = data[\"evolve_count\"]\n        agent.number_of_code_executions = data[\"number_of_code_executions\"]\n        agent.last_input = data[\"last_input\"]\n        return agent\n"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  return xnp.where(\n    srgb <= 0.04045,\n    srgb / 12.92,\n    ((srgb + eps) / (1 + eps) ** 2.4) ** 1 / 1.055,\n  )\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Adjust the spline degree to be at most one less than the number of points in x.\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Create a spline object.\n  spline = scipy.interpolate.UnivariateSpline(t_input, x, k=spline_degree, s=smoothness)\n\n  # Interpolate the signal.\n  return spline(t_output)\n\n"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.isupper():\n        return word\n    elif word.islower():\n        return word\n    else:\n        if word[0].isupper() and word[1].isupper():\n            return word.lower()\n        elif word[0].isupper() and word[1].islower():\n            return word.capitalize()\n        elif word[0].islower() and word[1].isupper():\n            return word.upper()\n        else:\n            return word\n\n"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.all(np.isin(v, [True, False])):\n        raise ValueError(f\"{cls.__name__}: {field.name} must be binary (i.e., contain only True or False values).\")\n\n    return v\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  inv_scale = 2 * jnp.sqrt(x_mag_sq) - x_mag_sq\n  x = x / inv_scale\n  return x\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n\n    for column in dict_columns:\n        summary_df[column] = summary_df[column].apply(ast.literal_eval)\n\n    return summary_df\n\n"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    det = jnp.linalg.det(cov)\n    iso_cov = cov / jnp.power(det, 1 / cov.shape[-1])\n  elif mode == 'accurate':\n    log_det = jnp.log(jnp.linalg.det(cov))\n    iso_cov = cov * jnp.exp(-log_det / cov.shape[-1])\n  else:\n    raise ValueError('Invalid mode.')\n\n  return iso_cov\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"XAgent: A general-purpose agent for executing tasks.\")\n    parser.add_argument(\"--task\", type=str, required=True, help=\"The task description, specifying what task should be performed.\")\n    parser.add_argument(\"--upload-files\", type=str, nargs=\"*\", help=\"List of files to upload, allowing multiple files to be specified.\")\n    parser.add_argument(\"--model\", type=str, help=\"Model identifier for the task, specifying which model to use.\")\n    parser.add_argument(\"--record-dir\", type=str, help=\"Directory to record task execution logs, specifying where to save the logs.\")\n    parser.add_argument(\"--mode\", type=str, default=\"auto\", choices=[\"auto\", \"manual\"], help=\"Operational mode, which can be 'auto' or 'manual', specifying how the task should be executed.\")\n    parser.add_argument(\"--quiet\", action=\"store_true\", help=\"If set, the program runs in quiet mode with minimal output.\")\n    parser.add_argument(\"--max-subtask-chain-length\", type=int, help=\"Maximum length of subtask chain, specifying how long a subtask chain can be.\")\n    parser.add_argument(\"--enable-ask-human-for-help\", action=\"store_true\", help=\"Flag to enable asking for human assistance during task execution.\")\n    parser.add_argument(\"--max-plan-refine-chain-length\", type=int, help=\"Maximum length of plan refinement chain, specifying the limit for refining plans.\")\n    parser.add_argument(\"--max-plan-tree-depth\", type=int, help=\"Maximum depth of the plan tree, specifying how deep the plan tree can be.\")\n    parser.add_argument(\"--max-plan-tree-width\", type=int, help=\"Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.\")\n    parser.add_argument(\"--max-retry-times\", type=int, help=\"Maximum number of retry attempts, specifying how many times a task can be retried upon failure.\")\n    parser.add_argument(\""}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[-1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    char_set = string.ascii_lowercase + string.digits + '_'\n\n    # If the integer is within the range of the character set, return the character at that index\n    if n < len(char_set):\n        return char_set[n]\n\n    # If the integer is not within the range of the character set, use recursion to encode the integer\n    else:\n        return encode_int(n // len(char_set)) + char_set[n % len(char_set)]\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  return jnp.log(jnp.where(x > eps, x, jnp.full_like(x, value_at_zero)))\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}\n    for worker_idx in indexes:\n        chunks_index[worker_idx] = 0\n\n    for worker_idx in indexes:\n        while indexes[worker_idx] >= workers_intervals[worker_idx][chunks_index[worker_idx]][1]:\n            indexes[worker_idx] -= workers_intervals[worker_idx][chunks_index[worker_idx]][1]\n            chunks_index[worker_idx] += 1\n\n    return chunks_index, indexes"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return trilerp_grid(values, coordinates)\n  elif datastructure == 'hash':\n    return trilerp_hash(values, coordinates)\n  else:\n    raise ValueError(f'Invalid datastructure {datastructure} passed to trilerp.')\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Generate the integer weights for each vertex of the triangle.\n  weights = np.array(list(itertools.product(range(v + 1), repeat=3)))\n\n  # Normalize the weights to get the barycentric coordinates.\n  weights = weights.astype(float)\n  weights[:, 0] /= v\n  weights[:, 1] /= v\n  weights[:, 2] /= v\n  weights = weights[weights.sum(axis=1) == 1]\n\n  return weights\n\n"}
{"namespace": "linspline.query", "completion": "  # Check that the input is valid.\n  checkify.check(jnp.all(t[Ellipsis, 1:] - t[Ellipsis, :-1] > 0), 'Time points must be strictly increasing.')\n  checkify.check(jnp.all(v[Ellipsis, 0] == 0), 'Splines must all start with 0.')\n  checkify.check(jnp.all(v[Ellipsis, -1] == 0), 'Splines must all end with 0.')\n\n  # Find the index of the last time point before each query point.\n  t_index = jnp.searchsorted(t, tq, side='right') - 1\n\n  # Find the fractional distance from each query point to the last time point.\n  t_frac = (tq - t[t_index]) / (t[t_index + 1] - t[t_index])\n\n  # Interpolate the values at each query point.\n  v_interp = (1 - t_frac) * v[t_index] + t_frac * v[t_index + 1]\n\n  # Set the values outside the original range to 0.\n  v_interp = jnp.where(jnp.logical_and(tq >= t[0], tq <= t[-1]), v_interp, 0)\n\n  return v_interp\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        if not all(x > 0 for x in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must contain only positive values.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Convert the rays to near plane coordinates.\n  origins = xnp.concatenate(\n      [origins, xnp.ones_like(origins[..., :1]) * near], axis=-1)\n\n  # Convert the rays to homogeneous coordinates.\n  origins = xnp.concatenate(\n      [origins, xnp.zeros_like(origins[..., :1])], axis=-1)\n  directions = xnp.concatenate(\n      [directions, xnp.zeros_like(directions[..., :1])], axis=-1)\n\n  # Apply the projection matrix to the rays.\n  origins = xnp.matmul(origins, pixtocam.T)\n  directions = xnp.matmul(directions, pixtocam.T)\n\n  # Normalize the ray directions to unit length.\n  directions = directions / xnp.linalg.norm(\n      directions, axis=-1, keepdims=True)\n\n  # Convert the rays from homogeneous coordinates to NDC.\n  origins = origins[..., :3] / origins[..., 3:]\n  directions = directions[..., :3] / directions[..., 3:]\n\n  return origins, directions\n\n"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors.\n  dir1 = spin_math.normalize(dir1)\n  dir2 = spin_math.normalize(dir2)\n\n  # Compute the dot product of the normalized direction vectors.\n  dot_product = jnp.dot(dir1, dir2)\n\n  # Compare the dot product to the numerical precision.\n  return jnp.abs(dot_product - 1.0) < 1e-6\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the continuation and reference text\n    continuation_tokens = tokenize(continuation)\n    reference_tokens = tokenize(reference)\n\n    # Calculate the BLEU-4 score\n    score = evaluate.bleu_score(\n        continuation_tokens,\n        reference_tokens,\n        n_gram=4,\n        smooth=True\n    )\n\n    # Optionally adjust the score for brevity penalty\n    if with_penalty:\n        score = adjust_for_brevity_penalty(\n            score,\n            continuation_tokens,\n            reference_tokens\n        )\n\n    return score\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.sqrt(jnp.maximum(x, eps))\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Check that the input is valid.\n  utils.assert_valid_stepfun(t, w)\n\n  # Compute the PDF.\n  return w / jnp.diff(t)\n\n"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            try:\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                pass\n    return total_size\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    if isinstance(val, torch.Tensor):\n        if torch.is_floating_point(val):\n            return torch.fmod(val + offset * period, period)\n        else:\n            return torch.fmod(val.float() + offset * period, period).int()\n    elif isinstance(val, np.ndarray):\n        if np.iscomplexobj(val):\n            return np.mod(val + offset * period, period)\n        else:\n            return np.mod(val.astype(np.float32) + offset * period, period)\n    else:\n        raise NotImplementedError\n\n"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent, MicroAgent):\n            return {\n                'dynamic_prompt': agent.dynamic_prompt,\n                'purpose': agent.purpose,\n                'purpose_embedding': agent.purpose_embedding.tolist() if isinstance(agent.purpose_embedding, np.ndarray) else agent.purpose_embedding,\n                'depth': agent.depth,\n                'max_depth': agent.max_depth,\n                'usage_count': agent.usage_count,\n                'id': agent.id,\n                'parent_id': agent.parent_id,\n                'working_agent': agent.working_agent,\n                'is_prime': agent.is_prime,\n                'evolve_count': agent.evolve_count,\n                'number_of_code_executions': agent.number_of_code_executions,\n                'last_input': agent.last_input\n            }\n        else:\n            raise TypeError('Argument is not of type MicroAgent')\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Check if the number of bins is positive\n    if num_bins <= 0:\n        raise ValueError(\"The number of bins must be positive.\")\n\n    # Check if the number of items and weights are equal\n    if len(items) != len(weights):\n        raise ValueError(\"The number of items and weights must be equal.\")\n\n    # Check if the weights are positive\n    if any(weight <= 0 for weight in weights):\n        raise ValueError(\"The weights must be positive.\")\n\n    # Check if the number of items is greater than the number of bins\n    if len(items) < num_bins:\n        raise ValueError(\"The number of items must be greater than the number of bins.\")\n\n    # Sort the items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Initialize the dictionaries\n    bin_to_items = defaultdict(list)\n    bin_to_total_weight = defaultdict(int)\n\n    # Loop through the items\n    for item, weight in sorted_items:\n\n        # Initialize the minimum total weight to infinity\n        min_total_weight = float(\"inf\")\n\n        # Initialize the bin index to -1\n        min_bin_index = -1\n\n        # Loop through the bins\n        for bin_index in range(num_bins):\n\n            # Check if the total weight in the bin is less than the minimum total weight\n            if bin_to_total_weight[bin_index] < min_total_weight:\n\n                # Update the minimum total weight\n                min_total_weight = bin_to_total_weight[bin_index]\n\n                # Update the minimum bin index\n                min_bin_index = bin_index\n\n        # Add the item to the bin with the minimum total weight\n        bin_to_items[min_bin_index].append(item)\n\n        # Update the total weight in the bin with the minimum total weight\n        bin_to_total_weight[min_bin_index] += weight\n\n    return bin_to_items, bin_"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = {\n            \"func_name\": func_name,\n            \"args\": args,\n            \"kwargs\": kwargs,\n        }\n        data_str = json.dumps(data, sort_keys=True)\n        return hashlib.sha256(data_str.encode(\"utf-8\")).hexdigest()\n"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Compute the distances between consecutive points\n    point_distances = np.linalg.norm(polygon[1:] - polygon[:-1], axis=1)\n\n    # Compute the total length of the polygon, excluding the distances that exceed the maximum threshold\n    polygon_length = np.sum(point_distances[point_distances < max_point_distance])\n\n    return polygon_length"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    if len(polygons) == 0:\n        return []\n\n    # Get the largest polygon's area\n    largest_area = max([area(polygon) for polygon in polygons])\n\n    # Filter out polygons based on their area\n    return [\n        polygon\n        for polygon in polygons\n        if area(polygon) > abs_tr or area(polygon) > rel_tr * largest_area\n    ]\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the number of samples that each worker has processed\n    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_remaining = num_samples_yielded % num_workers\n\n    # Calculate the number of batches that each worker has processed\n    num_batches_per_worker = num_samples_per_worker // batch_size\n    num_batches_remaining = num_samples_per_worker % batch_size\n\n    # Calculate the number of samples that each worker has processed\n    num_samples_per_worker = num_batches_per_worker * batch_size\n    num_samples_remaining += num_batches_remaining\n\n    # Distribute the remaining samples\n    num_samples_per_worker += num_samples_remaining // num_workers\n    num_samples_remaining %= num_workers\n\n    # Create a dictionary where each key is a worker index (starting from 0) and its value is the number of samples that worker has processed\n    indexes = {}\n    for i in range(num_workers):\n        indexes[i] = num_samples_per_worker\n\n    # Distribute the remaining samples\n    for i in range(num_samples_remaining):\n        indexes[i] += 1\n\n    return indexes\n\n"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    if len(results) != len(value) or len(results) != len(metadatas):\n        raise ValueError(\"The results, values, and metadata lists must have the same length.\")\n\n    filtered_results = []\n    filtered_metadatas = []\n    for result, val, metadata in zip(results, value, metadatas):\n        if val <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    return filtered_results, filtered_metadatas\n\n"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(f\"The input array does not have the correct shape. The shape of the array must be (_, 2), where _ can be any number of points. The shape of the input array is {array.shape}.\")\n\n    return 0.5 * np.abs(np.dot(array[:, 0], np.roll(array[:, 1], 1)) - np.dot(array[:, 1], np.roll(array[:, 0], 1)))\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Find the index of the first element in a that is greater than the corresponding element in v.\n    idx_hi = torch.searchsorted(a, v, right=True)\n    # Find the index of the last element in a that is less than or equal to the corresponding element in v.\n    idx_lo = torch.searchsorted(a, v, right=False)\n\n    return idx_lo, idx_hi\n\n"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n\n"}
{"namespace": "coord.contract", "completion": "  # Calculate the magnitude squared of the points.\n  x_mag_sq = jnp.sum(jnp.square(x), axis=-1, keepdims=True)\n\n  # Calculate the scaling factor for the points.\n  scale = jnp.sqrt(1 - jnp.sqrt(1 - x_mag_sq))\n\n  # Scale the points towards the origin.\n  return x * scale\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\"]:\n        if abs(num_bytes) < 1000.0:\n            return f\"{num_bytes:3.1f}{unit}B\"\n        num_bytes /= 1000.0\n    return f\"{num_bytes:.1f}YB\"\n\n"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def _is_array_n_dimensions(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be {nb_dimensions} dimensional. Got {len(v.shape)} dimensions.\")\n\n        return v\n\n    return _is_array_n_dimensions\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # Calculate the radius, r, of the point.\n  r = jnp.linalg.norm(cartesian_vector, axis=-1)\n\n  # Calculate the inclination, theta, of the point.\n  theta = jnp.arccos(jnp.clip(cartesian_vector[..., 2] / (r + eps), -1., 1.))\n\n  # Calculate the azimuth, phi, of the point.\n  phi = jnp.arctan2(cartesian_vector[..., 1], cartesian_vector[..., 0])\n\n  return r, theta, phi\n\n"}
{"namespace": "common.rougeL_score", "completion": "    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    return results['rougeL'].mid.fmeasure\n\n"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    # Try to find the object using the standard import mechanism.\n    try:\n        return pydoc.locate(name)\n    except pydoc.ErrorDuringImport:\n        pass\n\n    # Try to find the object as a fully-qualified name.\n    try:\n        return pydoc.locate(f\"{name}.{name}\")\n    except pydoc.ErrorDuringImport:\n        pass\n\n    # Try to find the object as a fully-qualified name, but ignore the first component.\n    parts = name.split(\".\")\n    try:\n        return pydoc.locate(\".\".join(parts[1:]))\n    except pydoc.ErrorDuringImport:\n        pass\n\n    # Try to find the object as a fully-qualified name, but ignore the last component.\n    try:\n        return pydoc.locate(\".\".join(parts[:-1]))\n    except pydoc.ErrorDuringImport:\n        pass\n\n    # Try to find the object as a fully-qualified name, but ignore the middle components.\n    try:\n        return pydoc.locate(\".\".join([parts[0], parts[-1]]))\n    except pydoc.ErrorDuringImport:\n        pass\n\n    # Try to find the object as a fully-qualified name, but ignore the middle components.\n    try:\n        return pydoc.locate(\".\".join([parts[0], parts[-1]]))\n    except pydoc.ErrorDuringImport:\n        pass\n\n    # Try to find the object as a fully-qualified name, but ignore the middle components.\n    try:\n        return pydoc.locate(\".\".join([parts[0], parts[-1]]))\n    except pydoc.ErrorDuringImport:\n        pass\n\n    # Try to find the object as a fully-qualified name, but ignore the middle components.\n    try:\n        return pydoc.locate(\".\".join([parts[0], parts[-1]]))\n    except pydoc.ErrorDuringImport:\n        pass\n\n    # Try to find the object as a fully-qualified name, but ignore the middle components.\n    try"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    return torch.jit.load(buffer)\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check that the ids and scores tuples have the same length\n    if len(ids) != len(scores):\n        raise ValueError(\"The ids and scores tuples must have the same length\")\n\n    # Check that the weights tuple has the same length as the ids and scores tuples\n    if len(weights) != len(ids):\n        raise ValueError(\"The weights tuple must have the same length as the ids and scores tuples\")\n\n    # Check that the weights tuple sums to 1\n    if round(sum(weights), 2) != 1:\n        raise ValueError(\"The weights tuple must sum to 1\")\n\n    # Normalize the scores\n    normalized_scores = []\n    for i in range(len(scores)):\n        normalized_scores.append([score / weights[i] for score in scores[i]])\n\n    # Combine the scores\n    combined_scores = []\n    for i in range(len(normalized_scores[0])):\n        combined_scores.append([sum(score[i] for score in normalized_scores)])\n\n    # Create a dataframe from the ids and scores\n    df = pd.DataFrame({\"id\": ids[0], \"score\": combined_scores[0]})\n\n    # Add the ids and scores from the other retrieval results\n    for i in range(1, len(ids)):\n        df_new = pd.DataFrame({\"id\": ids[i], \"score\": combined_scores[i]})\n        df = pd.concat([df, df_new])\n\n    # Sort the dataframe by score\n    df = df.sort_values(by=[\"score\"], ascending=False)\n\n    # Select the top_k results\n    df = df.head(top_k)\n\n    # Return the ids and scores\n    return df[\"id\"].tolist(), df[\"score\"].tolist()"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return str(x)\n\n    if percent:\n        x = x * 100\n\n    if x == 0:\n        return \"0\"\n    elif abs(x) > 1000000:\n        return f\"{x:.0f}\"\n    elif abs(x) > 1000:\n        return f\"{x:.1f}\"\n    elif abs(x) > 1:\n        return f\"{x:.2f}\"\n    else:\n        return f\"{x:.3g}\"\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while shutil.disk_usage(input_dir).free / 1024**3 > threshold_in_gb:\n        sleep(sleep_time)\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return math.safe_div(p, td)\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\"-\", \" \")\n    line_text = line_text.replace(\"\u2013\", \" \")\n    line_text = line_text.replace(\"\u2014\", \" \")\n    line_text = line_text.replace(\"_\", \" \")\n    line_text = line_text.replace(\"*\", \" \")\n    line_text = line_text.replace(\"\\\\\", \" \")\n    line_text = line_text.replace(\"~\", \" \")\n    line_text = line_text.replace(\"`\", \" \")\n    line_text = line_text.replace(\"|\", \" \")\n    line_text = line_text.replace(\"&\", \" \")\n    line_text = line_text.replace(\"^\", \" \")\n    line_text = line_text.replace(\"=\", \" \")\n    line_text = line_text.replace(\"+\", \" \")\n    line_text = line_text.replace(\"(\", \" \")\n    line_text = line_text.replace(\")\", \" \")\n    line_text = line_text.replace(\"{\", \" \")\n    line_text = line_text.replace(\"}\", \" \")\n    line_text = line_text.replace(\"[\", \" \")\n    line_text = line_text.replace(\"]\", \" \")\n    line_text = line_text.replace(\"<\", \" \")\n    line_text = line_text.replace(\">\", \" \")\n    line_text = line_text.replace(\";\", \" \")\n    line_text = line_text.replace(\":\", \" \")\n    line_text = line_text.replace(\"!\", \" \")\n    line_text = line_text.replace(\"?\", \" \")\n    line_text = line_text.replace(\",\", \" \")\n    line_text = line_text.replace(\".\", \" \")\n    line_text = line_text.replace(\"/\", \" \")\n    line_text = line_text.replace(\"\\\\\", \" \")\n    line_text = line_text.replace(\"$\", \" \")\n    line_text = line_text.replace(\"@\", \" \")\n    line_text = line_text.replace(\"#"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros > n:\n        raise ValueError(\"The number of zero weights cannot exceed the total number of weights.\")\n\n    weights = np.random.uniform(0, 1, n)\n    weights = weights / weights.sum()\n\n    if zeros > 0:\n        zero_indices = np.random.choice(n, zeros, replace=False)\n        weights[zero_indices] = 0\n\n    return weights\n\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.pop('module_type')\n        return cls(module_type, module_dict)\n"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    crop_size = np.asarray(crop_size, dtype=np.int32)\n    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    center_yx = (bbox[1] + bbox[3]) * 0.5, (bbox[0] + bbox[2]) * 0.5\n\n    assert (\n        image_size[0] > crop_size[0]\n    ), \"Can only crop image if the image height is larger than crop height\"\n    assert (\n        image_size[1] > crop_size[1]\n    ), \"Can only crop image if the image width is larger than crop width\"\n\n    min_yx = np.maximum(np.floor(center_yx).astype(np.int32) - crop_size, 0)\n    max_yx = np.maximum(np.asarray(image_size, dtype=np.int32) - crop_size, 0)\n    max_yx = np.minimum(max_yx, np.ceil(center_yx).astype(np.int32))\n\n    y0 = np.random.randint(min_yx[0], max_yx[0] + 1)\n    x0 = np.random.randint(min_yx[1], max_yx[1] + 1)\n    return T.CropTransform(x0, y0, crop_size[1], crop_size[0])\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  return x * jnp.reciprocal(jnp.maximum(jnp.linalg.norm(x, axis=-1, keepdims=True), grad_eps))\n\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name = agent_info.split(\":\")[0]\n        input_text = \"\"\n        if len(agent_info.split(\":\")) > 1:\n            input_text = agent_info.split(\":\")[1]\n        return agent_name, input_text\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    boxes = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n    target = Instances(image_size)\n    target.gt_boxes = Boxes(boxes)\n\n    classes = [obj[\"category_id\"] for obj in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if len(annos) and \"segmentation\" in annos[0]:\n        segms = [obj[\"segmentation\"] for obj in annos]\n        if mask_format == \"polygon\":\n            masks = PolygonMasks(segms)\n        else:\n            assert mask_format == \"bitmask\", mask_format\n            masks = []\n            for segm in segms:\n                if isinstance(segm, list):\n                    # polygon\n                    masks.append(polygons_to_bitmask(segm, *image_size))\n                elif isinstance(segm, dict):\n                    # COCO RLE\n                    masks.append(mask_util.decode(segm))\n                elif isinstance(segm, np.ndarray):\n                    assert segm.ndim == 2, \"Expect segmentation of 2 dimensions, got {}.\".format(\n                        segm.ndim\n                    )\n                    # mask array\n                    masks.append(segm)\n                else:\n                    raise ValueError(\n                        \"Cannot convert segmentation of type '{}' to BitMasks!\"\n                        \"Supported types are: polygons as list[list[float] or ndarray],\"\n                        \" COCO-style RLE as a dict, or a full-image segmentation mask \"\n                        \"as a 2D ndarray.\".format(type(segm))\n                    )\n            # torch.from_numpy does not support array with negative stride.\n            masks = BitMasks(\n                torch.stack([torch.from_numpy(np.as"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", str(Path.home() / \"skfolio_data\"))\n    data_home = Path(data_home).expanduser()\n    data_home.mkdir(parents=True, exist_ok=True)\n    return str(data_home)\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    # Check if the input is a 2D array\n    if cov.ndim != 2:\n        raise ValueError(\"The input array must be a 2D array\")\n\n    # Calculate the standard deviation for each variable\n    std = np.sqrt(np.diag(cov))\n\n    # Convert the covariance matrix into a correlation matrix\n    corr = cov / np.outer(std, std)\n\n    return corr, std\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    # Save the original state of the model\n    training_state = {}\n    for name, module in model.named_modules():\n        training_state[name] = module.training\n\n    # Temporarily modify the model's class definitions to set the \"training\" attribute to a constant value\n    for name, module in model.named_modules():\n        module.__class__.__getattr__ = lambda x, y: training_state[name] if y == \"training\" else getattr(x, y)\n\n    # Yield to the context manager\n    yield\n\n    # Revert the model's class definitions to their original state\n    for name, module in model.named_modules():\n        module.__class__.__getattr__ = lambda x, y: getattr(x, y)\n\n"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[Any]]) -> Dict[str, List[Any]]:\n        \"\"\"Check if the shape of field1 equals the shape of field2.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if not isinstance(metrics, list):\n        raise TypeError(\"The metrics argument must be a list.\")\n\n    if not all(isinstance(metric, (str, dict)) for metric in metrics):\n        raise TypeError(\"The metrics argument must be a list of strings or dictionaries.\")\n\n    if all(isinstance(metric, str) for metric in metrics):\n        return metrics, [{} for _ in range(len(metrics))]\n\n    if all(isinstance(metric, dict) for metric in metrics):\n        metric_names = [metric[\"name\"] for metric in metrics]\n        metric_params = [metric[\"params\"] for metric in metrics]\n        return metric_names, metric_params\n\n    raise TypeError(\"The metrics argument must be a list of strings or dictionaries.\")\n\n"}
{"namespace": "coord.construct_ray_warps", "completion": "  if fn_inv is None:\n    fn_inv = {\n        jnp.sinh: jnp.arcsinh,\n        jnp.tanh: jnp.arctanh,\n        jnp.sin: jnp.arcsin,\n        jnp.tan: jnp.arctan,\n        jnp.arcsinh: jnp.sinh,\n        jnp.arctanh: jnp.tanh,\n        jnp.arcsin: jnp.sin,\n        jnp.arctan: jnp.tan,\n    }.get(fn)\n    if fn_inv is None:\n      raise ValueError(f'No inverse found for function {fn}.')\n\n  def t_to_s(t):\n    \"\"\"Maps metric distances to normalized distances in the range [0, 1].\"\"\"\n    t = jnp.clip(t, t_near, t_far)\n    return (fn(t) + 1) / 2\n\n  def s_to_t(s):\n    \"\"\"Maps normalized distances to metric distances.\"\"\"\n    return fn_inv(2 * s - 1)\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n\n  return jnp.array([x, y, z])\n\n"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  return jnp.trapz(w, t)\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Check that the number of IDs, scores, and weights are the same\n    assert len(ids) == len(scores) == len(weights), \"The number of IDs, scores, and weights must be the same.\"\n\n    # Check that the weights sum to 1\n    assert round(sum(weights), 2) == 1, \"The weights must sum to 1.\"\n\n    # Check that the top_k is greater than 0\n    assert top_k > 0, \"top_k must be greater than 0.\"\n\n    # Check that the number of IDs and scores are greater than 0\n    assert len(ids) > 0, \"There must be more than 0 IDs.\"\n    assert len(scores) > 0, \"There must be more than 0 scores.\"\n\n    # Check that the number of IDs and scores are the same\n    assert len(ids[0]) == len(scores[0]), \"The number of IDs and scores must be the same.\"\n\n    # Check that the number of IDs and weights are the same\n    assert len(ids) == len(weights), \"The number of IDs and weights must be the same.\"\n\n    # Check that the number of scores and weights are the same\n    assert len(scores) == len(weights), \"The number of scores and weights must be the same.\"\n\n    # Check that the IDs are strings\n    for id_list in ids:\n        for id in id_list:\n            assert isinstance(id, str), \"The IDs must be strings.\"\n\n    # Check that the scores are floats\n    for score_list in scores:\n        for score in score_list:\n            assert isinstance(score, float), \"The scores must be floats.\"\n\n    # Check that the weights are floats\n    for weight in weights:\n        assert isinstance(weight, float), \"The weights must be floats.\"\n\n    # Check that the top_k is an integer\n    assert isinstance(top_k, int), \"top_k must be an integer.\"\n\n    # Calculate the weighted sum of scores for each ID\n    weighted_sums = []\n    for"}
{"namespace": "coord.track_linearize", "completion": "  # Compute the Jacobian of the function at the mean\n  jacobian = jax.jacfwd(fn)(mean)\n\n  # Compute the covariance of the function at the mean\n  cov_mean = jnp.einsum('...ij,...ik->...jk', jacobian, cov)\n  cov_mean = jnp.einsum('...ij,...ik->...jk', cov_mean, jacobian)\n\n  # Apply the function to the mean\n  fn_mean = fn(mean)\n\n  # Compute the covariance of the function\n  fn_cov = cov + cov_mean\n\n  return fn_mean, fn_cov\n\n"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for i in x:\n        if len(i) > 1:\n            yield [i[: len(i) // 2], i[len(i) // 2 :]]\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix is not square.\")\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  encoded_x = jnp.concatenate([jnp.sin(scaled_x), jnp.cos(scaled_x)], axis=-1)\n  if append_identity:\n    encoded_x = jnp.concatenate([x, encoded_x], axis=-1)\n  return encoded_x\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Check if field1.shape equals field2.shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                             f\"resp. {len(values[field1])} and {len(values[field2])}\")\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch at index {i}.\")\n        return values\n\n    return __root_validator\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "        # Resize the rendering context\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the mesh\n        with eglctx.makeCurrent(), self.render_context():\n            # Set the camera settings\n            gl.glViewport(0, 0, camera.width, camera.height)\n            gl.glClearColor(*camera.background_color)\n            gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)\n            gl.glEnable(gl.GL_DEPTH_TEST)\n            gl.glDepthFunc(gl.GL_LEQUAL)\n            gl.glDepthRange(0.0, 1.0)\n            gl.glEnable(gl.GL_CULL_FACE)\n            gl.glCullFace(gl.GL_BACK)\n            gl.glFrontFace(gl.GL_CCW)\n            gl.glEnable(gl.GL_BLEND)\n            gl.glBlendFunc(gl.GL_SRC_ALPHA, gl.GL_ONE_MINUS_SRC_ALPHA)\n            gl.glDisable(gl.GL_BLEND)\n\n            # Set the projection and view matrices\n            gl.glUniformMatrix4fv(self.uniforms.proj, 1, gl.GL_FALSE, camera.proj.numpy())\n            gl.glUniformMatrix4fv(self.uniforms.view, 1, gl.GL_FALSE, camera.view.numpy())\n\n            # Set the lighting\n            gl.glUniform3fv(self.uniforms.light_dir, 1, camera.light_dir.numpy())\n            gl.glUniform3fv(self.uniforms.light_color, 1, camera.light_color.numpy())\n\n            # Set the material\n            gl.glUniform3fv(self.uniforms.material_color, 1, self.material_color.numpy())\n            gl.glUniform1f(self.uniforms.material_shininess, self.material_"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a new dictionary with the same keys as the original configuration\n    config_dict = OrderedDict()\n    for key in bert_config.keys():\n        config_dict[key] = bert_config[key]\n\n    # Add new attributes to the dictionary\n    config_dict[\"num_items\"] = 100\n    config_dict[\"num_resources\"] = 10\n    config_dict[\"num_resource_categories\"] = 10\n    config_dict[\"num_event_categories\"] = 10\n    config_dict[\"num_players\"] = 10\n    config_dict[\"num_event_categories\"] = 10\n    config_dict[\"num_event_responses\"] = 10\n    config_dict[\"num_event_effects\"] = 10\n    config_dict[\"num_event_objs\"] = 10\n    config_dict[\"num_event_subjects\"] = 10\n    config_dict[\"num_event_objs_types\"] = 10\n    config_dict[\"num_event_subjects_types\"] = 10\n    config_dict[\"num_event_objs_categories\"] = 10\n    config_dict[\"num_event_subjects_categories\"] = 10\n    config_dict[\"num_event_objs_states\"] = 10\n    config_dict[\"num_event_subjects_states\"] = 10\n    config_dict[\"num_event_objs_states_types\"] = 10\n    config_dict[\"num_event_subjects_states_types\"] = 10\n    config_dict[\"num_event_objs_states_categories\"] = 10\n    config_dict[\"num_event_subjects_states_categories\"] = 10\n    config_dict[\"num_event_objs_states_resources\"] = 10\n    config_dict[\"num_event_subjects_states_resources\"] = 10\n    config_dict[\"num_event_objs_states_resources_types\"] = 10\n    config_dict[\"num_event_subjects_"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        # Select the shader program\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glUseProgram(self.point_program)\n        else:\n            gl.glUseProgram(self.mesh_program)\n\n        # Upload uniforms\n        self.upload_gl_uniforms(camera)\n\n        # Bind vertex array object\n        gl.glBindVertexArray(self.vao)\n\n        # Draw\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.LINES:\n            if self.faces.dtype == torch.int32:\n                gl.glDrawElements(gl.GL_LINES, len(self.faces), gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n            else:\n                gl.glDrawElements(gl.GL_LINES, len(self.faces), gl.GL_UNSIGNED_INT, self.faces.data_ptr())\n        elif self.render_type == Mesh.RenderType.TRIS:\n            if self.faces.dtype == torch.int32:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces), gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n            else:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces), gl.GL_UNSIGNED_INT, self.faces.data_ptr())\n        elif self.render_type == Mesh.RenderType.QUADS:\n            if self.faces.dtype == torch.int32:\n                gl.glDrawElements(gl.GL_QUADS, len(self.faces), gl.GL_UNSIGNED_INT, ctypes.c_void_p(0))\n            else:"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "        if not self.use_quad_cuda:\n            self.upload_to_texture(ptr)\n            return\n\n        if not hasattr(self, 'cu_tex'):\n            self.init_texture()\n\n        # assert self.use_quad_cuda, \"Need to enable cuda-opengl interop to copy from device to device, check creation of this Quad\"\n        w = w or self.W\n        h = h or self.H\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        from cuda import cudart\n        kind = cudart.cudaMemcpyKind.cudaMemcpyHostToDevice\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsMapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n        cu_tex_arr = CHECK_CUDART_ERROR(cudart.cudaGraphicsSubResourceGetMappedArray(self.cu_tex, 0, 0))\n\n        CHECK_CUDART_ERROR(cudart.cudaMemcpy2DToArrayAsync(cu_tex_arr,\n                                                           x * 4 * ptr.itemsize,\n                                                           y,\n                                                           ptr.ctypes.data,\n                                                           w * 4 * ptr.itemsize,  # differently sized\n                                                           w * 4 * ptr.itemsize,  # rgba, should do a composition first\n                                                           h,\n                                                           kind,\n                                                           torch.cuda.current_stream().cuda_stream))\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsUnmapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Validate the inputs\n    if not isinstance(R, torch.Tensor):\n        raise TypeError(f\"Expected input R to be of type torch.Tensor. Got {type(R)}\")\n    if not isinstance(tvec, torch.Tensor):\n        raise TypeError(f\"Expected input tvec to be of type torch.Tensor. Got {type(tvec)}\")\n    if not isinstance(camera_matrix, torch.Tensor):\n        raise TypeError(f\"Expected input camera_matrix to be of type torch.Tensor. Got {type(camera_matrix)}\")\n    if not isinstance(image_size, torch.Tensor):\n        raise TypeError(f\"Expected input image_size to be of type torch.Tensor. Got {type(image_size)}\")\n    if not isinstance(znear, float):\n        raise TypeError(f\"Expected input znear to be of type float. Got {type(znear)}\")\n\n    # Ensure the inputs are batched\n    R = torch.unsqueeze(R, 0) if R.ndimension() == 2 else R\n    tvec = torch.unsqueeze(tvec, 0) if tvec.ndimension() == 1 else tvec\n    camera_matrix = torch.unsqueeze(camera_matrix, 0) if camera_matrix.ndimension() == 2 else camera_matrix\n    image_size = torch.unsqueeze(image_size, 0) if image_size.ndimension() == 1 else image_size\n\n    # Validate the inputs\n    if R.ndimension() != 3:\n        raise ValueError(f\"Expected input R to have 3 dimensions. Got {R.ndimension()}.\")\n    if tvec.ndimension() != 2:\n        raise ValueError(f\"Expected input tvec to have 2 dimensions. Got {tvec.ndimension()}.\")\n    if camera_matrix.ndimension() != 3:\n        raise ValueError(f\"Expected input camera"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            self.draw_blit(x, y, w, h)\n            return\n\n        w = w or self.W\n        h = h or self.H\n\n        old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        old_scissor = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        gl.glUseProgram(self.quad_program)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n        gl.glBindVertexArray(0)\n\n        gl.glUseProgram(0)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, 0)\n\n        gl.glViewport(*old_viewport)\n        gl.glScissor(*old_scissor)\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    # Adjust R and T to match PyTorch3D's coordinate system.\n    R = R.mT\n    T = -R @ C\n    # Recalculate K for NDC.\n    K = K.clone()\n    K[:, :2, :] = K[:, :2, :] / W * 2.0\n    K[:, 2, :] = K[:, 2, :] - 1.0\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        w = w or self.W\n        h = h or self.H\n        _, _, W, H = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)  # only render in this small region of the viewport\n\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBindFramebuffer(gl.GL_DRAW_FRAMEBUFFER, 0)\n        gl.glBlitFramebuffer(0, 0, w, h, 0, 0, w, h, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, 0)\n\n        # Some house keepings\n        gl.glViewport(0, 0, W, H)\n        gl.glScissor(0, 0, W, H)\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Compute the inner measure\n    t0_ = t0.unsqueeze(-1)\n    t1_ = t1.unsqueeze(-1)\n    y1_ = y1.unsqueeze(-1)\n    t1_ = t1_.expand(-1, t0.size(-1))\n    y1_ = y1_.expand(-1, t0.size(-1))\n    t1_ = t1_.transpose(0, 1)\n    y1_ = y1_.transpose(0, 1)\n    t1_ = t1_.expand(-1, t0.size(-1))\n    y1_ = y1_.expand(-1, t0.size(-1))\n    t1_ = t1_.transpose(0, 1)\n    y1_ = y1_.transpose(0, 1)\n    t1_ = t1_.expand(-1, t0.size(-1))\n    y1_ = y1_.expand(-1, t0.size(-1))\n    t1_ = t1_.transpose(0, 2)\n    y1_ = y1_.transpose(0, 2)\n    t1_ = t1_.expand(-1, t0.size(-1))\n    y1_ = y1_.expand(-1, t0.size(-1))\n    t1_ = t1_.transpose(0, 2)\n    y1_ = y1_.transpose(0, 2)\n    t1_ = t1_.expand(-1, t0.size(-1))\n    y1_ = y1_.expand(-1, t0.size(-1))\n    t1_ = t1_.transpose(0, 2)\n    y1_ = y1_.transpose(0, 2)\n    t1_ = t1_.expand(-1, t0.size(-1))\n    y1_ = y1_.expand(-1, t0.size(-1))\n    t1_ = t1_.transpose(0, 2)\n    y1_ = y1_.transpose(0, 2"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # calculate the inner and outer weights\n    w_inner, w_outer = inner_outer(t, t_env, w_env)\n\n    # calculate the loss\n    loss = (w * w_inner + w_outer * w_outer / (2 * w + eps)).sum()\n\n    return loss\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # Calculate the inter-interval and intra-interval losses\n    inter_loss = lossfun_outer(t, w, t, w)\n    intra_loss = lossfun_zip_outer(t, w, t, w, 0.01)\n\n    # Combine the inter-interval and intra-interval losses\n    return inter_loss + intra_loss\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    return interpolate(torch.tensor(ps, device=t.device), cw, t)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Check that the input tensors have the same shape.\n    if t.shape != w.shape:\n        raise ValueError(\"t and w must have the same shape.\")\n\n    # Check that the input tensors are one-dimensional.\n    if t.ndim != 1:\n        raise ValueError(\"t and w must be one-dimensional.\")\n\n    # Check that the input tensors are sorted.\n    if not torch.all(t[1:] > t[:-1]):\n        raise ValueError(\"t must be sorted.\")\n\n    # Check that the input tensors are non-negative.\n    if torch.any(t < 0) or torch.any(w < 0):\n        raise ValueError(\"t and w must be non-negative.\")\n\n    # Check that the number of samples is non-negative.\n    if num_samples < 0:\n        raise ValueError(\"num_samples must be non-negative.\")\n\n    # Check that the perturbation flag is a boolean.\n    if not isinstance(perturb, bool):\n        raise ValueError(\"perturb must be a boolean.\")\n\n    # Check that the single jitter flag is a boolean.\n    if not isinstance(single_jitter, bool):\n        raise ValueError(\"single_jitter must be a boolean.\")\n\n    # Compute the CDF.\n    cw = integrate_weights(w)\n\n    # Generate samples.\n    u = torch.rand(num_samples, *t.shape, device=t.device)\n    t_new = interpolate(u, cw, t)\n\n    # Apply perturbation.\n    if perturb:\n        # Compute the perturbation.\n        eps = torch.rand(num_samples, *t.shape, device=t.device)\n        if single_jitter:\n            eps = eps - 0.5\n        else:\n            eps = eps - 0.5 + torch.rand(num_samples, device=t.device)\n        # Apply the perturbation.\n        t_new = t_new + eps * (t[1:]"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Ensure that the time steps are non-negative.\n    assert (t >= 0).all()\n\n    # Ensure that the weights are non-negative.\n    assert (w >= 0).all()\n\n    # Ensure that the dilation is positive.\n    assert dilation > 0\n\n    # Ensure that the domain is valid.\n    assert len(domain) == 2\n    assert domain[0] < domain[1]\n\n    # Ensure that the time steps and weights are the same size.\n    assert t.shape[-1] == w.shape[-1]\n\n    # Ensure that the time steps and weights are the same size.\n    assert t.shape[-1] == w.shape[-1]\n\n    # Ensure that the time steps and weights are the same size.\n    assert t.shape[-1] == w.shape[-1]\n\n    # Ensure that the time steps and weights are the same size.\n    assert t.shape[-1] == w.shape[-1]\n\n    # Ensure that the time steps and weights are the same size.\n    assert t.shape[-1] == w.shape[-1]\n\n    # Ensure that the time steps and weights are the same size.\n    assert t.shape[-1] == w.shape[-1]\n\n    # Ensure that the time steps and weights are the same size.\n    assert t.shape[-1] == w.shape[-1]\n\n    # Ensure that the time steps and weights are the same size.\n    assert t.shape[-1] == w.shape[-1]\n\n    # Ensure that the time steps and weights are the same size.\n    assert t.shape[-1] == w.shape[-1]\n\n    # Ensure that the time steps and weights are the same size.\n    assert t.shape[-1] == w.shape[-1]\n\n    # Ensure that the time steps and weights are the same size.\n    assert t.shape[-1] == w.shape[-1]\n\n    # Ensure that the time steps and weights are the same size.\n    assert t.shape[-1] == w.shape[-"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Check that the query times are within the range of the step function.\n    if torch.any(tq < t[..., 0]) or torch.any(tq > t[..., -1]):\n        raise ValueError('Query times outside the range of the step function.')\n\n    # Check that the query times are sorted.\n    if not torch.all(tq[..., 1:] > tq[..., :-1]):\n        raise ValueError('Query times must be sorted.')\n\n    # Check that the step function times are sorted.\n    if not torch.all(t[..., 1:] > t[..., :-1]):\n        raise ValueError('Step function times must be sorted.')\n\n    # Check that the query and step function times have the same shape.\n    if tq.shape[-1] != t.shape[-1]:\n        raise ValueError('Query and step function times must have the same shape.')\n\n    # Check that the query and step function values have the same shape.\n    if y.shape[-1] != t.shape[-1]:\n        raise ValueError('Query and step function values must have the same shape.')\n\n    # Check that the outside value is a numeric value.\n    if not isinstance(outside_value, (int, float)):\n        raise ValueError('Outside value must be a numeric value.')\n\n    # Find the query time indices.\n    tq_indices = torch.searchsorted(t, tq)\n\n    # Find the query time indices that exactly match a step change time.\n    tq_exact_indices = torch.where(tq == t[..., :-1])[1]\n\n    # Find the query time indices that fall between two step change times.\n    tq_interp_indices = torch.where(tq_indices > tq_exact_indices)\n\n    # Find the query time indices that exactly match a step change time.\n    tq_exact_indices = torch.where(tq_indices < tq_exact_indices)\n\n    # Initialize the query values.\n    y"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # Calculate the weights adjustment\n    w_adjust = (1.0 - train_frac) ** anneal_slope\n\n    # Calculate the weights adjustment for each interval\n    w_adjust = w_adjust[..., None] * (t[..., 1:] - t[..., :-1])\n\n    # Set the weights adjustment for intervals with zero distance to zero\n    w_adjust = torch.where(w_adjust == 0, torch.zeros_like(w_adjust), w_adjust)\n\n    # Calculate the adjusted weights\n    w_adjusted = w * w_adjust\n\n    # Normalize the adjusted weights\n    w_adjusted = w_adjusted / (torch.sum(w_adjusted, dim=-1, keepdim=True) + eps)\n\n    return w_adjusted\n\n"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, list):\n        return [to_cuda(b, device, ignore_list) for b in batch]\n    elif isinstance(batch, tuple):\n        return tuple([to_cuda(b, device, ignore_list) for b in batch])\n    elif isinstance(batch, dict):\n        if ignore_list:\n            return {k: to_cuda(b, device, ignore_list) for k, b in batch.items() if k != \"meta\"}\n        else:\n            return {k: to_cuda(b, device, ignore_list) for k, b in batch.items()}\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # check if the batch dimension of the vertices tensor matches the batch dimension of the faces tensor\n    if v.shape[0] != f.shape[0]:\n        # if not, expand the faces tensor to match the vertices tensor\n        f = f.expand(v.shape[0], *f.shape[1:])\n\n    # gather the vertices of the triangles\n    tri = multi_gather(v, f, dim)\n\n    # compute the normals of the triangles\n    tri_normals = torch.cross(tri[..., 1] - tri[..., 0], tri[..., 2] - tri[..., 0], dim=-1)\n\n    # reshape the result to maintain the structure of the original faces tensor with additional dimensions for batch processing\n    tri = tri.reshape(*f.shape[:-1], 3, *v.shape[1:])\n    tri_normals = tri_normals.reshape(*f.shape[:-1], 3, *v.shape[1:])\n\n    return tri, tri_normals\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, (torch.Tensor, np.ndarray)):  # numpy and others\n        batch = batch[None]\n    else:\n        batch = torch.as_tensor(batch)[None]\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        # Convert to batch\n        batch = dotdict()\n        batch.meta = dotdict()\n\n        # Camera parameters\n        batch.meta.H = self.H\n        batch.meta.W = self.W\n        batch.meta.K = self.K\n        batch.meta.R = self.R\n        batch.meta.T = self.T\n        batch.meta.n = self.n\n        batch.meta.f = self.f\n        batch.meta.t = self.t\n        batch.meta.v = self.v\n        batch.meta.bounds = self.bounds\n\n        # GUI related elements\n        batch.meta.origin = self.origin\n        batch.meta.world_up = self.world_up\n        batch.meta.movement_speed = self.movement_speed\n        batch.meta.movement_force = self.movement_force\n        batch.meta.drag_coeff_mult = self.drag_coeff_mult\n        batch.meta.constant_drag = self.constant_drag\n        batch.meta.mass = self.mass\n        batch.meta.moment_of_inertia = self.moment_of_inertia\n        batch.meta.movement_torque = self.movement_torque\n        batch.meta.angular_friction = self.angular_friction\n        batch.meta.constant_torque = self.constant_torque\n        batch.meta.min_interval = self.min_interval\n        batch.meta.pause_physics = self.pause_physics\n\n        # Convert to tensors\n        batch.H = torch.tensor(batch.meta.H, dtype=torch.float32)\n        batch.W = torch.tensor(batch.meta.W, dtype=torch.float32)\n        batch.K = torch.tensor(batch.meta.K, dtype=torch.float32)\n        batch.R = torch.tensor(batch.meta.R, dtype=torch.float32)\n        batch.T = torch.tensor(batch.meta.T, dtype=torch.float32"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent and not agent.is_prime_agent:\n            serialized_agent = AgentSerializer(agent)\n            self.persistence.save_agent(serialized_agent)\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if len(self.agents) == 0:\n                return None, -np.inf\n\n            similarities = [cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0] for agent in self.agents]\n            if len(similarities) == 0:\n                return None, -np.inf\n\n            max_similarity = max(similarities)\n            if max_similarity < 0.999:\n                return None, -np.inf\n\n            max_similarity_index = similarities.index(max_similarity)\n            return self.agents[max_similarity_index], max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            return None, -np.inf\n\n"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(\n            prompt=PRIME_PROMPT,\n            name=PRIME_NAME,\n            weight=PRIME_AGENT_WEIGHT,\n            prime=True,\n            prime_only=True\n        )\n        self.agents.append(prime_agent)\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    @memoize_to_sqlite\n    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent is not None:\n            return AgentSerializer.deserialize(\n                serialized_agent, agent_lifecycle, openai_wrapper\n            )\n\n        return None"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "    @memoize_to_sqlite\n    def load_all_agents(self, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads all agents from the database and returns a list of these agents if they are successfully loaded. Each agent is loaded based on its purpose, utilizing the provided agent lifecycle and OpenAI wrapper for the loading process.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param agent_lifecycle: The lifecycle manager for agents, used to manage the state and transitions of an agent throughout its lifecycle.\n        :param openai_wrapper: An interface or wrapper for OpenAI functionalities, used to interact with OpenAI services or models in the process of loading an agent.\n        :return: list. A list of agents that have been successfully loaded from the database.\n        \"\"\"\n        serialized_agents = self.persistence.fetch_all_agents()\n        return [\n            AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            for serialized_agent in serialized_agents\n        ]"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.error(f\"Error saving agent: {e}\")\n            raise e\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = PROMPT_ENGINEERING_SYSTEM_PROMPT.format(goal=goal, sample_input=sample_input)\n            prompt = self.openai_wrapper.get_chat_completion(prompt)\n            prompt = PROMPT_ENGINEERING_TEMPLATE.format(prompt=prompt)\n            return prompt\n        except Exception as e:\n            logger.exception(f\"Error in generating prompt: {e}\")\n            return \"\"\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"\"\"\n                INSERT OR REPLACE INTO agents (id, purpose, data)\n                VALUES (:id, :purpose, :data)\n            \"\"\", agent_dict)\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,))\n            row = cursor.fetchone()\n            if row is None:\n                return None\n            return json.loads(row[0])\n"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            return [row[0] for row in cursor.fetchall()]\n"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash=?\", (arg_hash,))\n        result = cursor.fetchone()\n        if result is None:\n            return None\n        return json.loads(result[0])\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update_config(args)\n\n    # If quiet mode is enabled, redirect the standard output to a file\n    if quiet_mode:\n        with open(os.path.join(CONFIG.record_dir, 'output.txt'), 'w') as f:\n            with redirect_stdout(f):\n                CommandLine(args).execute()\n    else:\n        CommandLine(args).execute()\n\n"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.Completion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        if self._client is None or (time() - self._last_time) > self._refetch_interval:\n            self._create_client()\n            self._last_time = time()\n        return self._client"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\n                \"The `state_dict` method should be called from the main process, not from the DataLoader workers.\"\n            )\n\n        state: Dict[str, Any] = {}\n        state[\"num_samples_yielded\"] = num_samples_yielded\n        state[\"num_workers\"] = num_workers\n        state[\"batch_size\"] = batch_size\n        state[\"current_epoch\"] = self.current_epoch\n        state[\"input_dir\"] = self.input_dir\n        state[\"item_loader_state\"] = self.item_loader.state_dict() if self.item_loader is not None else None\n        state[\"drop_last\"] = self.drop_last\n        state[\"seed\"] = self.seed\n        state[\"world_size\"] = self.distributed_env.world_size\n        state[\"shuffle\"] = self.shuffle\n\n        return state\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        self._state_dict = state_dict\n        self._validate_state_dict()\n\n        # Load the state\n        self.current_epoch = state_dict[\"current_epoch\"]\n        self.input_dir.path = state_dict[\"input_dir_path\"]\n        self.input_dir.url = state_dict[\"input_dir_url\"]\n        self.item_loader = state_dict[\"item_loader\"]\n        self.drop_last = state_dict[\"drop_last\"]\n        self.seed = state_dict[\"seed\"]\n        self.distributed_env.world_size = state_dict[\"world_size\"]\n        self.shuffle = state_dict[\"shuffle\"]\n"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        state: Dict[str, Any] = self._state_dict\n\n        if self.worker_env.world_size != state[\"world_size\"]:\n            raise ValueError(\n                f\"The world size of the current instance is {self.worker_env.world_size} but the world size of the state dict is {state['world_size']}.\"\n            )\n\n        if self.cache is None:\n            raise ValueError(\"The cache of the current instance is None but the state dict is not None.\")\n\n        if self.cache._get_cache_path() != state[\"input_dir_path\"]:\n            raise ValueError(\n                f\"The input directory path of the current instance is {self.cache._get_cache_path()} but the input directory path of the state dict is {state['input_dir_path']}.\"\n            )\n\n        if self.cache._get_cache_url() != state[\"input_dir_url\"]:\n            raise ValueError(\n                f\"The input directory URL of the current instance is {self.cache._get_cache_url()} but the input directory URL of the state dict is {state['input_dir_url']}.\"\n            )\n\n        if self.shuffle != state[\"shuffle\"]:\n            raise ValueError(\n                f\"The shuffle flag of the current instance is {self.shuffle} but the shuffle flag of the state dict is {state['shuffle']}.\"\n            )\n\n        if self.drop_last != state[\"drop_last\"]:\n            raise ValueError(\n                f\"The drop_last flag of the current instance is {self.drop_last} but the drop_last flag of the state dict is {state['drop_last']}.\"\n            )\n\n        if self.seed != state[\"seed\"]:\n            raise ValueError(\n                f\"The seed of the current instance is {self.seed} but the seed of the state dict is {state['seed']}.\"\n            )\n\n        if self.item_loader is None:\n            if state[\"item_loader\"] is not None:\n                raise ValueError(\n                    \"The item loader of the current instance"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Hash the input directory\n    hash_object = hashlib.md5(input_dir.encode())\n    cache_dir_name = hash_object.hexdigest()\n\n    # Create the cache directory\n    if os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"):\n        cache_dir = os.path.join(os.getenv(\"DATA_OPTIMIZER_CACHE_DIR\"), cache_dir_name)\n    else:\n        cache_dir = os.path.join(_DEFAULT_CACHE_DIR, cache_dir_name)\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        parsed_url = parse.urlparse(remote_filepath)\n        if parsed_url.scheme != \"s3\":\n            raise ValueError(\n                f\"The remote file path {remote_filepath} does not use the 's3' scheme.\"\n            )\n\n        if os.path.exists(local_filepath):\n            return\n\n        lock_filepath = local_filepath + \".lock\"\n        lock = FileLock(lock_filepath)\n        try:\n            with lock.acquire(timeout=10):\n                if not os.path.exists(local_filepath):\n                    if self._s5cmd_available:\n                        self._download_file_with_s5cmd(remote_filepath, local_filepath)\n                    else:\n                        self._download_file_with_boto3(remote_filepath, local_filepath)\n        except Timeout:\n            raise Timeout(\n                f\"The file lock for {local_filepath} could not be acquired within 10 seconds.\"\n            )\n"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # TODO: Implement a more sophisticated strategy to distribute chunks among workers.\n    # For now, we simply distribute chunks among workers in a round-robin fashion.\n    # The number of chunks per worker is also determined by the number of workers.\n    # For example, if there are 10 chunks and 3 workers, each worker will be assigned 4 chunks.\n    # If there are 10 chunks and 4 workers, each worker will be assigned 3 chunks.\n    # If there are 10 chunks and 5 workers, each worker will be assigned 2 chunks.\n    # If there are 10 chunks and 10 workers, each worker will be assigned 1 chunk.\n    # If there are 10 chunks and 11 workers, each worker will be assigned 1 chunk.\n    # If there are 10 chunks and 12 workers, each worker will be assigned 1 chunk.\n    # If there are 10 chunks and 13 workers, each worker will be assigned 1 chunk.\n    # If there are 10 chunks and 14 workers, each worker will be assigned 1 chunk.\n    # If there are 10 chunks and 15 workers, each worker will be assigned 1 chunk.\n    # If there are 10 chunks and 16 workers, each worker will be assigned 1 chunk.\n    # If there are 10 chunks and 17 workers, each worker will be assigned 1 chunk.\n    # If there are 10 chunks and 18 workers, each worker will be assigned 1 chunk.\n    # If there are 10 chunks and 19 workers, each worker will be assigned 1 chunk.\n    # If there are 10 chunks and 20 workers, each worker will be assigned 1 chunk.\n    # If there are 10 chunks and 21 workers, each worker will be assigned 1 chunk.\n    # If there are 10 chunks and 22 workers, each worker will be assigned 1 chunk.\n    # If there are 10 chunks and 23 workers,"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[len(\"local:\") :]\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        if not isinstance(item, Image.Image):\n            raise TypeError(f\"PILSerializer can only serialize PIL Images, not {type(item)}\")\n\n        buffer = io.BytesIO()\n        item.save(buffer, format=\"png\")\n        return buffer.getvalue(), None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile):\n            if item.filename and os.path.exists(item.filename):\n                with open(item.filename, \"rb\") as f:\n                    return f.read(), None\n            else:\n                return item.tobytes(), None\n        elif _TORCH_VISION_AVAILABLE and isinstance(item, torch.Tensor):\n            return encode_jpeg(item), None\n        elif _PIL_AVAILABLE and isinstance(item, Image.Image):\n            return item.convert(\"RGB\").tobytes(\"jpeg\"), None\n        else:\n            raise TypeError(f\"JPEGSerializer only supports PIL Image or torch.Tensor, but got {type(item)}\")\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[:12], np.uint32)\n        width, height, mode_len = ints[0], ints[1], ints[2]\n        mode = data[12:12 + mode_len].decode(\"utf-8\")\n        raw = data[12 + mode_len:]\n        return Image.frombytes(mode, (width, height), raw)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = int.from_bytes(data[:4], \"little\")\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        data = data[4:]\n        num_dims = int.from_bytes(data[:4], \"little\")\n        data = data[4:]\n        shape = []\n        for _ in range(num_dims):\n            dim = int.from_bytes(data[:4], \"little\")\n            data = data[4:]\n            shape.append(dim)\n        return torch.frombuffer(data, dtype=dtype).reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        if not isinstance(item, torch.Tensor):\n            raise TypeError(f\"The input item should be a torch.Tensor. Found {type(item)}.\")\n\n        dtype = self._dtype_to_indices[item.dtype]\n        shape = item.shape\n        data = item.data.tobytes()\n\n        return (dtype, shape, data), None\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except RuntimeError:\n                pass\n\n        image = Image.open(io.BytesIO(data))\n        if _TORCH_VISION_AVAILABLE:\n            return pil_to_tensor(image)\n        return image\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[self._dtype]\n        data = item.numpy().tobytes(order=\"C\")\n        return data, f\"no_header_tensor:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        return torch.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        return np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype).reshape(shape)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[self._dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_streaming,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n        else:\n            return {\n                \"dataset\": self.dataset.state_dict(),\n                \"current_epoch\": self.current_epoch,\n                \"num_samples_yielded\": self._num_samples_yielded_combined,\n                \"latest_worker_idx\": self._latest_worker_idx,\n            }\n"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _AV_AVAILABLE:\n            raise ImportError(\"The av library is not installed. Please install av to use the VideoSerializer.\")\n        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\"The torchvision library is not installed. Please install torchvision to use the VideoSerializer.\")\n\n        with tempfile.TemporaryDirectory() as tmpdirname:\n            tmp_file = os.path.join(tmpdirname, \"tmp.mp4\")\n            with open(tmp_file, \"wb\") as f:\n                f.write(data)\n            return torchvision.io.read_video(tmp_file)\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self._is_done:\n            return []\n\n        # Write any remaining chunks\n        while not self._should_write():\n            self.write_chunk()\n\n        # Write the index file\n        self.write_chunks_index()\n\n        # Mark the writer as done\n        self._is_done = True\n\n        # Return the list of written chunks\n        return [os.path.join(self._cache_dir, f\"chunk-{self.rank}-{i}.bin\") for i in range(self._chunk_index)]\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            assert self.batch_size\n            self.dataset.load_state_dict(\n                obj[\"dataset\"], self.num_workers, self.batch_size, obj[\"num_samples_yielded\"]\n            )\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n            self._worker_idx_iter = iter(self._worker_idx)\n            self.restore = True\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"], self.num_workers, self.batch_size)\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n            self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n            self._worker_idx_iter = iter(self._worker_idx)\n            self.restore = True\n        else:\n            raise RuntimeError(\n                \"The provided dataset is neither a StreamingDataset nor a CombinedStreamingDataset.\"\n            )\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            else:\n                return {__NUM_SAMPLES_YIELDED_KEY__: num_samples_yielded}\n        else:\n            return self._iterator.state_dict(num_workers, batch_size)\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if not state_dict:\n            return\n\n        if self._iterator is None:\n            self._num_samples_yielded = state_dict.get(__NUM_SAMPLES_YIELDED_KEY__)\n            return\n\n        self._iterator.load_state_dict(state_dict)\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if not isinstance(dir_path, str):\n        raise TypeError(f\"dir_path must be a string, not {type(dir_path)}\")\n\n    dir_path = dir_path.strip()\n\n    if not dir_path:\n        return Dir()\n\n    if dir_path.startswith(\"s3://\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"https://studio.lightning.net/\") or dir_path.startswith(\"https://studio.lightning.ai/\"):\n        return Dir(url=dir_path)\n\n    if dir_path.startswith(\"lightning-\"):\n        return Dir(url=f\"https://studio.lightning.net/{dir_path}\")\n\n    if dir_path.startswith(\"lightning-ai-\"):\n        return Dir(url=f\"https://studio.lightning.ai/{dir_path}\")\n\n    if dir_path.startswith(\"https://\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"http://\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"file://\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"./\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"../\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"~/\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"~\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\".\"):\n        return Dir(path=dir_path)\n\n    if dir_path.startswith(\"..\"):\n        return Dir(path"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise TypeError(f\"The `output_dir` must be a `Dir` object, got: {type(output_dir)}\")\n\n    if not output_dir.url.startswith(\"s3://\"):\n        raise ValueError(f\"The `output_dir` must be an S3 directory, got: {output_dir.url}\")\n\n    if append:\n        raise NotImplementedError(\"Appending data to the directory is not currently supported.\")\n\n    if overwrite:\n        raise NotImplementedError(\"Overwriting data in the directory is not currently supported.\")\n\n    if _BOTO3_AVAILABLE:\n        s3_client = boto3.client(\"s3\")\n        try:\n            s3_client.head_object(Bucket=output_dir.url[5:].split(\"/\")[0], Key=\"/\".join(output_dir.url[5:].split(\"/\")[1:]))\n            raise ValueError(f\"The directory {output_dir.url} already contains data.\")\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                pass\n            else:\n                raise e\n    else:\n        raise ImportError(\"The `boto3` library is required to check if the directory is empty.\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # If the index file is not found, delete all objects within the specified prefix in the bucket.\n    if objects[\"KeyCount\"] == 0:\n        s3.delete_objects(\n            Bucket=obj.netloc,\n            Delete={\n                \"Objects\": [{\"Key\": obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\" + obj.path.lstrip(\"/\").rstrip(\"/\")}],\n                \"Quiet\": True,\n            },\n        )\n        return\n\n    # If the index file is found, raise an error.\n    if \"Contents\" in objects:\n        for obj in objects[\"Contents\"]:\n            if obj[\"Key\"].endswith(\"index.json\"):\n                raise RuntimeError(\n                    f\"The provided output_dir `{output_dir.path}` already contains an index file and datasets are meant to be immutable.\"\n                    \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n                )\n\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait until all workers have finished\n        while len(os.listdir(self._cache_dir)) < num_workers:\n            sleep(1)\n\n        # Wait until the master node has finished\n        if node_rank is not None and node_rank != 0:\n            while not os.path.exists(os.path.join(self._cache_dir, f\"0.{_INDEX_FILENAME}\")):\n                sleep(1)\n\n        # Merge the index files\n        if node_rank == 0:\n            index_files = [\n                os.path.join(self._cache_dir, f)\n                for f in os.listdir(self._cache_dir)\n                if f.endswith(_INDEX_FILENAME)\n            ]\n            chunks_info = []\n            for index_file in index_files:\n                with open(index_file, \"r\") as f:\n                    chunks_info.extend(json.load(f)[\"chunks\"])\n            with open(os.path.join(self._cache_dir, _INDEX_FILENAME), \"w\") as out:\n                json.dump({\"chunks\": chunks_info}, out, sort_keys=True)\n"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\"The Lightning SDK is not available. Please install it with `pip install lightning-sdk`.\")\n\n    if machine is None:\n        machine = Machine.get_default()\n\n    if command is None:\n        command = f\"cd {os.getcwd()} && {os.getenv('SHELL', '/bin/bash')} -l -c 'printenv && sleep infinity'\"\n\n    studio = Studio(url=_get_lightning_cloud_url())\n    job = studio.jobs.create(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n    )\n\n    print(f\"Job created: {job.url}\")\n\n    while job.status not in [\"running\", \"failed\", \"succeeded\"]:\n        sleep(1)\n        job = studio.jobs.get(job.id)\n\n    if job.status == \"failed\":\n        raise RuntimeError(f\"Job failed: {job.url}\")\n\n    print(f\"Job started: {job.url}\")"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Load the chunks configuration\n        config = ChunksConfig(\n            cache_dir=self._cache_dir,\n            remote_input_dir=self._remote_input_dir,\n            serializers=self._serializers,\n            compression=self._compression,\n            item_loader=self._item_loader,\n        )\n\n        # Check whether the index files are available\n        if config.is_index_available():\n            self._config = config\n            return config\n\n        return None\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration is not set.\")\n        return self._config\n"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"The index must be an instance of ChunkedIndex, but got {index}.\")\n\n        if self._config is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index from the index\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Check if the chunk is available locally\n        if self._config.is_chunk_available_locally(chunk_index):\n            return self._item_loader.load_item(index, self._config[index])\n\n        # Check if the chunk is available in memory\n        if self._config.is_chunk_available_in_memory(chunk_index):\n            return self._item_loader.load_item(index, self._config[index])\n\n        # Check if the chunk is available remotely\n        if self._config.is_chunk_available_remotely(chunk_index):\n            self._config.download_chunk_from_index(chunk_index)\n            return self._item_loader.load_item(index, self._config[index])\n\n        # Check if the chunk is being downloaded\n        if self._prepare_thread is not None:\n            assert self._prepare_thread is not None\n            if chunk_index in self._prepare_thread._chunks_index_to_be_deleted:\n                raise Exception(\n                    f\"The chunk {chunk_index} is being deleted. Please wait for the chunk to be fully processed.\"\n                )\n\n            if chunk_index in self._prepare_thread._chunks_index_to_be_deleted:\n                raise Exception(\n                    f\"The chunk {chunk_index} is being downloaded. Please wait for the chunk to be fully processed.\"\n                )\n\n        # Download the chunk\n        self._config.download_chunk_from_index(chunk_index)\n        return self._item_loader.load_item(index, self._config[index])\n"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    try:\n        return _ImmutableDistributedMap().set_and_get(key, obj)\n    except (requests.exceptions.ConnectionError, urllib3.exceptions.MaxRetryError):\n        return obj\n\n"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # get the number of chunks assigned to each node\n    chunks_per_node = [len(chunks_per_ranks[rank]) for rank in range(distributed_env.world_size)]\n\n    # get the number of chunks assigned to each node\n    chunks_per_node = [len(chunks_per_ranks[rank]) for rank in range(distributed_env.world_size)]\n\n    # get the number of chunks assigned to each node\n    chunks_per_node = [len(chunks_per_ranks[rank]) for rank in range(distributed_env.world_size)]\n\n    # get the number of chunks assigned to each node\n    chunks_per_node = [len(chunks_per_ranks[rank]) for rank in range(distributed_env.world_size)]\n\n    # get the number of chunks assigned to each node\n    chunks_per_node = [len(chunks_per_ranks[rank]) for rank in range(distributed_env.world_size)]\n\n    # get the number of chunks assigned to each node\n    chunks_per_node = [len(chunks_per_ranks[rank]) for rank in range(distributed_env.world_size)]\n\n    # get the number of chunks assigned to each node\n    chunks_per_node = [len(chunks_per_ranks[rank]) for rank in range(distributed_env.world_size)]\n\n    # get the number of chunks assigned to each node\n    chunks_per_node = [len(chunks_per_ranks[rank]) for rank in range(distributed_env.world_size)]\n\n    # get the number of chunks assigned to each node\n    chunks_per_node = [len(chunks_per_ranks[rank]) for rank in range(distributed_env.world_size)]\n\n    # get the number of chunks assigned to each node\n    chunks_per_node = [len(chunks_per_ranks[rank]) for rank in range(distributed_env.world_size)]"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) == 0:\n        return None\n\n    if len(indexed_paths) > 1:\n        first_path = indexed_paths[0]\n        for path in indexed_paths.values():\n            if path != first_path:\n                raise ValueError(\n                    f\"Inconsistent file paths found in inputs: {first_path} and {path}.\"\n                )\n\n    input_path = Path(indexed_paths[0])\n    input_dir = _resolve_dir(input_path)\n\n    return input_dir\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    if enable:\n        Popen([\"sudo\", \"sysctl\", \"-w\", \"net.ipv6.conf.all.disable_ipv6=0\"], stdout=DEVNULL, stderr=DEVNULL)\n        Popen([\"sudo\", \"sysctl\", \"-w\", \"net.ipv6.conf.default.disable_ipv6=0\"], stdout=DEVNULL, stderr=DEVNULL)\n    else:\n        Popen([\"sudo\", \"sysctl\", \"-w\", \"net.ipv6.conf.all.disable_ipv6=1\"], stdout=DEVNULL, stderr=DEVNULL)\n        Popen([\"sudo\", \"sysctl\", \"-w\", \"net.ipv6.conf.default.disable_ipv6=1\"], stdout=DEVNULL, stderr=DEVNULL)\n\n    try:\n        yield\n    finally:\n        Popen([\"sudo\", \"sysctl\", \"-w\", \"net.ipv6.conf.all.disable_ipv6=1\"], stdout=DEVNULL, stderr=DEVNULL)\n        Popen([\"sudo\", \"sysctl\", \"-w\", \"net.ipv6.conf.default.disable_ipv6=1\"], stdout=DEVNULL, stderr=DEVNULL)\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    # calculate the number of items to be processed by each rank\n    num_items_per_rank = [\n        len(indexes) // distributed_env.world_size\n        if distributed_env.world_size > 1\n        else len(indexes)\n        for _ in range(distributed_env.world_size)\n    ]\n\n    # if drop_last is True, then the number of items to be processed by each rank is decreased by 1\n    if drop_last:\n        for i in range(len(num_items_per_rank) - 1):\n            num_items_per_rank[i] -= 1\n\n    # calculate the number of items to be processed by each rank\n    num_items_per_rank = [\n        len(indexes) // distributed_env.world_size\n        if distributed_env.world_size > 1\n        else len(indexes)\n        for _ in range(distributed_env.world_size)\n    ]\n\n    # if drop_last is True, then the number of items to be processed by each rank is decreased by 1\n    if drop_last:\n        for i in range(len(num_items_per_rank) - 1):\n            num_items_per_rank[i] -= 1\n\n    # calculate the number of items to be processed by each rank\n    num_items_per_rank = [\n        len(indexes) // distributed_env.world_size\n        if distributed_env.world_size > 1\n        else len(indexes)\n        for _ in range(distributed_env.world_size)\n    ]\n\n    # if drop_last is True, then the number of items to be processed by each rank is decreased by 1\n    if drop_last:\n        for i in range(len(num_items_per_rank) - 1):\n            num_items_per_rank[i] -= 1\n\n    # calculate the number of items to be processed by each rank\n    num_items_per_rank = [\n        len(indexes) // distributed_env.world_size\n        if"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        kwargs = {}\n        if self._contains_device:\n            kwargs[\"device\"] = self._device\n        if self._contains_is_last:\n            kwargs[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir, *self._inputs, **kwargs)\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            return s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n        except s3.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n                continue\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.functions.optimize", "completion": "    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        _assert_dir_has_index_file(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range("}
{"namespace": "litdata.processing.functions.map", "completion": "    if not callable(fn):\n        raise ValueError(f\"The provided fn {fn} isn't callable.\")\n\n    if not isinstance(inputs, Sequence):\n        raise ValueError(f\"The provided inputs {inputs} isn't a sequence.\")\n\n    if len(inputs) == 0:\n        raise ValueError(\"The provided inputs is empty.\")\n\n    if not isinstance(output_dir, (str, Dir)):\n        raise ValueError(f\"The provided output_dir {output_dir} isn't a string or a Dir object.\")\n\n    if isinstance(output_dir, str):\n        output_dir = Dir(output_dir)\n\n    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"The provided output_dir {output_dir} isn't a Dir object.\")\n\n    if weights is not None and not isinstance(weights, Sequence):\n        raise ValueError(f\"The provided weights {weights} isn't a sequence.\")\n\n    if weights is not None and len(weights) != len(inputs):\n        raise ValueError(f\"The provided weights {weights} doesn't match the number of inputs {len(inputs)}.\")\n\n    if num_workers is not None and not isinstance(num_workers, int):\n        raise ValueError(f\"The provided num_workers {num_workers} isn't an integer.\")\n\n    if num_workers is not None and num_workers <= 0:\n        raise ValueError(f\"The provided num_workers {num_workers} isn't a positive integer.\")\n\n    if not isinstance(fast_dev_run, (bool, int)):\n        raise ValueError(f\"The provided fast_dev_run {fast_dev_run} isn't a boolean or integer.\")\n\n    if isinstance(fast_dev_run, int) and fast_dev_run <= 0:\n        raise ValueError(f\"The provided fast_dev_run {fast_dev_run} isn't a positive integer.\")\n\n    if num_nodes is not None and not"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client(input_dir.scheme)\n\n    while True:\n        try:\n            task_index, file_paths = queue_in.get(block=True, timeout=1)\n        except Empty:\n            break\n\n        for file_path in file_paths:\n            file_path = file_path.lstrip(\"/\")\n            file_path_in_cache = os.path.join(cache_dir, file_path)\n            file_path_in_input_dir = os.path.join(input_dir.path, file_path)\n\n            if not os.path.exists(file_path_in_cache):\n                os.makedirs(os.path.dirname(file_path_in_cache), exist_ok=True)\n                obj = _resolve_dir(file_path_in_input_dir)\n                _wait_for_file_to_exist(s3, obj)\n                s3.download_file(obj, file_path_in_cache)\n\n        queue_out.put(task_index)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Fetch from the queue\n        r: Optional[Tuple[int, List[str]]] = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if r is None:\n            return\n\n        # 3. Unpack\n        index, paths = r\n\n        # 4. Upload all the required paths to unblock the current index\n        for path in paths:\n            if not path.startswith(cache_dir):\n                path = os.path.join(cache_dir, path)\n\n            if output_dir.url:\n                obj = parse.urlparse(output_dir.url)\n\n                if obj.scheme == \"s3\":\n                    s3.client.upload_file(path, obj.netloc, obj.path.lstrip(\"/\"))\n                else:\n                    raise ValueError(f\"The provided {output_dir.url} isn't supported.\")\n\n            elif output_dir.path:\n                shutil.copyfile(path, os.path.join(output_dir.path, os.path.basename(path)))\n\n        # 5. Inform the worker the current files are available\n        remove_queue.put(index)\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Associate the items to the workers based on number of nodes and node rank.\n    worker_items, worker_weights = _pack_greedily(items=user_items, weights=weights, num_bins=world_size)\n    worker_ids_this_node = range(node_rank * num_workers, (node_rank + 1) * num_workers)\n\n    # Print the distribution details for workers on the current node.\n    for worker_id in worker_ids_this_node:\n        if file_size:\n            print(\n                f\"Worker {worker_id} has {len(worker_items[worker_id])} files with a total size of {sum(worker_weights[worker_id]) / 1e6} MB.\"\n            )\n        else:\n            print(f\"Worker {worker_id} has {len(worker_items[worker_id])} items with a total weight of {sum(worker_weights[worker_id])}.\")\n\n    # Return a list of items for each worker, with the items shuffled.\n    return [worker_items[worker_id] for worker_id in worker_ids_this_node]\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Get the total number of workers across all nodes\n    total_num_workers = num_workers * _get_num_nodes()\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // total_num_workers\n\n    # Calculate the remainder\n    remainder = len(user_items) % total_num_workers\n\n    # Calculate the start and end indices for each worker's items\n    cumsum = np.cumsum([num_items_per_worker + 1 if i < remainder else num_items_per_worker for i in range(total_num_workers)])\n    start_indices = cumsum - num_items_per_worker\n    end_indices = cumsum\n\n    # Assign items to workers\n    worker_items = [user_items[start:end] for start, end in zip(start_indices, end_indices)]\n\n    # Ensure the output list has a length equal to the number of workers\n    if len(worker_items) != total_num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers.\")\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        # Cleanup the cache dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(self.cache_dir):\n            shutil.rmtree(self.cache_dir, ignore_errors=True)\n\n        os.makedirs(self.cache_dir, exist_ok=True)\n\n        # Cleanup the cache data dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(self.cache_data_dir):\n            shutil.rmtree(self.cache_data_dir, ignore_errors=True)\n\n        os.makedirs(self.cache_data_dir, exist_ok=True)\n\n        # Cleanup the cache chunks dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(self.cache_chunks_dir):\n            shutil.rmtree(self.cache_chunks_dir, ignore_errors=True)\n\n        os.makedirs(self.cache_chunks_dir, exist_ok=True)\n\n        # Cleanup the cache index dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(self.cache_index_dir):\n            shutil.rmtree(self.cache_index_dir, ignore_errors=True)\n\n        os.makedirs(self.cache_index_dir, exist_ok=True)\n\n        # Cleanup the cache index dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(self.cache_index_dir):\n            shutil.rmtree(self.cache_index_dir, ignore_errors=True)\n\n        os.makedirs(self.cache_index_dir, exist_ok=True)\n\n        # Cleanup the cache index dir folder to avoid corrupted files from previous run to be there.\n        if os.path.exists(self.cache_index_dir):\n            shutil.rmtree(self.cache_index_dir, ignore_errors=True)\n\n        os.makedirs(self"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    for future in concurrent.futures.as_completed(futures):\n        item_sizes.append(future.result())\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if not isinstance(element, str):\n        return False\n\n    if _IS_IN_STUDIO and input_dir is not None:\n        if element.startswith(input_dir):\n            return True\n\n        element = str(Path(element).absolute())\n\n    return os.path.exists(element)\n\n"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0\n        assert n_neurons > 0\n\n        if self.tcnn:\n            if n_neurons <= 256:\n                return tcnn.NetworkWithInputEncoding(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    network_config={\n                        \"otype\": \"FullyFusedMLP\",\n                        \"activation\": activation,\n                        \"output_activation\": output_activation,\n                        \"n_neurons\": n_neurons,\n                        \"n_hidden_layers\": n_layers - 1,\n                        \"bias\": True,\n                        \"n_inputs\": n_input_dims,\n                        \"n_outputs\": n_output_dims,\n                    },\n                    seed=self._get_seed(),\n                )\n            else:\n                return tcnn.NetworkWithInputEncoding(\n                    n_input_dims=n_input_dims,\n                    n_output_dims=n_output_dims,\n                    network_config={\n                        \"otype\": \"FullyFusedMLP\",\n                        \"activation\": activation,\n                        \"output_activation\": output_activation,\n                        \"n_neurons\": n_neurons,\n                        \"n_hidden_layers\": n_layers - 1,\n                        \"bias\": True,\n                        \"n_inputs\": n_input_dims,\n                        \"n_outputs\": n_output_dims,\n                        \"n_blocks\": 3,\n                    },\n                    seed=self._get_seed(),\n                )\n        else:\n            layers = []\n            for i in range(n_layers - 1):\n                layers.append(nn.Linear(n_input_dims if i == 0 else n_neurons, n_neurons))\n                if activation == \"ReLU\":\n                    layers.append(nn.ReLU())"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Create a copy of the input signal to avoid modifying it.\n        signal_copy = np.copy(signal)\n\n        # Create a list of shifted versions of the input signal.\n        shifted_signal_list = []\n        for i in range(-kernel_offset, kernel_offset + 1):\n            shifted_signal_list.append(np.roll(signal_copy, i))\n\n        # Calculate the median of the shifted versions of the input signal.\n        median_signal = np.median(shifted_signal_list, axis=0)\n\n        # Trim the median signal to remove edge effects.\n        median_signal = median_signal[kernel_offset:-kernel_offset]\n\n        return median_signal"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Check if the probe and gallery templates have the same code size\n    if template_probe.code_size != template_gallery.code_size:\n        raise MatcherError(\"The probe and gallery templates must have the same code size.\")\n\n    # Check if the probe and gallery templates have the same number of rotations\n    if template_probe.num_rotations != template_gallery.num_rotations:\n        raise MatcherError(\"The probe and gallery templates must have the same number of rotations.\")\n\n    # Check if the probe and gallery templates have the same number of masks\n    if template_probe.num_masks != template_gallery.num_masks:\n        raise MatcherError(\"The probe and gallery templates must have the same number of masks.\")\n\n    # Check if the rotation shift is valid\n    if rotation_shift < 0 or rotation_shift > template_probe.num_rotations:\n        raise MatcherError(\"The rotation shift must be a positive integer smaller than the number of rotations.\")\n\n    # Check if the nonmatch distance is valid\n    if nm_dist is not None and nm_dist < 0:\n        raise MatcherError(\"The nonmatch distance must be a positive number.\")\n\n    # Check if the weights are valid\n    if weights is not None:\n        for weight in weights:\n            if weight.shape != (template_probe.num_masks, template_probe.code_size, template_probe.code_size):\n                raise MatcherError(\"The weights must have the same shape as the probe and gallery templates.\")\n\n    # Calculate the minimum Hamming distance\n    min_HD = np.inf\n    min_rotation_shift = 0\n    for rotation_shift in range(template_probe.num_rotations):\n        # Calculate the Hamming distance\n        if weights is None:\n            HD = np.sum(\n                np.logical_xor(\n                    template_probe.rotations[rotation_shift],\n                    template_gallery.rotations[rotation_shift],\n                )\n            )\n        else"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Initialize the number of iterations to zero.\n        num_iterations = 0\n\n        # Initialize the number of points in the polygon.\n        num_points = polygon.shape[0]\n\n        # Initialize the number of points that meet the minimum distance criterion.\n        num_points_meeting_criterion = 0\n\n        # Initialize the first point of the first pair of points.\n        first_point_first_pair = None\n\n        # Initialize the second point of the first pair of points.\n        second_point_first_pair = None\n\n        # Initialize the first point of the second pair of points.\n        first_point_second_pair = None\n\n        # Initialize the second point of the second pair of points.\n        second_point_second_pair = None\n\n        # Initialize the first point of the perpendicular bisector.\n        first_bisector_point = None\n\n        # Initialize the second point of the perpendicular bisector.\n        second_bisector_point = None\n\n        # Initialize the first point of the perpendicular bisector.\n        first_bisector_point = None\n\n        # Initialize the second point of the perpendicular bisector.\n        second_bisector_point = None\n\n        # Initialize the first point of the perpendicular bisector.\n        first_bisector_point = None\n\n        # Initialize the second point of the perpendicular bisector.\n        second_bisector_point = None\n\n        # Initialize the first point of the perpendicular bisector.\n        first_bisector_point = None\n\n        # Initialize the second point of the perpendicular bisector.\n        second_bisector_point = None\n\n        # Initialize the first point of the perpendicular bisector.\n        first_bisector_point = None\n\n        # Initialize the second point of the perpendicular bisector.\n        second_bisector_point = None\n\n        # Initialize the first point of the perpendicular bisector.\n        first_bisector_point = None\n\n        # Initialize the second point of the perpendicular bisector.\n        second_bisector_point = None\n\n        # Initialize the"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        for callback in self._callbacks:\n            callback.on_algorithm_start(self, *args, **kwargs)\n\n        result = self.run(*args, **kwargs)\n\n        for callback in self._callbacks:\n            callback.on_algorithm_end(self, *args, **kwargs)\n\n        return result\n"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n        return self.check_type(deserialized_output, type_definition)\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        # Get function name\n        func_name = func_object.__name__\n\n        # Get function signature\n        signature = inspect.signature(func_object)\n\n        # Get input and output type hints\n        input_type_hints = get_type_hints(func_object)\n        output_type_hints = get_type_hints(func_object).pop(\"return\")\n\n        # Get input and output class definitions\n        input_class_definition = Register.get_class_definition(input_type_hints)\n        output_class_definition = Register.get_class_definition(output_type_hints)\n\n        # Get function type\n        if get_origin(output_type_hints) is Union:\n            if Embedding in get_args(output_type_hints):\n                function_type = FunctionType.EMBEDDABLE\n            else:\n                function_type = FunctionType.SYMBOLIC\n        elif issubclass(output_type_hints, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        # Get function description\n        docstring = inspect.getdoc(func_object)\n        if docstring is None:\n            docstring = \"\"\n\n        # Return function description\n        return FunctionDescription(\n            name=func_name,\n            input_class_definition=input_class_definition,\n            output_class_definition=output_class_definition,\n            function_type=function_type,\n            docstring=docstring\n        )\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] = seed\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        bit_array = self.persistence.load()\n        if len(bit_array) != self.size:\n            logging.warning(f\"Loaded bit array length {len(bit_array)} does not match expected length {self.size}. Reinitializing bit array and indices.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = bit_array\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        indices = [hash1 % self.size for _ in range(self.hash_count)]\n        return all(self.bit_array[indices])\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        if \"distilled_model\" in json_dict:\n            self.distilled_model = config_factory.create_model_config(json_dict[\"distilled_model\"][\"type\"], json_dict[\"distilled_model\"])\n        if \"current_model_stats\" in json_dict:\n            self.current_model_stats = json_dict[\"current_model_stats\"]\n        if \"last_training_run\" in json_dict:\n            self.last_training_run = json_dict[\"last_training_run\"]\n        if \"current_training_run\" in json_dict:\n            self.current_training_run = json_dict[\"current_training_run\"]\n        if \"nr_of_training_runs\" in json_dict:\n            self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        if \"teacher_models\" in json_dict:\n            self.teacher_models = [config_factory.create_model_config(teacher_model[\"type\"], teacher_model) for teacher_model in json_dict[\"teacher_models\"]]\n        return self\n"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # Validate the model\n        if not isinstance(model, OpenAIConfig):\n            raise TypeError(f\"model must be an instance of OpenAIConfig, not {type(model)}\")\n\n        # Validate the system message\n        if not isinstance(system_message, str):\n            raise TypeError(f\"system_message must be a string, not {type(system_message)}\")\n\n        # Validate the prompt\n        if not isinstance(prompt, str):\n            raise TypeError(f\"prompt must be a string, not {type(prompt)}\")\n\n        # Validate the kwargs\n        for key, value in kwargs.items():\n            if key not in LLM_GENERATION_PARAMETERS:\n                raise ValueError(f\"{key} is not a valid generation parameter\")\n            if key == \"temperature\":\n                if not isinstance(value, float):\n                    raise TypeError(f\"temperature must be a float, not {type(value)}\")\n            if key == \"top_p\":\n                if not isinstance(value, float):\n                    raise TypeError(f\"top_p must be a float, not {type(value)}\")\n            if key == \"max_new_tokens\":\n                if not isinstance(value, int):\n                    raise TypeError(f\"max_new_tokens must be an int, not {type(value)}\")\n            if key == \"frequency_penalty\":\n                if not isinstance(value, float):\n                    raise TypeError(f\"frequency_penalty must be a float, not {type(value)}\")\n            if key == \"presence_penalty\":\n                if not isinstance(value, float):\n                    raise TypeError(f\"presence_penalty must be a float, not {type(value)}\")\n\n        # Create the request\n        request = {\n            \"model\": model.model_name,\n            \"prompt\": system_message + prompt,\n            \"max_tokens\": 1,\n            \"temperature\": model.temperature,\n            \"top_p\": model.top"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n    if not np.allclose(np.diag(x), np.zeros(x.shape[0])):\n        raise ValueError(\"The diagonal elements of the matrix must be close to zero\")\n\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # if the function is not initialized, initialize it\n        if func_hash not in self.initialized_functions:\n            self.initialized_functions[func_hash] = {}\n            self.initialized_functions[func_hash][\"model\"] = \"\"\n            self.initialized_functions[func_hash][\"examples\"] = []\n\n        # if the function is initialized, check if it needs to be saved for fine-tuning\n        if len(self.initialized_functions[func_hash][\"examples\"]) > 0:\n            # if the function is initialized and has examples, check if the examples are suitable for fine-tuning\n            if self.check_examples_for_finetuning(args, kwargs, function_description, llm_parameters):\n                # if the examples are suitable for fine-tuning, check if the function is already initialized with a teacher model\n                if self.initialized_functions[func_hash][\"model\"] != \"\":\n                    # if the function is already initialized with a teacher model, return the teacher model and prompt\n                    model = self.initialized_functions[func_hash][\"model\"]\n                    prompt = self.construct_prompt(args, kwargs, function_description, llm_parameters, model)\n                    return prompt, model, False, False\n                else:\n                    # if the function is not initialized with a teacher model, initialize it with a teacher model and return the teacher model and prompt\n                    model = self.initialize_function(args, kwargs, function_description, llm_parameters)\n                    prompt = self.construct_prompt(args, kwargs, function_description, llm_parameters, model)\n                    return prompt, model, False, False\n            else:\n                # if the examples are not suitable for fine-tuning, save the examples for distillation\n                self.initialized_functions[func_hash][\"examples\"] = []\n\n        # if the function is initialized and does not need to be saved for fine-tuning, check if the function is already initialized with a teacher model\n        if self.initialized_functions[func_hash][\"model\"] != \"\":\n            # if the function is"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n\n    if higham:\n        # Higham & Nick (2002) algorithm\n        # https://www.sciencedirect.com/science/article/pii/S0022247X01002089\n        # https://www.mathworks.com/matlabcentral/fileexchange/34433-nearestspd\n        # https://www.mathworks.com/matlabcentral/fileexchange/34433-nearestspd?s_tid=prof_contriblnk\n        # https://www.mathworks.com/matlabcentral/fileexchange/34433-nearestspd?s_tid=prof_contriblnk#evaluation\n        # https://www.mathworks.com/matlabcentral/fileexchange/34433-nearestspd?s_tid=prof_contriblnk#evaluation\n        # https://www.mathworks.com/matlabcentral/fileexchange/34433-nearestspd?s_tid=prof_contriblnk#evaluation\n        # https://www.mathworks.com/matlabcentral/fileexchange/34433-nearestspd?s_tid=prof_contriblnk#evaluation\n        # https://www.mathworks.com/matlabcentral/fileexchange/34433-nearestspd?s_tid=prof_contriblnk#evaluation\n        # https://www.mathworks.com/matlabcentral/fileexchange/34433-nearestspd?s_tid=prof_contriblnk#evaluation\n        # https://www.mathworks.com/matlabcentral/fileexchange/34433-nearestspd?s_tid=prof_contriblnk#evaluation\n        # https://www.mathworks.com/matlabcentral/fileexchange/34433-nearestspd?s_tid=prof_contribln"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        #\n        # handle str and bytes separately as they are immutable in python\n        # and hence can be hashed\n        #\n        return (id(obj), len(obj)), IdentitySchema()\n    elif isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, (list, tuple)):\n        res, schema = ListSchema.flatten(obj)\n        return res, schema\n    elif isinstance(obj, collections.abc.Mapping):\n        res, schema = DictSchema.flatten(obj)\n        return res, schema\n    elif isinstance(obj, Instances):\n        res, schema = InstancesSchema.flatten(obj)\n        return res, schema\n    elif isinstance(obj, (Boxes, ROIMasks)):\n        return (obj.tensor,), TensorWrapSchema(_convert_target_to_string(type(obj)))\n    else:\n        raise ValueError(f\"Cannot flatten {type(obj)}\")\n\n"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Check inputs\n    groups = np.asarray(groups)\n    equations = np.asarray(equations)\n    if groups.ndim != 2:\n        raise ValueError(f\"{names[0]} must be a 2D array\")\n    if equations.ndim != 1:\n        raise ValueError(f\"{names[1]} must be a 1D array\")\n    if groups.shape[1] != equations.shape[1]:\n        raise ValueError(\n            f\"The number of columns in {names[0]} must be the same as the number of columns in {names[1]}\"\n        )\n\n    # Find groups in equations\n    groups_in_equations = set()\n    for equation in equations:\n        groups_in_equations.update(re.findall(r\"\\w+\", equation))\n\n    # Check if groups in equations are in groups\n    if not set(groups_in_equations).issubset(set(groups)):\n        if raise_if_group_missing:\n            raise GroupNotFoundError(\n                f\"The following groups were not found in {names[0]}: {list(groups_in_equations - set(groups))}\"\n            )\n        else:\n            warnings.warn(\n                f\"The following groups were not found in {names[0]}: {list(groups_in_equations - set(groups))}\"\n            )\n\n    # Convert equations to matrix\n    if len(groups_in_equations) > 0:\n        left_matrix = np.zeros((equations.shape[0], groups.shape[1]))\n        right_matrix = np.zeros((equations.shape[0],))\n        for i, equation in enumerate(equations):\n            for j, group in enumerate(groups):\n                if group in equation:\n                    left_matrix[i, j] = float(re.findall(f\"[-+]?\\\\b{group}\\\\b\", equation)[0])\n            right_matrix[i] = float(re.findall(r\"[-+]?\\b\\d+\\b\", equation)[0])\n        if sum_to"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n\n    # Define new Instances class\n    cls_name = \"Instances\"\n    cls_code_str = f\"\"\""}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n\n        # work around this bug: https://github.com/python-pillow/Pillow/issues/3973\n        image = _apply_exif_orientation(image)\n        return convert_PIL_to_numpy(image, format)\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    if isinstance(transforms, (tuple, list)):\n        transforms = T.TransformList(transforms)\n    # bbox is 1d (per-instance bounding box)\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    # clip transformed bbox to image size\n    bbox = transforms.apply_box(np.array([bbox]))[0].clip(min=0)\n    annotation[\"bbox\"] = np.minimum(bbox, list(image_size + image_size)[::-1])\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    if \"segmentation\" in annotation:\n        # each instance contains 1 or more polygons\n        segm = annotation[\"segmentation\"]\n        if isinstance(segm, list):\n            # polygons\n            polygons = [np.asarray(p).reshape(-1, 2) for p in segm]\n            annotation[\"segmentation\"] = [\n                p.reshape(-1) for p in transforms.apply_polygons(polygons)\n            ]\n        elif isinstance(segm, dict):\n            # RLE\n            mask = mask_util.decode(segm)\n            mask = transforms.apply_segmentation(mask)\n            assert tuple(mask.shape[:2]) == image_size\n            annotation[\"segmentation\"] = mask\n        else:\n            raise ValueError(\n                \"Cannot transform segmentation of type '{}'!\"\n                \"Supported types are: polygons as list[list[float] or ndarray],\"\n                \" COCO-style RLE as a dict.\".format(type(segm))\n            )\n\n    if \"keypoints\" in annotation:\n        keypoints = transform_keypoint_annotations(\n            annotation[\"keypoints\"], transforms, image_size, keypoint_hflip_indices\n        )\n        annotation[\"keypoints\"] = keypoints\n\n    return annotation\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        # return coords\n        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        coords = np.asarray(coords, dtype=float)\n        return cv2.transform(coords[:, np.newaxis, :], self.rm_coords)[:, 0, :]\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Set up the model for training\n    model.train()\n    flop_counter = FlopCountAnalysis(model, inputs)\n    flop_count = flop_counter.total()\n    return flop_count\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img.shape[:2] == (0, 0) or abs(self.angle) % 360 == 0:\n            return img\n        if img.ndim == 3 and img.shape[2] > 4:\n            # warpAffine is fast but doesn't work with many channels\n            img = img[:, :, :3]\n        ret = cv2.warpAffine(\n            img,\n            self.rm_image,\n            (int(self.bound_w), int(self.bound_h)),\n            flags=self.interp,\n        )\n        if img.ndim == 3 and img.shape[2] > 3:\n            ret = ret[:, :, :, np.newaxis]\n        return ret\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if predictions.has(\"pred_masks\"):\n            masks = np.asarray(predictions.pred_masks)\n            masks = [GenericMask(x, self.output.height, self.output.width) for x in masks]\n        else:\n            masks = None\n\n        if self._instance_mode == ColorMode.SEGMENTATION and self.metadata.get(\"thing_colors\"):\n            colors = [\n                self._jitter([x / 255 for x in self.metadata.thing_colors[c]]) for c in classes\n            ]\n            alpha = 0.8\n        else:\n            colors = None\n            alpha = 0.5\n\n        if self._instance_mode == ColorMode.IMAGE_BW:\n            self.output.img = self._create_grayscale_image(\n                (predictions.pred_masks.any(dim=0) > 0).numpy()\n            )\n            alpha = 0.3\n\n        self.overlay_instances(\n            masks=masks,\n            boxes=boxes,\n            labels=labels,\n            keypoints=keypoints,\n            assigned_colors=colors,\n            alpha=alpha,\n        )\n        return self.output\n"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        canvas = self.canvas.figure.canvas\n        # canvas.draw()\n        buf = canvas.buffer_rgba()\n        #self.figure.canvas.draw()\n        width, height = self.get_image_width_height()\n        ret = np.frombuffer(buf, dtype=np.uint8).reshape(height, width, 4)\n        # The first channel is R, the second channel is G, the third channel is B, and the fourth channel is A.\n        # The values in the first three channels are in the range [0, 255], and the value in the fourth channel is in the range [0, 1].\n        # The original image is in RGB format, so we only need to extract the first three channels.\n        ret = ret[:, :, :3]\n        return ret\n"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        # Draw panoptic segmentation on the image\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(dic[\"panoptic_seg\"])\n\n        # Draw semantic segmentation on the image\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"])\n\n        # Draw instance-level annotations on the image\n        for x in dic[\"annotations\"]:\n            self.draw_bbox(x[\"bbox\"], edge_color=_RED)\n            if \"bbox_mode\" in x:\n                bbox_mode = x[\"bbox_mode\"]\n            else:\n                bbox_mode = BoxMode.XYXY_ABS\n            text = \"{}\".format(x[\"category_id\"])\n            if \"score\" in x:\n                text = \"{:.2f}: \".format(x[\"score\"]) + text\n            elif \"scores\" in x:\n                text = \"{:.2f}: \".format(x[\"scores\"][x[\"category_id\"]]) + text\n            self.draw_text(text, x[\"bbox\"][:2], color=_RED)\n\n            # Draw keypoints on the image\n            if \"keypoints\" in x:\n                # determine keypoint color\n                if \"keypoint_colors\" in self.metadata:\n                    keypoint_colors = self.metadata.keypoint_colors\n                else:\n                    keypoint_colors = None\n                if keypoint_colors is None:\n                    keypoint_colors = _KEYPOINT_COLORS\n                keypoints = np.array(x[\"keypoints\"])\n                visibility = keypoints[2::3]\n                keypoints = keypoints.reshape(-1, 3)\n                self.draw_keypoints(keypoints, visibility, keypoint_colors)\n\n            # Draw bounding boxes on the image\n            if \"bbox\" in x:\n                # determine bounding box color\n                if \"bbox_colors\" in self.metadata:\n                    bbox_colors = self.metadata.bbox_colors\n                else:\n                    b"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        # Generate random colors for masks\n        if color is None:\n            color = random_color(rgb=True, maximum=1)\n\n        if area_threshold is None:\n            area_threshold = 0\n\n        # Convert binary masks to polygons\n        padded_binary_mask = np.pad(binary_mask, pad_width=1, mode=\"constant\", constant_values=0)\n        contours = cv2.findContours(padded_binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)[-2]\n        polygons = [c.reshape(-1).tolist() for c in contours if c.shape[0] >= 3]\n\n        # Draw polygons\n        if edge_color is None:\n            edge_color = color\n        edge_color = mplc.to_rgb(edge_color) + (1,)\n\n        self.draw_polygon(polygons, edge_color=edge_color, fill_color=color, alpha=alpha)\n\n        # Draw text\n        if text is not None:\n            # Set an appropriate font size\n            # font_size = max(np.sqrt(self.height * self.width) // 35, 5)\n            font_size = self._default_font_size\n\n            # Set the text color to be the opposite of the color of the mask\n            text_color = self._change_color_brightness(color, brightness_factor=-0.7)\n\n            # Draw the text on the image\n            text_pos = (0, 0)  # if the mask is at the top left, then the text should be at the bottom left\n            horizontal_alignment = \"left\"\n            vertical_alignment = \"top\"\n\n            self.draw_text(\n                text,\n                text_pos,\n                color=text_color,\n                horizontal_alignment=horizontal_alignment,\n                vertical_alignment=vertical_alignment,\n                font_size=font_size,\n            )\n\n        return self.output"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if not isinstance(input, Instances) or not isinstance(other, Instances):\n        raise ValueError(\"input and other should be of type Instances!\")\n\n    if not isinstance(rtol, float):\n        raise ValueError(\"rtol should be a float!\")\n\n    if not isinstance(msg, str):\n        raise ValueError(\"msg should be a string!\")\n\n    if not isinstance(size_as_tensor, bool):\n        raise ValueError(\"size_as_tensor should be a boolean!\")\n\n    if not isinstance(input.image_size, (tuple, torch.Tensor)) or not isinstance(other.image_size, (tuple, torch.Tensor)):\n        raise ValueError(\"image_size should be a tuple or a tensor!\")\n\n    if isinstance(input.image_size, tuple) and isinstance(other.image_size, tuple):\n        if size_as_tensor:\n            assert torch.allclose(torch.as_tensor(input.image_size), torch.as_tensor(other.image_size)), \\\n                f\"{msg}image_size does not match: {input.image_size} != {other.image_size}!\"\n        else:\n            assert input.image_size == other.image_size, f\"{msg}image_size does not match: {input.image_size} != {other.image_size}!\"\n    else:\n        assert torch.allclose(input.image_size, other.image_size), f\"{msg}image_size does not match: {input.image_size} != {other.image_size}!\"\n\n    # check that all fields in 'input' are also present in 'other'\n    assert set(input._fields.keys()) == set(other._fields.keys()), f\"{msg}fields do not match: {input._fields.keys()} != {other._fields.keys()}!\"\n\n    # check that all fields have the same type\n    for field in input._fields.keys():\n        assert type(input._fields[field]) == type(other._fields[field]), f\"{msg"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        box = self.tensor\n        area = box[:, 2] * box[:, 3]\n\n        return area\n"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n    else:\n        proposal_generator = PROPOSAL_GENERATOR_REGISTRY.get(cfg.MODEL.PROPOSAL_GENERATOR.NAME)(cfg, input_shape)\n        return proposal_generator"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        losses = FastRCNNOutputs(\n            self.box2box_transform,\n            scores,\n            proposal_deltas,\n            proposals,\n            self.smooth_l1_beta,\n            self.box_reg_loss_type,\n        ).losses()\n        return {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}\n"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    tracker_name = cfg.TRACKER_NAME\n    tracker = TRACKER_HEADS_REGISTRY.get(tracker_name)\n    tracker = tracker(cfg)\n    return tracker"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        \"\"\"\n        Apply transformation `deltas` (dx, dy, dw, dh) to `boxes`.\n\n        Args:\n            deltas (Tensor): transformation deltas of shape (N, k*4), where k >= 1.\n                deltas[i] represents k potentially different class-specific\n                box transformations for the single box boxes[i].\n            boxes (Tensor): boxes to transform, of shape (N, 4)\n        \"\"\"\n        boxes = boxes.to(deltas.dtype)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w  # x1\n        pred_boxes[:, 1::4"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        if anno_type is None:\n            return self.general_ins(image)\n        elif isinstance(anno_type, str):\n            return self.general_ins(image)[anno_type]\n        elif isinstance(anno_type, (list, tuple)):\n            return {\n                tp: self.general_ins(image)[tp]\n                for tp in anno_type if tp in self.general_ins(image)\n            }\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        normalized_query = normalize_string(query)\n        keywords = normalized_query.split()\n        result = {}\n        for keyword in keywords:\n            result = update_url_scores(result, self.bm25(keyword))\n        return result\n"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)\n"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        # Normalize angles to be within (-180, 180] degrees.\n        self.normalize_angles()\n\n        # Identify indices of boxes that are nearly horizontal.\n        cos_clip_angle_threshold = math.cos(clip_angle_threshold * math.pi / 180.0)\n        keep_indices = self.tensor[:, 2] / self.tensor[:, 3] <= cos_clip_angle_threshold\n\n        # For those boxes, convert representation from (center x, center y, width, height, angle) to (x1, y1, x2, y2).\n        box = self.tensor[keep_indices, :]\n        c_x = box[:, 0]\n        c_y = box[:, 1]\n        widths = box[:, 2]\n        heights = box[:, 3]\n        angle = box[:, 4]\n        angle = angle * math.pi / 180.0\n        half_widths = widths / 2.0\n        half_heights = heights / 2.0\n        x1 = c_x - half_widths * math.cos(angle) + half_heights * math.sin(angle)\n        y1 = c_y - half_widths * math.sin(angle) - half_heights * math.cos(angle)\n        x2 = c_x + half_widths * math.cos(angle) + half_heights * math.sin(angle)\n        y2 = c_y + half_widths * math.sin(angle) - half_heights * math.cos(angle)\n\n        # Clamp values to ensure they do not exceed the box_size limits.\n        x1 = torch.clamp(x1, min=0, max=box_size[1])\n        y1 = torch.clamp(y1, min=0, max=box_size[0])\n        x2 = torch.clamp(x2, min=0, max=box_size[1])\n        y2 = torch.clamp("}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n        for item in self.data:\n            statistics[item['type']] += 1\n        return statistics\n"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        raise NotImplementedError\n\n"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if 'train_cfg' in cfg or 'test_cfg' in cfg:\n        warnings.warn(\n            'train_cfg and test_cfg in plain options is deprecated, '\n            'it should be specified in model field.')\n\n    if 'train_cfg' in cfg:\n        assert train_cfg is None, 'train_cfg specified in both outer field and ' \\\n                                 'model field, which is confusing.'\n        train_cfg = cfg.pop('train_cfg')\n    if 'test_cfg' in cfg:\n        assert test_cfg is None, 'test_cfg specified in both outer field and ' \\\n                                'model field, which is confusing.'\n        test_cfg = cfg.pop('test_cfg')\n\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(cfg, train_cfg, test_cfg)\n    else:\n        return MMDET_DETECTORS.build(cfg, train_cfg, test_cfg)\n\n"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    assert len(dt_annos) == len(gt_annos)\n    assert len(metric) > 0\n    assert box_type_3d is not None\n    assert box_mode_3d is not None\n\n    # sort the detection annotations by score\n    for i in range(len(dt_annos)):\n        dt_annos[i] = sorted(dt_annos[i], key=lambda x: x['score'], reverse=True)\n\n    # get the number of classes\n    num_classes = len(label2cat)\n\n    # construct the confusion matrix\n    confusion_matrix = np.zeros((num_classes, num_classes))\n    for gt_anno, dt_anno in zip(gt_annos, dt_annos):\n        gt_boxes = gt_anno['gt_boxes_3d'].tensor\n        gt_labels = gt_anno['gt_labels_3d']\n        dt_boxes = dt_anno['boxes_3d'].tensor\n        dt_labels = dt_anno['labels_3d']\n        dt_scores = dt_anno['scores_3d']\n        # construct the confusion matrix\n        for gt_box, gt_label, dt_box, dt_label, dt_score in zip(gt_boxes, gt_labels, dt_boxes, dt_labels, dt_scores):\n            if gt_label < 0 or dt_label < 0:\n                continue\n            confusion_matrix[gt_label][dt_label] += 1\n\n    # compute the precision and recall\n    precision = np.zeros((num_classes, len(metric)))\n    recall = np.zeros((num_classes, len(metric)))\n    for i in range(num_classes):\n        tp = confusion_matrix[i, i]\n        fp = np.sum(confusion_matrix[:, i]) - tp\n        fn = np.sum(confusion_matrix[i, :]) - tp\n        for j, thresh in enumerate(metric):\n            precision[i, j] = tp / ("}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        box_type_3d = Box3D\n        box_mode_3d = Box3DMode.LIDAR\n    elif box_type == 'Camera':\n        box_type_3d = Box3D\n        box_mode_3d = Box3DMode.CAM\n    elif box_type == 'Depth':\n        box_type_3d = DepthInstance3DBoxes\n        box_mode_3d = Box3DMode.DEPTH\n    else:\n        raise ValueError('Input box type is not recognized ' + box_type)\n\n    return box_type_3d, box_mode_3d\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if messages is None:\n      messages = []\n\n    if not isinstance(messages, list):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    if not all(isinstance(message, (dict, Message)) for message in messages):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': [_encode_message(message) for message in messages],\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'model': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise RequestError('Model is required')\n\n    if not isinstance(context, (list, tuple)):\n      context = []\n\n    if not isinstance(images, (list, tuple)):\n      images = []\n\n    if not isinstance(options, dict):\n      options = {}\n\n    if keep_alive is not None and not isinstance(keep_alive, (float, str)):\n      raise RequestError('Keep-alive must be a float or a string')\n\n    if format not in ['', 'json']:\n      raise RequestError('Format must be an empty string or \"json\"')\n\n    if format == 'json':\n      format = 'application/json'\n\n    if format == '':\n      format = 'text/plain'\n\n    if images:\n      images = [self._encode_image(image) for image in images]\n\n    if keep_alive is not None:\n      if isinstance(keep_alive, str):\n        keep_alive = float(keep_alive)\n\n      if keep_alive < 0:\n        raise RequestError('Keep-alive must be a positive number')\n\n    return self._request_stream(\n      'POST',\n      '/generate',\n      stream=stream,\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context,\n        'raw': raw,\n        'format': format,\n        'images': images,\n        'options': options,\n        'keep_alive': keep_alive,\n      },\n    )\n"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not (path or modelfile):\n      raise RequestError('must provide a path or modelfile')\n\n    if path:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, 'rb') as f:\n      digest = sha256(f.read()).hexdigest()\n\n    r = self._client.head(f'/api/blobs/{digest}')\n    if r.status_code == 404:\n      with open(path, 'rb') as f:\n        self._client.post(f'/api/blobs/{digest}', data=f)\n\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary file to store the user code and test code\n        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\") as f:\n            f.write(user_code)\n            f.flush()\n\n            # Create a temporary file to store the pyright output\n            with tempfile.NamedTemporaryFile(mode=\"r\") as g:\n                # Run pyright on the combined user code and test code\n                subprocess.run(\n                    [\"pyright\", f.name, \"--outputjson\", g.name],\n                    stdout=subprocess.DEVNULL,\n                    stderr=subprocess.DEVNULL,\n                )\n\n                # Read the pyright output\n                pyright_output = g.read()\n\n                # Parse the pyright output\n                pyright_output_dict = json.loads(pyright_output)\n\n                # Initialize a list to store the error messages\n                error_messages = []\n\n                # Iterate through the diagnostics in the pyright output\n                for diagnostic in pyright_output_dict[\"generalDiagnostics\"]:\n                    # If the diagnostic is not an error, skip it\n                    if diagnostic[\"severity\"] != \"error\":\n                        continue\n\n                    # Extract the line number and error message from the diagnostic\n                    line_no = diagnostic[\"file\"][\"lineMap\"][str(diagnostic[\"range\"][\"start\"][\"line\"])]\n                    error_message = diagnostic[\"messageText\"]\n\n                    # If the error message is expected, skip it\n                    if error_message.startswith(cls.EXPECT_ERROR_COMMENT):\n                        continue\n\n                    # Add the error message to the list\n                    error_messages.append(f\"Line {line_no}: {error_message}\")\n\n                # If there are any error messages, the type check failed\n                if error_messages:\n                    return TypeCheckResult(\n                        message=\"\\n\".join(error_messages),\n                        passed=False,\n                    )\n\n                # Otherwise, the type check passed\n                return TypeCheckResult(\n                "}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        return aot_module(fn, get_compiler_fn(\"Module Code:\"))\n    else:\n        return aot_function(fn, get_compiler_fn(\"Function Code:\"))"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    config_dict = yaml.load(open(os.path.join(trial_path, 'config.yaml'), 'r'), Loader=yaml.FullLoader)\n    best_config_dict = summary_df_to_yaml(summary_df, config_dict)\n\n    if output_path is not None:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError('The output_path must end with .yaml or .yml if specified.')\n        with open(output_path, 'w') as f:\n            yaml.dump(best_config_dict, f)\n\n    return best_config_dict\n\n"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    # Create a lock to ensure thread safety\n    lock = threading.Lock()\n\n    # Create a cache to store traced modules\n    cache = {}\n\n    # Define a function to trace and cache the module\n    @functools.wraps(func)\n    def trace_and_cache(func, *args, **kwargs):\n\n        # Get the function's name\n        name = func.__name__\n\n        # Get the function's arguments\n        args = args if args else tuple()\n        kwargs = kwargs if kwargs else {}\n\n        # Get the function's signature\n        signature = (name, args, kwargs)\n\n        # Acquire the lock\n        lock.acquire()\n\n        # If the signature is not in the cache, trace the function and cache the result\n        if signature not in cache:\n\n            # Trace the function\n            traced_module = trace_with_kwargs(func, args, kwargs, **kwargs_)\n\n            # If a compiler is provided, compile the traced module\n            if ts_compiler is not None:\n                traced_module = ts_compiler(traced_module, **kwargs_)\n\n            # Cache the traced module\n            cache[signature] = traced_module\n\n        # Release the lock\n        lock.release()\n\n        # Return the traced module\n        return cache[signature]\n\n    # Return the wrapped function\n    return trace_and_cache(func)\n\n"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config = extract_best_config(trial_path)\n        return cls(config, project_dir=os.path.dirname(trial_path))\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Create the node line directory if it does not exist\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Run each module with its parameters\n    results = []\n    for module, params in zip(modules, module_params):\n        logger.info(f\"Running module {module.__name__} with parameters {params}\")\n        result = module(**params)\n        results.append(result)\n\n    # Measure the execution times\n    times = measure_speed(results)\n\n    # Evaluate the results\n    metrics = evaluate_retrieval(results, previous_result)\n\n    # Apply strategies\n    if \"threshold\" in strategies:\n        results, metrics, times = filter_by_threshold(results, metrics, times, strategies[\"threshold\"])\n    if \"select\" in strategies:\n        results, metrics, times = select_best_average(results, metrics, times, strategies[\"select\"])\n\n    # Save the results and summaries\n    for result, metric, time in zip(results, metrics, times):\n        result.to_csv(os.path.join(node_line_dir, \"result.csv\"), index=False)\n        metric.to_csv(os.path.join(node_line_dir, \"metric.csv\"), index=False)\n        time.to_csv(os.path.join(node_line_dir, \"time.csv\"), index=False)\n\n    # Return the best result\n    return results[0]\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Create a directory for the node\n    node_dir = os.path.join(node_line_dir, \"query_expansion\")\n    pathlib.Path(node_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the node\n    node_dir = os.path.join(node_line_dir, \"query_expansion\")\n    pathlib.Path(node_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the node\n    node_dir = os.path.join(node_line_dir, \"query_expansion\")\n    pathlib.Path(node_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the node\n    node_dir = os.path.join(node_line_dir, \"query_expansion\")\n    pathlib.Path(node_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the node\n    node_dir = os.path.join(node_line_dir, \"query_expansion\")\n    pathlib.Path(node_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the node\n    node_dir = os.path.join(node_line_dir, \"query_expansion\")\n    pathlib.Path(node_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the node\n    node_dir = os.path.join(node_line_dir, \"query_expansion\")\n    pathlib.Path(node_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the node\n    node_dir = os.path.join(node_line_dir, \"query_expansion\")\n    pathlib.Path(node_dir).mkdir(parents=True, exist_ok=True)\n\n    # Create a directory for the node\n    node_dir = os.path.join(node_line_dir,"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create the output directory\n    os.makedirs(node_line_dir, exist_ok=True)\n\n    # Create the output directory for the prompt maker module\n    prompt_maker_dir = os.path.join(node_line_dir, \"prompt_maker\")\n    os.makedirs(prompt_maker_dir, exist_ok=True)\n\n    # Create the output directory for the prompt maker module's results\n    prompt_maker_result_dir = os.path.join(prompt_maker_dir, \"results\")\n    os.makedirs(prompt_maker_result_dir, exist_ok=True)\n\n    # Create the output directory for the prompt maker module's results\n    prompt_maker_summary_dir = os.path.join(prompt_maker_dir, \"summary\")\n    os.makedirs(prompt_maker_summary_dir, exist_ok=True)\n\n    # Create the output directory for the prompt maker module's results\n    prompt_maker_time_dir = os.path.join(prompt_maker_dir, \"time\")\n    os.makedirs(prompt_maker_time_dir, exist_ok=True)\n\n    # Create the output directory for the prompt maker module's results\n    prompt_maker_combinations_dir = os.path.join(prompt_maker_dir, \"combinations\")\n    os.makedirs(prompt_maker_combinations_dir, exist_ok=True)\n\n    # Create the output directory for the prompt maker module's results\n    prompt_maker_combinations_result_dir = os.path.join(prompt_maker_combinations_dir, \"results\")\n    os.makedirs(prompt_maker_combinations_result_dir, exist_ok=True)\n\n    # Create the output directory for the prompt maker module's results\n    prompt_maker_combinations_summary_dir = os.path.join(prompt_maker_combinations_dir, \"summary\")\n    os.makedirs(prompt_maker"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    values = list(itertools.chain.from_iterable(list(map(lambda x: extract_values(x, key), nodes))))\n    return list(set(values))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model('all-mpnet-base-v2')\n\n    # Convert the strings into embeddings\n    gt_embeddings = embedding_model.encode(generation_gt)\n    pred_embedding = embedding_model.encode([pred])\n\n    # Calculate the cosine similarity between the prediction and each ground truth string\n    cosine_similarities = calculate_cosine_similarity(gt_embeddings, pred_embedding)\n\n    # Return the maximum cosine similarity\n    return max(cosine_similarities)\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    global gfpgan_face_restorer\n\n    if gfpgan_face_restorer is None:\n        gfpgan_face_restorer = FaceRestorerGFPGAN(\n            model_path=shared.model_path,\n            upscale_factor=2,\n            model_type='clean',\n            num_face=1,\n            only_center_face=False,\n            aligned=False,\n            ext='auto',\n        )\n\n    try:\n        return gfpgan_face_restorer.restore(np_image)\n    except errors.ModelNotSetUpError:\n        logger.warning(\"GFPGAN face restorer is not set up. Skipping face restoration.\")\n        return np_image"}
{"namespace": "codeformer_model.setup_model", "completion": "    global codeformer\n\n    try:\n        codeformer = FaceRestorerCodeFormer(dirname)\n    except errors.InvalidModelError as e:\n        logger.error(f\"Could not load codeformer model: {e}\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n\n    try:\n        gfpgan_face_restorer = FaceRestorerGFPGAN(dirname)\n    except Exception as e:\n        gfpgan_face_restorer = None\n        raise errors.ModelSetupError(\n            f\"Error setting up GFPGAN face restorer. Please check your model directory and try again. Full error: {e}\"\n        )\n\n"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion.\n  v = jnp.concatenate([v, [0.0]], axis=-1)\n\n  # Apply the quaternion rotation.\n  q_rotated = multiply(multiply(q, v), conjugate(q))\n\n  # Convert the quaternion back to a vector.\n  return q_rotated[Ellipsis, :3]\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  # Normalize the axis-angle vector.\n  axis_angle = jnp.asarray(axis_angle)\n  axis_angle = axis_angle / (jnp.linalg.norm(axis_angle) + eps)\n\n  # Calculate the quaternion.\n  angle = jnp.linalg.norm(axis_angle)\n  half_angle = 0.5 * angle\n  q = jnp.concatenate([\n    axis_angle * jnp.sin(half_angle),\n    jnp.cos(half_angle)\n  ])\n\n  return q\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # check if idx is the argmax\n    num_calls = k\n    if model.argmax(prefix) == idx:\n        return 0, num_calls\n\n    # initialize high\n    logit_bias = {idx: high}\n    while model.argmax(prefix, logit_bias) != idx:\n        logit_bias[idx] *= 2\n        num_calls += k\n    high = logit_bias[idx]\n\n    # improve estimate\n    low = 0\n    mid = (high + low) / 2\n    while high >= low + 1e-8:\n        logit_bias[idx] = mid\n        if model.argmax(prefix, logit_bias) == idx:\n            high = mid\n        else:\n            low = mid\n        mid = (high + low) / 2\n        num_calls += k\n    return -mid, num_calls\n\n"}
{"namespace": "resample.resample_3d", "completion": "  # Get the data shape.\n  data_shape = data.shape\n  data_volume_shape = data_shape[:3]\n  data_channels = data_shape[3:]\n\n  # Get the locations shape.\n  locations_shape = locations.shape\n  locations_batch_shape = locations_shape[:1]\n  locations_sample_shape = locations_shape[1:-1]\n\n  # Get the number of channels.\n  num_channels = data_channels.num_elements()\n\n  # Get the number of dimensions.\n  num_dimensions = 3\n\n  # Get the number of samples.\n  num_samples = locations_sample_shape.num_elements()\n\n  # Get the number of batches.\n  num_batches = locations_batch_shape.num_elements()\n\n  # Get the locations shape.\n  locations_shape = [num_batches, num_samples, num_dimensions]\n\n  # Get the data shape.\n  data_shape = [num_batches, num_samples, num_channels]\n\n  # Get the data shape.\n  data_volume_shape = [num_batches, num_samples, num_channels]\n\n  # Get the data volume shape.\n  data_volume_shape = [num_batches, num_samples, num_channels]\n\n  # Get the data volume shape.\n  data_volume_shape = [num_batches, num_samples, num_channels]\n\n  # Get the data volume shape.\n  data_volume_shape = [num_batches, num_samples, num_channels]\n\n  # Get the data volume shape.\n  data_volume_shape = [num_batches, num_samples, num_channels]\n\n  # Get the data volume shape.\n  data_volume_shape = [num_batches, num_samples, num_channels]\n\n  # Get the data volume shape.\n  data_volume_shape = [num_batches, num_samples, num_channels]\n\n  # Get the data volume shape.\n  data_volume_shape ="}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(x < tiny_val, tiny_val, x + tiny_val)\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val,\n      jnp.nextafter(jnp.float32(x), jnp.float32(x) - 1)\n  )\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: x_dot * y,\n      (min_val, 100.0),\n  )(x)\n\n"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, lambda x, y, y_dot: y_dot / y, [tiny_val, max_val])(x)\n\n"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: x_dot / (2 * y),\n      (tiny_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 1\n  elif p == 1:\n    return 2\n  elif p == 2:\n    return 3\n  elif p == 3:\n    return 4\n  elif p == 4:\n    return 5\n  elif p == 5:\n    return 6\n  elif p == 6:\n    return 7\n  elif p == 7:\n    return 8\n  elif p == 8:\n    return 9\n  elif p == 9:\n    return 10\n  elif p == 10:\n    return 11\n  elif p == 11:\n    return 12\n  elif p == 12:\n    return 13\n  elif p == 13:\n    return 14\n  elif p == 14:\n    return 15\n  elif p == 15:\n    return 16\n  elif p == 16:\n    return 17\n  elif p == 17:\n    return 18\n  elif p == 18:\n    return 19\n  elif p == 19:\n    return 20\n  elif p == 20:\n    return 21\n  elif p == 21:\n    return 22\n  elif p == 22:\n    return 23\n  elif p == 23:\n    return 24\n  elif p == 24:\n    return 25\n  elif p == 25:\n    return 26\n  elif p == 26:\n    return 27\n  elif p == 27:\n    return 28\n  elif p == 28:\n    return 29\n  elif p == 29:\n    return 30\n  elif p == 30:\n    return 31\n  elif p == 31:\n    return 32\n  elif p == 32:\n    return 33\n  elif p == 33:\n    return 34\n  elif p == 34:"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    base_verts = np.array([[1, 1, 1], [-1, -1, 1], [1, -1, -1], [-1, 1, -1]])\n    base_faces = np.array([[0, 1, 2], [0, 2, 3], [0, 3, 1], [1, 2, 3]])\n  elif base_shape == 'icosahedron':\n    phi = (1 + np.sqrt(5)) / 2\n    base_verts = np.array(\n      [\n        [0, 1, phi],\n        [0, 1, -phi],\n        [0, -1, phi],\n        [0, -1, -phi],\n        [1, phi, 0],\n        [1, -phi, 0],\n        [-1, phi, 0],\n        [-1, -phi, 0],\n        [phi, 0, 1],\n        [phi, 0, -1],\n        [-phi, 0, 1],\n        [-phi, 0, -1],\n      ]\n    )\n    base_faces = np.array(\n      [\n        [0, 2, 1],\n        [0, 3, 2],\n        [0, 4, 3],\n        [0, 5, 4],\n        [0, 1, 5],\n        [5, 10, 6],\n        [5, 6, 11],\n        [5, 11, 10],\n        [6, 7, 11],\n        [7, 8, 11],\n        [7, 10, 8],\n        [8, 10, 9],\n        [8, 9, 6],\n        [9, 2, 6],\n        [9, 1, 2],\n        [2, 1, 10],\n        [10, 3, 11],"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / jnp.maximum(tiny_val, jnp.where(x < 0, x + 1, 1)),\n      (-1, max_val),\n  )(x)\n\n"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = premult * x\n\n  if p == 1:\n    y = x\n  elif p == 0:\n    y = jnp.log(x)\n  elif p == -jnp.inf:\n    y = 1 / x\n  elif p == jnp.inf:\n    y = x ** 2\n  else:\n    y = x ** p\n\n  if postmult is not None:\n    y = y * postmult\n\n  return y\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  if premult is not None:\n    y = y / premult\n  y_safe = clip_finite_nograd(y)\n  p_safe = clip_finite_nograd(p)\n  x = select(\n      [\n          (p == 1, y),\n          (p == 0, safe_expm1(y)),\n          (p == -jnp.inf, -safe_log1p(-y)),\n          (p == jnp.inf, safe_log1p(y)),\n      ],\n      clip_finite_nograd(\n          (\n              (\n                  (jnp.abs(p_safe - 1) / p_safe)\n                  * (\n                      (jnp.abs(y_safe) + 1) ** (1 / p_safe)\n                      - 1\n                  )\n              )\n              * jnp.sign(y_safe)\n          )\n      ),\n  )\n  if postmult is not None:\n    x = x * postmult\n  return x\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  # Calculate the learning rate for the current step\n  lr = log_lerp(step / max_steps, lr_init, lr_final)\n\n  # Apply the delay if one is specified\n  if lr_delay_steps > 0:\n    lr = jnp.where(\n        step < lr_delay_steps,\n        lr * lr_delay_mult,\n        lr,\n    )\n\n  return lr\n\n"}
{"namespace": "utils.dummy_rays", "completion": "  return generate_random_rays(\n      random.PRNGKey(0),\n      1,\n      -1,\n      1,\n      0,\n      1,\n      0,\n      1,\n      0,\n      1,\n      include_exposure_idx,\n      include_exposure_values,\n      include_device_idx,\n  )\n\n"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # TODO(barron): Add support for other camera models.\n  if camtype != ProjectionType.PERSPECTIVE:\n    raise NotImplementedError(\n        'Only perspective projection is currently supported.'\n    )\n\n  # TODO(barron): Add support for batching over multiple cameras.\n  if len(pixtocams.shape) != 2 or pixtocams.shape[1] != 3 or pixtocams.shape[2] != 3:\n    raise ValueError(\n        'pixtocams must be a [3, 3] matrix, or a batch of [N, 3, 3] matrices.'\n    )\n\n  if len(camtoworlds.shape) != 3 or camtoworlds.shape[2] != 4 or camtoworlds.shape[\n      2] != 4:\n    raise ValueError(\n        'camtoworlds must be a [4, 4] matrix, or a batch of [N, 4, 4] matrices.'\n    )\n\n  # TODO(barron): Add support for batching over multiple cameras.\n  if len(points.shape) != 2 or points.shape[1] != 3:\n    raise ValueError('points must be a [3] vector, or a batch of [N, 3] vectors.')\n\n  # TODO(barron): Add support for batching over multiple cameras.\n  if len(pixtocams.shape) != 2 or pixtocams.shape[1] != 3 or pixtocams.shape[2] != 3:\n    raise ValueError(\n        'pixtocams must be a [3, 3] matrix, or a batch of [N, 3, 3] matrices.'\n    )\n\n  # TODO(barron): Add support for batching over multiple cameras.\n  if len(camtoworlds.shape) != 3 or camtoworlds.shape[2] != 4 or camtoworlds.shape[\n      2] != 4:\n    raise ValueError(\n        'camtoworld"}
{"namespace": "rigid_body.exp_se3", "completion": "  # Extract the rotation and translation components of the screw axis\n  w = screw_axis[:3]\n  v = screw_axis[3:]\n\n  # Compute the magnitude of the rotation\n  theta = jnp.linalg.norm(w)\n\n  # Compute the rotation matrix\n  R = exp_so3(w, eps)\n\n  # Compute the translation\n  p = v * theta\n\n  # Compute the homogeneous transformation matrix\n  X = rp_to_se3(R, p)\n\n  return X\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Compute theta (rotation angle) and u (rotation axis)\n  theta = jnp.linalg.norm(axis_angle)\n  u = axis_angle / (theta + eps)\n\n  # Compute the skew-symmetric cross product matrix T\n  T = skew(u)\n\n  # Compute the exponential map\n  R = jnp.eye(3) + jnp.sin(theta) * T + (1 - jnp.cos(theta)) * jnp.dot(T, T)\n\n  return R\n\n"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Calculate the mean and covariance of the Gaussian distribution.\n  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  # A more stable version of Equation 7 from https://arxiv.org/abs/2103.13415.\n  s = t0 + t1\n  d = t1 - t0\n  eps = jnp.finfo(jnp.float32).eps ** 2\n  ratio = d**2 / jnp.maximum(eps, 3 * s**2 + d**2)\n  t_mean = s * (1 / 2 + ratio)\n  t_var = (1 / 12) * d**2 - (1 / 15) * ratio**2 * (12 * s**2 - d**2)\n  r_var = (1 / 16) * s**2 + d**2 * (5 / 48 - (1 / 15) * ratio)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Compute the ray origins and directions in camera space.\n  if camtype == ProjectionType.PERSPECTIVE:\n    # Perspective pinhole camera.\n    pixvec = xnp.stack(\n        [\n            (pix_x_int - pixtocams[Ellipsis, 0, 2]) / pixtocams[Ellipsis, 0, 0],\n            (pix_y_int - pixtocams[Ellipsis, 1, 2]) / pixtocams[Ellipsis, 1, 1],\n            -xnp.ones_like(pix_x_int),\n        ],\n        axis=-1,\n    )\n    origins = xnp.broadcast_to(camtoworlds[Ellipsis, :3, 3], pixvec.shape)\n    directions = (camtoworlds[Ellipsis, :3, :3] @ pixvec[Ellipsis, None]).squeeze(\n        -1\n    )\n  elif camtype == ProjectionType.FISHEYE:\n    # Fisheye camera.\n    # Convert pixel coordinates to NDC.\n    pixvec = xnp.stack(\n        [\n            (pix_x_int - pixtocams[Ellipsis, 0, 2]) / pixtocams[Ellipsis, 0, 0],\n            (pix_y_int - pixtocams[Ellipsis, 1, 2]) / pixtocams[Ellipsis, 1, 1],\n            xnp.ones_like(pix_x_int),\n        ],\n        axis=-1,\n    )\n    # Apply inverse fisheye projection.\n    r = xnp.linalg.norm(pixvec, axis=-1, keepdims=True)\n    directions = (\n        xnp.arctan2(r, pixvec[Ellipsis, 2]) * pixvec / (r + 1e-5)\n    )  # Add epsilon to avoid"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist / jnp.linalg.norm(dirs, axis=-1)\n  return compute_alpha_weights_helper(density_delta, **kwargs)\n\n"}
{"namespace": "stepfun.sample", "completion": "  utils.assert_valid_stepfun(t, w_logits)\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n  # Compute the number of intervals.\n  num_intervals = t.shape[-1]\n  # Compute the number of samples per interval.\n  num_samples_per_interval = num_samples // num_intervals\n  # Compute the number of samples to jitter.\n  num_samples_to_jitter = num_samples - num_samples_per_interval * num_intervals\n  # Compute the center of each interval.\n  t_center = (t[Ellipsis, :-1] + t[Ellipsis, 1:]) / 2\n  # Compute the width of each interval.\n  t_width = t[Ellipsis, 1:] - t[Ellipsis, :-1]\n  # Compute the center of each sample.\n  if deterministic_center:\n    # Compute the center of each sample as the center of each interval.\n    t_sample_center = jnp.repeat(t_center, num_samples_per_interval, axis=-1)\n  else:\n    # Compute the center of each sample as the left endpoint of each interval.\n    t_sample_center = jnp.repeat(t[Ellipsis, :-1], num_samples_per_interval, axis=-1)\n  # Compute the jittered center of each sample.\n  if rng is None:\n    # Compute the center of each sample as the left endpoint of each interval.\n    t_sample_center_jittered = t_sample_center\n  else:\n    # Compute the jittered center of each sample.\n    t_sample_center_jittered = t_sample_center + jax.random.uniform(\n      rng,\n      t_sample_center.shape,\n      minval=0,\n      maxval=t_width,\n      dtype=t"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function.\n  t_samples = sample(rng, t, w_logits, num_samples, single_jitter)\n\n  # Calculate midpoints between adjacent samples.\n  t_midpoints = (t_samples[Ellipsis, 1:] + t_samples[Ellipsis, :-1]) / 2\n\n  # Adjust the first and last intervals to ensure they are within the specified domain.\n  t_samples = jnp.concatenate([jnp.minimum(domain[1], jnp.maximum(domain[0], t_samples[Ellipsis, 0:1])), t_midpoints, jnp.minimum(domain[1], jnp.maximum(domain[0], t_samples[Ellipsis, -1:]))], axis=-1)\n\n  return t_samples\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n\n  # Integrate the weights.\n  cw = integrate_weights(w)\n\n  # Interpolate the percentiles.\n  t_percentiles = math.sorted_interp(ps / 100, cw, t, utils.device_is_tpu())\n\n  return t_percentiles\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a PDF.\n  w_pdf = weight_to_pdf(t, w)\n\n  # Blur the PDF.\n  w_pdf_blurred = linspline.blur_weights(t, w_pdf, blur_halfwidth)\n\n  # Resample the PDF.\n  w_resampled = resample(tq, t, w_pdf_blurred)\n\n  return w_resampled\n\n"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Convert the input to a homogeneous representation.\n  homogeneous_vectors = to_homogeneous(vectors)\n\n  # Apply the transformation.\n  transformed_vectors = matmul(homogeneous_vectors, transform)\n\n  # Convert the output to a non-homogeneous representation.\n  return from_homogeneous(transformed_vectors)\n\n"}
{"namespace": "stepfun.resample", "completion": "  # Check if the input is valid\n  utils.assert_valid_stepfun(tp, vp)\n\n  # If the input is a single point, return the value\n  if tp.shape[-1] == 1:\n    return vp\n\n  # If the input is a single interval, return the average value\n  if tp.shape[-1] == 2:\n    return jnp.mean(vp)\n\n  # If the input is a single interval, return the average value\n  if tp.shape[-1] == 2:\n    return jnp.mean(vp)\n\n  # If the input is a single interval, return the average value\n  if tp.shape[-1] == 2:\n    return jnp.mean(vp)\n\n  # If the input is a single interval, return the average value\n  if tp.shape[-1] == 2:\n    return jnp.mean(vp)\n\n  # If the input is a single interval, return the average value\n  if tp.shape[-1] == 2:\n    return jnp.mean(vp)\n\n  # If the input is a single interval, return the average value\n  if tp.shape[-1] == 2:\n    return jnp.mean(vp)\n\n  # If the input is a single interval, return the average value\n  if tp.shape[-1] == 2:\n    return jnp.mean(vp)\n\n  # If the input is a single interval, return the average value\n  if tp.shape[-1] == 2:\n    return jnp.mean(vp)\n\n  # If the input is a single interval, return the average value\n  if tp.shape[-1] == 2:\n    return jnp.mean(vp)\n\n  # If the input is a single interval, return the average value\n  if tp.shape[-1] == 2:\n    return jnp.mean(vp)\n\n  # If the input is a single interval, return the average value\n  if tp.shape[-1] == 2"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance of the coordinates.\n  mean = mean * jnp.sqrt(var)\n  var = jnp.sqrt(var)\n\n  # Concatenate the mean and variance of the coordinates.\n  mean_var = jnp.concatenate([mean, var], axis=-1)\n\n  # Apply a sinusoidal encoding to the concatenated mean and variance.\n  mean_var = geopoly.sinusoid_encode(mean_var, min_deg, max_deg)\n\n  return mean_var\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Generate the integrated directional encoding function.\n  ide_fn = generate_ide_fn(deg_view)\n\n  def dir_enc_fn(xyz, kappa_inv):\n\n    \"\"\"\n    Function for evaluating the directional encoding for given inputs.\n\n    Input-Output Arguments\n    :param xyz: 3D point (or points) to evaluate the directional encoding for.\n    :param kappa_inv: Reciprocal of the concentration parameter of the von Mises-Fisher distribution.\n    :return: Directional encoding of the input 3D point (or points).\n    \"\"\"\n\n    # Get the integrated directional encoding.\n    ide = ide_fn(xyz, kappa_inv)\n\n    # Compute the norm of the encoding.\n    norm = jnp.sqrt(jnp.sum(ide**2, axis=-1, keepdims=True))\n\n    # Normalize the encoding.\n    return ide / norm\n\n  return dir_enc_fn\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # remove duplicate lines\n    lines = list(dict.fromkeys(lines))\n\n    # remove lines with only numbers\n    lines = [line for line in lines if not line.isnumeric()]\n\n    # remove lines with only punctuation\n    lines = [line for line in lines if not line.replace(\" \", \"\").isalpha()]\n\n    # remove lines with only numbers and punctuation\n    lines = [line for line in lines if not line.replace(\" \", \"\").replace(\",\", \"\").isalpha()]\n\n    # remove lines with only numbers and punctuation\n    lines = [line for line in lines if not line.replace(\" \", \"\").replace(\".\", \"\").isalpha()]\n\n    # remove lines with only numbers and punctuation\n    lines = [line for line in lines in lines if not line.replace(\" \", \"\").replace(\"-\", \"\").isalpha()]\n\n    # remove lines with only numbers and punctuation\n    lines = [line for line in lines if not line.replace(\" \", \"\").replace(\":\", \"\").isalpha()]\n\n    # remove lines with only numbers and punctuation\n    lines = [line for line in lines if not line.replace(\" \", \"\").replace(\";\", \"\").isalpha()]\n\n    # remove lines with only numbers and punctuation\n    lines = [line for line in lines if not line.replace(\" \", \"\").replace(\"(\", \"\").isalpha()]\n\n    # remove lines with only numbers and punctuation\n    lines = [line for line in lines if not line.replace(\" \", \"\").replace(\")\", \"\").isalpha()]\n\n    # remove lines with only numbers and punctuation\n    lines = [line for line in lines if not line.replace(\" \", \"\").replace(\"*\", \"\").isalpha()]\n\n    # remove lines with only numbers and punctuation\n    lines = [line for line in lines if not line.replace(\" \", \"\").replace(\"+\", \"\").isalpha()]\n\n    # remove lines with only numbers and punctuation\n    lines = [line for line in lines if not line.replace(\" \", \"\").replace(\"=\", \"\").isalpha()]"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    # normalize quotation marks\n    text = quotation_pattern.sub(\"'\", org_texts)\n\n    # tokenize sentences\n    sentences = nltk_tokenzier.tokenize(text)\n\n    # apply rules\n    for rule, replaced in rules:\n        sentences = [rule.sub(replaced, sent) for sent in sentences]\n\n    # handle sentences within brackets\n    sentences = [bracket_rule.sub(r\"\\1\", sent) for sent in sentences]\n\n    # handle space after punctuation\n    sentences = [space_rule.sub(r\"\\1\", sent) for sent in sentences]\n\n    return sentences\n\n"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        # Check if the token is a string\n        if not isinstance(token, str):\n            raise TypeError(\"The token argument must be a string.\")\n\n        # Check if the key is a string\n        if key is not None and not isinstance(key, str):\n            raise TypeError(\"The key argument must be a string.\")\n\n        # Check if the token is in the term dictionary\n        if token not in self.term_dict:\n            raise TermMissingError(\"The token argument is not in the term dictionary.\")\n\n        # Check if the key is in the document dictionary\n        if key is not None and key not in self.doc_lens:\n            raise TermMissingError(\"The key argument is not in the document dictionary.\")\n\n        # If a key is provided, return the positions for that document\n        if key is not None:\n            return self.posns.positions(self.term_dict.get_term_id(token), key)\n\n        # If no key is provided, return the positions for all documents\n        else:\n            return self.posns.positions(self.term_dict.get_term_id(token))\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # If the spec is a number, return it\n    if spec.isnumeric():\n        return int(spec)\n\n    # If the spec is a percentage, return the number of clauses that must match\n    if spec.endswith(\"%\"):\n        return int(float(spec[:-1]) / 100 * num_clauses)\n\n    # If the spec is a conditional expression, return the number of clauses that must match\n    if spec.startswith(\"<\"):\n        return int(num_clauses - int(spec[1:]))\n\n    # If the spec is a conditional expression, return the number of clauses that must match\n    if spec.startswith(\">\"):\n        return int(int(spec[1:]) - num_clauses)\n\n    # If the spec is a conditional expression, return the number of clauses that must match\n    if spec.startswith(\">=\"):\n        return int(num_clauses - int(spec[2:]))\n\n    # If the spec is a conditional expression, return the number of clauses that must match\n    if spec.startswith(\"<=\"):\n        return int(int(spec[2:]) - num_clauses)\n\n    # If the spec is a conditional expression, return the number of clauses that must match\n    if spec.startswith(\"=\"):\n        return int(int(spec[1:]))\n\n    # If the spec is a conditional expression, return the number of clauses that must match\n    if spec.startswith(\"!=\"):\n        return int(num_clauses - int(spec[2:]))\n\n    # If the spec is a conditional expression, return the number of clauses that must match\n    if spec.startswith(\">=\"):\n        return int(num_clauses - int(spec[2:]))\n\n    # If the spec is a conditional expression, return the number of clauses that must match\n    if spec.startswith(\"<=\"):\n        return int(int(spec[2:]) - num_clauses)\n\n    # If the spec is a conditional expression, return the number of clauses that must match\n    if spec.startswith(\"=\"):\n        return"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        # If the slop is 1 and all tokens are unique, we can use the positions of terms to calculate the phrase frequency.\n        if slop == 1 and len(tokens) == len(set(tokens)):\n            return self.posns.phrase_freq(tokens)\n        else:\n            return self.phrase_freq_general(tokens, slop)\n"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # Check dtype, raise TypeError\n        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        # Check dtype, raise TypeError\n        if not callable(tokenizer):\n            raise TypeError(\"Expected callable, got {}\".format(type(tokenizer)))\n\n        # Check dtype, raise TypeError\n        if not isinstance(truncate, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type(truncate)))\n\n        # Check dtype, raise TypeError\n        if not isinstance(batch_size, int):\n            raise TypeError(\"Expected int, got {}\".format(type(batch_size)))\n\n        # Check dtype, raise TypeError\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type(avoid_copies)))\n\n        # Check dtype, raise TypeError\n        if not isinstance(array, list):\n            array = list(array)\n\n        # Check dtype, raise TypeError\n        if not isinstance(array, list):\n            raise TypeError(\"Expected list, got {}\".format(type(array)))\n\n        # Check dtype, raise TypeError\n        if not isinstance(truncate, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type(truncate)))\n\n        # Check dtype, raise TypeError\n        if not isinstance(batch_size, int):\n            raise TypeError(\"Expected int, got {}\".format(type(batch_size)))\n\n        # Check dtype, raise TypeError\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type(avoid_copies)))\n\n        # Check dtype, raise TypeError\n        if not isinstance(array, list):\n            raise TypeError(\"Expected list, got {}\".format(type(array)))\n\n        # Check dtype, raise TypeError\n        if not isinstance(truncate, bool):\n            raise TypeError(\"Expected bool, got {}\".format(type(truncate)))\n\n        # Check dtype, raise TypeError\n        if not"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(self.config['proxyHost'], self.config['proxyPort'], self.config['strategy'], self.config['strategies'], self.config['autoCloseConnections'], self.config['multipleConnections'], self.logger)\n        self.server.start()\n        self.connections = {}\n        self.lock = threading.Lock()\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Count the number of bits set to 1 in each element of the array.\n    arr = (arr & s55) + ((arr >> np.uint64(1)) & s55)\n    arr = (arr & s33) + ((arr >> np.uint64(2)) & s33)\n    arr = (arr & s0F) + ((arr >> np.uint64(4)) & s0F)\n    arr = (arr & s01) + ((arr >> np.uint64(8)) & s01)\n    arr = (arr & s01) + ((arr >> np.uint64(16)) & s01)\n    arr = (arr & s01) + ((arr >> np.uint64(32)) & s01)\n    arr = (arr & s01) + ((arr >> np.uint64(48)) & s01)\n    arr = (arr & s01) + ((arr >> np.uint64(56)) & s01)\n    arr = (arr & s01) + ((arr >> np.uint64(64-8)) & s01)\n\n    return arr\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Parse query fields\n    query_fields = parse_field_boosts(qf)\n\n    # Parse phrase fields\n    phrase_fields = parse_field_boosts(pf) if pf is not None else {}\n    bigram_fields = parse_field_boosts(pf2) if pf2 is not None else {}\n    trigram_fields = parse_field_boosts(pf3) if pf3 is not None else {}\n\n    # Parse query terms\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    # Compute scores\n    if term_centric:\n        qf_scores, explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    # Add phrase, bigram, and trigram scores\n    if phrase_fields:\n        qf_scores += _edismax_term_centric(frame, phrase_fields, num_search_terms, search_terms, mm, similarity)[0]\n        explain += \" + \" + _edismax_term_centric(frame, phrase_fields, num_search_terms, search_terms, mm, similarity)[1]\n    if bigram_fields:\n        qf_scores += _edismax_term_centric(frame, bigram_fields, num_search_terms, search_terms, mm, similarity)[0]\n        explain += \" + \" + _edismax_term_centric(frame, bigram_fields, num_search_terms, search_terms, mm, similarity)[1]\n    if trigram_fields:\n        qf_scores += _edismax_term_centric(frame, trigram_fields, num_search_terms, search_terms, mm, similarity)[0]\n       "}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            with self.lock:\n                if self.config['multipleConnections']:\n                    connection_id = process.id\n                else:\n                    connection_id = DEFAULT_CONNECTION_ID\n\n                if connection_id not in self.connections:\n                    self.connections[connection_id] = Connection(\n                        connection_id=connection_id,\n                        server=self.server,\n                        auto_close=self.config['autoCloseConnections'],\n                        logger=self.logger\n                    )\n\n                self.connections[connection_id].c2s(message)\n\n        elif isinstance(message, RecvMessage):\n            with self.lock:\n                if self.config['multipleConnections']:\n                    connection_id = process.id\n                else:\n                    connection_id = DEFAULT_CONNECTION_ID\n\n                if connection_id not in self.connections:\n                    self.connections[connection_id] = Connection(\n                        connection_id=connection_id,\n                        server=self.server,\n                        auto_close=self.config['autoCloseConnections'],\n                        logger=self.logger\n                    )\n\n                message.data = self.connections[connection_id].s2c(message)\n\n        elif isinstance(message, CloseMessage):\n            with self.lock:\n                if self.config['multipleConnections']:\n                    connection_id = process.id\n                else:\n                    connection_id = DEFAULT_CONNECTION_ID\n\n                if connection_id in self.connections:\n                    self.connections[connection_id].close()\n                    del self.connections[connection_id]\n\n        else:\n            raise ValueError(f'Unknown message type: {type(message)}')"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        self.logger.debug('Destroying ProxifierMessageInterceptor')\n        with self.lock:\n            for connection in self.connections.values():\n                connection.close()\n            self.connections = {}\n        self.server.stop()\n"}
