{"namespace": "iris.io.validators.are_shapes_equal", "completion": "\n    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if the shapes of field1 and field2 are equal.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shapes mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        # Add the main algorithm logic here\n        pass"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "\n        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)"}
{"namespace": "tanuki.utils.encode_int", "completion": "    character_set = string.ascii_lowercase + string.digits + \"_\"\n\n    \"\"\"\n    Encodes an integer into a single character based on a predefined character set. The character set consists of lowercase letters, digits, and an underscore.\n\n    Input-Output Arguments\n    :param n: Integer. The integer to be encoded. It is used as an index to select a character from the character set.\n    :return: String. The encoded character corresponding to the input integer.\n    \"\"\""}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "\n        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        input_type_hints = [type_hints[param.name] for param in signature.parameters.values()]\n        output_type_hint = type_hints['return']\n\n        input_class_definitions = [get_class_definition(hint) for hint in input_type_hints]\n        output_class_definition = get_class_definition(output_type_hint)\n\n        function_type = FunctionType.SYMBOLIC if get_origin(output_type_hint) == Union or issubclass(output_type_hint, Embedding) else FunctionType.EMBEDDABLE\n\n        return FunctionDescription(\n            name=func_object.__name__,\n            docstring=func_object.__doc__,\n            input_type_hints=input_type_hints,\n            output_type_hint=output_type_hint,\n            input_class_definitions=input_class_definitions,\n            output_class_definition=output_class_definition,\n            type=function_type\n        )"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "\n        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        if len(loaded_bit_array) != self.size:\n            logging.warning(\"Mismatch in loaded bit array length. Reinitializing bit array and indices.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        indices = [(hash1 + i * hash2) % self.size for i in range(self.hash_count)]\n        for index in indices:\n            if not self.bit_array[index]:\n                return False\n        return True"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    weights = np.random.dirichlet(np.ones(n))\n    if zeros > 0:\n        zero_indices = np.random.choice(range(n), zeros, replace=False)\n        weights[zero_indices] = 0\n        weights /= np.sum(weights)  # Normalize the weights to sum up to one\n    return weights"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.create_model_config(json_dict.get('distilled_model', DEFAULT_STUDENT_MODELS[DEFAULT_DISTILLED_MODEL_NAME]))\n        self.current_model_stats = json_dict.get('current_model_stats', {\"trained_on_datapoints\": 0, \"running_faults\": []})\n        self.last_training_run = json_dict.get('last_training_run', {\"trained_on_datapoints\": 0})\n        self.current_training_run = json_dict.get('current_training_run', {})\n        self.nr_of_training_runs = json_dict.get('nr_of_training_runs', 0)\n        self.teacher_models = [config_factory.create_model_config(teacher_model) for teacher_model in json_dict.get('teacher_models', [DEFAULT_TEACHER_MODELS[teacher_model_name] for teacher_model_name in DEFAULT_TEACHER_MODEL_NAMES])]"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        try:\n            response = requests.post(\n                OPENAI_URL,\n                headers={\"Authorization\": f\"Bearer {self.api_key}\"},\n                json={\n                    \"model\": model.model_name,\n                    \"messages\": [\n                        {\"role\": \"system\", \"content\": system_message},\n                        {\"role\": \"user\", \"content\": prompt}\n                    ],\n                    **kwargs\n                }\n            )\n            response.raise_for_status()\n            data = response.json()\n            generated_text = data[\"choices\"][0][\"message\"][\"content\"]\n            return generated_text\n        except Exception as e:\n            print(f\"An error occurred: {e}\")\n            return None"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2 or x.shape[0] != x.shape[1]:\n        raise ValueError(\"Input matrix is not a square matrix\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric\")"}
{"namespace": "ref_utils.l2_normalize", "completion": "  x_norm = jnp.linalg.norm(x, axis=-1, keepdims=True)\n  x_norm = jnp.maximum(x_norm, grad_eps)\n  return x / x_norm"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    if word.islower():\n        return word\n    # if upper no lowers after\n    if word.isupper():\n        return word\n    # if mixed case\n    if word[0].isupper() and word[1].islower():\n        return word.capitalize()\n    if word[0].islower() and word[1].isupper():\n        return word.lower()\n    return word"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    line_text = line_text.replace(\" \", \"\")  # Remove all whitespace characters\n    tokens = line_text.split()  # Segment the modified text into smaller parts or tokens\n    return tokens"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "\n  integrated_dir_enc_fn = generate_ide_fn(deg_view)\n\n  def dir_enc_fn(xyz, kappa_inv):\n    \"\"\"\n    Function for evaluating the directional encoding for given inputs.\n\n    Args:\n    :param xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n    :param kappa_inv: [..., 1] reciprocal of the concentration parameter of the von Mises-Fisher distribution.\n\n    Returns:\n    :return: An array with the resulting directional encoding.\n    \"\"\"\n    return integrated_dir_enc_fn(xyz, kappa_inv)\n\n  return dir_enc_fn"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    cleaned_lines = []\n    cleaned_line_index = 0\n    current_block = defaultdict(list)\n    header_stack = []\n    list_stack = []\n    list_level = 0\n\n    for i, line in enumerate(lines):\n        line = line.strip()\n        if should_skip(line, xml):\n            continue\n\n        if is_table_row(line):\n            continue\n\n        if find_floating_chars(line):\n            continue\n\n        if not check_parentheses(line):\n            continue\n\n        tokens = nlm_tokenize(line)\n        if len(tokens) == 1 and tokens[0] == \"unknown\":\n            continue\n\n        if len(tokens) > 0:\n            if tokens not in current_block[\"text\"]:\n                current_block[\"text\"].append(tokens)\n                current_block[\"index\"] = cleaned_line_index\n                cleaned_line_index += 1\n\n    if current_block:\n        cleaned_lines.append(current_block)\n\n    return cleaned_lines"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or org_texts == \"\":\n        return [org_texts]\n\n    # Apply predefined rules and patterns to tokenize and normalize the text\n    for rule, replaced in rules:\n        org_texts = rule.sub(replaced, org_texts)\n\n    org_texts = bracket_rule.sub(r\"\\1\", org_texts)  # Remove brackets and keep the content\n    org_texts = space_rule.sub(r\"\\1\", org_texts)  # Remove space between punctuations\n    org_texts = quotation_pattern.sub('\"', org_texts)  # Normalize quotation marks\n\n    # Use the NLTK tokenizer to tokenize the text into sentences\n    tokenized_sentences = nltk_tokenzier.tokenize(org_texts)\n\n    return tokenized_sentences"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        if key is not None:\n            return self[key].positions(token)\n        else:\n            return [doc.positions(token) for doc in self]"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    if spec.isdigit():\n        return int(spec)\n    elif spec.endswith('%'):\n        percentage = int(spec[:-1])\n        return int(num_clauses * (percentage / 100))\n    elif '<' in spec:\n        left, right = spec.split('<')\n        left = int(left)\n        right = int(right)\n        return max(left, int(num_clauses * (right / 100)))\n    else:\n        raise ValueError(\"Invalid 'min should match' specification\")"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        if slop == 1 and len(set(tokens)) == len(tokens):\n            return compute_phrase_freqs(self, tokens)\n        else:\n            return scan_merge_ins(self, tokens, slop)"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "\n        if not callable(tokenizer):\n            raise TypeError(\"The 'tokenizer' parameter must be a callable function\")\n\n        if not isinstance(truncate, bool):\n            raise TypeError(\"The 'truncate' parameter must be a boolean\")\n\n        if not isinstance(batch_size, numbers.Integral):\n            raise TypeError(\"The 'batch_size' parameter must be an integer\")\n\n        if not isinstance(avoid_copies, bool):\n            raise TypeError(\"The 'avoid_copies' parameter must be a boolean\")\n\n        if not is_list_like(array):\n            raise TypeError(\"The 'array' parameter must be a list-like object\")\n\n        postings = []\n        for batch_start in range(0, len(array), batch_size):\n            batch_end = min(batch_start + batch_size, len(array))\n            batch = array[batch_start:batch_end]\n            tokenized_batch = [tokenizer(item) for item in batch]\n            postings.extend(tokenized_batch)\n\n        return cls(postings, tokenizer, avoid_copies)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "class ProxifierMessageInterceptor:\n    def __init__(self):\n        self.server = Server(self.config['serverHost'], self.config['serverPort'], self.logger)\n        self.connections = {}\n        self.lock = threading.Lock()\n        self.server.start()"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr -= (arr >> _1) & s55\n    arr = (arr & m1) + ((arr >> _2) & m1)\n    arr = (arr + (arr >> _4)) & s33\n    arr += arr >> _8\n    arr += arr >> _16\n    arr += arr >> _32\n    arr &= mask\n    return arr"}
{"namespace": "searcharray.solr.edismax", "completion": "    query_fields = parse_field_boosts(qf)\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    if term_centric:\n        return _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        return _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if isinstance(message, SendMessage):\n            with self.lock:\n                connection = self.connections.get(process.id)\n                if connection:\n                    message.data = connection.c2s(message.data)\n        elif isinstance(message, RecvMessage):\n            with self.lock:\n                connection = self.connections.get(process.id)\n                if connection:\n                    message.data = connection.s2c(message.data)\n        elif isinstance(message, CloseMessage):\n            with self.lock:\n                connection = self.connections.pop(process.id, None)\n                if connection and self.config['autoCloseConnections']:\n                    connection.close()"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        with self.lock:\n            for connection_id, connection in self.connections.items():\n                connection.close()\n            self.connections.clear()\n        if self.server:\n            self.server.stop()"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert_is_symmetric(cov)\n    std_dev = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std_dev, std_dev)\n    return corr, std_dev"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    if not np.all(np.diagonal(x) == 0):\n        raise ValueError(\"The matrix diagonal elements must be zero\")\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "\n        # Determine the appropriate language model based on function description and arguments\n        model, suitable_for_distillation = self.function_modeler.get_model(function_description, args, kwargs)\n\n        # Check if the function is already initialized and does not require saving examples for fine-tuning\n        initialized = func_hash in self.initialized_functions\n\n        # Construct the prompt to be used for generation\n        prompt = self.function_modeler.construct_prompt(function_description, args, kwargs)\n\n        return prompt, model, suitable_for_distillation, initialized"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    if higham:\n        return _nearest_corr_higham(cov, higham_max_iteration)\n    else:\n        return _nearest_corr_eigen(cov)"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for arr in x:\n        if len(arr) > 1:\n            mid = len(arr) // 2\n            yield [arr[:mid], arr[mid:]]"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    if percent:\n        x *= 100\n    if abs(x) < 1:\n        return f\"{x:.4f}\"\n    elif abs(x) < 10:\n        return f\"{x:.3f}\"\n    elif abs(x) < 100:\n        return f\"{x:.2f}\"\n    else:\n        return f\"{x:.1f}\""}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(\"assets_names must be provided when items is a dictionary\")\n        array = np.full((n_assets,), fill_value)\n        for i, asset_name in enumerate(assets_names):\n            if asset_name in items:\n                array[i] = items[asset_name]\n    else:\n        array = np.asarray(items)\n\n    if dim == 1:\n        if array.shape != (n_assets,):\n            raise ValueError(f\"The shape of {name} must be ({n_assets},) for dim=1\")\n    elif dim == 2:\n        if array.shape[1] != n_assets:\n            raise ValueError(f\"The second dimension of {name} must be {n_assets} for dim=2\")\n    else:\n        raise ValueError(\"dim must be either 1 or 2\")\n\n    return array"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", Path.home() / \"skfolio_data\")\n    data_home = Path(data_home).expanduser()\n    if not data_home.exists():\n        data_home.mkdir(parents=True)\n    return str(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home, ignore_errors=True)\n    os.makedirs(data_home, exist_ok=True)"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes, int, float, bool, type(None))):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, (list, tuple)):\n        res = [flatten_to_tuple(k) for k in obj]\n        values, sizes = ListSchema._concat([k[0] for k in res])\n        return values, ListSchema([k[1] for k in res], sizes)\n    elif isinstance(obj, collections.abc.Mapping):\n        keys = list(obj.keys())\n        values = [obj[k] for k in keys]\n        res, schema = ListSchema.flatten(values)\n        return res, DictSchema(schema.schemas, schema.sizes, keys)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, (Boxes, ROIMasks)):\n        return TensorWrapSchema.flatten(obj)\n    else:\n        raise NotImplementedError(f\"Flattening {type(obj)} is not supported.\")"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    # Initialize an empty list to store the left and right matrices\n    left_matrices = []\n    right_matrices = []\n\n    # Loop through each equation\n    for equation in equations:\n        # Initialize an empty list to store the coefficients of the equation\n        coefficients = []\n        # Split the equation into individual terms\n        terms = re.split(r\"([+-])\", equation)\n        # Remove any empty strings from the list of terms\n        terms = [term for term in terms if term]\n\n        # Loop through each group to check for coefficients\n        for group in groups:\n            # Initialize the coefficient for the group\n            coefficient = 0\n            # Loop through each term in the equation\n            for term in terms:\n                # Check if the group is mentioned in the term\n                if group in term:\n                    # Extract the coefficient from the term\n                    if term.startswith(\"-\"):\n                        coefficient -= 1\n                    elif term.startswith(\"+\"):\n                        coefficient += 1\n                    else:\n                        coefficient += 1\n            # Append the coefficient to the list of coefficients\n            coefficients.append(coefficient)\n\n        # Append the coefficients as a row in the left matrix\n        left_matrices.append(coefficients)\n        # Extract the constant term from the equation and append it to the right matrix\n        constant = int(terms[-1])\n        right_matrices.append(constant)\n\n    # Convert the lists of matrices to numpy arrays\n    left_matrix = np.array(left_matrices)\n    right_matrix = np.array(right_matrices)\n\n    # Check if any of the groups in the equations are missing from the input groups\n    missing_groups = [eq_group for eq_group in re.findall(r\"\\b\\w+\\b\", \" \".join(equations)) if eq_group not in np.concatenate(groups)]\n    if missing_groups:\n        if raise_if_group_missing:\n            raise GroupNotFoundError(f\"The following groups mentioned in the {names[1]} are missing from the {names[0]}: {missing_groups}\")\n        else:\n            warnings.warn(f\"The following groups mentioned in the {names[1]} are missing from the {names[0]}: {missing_groups}\", UserWarning)\n        return None\n\n    # Return the left and right matrices\n    return left_matrix, right_matrix"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "\n    class_name = \"NewInstances\" + str(_counter)\n    _counter += 1\n\n    # Define the new class\n    class NewInstances(Instances):\n        def __init__(self, image_size):\n            super().__init__(image_size)\n            for field_name, field_type in fields:\n                setattr(self, field_name, field_type)\n\n    # Add from_instances method to the new class\n    _add_instances_conversion_methods(NewInstances)\n\n    # Create a new module for the new class\n    module_code = f\"\"\"\n    import torch\n    from detectron2.structures import Instances\n    from copy import deepcopy\n\n    class {class_name}(Instances):\n        def __init__(self, image_size):\n            super().__init__(image_size)\n            for field_name, field_type in {fields}:\n                setattr(self, field_name, field_type)\n\n    def from_instances(instances: Instances):\n        fields = instances.get_fields()\n        image_size = instances.image_size\n        ret = {class_name}(image_size)\n        for name, val in fields.items():\n            assert hasattr(ret, f\"_{name}\"), f\"No attribute named {name} in {class_name}\"\n            setattr(ret, name, deepcopy(val))\n        return ret\n\n    {class_name}.from_instances = from_instances\n    \"\"\"\n\n    # Write the module to a temporary file\n    with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".py\", delete=False) as f:\n        f.write(module_code)\n        module_file = f.name\n\n    # Import the new module\n    new_module = _import_file(module_file, class_name)\n\n    # Set up the environment for torchscript to recognize the new class\n    with ExitStack() as stack:\n        stack.enter_context(mock.patch.dict(sys.modules, {new_module.__name__: new_module}))\n        stack.enter_context(mock.patch(\"torch.jit._recursive.concrete_type_store.type_store\", clear=True))\n        stack.enter_context(mock.patch(\"torch.jit._state._jit_caching_layer\", clear=True))\n\n        yield new_module\n\n    # Clean up by removing modifications\n    os.remove(module_file)\n    _clear_jit_cache()"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    class FreezeTrainingModeContext:\n        def __enter__(self):\n            for module in model.modules():\n                if hasattr(module, \"training\"):\n                    module.__orig_training = module.training\n                    module.training = False\n\n        def __exit__(self, exc_type, exc_value, traceback):\n            for module in model.modules():\n                if hasattr(module, \"__orig_training\"):\n                    module.training = module.__orig_training\n                    del module.__orig_training\n\n    return FreezeTrainingModeContext()"}
{"namespace": "spin_math.safe_sqrt", "completion": "  return jnp.where(x < eps, jnp.sqrt(value_at_zero), jnp.sqrt(x))"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  homogeneous_vectors = to_homogeneous(vectors)\n  transformed_vectors = jnp.matmul(homogeneous_vectors, transform.T)\n  return from_homogeneous(transformed_vectors)"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "stepfun.resample", "completion": "  # Find the indices of the original time points in the new time points\n  indices = jnp.searchsorted(t, tp) - 1\n  indices = jnp.clip(indices, 0, len(t) - 1)\n\n  # Calculate the width of each interval in the new time points\n  widths = jnp.diff(t)\n\n  # Use the indices to assign the original values to the new intervals\n  resampled_values = jnp.zeros_like(t)\n  jnp.add.at(resampled_values, indices, vp)\n\n  # If use_avg is True, divide the resampled values by the width of each interval\n  if use_avg:\n    resampled_values /= widths\n\n  return resampled_values"}
{"namespace": "coord.contract", "completion": "  # Implementation of the contract function goes here\n  magnitude_squared = jnp.sum(x**2, axis=-1, keepdims=True)\n  scale = 1.0 / (1.0 + magnitude_squared)\n  return x * scale"}
{"namespace": "coord.inv_contract", "completion": "  x_mag_sq = jnp.sum(z**2, axis=-1, keepdims=True)\n  scale = 1 / (1 - jnp.sqrt(1 - x_mag_sq))\n  x = scale * z\n  return x"}
{"namespace": "grid_utils.trilerp", "completion": "\n  if datastructure == 'grid':\n      return resample.trilinear_interpolation(values, coordinates)\n  elif datastructure == 'hash':\n      return hash_resample.trilinear_interpolation(values, coordinates)\n  else:\n      raise ValueError(\"Invalid datastructure. Supported data structures are 'grid' and 'hash'.\")"}
{"namespace": "coord.track_linearize", "completion": "  # Linearize the function around the mean\n  fn_mean = fn(mean)\n\n  # Compute the Jacobian of the function at the mean\n  jacobian = jax.jacobian(fn)(mean)\n\n  # Transform the covariances using the Jacobian\n  fn_cov = jnp.einsum('...ij,...jk->...ik', jnp.einsum('...ij,...jk->...ik', jacobian, cov), jacobian)\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Clamping to 1 produces correct scale inside |x| < 1\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n  z = scale * x\n  return z"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  encoded = expected_sin(\n      jnp.concatenate([scaled_x, scaled_x + 0.5 * jnp.pi], axis=-1),\n      jnp.concatenate([jnp.zeros_like(scaled_x)] * 2, axis=-1),\n  )\n  if append_identity:\n    encoded = jnp.concatenate([x, encoded], axis=-1)\n  return encoded"}
{"namespace": "coord.integrated_pos_enc", "completion": "  scaled_mean = mean * (2 ** jnp.linspace(min_deg, max_deg - 1, mean.shape[-1]))\n  scaled_var = var * (2 ** (2 * jnp.linspace(min_deg, max_deg - 1, var.shape[-1])))\n\n  # Concatenate the scaled mean and variance\n  concatenated = jnp.concatenate([scaled_mean, scaled_var], axis=-1)\n\n  # Apply sinusoidal encoding\n  encoding = jnp.sin(concatenated)\n\n  return encoding"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # Compute the determinant of the covariance matrix\n    det = jnp.linalg.det(cov)\n    # Compute the isotropic scaling factor\n    scale = jnp.abs(det) ** (1 / cov.shape[-1])\n    # Construct the isotropic covariance matrix\n    isotropic_cov = scale * jnp.eye(cov.shape[-1])\n  elif mode == 'accurate':\n    # Compute the logarithm of the determinant for stability\n    log_det = jnp.log(jnp.abs(jnp.linalg.det(cov)))\n    # Compute the isotropic scaling factor\n    scale = jnp.exp(log_det / cov.shape[-1])\n    # Construct the isotropic covariance matrix\n    isotropic_cov = scale * jnp.eye(cov.shape[-1])\n  else:\n    raise ValueError(\"Invalid mode. Mode must be 'fast' or 'accurate'.\")\n  \n  return isotropic_cov"}
{"namespace": "coord.construct_ray_warps", "completion": "    if fn_inv is None:\n      # Attempt to automatically determine the inverse based on a predefined mapping of functions to their inverses\n      fn_inv = {\n          contract: inv_contract,\n          contract3_isoscale: inv_contract,\n          # Add more function-inverse pairs as needed\n      }.get(fn, None)\n\n    def t_to_s(t):\n      \"\"\"Maps metric distances to normalized distances in the range [0, 1].\"\"\"\n      return (fn(t) - fn(t_near)) / (fn(t_far) - fn(t_near))\n\n    def s_to_t(s):\n      \"\"\"Maps normalized distances back to metric distances.\"\"\"\n      return fn_inv(s * (fn(t_far) - fn(t_near)) + fn(t_near))\n\n    return t_to_s, s_to_t"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        if self.tcnn:\n            if n_neurons <= 64:\n                # Create network using tinycudann\n                pass\n            else:\n                # Create network using PyTorch\n                layers = []\n                layers.append(nn.Linear(n_input_dims, n_neurons))\n                layers.append(nn.ReLU() if activation == \"ReLU\" else nn.Identity())\n                for _ in range(n_layers - 2):\n                    layers.append(nn.Linear(n_neurons, n_neurons))\n                    layers.append(nn.ReLU() if activation == \"ReLU\" else nn.Identity())\n                layers.append(nn.Linear(n_neurons, n_output_dims))\n                if output_activation == \"ReLU\":\n                    layers.append(nn.ReLU())\n                elif output_activation == \"Sigmoid\":\n                    layers.append(nn.Sigmoid())\n                else:\n                    layers.append(nn.Identity())\n                return nn.Sequential(*layers)\n        else:\n            # Create network using tinycudann\n            pass"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    areas = [area(polygon) for polygon in polygons]\n    max_area = max(areas)\n    filtered_polygons = [polygon for polygon, polygon_area in zip(polygons, areas) if polygon_area >= max_area * rel_tr and polygon_area >= abs_tr]\n    return filtered_polygons"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        rolling_medians = np.zeros(len(signal) - 2 * kernel_offset)\n        for i in range(len(signal) - 2 * kernel_offset):\n            rolling_medians[i] = np.median(signal[i:i + 2 * kernel_offset + 1])\n        return rolling_medians"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "\n    # Extract iris bits and mask bits from the probe and gallery templates\n    irisbits_probe = template_probe.iriscode\n    maskbits_probe = template_probe.mask\n    irisbits_gallery = template_gallery.iriscode\n    maskbits_gallery = template_gallery.mask\n\n    # Calculate the half width of the iris codes\n    half_width = [irisbits_probe.shape[1] // 2, irisbits_gallery.shape[1] // 2]\n\n    # Calculate the total bit counts\n    sqrt_totalbitcount, sqrt_totalbitcount_top, sqrt_totalbitcount_bot = count_sqrt_totalbits(\n        irisbits_probe.size + irisbits_gallery.size, half_width, weights\n    )\n\n    # Calculate the nonmatch bits for Hamming distance\n    irisbitcount_probe_top, maskbitcount_probe_top, irisbitcount_probe_bot, maskbitcount_probe_bot = count_nonmatchbits(\n        irisbits_probe, maskbits_probe, half_width, weights\n    )\n    irisbitcount_gallery_top, maskbitcount_gallery_top, irisbitcount_gallery_bot, maskbitcount_gallery_bot = count_nonmatchbits(\n        irisbits_gallery, maskbits_gallery, half_width, weights\n    )\n\n    # Calculate the normalized Hamming distance if nm_dist is provided\n    if nm_dist is not None:\n        norm_HD_probe_top = normalized_HD(\n            irisbitcount_probe_top, maskbitcount_probe_top, sqrt_totalbitcount_top, nm_dist\n        )\n        norm_HD_probe_bot = normalized_HD(\n            irisbitcount_probe_bot, maskbitcount_probe_bot, sqrt_totalbitcount_bot, nm_dist\n        )\n        norm_HD_gallery_top = normalized_HD(\n            irisbitcount_gallery_top, maskbitcount_gallery_top, sqrt_totalbitcount_top, nm_dist\n        )\n        norm_HD_gallery_bot = normalized_HD(\n            irisbitcount_gallery_bot, maskbitcount_gallery_bot, sqrt_totalbitcount_bot, nm_dist\n        )\n\n        # Calculate the minimum normalized Hamming distance and the corresponding rotation shift\n        min_norm_HD = min(\n            norm_HD_probe_top + norm_HD_gallery_top, norm_HD_probe_top + norm_HD_gallery_bot,\n            norm_HD_probe_bot + norm_HD_gallery_top, norm_HD_probe_bot + norm_HD_gallery_bot\n        )\n        min_rotation_shift = rotation_shift\n\n        return min_norm_HD, min_rotation_shift\n\n    else:\n        # Calculate the Hamming distance without normalization\n        HD_probe_top = irisbitcount_probe_top + irisbitcount_gallery_top - 2 * (irisbits_probe[:, -rotation_shift:] & irisbits_gallery[:, :rotation_shift]).sum()\n        HD_probe_bot = irisbitcount_probe_bot + irisbitcount_gallery_bot - 2 * (irisbits_probe[:, :rotation_shift] & irisbits_gallery[:, -rotation_shift:]).sum()\n        HD_gallery_top = irisbitcount_probe_top + irisbitcount_gallery_top - 2 * (irisbits_probe[:, :rotation_shift] & irisbits_gallery[:, -rotation_shift:]).sum()\n        HD_gallery_bot = irisbitcount_probe_bot + irisbitcount_gallery_bot - 2 * (irisbits_probe[:, -rotation_shift:] & irisbits_gallery[:, :rotation_shift]).sum()\n\n        # Calculate the minimum Hamming distance and the corresponding rotation shift\n        min_HD = min(HD_probe_top + HD_gallery_top, HD_probe_top + HD_gallery_bot, HD_probe_bot + HD_gallery_top, HD_probe_bot + HD_gallery_bot)\n        min_rotation_shift = rotation_shift\n\n        return min_HD, min_rotation_shift"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have the shape (_, 2)\")\n\n    n = len(array)\n    area = 0.0\n    for i in range(n):\n        j = (i + 1) % n\n        area += array[i, 0] * array[j, 1]\n        area -= array[j, 0] * array[i, 1]\n\n    area = abs(area) / 2.0\n    return area"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "        # Implementation of the function goes here\n        # Calculate perpendicular bisectors and return the starting and ending points\n        # Handle the case where the function fails to find a sufficient number of point pairs within the maximum iterations allowed"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    distances = np.linalg.norm(np.diff(polygon, axis=0), axis=1)\n    valid_distances = distances[distances <= max_point_distance]\n    total_length = np.sum(valid_distances)\n    return float(total_length)"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(f\"{cls.__name__}: {field.name} must contain only boolean values.\")\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points with shape (_, 2).\")\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    if isinstance(v, Iterable):\n        for val in v:\n            if val <= 0:\n                raise ValueError(f\"{cls.__name__}: {field.name} must contain only positive values.\")\n    else:\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be a positive value.\")\n\n    return v"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min = values.get(\"x_min\")\n    x_max = values.get(\"x_max\")\n    y_min = values.get(\"y_min\")\n    y_max = values.get(\"y_max\")\n\n    if x_min >= x_max:\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max.\")\n\n    if y_min >= y_max:\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max.\")\n\n    return values"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "\n    def validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"Check if np.ndarray has a specific number of dimensions.\n\n        Args:\n            cls (type): Class type.\n            v (np.ndarray): Value to check.\n            field (fields.ModelField): Field descriptor.\n\n        Raises:\n            ValueError: Exception raised if array doesn't have the specified number of dimensions.\n\n        Returns:\n            np.ndarray: `v` sent for further processing.\n        \"\"\"\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions.\")\n\n        return v\n\n    return validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if field1.shape equals field2.shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(f\"{cls.__name__}: {field1} and {field2} length mismatch.\")\n        for arr1, arr2 in zip(values[field1], values[field2]):\n            if arr1.shape != arr2.shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch.\")\n        return values\n\n    return __root_validator"}
{"namespace": "math.safe_log", "completion": "  return generate_safe_fn(jnp.log, lambda x, y, x_dot: x_dot / x, (tiny_val, max_val))(x)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: 0.5 * x_dot / y,\n      (0, max_val),\n  )(x)"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p > 1:\n    return jnp.inf\n  elif p == 1:\n    return 1\n  elif p > 0:\n    return jnp.sign(p)\n  else:\n    return 0"}
{"namespace": "geopoly.generate_basis", "completion": "\n  if base_shape == 'tetrahedron':\n    base_verts = np.array([\n        [1, 1, 1],\n        [1, -1, -1],\n        [-1, 1, -1],\n        [-1, -1, 1],\n    ], dtype=np.float32)\n    base_faces = np.array([\n        [0, 1, 2],\n        [0, 1, 3],\n        [0, 2, 3],\n        [1, 2, 3],\n    ], dtype=np.int32)\n  elif base_shape == 'icosahedron':\n    phi = (1 + np.sqrt(5)) / 2\n    base_verts = np.array([\n        [0, 1, phi],\n        [0, -1, phi],\n        [0, 1, -phi],\n        [0, -1, -phi],\n        [1, phi, 0],\n        [-1, phi, 0],\n        [1, -phi, 0],\n        [-1, -phi, 0],\n        [phi, 0, 1],\n        [-phi, 0, 1],\n        [phi, 0, -1],\n        [-phi, 0, -1],\n    ], dtype=np.float32)\n    base_faces = np.array([\n        [0, 1, 4],\n        [0, 1, 5],\n        [0, 4, 8],\n        [0, 5, 8],\n        [0, 4, 9],\n        [0, 5, 10],\n        [1, 4, 9],\n        [1, 5, 10],\n        [1, 4, 6],\n        [1, 5, 7],\n        [1, 6, 11],\n        [1, 7, 11],\n        [2, 3, 6],\n        [2, 3, 7],\n        [2, 6, 10],\n        [2, 7, 11],\n        [2, 6, 9],\n        [2, 7, 8],\n        [2, 8, 10],\n        [2, 9, 11],\n        [3, 6, 9],\n        [3, 7, 8],\n        [3, 6, 10],\n        [3, 7, 11],\n        [3, 8, 9],\n        [3, 8, 10],\n        [4, 5, 8],\n        [4, 5, 9],\n        [5, 8, 10],\n        [5, 9, 11],\n        [6, 9, 10],\n        [6, 10, 11],\n        [7, 8, 10],\n        [7, 9, 11],\n    ], dtype=np.int32)\n  elif base_shape == 'octahedron':\n    base_verts = np.array([\n        [1, 0, 0],\n        [-1, 0, 0],\n        [0, 1, 0],\n        [0, -1, 0],\n        [0, 0, 1],\n        [0, 0, -1],\n    ], dtype=np.float32)\n    base_faces = np.array([\n        [0, 2, 4],\n        [0, 3, 4],\n        [0, 2, 5],\n        [0, 3, 5],\n        [1, 2, 4],\n        [1, 3, 4],\n        [1, 2, 5],\n        [1, 3, 5],\n    ], dtype=np.int32)\n  else:\n    raise ValueError(f'base_shape {base_shape} not supported')\n\n  verts = tesselate_geodesic(base_verts, base_faces, angular_tesselation, eps)\n\n  if remove_symmetries:\n    sq_dist = compute_sq_dist(verts.T)\n    assignment = np.array([np.min(np.argwhere(d <= eps)) for d in sq_dist])\n    unique = np.unique(assignment)\n    verts = verts[unique, :]\n\n  return verts.T"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + x),\n      (-1, max_val - 1),\n  )(x)"}
{"namespace": "math.power_ladder", "completion": "  if premult is not None:\n    x = x * premult\n\n  if p == 1:\n    y = x\n  elif p == 0:\n    y = jnp.log(x)\n  elif p == -jnp.inf:\n    y = safe_sign(x) * jnp.log(jnp.abs(x))\n  elif p == jnp.inf:\n    y = safe_sign(x) * jnp.exp(x)\n  else:\n    y = safe_sign(x) * jnp.power(jnp.abs(x), p)\n\n  if postmult is not None:\n    y = y * postmult\n\n  return y"}
{"namespace": "math.inv_power_ladder", "completion": "    if postmult is not None:\n        y = y * postmult\n    yp = jnp.abs(y)\n    ys = yp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n    p_safe = clip_finite_nograd(remove_zero(p))\n    x = safe_sign(y) * select(\n        [\n            (p == 1, yp),\n            (p == 0, safe_exp(yp) - 1),\n            (p == -jnp.inf, -safe_log1p(-yp)),\n            (p == jnp.inf, safe_log1p(yp)),\n        ],\n        clip_finite_nograd(\n            (jnp.power(1 + yp * safe_sign(p) * (p - 1) / jnp.abs(p - 1), 1 / p_safe) - 1) * jnp.abs(p - 1) / p_safe\n        ),\n    )\n    if premult is not None:\n        x = x * premult\n    return x"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Convert origins and directions to homogeneous coordinates\n  origins_h = xnp.concatenate([origins, xnp.ones((origins.shape[0], 1), dtype=xnp.float32)], axis=1)\n  directions_h = xnp.concatenate([directions, xnp.zeros((directions.shape[0], 1), dtype=xnp.float32)], axis=1)\n\n  # Apply the inverse intrinsic matrix to the origins and directions\n  origins_cam = xnp.dot(origins_h, pixtocam.T)\n  directions_cam = xnp.dot(directions_h, pixtocam.T)\n\n  # Calculate the NDC coordinates\n  origins_ndc = origins_cam[:, :2] / origins_cam[:, 2, None]\n  directions_ndc = directions_cam[:, :2] / origins_cam[:, 2, None]\n\n  return origins_ndc, directions_ndc"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  tck = scipy.interpolate.splrep(t_input, x, k=min(spline_degree, len(x) - 1), s=smoothness)\n  return scipy.interpolate.splev(t_output, tck)"}
{"namespace": "math.learning_rate_decay", "completion": "  if step < lr_delay_steps:\n    lr = lr_init * lr_delay_mult\n  else:\n    progress = min(step / max_steps, 1.0)\n    lr = log_lerp(progress, lr_init, lr_final)\n  return lr"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])"}
{"namespace": "utils.dummy_rays", "completion": "  rng = jax.random.PRNGKey(0)  # Initialize random number generator\n  n = 1000  # Number of rays to generate\n  origin_lo = -1.0  # Lower bound for ray origins\n  origin_hi = 1.0  # Upper bound for ray origins\n  radius_lo = 0.1  # Lower bound for ray radii\n  radius_hi = 0.5  # Upper bound for ray radii\n  near_lo = 0.1  # Lower bound for near plane\n  near_hi = 1.0  # Upper bound for near plane\n  far_lo = 3.0  # Lower bound for far plane\n  far_hi = 5.0  # Upper bound for far plane\n\n  return generate_random_rays(\n      rng,\n      n,\n      origin_lo,\n      origin_hi,\n      radius_lo,\n      radius_hi,\n      near_lo,\n      near_hi,\n      far_lo,\n      far_hi,\n      include_exposure_idx,\n      include_exposure_values,\n      include_device_idx,\n  )"}
{"namespace": "camera_utils.points_to_pixels", "completion": "\n  # Must add half pixel offset to shoot rays through pixel centers.\n  def dir_to_pix(x, y):\n    return x - 0.5, y - 0.5\n\n  # Apply camera rotation matrices.\n  camera_points = points @ camtoworlds[Ellipsis, :3, :3]\n  camera_origins = camtoworlds[Ellipsis, :3, -1]\n\n  # Apply camera intrinsics.\n  pixel_points = camera_points @ pixtocams.T\n  pixel_origins = camera_origins @ pixtocams.T\n\n  if distortion_params is not None:\n    # Apply distortion correction.\n    x, y = _radial_and_tangential_distort(\n        pixel_points[Ellipsis, 0],\n        pixel_points[Ellipsis, 1],\n        **distortion_params,\n    )\n    pixel_points = xnp.stack([x, y], axis=-1)\n\n  # Convert to pixel coordinates.\n  pixel_x, pixel_y = pixel_points[Ellipsis, 0], pixel_points[Ellipsis, 1]\n  pixel_x_orig, pixel_y_orig = pixel_origins[Ellipsis, 0], pixel_origins[Ellipsis, 1]\n  pixel_x, pixel_y = dir_to_pix(pixel_x, pixel_y)\n  pixel_x_orig, pixel_y_orig = dir_to_pix(pixel_x_orig, pixel_y_orig)\n\n  # Compute depth values.\n  depth = xnp.linalg.norm(points - camera_origins, axis=-1)\n\n  return (pixel_x, pixel_y), depth"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # Initialize variables to store results and execution times\n    results = []\n    execution_times = []\n\n    # Iterate through each module and its parameters\n    for module, params in zip(modules, module_params):\n        # Make a deep copy of the previous result to avoid modifying the original dataframe\n        expanded_result = deepcopy(previous_result)\n\n        # Execute the query expansion module with the specified parameters\n        expanded_result = module(expanded_result, **params)\n\n        # Measure the execution time of the module\n        time_taken = measure_speed(module, expanded_result, **params)\n        execution_times.append((module.__name__, time_taken))\n\n        # Evaluate the performance of the expanded result based on the specified strategies\n        evaluated_result = evaluate_retrieval_node(expanded_result, strategies)\n\n        # Store the evaluated result\n        results.append((module.__name__, evaluated_result))\n\n    # Save the execution times to a file\n    execution_times_df = pd.DataFrame(execution_times, columns=[\"Module\", \"Execution Time\"])\n    execution_times_df.to_csv(os.path.join(node_line_dir, \"execution_times.csv\"), index=False)\n\n    # Select the best result based on the specified strategies\n    best_result = select_best_average(results, strategies)\n\n    # Save the best result to a file\n    best_result[1].to_csv(os.path.join(node_line_dir, \"best_result.csv\"), index=False)\n\n    return best_result[1]"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create necessary directories\n    os.makedirs(node_line_dir, exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'prompt_maker'), exist_ok=True)\n    os.makedirs(os.path.join(node_line_dir, 'summary'), exist_ok=True)\n\n    # Initialize variables\n    best_module = None\n    best_metrics = None\n    best_execution_time = float('inf')\n    best_result = None\n\n    # Iterate through prompt maker modules and evaluate their performance\n    for i, module in enumerate(modules):\n        params = module_params[i]\n        support_modules = get_support_modules(module)\n\n        # Generate combinations of parameters\n        param_combinations = make_combinations(params)\n\n        for combination in param_combinations:\n            # Execute the prompt maker module with the current combination of parameters\n            result, execution_time = module(**combination, support_modules=support_modules)\n\n            # Evaluate the performance of the prompt maker module\n            metrics = evaluate_generation(result, previous_result, strategies.get('generator', None))\n\n            # Check if the current module is the best based on the specified strategies\n            if (measure_speed(execution_time, strategies.get('speed_threshold', float('inf'))) and\n                    select_best_average(metrics, strategies.get('metrics', ['f1_score']))):\n                best_module = module\n                best_metrics = metrics\n                best_execution_time = execution_time\n                best_result = result\n\n    # Save the best prompt maker's output and summary\n    best_result.to_csv(os.path.join(node_line_dir, 'prompt_maker', 'best_prompt_maker_output.csv'), index=False)\n    summary = pd.DataFrame({'best_module': [best_module.__name__],\n                            'best_metrics': [cast_metrics(best_metrics)],\n                            'best_execution_time': [best_execution_time]})\n    summary.to_csv(os.path.join(node_line_dir, 'summary', 'summary.csv'), index=False)\n\n    # Combine the best prompt maker's output with the previous result\n    combined_result = pd.concat([previous_result, best_result], ignore_index=True)\n\n    return combined_result"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "\n    extracted_values = []\n    for node in nodes:\n        values = extract_values(node, key)\n        extracted_values.extend(values)\n\n    return list(set(extracted_values))"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "\n    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path)\n\n    for col in dict_columns:\n        summary_df[col] = summary_df[col].apply(ast.literal_eval)\n\n    return summary_df"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        module_type = module_dict.get('module_type')\n        module_param = deepcopy(module_dict)\n        del module_param['module_type']\n        return cls(module_type=module_type, module_param=module_param)"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    metric_names = []\n    metric_params = []\n\n    for metric in metrics:\n        if isinstance(metric, str):\n            metric_names.append(metric)\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            metric_names.append(list(metric.keys())[0])\n            metric_params.append(metric)\n\n    return metric_names, metric_params"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "\n    if embedding_model is None:\n        embedding_model = embedding_models.get_embedding_model('all-mpnet-base-v2')\n\n    gt_embeddings = [embedding_model.encode(gt) for gt in generation_gt]\n    pred_embedding = embedding_model.encode(pred)\n\n    similarities = [calculate_cosine_similarity(pred_embedding, gt_emb) for gt_emb in gt_embeddings]\n    max_similarity = max(similarities)\n\n    return max_similarity"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "\n    global gfpgan_face_restorer\n\n    if gfpgan_face_restorer is None:\n        logging.warning(\"GFPGAN face restorer is not set up. Returning the original image.\")\n        return np_image\n\n    try:\n        restored_image = gfpgan_face_restorer.restore(np_image)\n        return restored_image\n    except Exception as e:\n        logging.warning(f\"Failed to restore faces using GFPGAN face restorer: {e}\")\n        return np_image"}
{"namespace": "codeformer_model.setup_model", "completion": "    try:\n        global codeformer\n        codeformer = FaceRestorerCodeFormer(model_path=dirname)\n    except errors.ModelLoadError as e:\n        logger.error(f\"Failed to load CodeFormer model from {dirname}: {e}\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        shared.patch_facexlib(dirname)\n        global gfpgan_face_restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(model_path=dirname)\n    except Exception as e:\n        logger.error(f\"Failed to set up GFPGAN model: {e}\")"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion format\n  v_quat = jnp.concatenate([v, jnp.array([0.0])])\n\n  # Apply the quaternion rotation\n  q_conj = conjugate(q)\n  v_rotated = multiply(multiply(q, v_quat), q_conj)\n\n  # Convert the rotated vector back to 3D space\n  return v_rotated[:3]"}
{"namespace": "quaternion.from_axis_angle", "completion": "  angle = jnp.linalg.norm(axis_angle)\n  half_angle = angle / 2.0\n  axis = axis_angle / (angle + eps)\n  w = jnp.cos(half_angle)\n  xyz = jnp.sin(half_angle) * axis\n  return jnp.concatenate((xyz, w), axis=-1)"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    # get raw topk, could be done outside and passed in\n    num_calls = 0\n    logit_bias = {idx: high}\n    while True:\n        logprobs, _, calls = exact_solve(model, prefix, [idx], bias=logit_bias[idx])\n        num_calls += calls\n        if max(logprobs, key=logprobs.get) == idx:\n            return logprobs[idx], num_calls\n        logit_bias[idx] *= 2"}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The output_dir must be an instance of the Dir class.\")\n\n    if not output_dir.url.startswith(\"s3://\"):\n        raise ValueError(\"The output_dir must start with 's3://'.\")\n\n    # Check if the directory is empty\n    client = boto3.client(\"s3\")\n    response = client.list_objects_v2(Bucket=output_dir.url.split(\"/\")[2], Prefix=\"/\".join(output_dir.url.split(\"/\")[3:]))\n    if \"Contents\" in response and len(response[\"Contents\"]) > 0:\n        raise ValueError(f\"The directory {output_dir.url} is not empty.\")\n\n    if append:\n        raise NotImplementedError(\"Appending data to the directory is not currently supported.\")\n\n    if overwrite:\n        raise NotImplementedError(\"Overwriting data in the directory is not currently supported.\")"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    if \"Contents\" in objects:\n        for obj in objects[\"Contents\"]:\n            if obj[\"Key\"].endswith(\"index.json\"):\n                raise RuntimeError(f\"The directory already contains an index file: {obj['Key']}.\")\n    \n    # If index file is not found, delete all objects within the specified prefix in the bucket\n    if \"Contents\" in objects:\n        for obj in objects[\"Contents\"]:\n            s3.delete_object(Bucket=obj.netloc, Key=obj[\"Key\"])"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        if self.rank != 0:\n            # Wait for the master node to perform the merge\n            while not os.path.exists(os.path.join(self._cache_dir, f\"0.{_INDEX_FILENAME}\")):\n                sleep(1)\n            return\n\n        # Master node (rank 0) performs the merge\n        while len(os.listdir(self._cache_dir)) < num_workers:\n            # Wait for all parts to be available\n            sleep(1)\n\n        # Merge the index files\n        merged_index = {}\n        for worker_rank in range(num_workers):\n            index_file = os.path.join(self._cache_dir, f\"{worker_rank}.{_INDEX_FILENAME}\")\n            with open(index_file, \"r\") as f:\n                data = json.load(f)\n                merged_index.update(data[\"chunks\"])\n\n        # Write the merged index to a file\n        merged_index_file = os.path.join(self._cache_dir, f\"merged_index.{_INDEX_FILENAME}\")\n        with open(merged_index_file, \"w\") as f:\n            json.dump(merged_index, f, sort_keys=True)"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "\n    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\"The Lightning SDK is not available. Please install the SDK to use this function.\")\n\n    if machine is None:\n        machine = Machine()\n\n    if command is None:\n        command = f\"cd {os.getcwd()} && {os.getenv('LIGHTNING_COMMAND', 'python')} operator.py\"\n\n    studio = Studio()\n\n    job = studio.create_job(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command\n    )\n\n    print(f\"Job created. Job URL: {job.url}\")\n\n    while job.status != \"running\":\n        sleep(5)\n        job.refresh()\n\n    print(f\"Job started. Job URL: {job.url}\")\n\n    if job.status == \"failed\":\n        raise RuntimeError(f\"Job failed. Job URL: {job.url}\")"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        try:\n            # Load the ChunksConfig object using the provided attributes\n            config = ChunksConfig(\n                cache_dir=self._cache_dir,\n                remote_input_dir=self._remote_input_dir,\n                serializers=self._serializers,\n                item_loader=self._item_loader,\n            )\n            # Update the instance's configuration with the loaded ChunksConfig object\n            self._config = config\n            return config\n        except Exception as e:\n            # Handle any exceptions that occur during the loading of the configuration\n            logger.warning(f\"Failed to load chunks configuration: {e}\")\n            return None"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        for index in chunk_indexes:\n            self._to_download_queue.put(index)"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"Configuration should be defined before accessing it.\")\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The provided index is not an instance of ChunkedIndex.\")\n\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader's index configuration is not defined.\")\n\n        if self._prepare_thread is None:\n            raise AssertionError(\"The prepare thread is not defined. Please ensure that the thread is correctly managing the chunks.\")\n\n        chunk_index = self._get_chunk_index_from_index(index.index)\n        self._prepare_thread.download([chunk_index])\n\n        return self._item_loader.load(index)"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            try:\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                # Ignore FileNotFoundError exceptions\n                pass\n    return total_size"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    distributed_map = _ImmutableDistributedMap()\n    try:\n        return distributed_map.set_and_get(key, obj)\n    except RuntimeError:\n        return obj"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Sort items by weight in descending order\n    sorted_items = sorted(zip(items, weights), key=lambda x: x[1], reverse=True)\n\n    # Place each item into the bin with the current lowest total weight\n    for item, weight in sorted_items:\n        min_bin = min(bin_weights, key=bin_weights.get)\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weight\n\n    return bins, bin_weights"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Flatten the list of chunk indexes\n    all_chunks = [chunk for chunks in chunks_per_ranks for chunk in chunks]\n\n    # Set the random seed based on the current epoch\n    np.random.seed(seed + current_epoch)\n\n    # Shuffle the chunk indexes\n    np.random.shuffle(all_chunks)\n\n    # Calculate the number of chunks per node\n    chunks_per_node = len(all_chunks) // distributed_env.world_size\n\n    # Distribute the shuffled chunk indexes to each node\n    distributed_chunks = [all_chunks[i * chunks_per_node: (i + 1) * chunks_per_node] for i in range(distributed_env.world_size)]\n\n    # Flatten the distributed chunks for all nodes\n    flattened_distributed_chunks = [chunk for chunks in distributed_chunks for chunk in chunks]\n\n    return flattened_distributed_chunks"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    for suffix in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]:\n        if num_bytes < 1000:\n            return f\"{num_bytes:.2f} {suffix}\"\n        num_bytes /= 1000\n    return f\"{num_bytes:.2f} PB\""}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    indexed_paths = _get_indexed_paths(inputs)\n\n    if not indexed_paths:\n        return None\n\n    input_dir = os.path.dirname(list(indexed_paths.values())[0])\n    return input_dir"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "\n    # Dilate the time steps\n    dilated_t = t * dilation\n\n    # Clip the dilated time steps within the specified domain\n    dilated_t = torch.clamp(dilated_t, min=domain[0], max=domain[1])\n\n    # Adjust the weights to match the dilated time steps\n    adjusted_w = w / dilation\n\n    return dilated_t, adjusted_w"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Find the indices where tq should be inserted into t to maintain order\n    idx_lo, idx_hi = searchsorted(t, tq)\n\n    # Check if tq exactly matches a step change time\n    exact_match = torch.eq(tq, t[idx_lo])\n\n    # Interpolate the values at the query times based on the step function\n    interpolated_values = interpolate(tq, t, y)\n\n    # Return the outside value for query times that exactly match a step change time\n    result = torch.where(exact_match, outside_value, interpolated_values)\n\n    return result"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # Calculate the annealing factor using Schlick's bias function\n    bias = (1 - train_frac) / (train_frac * anneal_slope + eps)\n    annealing_factor = bias / (bias + w)\n\n    # Adjust the weights based on the annealing factor\n    adjusted_weights = w * annealing_factor\n\n    # Handle cases where adjacent intervals have zero distance\n    adjusted_weights = torch.where(t[..., 1:] - t[..., :-1] <= 0, torch.zeros_like(adjusted_weights), adjusted_weights)\n\n    # Prevent NaN values by using softmax operation on the adjusted weights\n    adjusted_weights = torch.nn.functional.softmax(adjusted_weights, dim=-1)\n\n    return adjusted_weights"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "\n    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, (tuple, list)):\n        return [to_cuda(item, device, ignore_list) for item in batch]\n    elif isinstance(batch, dict):\n        return {key: to_cuda(value, device, ignore_list) if key != \"meta\" else value for key, value in batch.items()}\n    else:\n        return batch"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "\n    # Gather the vertices for each face using the indices from the faces tensor\n    gathered_vertices = multi_gather(v, f, dim)\n\n    return gathered_vertices"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, torch.Tensor):\n        batch = batch.unsqueeze(0)\n    else:  # numpy and others\n        batch = np.expand_dims(batch, axis=0)\n    return batch"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H, batch.W, batch.K, batch.R, batch.T, batch.n, batch.f, batch.t, batch.v, batch.bounds = (\n            torch.tensor(self.H),\n            torch.tensor(self.W),\n            torch.tensor(self.K),\n            torch.tensor(self.R),\n            torch.tensor(self.T),\n            torch.tensor(self.n),\n            torch.tensor(self.f),\n            torch.tensor(self.t),\n            torch.tensor(self.v),\n            torch.tensor(self.bounds),\n        )\n\n        batch.meta = dotdict()\n        batch.meta.origin = torch.tensor(self.origin)\n        batch.meta.world_up = torch.tensor(self.world_up)\n        batch.meta.movement_speed = torch.tensor(self.movement_speed)\n        batch.meta.movement_force = torch.tensor(self.movement_force)\n        batch.meta.drag_coeff_mult = torch.tensor(self.drag_coeff_mult)\n        batch.meta.constant_drag = torch.tensor(self.constant_drag)\n        batch.meta.mass = torch.tensor(self.mass)\n        batch.meta.moment_of_inertia = torch.tensor(self.moment_of_inertia)\n        batch.meta.movement_torque = torch.tensor(self.movement_torque)\n        batch.meta.angular_friction = torch.tensor(self.angular_friction)\n        batch.meta.constant_torque = torch.tensor(self.constant_torque)\n        batch.meta.min_interval = torch.tensor(self.min_interval)\n        batch.meta.pause_physics = torch.tensor(self.pause_physics)\n\n        return batch"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working_agent() and not agent.is_prime_agent():\n            serialized_agent = AgentSerializer.serialize(agent)\n            self.persistence.save_agent(serialized_agent)"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            if not self.agents:\n                return None, float('-inf')\n\n            max_similarity = float('-inf')\n            closest_agent = None\n\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n\n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n                if similarity > max_similarity:\n                    max_similarity = similarity\n                    closest_agent = agent\n\n            return closest_agent, max_similarity\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            return None, float('-inf')"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        prime_agent = MicroAgent(\n            prompt=PRIME_PROMPT,\n            name=PRIME_NAME,\n            weight=PRIME_AGENT_WEIGHT,\n            is_prime=True,\n            unspecified_flag=False\n        )\n        self.agents.append(prime_agent)"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        agent_dict = self.persistence.load_agent(purpose)\n        if agent_dict:\n            agent = AgentSerializer.from_dict(agent_dict, agent_lifecycle, openai_wrapper)\n            return agent\n        else:\n            return None"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_info = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n        agent_name, input_text = agent_info.split(\":\") if \":\" in agent_info else (agent_info, \"\")\n        return agent_name, input_text"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        loaded_agents = []\n        purposes = self.persistence.get_all_agent_purposes()\n        for purpose in purposes:\n            serialized_agent = self.persistence.fetch_agent(purpose)\n            if serialized_agent:\n                agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n                loaded_agents.append(agent)\n        return loaded_agents"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.error(f\"Error occurred while saving agent: {e}\")\n            raise e"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Calculate the center of the instance's bounding box\n    bbox = instance[\"bbox\"]\n    x_center = (bbox[0] + bbox[2]) / 2\n    y_center = (bbox[1] + bbox[3]) / 2\n\n    # Calculate the top-left corner of the crop region\n    crop_x1 = max(0, int(x_center - crop_size[1] / 2))\n    crop_y1 = max(0, int(y_center - crop_size[0] / 2))\n\n    # Adjust the crop region if it exceeds the image boundaries\n    crop_x2 = min(image_size[1], crop_x1 + crop_size[1])\n    crop_y2 = min(image_size[0], crop_y1 + crop_size[0])\n\n    # Create a CropTransform object with the calculated crop region\n    return T.CropTransform(crop_x1, crop_y1, crop_x2, crop_y2)"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n    return image"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # Transform the bounding box\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox = transforms.apply_box([bbox])[0]\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform the segmentation\n    if \"segmentation\" in annotation:\n        if annotation[\"segmentation\"] is None:\n            # fill in \"segmentation\" if it is empty\n            annotation[\"segmentation\"] = []\n        # Convert segmentation from (polygon) contours to RLE\n        if isinstance(annotation[\"segmentation\"], list):\n            polygons = [np.asarray(p).reshape(-1, 2) for p in annotation[\"segmentation\"]]\n            rles = [\n                mask_util.frPyObjects([p], image_size[0], image_size[1]) for p in polygons\n            ]\n            rle = mask_util.merge(rles)\n            annotation[\"segmentation\"] = mask_util.decode(rle).astype(\"uint8\")\n        else:  # rle\n            mask = mask_util.decode(annotation[\"segmentation\"])\n            assert mask.shape[0] == image_size[0] and mask.shape[1] == image_size[1], (\n                f\"Mask shape {mask.shape} does not match image size {image_size}\"\n            )\n            annotation[\"segmentation\"] = mask\n\n        # Apply transformation\n        annotation[\"segmentation\"] = transforms.apply_segmentation(\n            annotation[\"segmentation\"]\n        )\n\n    # Transform the keypoints\n    if \"keypoints\" in annotation:\n        keypoints = Keypoints(annotation[\"keypoints\"], annotation[\"keypoints_mode\"], image_size)\n        keypoints = keypoints.flip(\"h\", image_size)\n        keypoints = keypoints.apply_transform(transforms)\n        annotation[\"keypoints\"] = keypoints.tensor\n        annotation[\"keypoints_mode\"] = \"xy\"\n        if keypoint_hflip_indices is not None:\n            for l, r in keypoint_hflip_indices:\n                annotation[\"keypoints\"][l], annotation[\"keypoints\"][r] = (\n                    annotation[\"keypoints\"][r],\n                    annotation[\"keypoints\"][l],\n                )"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n\n        # Apply the rotation matrix to each coordinate\n        rotated_coords = np.dot(coords, self.rm_coords.T)\n        return rotated_coords"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    # Create empty lists to store the annotations\n    boxes = []\n    classes = []\n    masks = []\n    keypoints = []\n\n    # Iterate through each annotation\n    for anno in annos:\n        # Extract bounding box coordinates\n        bbox = anno[\"bbox\"]\n        x0, y0, x1, y1 = bbox\n        # Convert to (x, y, width, height) format\n        bbox = [x0, y0, x1, y1]\n        boxes.append(bbox)\n\n        # Extract class label\n        classes.append(anno[\"category_id\"])\n\n        # Extract segmentation mask\n        if \"segmentation\" in anno:\n            if mask_format == \"polygon\":\n                # Convert polygon to bitmask\n                mask = polygons_to_bitmask(anno[\"segmentation\"], image_size[0], image_size[1])\n                masks.append(mask)\n            elif mask_format == \"bitmask\":\n                # Directly append the bitmask\n                masks.append(anno[\"segmentation\"])\n\n        # Extract keypoints\n        if \"keypoints\" in anno:\n            keypoints.append(anno[\"keypoints\"])\n\n    # Create Instances object\n    instances = Instances(image_size)\n    instances.gt_boxes = Boxes(boxes)\n    instances.gt_classes = torch.tensor(classes)\n    if masks:\n        instances.gt_masks = PolygonMasks(masks)\n    if keypoints:\n        instances.gt_keypoints = Keypoints(keypoints)\n\n    return instances"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    analysis = FlopCountAnalysis(model, inputs)\n    return analysis.by_operator()"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if self.angle % 360 == 0:\n            return img\n        if len(img.shape) == 3:\n            img = cv2.warpAffine(\n                img,\n                self.rm_image,\n                (self.bound_w, self.bound_h),\n                flags=self.interp,\n                borderMode=cv2.BORDER_CONSTANT,\n                borderValue=0,\n            )\n        else:\n            img = cv2.warpAffine(\n                img,\n                self.rm_image,\n                (self.bound_w, self.bound_h),\n                flags=self.interp,\n                borderMode=cv2.BORDER_CONSTANT,\n                borderValue=0,\n            )\n        return img"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes if predictions.has(\"pred_classes\") else None\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n        masks = predictions.pred_masks if predictions.has(\"pred_masks\") else None\n        masks_rle = predictions.pred_masks_rle if predictions.has(\"pred_masks_rle\") else None\n\n        num_instances = len(boxes) if boxes is not None else len(masks_rle)\n\n        if num_instances == 0:\n            return self.output\n\n        if self._instance_mode == ColorMode.SEGMENTATION:\n            colors = [self.metadata.thing_colors[c] for c in classes] if classes is not None else None\n        else:\n            colors = None\n\n        for i in range(num_instances):\n            color = random_color(rgb=True) if colors is None else colors[i]\n            color = mplc.to_rgb(color)\n\n            if boxes is not None:\n                box = boxes.tensor[i].cpu().numpy()\n                self.draw_box(box, edge_color=color)\n\n            if masks is not None:\n                mask = masks[i].cpu().numpy()\n                self.draw_binary_mask(mask, color=color)\n\n            if masks_rle is not None:\n                mask_rle = mask_util.decode(masks_rle[i])\n                mask = mask_rle.astype(np.bool)\n                self.draw_binary_mask(mask, color=color)\n\n            if keypoints is not None:\n                keypoints_i = keypoints[i].cpu().numpy()\n                self.draw_and_connect_keypoints(keypoints_i, keypoint_threshold=self.keypoint_threshold)\n\n            if classes is not None and scores is not None:\n                label = self.metadata.thing_classes[classes[i]] + \": \" + \"{:.0f}%\".format(scores[i] * 100)\n                self.draw_text(label, (box[0], box[1]), color=color)\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        self.canvas.draw()  # Draw the canvas to render the image\n        buf = self.canvas.buffer_rgba()  # Get the buffer in RGBA format\n        buf = np.asarray(buf)  # Convert the buffer to a numpy array\n        return buf[:, :, :3]  # Return the RGB channels of the image"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        if \"annotations\" in dic:\n            # Process annotations\n            annotations = dic[\"annotations\"]\n            instances = Instances(\n                image_size=(self.output.height, self.output.width),\n                pred_boxes=Boxes(annotations[\"bbox\"]),\n                scores=annotations[\"score\"],\n                pred_classes=annotations[\"category_id\"],\n                pred_masks=annotations[\"segmentation\"],\n            )\n            return self.draw_instance_predictions(instances)\n        \n        if \"sem_seg\" in dic:\n            # Process semantic segmentation\n            return self.draw_sem_seg(dic[\"sem_seg\"])\n        \n        if \"panoptic_seg\" in dic and \"segments_info\" in dic:\n            # Process panoptic segmentation\n            return self.draw_panoptic_seg(dic[\"panoptic_seg\"], dic[\"segments_info\"])\n        \n        # If none of the above, return the original image\n        return self.output"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    buffer.seek(0)\n    reloaded_module = torch.jit.load(buffer)\n    return reloaded_module"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n\n        mask = np.asarray(binary_mask, dtype=np.uint8)\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        for contour in contours:\n            area = cv2.contourArea(contour)\n            if area < area_threshold:\n                continue\n\n            if edge_color is not None:\n                edge_color = mplc.to_rgb(edge_color)\n                edge_color = [c * 255 for c in edge_color]\n                edge_color = tuple(edge_color)\n\n                cv2.drawContours(self.output.img, [contour], -1, edge_color, 2)\n\n            if text is not None:\n                M = cv2.moments(contour)\n                cX = int(M[\"m10\"] / M[\"m00\"])\n                cY = int(M[\"m01\"] / M[\"m00\"])\n                cv2.putText(\n                    self.output.img,\n                    text,\n                    (cX, cY),\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    0.5,\n                    (255, 255, 255),\n                    2,\n                )\n\n            if color is not None:\n                color = mplc.to_rgb(color)\n                color = [c * 255 for c in color]\n                color = tuple(color)\n\n                cv2.drawContours(self.output.img, [contour], -1, color, -1)\n\n        return self.output"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    if size_as_tensor:\n        assert torch.allclose(\n            input.image_size, other.image_size, rtol=rtol\n        ), f\"{msg}Image sizes do not match: {input.image_size} vs {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, f\"{msg}Image sizes do not match: {input.image_size} vs {other.image_size}\"\n\n    for field in input._field_names:\n        input_val = getattr(input, field)\n        other_val = getattr(other, field)\n\n        if isinstance(input_val, Boxes):\n            assert torch.allclose(\n                input_val.tensor, other_val.tensor, rtol=rtol\n            ), f\"{msg}Field {field} does not match\"\n        elif isinstance(input_val, ROIMasks):\n            assert torch.allclose(\n                input_val.tensor, other_val.tensor, rtol=rtol\n            ), f\"{msg}Field {field} does not match\"\n        elif isinstance(input_val, torch.Tensor):\n            assert torch.allclose(\n                input_val, other_val, rtol=rtol\n            ), f\"{msg}Field {field} does not match\"\n        else:\n            assert input_val == other_val, f\"{msg}Field {field} does not match\""}
{"namespace": "detectron2.utils.registry.locate", "completion": "    obj = pydoc.locate(name)\n    if obj is None:\n        raise ImportError(f\"Error locating object: {name}\")\n    return obj"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "\n    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        raise NotImplementedError(f\"Segmentor model of type {cfg['type']} is not implemented yet.\")"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    if train_cfg is not None:\n        warnings.warn('train_cfg is deprecated, please specify it in the model config')\n    if test_cfg is not None:\n        warnings.warn('test_cfg is deprecated, please specify it in the model config')\n\n    assert 'train_cfg' not in cfg, 'train_cfg should not be specified in both outer field and model field'\n    assert 'test_cfg' not in cfg, 'test_cfg should not be specified in both outer field and model field'\n\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(cfg, train_cfg, test_cfg)\n    else:\n        return MMDET_DETECTORS.build(cfg, train_cfg, test_cfg)"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    val = val - period * offset\n    val = val - torch.floor(val / period) * period\n    return val + period * offset"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    gt = {}\n    dt = {}\n    for anno in gt_annos:\n        img_id = anno['image']['image_id']\n        gt[img_id] = anno['gt_boxes_2d']\n    for anno in dt_annos:\n        img_id = anno['image']['image_id']\n        dt[img_id] = [(box, score) for box, score in zip(anno['pred_boxes_2d'], anno['scores'])]\n\n    pred = {}\n    for img_id, boxes in dt.items():\n        pred[img_id] = boxes\n\n    gt_dict = {}\n    for anno in gt_annos:\n        img_id = anno['image']['image_id']\n        gt_dict[img_id] = anno['gt_boxes_2d']\n\n    pred_dict = {}\n    for anno in dt_annos:\n        img_id = anno['image']['image_id']\n        pred_dict[img_id] = [(box, score) for box, score in zip(anno['pred_boxes_2d'], anno['scores'])]\n\n    recall, precision, ap = eval_map_recall(pred, gt, metric)\n\n    ret_dict = {}\n    for i, thresh in enumerate(metric):\n        ret_dict[f'recall@{thresh}'] = recall[i]\n        ret_dict[f'precision@{thresh}'] = precision[i]\n        ret_dict[f'AP@{thresh}'] = ap[i]\n\n    if logger is not None:\n        table_data = [['Class', 'AP'] + [f'AP@{thresh}' for thresh in metric]]\n        for label, cat in label2cat.items():\n            aps = [ret_dict[f'AP@{thresh}'][label] for thresh in metric]\n            table_data.append([cat, ret_dict[f'AP'][label]] + aps)\n        table = AsciiTable(table_data)\n        print_log(table.table, logger=logger)\n\n    return ret_dict"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        return 'LiDARBox3D', 'LiDAR'\n    elif box_type == 'Camera':\n        return 'CameraBox3D', 'Camera'\n    elif box_type == 'Depth':\n        return 'DepthBox3D', 'Depth'\n    else:\n        raise ValueError(f'Unsupported box type {box_type}. '\n                         f'Supported box types are \"LiDAR\", \"Camera\", or \"Depth\".')"}
{"namespace": "ollama._client.Client.chat", "completion": "\n    if not model:\n      raise RequestError('must provide a model')\n\n    if messages:\n      if not all(isinstance(msg, (Message, dict)) for msg in messages):\n        raise TypeError(\"messages must be a list of Message or dict-like objects\")\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages or [],\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'model': model,\n        'insecure': insecure,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError(\"Model is required for generating a response\")\n\n    request_data = {\n      \"model\": model,\n      \"prompt\": prompt,\n      \"system\": system,\n      \"template\": template,\n      \"context\": context or [],\n      \"raw\": raw,\n      \"format\": format,\n      \"images\": images or [],\n      \"options\": options or {},\n      \"keep_alive\": keep_alive\n    }\n\n    if stream:\n      return self._request_stream(\"POST\", \"/generate\", json=request_data, stream=True)\n    else:\n      return self._request_stream(\"POST\", \"/generate\", json=request_data)"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.create", "completion": "\n    if not path and not modelfile:\n      raise RequestError('either path or modelfile must be provided')\n\n    if path:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'model': model,\n        'modelfile': b64encode(modelfile).decode('utf-8'),\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    with open(path, 'rb') as file:\n        file_content = file.read()\n        file_hash = sha256(file_content).hexdigest()\n        blob_digest = f'sha256:{file_hash}'\n\n    # Check if the blob exists on the server\n    response = self._client.head(f'/api/blobs/{blob_digest}')\n    if response.status_code == 404:\n        # Blob does not exist, upload the file\n        self._client.post(f'/api/blobs/{blob_digest}', content=file_content)\n\n    return blob_digest"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        self._state_dict = state_dict"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict:\n            state: Dict[str, Any] = self._state_dict\n            if state[\"shuffle\"] != self.shuffle:\n                raise ValueError(\"Mismatch in shuffle parameter between state dictionary and current state\")\n            if state[\"num_workers\"] != self.worker_env.world_size:\n                raise ValueError(\"Mismatch in num_workers parameter between state dictionary and current state\")\n            if state[\"input_dir_path\"] != self.input_dir.path:\n                raise ValueError(\"Mismatch in input directory path between state dictionary and current state\")\n            if state[\"input_dir_url\"] != self.input_dir.url:\n                raise ValueError(\"Mismatch in input directory URL between state dictionary and current state\")\n            if state[\"seed\"] != self.seed:\n                raise ValueError(\"Mismatch in seed parameter between state dictionary and current state\")\n            if state[\"item_loader\"] != self.item_loader.state_dict() if self.item_loader else None:\n                raise ValueError(\"Mismatch in item_loader state between state dictionary and current state\")\n            if state[\"drop_last\"] != self.drop_last:\n                raise ValueError(\"Mismatch in drop_last flag between state dictionary and current state\")"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='XAgent Command Line Interface')\n    parser.add_argument('--task', type=str, required=True, help='The task description, specifying what task should be performed.')\n    parser.add_argument('--upload-files', nargs='+', type=str, help='List of files to upload, allowing multiple files to be specified.')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use.')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs.')\n    parser.add_argument('--mode', type=str, default='auto', choices=['auto', 'manual'], help='Operational mode, which can be \"auto\" or \"manual\", specifying how the task should be executed.')\n    parser.add_argument('--quiet', action='store_true', help='If set, the program runs in quiet mode with minimal output.')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be.')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution.')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans.')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be.')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts, specifying how many times a task can be retried upon failure.')\n    parser.add_argument('--config-file', type=str, default=os.getenv('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file, specifying where to find the configuration settings.')\n\n    return parser.parse_args()"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    if input_dir is None:\n        input_dir = \"\"\n\n    # Generate a unique directory name by hashing the input directory\n    cache_dir_name = hashlib.md5(input_dir.encode()).hexdigest()\n\n    # Check if environment variables are set for cache directory location\n    cache_dir_env = os.getenv(\"CACHE_DIR\")\n    if cache_dir_env:\n        cache_dir = os.path.join(cache_dir_env, cache_dir_name)\n    else:\n        cache_dir = os.path.join(_DEFAULT_CACHE_DIR, cache_dir_name)\n\n    # Attempt to create the cache directory\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n    except Exception as e:\n        logger.error(f\"Failed to create cache directory: {e}\")\n        return None"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None or path == \"\" or path.startswith(\"/cache\") or path.startswith(_DEFAULT_CACHE_DIR):\n        return True\n    else:\n        return False"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    samples_per_worker = num_samples_yielded // num_workers\n    remaining_samples = num_samples_yielded % num_workers\n\n    samples_distribution = {i: samples_per_worker for i in range(num_workers)}\n    for i in range(remaining_samples):\n        samples_distribution[i] += 1\n\n    return samples_distribution"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        parsed_url = parse.urlparse(remote_filepath)\n        if parsed_url.scheme != 's3':\n            raise ValueError(\"Remote file path must use the 's3' scheme\")\n\n        if not os.path.exists(local_filepath):\n            with FileLock(local_filepath + \".lock\", timeout=10):\n                if self._s5cmd_available:\n                    subprocess.run([\"s5cmd\", \"cp\", remote_filepath, local_filepath])\n                else:\n                    self._client.download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    chunks_per_replica = len(chunks_replica)\n    workers_chunks = {i: [] for i in range(num_workers)}\n    workers_intervals = {i: [] for i in range(num_workers)}\n\n    for i in range(chunks_per_replica):\n        worker_index = i % num_workers\n        workers_chunks[worker_index].append(chunks_replica[i])\n        workers_intervals[worker_index].append(intervals_replica[i])\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath.replace(\"local:\", \"\")\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "\n    chunks_index = {}\n    updated_indexes = {}\n\n    for worker_idx, intervals in workers_intervals.items():\n        chunk_index = 0\n        current_index = indexes[worker_idx]\n\n        for interval in intervals:\n            interval_size = interval[1] - interval[0]\n            if current_index < interval_size:\n                chunks_index[worker_idx] = chunk_index\n                updated_indexes[worker_idx] = current_index\n                break\n            else:\n                current_index -= interval_size\n                chunk_index += 1\n\n    return chunks_index, updated_indexes"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        with io.BytesIO() as output:\n            item.save(output, format=\"PNG\")\n            serialized_data = output.getvalue()\n            mode_bytes = item.mode.encode(\"utf-8\")\n            return (\n                serialized_data,\n                mode_bytes,\n            )"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile) and hasattr(item, \"filename\") and os.path.exists(item.filename):\n            with open(item.filename, \"rb\") as f:\n                return f.read(), None\n        elif _PIL_AVAILABLE and isinstance(item, Image.Image):\n            with io.BytesIO() as output:\n                item.save(output, format=\"JPEG\")\n                return output.getvalue(), None\n        else:\n            raise TypeError(\"Unsupported image type for JPEG serialization\")"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        ints = np.frombuffer(data[:12], dtype=np.uint32)\n        width, height, mode_length = ints\n        mode = data[12:12 + mode_length].decode(\"utf-8\")\n        raw = data[12 + mode_length:]\n        return Image.frombytes(mode, (width, height), raw)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[:4], np.uint32)[0]\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_len = np.frombuffer(data[4:8], np.uint32)[0]\n        shape = tuple(np.frombuffer(data[8:8 + shape_len * 4], np.uint32))\n        tensor_data = np.frombuffer(data[8 + shape_len * 4:], dtype=_NUMPY_DTYPES_MAPPING[dtype])\n        return torch.from_numpy(tensor_data).view(shape).to(dtype)"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        dtype_idx = self._dtype_to_indices[item.dtype]\n        shape = np.array(item.shape, dtype=np.uint32)\n        raw = item.numpy().tobytes()\n        return dtype_idx.to_bytes(1, byteorder=\"big\") + shape.tobytes() + raw, None"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb / 12.92\n  linear1 = xnp.maximum(eps, ((srgb + 0.055) / 1.055) ** 2.4)\n  return xnp.where(srgb <= 0.04045, linear0, linear1)"}
{"namespace": "resample.resample_3d", "completion": "  if method == 'NEAREST':\n    # Round the locations to the nearest integer\n    locations = jnp.round(locations).astype(jnp.int32)\n    # Gather from data at rounded locations\n    resampled_data = gather_volume(data, locations, coordinate_order)\n  elif method == 'TRILINEAR':\n    if half_pixel_center:\n      # Adjust the locations for half-pixel centering\n      locations -= 0.5\n    # Extract the integer and fractional parts of the locations\n    int_locations = jnp.floor(locations).astype(jnp.int32)\n    frac_locations = locations - int_locations\n    # Handle edge behavior based on the specified method\n    if edge_behavior == 'CONSTANT_OUTSIDE':\n      # Pad the input volume with constant values\n      padded_data = jnp.pad(data, 1, constant_values=constant_values)\n      # Perform trilinear interpolation with padding\n      resampled_data = _trilinear_interpolation(padded_data, int_locations, frac_locations, coordinate_order)\n    elif edge_behavior == 'CLAMP':\n      # Perform trilinear interpolation with clamping\n      resampled_data = _trilinear_interpolation(data, int_locations, frac_locations, coordinate_order)\n  return resampled_data"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  return jnp.trapz(w, t)"}
{"namespace": "linspline.query", "completion": "  check_zero_endpoints(v)\n  spline_fn = functools.partial(math.linear_spline, t=t, v=v)\n  return jnp.where((tq >= t[0]) & (tq <= t[-1]), spline_fn(tq), 0)"}
{"namespace": "geometry.are_lines_parallel", "completion": "  epsilon = 1e-6  # Small epsilon to account for numerical precision\n  dir1_normalized = spin_math.normalize(dir1)  # Normalize the direction vector of the first line\n  dir2_normalized = spin_math.normalize(dir2)  # Normalize the direction vector of the second line\n  dot_product = jnp.dot(dir1_normalized, dir2_normalized)  # Compute the dot product of the normalized direction vectors\n  return jnp.abs(dot_product - 1) < epsilon  # Check if the absolute difference between the dot product and 1 is less than epsilon"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  return jnp.array([x, y, z])"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Generate all possible combinations of integer weights that sum to v\n  weights = np.array(list(itertools.product(range(v + 1), repeat=2)))\n  weights = weights[weights.sum(axis=1) <= v]\n\n  # Normalize the weights to get the barycentric coordinates\n  barycentric_weights = weights / v\n\n  return barycentric_weights"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = jnp.split(cartesian_vector, 3, axis=-1)\n  r = jnp.linalg.norm(cartesian_vector, axis=-1)\n  theta = jnp.arccos(z / (r + eps))\n  phi = jnp.arctan2(y, x)\n  return r, theta, phi"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n  norms_mat0 = np.sum(mat0 ** 2, axis=0, keepdims=True)\n  norms_mat1 = np.sum(mat1 ** 2, axis=0, keepdims=True)\n  sq_dist = norms_mat0.T + norms_mat1 - 2 * np.dot(mat0.T, mat1)\n  sq_dist[sq_dist < 0] = 0\n  return sq_dist"}
{"namespace": "math.plus_eps", "completion": "  return jnp.maximum(tiny_val, x)"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: y * x_dot,\n      (min_val, max_val),\n  )(x)"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                # Try to decode using torchvision\n                tensor = decode_jpeg(data)\n                return tensor\n            except RuntimeError:\n                # Fallback to using PIL to deserialize the data\n                pil_image = PILSerializer.deserialize(data)\n                return pil_to_tensor(pil_image)\n        else:\n            # Fallback to using PIL to deserialize the data\n            pil_image = PILSerializer.deserialize(data)\n            return pil_image"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        serialized_data = b\"\".join(data)\n        data_type_str = f\"no_header_tensor:{dtype_indice}\"\n        return serialized_data, data_type_str"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        dtype = self._dtype\n        tensor = torch.frombuffer(data, dtype=dtype)\n        return tensor"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        array_data = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        return array_data.reshape(shape)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\""}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_indice = _NUMPY_DTYPES_MAPPING[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_streaming,\n            \"latest_worker_idx\": self._latest_worker_idx\n        }\n\n        if isinstance(self.dataset, StreamingDataset):\n            state[\"dataset\"] = self.dataset.state_dict()\n        else:\n            state[\"dataset\"] = {\n                \"type\": \"CombinedStreamingDataset\",\n                \"num_samples_yielded\": self._num_samples_yielded_combined\n            }\n\n        return state"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE or not _AV_AVAILABLE:\n            raise Exception(\"torchvision and av libraries are required for deserializing video data.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as temp_file:\n            temp_file.write(data)\n            temp_file_path = temp_file.name\n\n        try:\n            video, audio, info = torchvision.io.read_video(temp_file_path, pts_unit=\"sec\")\n            return video\n        finally:\n            os.unlink(temp_file_path)"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        written_chunks = []\n        while not self.filled:\n            chunk_path = self.write_chunk()\n            written_chunks.append(chunk_path)\n        index_path = self.write_chunks_index()\n        written_chunks.append(index_path)\n        self._is_done = True\n        return written_chunks"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        else:\n            raise RuntimeError(\"The dataset associated with the StreamingDataLoader must be either a StreamingDataset or a CombinedStreamingDataset.\")"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "\n        state_dict = {}\n\n        if self._iterator is None and num_samples_yielded is None:\n            return state_dict\n\n        if self._iterator is not None:\n            state_dict = self._iterator.state_dict()\n        else:\n            state_dict[\"num_samples_yielded\"] = num_samples_yielded\n            state_dict[\"num_workers\"] = num_workers\n            state_dict[\"batch_size\"] = batch_size\n\n        return state_dict"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        if self._iterator is None:\n            # If the iterator is not initialized, update the state of each dataset\n            for dataset, dataset_state in zip(self._datasets, state_dict[\"datasets\"]):\n                dataset.load_state_dict(dataset_state)\n            if state_dict[\"num_samples_yielded\"] is not None:\n                self._num_samples_yielded = state_dict[\"num_samples_yielded\"]\n        else:\n            # If the iterator is initialized, update the state of the iterator\n            self._iterator.load_state_dict(state_dict)"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if dir_path is None:\n        return Dir()\n\n    if \"s3://\" in dir_path:\n        parsed_url = parse.urlparse(dir_path)\n        return Dir(path=None, url=dir_path)\n\n    if _LIGHTNING_SDK_AVAILABLE and Machine.is_project_path(dir_path):\n        return Dir(path=dir_path, url=Studio.get_project_url(dir_path))\n\n    return Dir(path=dir_path)"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    class DNSOptimizationContext:\n        def __enter__(self):\n            if enable:\n                # Enable DNS optimization\n                process = Popen([\"sudo\", \"networksetup\", \"-setdnsservers\", \"Wi-Fi\", \"8.8.8.8\"], stdout=DEVNULL, stderr=DEVNULL)\n                process.wait()\n            return self\n\n        def __exit__(self, exc_type, exc_value, traceback):\n            # Always disable DNS optimization after operations\n            process = Popen([\"sudo\", \"networksetup\", \"-setdnsservers\", \"Wi-Fi\", \"empty\"], stdout=DEVNULL, stderr=DEVNULL)\n            process.wait()\n\n    return DNSOptimizationContext()"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    total_chunks = len(indexes)\n    chunks_per_rank = total_chunks // distributed_env.world_size\n    remainder = total_chunks % distributed_env.world_size\n\n    chunks_per_ranks = []\n    chunk_intervals_per_ranks = []\n\n    start = 0\n    for rank in range(distributed_env.world_size):\n        num_chunks = chunks_per_rank + (1 if rank < remainder else 0)\n        end = start + num_chunks\n        chunks_per_ranks.append(indexes[start:end])\n        chunk_intervals_per_ranks.append(chunk_intervals[start:end])\n        start = end\n\n    if drop_last and remainder > 0:\n        chunks_per_ranks[-1] = chunks_per_ranks[-1][:-1]\n        chunk_intervals_per_ranks[-1] = chunk_intervals_per_ranks[-1][:-1]\n\n    return chunks_per_ranks, chunk_intervals_per_ranks"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        if self._contains_device:\n            self._fn(item_metadata, output_dir, device=self._device)\n        elif self._contains_is_last:\n            self._fn(item_metadata, output_dir, is_last=is_last)\n        else:\n            self._fn(item_metadata, output_dir)"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    while True:\n        try:\n            response = s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\"))\n            return response\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                sleep(sleep_time)\n            else:\n                raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        total, used, free = shutil.disk_usage(input_dir)\n        free_gb = free / (2**30)  # Convert bytes to gigabytes\n        if free_gb <= threshold_in_gb:\n            break\n        sleep(sleep_time)"}
{"namespace": "litdata.processing.functions.optimize", "completion": "\n    map(\n        fn=fn,\n        inputs=inputs,\n        output_dir=output_dir,\n        weights=weights,\n        num_workers=num_workers,\n        fast_dev_run=fast_dev_run,\n        num_nodes=num_nodes,\n        machine=machine,\n        num_downloaders=num_downloaders,\n        num_uploaders=num_uploaders,\n        reorder_files=reorder_files,\n        reader=reader,\n        batch_size=batch_size,\n    )"}
{"namespace": "litdata.processing.functions.map", "completion": "\n    input_dir = _get_input_dir(inputs)\n    num_workers = num_workers or _get_default_num_workers()\n\n    if _IS_IN_STUDIO:\n        num_workers = 1\n\n    if isinstance(output_dir, Dir):\n        output_dir = _resolve_dir(output_dir, input_dir)\n\n    if error_when_not_empty:\n        _assert_dir_is_empty(output_dir)\n\n    if reorder_files:\n        _assert_dir_has_index_file(output_dir)\n\n    if weights is not None:\n        if len(weights) != len(inputs):\n            raise ValueError(\"The length of weights should be equal to the length of inputs.\")\n\n    if batch_size is not None:\n        inputs = [inputs[i : i + batch_size] for i in range(0, len(inputs), batch_size)]\n\n    with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as executor:\n        future_to_input = {\n            executor.submit(\n                _execute,\n                fn,\n                input_item,\n                output_dir,\n                weights[i] if weights else None,\n                reader,\n            ): input_item\n            for i, input_item in enumerate(inputs)\n        }\n\n        for future in concurrent.futures.as_completed(future_to_input):\n            future.result()"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "\n    s3 = S3Client()\n    while True:\n        try:\n            index, file_paths = queue_in.get(timeout=1)\n        except Empty:\n            continue\n\n        for file_path in file_paths:\n            obj = parse.urlparse(file_path)\n            local_path = os.path.join(cache_dir, obj.path.lstrip(\"/\"))\n\n            if not os.path.exists(local_path):\n                os.makedirs(os.path.dirname(local_path), exist_ok=True)\n                s3.download_file(obj.netloc, obj.path.lstrip(\"/\"), local_path)\n\n        queue_out.put(index)"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "\n    s3 = S3Client()\n\n    while True:\n        # 1. Fetch from the queue\n        r = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if r is None:\n            remove_queue.put(None)\n            return\n\n        # 3. Unpack\n        if isinstance(r, tuple):\n            temp_dir, path = r\n        else:\n            temp_dir = None\n            path = r\n\n        # 4. Upload the file to the output directory\n        if output_dir.url is not None or output_dir.path is not None:\n            if output_dir.url:\n                # 5. Wait for the removers to catch up when we are uploading data.\n                _wait_for_disk_usage_higher_than_threshold(\"/\", 25)\n\n            if output_dir.path:\n                local_path = path.replace(cache_dir, output_dir.path)\n\n            if output_dir.url and output_dir.path:\n                path = path.replace(cache_dir, output_dir.url)\n\n            obj = parse.urlparse(path)\n\n            if obj.scheme == \"s3\":\n                with open(local_path, \"rb\") as f:\n                    s3.client.upload_fileobj(f, obj.netloc, obj.path.lstrip(\"/\"))\n\n            elif os.path.isfile(path):\n                if not path.startswith(\"/teamspace/studios/this_studio\"):\n                    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n                    shutil.copyfile(path, local_path)\n            else:\n                raise ValueError(f\"The provided {output_dir.url} isn't supported.\")\n\n        # 6. Inform the worker the current file is uploaded\n        remove_queue.put(path)"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if weights is None:\n        weights = [1] * len(user_items)\n\n    total_weight = sum(weights)\n    num_nodes = _get_num_nodes()\n    world_size = num_nodes * num_workers\n\n    if file_size:\n        weights = [w / (1024 * 1024) for w in weights]  # Convert to megabytes\n\n    weighted_items = []\n    for item, weight in zip(user_items, weights):\n        weighted_items.extend([item] * int((weight / total_weight) * world_size))\n\n    random.shuffle(weighted_items)\n\n    num_items_per_worker = len(weighted_items) // world_size\n    num_items_per_worker = [num_items_per_worker for _ in range(world_size)]\n    reminder = len(weighted_items) % world_size\n\n    for worker_idx in range(len(num_items_per_worker) - 1, -1, -1):\n        if reminder == 0:\n            break\n        num_items_per_worker[worker_idx] += 1\n        reminder -= 1\n\n    num_items_cumsum_per_worker = np.cumsum([0] + num_items_per_worker)\n\n    out = []\n    node_rank = _get_node_rank()\n    worker_idx_start = node_rank * num_workers\n    worker_idx_end = (node_rank + 1) * num_workers\n\n    for worker_idx in range(world_size):\n        if worker_idx_start <= worker_idx and worker_idx < worker_idx_end:\n            start = num_items_cumsum_per_worker[worker_idx]\n            end = num_items_cumsum_per_worker[worker_idx + 1]\n            out.append(weighted_items[start:end])\n\n    if len(out) != num_workers:\n        raise RuntimeError(\"The items didn't haven't been assigned properly. Please, open an issue on Github.\")\n\n    return out"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "\n    total_items = len(user_items)\n    total_workers = _get_num_nodes() * num_workers\n    items_per_worker = total_items // total_workers\n    remainder = total_items % total_workers\n\n    start_idx = 0\n    end_idx = 0\n    assigned_items = []\n\n    for i in range(total_workers):\n        end_idx += items_per_worker + (1 if i < remainder else 0)\n        assigned_items.append(user_items[start_idx:end_idx])\n        start_idx = end_idx\n\n    if len(assigned_items) != total_workers:\n        raise RuntimeError(\"Improper assignment of items to workers\")\n\n    return assigned_items"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "    cache_dirs = [_get_cache_dir(), _get_cache_data_dir()]\n    for cache_dir in cache_dirs:\n        if os.path.exists(cache_dir):\n            shutil.rmtree(cache_dir)\n        os.makedirs(cache_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    with concurrent.futures.ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n        file_sizes = list(executor.map(lambda item: _get_num_bytes(item, base_path), items))\n    return file_sizes"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if isinstance(element, str):\n        path = Path(element)\n        if input_dir:\n            input_path = Path(input_dir)\n            if path.is_absolute():\n                return path.is_file() or path.is_dir()\n            else:\n                return (input_path / path).is_file() or (input_path / path).is_dir()\n        else:\n            return path.is_file() or path.is_dir()\n    else:\n        return False"}
{"namespace": "rigid_body.exp_se3", "completion": "  theta = _safe_sqrt(jnp.sum(screw_axis[:3]**2))\n  w = screw_axis[:3] / theta\n  v = screw_axis[3:]\n\n  W = skew(w)\n  R = exp_so3(w * theta, eps)\n  p = jnp.dot((jnp.eye(3) * theta + (1 - jnp.cos(theta)) * W + (theta - jnp.sin(theta)) * jnp.dot(W, W)), v) / theta\n\n  X = jnp.block([[R, p.reshape(3, 1)], [jnp.array([[0.0, 0.0, 0.0, 1.0]])]])\n  return X"}
{"namespace": "rigid_body.exp_so3", "completion": "  theta = jnp.linalg.norm(axis_angle)\n  if theta < eps:\n    return jnp.eye(3) + skew(axis_angle)\n  else:\n    k = axis_angle / theta\n    K = skew(k)\n    return jnp.eye(3) + jnp.sin(theta) * K + (1 - jnp.cos(theta)) * (K @ K)"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  t_mean *= d\n  t_var *= jnp.sum(d**2)\n  r_var *= base_radius**2\n\n  return lift_gaussian(d, t_mean, t_var, r_var, diag)"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "\n  # Convert pixel coordinates to camera coordinates\n  origins_cam = xnp.matmul(pixtocams, xnp.stack([pix_x_int, pix_y_int, xnp.ones_like(pix_x_int)], axis=0))\n  \n  # Convert camera coordinates to world coordinates\n  origins_world = xnp.matmul(camtoworlds, xnp.concatenate([origins_cam, xnp.ones_like(origins_cam[[0]])], axis=0))\n  \n  # Compute normalized view directions\n  viewdirs = xnp.linalg.norm(origins_world, axis=0)\n  \n  # Compute ray directions\n  directions = origins_world / viewdirs\n  \n  # Compute differential radii\n  radii = xnp.linalg.norm(directions, axis=0)\n  \n  # Compute image plane coordinates\n  imageplane = origins_cam[:2] / origins_cam[2]\n\n  return origins_world, directions, viewdirs, radii, imageplane"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Calculate the differences between consecutive elements in t\n  dt = jnp.diff(t)\n\n  # Divide the weights by the differences to obtain the PDF\n  pdf = w / dt\n\n  return pdf"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = density * tdist * jnp.linalg.norm(dirs, axis=-1, keepdims=True)\n  return compute_alpha_weights_helper(density_delta, **kwargs)"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return jnp.where(td < np.finfo(np.float32).tiny, 0, p * td)"}
{"namespace": "stepfun.sample", "completion": "  if rng is not None:\n    u = jax.random.uniform(rng, shape=(num_samples,) + t.shape[:-1] + (1,), dtype=t.dtype)\n    t_samples = invert_cdf(u, t, w_logits)\n    if single_jitter:\n      jitter = jax.random.uniform(rng, shape=(num_samples,) + t.shape[:-1] + (1,), dtype=t.dtype)\n      t_samples = t_samples + (t[Ellipsis, 1:] - t[Ellipsis, :-1]) * (jitter - 0.5)\n  else:\n    if deterministic_center:\n      t_samples = linspline.linspace(t, num_samples, dtype=t.dtype)\n    else:\n      t_samples = linspline.linspace(t, num_samples, dtype=t.dtype) + 0.5 * (t[Ellipsis, 1:] - t[Ellipsis, :-1])\n  return t_samples"}
{"namespace": "stepfun.sample_intervals", "completion": "  t_samples = sample(rng, t, w_logits, num_samples, single_jitter=single_jitter)\n\n  # Calculate midpoints between adjacent samples\n  t_midpoints = (t_samples[..., :-1] + t_samples[..., 1:]) / 2\n\n  # Adjust the first and last intervals to fit within the specified domain\n  t_midpoints = jnp.clip(t_midpoints, domain[0], domain[1])\n\n  return t_midpoints"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  cw = integrate_weights(w)\n  return math.sorted_interp(ps / 100, cw, t, utils.device_is_tpu())"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert weights to a probability density function (PDF)\n  p = weight_to_pdf(t, w)\n  \n  # Blur the PDF using a Gaussian filter\n  blurred_p = linspline.gaussian_filter1d(p, blur_halfwidth)\n  \n  # Resample the blurred PDF to match the new time points\n  resampled_w = pdf_to_weight(tq, resample(tq, t, blurred_p))\n  \n  return resampled_w"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    return await self._request_stream(\n      'POST',\n      '/api/generate',\n      json={\n        'model': model,\n        'prompt': prompt,\n        'system': system,\n        'template': template,\n        'context': context or [],\n        'stream': stream,\n        'raw': raw,\n        'images': [_encode_image(image) for image in images or []],\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    async with aiofiles.open(path, 'rb') as r:\n      while True:\n        chunk = await r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      async with aiofiles.open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=await r.read())\n\n    return digest"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Run Pyright to perform type checking\n        pyright_output = subprocess.run(\n            [\"npx\", \"pyright\", \"--outputjson\", \"-\"],\n            input=(user_code + test_code).encode(\"utf-8\"),\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            check=False,\n        )\n\n        # Parse Pyright output to identify type errors\n        error_messages = []\n        for line in pyright_output.stderr.decode(\"utf-8\").splitlines():\n            match = re.match(cls.PYRIGHT_MESSAGE_REGEX, line)\n            if match:\n                line_number, error_message = match.groups()\n                error_messages.append(f\"Line {line_number}: {error_message}\")\n\n        # Determine if the type check passed or failed\n        passed = len(error_messages) == 0\n        message = \"Type check passed\" if passed else \"Type check failed\"\n\n        # Create TypeCheckResult object with the result of the type check\n        result = TypeCheckResult(message, passed, {\"error_messages\": error_messages})\n        return result"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    if isinstance(fn, torch.nn.Module):\n        with no_fake_tensor():\n            compiled_module = aot_module(fn, get_compiler_fn(\"Forward Graph\"), get_compiler_fn(\"Backward Graph\"))\n        return compiled_module\n    else:\n        with no_fake_tensor():\n            compiled_function = aot_function(fn, get_compiler_fn(\"Forward Graph\"), get_compiler_fn(\"Backward Graph\"))\n        return compiled_function"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_file = os.path.join(trial_path, 'summary.csv')\n    config_file = os.path.join(trial_path, 'config.yaml')\n\n    if not os.path.exists(summary_file):\n        raise FileNotFoundError(f\"Summary file not found in {trial_path}\")\n\n    if not os.path.exists(config_file):\n        raise FileNotFoundError(f\"Config file not found in {trial_path}\")\n\n    summary_df = load_summary_file(summary_file)\n    with open(config_file, 'r') as file:\n        config_dict = yaml.safe_load(file)\n\n    best_config = summary_df_to_yaml(summary_df, config_dict)\n\n    if output_path:\n        if not output_path.endswith('.yaml') and not output_path.endswith('.yml'):\n            raise ValueError(\"Output file must have .yaml or .yml extension\")\n        with open(output_path, 'w') as file:\n            yaml.dump(best_config, file)\n\n    return best_config"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    cache = {}\n    lock = threading.Lock()\n\n    @functools.wraps(func)\n    def lazy_traced_func(*args, **kwargs):\n        nonlocal cache\n        key = (func, args, frozenset(kwargs.items()))\n        with lock:\n            if key not in cache:\n                if isinstance(func, torch.nn.Module):\n                    traced_module, _ = trace_with_kwargs(func, **kwargs_)\n                    if ts_compiler:\n                        traced_module = ts_compiler(traced_module)\n                    cache[key] = traced_module\n                else:\n                    traced_func = better_trace(func, *args, **kwargs_)\n                    if ts_compiler:\n                        traced_func = ts_compiler(traced_func)\n                    cache[key] = traced_func\n        return cache[key](*args, **kwargs)\n\n    return lazy_traced_func"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        output_path = os.path.join(trial_path, 'best_config.yaml')\n        yaml_dict = extract_best_config(trial_path, output_path)\n        project_dir = os.path.dirname(trial_path)\n        return cls(yaml_dict, project_dir)"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "\n    @avoid_empty_result([0, 1])\n    def filter_results(results, value, threshold, metadatas=None) -> Tuple[List, List]:\n        filtered_results = [result for result, val in zip(results, value) if val <= threshold]\n        if metadatas:\n            filtered_metadatas = [metadata for result, val, metadata in zip(results, value, metadatas) if val <= threshold]\n        else:\n            filtered_metadatas = []\n        return filtered_results, filtered_metadatas\n\n    return filter_results(results, value, threshold, metadatas)"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Initialize empty list to store results\n    results = []\n\n    # Loop through each module and its parameters\n    for module, params in zip(modules, module_params):\n        # Execute the module with the given parameters\n        result = module(**params)\n\n        # Measure the speed of the module execution\n        speed = measure_speed(result)\n\n        # Filter the result based on speed threshold\n        filtered_result = filter_by_threshold(result, speed, strategies['speed_threshold'])\n\n        # Evaluate the filtered result\n        evaluation_metrics = evaluate_retrieval(filtered_result, previous_result, strategies['evaluation_metrics'])\n\n        # Store the result, speed, and evaluation metrics in a dictionary\n        result_info = {\n            'result': filtered_result,\n            'speed': speed,\n            'evaluation_metrics': evaluation_metrics\n        }\n\n        # Append the result info to the results list\n        results.append(result_info)\n\n    # Select the best result based on the evaluation metrics\n    best_result = select_best_average(results, strategies['evaluation_metrics'])\n\n    # Save the results and summary to disk\n    save_path = os.path.join(node_line_dir, 'retrieval_results.csv')\n    summary_path = os.path.join(node_line_dir, 'retrieval_summary.csv')\n\n    best_result['result'].to_csv(save_path, index=False)\n    summary_df = pd.DataFrame([{'module': module.__name__, 'speed': result['speed'], **result['evaluation_metrics']} for module, result in zip(modules, results)])\n    summary_df.to_csv(summary_path, index=False)\n\n    # Return the best result dataframe\n    return best_result['result']"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "\n    # Normalize scores\n    normalized_scores = []\n    for score_list in scores:\n        max_score = max(score_list)\n        normalized_score_list = [score / max_score for score in score_list]\n        normalized_scores.append(normalized_score_list)\n\n    # Combine scores using convex combination\n    combined_scores = []\n    for i in range(len(ids)):\n        combined_score_list = [normalized_scores[i][j] * weights[i] for j in range(len(ids[i]))]\n        combined_scores.append(combined_score_list)\n\n    # Sum the combined scores\n    summed_scores = [sum(x) for x in zip(*combined_scores)]\n\n    # Select top_k results\n    sorted_results = sorted(zip(summed_scores, *ids), reverse=True)[:top_k]\n    fused_ids = [list(x[1:]) for x in sorted_results]\n    fused_scores = [x[0] for x in sorted_results]\n\n    return fused_ids, fused_scores"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "\n    # Calculate the weighted sum of scores for each ID\n    weighted_sums = [sum(w * s for w, s in zip(weights, score)) for score in zip(*scores)]\n\n    # Normalize the weighted sums\n    normalized_sums = [(x - min(weighted_sums)) / (max(weighted_sums) - min(weighted_sums)) for x in weighted_sums]\n\n    # Get the top K IDs and their corresponding scores based on the weighted sum\n    top_k_indices = sorted(range(len(normalized_sums)), key=lambda i: normalized_sums[i], reverse=True)[:top_k]\n    top_k_ids = [ids[i] for i in top_k_indices]\n    top_k_scores = [normalized_sums[i] for i in top_k_indices]\n\n    return top_k_ids, top_k_scores"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        self.cleanup_agents()\n        return self.agent_lifecycle.agents"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        if isinstance(agent.purpose_embedding, np.ndarray):\n            purpose_embedding = agent.purpose_embedding.tolist()\n        else:\n            purpose_embedding = agent.purpose_embedding\n\n        agent_dict = {\n            \"dynamic_prompt\": agent.dynamic_prompt,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": purpose_embedding,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input\n        }\n\n        return agent_dict"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            prompt = f\"{PROMPT_ENGINEERING_SYSTEM_PROMPT} {goal}\\n{PROMPT_ENGINEERING_TEMPLATE}\\n{EXAMPLES}\\n{sample_input}\"\n            chat_completion = self.openai_wrapper.get_chat_completion(prompt)\n            return chat_completion\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        purpose_embedding = data.get(\"purpose_embedding\")\n        if purpose_embedding is not None and isinstance(purpose_embedding, list):\n            purpose_embedding = np.array(purpose_embedding)  # Convert list to ndarray\n\n        return MicroAgent(\n            dynamic_prompt=data.get(\"dynamic_prompt\"),\n            purpose=data.get(\"purpose\"),\n            purpose_embedding=purpose_embedding,\n            depth=data.get(\"depth\"),\n            max_depth=data.get(\"max_depth\"),\n            usage_count=data.get(\"usage_count\"),\n            id=data.get(\"id\"),\n            parent_id=data.get(\"parent_id\"),\n            working_agent=data.get(\"working_agent\"),\n            is_prime=data.get(\"is_prime\"),\n            evolve_count=data.get(\"evolve_count\"),\n            number_of_code_executions=data.get(\"number_of_code_executions\"),\n            last_input=data.get(\"last_input\"),\n            agent_lifecycle=agent_lifecycle,\n            openai_wrapper=openai_wrapper\n        )"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT * FROM agents WHERE id = ?\", (agent_dict['id'],))\n            existing_agent = cursor.fetchone()\n            if existing_agent:\n                conn.execute(\"UPDATE agents SET purpose = ?, data = ? WHERE id = ?\", (agent_dict['purpose'], json.dumps(agent_dict), agent_dict['id']))\n            else:\n                conn.execute(\"INSERT INTO agents (id, purpose, data) VALUES (?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict)))"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            agent_data = cursor.fetchone()\n            if agent_data:\n                return json.loads(agent_data[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows]"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        data = (func_name, args, kwargs)\n        data_str = json.dumps(data, sort_keys=True)\n        return hashlib.sha256(data_str.encode()).hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "    cursor = self.connection.execute(\n        \"SELECT result FROM cache WHERE hash = ?\", (arg_hash,)\n    )\n    row = cursor.fetchone()\n    if row is not None:\n        return json.loads(row[0])\n    else:\n        return None"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        conn = sqlite3.connect(filename)\n        c = conn.cursor()\n        c.execute(f\"CREATE TABLE IF NOT EXISTS {func_name} (input TEXT, output TEXT, PRIMARY KEY (input))\")\n        conn.commit()\n\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            input_str = json.dumps([args, kwargs], sort_keys=True)\n            key = hashlib.md5(input_str.encode()).hexdigest()\n            c.execute(f\"SELECT output FROM {func_name} WHERE input = ?\", (key,))\n            result = c.fetchone()\n            if result:\n                return json.loads(result[0])\n            else:\n                output = func(*args, **kwargs)\n                c.execute(f\"INSERT OR REPLACE INTO {func_name} (input, output) VALUES (?, ?)\", (key, json.dumps(output)))\n                conn.commit()\n                return output\n\n        conn.close()\n        return wrapper\n\n    return decorator"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        serialized_result = json.dumps(result)\n        self.connection.execute(\"INSERT INTO cache (hash, result) VALUES (?, ?)\", (arg_hash, serialized_result))"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    for arg in vars(args):\n        if getattr(args, arg) is not None:\n            CONFIG[arg.upper()] = getattr(args, arg)\n\n    # Redirect standard output to a file if quiet mode is enabled\n    if quiet_mode:\n        with open('output.log', 'w') as f:\n            with redirect_stdout(f):\n                command_line = CommandLine()\n                command_line.execute()\n    else:\n        command_line = CommandLine()\n        command_line.execute()"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "        model_name = get_model_name(\n            kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n        )\n        logger.debug(\"chatcompletion: using \" + model_name)\n        chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n        if \"azure_endpoint\" in chatcompletion_kwargs:\n            api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n            chatcompletion_kwargs.update({\"api_base\": api_base})\n        chatcompletion_kwargs.update(kwargs)\n\n        try:\n            response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n            response = json.loads(str(response))\n            if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                raise BadRequestError(\"maximum context length exceeded\", None)\n        except BadRequestError as e:\n            if \"maximum context length\" in e._message:\n                if model_name == \"gpt-4\":\n                    if \"gpt-4-32k\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-32k\"\n                    elif \"gpt-4-1106-preview\" in CONFIG.api_keys:\n                        model_name = \"gpt-4-1106-preview\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                elif model_name == \"gpt-3.5-turbo\":\n                    if \"gpt-3.5-turbo-1106\" in CONFIG.api_keys:\n                        model_name = \"gpt-3.5-turbo-1106\"\n                    else:\n                        model_name = \"gpt-3.5-turbo-16k\"\n                else:\n                    raise e\n                print(\"max context length reached, retrying with \" + model_name)\n                chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n                chatcompletion_kwargs.update(kwargs)\n                chatcompletion_kwargs.pop(\"schema_error_retry\", None)\n\n                response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n                response = json.loads(str(response))\n            else:\n                raise e\n\n        return response"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        current_time = time()\n        if self._client is None or self._last_time is None or current_time - self._last_time > self._refetch_interval:\n            self._create_client()\n            self._last_time = current_time\n        return self._client"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"state_dict should not be called from a DataLoader worker process.\")\n\n        state_dict = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir\": self.input_dir.path if self.input_dir.path else self.input_dir.url,\n            \"item_loader_state\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle\n        }\n\n        return state_dict"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "\n        # Resize the rendering context to match the camera's width and height\n        eglctx.resize(camera.width, camera.height)\n\n        # Set up the offscreen rendering environment\n        eglctx.make_current()\n        common_opengl_options()\n        use_gl_program(self.mesh_program)\n\n        # Set the camera parameters for rendering\n        view_matrix = glm.lookAt(camera.position, camera.look_at, camera.up)\n        projection_matrix = glm.perspective(glm.radians(camera.fov), camera.aspect_ratio, camera.near, camera.far)\n\n        # Set the uniform values for the mesh program\n        gl.glUniformMatrix4fv(gl.glGetUniformLocation(self.mesh_program, \"view\"), 1, gl.GL_FALSE, glm.value_ptr(view_matrix))\n        gl.glUniformMatrix4fv(gl.glGetUniformLocation(self.mesh_program, \"projection\"), 1, gl.GL_FALSE, glm.value_ptr(projection_matrix))\n\n        # Bind the vertex array object and draw the mesh\n        gl.glBindVertexArray(self.vao)\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        else:\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, None)\n\n        # Unbind the vertex array object and reset the rendering context\n        gl.glBindVertexArray(0)\n        eglctx.reset_current()"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    nomic_config = NomicBertConfig(\n        vocab_size=bert_config.vocab_size,\n        hidden_size=bert_config.hidden_size,\n        num_hidden_layers=bert_config.num_hidden_layers,\n        num_attention_heads=bert_config.num_attention_heads,\n        intermediate_size=bert_config.intermediate_size,\n        hidden_act=bert_config.hidden_act,\n        hidden_dropout_prob=bert_config.hidden_dropout_prob,\n        attention_probs_dropout_prob=bert_config.attention_probs_dropout_prob,\n        max_position_embeddings=bert_config.max_position_embeddings,\n        type_vocab_size=bert_config.type_vocab_size,\n        initializer_range=bert_config.initializer_range,\n        layer_norm_eps=bert_config.layer_norm_eps,\n        output_past=True,\n        is_decoder=False,\n        use_cache=True\n    )\n    \n    return nomic_config"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        if self.render_type == Mesh.RenderType.POINTS:\n            program = use_gl_program(self.point_program)\n            gl.glUniform1f(gl.glGetUniformLocation(program, \"point_radius\"), self.point_radius)\n        else:\n            program = use_gl_program(self.mesh_program)\n            gl.glUniform1i(gl.glGetUniformLocation(program, \"shade_flat\"), self.shade_flat)\n\n        self.upload_gl_uniforms(program, camera)\n\n        gl.glBindVertexArray(self.vao)\n\n        if self.faces is not None:\n            if self.render_type == Mesh.RenderType.LINES:\n                gl.glDrawElements(gl.GL_LINES, len(self.faces), gl.GL_UNSIGNED_INT, None)\n            elif self.render_type == Mesh.RenderType.TRIS:\n                gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces), gl.GL_UNSIGNED_INT, None)\n            elif self.render_type == Mesh.RenderType.QUADS:\n                gl.glDrawElements(gl.GL_QUADS, len(self.faces), gl.GL_UNSIGNED_INT, None)\n            elif self.render_type == Mesh.RenderType.STRIPS:\n                gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces), gl.GL_UNSIGNED_INT, None)\n        else:\n            if self.render_type == Mesh.RenderType.LINES:\n                gl.glDrawArrays(gl.GL_LINES, 0, len(self.verts))\n            elif self.render_type == Mesh.RenderType.TRIS:\n                gl.glDrawArrays(gl.GL_TRIANGLES, 0, len(self.verts))\n            elif self.render_type == Mesh.RenderType.QUADS:\n                gl.glDrawArrays(gl.GL_QUADS, 0, len(self.verts))\n            elif self.render_type == Mesh.RenderType.STRIPS:\n                gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, len(self.verts))\n\n        gl.glBindVertexArray(0)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.cpu().numpy()  # convert PyTorch tensor to numpy array\n\n        if w == 0:\n            w = self.W  # default to object's width if not provided\n        if h == 0:\n            h = self.H  # default to object's height if not provided\n\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)  # bind the texture to be updated\n        gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)  # update the texture content\n        gl.glBindTexture(gl.GL_TEXTURE_2D, 0)  # unbind the texture"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Validate input shapes\n    assert R.shape[:-2] == tvec.shape[:-2] == camera_matrix.shape[:-2] == image_size.shape[:-1], \"Input shapes do not match\"\n\n    # Validate input values\n    assert torch.all(torch.abs(torch.det(R) - 1) < 1e-3), \"Rotation matrix determinant should be 1\"\n    assert torch.all(torch.abs(camera_matrix[..., 1, 0] - 0) < 1e-3), \"Camera matrix should have 0 skew\"\n    assert torch.all(torch.abs(camera_matrix[..., 0, 1] - 0) < 1e-3), \"Camera matrix should have 0 skew\"\n    assert torch.all(torch.abs(camera_matrix[..., 2, :2] - 0) < 1e-3), \"Principal point should be at the center\"\n\n    # Compute focal length\n    fx = (camera_matrix[..., 0, 0] + camera_matrix[..., 1, 1]) / 2\n    fy = (camera_matrix[..., 0, 0] + camera_matrix[..., 1, 1]) / 2\n\n    # Compute sensor width\n    sensor_width = image_size[..., 0]\n\n    # Compute camera position\n    tvec = tvec.view(-1, 3, 1)\n    camera_position = -R.transpose(-1, -2) @ tvec\n\n    # Convert rotation matrix to 6D representation\n    rotation_6d = matrix_to_rotation_6d(R)\n\n    # Return camera parameters\n    return torch.cat([camera_position, rotation_6d, fx.view(-1, 1), fy.view(-1, 1), sensor_width.view(-1, 1)], dim=-1)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "    if not self.use_quad_draw:\n        self.blit(x, y, w, h)\n    else:\n        if w == 0:\n            w = self.W\n        if h == 0:\n            h = self.H\n\n        old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        old_scissor_box = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        gl.glUseProgram(self.quad_program)\n        gl.glBindVertexArray(self.vao)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n\n        gl.glBindVertexArray(0)\n        gl.glUseProgram(0)\n\n        gl.glViewport(*old_viewport)\n        gl.glScissor(*old_scissor_box)"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n\n    # Adjust R and T to match PyTorch3D's requirements\n    R = R.permute(0, 2, 1)  # Transpose R\n    T = -torch.bmm(R, T)  # Adjust T\n\n    # Recalculate the intrinsic matrix K for NDC\n    K = K.clone()\n    K[:, 0, 2] = W - K[:, 0, 2]  # Adjust principal point x-coordinate\n    K[:, 1, 2] = H - K[:, 1, 2]  # Adjust principal point y-coordinate\n    K[:, 1, 1] = -K[:, 1, 1]  # Flip y-focal length\n\n    return H, W, K, R, T, C"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBlitFramebuffer(0, 0, self.W, self.H, x, y, x + (self.W if w == 0 else w), y + (self.H if h == 0 else h), gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, 0)"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Find the indices of the closest elements in t1 for each element in t0\n    idx = searchsorted(t1, t0)\n\n    # Match up the channels of y1 with the indices found\n    y1_matched = matchup_channels(y1, idx)\n\n    # Calculate the cumulative sum of y1_matched\n    inner = torch.cumsum(y1_matched, dim=1)\n\n    # Calculate the outer measure by subtracting the inner measure from the cumulative sum of y1\n    outer = torch.cumsum(y1, dim=1) - inner\n\n    return inner, outer"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    y0_inner, y0_outer = inner_outer(t, t_env, w_env)\n\n    w_env_outer = y0_outer * w_env\n    w_env_outer_sum = torch.sum(w_env_outer, dim=-1)\n\n    loss = torch.sum(F.relu(w - w_env_outer_sum) ** 2 / (w + eps))\n\n    return loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "\n    # Calculate the outer loss using the provided tensors\n    outer_loss = lossfun_outer(t, w, t, w)\n\n    # Calculate the zip outer loss using the provided tensors and a pulse width\n    zip_outer_loss = lossfun_zip_outer(t, w, t, w, pulse_width=0.1)\n\n    # Combine the outer and zip outer losses to produce the total distortion loss\n    total_distortion_loss = outer_loss + zip_outer_loss\n\n    return total_distortion_loss"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "\n    t, w = matchup_channels(t, w)\n    cw = integrate_weights(w)\n    percentiles = interpolate(torch.tensor(ps), cw[..., :-1], t)\n    return percentiles"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    idx_lo = torch.searchsorted(a, v, right=False)\n    idx_hi = torch.searchsorted(a, v, right=True)\n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Compute the cumulative sum of the weights\n    cw = integrate_weights(w)\n\n    # Generate uniform random numbers for sampling\n    u = torch.rand(num_samples, device=t.device)\n\n    # Invert the CDF to generate samples\n    samples = invert_cdf(u, t, cw)\n\n    # Apply perturbation if specified\n    if perturb:\n        if single_jitter:\n            # Apply the same jitter to every sample along each dimension\n            jitter = torch.rand(samples.shape, device=samples.device) * (t[1] - t[0])\n        else:\n            # Apply independent jitter to each sample\n            jitter = torch.rand_like(samples) * (t[1] - t[0])\n        samples += jitter\n\n    return samples"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        width = self.tensor[:, 2]\n        height = self.tensor[:, 3]\n        area = width * height\n        return area"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    if name == \"PrecomputedProposals\":\n        return None\n    return PROPOSAL_GENERATOR_REGISTRY.get(name)(cfg, input_shape)"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "\n        scores, proposal_deltas = predictions\n        gt_boxes = [x.gt_boxes for x in proposals]\n        gt_classes = [x.gt_classes for x in proposals]\n\n        pred_class_logits, pred_proposal_deltas = scores, proposal_deltas\n\n        gt_classes, gt_proposal_deltas = self.prepare_targets(gt_classes, gt_boxes, pred_class_logits.device)\n\n        loss_cls = F.cross_entropy(pred_class_logits, gt_classes)\n\n        loss_box_reg = _dense_box_regression_loss(\n            pred_proposal_deltas,\n            gt_proposal_deltas,\n            gt_classes,\n            self.smooth_l1_beta,\n            box_reg_loss_type=self.box_reg_loss_type,\n            num_classes=self.num_classes,\n        )\n\n        losses = {\"loss_cls\": loss_cls, \"loss_box_reg\": loss_box_reg}\n        return {k: v * self.loss_weight[k] for k, v in losses.items()}"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    name = cfg.MODEL.TRACKER_HEAD\n    return TRACKER_HEADS_REGISTRY.get(name)(cfg)"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent large transformation\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        # x1\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        # y1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        # x2\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        # y2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        img, k = resize_image(image, 512)\n        img = np.transpose(img, (2, 0, 1))\n        img = np.expand_dims(img, axis=0)\n        img = img.astype(np.float32)\n        img = (img - 127.5) / 127.5\n\n        with torch.no_grad():\n            output = self.general_ins({'img': img})\n\n        if anno_type is None:\n            return output\n        elif isinstance(anno_type, str):\n            return output.get(anno_type, None)\n        elif isinstance(anno_type, (list, tuple)):\n            return {key: output.get(key, None) for key in anno_type}\n        else:\n            raise Exception(f'Error anno_type: {anno_type}')"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        normalized_query = normalize_string(query)\n        keywords = normalized_query.split()\n        result = {}\n        for kw in keywords:\n            scores = self.bm25(kw)\n            result = update_url_scores(result, scores)\n        return result"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n        nearly_horizontal = torch.abs(self.tensor[:, 4]) < clip_angle_threshold\n        if nearly_horizontal.any():\n            boxes = self.tensor[nearly_horizontal]\n            x_c, y_c, w, h, angle = boxes.unbind(1)\n            cos_theta = torch.cos(angle * math.pi / 180)\n            sin_theta = torch.sin(angle * math.pi / 180)\n            x1 = x_c - w / 2\n            y1 = y_c - h / 2\n            x2 = x_c + w / 2\n            y2 = y_c + h / 2\n            # Clamp x1, y1, x2, y2 to ensure they fit within box_size\n            x1 = torch.clamp(x1, 0, box_size[1])\n            y1 = torch.clamp(y1, 0, box_size[0])\n            x2 = torch.clamp(x2, 0, box_size[1])\n            y2 = torch.clamp(y2, 0, box_size[0])\n            # Convert back to original representation\n            w = x2 - x1\n            h = y2 - y1\n            x_c = (x1 + x2) / 2\n            y_c = (y1 + y2) / 2\n            angle = torch.atan2(sin_theta, cos_theta) * 180 / math.pi\n            self.tensor[nearly_horizontal] = torch.stack((x_c, y_c, w, h, angle), dim=1)"}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        statistics = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n        for item in self.data:\n            if 'type' in item:\n                statistics[item['type']] += 1\n        return statistics"}
{"namespace": "common.bleu4_score", "completion": "\n    # Tokenize the input texts using a custom tokenizer function\n    continuation_tokens = jieba.lcut(continuation)\n    reference_tokens = jieba.lcut(reference)\n\n    # Compute the BLEU-4 score using the tokenized texts\n    bleu_score = evaluate.bleu(continuation_tokens, [reference_tokens], 4)\n\n    # Optionally adjust the score for brevity penalty\n    if with_penalty:\n        brevity_penalty = min(1, len(continuation_tokens) / len(reference_tokens))\n        bleu_score *= brevity_penalty\n\n    return bleu_score"}
{"namespace": "common.rougeL_score", "completion": "\n    f = lambda text: list(jieba.cut(text))\n    rouge = evaluate.load('uhgeval/.cache/huggingface/rouge')\n    results = rouge.compute(predictions=[continuation], references=[[reference]], tokenizer=f)\n    score = results['rougeL']\n    return score"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Running command: {cmd}\")\n    return os.system(cmd)"}
