{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"output_dir must be an instance of Dir\")\n\n    if output_dir.url and output_dir.url.startswith(\"s3://\"):\n        if not _BOTO3_AVAILABLE:\n            raise ImportError(\"boto3 is required to check directories in S3 but is not available.\")\n        if overwrite:\n            return\n        # Parse the S3 URL to get the bucket name and prefix\n        parsed_url = parse.urlparse(output_dir.url)\n        bucket_name = parsed_url.netloc\n        prefix = parsed_url.path.lstrip('/')\n\n        # Initialize a boto3 S3 client\n        s3_client = boto3.client('s3')\n        # List objects in the specified S3 bucket and prefix\n        objects = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n        # Check if the directory is empty\n        if objects.get('Contents'):\n            raise FileExistsError(f\"The directory at {output_dir.url} is not empty.\")\n\n    elif output_dir.path:\n        if overwrite:\n            return\n        # Check if the directory is empty for local paths\n        if os.path.exists(output_dir.path) and os.listdir(output_dir.path):\n            raise FileExistsError(f\"The directory at {output_dir.path} is not empty.\")"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        raise ValueError(\"The provided output_dir does not have a URL.\")\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.url}.\")\n\n    if not _BOTO3_AVAILABLE:\n        raise ImportError(\"boto3 is required to check the S3 bucket but it's not installed.\")\n\n    s3 = boto3.client(\"s3\")\n\n    # Check if the index.json file exists in the specified S3 bucket directory\n    try:\n        s3.head_object(Bucket=obj.netloc, Key=obj.path.lstrip(\"/\") + \"/index.json\")\n    except botocore.exceptions.ClientError as e:\n        error_code = int(e.response['Error']['Code'])\n        if error_code == 404:\n            # The index.json file does not exist, raise an error\n            raise FileNotFoundError(f\"No index.json file found in the specified S3 bucket directory: {output_dir.url}\")\n        else:\n            # Some other error occurred, re-raise the exception\n            raise"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "        # Wait until all workers have written their index files\n        expected_index_files = num_workers\n        index_files = []\n        while len(index_files) < expected_index_files:\n            index_files = [\n                f for f in os.listdir(self._cache_dir) if f.endswith(_INDEX_FILENAME)\n            ]\n            if len(index_files) < expected_index_files:\n                sleep(1)  # Wait for a second before checking again\n\n        # If node_rank is not provided, assume single-node or rank is handled externally\n        if node_rank is None or node_rank == 0:\n            # Only the master node should perform the merge\n            all_chunks_info = []\n            all_configs = []\n\n            # Read all index files and collect their contents\n            for index_file in index_files:\n                with open(os.path.join(self._cache_dir, index_file), 'r') as f:\n                    index_data = json.load(f)\n                    all_chunks_info.extend(index_data['chunks'])\n                    all_configs.append(index_data['config'])\n\n            # Verify that all configs are identical\n            if not all(config == all_configs[0] for config in all_configs):\n                raise ValueError(\"Not all configs are identical across index files.\")\n\n            # Sort chunks by their indices\n            all_chunks_info.sort(key=lambda x: (x['filename']))\n\n            # Write the merged index file\n            merged_index_file = os.path.join(self._cache_dir, f\"merged.{_INDEX_FILENAME}\")\n            with open(merged_index_file, 'w') as f:\n                json.dump({\"chunks\": all_chunks_info, \"config\": all_configs[0]}, f, sort_keys=True)\n\n            # Optionally, remove individual index files after merging\n            for index_file in index_files:\n                os.remove(os.path.join(self._cache_dir, index_file))\n        else:\n            # Non-master nodes wait until the merged index file is available\n            merged_index_file = os.path.join(self._cache_dir, f\"merged.{_INDEX_FILENAME}\")\n            while not os.path.exists(merged_index_file):\n                sleep(1)  # Wait for a second before checking again"}
{"namespace": "litdata.streaming.resolver._execute", "completion": "    if not _LIGHTNING_SDK_AVAILABLE:\n        raise ImportError(\"The required Lightning SDK is not available.\")\n\n    # Fetch default machine configuration if not provided\n    if machine is None:\n        machine = Machine()  # Assuming Machine() fetches the default configuration\n\n    # Construct the default command if not provided\n    if command is None:\n        command = f\"cd {os.getcwd()} && {' '.join([f'{k}={v}' for k, v in os.environ.items()])} && python -u {sys.argv[0]}\"\n\n    # Create a Studio instance\n    studio = Studio()\n\n    # Create the job\n    job = studio.create_job(\n        name=name,\n        num_nodes=num_nodes,\n        machine_type=machine.type,\n        command=command,\n        environment_variables=os.environ,\n    )\n\n    # Wait for the job to start and print the job URL\n    while job.status not in [\"running\", \"failed\"]:\n        sleep(5)  # Wait for 5 seconds before checking the status again\n        job.refresh()  # Refresh the job status\n\n    if job.status == \"running\":\n        print(f\"Job URL: {job.url}\")\n    elif job.status == \"failed\":\n        raise RuntimeError(f\"Job {name} failed to start.\")"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "        # Attempt to load the configuration from the cache directory\n        try:\n            # The ChunksConfig class is typically responsible for loading the configuration\n            # from a set of index files that describe how the data is chunked.\n            # Here, we assume that the ChunksConfig class has a method `load_from_cache_dir`\n            # which takes the cache directory, serializers, remote input directory, and item loader\n            # as arguments and returns a ChunksConfig object if the index files are present and valid.\n            self._config = ChunksConfig.load_from_cache_dir(\n                cache_dir=self._cache_dir,\n                serializers=self._serializers,\n                remote_input_dir=self._remote_input_dir,\n                item_loader=self._item_loader\n            )\n            return self._config\n        except Exception as e:\n            # If there is an error loading the configuration, log the exception and return None\n            logger.error(f\"Failed to load chunks configuration: {e}\")\n            return None"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "        # Enqueue the chunk indexes into the download queue\n        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n\n        # Optionally, you can log the enqueued chunks for debugging purposes\n        logger.debug(f\"Enqueued chunks for download: {chunk_indexes}\")"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "        if self._config is None:\n            raise RuntimeError(\"The configuration must be defined before accessing it.\")\n        return self._config"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "        if not isinstance(index, ChunkedIndex):\n            raise ValueError(f\"Expected index to be of type ChunkedIndex, got {type(index)}\")\n\n        # Ensure the config is loaded\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        # Get the chunk index from the provided index\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # If the chunk index is different from the last one, we need to prepare it\n        if chunk_index != self._last_chunk_index:\n            # Ensure the prepare thread is running\n            if self._prepare_thread is None:\n                self._prepare_thread = PrepareChunksThread(\n                    config=self._config,\n                    item_loader=self._item_loader,\n                    distributed_env=self._distributed_env,\n                    max_cache_size=self._max_cache_size,\n                )\n                self._prepare_thread.start()\n\n            # Download the chunk if it's not already being downloaded\n            self._prepare_thread.download([chunk_index])\n            self._last_chunk_index = chunk_index\n\n        # Wait for the chunk to be available\n        chunk_filepath, _, _ = self._config[index]\n        self._item_loader.wait_for_chunk(chunk_index, chunk_filepath)\n\n        # Read the item from the chunk\n        return self._item_loader.load_item(index)"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "    total_size = 0\n    for dirpath, dirnames, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            try:\n                total_size += os.path.getsize(fp)\n            except FileNotFoundError:\n                # If the file was deleted between os.walk and os.path.getsize, ignore it\n                pass\n    return total_size"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "    # Check if the application is running in a distributed environment\n    lightning_app_external_url = os.getenv(\"LIGHTNING_APP_EXTERNAL_URL\")\n    if lightning_app_external_url:\n        # We are in a distributed environment, so use the _ImmutableDistributedMap to broadcast the object\n        distributed_map = _ImmutableDistributedMap()\n        return distributed_map.set_and_get(key, obj)\n    else:\n        # We are not in a distributed environment, so return the object as is\n        return obj"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Create a list of items with their weights and sort it in descending order by weight\n    items_with_weights = list(zip(items, weights))\n    items_with_weights.sort(key=lambda x: x[1], reverse=True)\n\n    # Iterate over the items and place each one in the bin with the lowest total weight\n    for item, weight in items_with_weights:\n        # Find the bin with the lowest total weight\n        lightest_bin = min(bin_weights, key=bin_weights.get, default=0)\n        # Place the item in the bin\n        bins[lightest_bin].append(item)\n        # Update the total weight of the bin\n        bin_weights[lightest_bin] += weight\n\n        # If we have fewer bins than the specified number, create a new one\n        if len(bin_weights) < num_bins:\n            next_bin_index = len(bin_weights)\n            bins[next_bin_index]  # This ensures the bin exists even if it remains empty\n            bin_weights[next_bin_index] = 0\n\n    return dict(bins), dict(bin_weights)"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Create a new random number generator with the given seed and epoch\n    rng = np.random.default_rng(seed + current_epoch)\n\n    # Shuffle the chunks for each rank within the node\n    shuffled_chunks = []\n    for rank_chunks in chunks_per_ranks:\n        # Shuffle the chunks using the random number generator\n        rng.shuffle(rank_chunks)\n        shuffled_chunks.extend(rank_chunks)\n\n    return shuffled_chunks"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "    if num_bytes < 1000:\n        return f\"{num_bytes} B\"\n    for unit, ratio in reversed(list(_FORMAT_TO_RATIO.items())):\n        if num_bytes >= ratio:\n            value = float(num_bytes) / ratio\n            return f\"{value:.1f} {unit.upper()}\"\n    # If the number is larger than the largest unit in the dictionary, we can add support for petabytes\n    if num_bytes >= 1000**5:\n        value = float(num_bytes) / 1000**5\n        return f\"{value:.1f} PB\"\n    raise ValueError(\"num_bytes must be a non-negative number\")"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "    # Check the first two elements for valid file paths\n    paths_to_check = inputs[:2]\n    valid_paths = []\n\n    for item in paths_to_check:\n        indexed_paths = _get_indexed_paths(item)\n        if indexed_paths:\n            # Take the first valid path found\n            valid_paths.append(next(iter(indexed_paths.values())))\n\n    if not valid_paths:\n        # No valid file paths found in the inputs\n        return None\n\n    # Ensure that all valid paths found are consistent\n    common_prefix = os.path.commonprefix(valid_paths)\n    if not common_prefix or not os.path.isdir(common_prefix):\n        raise ValueError(\"Inconsistent file paths found in inputs.\")\n\n    # Resolve the input directory to an absolute path\n    input_dir = _resolve_dir(Dir(common_prefix))\n\n    return input_dir"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "    # Placeholder for the original DNS optimization state\n    original_state = False  # Assume DNS optimization is initially disabled\n\n    try:\n        if enable:\n            # Placeholder for enabling DNS optimization\n            print(\"DNS optimization enabled.\")\n        else:\n            # Placeholder for disabling DNS optimization\n            print(\"DNS optimization disabled.\")\n        # Yield control back to the context block\n        yield\n    finally:\n        # Reset DNS optimization to its original state\n        if original_state:\n            # Placeholder for enabling DNS optimization\n            print(\"DNS optimization reset to enabled.\")\n        else:\n            # Placeholder for disabling DNS optimization\n            print(\"DNS optimization reset to disabled.\")"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "    total_items = len(indexes)\n    items_per_rank = total_items // distributed_env.world_size\n    if not drop_last:\n        items_per_rank += int(total_items % distributed_env.world_size > 0)\n\n    chunks_per_ranks = [[] for _ in range(distributed_env.world_size)]\n    intervals_per_ranks = [[] for _ in range(distributed_env.world_size)]\n\n    for rank in range(distributed_env.world_size):\n        start_idx = rank * items_per_rank\n        end_idx = start_idx + items_per_rank\n        if drop_last and end_idx > total_items:\n            end_idx = total_items\n\n        for idx in indexes[start_idx:end_idx]:\n            chunks_per_ranks[rank].append(idx)\n            intervals_per_ranks[rank].append(chunk_intervals[idx])\n\n    return chunks_per_ranks, intervals_per_ranks"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "        # Prepare the keyword arguments for the transformation function\n        kwargs = {}\n        if self._contains_device:\n            kwargs['device'] = self._device\n        if self._contains_is_last:\n            kwargs['is_last'] = is_last\n\n        # Call the transformation function with the item metadata, output directory, and any additional keyword arguments\n        self._fn(item_metadata, output_dir, **kwargs)"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "    bucket_name = obj.netloc\n    object_key = obj.path.lstrip('/')\n    \n    while True:\n        try:\n            # Attempt to retrieve the file metadata using the head_object method\n            response = s3.head_object(Bucket=bucket_name, Key=object_key)\n            return response\n        except s3.exceptions.NoSuchKey:\n            # If the file is not found, wait for sleep_time seconds before trying again\n            sleep(sleep_time)\n        except Exception as e:\n            # If any other error occurs, raise the error\n            raise e"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    threshold_in_bytes = threshold_in_gb * 1024**3  # Convert GB to bytes\n    while True:\n        # Get the total, used, and free disk space in bytes\n        total, used, free = shutil.disk_usage(input_dir)\n        # Check if the used disk space is higher than the threshold\n        if used > threshold_in_bytes:\n            break  # Exit the loop if the threshold is exceeded\n        # Sleep for the specified amount of time before checking again\n        sleep(sleep_time)"}
{"namespace": "litdata.processing.functions.optimize", "completion": "\n    # Resolve the output directory\n    _output_dir: Dir = _resolve_dir(output_dir)\n\n    # Check if the output directory is valid\n    if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n        raise ValueError(\n            f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n            \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n        )\n\n    # Check if the output directory should be empty\n    if error_when_not_empty:\n        _assert_dir_is_empty(_output_dir)\n\n    # Resolve the input directory\n    input_dir = _resolve_dir(_get_input_dir(inputs))\n\n    # If batch_size is specified, group the inputs into batches\n    if isinstance(batch_size, int) and batch_size > 1:\n        inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n\n    # Create a DataProcessor instance\n    data_processor = DataProcessor(\n        input_dir=input_dir,\n        output_dir=_output_dir,\n        num_workers=num_workers or _get_default_num_workers(),\n        fast_dev_run=fast_dev_run,\n        num_downloaders=num_downloaders,\n        num_uploaders=num_uploaders,\n        reorder_files=reorder_files,\n        weights=weights,\n        reader=reader,\n    )\n\n    # Create a LambdaDataChunkRecipe instance\n    data_chunk_recipe = LambdaDataChunkRecipe(\n        fn=fn,\n        inputs=inputs,\n        chunk_size=chunk_size,\n        chunk_bytes=chunk_bytes,\n        compression=compression,\n    )\n\n    # Run the data processing\n    with optimize_dns_context(True):\n        data_processor.run(data_chunk_recipe)"}
{"namespace": "litdata.processing.functions.map", "completion": "    output_dir = _resolve_dir(output_dir)\n\n    # Check if the output directory is empty if required\n    if error_when_not_empty and not _assert_dir_is_empty(output_dir):\n        raise ValueError(f\"The output directory {output_dir} is not empty.\")\n\n    # Determine the number of workers to use\n    if num_workers is None:\n        num_workers = _get_default_num_workers()\n\n    # If fast_dev_run is enabled, limit the number of inputs to process\n    if fast_dev_run:\n        if isinstance(fast_dev_run, bool):\n            inputs = inputs[:1]\n        elif isinstance(fast_dev_run, int) and fast_dev_run > 0:\n            inputs = inputs[:fast_dev_run]\n\n    # Create a DataProcessor instance\n    data_processor = DataProcessor(\n        recipe=LambdaDataTransformRecipe(fn, inputs),\n        output_dir=output_dir,\n        num_workers=num_workers,\n        weights=weights,\n        reader=reader,\n        batch_size=batch_size,\n        num_downloaders=num_downloaders,\n        num_uploaders=num_uploaders,\n        reorder_files=reorder_files,\n    )\n\n    # Execute the data processing\n    data_processor.execute()"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client()\n\n    while True:\n        try:\n            # Fetch a download task from the input queue\n            task = queue_in.get(timeout=5)  # Adjust timeout as needed\n            if task is None:\n                # A None task is a signal to stop the worker\n                break\n\n            task_index, file_paths = task\n            for file_path in file_paths:\n                # Construct the full source and destination paths\n                source_url = input_dir.get_url(file_path)\n                destination_path = os.path.join(cache_dir, file_path)\n\n                # Check if the file already exists in the cache\n                if not os.path.exists(destination_path):\n                    # Ensure the destination directory exists\n                    os.makedirs(os.path.dirname(destination_path), exist_ok=True)\n\n                    # Parse the source URL\n                    obj = parse.urlparse(source_url)\n\n                    # Wait for the file to exist in the source if it's not found\n                    _wait_for_file_to_exist(s3, obj)\n\n                    # Download the file from the source to the destination\n                    s3.download_file(obj.netloc, obj.path.lstrip(\"/\"), destination_path)\n\n            # Signal that the download task is complete by putting the index into the output queue\n            queue_out.put(task_index)\n\n        except Empty:\n            # No task in the queue, worker can do a short sleep before trying again\n            sleep(1)\n        except Exception as e:\n            logger.error(f\"Error occurred while downloading data: {e}\")\n            traceback.print_exc()\n            # Optionally, put a sentinel value or error message into the output queue\n            queue_out.put(f\"Error: {e}\")\n            break\n\n    # Clean up before exiting\n    logger.info(f\"Worker process for downloading data to {cache_dir} is shutting down.\")"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client() if output_dir.url and output_dir.url.startswith(\"s3://\") else None\n\n    while True:\n        # Fetch from the queue\n        item = upload_queue.get()\n\n        # Terminate the process if we received a termination signal\n        if item is None:\n            return\n\n        # Unpack the item if it's a tuple of (temp_dir, file_path)\n        if isinstance(item, tuple):\n            temp_dir, file_path = item\n        else:\n            file_path = item\n\n        # Ensure the file path is absolute by prepending the cache directory if necessary\n        if not file_path.startswith(cache_dir):\n            file_path = os.path.join(cache_dir, file_path)\n\n        # Determine the target path based on the output directory\n        if output_dir.url:\n            target_path = file_path.replace(cache_dir, output_dir.url)\n        else:\n            target_path = file_path.replace(cache_dir, output_dir.path)\n\n        # Upload or move the file\n        try:\n            if s3:\n                # Parse the URL to get the bucket and key\n                obj = parse.urlparse(target_path)\n                bucket = obj.netloc\n                key = obj.path.lstrip('/')\n\n                # Upload the file to S3\n                with open(file_path, 'rb') as f:\n                    s3.client.upload_fileobj(f, bucket, key)\n            else:\n                # Ensure the target directory exists\n                os.makedirs(os.path.dirname(target_path), exist_ok=True)\n\n                # Move the file to the target directory\n                shutil.move(file_path, target_path)\n\n            # Notify the removal queue that the file has been uploaded\n            remove_queue.put(file_path)\n        except Exception as e:\n            logger.error(f\"Failed to upload {file_path} to {target_path}: {e}\")\n            traceback.print_exc()"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    if weights is None:\n        weights = [1] * len(user_items)  # Assign equal weight if no weights are provided\n\n    if len(weights) != len(user_items):\n        raise ValueError(\"The length of weights must match the length of user_items\")\n\n    # Calculate the total weight and the cumulative weight for each item\n    total_weight = sum(weights)\n    cumulative_weights = list(np.cumsum(weights))\n\n    # Calculate the total number of workers across all nodes\n    num_nodes = _get_num_nodes()\n    world_size = num_nodes * num_workers\n\n    # Calculate the weight per worker\n    weight_per_worker = total_weight / world_size\n\n    # Distribute items to workers based on weights\n    worker_items = [[] for _ in range(num_workers)]\n    current_worker = 0\n    current_worker_weight = 0\n\n    for item, weight in zip(user_items, weights):\n        if current_worker_weight + weight > weight_per_worker and current_worker < num_workers - 1:\n            # Move to the next worker if the current one has reached its capacity\n            current_worker += 1\n            current_worker_weight = 0\n        worker_items[current_worker].append(item)\n        current_worker_weight += weight\n\n    # Shuffle the items for each worker\n    for items in worker_items:\n        random.shuffle(items)\n\n    # Print distribution details for workers on the current node\n    node_rank = _get_node_rank()\n    for i, items in enumerate(worker_items):\n        worker_global_idx = node_rank * num_workers + i\n        if file_size:\n            total_size_mb = sum(weights[j] for j in range(len(user_items)) if user_items[j] in items) / (1024 * 1024)\n            print(f\"Worker {worker_global_idx} will process {len(items)} files with a total size of {total_size_mb:.2f} MB\")\n        else:\n            total_weight = sum(weights[j] for j in range(len(user_items)) if user_items[j] in items)\n            print(f\"Worker {worker_global_idx} will process {len(items)} items with a total weight of {total_weight}\")\n\n    return worker_items"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Calculate the total number of workers across all nodes\n    total_workers = _get_num_nodes() * num_workers\n\n    # Calculate the number of items each worker should process\n    items_per_worker = len(user_items) // total_workers\n    remainder = len(user_items) % total_workers\n\n    # Distribute the items among the workers\n    items_to_workers = []\n    start_index = 0\n    for i in range(total_workers):\n        # Calculate the end index, adding an extra item if there's a remainder\n        end_index = start_index + items_per_worker + (1 if i < remainder else 0)\n        # Assign the items to the worker\n        items_to_workers.append(user_items[start_index:end_index])\n        # Update the start index for the next worker\n        start_index = end_index\n\n    # Ensure that the output list has a length equal to the number of workers\n    if len(items_to_workers) != total_workers:\n        raise RuntimeError(\"The number of workers does not match the number of assigned item lists.\")\n\n    # Since we need to return the items for workers on the current node only, we slice the list accordingly\n    node_rank = _get_node_rank()\n    start_index = node_rank * num_workers\n    end_index = start_index + num_workers\n    return items_to_workers[start_index:end_index]"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": "        # Define the cache directories\n        cache_data_dir = _get_cache_data_dir()\n        cache_chunks_dir = _get_cache_dir()\n\n        # Remove the cache data directory if it exists\n        if os.path.exists(cache_data_dir):\n            shutil.rmtree(cache_data_dir, ignore_errors=True)\n\n        # Remove the cache chunks directory if it exists\n        if os.path.exists(cache_chunks_dir):\n            shutil.rmtree(cache_chunks_dir, ignore_errors=True)\n\n        # Recreate the cache directories\n        os.makedirs(cache_data_dir, exist_ok=True)\n        os.makedirs(cache_chunks_dir, exist_ok=True)"}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "    # Determine the number of workers based on the CPU count\n    num_workers = min(32, max(1, cpu_count() // 2))\n\n    # Define a function to get the size of a single item\n    def get_size(item):\n        return _get_num_bytes(item, base_path)\n\n    # Use ThreadPoolExecutor to parallelize the file size retrieval\n    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n        # Map the get_size function to all items and collect the results\n        sizes = list(executor.map(get_size, items))\n\n    return sizes"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "    if not isinstance(element, str):\n        return False\n\n    element_path = Path(element)\n    if input_dir:\n        input_dir_path = Path(input_dir)\n        try:\n            # Check if element is a subpath of input_dir\n            element_path.relative_to(input_dir_path)\n            return True\n        except ValueError:\n            # element is not a subpath of input_dir\n            pass\n\n    # Check if the element exists in the file system\n    return element_path.exists()"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "        assert n_layers > 0, \"Number of layers must be greater than 0\"\n        assert n_neurons > 0, \"Number of neurons must be greater than 0\"\n\n        layers = []\n\n        # Input layer\n        layers.append(nn.Linear(n_input_dims, n_neurons))\n        if activation == \"ReLU\":\n            layers.append(nn.ReLU())\n\n        # Hidden layers\n        for _ in range(n_layers - 2):  # Subtract 2 because we already added the input layer and will add the output layer\n            layers.append(nn.Linear(n_neurons, n_neurons))\n            if activation == \"ReLU\":\n                layers.append(nn.ReLU())\n\n        # Output layer\n        layers.append(nn.Linear(n_neurons, n_output_dims))\n        if output_activation == \"ReLU\":\n            layers.append(nn.ReLU())\n        elif output_activation == \"Sigmoid\":\n            layers.append(nn.Sigmoid())\n\n        # Create the sequential model\n        model = nn.Sequential(*layers)\n\n        return model"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "    # Calculate the area for each polygon\n    areas = [cv2.contourArea(polygon) for polygon in polygons]\n\n    # Find the largest area\n    max_area = max(areas) if areas else 0\n\n    # Calculate the relative area threshold based on the largest area\n    rel_area_threshold = rel_tr * max_area\n\n    # Determine the effective threshold by taking the maximum of the relative and absolute thresholds\n    effective_threshold = max(rel_area_threshold, abs_tr)\n\n    # Filter out polygons that don't meet the area criteria\n    filtered_polygons = [polygon for polygon, area in zip(polygons, areas) if area >= effective_threshold]\n\n    return filtered_polygons"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "        # Initialize an empty list to hold the medians\n        medians = []\n\n        # Loop over the signal array while applying the kernel offset\n        for i in range(kernel_offset, len(signal) - kernel_offset):\n            # Extract the window of values around the current index\n            window = signal[i - kernel_offset:i + kernel_offset + 1]\n            # Compute the median of the window and append to the medians list\n            medians.append(np.median(window))\n\n        # Convert the list of medians to a numpy array and return\n        return np.array(medians)"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "    # Extract iris and mask bits from the templates\n    irisbits_probe = template_probe.irisbits\n    maskbits_probe = template_probe.maskbits\n    irisbits_gallery = template_gallery.irisbits\n    maskbits_gallery = template_gallery.maskbits\n\n    # Determine the total code size and half width\n    total_codesize = irisbits_probe.size\n    half_width = [irisbits_probe.shape[1] // 2]\n\n    # Calculate the square root of total bits\n    sqrt_totalbitcount, _, _ = count_sqrt_totalbits(total_codesize, half_width, weights)\n\n    # Initialize variables to store the minimum Hamming distance and corresponding shift\n    min_HD = float('inf')\n    min_shift = 0\n\n    # Loop over the allowed rotation shifts\n    for shift in range(-rotation_shift, rotation_shift + 1):\n        # Shift the gallery iris and mask bits\n        shifted_irisbits_gallery = np.roll(irisbits_gallery, shift, axis=1)\n        shifted_maskbits_gallery = np.roll(maskbits_gallery, shift, axis=1)\n\n        # Calculate the common mask bits\n        common_maskbits = maskbits_probe & shifted_maskbits_gallery\n\n        # Calculate nonmatch bits\n        irisbitcount, maskbitcount, _, _ = count_nonmatchbits(\n            [irisbits_probe], [shifted_irisbits_gallery], half_width, weights\n        )\n\n        # Calculate the Hamming distance\n        HD = irisbitcount / maskbitcount if maskbitcount != 0 else 0\n\n        # Normalize the Hamming distance if nm_dist is provided\n        if nm_dist is not None:\n            HD = normalized_HD(irisbitcount, maskbitcount, sqrt_totalbitcount, nm_dist)\n\n        # Update the minimum Hamming distance and corresponding shift\n        if HD < min_HD:\n            min_HD = HD\n            min_shift = shift\n\n    return min_HD, min_shift"}
{"namespace": "iris.utils.math.area", "completion": "    if array.shape[1] != 2:\n        raise ValueError(\"Input array must have shape (_, 2).\")\n\n    n = array.shape[0]\n    sum1 = np.dot(array[:, 0], np.roll(array[:, 1], -1))\n    sum2 = np.dot(array[:, 1], np.roll(array[:, 0], -1))\n    area = abs(sum1 - sum2) / 2.0\n\n    return area"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "    num_points = polygon.shape[0]\n    num_bisectors = self.params.num_bisectors\n    max_iterations = self.params.max_iterations\n\n    first_bisectors_point = []\n    second_bisectors_point = []\n\n    for _ in range(num_bisectors):\n        iterations = 0\n        while iterations < max_iterations:\n            # Randomly select two points from the polygon\n            idx1, idx2 = np.random.choice(num_points, 2, replace=False)\n            point1 = polygon[idx1]\n            point2 = polygon[idx2]\n\n            # Calculate the distance between the two points\n            distance = np.linalg.norm(point1 - point2)\n\n            # Check if the distance is greater than the minimum required\n            if distance >= min_distance_between_sector_points_in_px:\n                # Calculate the midpoint\n                midpoint = (point1 + point2) / 2\n\n                # Calculate the slope of the perpendicular bisector\n                if point2[0] - point1[0] == 0:  # Avoid division by zero\n                    slope_perpendicular = 0\n                else:\n                    slope = (point2[1] - point1[1]) / (point2[0] - point1[0])\n                    if slope == 0:\n                        slope_perpendicular = float('inf')  # Vertical line\n                    else:\n                        slope_perpendicular = -1 / slope\n\n                # Calculate the starting and ending points of the bisector\n                if slope_perpendicular == float('inf'):\n                    start_point = np.array([midpoint[0], 0])\n                    end_point = np.array([midpoint[0], 1])\n                elif slope_perpendicular == 0:\n                    start_point = np.array([0, midpoint[1]])\n                    end_point = np.array([1, midpoint[1]])\n                else:\n                    start_point = midpoint + np.array([1, slope_perpendicular])\n                    end_point = midpoint - np.array([1, slope_perpendicular])\n\n                first_bisectors_point.append(start_point)\n                second_bisectors_point.append(end_point)\n                break\n            iterations += 1\n\n        if iterations == max_iterations:\n            raise EyeCentersEstimationError(\"Failed to find sufficient point pairs for bisectors.\")\n\n    return np.array(first_bisectors_point), np.array(second_bisectors_point)"}
{"namespace": "iris.utils.math.polygon_length", "completion": "    # Ensure the polygon has at least two points to calculate a length\n    if polygon.shape[0] < 2:\n        return 0.0\n\n    # Calculate the distances between consecutive points\n    distances = np.linalg.norm(np.diff(polygon, axis=0), axis=1)\n\n    # Sum the distances that are less than or equal to the max_point_distance\n    length = np.sum(distances[distances <= max_point_distance])\n\n    return length"}
{"namespace": "iris.io.validators.is_binary", "completion": "    if v.dtype != np.bool_:\n        raise ValueError(f\"{cls.__name__}: {field.name} must contain only boolean values.\")\n\n    return v"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "    if v.ndim != 2 or v.shape[1] != 2:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points with shape (_, 2).\")\n\n    return v"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "    # Check if v is an iterable (but not a string, as strings are also iterable)\n    if isinstance(v, Iterable) and not isinstance(v, str):\n        if not all(item > 0 for item in v):\n            raise ValueError(f\"{cls.__name__}: {field.name} must contain only positive values.\")\n    else:\n        # Check if v is a single value\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    \n    return v"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "    x_min = values.get(\"x_min\")\n    x_max = values.get(\"x_max\")\n    y_min = values.get(\"y_min\")\n    y_max = values.get(\"y_max\")\n\n    if x_min is None or x_max is None or y_min is None or y_max is None:\n        raise ValueError(f\"{cls.__name__}: Bounding box values must include 'x_min', 'x_max', 'y_min', and 'y_max'.\")\n\n    if x_min >= x_max:\n        raise ValueError(f\"{cls.__name__}: 'x_min' must be less than 'x_max'. Received x_min={x_min}, x_max={x_max}\")\n\n    if y_min >= y_max:\n        raise ValueError(f\"{cls.__name__}: 'y_min' must be less than 'y_max'. Received y_min={y_min}, y_max={y_max}\")\n\n    return values"}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "    def validator(cls: type, v: np.ndarray, field: fields.ModelField) -> np.ndarray:\n        \"\"\"\n        Validator function to check if an array has the specified number of dimensions.\n\n        Args:\n            cls (type): Class type.\n            v (np.ndarray): Array to check.\n            field (fields.ModelField): Field descriptor.\n\n        Raises:\n            ValueError: Exception raised if array doesn't have the specified number of dimensions.\n\n        Returns:\n            np.ndarray: `v` sent for further processing.\n        \"\"\"\n        if v.ndim != nb_dimensions:\n            raise ValueError(f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions. Got {v.ndim} dimensions.\")\n        return v\n\n    return validator"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "\n    def __root_validator(cls: type, values: Dict[str, List[np.ndarray]]) -> Dict[str, List[np.ndarray]]:\n        \"\"\"Check if all arrays in field1 have the same shape as their counterparts in field2.\"\"\"\n        list1 = values.get(field1, [])\n        list2 = values.get(field2, [])\n\n        if len(list1) != len(list2):\n            raise ValueError(f\"{cls.__name__}: {field1} and {field2} have different lengths.\")\n\n        for i, (arr1, arr2) in enumerate(zip(list1, list2)):\n            if arr1.shape != arr2.shape:\n                raise ValueError(f\"{cls.__name__}: Shapes of arrays at index {i} in {field1} and {field2} do not match ({arr1.shape} vs {arr2.shape}).\")\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "\n    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if the shapes of field1 and field2 are equal.\"\"\"\n        if field1 not in values or field2 not in values:\n            raise ValueError(f\"{cls.__name__}: Both {field1} and {field2} must be present in the values.\")\n\n        shape1 = values[field1].shape\n        shape2 = values[field2].shape\n\n        if shape1 != shape2:\n            raise ValueError(\n                f\"{cls.__name__}: The shapes of {field1} and {field2} do not match. \"\n                f\"Shape of {field1} is {shape1}, while shape of {field2} is {shape2}.\"\n            )\n\n        return values\n\n    return __root_validator"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": "        # Call pre-execution hooks\n        for callback in self._callbacks:\n            callback.pre_execute(self, *args, **kwargs)\n\n        # Run the main algorithm logic\n        result = self.run(*args, **kwargs)\n\n        # Call post-execution hooks\n        for callback in self._callbacks:\n            callback.post_execute(self, *args, **kwargs, result=result)\n\n        return result"}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            deserialized_output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(deserialized_output, type_definition)"}
{"namespace": "tanuki.utils.encode_int", "completion": "    charset = string.ascii_lowercase + string.digits + '_'\n    \n    # Ensure the input integer is within the valid range\n    if 0 <= n < len(charset):\n        # Return the character at the index 'n' in the character set\n        return charset[n]\n    else:\n        # If 'n' is out of range, raise an error\n        raise ValueError(f\"Integer {n} is out of encoding range.\")"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "\n        # Helper function to get class definitions for type hints\n        def get_class_definition(type_hint):\n            origin = get_origin(type_hint)\n            if origin is not None:  # This is a generic type\n                return [get_class_definition(arg) for arg in type_hint.__args__]\n            elif hasattr(type_hint, '__module__') and type_hint.__module__ != 'builtins':\n                # Non-built-in type, fetch its class definition\n                return type_hint\n            return None\n\n        # Get the function's signature and type hints\n        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        # Extract input and output type hints\n        input_type_hints = {k: v for k, v in type_hints.items() if k in signature.parameters}\n        output_type_hint = type_hints.get('return', None)\n\n        # Fetch class definitions for input and output type hints\n        input_class_definitions = {k: get_class_definition(v) for k, v in input_type_hints.items()}\n        output_class_definition = get_class_definition(output_type_hint)\n\n        # Determine the function type based on the output type hint\n        if inspect.isclass(output_type_hint) and issubclass(output_type_hint, Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        elif get_origin(output_type_hint) is Union:\n            function_type = FunctionType.SYMBOLIC\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        # Create the FunctionDescription instance\n        function_description = FunctionDescription(\n            name=func_object.__name__,\n            docstring=func_object.__doc__,\n            input_type_hints=input_type_hints,\n            output_type_hint=output_type_hint,\n            input_class_definitions=input_class_definitions,\n            output_class_definition=output_class_definition,\n            type=function_type\n        )\n\n        return function_description"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n            self.bit_array[index] = 1\n            self.indices[index] += 1"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        loaded_bit_array = self.persistence.load()\n        if len(loaded_bit_array) != self.size:\n            logging.warning(\"Loaded bit array length does not match expected size. Reinitializing the bit array.\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n        else:\n            self.bit_array = loaded_bit_array\n            # Assuming that indices are not saved and loaded, as they are not used in the provided code.\n            # If indices need to be recalculated or loaded, additional code would be required here."}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for i in range(self.hash_count):\n            # Generate the ith index using double hashing\n            index = (hash1 + i * hash2) % self.size\n            # Check if the bit at the index is not set\n            if not self.bit_array[index]:\n                # If any bit is not set, the string is definitely not in the filter\n                return False\n        # If all bits are set, the string might be in the filter\n        return True"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "    if zeros >= n:\n        raise ValueError(\"The number of zeros must be less than the number of weights\")\n\n    # Generate n-zeros positive weights that sum to one\n    positive_weights = np.random.rand(n - zeros)\n    positive_weights /= positive_weights.sum()\n\n    # Initialize the final weights array with zeros\n    weights = np.zeros(n)\n\n    # Choose random indices to set the positive weights\n    positive_indices = np.random.choice(n, size=n - zeros, replace=False)\n\n    # Set the positive weights at the chosen indices\n    weights[positive_indices] = positive_weights\n\n    return weights"}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "    # Check if the matrix is square\n    if x.shape[0] != x.shape[1]:\n        return False\n\n    # Attempt Cholesky decomposition\n    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        if 'distilled_model' in json_dict:\n            self.distilled_model = config_factory.create_model_config(json_dict['distilled_model'])\n        if 'current_model_stats' in json_dict:\n            self.current_model_stats = json_dict['current_model_stats']\n        if 'last_training_run' in json_dict:\n            self.last_training_run = json_dict['last_training_run']\n        if 'current_training_run' in json_dict:\n            self.current_training_run = json_dict['current_training_run']\n        if 'nr_of_training_runs' in json_dict:\n            self.nr_of_training_runs = json_dict['nr_of_training_runs']\n        if 'teacher_models' in json_dict:\n            self.teacher_models = [config_factory.create_model_config(teacher_model) for teacher_model in json_dict['teacher_models']]"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # Validate and prepare the parameters for the API call\n        generation_params = {k: v for k, v in kwargs.items() if k in LLM_GENERATION_PARAMETERS}\n        payload = {\n            \"model\": model.model_name,\n            \"prompt\": f\"{system_message}{prompt}\",\n            **generation_params\n        }\n\n        # Attempt to call the API up to 5 times with exponential backoff\n        max_retries = 5\n        backoff_factor = 2\n        for attempt in range(max_retries):\n            try:\n                response = requests.post(\n                    OPENAI_URL,\n                    headers={\"Authorization\": f\"Bearer {self.api_key}\"},\n                    json=payload\n                )\n                response.raise_for_status()  # Raise an HTTPError if the HTTP request returned an unsuccessful status code\n\n                # Process the response\n                response_data = response.json()\n                generated_text = response_data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n                \n                # Remove parsing helper tokens if present in the model configuration\n                if hasattr(model, 'parsing_helper_tokens'):\n                    for token in model.parsing_helper_tokens:\n                        generated_text = generated_text.replace(token, '')\n\n                return generated_text.strip()\n\n            except requests.exceptions.HTTPError as http_err:\n                logging.error(f\"HTTP error occurred: {http_err}\")\n                time.sleep(backoff_factor ** attempt)\n            except Exception as err:\n                logging.error(f\"An error occurred: {err}\")\n                time.sleep(backoff_factor ** attempt)\n\n        logging.error(\"Maximum retries reached. Failed to generate text.\")\n        return None"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2 or x.shape[0] != x.shape[1]:\n        raise ValueError(\"The input array must be a square matrix (same number of rows and columns).\")"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    assert_is_square(x)  # Check if the matrix is square\n    if not np.allclose(x, x.T, atol=1e-8):  # Check if the matrix is equal to its transpose\n        raise ValueError(\"The matrix is not symmetric\")"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "    assert_is_square(cov)  # Ensure the covariance matrix is square\n\n    # Calculate the standard deviations\n    std_devs = np.sqrt(np.diag(cov))\n\n    # Avoid division by zero\n    std_devs[std_devs == 0] = 1\n\n    # Create the correlation matrix\n    corr = cov / np.outer(std_devs, std_devs)\n\n    # Ensure the diagonal elements are exactly 1\n    np.fill_diagonal(corr, 1.0)\n\n    return corr, std_devs"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)  # Check if the matrix is square\n    assert_is_symmetric(x)  # Check if the matrix is symmetric\n\n    # Check if the diagonal elements are close to zero\n    if not np.allclose(np.diag(x), 0):\n        raise ValueError(\"The diagonal elements of the distance matrix must be close to zero\")"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # Check if the function has been initialized\n        if func_hash not in self.initialized_functions:\n            # Initialize the function with default values\n            self.initialized_functions[func_hash] = {\n                \"model\": \"\",\n                \"examples\": [],\n                \"align_statements\": []\n            }\n            # Add examples for fine-tuning if necessary\n            # This part of the code would depend on how examples are added in your system\n            # For example:\n            # self.initialized_functions[func_hash][\"examples\"].extend(self._get_examples_for_finetuning(function_description))\n\n        # Determine if the function is suitable for distillation\n        suitable_for_distillation = self._is_suitable_for_distillation(function_description, args, kwargs)\n\n        # Select the model based on suitability for distillation\n        if suitable_for_distillation:\n            model = self.api_provider.get_distilled_model()\n        else:\n            model = self.api_provider.get_teacher_model()\n\n        # Construct the prompt based on the function description and arguments\n        prompt = self._construct_prompt(function_description, args, kwargs)\n\n        # Check if the token count exceeds the limit\n        token_count = approximate_token_count(prompt)\n        self.token_counts[func_hash] = token_count\n        if token_count > self.default_generation_length:\n            # Adjust the model or parameters if necessary\n            # For example, you might switch to a model that can handle more tokens\n            pass\n\n        # Determine if the function is already initialized and does not require saving examples for fine-tuning\n        save_to_finetune = not suitable_for_distillation and token_count <= self.default_generation_length\n\n        return prompt, model, suitable_for_distillation, save_to_finetune"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "    assert_is_square(cov)\n    \n    if higham:\n        # Higham & Nick (2002) algorithm\n        n = cov.shape[0]\n        A_k = cov\n        for _ in range(higham_max_iteration):\n            R_k = np.linalg.cholesky(A_k)\n            S_k = (A_k + A_k.T) / 2\n            Y_k = np.dot(R_k, R_k.T)\n            A_k1 = S_k + (S_k - Y_k) / 2\n            A_k = A_k1\n            if is_cholesky_dec(A_k):\n                break\n        return A_k\n    else:\n        # Eigenvalue clipping method\n        eigvals, eigvecs = np.linalg.eigh(cov)\n        eigvals_clipped = np.maximum(eigvals, _CLIPPING_VALUE)\n        cov_nearest = np.dot(eigvecs, np.dot(np.diag(eigvals_clipped), eigvecs.T))\n        return cov_nearest"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "    for arr in x:\n        if arr.size > 1:  # Check if the array has more than one element\n            mid = arr.size // 2  # Find the middle index\n            yield [arr[:mid], arr[mid:]]  # Yield the two halves"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "    if np.isnan(x):\n        return \"NaN\"\n    \n    if percent:\n        x *= 100\n    \n    # Determine the number of decimal places based on the magnitude of x\n    if abs(x) < 0.01:\n        fmt_str = \"{:.4f}\"\n    elif abs(x) < 1:\n        fmt_str = \"{:.2f}\"\n    else:\n        fmt_str = \"{:.1f}\"\n    \n    formatted_str = fmt_str.format(x)\n    \n    if percent:\n        formatted_str += \"%\"\n    \n    return formatted_str"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "    if isinstance(items, dict):\n        if assets_names is None:\n            raise ValueError(f\"When {name} is a dictionary, assets_names cannot be None.\")\n        array = np.full((n_assets,), fill_value, dtype=np.float64)\n        for i, asset_name in enumerate(assets_names):\n            array[i] = items.get(asset_name, fill_value)\n    else:\n        array = np.asarray(items, dtype=np.float64)\n        if array.ndim != dim:\n            raise ValueError(f\"The dimension of {name} must be {dim}, got {array.ndim}.\")\n        if dim == 1 and array.shape[0] != n_assets:\n            raise ValueError(f\"The shape of {name} must be ({n_assets},), got {array.shape}.\")\n        if dim == 2 and array.shape[1] != n_assets:\n            raise ValueError(f\"The shape of {name} must be (n_groups, {n_assets}), got {array.shape}.\")\n\n    return array"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "    if data_home is None:\n        data_home = os.environ.get('SKFOLIO_DATA', Path.home() / 'skfolio_data')\n    else:\n        data_home = Path(data_home)\n\n    data_home = Path(data_home)\n    if not data_home.exists():\n        data_home.mkdir(parents=True, exist_ok=True)\n\n    return str(data_home)"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)  # Get the correct data home directory path\n\n    # Check if the data home directory exists\n    if os.path.exists(data_home):\n        # Remove all contents of the directory\n        for filename in os.listdir(data_home):\n            file_path = os.path.join(data_home, filename)\n            try:\n                if os.path.isfile(file_path) or os.path.islink(file_path):\n                    os.unlink(file_path)  # Remove file or link\n                elif os.path.isdir(file_path):\n                    shutil.rmtree(file_path)  # Remove directory and all its contents\n            except Exception as e:\n                print(f'Failed to delete {file_path}. Reason: {e}')"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, (str, bytes)):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, collections.abc.Mapping):\n        return DictSchema.flatten(obj)\n    elif isinstance(obj, collections.abc.Sequence) and not isinstance(obj, (str, bytes)):\n        if isinstance(obj, list):\n            return ListSchema.flatten(obj)\n        elif isinstance(obj, tuple):\n            return TupleSchema.flatten(obj)\n    elif isinstance(obj, Instances):\n        return InstancesSchema.flatten(obj)\n    elif isinstance(obj, (Boxes, ROIMasks)):\n        return TensorWrapSchema.flatten(obj)\n    elif isinstance(obj, torch.Tensor):\n        return (obj,), IdentitySchema()\n    else:\n        raise ValueError(f\"Unsupported type to flatten: {type(obj)}\")"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "    groups = np.asarray(groups)\n    n_groups, n_assets = groups.shape\n\n    # Initialize the left and right matrices\n    left = np.zeros((len(equations), n_assets))\n    right = np.zeros(len(equations))\n\n    # Compile regex pattern for parsing equations\n    pattern = re.compile(r'([+-]?)(\\d*\\.?\\d*)\\*?(\\w+)')\n\n    for i, eq in enumerate(equations):\n        # Parse the equation using regex\n        terms = pattern.findall(eq)\n        if not terms:\n            raise EquationToMatrixError(f\"Invalid equation format in {names[1]}[{i}]: '{eq}'\")\n\n        for sign, coeff, group in terms:\n            # Find the index of the group in the groups array\n            try:\n                group_idx = next(idx for idx, g in enumerate(groups) if g == group)\n            except StopIteration:\n                if raise_if_group_missing:\n                    raise GroupNotFoundError(f\"Group '{group}' not found in {names[0]}\")\n                else:\n                    warnings.warn(f\"Group '{group}' not found in {names[0]}\")\n                    continue\n\n            # Convert coefficient to float and apply sign\n            coeff = float(coeff) if coeff else 1.0\n            coeff = -coeff if sign == '-' else coeff\n\n            # Update the left matrix with the coefficient\n            left[i, :] += coeff * groups[group_idx, :]\n\n        # Extract the right-hand side value from the equation\n        rhs = eq.split('<=')[-1].strip()\n        try:\n            right[i] = float(rhs)\n        except ValueError:\n            raise EquationToMatrixError(f\"Invalid right-hand side value in {names[1]}[{i}]: '{rhs}'\")\n\n    if sum_to_one:\n        # Add an additional row to the left matrix to enforce the sum-to-one constraint\n        sum_to_one_row = np.ones((1, n_assets))\n        left = np.vstack([left, sum_to_one_row])\n        right = np.append(right, 1.0)\n\n    return left, right"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "    global _counter\n    _counter += 1\n    cls_name = f\"Instances{str(_counter)}\"\n    fields_code = \"\\n\".join([f\"    _{name}: {torch.jit.annotations.Optional[torch.Tensor]}\" for name, _ in fields])\n\n    # Define the new class with the specified fields\n    new_class_code = f\"\"\"\nimport torch\nfrom detectron2.structures import Boxes, Instances\n\n@torch.jit.script\nclass {cls_name}(Instances):\n{fields_code}\n\n    def __init__(self, image_size: torch.Size):\n        super().__init__(image_size)\n        {\"\".join([f\"self._{name} = None\\n        \" for name, _ in fields])}\n\"\"\"\n\n    with ExitStack() as stack:\n        # Create a temporary file to store the new class definition\n        tmp_file = stack.enter_context(tempfile.NamedTemporaryFile(mode=\"w+\", suffix=\".py\", delete=False))\n        tmp_file_name = tmp_file.name\n        try:\n            tmp_file.write(new_class_code)\n            tmp_file.flush()\n\n            # Import the new class from the temporary file\n            new_module = _import_file(cls_name, tmp_file_name)\n            newInstances = getattr(new_module, cls_name)\n\n            # Add conversion methods to the new class\n            _add_instances_conversion_methods(newInstances)\n\n            # Replace the original Instances class with the new class\n            original_Instances = detectron2.structures.Instances\n            detectron2.structures.Instances = newInstances\n\n            # Yield the new class within the context\n            yield newInstances\n\n        finally:\n            # Restore the original Instances class after exiting the context\n            detectron2.structures.Instances = original_Instances\n            _clear_jit_cache()\n            os.unlink(tmp_file_name)  # Clean up the temporary file"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    original_training_states = {}\n\n    # Save the original training state of each submodule\n    for name, submodule in model.named_modules():\n        original_training_states[name] = submodule.training\n        # Annotate the training attribute as a constant for TorchScript\n        submodule.__dict__[\"_is_training\"] = torch.jit.Final[bool](submodule.training)\n\n    try:\n        yield\n    finally:\n        # Revert the training state of each submodule to its original state\n        for name, submodule in model.named_modules():\n            if name in original_training_states:\n                # Remove the annotation\n                del submodule.__dict__[\"_is_training\"]\n                # Restore the original training state\n                submodule.training = original_training_states[name]"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  linear0 = srgb / (323 / 25)\n  linear1 = xnp.maximum(eps, ((200 * srgb + 11) / 211) ** (12 / 5))\n  return xnp.where(srgb <= 0.04045, linear0, linear1)"}
{"namespace": "resample.resample_3d", "completion": "  # Adjust for half-pixel centering if necessary\n  if half_pixel_center:\n    locations -= 0.5\n\n  # Handle edge behavior\n  if edge_behavior == 'CONSTANT_OUTSIDE':\n    # Pad the data with constant values\n    pad_width = [(1, 1) for _ in range(data.ndim)]\n    data = jnp.pad(data, pad_width, mode='constant', constant_values=constant_values)\n    # Adjust locations due to padding\n    locations += 1\n  elif edge_behavior == 'CLAMP':\n    # Clamp locations to be within the valid range\n    locations = jnp.clip(locations, 0, jnp.array(data.shape[:3]) - 1)\n\n  # Resample using the specified method\n  if method == 'TRILINEAR':\n    # Compute the integer and fractional parts of the locations\n    int_locations = jnp.floor(locations).astype(jnp.int32)\n    frac_locations = locations - int_locations\n\n    # Gather the eight corner points around each location\n    offsets = jnp.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n                         [1, 0, 0], [1, 0, 1], [1, 1, 0], [1, 1, 1]])\n    corner_points = int_locations[..., None, :] + offsets\n    corner_values = gather_volume(data, corner_points.reshape(-1, 3), coordinate_order)\n\n    # Interpolate along the z-axis\n    z_interp0 = (1 - frac_locations[..., 2]) * corner_values[..., 0, :] + frac_locations[..., 2] * corner_values[..., 1, :]\n    z_interp1 = (1 - frac_locations[..., 2]) * corner_values[..., 2, :] + frac_locations[..., 2] * corner_values[..., 3, :]\n    z_interp2 = (1 - frac_locations[..., 2]) * corner_values[..., 4, :] + frac_locations[..., 2] * corner_values[..., 5, :]\n    z_interp3 = (1 - frac_locations[..., 2]) * corner_values[..., 6, :] + frac_locations[..., 2] * corner_values[..., 7, :]\n\n    # Interpolate along the y-axis\n    y_interp0 = (1 - frac_locations[..., 1]) * z_interp0 + frac_locations[..., 1] * z_interp1\n    y_interp1 = (1 - frac_locations[..., 1]) * z_interp2 + frac_locations[..., 1] * z_interp3\n\n    # Interpolate along the x-axis\n    x_interp = (1 - frac_locations[..., 0]) * y_interp0 + frac_locations[..., 0] * y_interp1\n\n    return x_interp\n\n  elif method == 'NEAREST':\n    # Round locations to nearest integer\n    nearest_locations = jnp.round(locations).astype(jnp.int32)\n    # Gather the nearest values\n    return gather_volume(data, nearest_locations, coordinate_order)\n\n  else:\n    raise ValueError(f\"Unsupported interpolation method: {method}\")"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  # Compute the integral using the trapezoid rule\n  integral = jnp.trapz(w, t)\n  return integral"}
{"namespace": "linspline.query", "completion": "  # Ensure the spline starts and ends with zero\n  check_zero_endpoints(v)\n\n  # Find the indices of the segments each tq value falls into\n  segment_indices = jnp.searchsorted(t, tq, side='right') - 1\n\n  # Clip the indices to ensure they are within the valid range\n  segment_indices = jnp.clip(segment_indices, 0, len(t) - 2)\n\n  # Get the corresponding t values for the segments\n  t_low = t[segment_indices]\n  t_high = t[segment_indices + 1]\n\n  # Get the corresponding v values for the segments\n  v_low = v[segment_indices]\n  v_high = v[segment_indices + 1]\n\n  # Compute the slope of the segments\n  slopes = (v_high - v_low) / (t_high - t_low)\n\n  # Interpolate to find the value at each tq\n  interpolated_values = v_low + slopes * (tq - t_low)\n\n  # Set values outside the original t range to 0\n  interpolated_values = jnp.where((tq < t[0]) | (tq > t[-1]), 0, interpolated_values)\n\n  return interpolated_values"}
{"namespace": "geometry.are_lines_parallel", "completion": "  # Normalize the direction vectors\n  dir1_normalized = spin_math.normalize(dir1)\n  dir2_normalized = spin_math.normalize(dir2)\n\n  # Calculate the cross product of the normalized direction vectors\n  cross_product = jnp.cross(dir1_normalized, dir2_normalized)\n\n  # Check if the magnitude of the cross product is close to zero\n  return jnp.linalg.norm(cross_product) < epsilon"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  # Convert spherical to cartesian coordinates\n  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n  \n  return jnp.array([x, y, z])"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  assert v >= 1, \"Tessellation factor must be greater than or equal to 1.\"\n\n  # Generate all combinations of integer weights that sum up to v\n  weights = []\n  for i in range(v + 1):\n    for j in range(v + 1 - i):\n      k = v - i - j\n      weights.append((i, j, k))\n\n  # Convert to barycentric coordinates by dividing by the tessellation factor\n  weights = np.array(weights, dtype=np.float64) / v\n\n  return weights"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  x, y, z = cartesian_vector[..., 0], cartesian_vector[..., 1], cartesian_vector[..., 2]\n  r = jnp.sqrt(x**2 + y**2 + z**2)\n  theta = jnp.arccos(jnp.clip(z / (r + eps), -1.0, 1.0))  # Inclination\n  phi = jnp.arctan2(y, x)  # Azimuth\n  return r, theta, phi"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the sum of squares of each column (vector) in both matrices\n  sum_of_squares_0 = np.sum(mat0**2, axis=0)\n  sum_of_squares_1 = np.sum(mat1**2, axis=0)\n\n  # Compute the dot product between all pairs of columns (vectors)\n  dot_product = np.dot(mat0.T, mat1)\n\n  # Compute the squared distances using the mathematical property\n  # dist(x, y)^2 = ||x||^2 + ||y||^2 - 2 * dot(x, y)\n  sq_dists = np.add.outer(sum_of_squares_0, sum_of_squares_1) - 2 * dot_product\n\n  # Ensure numerical stability by setting negative distances to zero\n  sq_dists[sq_dists < 0] = 0\n\n  return sq_dists"}
{"namespace": "math.plus_eps", "completion": "  return jnp.where(x > tiny_val, jnp.nextafter(x, jnp.inf), tiny_val)"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), -jnp.inf)\n  )"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, y, x_dot: x_dot * y,\n      (-np.inf, np.log(max_val)),\n  )(x)"}
{"namespace": "math.safe_log", "completion": "def safe_log_grad(x, y, x_dot):\n  \"\"\"Gradient for the safe_log function.\"\"\"\n  return x_dot / clip_pos(x)\n\n# Define the range for x to avoid taking the log of non-positive values.\n# We use tiny_val as the lower bound to avoid log(0) which is undefined.\nx_range = (tiny_val, max_val)\n\n# Now we use generate_safe_fn to create the safe_log function.\nsafe_log = generate_safe_fn(jnp.log, safe_log_grad, x_range)"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda x, y, x_dot: 0.5 * x_dot / y,\n      (0, max_val),\n  )(x)"}
{"namespace": "math.power_ladder_max_output", "completion": "  # If p is less than 1, the output will converge to a finite value as x approaches infinity.\n  if p < 1:\n    return 1 / (1 - p)\n  # If p is exactly 1, the output will grow logarithmically, and there is no finite limit as x approaches infinity.\n  elif p == 1:\n    return jnp.inf\n  # If p is greater than 1, the output will grow to infinity at a rate determined by the power p.\n  else:\n    return jnp.inf"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape == 'tetrahedron':\n    # Vertices of a tetrahedron\n    sqrt_3 = np.sqrt(1.0 / 3.0)\n    base_verts = np.array([\n      [sqrt_3, sqrt_3, sqrt_3],\n      [sqrt_3, -sqrt_3, -sqrt_3],\n      [-sqrt_3, sqrt_3, -sqrt_3],\n      [-sqrt_3, -sqrt_3, sqrt_3]\n    ])\n    # Faces of a tetrahedron\n    base_faces = np.array([\n      [0, 1, 2],\n      [0, 3, 1],\n      [0, 2, 3],\n      [1, 3, 2]\n    ])\n  elif base_shape == 'icosahedron':\n    # Vertices of an icosahedron\n    phi = (1 + np.sqrt(5)) / 2\n    base_verts = np.array([\n      [-1, phi, 0],\n      [1, phi, 0],\n      [-1, -phi, 0],\n      [1, -phi, 0],\n      [0, -1, phi],\n      [0, 1, phi],\n      [0, -1, -phi],\n      [0, 1, -phi],\n      [phi, 0, -1],\n      [phi, 0, 1],\n      [-phi, 0, -1],\n      [-phi, 0, 1],\n    ])\n    base_verts /= np.linalg.norm(base_verts[0])\n    # Faces of an icosahedron\n    base_faces = np.array([\n      [0, 11, 5],\n      [0, 5, 1],\n      [0, 1, 7],\n      [0, 7, 10],\n      [0, 10, 11],\n      [1, 5, 9],\n      [5, 11, 4],\n      [11, 10, 2],\n      [10, 7, 6],\n      [7, 1, 8],\n      [3, 9, 4],\n      [3, 4, 2],\n      [3, 2, 6],\n      [3, 6, 8],\n      [3, 8, 9],\n      [4, 9, 5],\n      [2, 4, 11],\n      [6, 2, 10],\n      [8, 6, 7],\n      [9, 8, 1],\n    ])\n  elif base_shape == 'octahedron':\n    # Vertices of an octahedron\n    base_verts = np.array([\n      [1, 0, 0],\n      [-1, 0, 0],\n      [0, 1, 0],\n      [0, -1, 0],\n      [0, 0, 1],\n      [0, 0, -1]\n    ])\n    # Faces of an octahedron\n    base_faces = np.array([\n      [0, 2, 4],\n      [2, 1, 4],\n      [1, 3, 4],\n      [3, 0, 4],\n      [0, 5, 2],\n      [2, 5, 1],\n      [1, 5, 3],\n      [3, 5, 0]\n    ])\n  else:\n    raise ValueError(f'Unsupported base_shape: {base_shape}')\n\n  # Normalize the base vertices\n  base_verts /= np.linalg.norm(base_verts, axis=1)[:, np.newaxis]\n\n  # Tessellate the polyhedron\n  verts = tesselate_geodesic(base_verts, base_faces, angular_tesselation, eps)\n\n  # Optionally remove symmetries\n  if remove_symmetries:\n    # Compute the distance between each pair of vertices\n    sq_dist = compute_sq_dist(verts.T)\n    # Find the indices of unique vertices by checking the distances\n    unique_indices = np.unique(np.argwhere(sq_dist <= eps)[:, 0])\n    verts = verts[unique_indices]\n\n  return verts.T"}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + x),\n      (-1 + tiny_val, max_val),\n  )(x)"}
{"namespace": "math.power_ladder", "completion": "  # Apply pre-multiplication if specified\n  if premult is not None:\n    x = premult * x\n\n  # Handle special cases for the power parameter p\n  transformed_x = select(\n      [\n          (p == 0, safe_log(x)),\n          (p == 1, x),\n          (p == -jnp.inf, safe_sign(x) * safe_log1p(jnp.abs(x))),\n          (p == jnp.inf, safe_sign(x) * safe_expm1(jnp.abs(x))),\n      ],\n      safe_div(safe_expm1(p * safe_log(x)), p)\n  )\n\n  # Apply post-multiplication if specified\n  if postmult is not None:\n    transformed_x = postmult * transformed_x\n\n  return transformed_x"}
{"namespace": "math.inv_power_ladder", "completion": "  if postmult is not None:\n    y = y / postmult\n  p_safe = clip_finite_nograd(remove_zero(p))\n  x = select(\n      [\n          (p == 1, y),\n          (p == 0, safe_expm1(y)),\n          (p == -jnp.inf, -safe_log1p(-y)),\n          (p == jnp.inf, safe_log1p(y)),\n      ],\n      clip_finite_nograd(\n          safe_sign(y) * (jnp.abs(p_safe - 1) / p_safe) *\n          (((jnp.abs(y) / (jnp.abs(p_safe - 1) / p_safe)) + 1) ** (1 / p_safe) - 1)\n      ),\n  )\n  if premult is not None:\n    x = x / premult\n  return x"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "\n  # Convert origins and directions from world space to camera space\n  origins_cam = xnp.dot(origins, pixtocam.T)\n  directions_cam = xnp.dot(directions, pixtocam.T)\n\n  # Normalize directions to make z-component -1 (since camera looks along -z in OpenGL convention)\n  directions_cam = directions_cam / -directions_cam[..., 2:3]\n\n  # Calculate intersection of rays with near plane\n  t = -(near + origins_cam[..., 2:3]) / directions_cam[..., 2:3]\n  origins_ndc = origins_cam + t * directions_cam\n\n  # Divide x and y by z to perform perspective divide, making z = -1\n  origins_ndc = origins_ndc[..., :2] / origins_ndc[..., 2:3]\n\n  # Set z to -1 after perspective divide\n  origins_ndc = xnp.concatenate([origins_ndc, -xnp.ones_like(origins_ndc[..., :1])], axis=-1)\n\n  # Directions are already normalized such that z = -1, so we only need to take the x and y components\n  directions_ndc = directions_cam[..., :2]\n\n  return origins_ndc, directions_ndc"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Ensure that the spline degree is not greater than the number of points in x minus 1\n  spline_degree = min(spline_degree, len(x) - 1)\n\n  # Perform the spline interpolation\n  tck = scipy.interpolate.splrep(t_input, x, k=spline_degree, s=smoothness)\n  \n  # Evaluate the spline at the output times\n  x_output = scipy.interpolate.splev(t_output, tck)\n\n  return x_output"}
{"namespace": "math.learning_rate_decay", "completion": "  # Calculate the progress as a value between 0 and 1\n  progress = jnp.clip(step / max_steps, 0.0, 1.0)\n\n  # Calculate the learning rate without delay\n  lr = log_lerp(progress, lr_init, lr_final)\n\n  # If there's a delay, calculate the adjusted learning rate\n  if step < lr_delay_steps:\n    # Calculate the delayed progress\n    delay_progress = jnp.clip(step / lr_delay_steps, 0.0, 1.0)\n    # Calculate the learning rate during the delay\n    lr_delayed = log_lerp(delay_progress, lr_init * lr_delay_mult, lr_init)\n    # Use the delayed learning rate instead of the normal one\n    lr = jnp.where(step < lr_delay_steps, lr_delayed, lr)\n\n  return lr"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  # Create the intrinsic matrix using the provided focal lengths and optical center coordinates.\n  intrinsic_matrix = xnp.array([\n    [fx,  0, cx],\n    [ 0, fy, cy],\n    [ 0,  0,  1]\n  ])\n  return intrinsic_matrix"}
{"namespace": "utils.dummy_rays", "completion": "  # Define the random number generator seed\n  rng = random.PRNGKey(0)\n\n  # Define the number of rays to generate\n  n = 1\n\n  # Define the range for the origins\n  origin_lo = -1.0\n  origin_hi = 1.0\n\n  # Define the range for the radii\n  radius_lo = 0.0\n  radius_hi = 0.1\n\n  # Define the range for the near and far values\n  near_lo = 0.5\n  near_hi = 1.5\n  far_lo = 1.5\n  far_hi = 2.5\n\n  # Generate the random rays using the predefined function\n  return generate_random_rays(\n      rng,\n      n,\n      origin_lo,\n      origin_hi,\n      radius_lo,\n      radius_hi,\n      near_lo,\n      near_hi,\n      far_lo,\n      far_hi,\n      include_exposure_idx=include_exposure_idx,\n      include_exposure_values=include_exposure_values,\n      include_device_idx=include_device_idx,\n  )"}
{"namespace": "camera_utils.points_to_pixels", "completion": "    # Apply camera world-to-camera rotation matrices.\n    worldtocams = xnp.linalg.inv(camtoworlds)\n    camera_points = math.matmul(worldtocams[Ellipsis, :3, :3], points[..., None])[..., 0]\n    # Apply camera translation.\n    camera_points += worldtocams[Ellipsis, :3, -1]\n\n    # Project 3D points to 2D using camera intrinsics.\n    pix_coords = math.matmul(pixtocams, camera_points[..., None])[..., 0]\n\n    # Normalize by the third (z) coordinate to get pixel coordinates.\n    pix_coords = pix_coords[..., :2] / pix_coords[..., 2:3]\n\n    # Apply distortion if parameters are provided.\n    if distortion_params is not None:\n        pix_coords[..., 0], pix_coords[..., 1] = _radial_and_tangential_distort(\n            pix_coords[..., 0], pix_coords[..., 1], **distortion_params\n        )\n\n    # Flip y-axis to convert from OpenCV to image coordinates.\n    pix_coords[..., 1] = -pix_coords[..., 1]\n\n    # Depth is the negative z-coordinate in camera coordinates.\n    depth = -camera_points[..., 2]\n\n    return pix_coords, depth"}
{"namespace": "rigid_body.exp_se3", "completion": "  # Extract the angular (w) and linear (v) parts of the screw axis\n  w = screw_axis[:3]\n  v = screw_axis[3:]\n\n  # Compute the rotation matrix using the exponential map for SO(3)\n  R = exp_so3(w, eps)\n\n  # Compute the magnitude of the rotation (theta)\n  theta = _safe_sqrt(jnp.dot(w, w))\n\n  # Compute the skew-symmetric matrix of the angular part\n  w_skew = skew(w)\n\n  # Compute the matrix G(theta) as defined in Modern Robotics Eqn 4.32\n  G_theta = jnp.eye(3) * theta + (1 - jnp.cos(theta)) * w_skew + (theta - jnp.sin(theta)) * jnp.dot(w_skew, w_skew)\n\n  # Avoid division by zero by using a safe version of theta\n  theta_safe = jnp.where(theta > eps, theta, 1.0)\n\n  # Compute the translation part using G(theta)\n  p = jnp.dot(G_theta, v) / theta_safe\n\n  # Construct the homogeneous transformation matrix\n  T = rp_to_se3(R, p)\n\n  return T"}
{"namespace": "rigid_body.exp_so3", "completion": "  # Compute the angle (magnitude of rotation) from the axis-angle vector\n  angle = jnp.linalg.norm(axis_angle)\n\n  # Normalize the axis of rotation\n  axis = jnp.where(angle > eps, axis_angle / angle, axis_angle)\n\n  # Compute the skew-symmetric matrix of the axis\n  axis_skew = skew(axis)\n\n  # Use Rodrigues' formula for the rotation matrix\n  I = jnp.eye(3)\n  R = I + jnp.where(angle > eps, jnp.sin(angle) * axis_skew, 0.0) + \\\n      jnp.where(angle > eps, (1 - jnp.cos(angle)) * jnp.dot(axis_skew, axis_skew), 0.0)\n\n  return R"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # First, we need to convert the conical frustum intervals into means and variances\n  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n\n  # The radius variance needs to be scaled by the square of the base_radius\n  r_var *= base_radius ** 2\n\n  # Now we lift the Gaussian defined along the ray to 3D coordinates\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n\n  return mean, cov"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  # The mean along the axis of the cylinder is the midpoint of t0 and t1\n  t_mean = (t0 + t1) / 2.0\n  # The variance along the axis is the squared difference divided by 12 (uniform distribution variance)\n  t_var = ((t1 - t0) ** 2) / 12.0\n  # The variance in the radial direction is the radius squared divided by 4 (uniform distribution variance)\n  r_var = radius ** 2 / 4.0\n\n  # Use the lift_gaussian function to lift the 1D Gaussian along the axis to a 3D Gaussian\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # Convert pixel coordinates to floats and add a dummy dimension for depth.\n  pix_x = pix_x_int.astype(xnp.float32)\n  pix_y = pix_y_int.astype(xnp.float32)\n  pix_z = xnp.ones_like(pix_x)\n\n  # Stack to get pixel coordinates in homogeneous form.\n  pixels = xnp.stack([pix_x, pix_y, pix_z], axis=-1)\n\n  # Apply distortion correction if parameters are provided.\n  if distortion_params is not None:\n    pixels[..., :2] = _radial_and_tangential_undistort(\n        pixels[..., 0], pixels[..., 1],\n        k1=distortion_params.get('k1', 0),\n        k2=distortion_params.get('k2', 0),\n        k3=distortion_params.get('k3', 0),\n        k4=distortion_params.get('k4', 0),\n        p1=distortion_params.get('p1', 0),\n        p2=distortion_params.get('p2', 0),\n        xnp=xnp\n    )\n\n  # Transform pixel coordinates to camera space.\n  pixels_cam = xnp.einsum('...ij,...j->...i', pixtocams, pixels)\n\n  # Depending on the camera type, compute the ray directions.\n  if camtype == ProjectionType.PERSPECTIVE:\n    directions = pixels_cam\n  elif camtype == ProjectionType.FISHEYE:\n    # Fisheye model: Assume unit sphere projection.\n    theta = xnp.linalg.norm(pixels_cam[..., :2], axis=-1, keepdims=True)\n    phi = xnp.arctan2(pixels_cam[..., 1], pixels_cam[..., 0], keepdims=True)\n    directions = xnp.concatenate([\n        xnp.sin(theta) * xnp.cos(phi),\n        xnp.sin(theta) * xnp.sin(phi),\n        xnp.cos(theta)\n    ], axis=-1)\n  elif camtype == ProjectionType.PANORAMIC:\n    # Equirectangular projection for 360 panoramic images.\n    theta = (0.5 - pix_y / pixtocams.shape[0]) * xnp.pi\n    phi = (pix_x / pixtocams.shape[1] - 0.5) * 2 * xnp.pi\n    directions = xnp.stack([\n        xnp.sin(phi) * xnp.cos(theta),\n        xnp.sin(theta),\n        xnp.cos(phi) * xnp.cos(theta)\n    ], axis=-1)\n  else:\n    raise ValueError(f'Unknown camera type: {camtype}')\n\n  # Normalize the ray directions.\n  viewdirs = directions / xnp.linalg.norm(directions, axis=-1, keepdims=True)\n\n  # Transform ray directions to world space.\n  directions = xnp.einsum('...ij,...j->...i', camtoworlds[..., :3, :3], directions)\n\n  # Ray origins are the camera positions in world space.\n  origins = camtoworlds[..., :3, 3]\n\n  # Compute differential radii for mip-NeRF (approximate pixel footprint size).\n  radii = xnp.linalg.norm(pixels_cam[..., :2], axis=-1, keepdims=True)\n  radii = radii * xnp.abs(pixels_cam[..., 2:3]) / pixtocams[..., 0, 0:1]\n\n  # If NDC is used, convert ray origins and directions to NDC space.\n  if pixtocam_ndc is not None:\n    origins, directions = convert_to_ndc(origins, directions, pixtocam_ndc, xnp=xnp)\n\n  # Compute image plane coordinates.\n  imageplane = pixels_cam[..., :2]\n\n  return origins, directions, viewdirs, radii, imageplane"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Calculate the differences between consecutive elements in t\n  dt = np.diff(t)\n  \n  # Check if the sum of weights is close to 1\n  if not np.isclose(np.sum(w), 1):\n    raise ValueError(\"The sum of weights must be close to 1.\")\n  \n  # Check if the length of w is one less than the length of t\n  if len(w) != len(t) - 1:\n    raise ValueError(\"The length of weights vector w must be one less than the length of vector t.\")\n  \n  # Divide the weights by the differences to get the PDF\n  pdf = w / dt\n  \n  return pdf"}
{"namespace": "render.compute_alpha_weights", "completion": "  # Calculate the differences between consecutive tdist values\n  delta_tdist = tdist[..., 1:] - tdist[..., :-1]\n  \n  # Calculate the norm of the direction vectors\n  norm_dirs = jnp.linalg.norm(dirs, axis=-1, keepdims=True)\n  \n  # Adjust the distances by the norm of the direction vectors\n  adjusted_deltas = delta_tdist * norm_dirs\n  \n  # Compute the density delta, which is the product of density and adjusted distances\n  density_delta = density * adjusted_deltas\n  \n  # Compute the alpha weights using the helper function\n  weights = compute_alpha_weights_helper(density_delta)\n  \n  return weights"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  w = p * td\n  return w"}
{"namespace": "stepfun.sample", "completion": "  # Convert logits to probabilities\n  w = jax.nn.softmax(w_logits, axis=-1)\n  \n  # If rng is None, perform deterministic sampling\n  if rng is None:\n    if deterministic_center:\n      # Sample at the center of each bin\n      centers = (t[..., :-1] + t[..., 1:]) / 2\n      return jnp.tile(centers, (num_samples, 1))\n    else:\n      # Sample uniformly across the entire range\n      return jnp.linspace(t[..., 0], t[..., -1], num_samples)\n  \n  # Generate uniform samples in the range [0, 1)\n  if single_jitter:\n    # Single jitter for all samples\n    u = jax.random.uniform(rng, shape=(1,), minval=eps, maxval=1.0 - eps)\n    u = jnp.tile(u, (num_samples,))\n  else:\n    # Independent jitter for each sample\n    u = jax.random.uniform(rng, shape=(num_samples,), minval=eps, maxval=1.0 - eps)\n  \n  # Invert the CDF to get samples from the PDF\n  cw = integrate_weights(w)\n  samples = math.sorted_interp(u, cw, t, utils.device_is_tpu())\n  \n  return samples"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Sample points from the step function\n  sampled_points = sample(\n      rng,\n      t,\n      w_logits,\n      num_samples + 1,  # Sample one more point to create intervals\n      single_jitter=single_jitter\n  )\n\n  # Calculate midpoints between adjacent samples to form intervals\n  intervals = (sampled_points[..., :-1] + sampled_points[..., 1:]) / 2\n\n  # Adjust the first interval to be within the domain\n  intervals = jax.ops.index_update(\n      intervals, jax.ops.index[..., 0], jnp.maximum(intervals[..., 0], domain[0])\n  )\n\n  # Adjust the last interval to be within the domain\n  intervals = jax.ops.index_update(\n      intervals, jax.ops.index[..., -1], jnp.minimum(intervals[..., -1], domain[1])\n  )\n\n  return intervals"}
{"namespace": "stepfun.weighted_percentile", "completion": "  # Ensure that the weights sum to 1\n  w = w / w.sum(axis=-1, keepdims=True)\n\n  # Compute the cumulative sum of weights, which will serve as the CDF\n  cdf = jnp.cumsum(w, axis=-1)\n\n  # The CDF should start at 0 and end at 1, so we prepend a 0 and append a 1\n  cdf = jnp.concatenate([jnp.zeros(cdf.shape[:-1] + (1,)), cdf], axis=-1)\n  t = jnp.concatenate([t[..., :1], t], axis=-1)\n\n  # Convert percentiles from 0-100 to 0-1 range\n  ps = jnp.asarray(ps) / 100.0\n\n  # Interpolate the percentiles into the CDF to find the corresponding values in 't'\n  percentile_values = jnp.interp(ps, cdf, t)\n\n  return percentile_values"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram weights to a PDF\n  p = weight_to_pdf(t, w)\n\n  # Apply a blur to the PDF\n  # Assuming a simple box blur for demonstration purposes\n  # This can be replaced with a more sophisticated blur if needed\n  blur_width = 2 * blur_halfwidth + 1\n  kernel = jnp.ones(blur_width) / blur_width\n  p_blurred = jnp.convolve(p, kernel, mode='same')\n\n  # Convert the blurred PDF back to weights\n  w_blurred = pdf_to_weight(t, p_blurred)\n\n  # Resample the blurred weights at the new time points\n  w_resampled = resample(tq, t, w_blurred, use_avg=True)\n\n  return w_resampled"}
{"namespace": "spin_math.safe_sqrt", "completion": "  # Use jnp.maximum to ensure that x is not less than eps\n  safe_x = jnp.maximum(x, eps)\n  # Compute the square root of safe_x\n  sqrt_safe_x = jnp.sqrt(safe_x)\n  # If x is less than eps, use the square root of value_at_zero instead\n  return jnp.where(x < eps, jnp.sqrt(value_at_zero), sqrt_safe_x)"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Convert the vectors to homogeneous coordinates\n  vectors_homogeneous = to_homogeneous(vectors)\n  \n  # Apply the transformation matrix to the vectors\n  transformed_homogeneous = matmul(vectors_homogeneous, transform.T)\n  \n  # Convert back from homogeneous coordinates to standard coordinates\n  transformed_vectors = from_homogeneous(transformed_homogeneous)\n  \n  return transformed_vectors"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)"}
{"namespace": "stepfun.resample", "completion": "  # Ensure that the input step function is valid.\n  utils.assert_valid_stepfun(tp, vp)\n\n  # Initialize the resampled values array.\n  resampled_values = jnp.zeros_like(t[:-1])\n\n  # Iterate over each interval in the new time points.\n  for i in range(len(t) - 1):\n    # Find the indices of the original time points that fall within the current interval.\n    indices = jnp.where((tp >= t[i]) & (tp < t[i+1]))[0]\n\n    # If averaging is requested, compute the weighted average.\n    if use_avg:\n      # Calculate the width of each interval in the original time points.\n      widths = jnp.diff(jnp.concatenate([tp[indices], jnp.array([t[i+1]])]))\n      # Compute the weighted average of the values in the interval.\n      weighted_values = vp[indices] * widths\n      # Sum the weighted values and divide by the total width to get the average.\n      interval_width = t[i+1] - t[i]\n      resampled_values = resampled_values.at[i].set(jnp.sum(weighted_values) / interval_width)\n    else:\n      # If not averaging, simply sum the values in the interval.\n      resampled_values = resampled_values.at[i].set(jnp.sum(vp[indices]))\n\n  return resampled_values"}
{"namespace": "coord.contract", "completion": "  # Calculate the squared magnitude of each point\n  magnitude_squared = jnp.sum(x**2, axis=-1)\n\n  # Determine the scaling factor based on the magnitude\n  # (This is a placeholder and should be replaced with the actual formula)\n  scale_factor = jnp.where(magnitude_squared < 1.0, 0.5, 1.0)\n\n  # Apply the scaling factor to each point\n  contracted_points = x * scale_factor[..., jnp.newaxis]\n\n  return contracted_points"}
{"namespace": "coord.inv_contract", "completion": "  z_mag_sq = jnp.sum(z**2, axis=-1, keepdims=True)\n  # To invert the contract operation, we solve the quadratic equation for x_mag_sq:\n  # (2 * sqrt(x_mag_sq) - 1)^2 = z_mag_sq * x_mag_sq\n  # which simplifies to:\n  # 4 * x_mag_sq - 4 * sqrt(x_mag_sq) - z_mag_sq = 0\n  # We can use the quadratic formula to solve for sqrt(x_mag_sq):\n  # sqrt(x_mag_sq) = (4 \u00b1 sqrt(16 + 4 * z_mag_sq)) / 8\n  # We take the positive root because magnitude must be positive:\n  sqrt_x_mag_sq = (2 + jnp.sqrt(4 + z_mag_sq)) / 4\n  x_mag_sq = sqrt_x_mag_sq**2\n  # Now we can compute the inverse scale:\n  inv_scale = x_mag_sq / (2 * sqrt_x_mag_sq - 1)\n  x = inv_scale * z\n  return x"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    # Get the dimensions of the grid\n    D, H, W, C = values.shape\n\n    # Get the integer part of the coordinates (as indices)\n    coords_floor = jnp.floor(coordinates).astype(int)\n\n    # Get the fractional part of the coordinates\n    coords_alpha = coordinates - coords_floor\n\n    # Ensure the coordinates are within the bounds of the grid\n    coords_floor = jnp.clip(coords_floor, 0, jnp.array([D - 1, H - 1, W - 1]))\n\n    # Calculate the indices of the 8 neighbors for each coordinate\n    indices = jnp.stack([\n        coords_floor,\n        coords_floor + jnp.array([[0, 0, 1]]),\n        coords_floor + jnp.array([[0, 1, 0]]),\n        coords_floor + jnp.array([[0, 1, 1]]),\n        coords_floor + jnp.array([[1, 0, 0]]),\n        coords_floor + jnp.array([[1, 0, 1]]),\n        coords_floor + jnp.array([[1, 1, 0]]),\n        coords_floor + jnp.array([[1, 1, 1]])\n    ], axis=-2)\n\n    # Clip the indices to ensure they are within the grid bounds\n    indices = jnp.clip(indices, 0, jnp.array([D - 1, H - 1, W - 1]))\n\n    # Gather the values at the 8 neighbor indices\n    neighbor_values = values[indices[..., 0], indices[..., 1], indices[..., 2], :]\n\n    # Perform trilinear interpolation\n    interp_values = math.trilinear_interpolation(neighbor_values, coords_alpha)\n\n    return interp_values\n\n  elif datastructure == 'hash':\n    # Perform the hash lookup and interpolation\n    interp_values = hash_resample.hash_interpolate(values, coordinates)\n    return interp_values\n\n  else:\n    raise ValueError(\"Invalid datastructure type. Supported types are 'grid' and 'hash'.\")"}
{"namespace": "coord.track_linearize", "completion": "  # Apply the function to the mean\n  fn_mean = fn(mean)\n\n  # Compute the Jacobian of the function at the mean\n  jacobian_fn = jax.jacfwd(fn)\n  J = jacobian_fn(mean)\n\n  # Transform the covariance using the Jacobian\n  fn_cov = J @ cov @ J.T\n\n  return fn_mean, fn_cov"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Ensure the input is 3-dimensional\n  if x.shape[-1] != 3:\n    raise ValueError(\"Input must be 3-dimensional\")\n\n  # Compute the squared magnitude of the input vectors\n  x_mag_sq = jnp.maximum(1, jnp.sum(x**2, axis=-1, keepdims=True))\n\n  # Compute the scaling factor\n  scale = (2 * jnp.sqrt(x_mag_sq) - 1) / x_mag_sq\n\n  # Apply the scaling to the input vectors\n  z = scale * x\n\n  return z"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2 ** jnp.arange(min_deg, max_deg)\n  x_scaled = x[..., None] * scales[None, :]\n  encoded = jnp.concatenate([jnp.sin(x_scaled), jnp.cos(x_scaled)], axis=-1)\n\n  if append_identity:\n    encoded = jnp.concatenate([x] + [encoded], axis=-1)\n\n  return encoded"}
{"namespace": "coord.integrated_pos_enc", "completion": "  scales = 2**jnp.arange(min_deg, max_deg)\n  scales = scales.reshape((1,) * (mean.ndim - 1) + (-1,))\n  mean_scaled = mean[..., None] * scales\n  var_scaled = var[..., None] * scales**2\n\n  # Concatenate the scaled mean and variance\n  mean_var_scaled = jnp.concatenate([mean_scaled, var_scaled], axis=-1)\n\n  # Apply sinusoidal encoding\n  encoding = jnp.concatenate([math.safe_sin(mean_var_scaled),\n                               math.safe_cos(mean_var_scaled)], axis=-1)\n  return encoding"}
{"namespace": "coord.isotropize", "completion": "  if mode not in ['fast', 'accurate']:\n    raise ValueError(\"Mode must be either 'fast' or 'accurate'.\")\n\n  # Compute the determinant of the covariance matrix/matrices.\n  det_cov = jnp.linalg.det(cov)\n\n  # Ensure the determinant is positive to compute its logarithm.\n  if jnp.any(det_cov <= 0):\n    raise ValueError(\"Covariance matrix/matrices must be positive definite.\")\n\n  # Compute the isotropic scale factor.\n  if mode == 'fast':\n    # In 'fast' mode, use the determinant directly.\n    isotropic_scale = det_cov ** (1.0 / cov.shape[-1])\n  else:\n    # In 'accurate' mode, use the logarithm of the determinant for numerical stability.\n    log_det_cov = jnp.log(det_cov)\n    isotropic_scale = jnp.exp(log_det_cov / cov.shape[-1])\n\n  # Create isotropic covariance matrices with the same determinant.\n  isotropic_cov = isotropic_scale * jnp.eye(cov.shape[-1])\n\n  # If cov is a batch of matrices, expand isotropic_cov to match the batch size.\n  if len(cov.shape) > 2:\n    isotropic_cov = jnp.expand_dims(isotropic_cov, axis=0)\n    isotropic_cov = jnp.broadcast_to(isotropic_cov, cov.shape)\n\n  return isotropic_cov"}
{"namespace": "coord.construct_ray_warps", "completion": "\n  # If the inverse function is not provided, we need to define it.\n  # For this example, let's assume that the inverse of `fn` is not known\n  # and raise an error. In practice, you would define `fn_inv` based on `fn`.\n  if fn_inv is None:\n    raise NotImplementedError(\"Inverse function `fn_inv` must be provided.\")\n\n  # Define the forward mapping function from metric distances to normalized distances.\n  def t_to_s(t):\n    # Clip the input distances to the range [t_near, t_far].\n    t_clipped = jnp.clip(t, t_near, t_far)\n    # Apply the transformation function to the clipped distances.\n    s = fn(t_clipped)\n    # Normalize the transformed distances to the range [0, 1].\n    s_normalized = (s - fn(t_near)) / (fn(t_far) - fn(t_near))\n    return s_normalized\n\n  # Define the inverse mapping function from normalized distances to metric distances.\n  def s_to_t(s):\n    # Scale and shift the normalized distances to the transformed space.\n    s_scaled = s * (fn(t_far) - fn(t_near)) + fn(t_near)\n    # Apply the inverse transformation function to the scaled distances.\n    t = fn_inv(s_scaled)\n    return t\n\n  return t_to_s, s_to_t"}
{"namespace": "ref_utils.l2_normalize", "completion": "  # Calculate the L2 norm squared along the last axis and clamp it to grad_eps\n  norm_sq = jnp.maximum(jnp.sum(x**2, axis=-1, keepdims=True), grad_eps)\n  # Calculate the L2 norm\n  norm = jnp.sqrt(norm_sq)\n  # Normalize the input array\n  normalized_x = x / norm\n  return normalized_x"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    # Check if the word is entirely uppercase or lowercase\n    if word.isupper() or word.islower():\n        return word\n    \n    # Check if the word is mixed case\n    if any(c.isupper() for c in word[1:]):\n        # Apply different rules based on the casing of the first two letters\n        if word[0].isupper():\n            if len(word) > 1 and word[1].isupper():\n                # If the first two letters are uppercase, keep the word in uppercase\n                return word.upper()\n            else:\n                # If only the first letter is uppercase, capitalize the word\n                return word.capitalize()\n        else:\n            # If the first letter is lowercase, convert the entire word to lowercase\n            return word.lower()\n    else:\n        # If the word does not have uppercase letters after the first character, capitalize it\n        return word.capitalize()"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove extra spaces between characters\n    collapsed_text = re.sub(r'(?<=\\w) (?=\\w)', '', line_text)\n\n    # Use the spell_utils module to segment the text into words\n    # Assuming the spell_utils module has a method called 'segment' that can segment a string into words\n    segmented_text = su.segment(collapsed_text)\n\n    return segmented_text"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  # Generate the integrated directional encoding function with the specified degree\n  ide_fn = generate_ide_fn(deg_view)\n\n  def dir_enc_fn(xyz):\n    \"\"\"\n    The directional encoding function that evaluates the encoding for given 3D points.\n\n    :param xyz: Array-like. The input 3D points to be encoded. Should have a shape of [..., 3].\n    :return: Array. The directional encoding of the input points.\n    \"\"\"\n    # Normalize the input directions\n    xyz_normalized = l2_normalize(xyz)\n\n    # Set kappa_inv to 0 as we are not using the von Mises-Fisher distribution here\n    kappa_inv = jnp.zeros(xyz.shape[:-1] + (1,))\n\n    # Evaluate the integrated directional encoding function\n    encoding = ide_fn(xyz_normalized, kappa_inv)\n\n    return encoding\n\n  return dir_enc_fn"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    cleaned_lines = []\n    block_index = 0\n    for line in lines:\n        if should_skip(line, xml):\n            continue\n        if is_table_row(line):\n            continue\n        if find_floating_chars(line):\n            continue\n        if not check_parentheses(line):\n            continue\n        line = formatter.format_line(line)\n        tokens = nlm_tokenize(line)\n        if tokens:\n            cleaned_line = ' '.join(tokens)\n            cleaned_lines.append({\n                'index': block_index,\n                'text': cleaned_line,\n                'type': 'paragraph',  # Assuming default type is 'paragraph'\n                'start_index': block_index,  # Assuming each line is a new block\n                'blocks': [],  # Assuming no nested blocks\n                'header_index': None,  # Assuming no associated header block\n                'list_level': None,  # Assuming no list indentation level\n            })\n            block_index += 1\n    return cleaned_lines"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if not org_texts:\n        return org_texts\n\n    # Normalize quotation marks\n    org_texts = quotation_pattern.sub('\"', org_texts)\n\n    # Apply rules for abbreviations\n    for rule, replaced in rules:\n        org_texts = rule.sub(replaced, org_texts)\n\n    # Handle content inside brackets as a single sentence\n    bracket_contents = bracket_rule.findall(org_texts)\n    bracket_replacements = []\n    for content in bracket_contents:\n        bracket_replacements.append(content.replace('.', '_DOT_'))\n    for original, replacement in zip(bracket_contents, bracket_replacements):\n        org_texts = org_texts.replace(f\"({original})\", f\"({replacement})\")\n\n    # Tokenize sentences\n    sentences = nltk_tokenzier.tokenize(org_texts)\n\n    # Replace back the placeholders for dots inside brackets\n    for original, replacement in zip(bracket_replacements, bracket_contents):\n        sentences = [sentence.replace(f\"({replacement})\", f\"({original})\") for sentence in sentences]\n\n    # Replace back the placeholders for abbreviations\n    for rule, replaced in rules:\n        sentences = [sentence.replace(replaced.strip(), f\"{rule.pattern[:-4].strip()}.\") for sentence in sentences]\n\n    # Remove any space between punctuations\n    sentences = [space_rule.sub(r'\\1', sentence) for sentence in sentences]\n\n    # Replace back the placeholders for dots inside brackets\n    sentences = [sentence.replace('_DOT_', '.') for sentence in sentences]\n\n    return sentences"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        # Check if the token is a valid term in the term dictionary\n        try:\n            term_id = self.term_dict.get_term_id(token)\n        except TermMissingError:\n            # If the term is not in the term dictionary, return an empty list\n            return []\n\n        # If a specific document key is provided\n        if key is not None:\n            # Check if the key is an integer and within the range of documents\n            if isinstance(key, numbers.Integral) and 0 <= key < len(self):\n                # Retrieve the positions for the given term in the specified document\n                positions = self.posns.doc_posns(term_id, doc_id=key)\n                return [positions] if positions is not None else []\n            else:\n                # If the key is not valid, raise an IndexError\n                raise IndexError(\"Document key is out of bounds\")\n\n        # If no specific document key is provided, retrieve positions for all documents\n        all_positions = []\n        for doc_id in range(len(self)):\n            positions = self.posns.doc_posns(term_id, doc_id=doc_id)\n            if positions is not None:\n                all_positions.append(positions)\n\n        return all_positions"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Check if the spec is a percentage\n    if spec.endswith('%'):\n        # Calculate the percentage of the total number of clauses\n        percentage = int(spec[:-1])\n        min_should_match = int(np.ceil((percentage / 100) * num_clauses))\n    # Check if the spec is a conditional expression\n    elif '<' in spec:\n        # Split the condition and the value\n        condition, value = spec.split('<')\n        condition = int(condition)\n        value = int(value)\n        # If the number of clauses is less than the condition, use the value\n        if num_clauses < condition:\n            min_should_match = value\n        else:\n            # Otherwise, use the condition as the minimum should match\n            min_should_match = condition\n    else:\n        # If the spec is an absolute number\n        min_should_match = int(spec)\n\n    # Ensure that the minimum should match does not exceed the number of clauses\n    min_should_match = min(num_clauses, max(0, min_should_match))\n\n    return min_should_match"}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        # Check if all tokens are unique, which allows for direct calculation\n        if slop == 1 and len(tokens) == len(set(tokens)):\n            term_ids = [self.term_dict.get_term_id(token) for token in tokens]\n            phrase_freqs = np.zeros(len(self), dtype=int)\n            for doc_id in range(len(self)):\n                positions_list = [self.posns.positions(term_id, doc_ids=[doc_id]) for term_id in term_ids]\n                # Check if any term is missing in the document\n                if any(len(positions) == 0 for positions in positions_list):\n                    continue\n                # Compute the phrase frequency for the document\n                phrase_freqs[doc_id] = compute_phrase_freqs(positions_list, slop)\n            return phrase_freqs\n        else:\n            # Handle cases where slop is not 1 or tokens are not unique\n            return self._compute_complex_phrase_freq(tokens, slop)"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        # The following code completes the index function of the SearchArray class.\n        # It builds an index from the given array and tokenizer, handling large arrays\n        # by processing them in batches if necessary.\n\n        # Initialize variables to hold the combined results from each batch\n        combined_term_mat = None\n        combined_posns = None\n        combined_term_dict = None\n        combined_avg_doc_length = 0\n        combined_doc_lens = []\n\n        # Process the array in batches\n        for i in range(0, len(array), batch_size):\n            batch = array[i:i + batch_size]\n            # Tokenize and index the current batch\n            term_mat, posns, term_dict, avg_doc_length, doc_lens = build_index_from_tokenizer(\n                batch, tokenizer, Terms, truncate=truncate\n            )\n\n            # Combine the results with those from previous batches\n            if combined_term_mat is None:\n                combined_term_mat = term_mat\n                combined_posns = posns\n                combined_term_dict = term_dict\n            else:\n                combined_term_mat = np.concatenate((combined_term_mat, term_mat), axis=0)\n                combined_posns.extend(posns)\n                combined_term_dict.merge(term_dict)\n\n            # Update the combined average document length and document lengths\n            total_docs = len(combined_doc_lens) + len(doc_lens)\n            combined_avg_doc_length = (\n                (combined_avg_doc_length * len(combined_doc_lens) + avg_doc_length * len(doc_lens))\n                / total_docs\n            )\n            combined_doc_lens.extend(doc_lens)\n\n        # Create a new SearchArray instance with the combined results\n        return cls(combined_term_mat, tokenizer=tokenizer, avoid_copies=avoid_copies)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        # Initialize the lock\n        self.lock = threading.Lock()\n\n        # Initialize the connections dictionary\n        self.connections = {}\n\n        # Create the server with the configuration from the interceptor\n        self.server = Server(\n            host=self.config['serverHost'],\n            port=self.config['serverPort'],\n            proxy_host=self.config['proxyHost'],\n            proxy_port=self.config['proxyPort'],\n            strategy=self.config['strategy'],\n            strategies_config=self.config['strategies'],\n            auto_close_connections=self.config['autoCloseConnections'],\n            multiple_connections=self.config['multipleConnections'],\n            interceptor=self\n        )\n\n        # Start the server\n        self.server.start()\n\n        # Log that the interceptor has been initialized\n        self.logger.info(\"ProxifierMessageInterceptor initialized with server at {}:{}\".format(\n            self.config['serverHost'], self.config['serverPort']))"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    arr = (arr & s55) + ((arr >> _1) & s55)  # Put count of each 2 bits into those 2 bits\n    arr = (arr & s33) + ((arr >> _2) & s33)  # Put count of each 4 bits into those 4 bits\n    arr = (arr & s0F) + ((arr >> _4) & s0F)  # Put count of each 8 bits into those 8 bits\n    arr = (arr * s01) >> all_but_one_bit      # Put count of each 64 bits into those 64 bits\n    return arr"}
{"namespace": "searcharray.solr.edismax", "completion": "    # Parse query fields and boosts\n    query_fields = parse_field_boosts(qf)\n    # Parse phrase fields and boosts\n    phrase_fields = parse_field_boosts(pf) if pf else {}\n    bigram_fields = parse_field_boosts(pf2) if pf2 else {}\n    trigram_fields = parse_field_boosts(pf3) if pf3 else {}\n\n    # Parse the query terms\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, list(query_fields.keys()))\n\n    # If no minimum match is specified, default to 100% if q_op is AND, else 0%\n    if mm is None:\n        mm = \"100%\" if q_op.upper() == \"AND\" else \"0%\"\n\n    # Calculate term-centric or field-centric scores\n    if term_centric:\n        qf_scores, qf_explain = _edismax_term_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n    else:\n        qf_scores, qf_explain = _edismax_field_centric(frame, query_fields, num_search_terms, search_terms, mm, similarity)\n\n    # TODO: Implement phrase, bigram, and trigram scoring\n    # For now, we only handle qf (query fields) scoring\n    # Phrase (pf), bigram (pf2), and trigram (pf3) scoring would require additional implementation\n\n    # Combine scores and explanations\n    scores = qf_scores\n    explain = qf_explain\n\n    return scores, explain"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "    connection_id = message.connection_id or DEFAULT_CONNECTION_ID\n    with self.lock:\n        if connection_id not in self.connections:\n            self.connections[connection_id] = self.server.create_connection(connection_id)\n\n    connection = self.connections[connection_id]\n\n    if isinstance(message, SendMessage):\n        # Transform the message data using the connection's client-to-server method\n        message.data = connection.c2s(message.data)\n    elif isinstance(message, RecvMessage):\n        # Transform the message data using the connection's server-to-client method\n        message.data = connection.s2c(message.data)\n    elif isinstance(message, CloseMessage):\n        # Handle the connection closure\n        with self.lock:\n            if connection_id in self.connections:\n                connection.close()\n                del self.connections[connection_id]"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        # Acquire the lock to ensure thread safety while modifying connections\n        with self.lock:\n            # Close all active connections\n            for connection_id, connection in self.connections.items():\n                connection.close()\n            # Clear the connections dictionary\n            self.connections.clear()\n\n        # Stop the server if it's running\n        if self.server:\n            self.server.stop()"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    assert \"bbox\" in instance and \"bbox_mode\" in instance, \"Instance must have 'bbox' and 'bbox_mode' fields.\"\n\n    # Convert bbox to XYXY_ABS format\n    bbox = BoxMode.convert(instance[\"bbox\"], instance[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox_center = [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2]\n\n    # Calculate crop region, ensuring it is within image boundaries\n    crop_width, crop_height = crop_size\n    image_width, image_height = image_size\n\n    x1 = max(min(bbox_center[0] - crop_width / 2, image_width - crop_width), 0)\n    y1 = max(min(bbox_center[1] - crop_height / 2, image_height - crop_height), 0)\n    x2 = x1 + crop_width\n    y2 = y1 + crop_height\n\n    # Ensure the crop region is within the image boundaries\n    x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n    x2 = min(x2, image_width)\n    y2 = min(y2, image_height)\n\n    # Create the CropTransform object\n    crop_transform = T.CropTransform(x1, y1, x2 - x1, y2 - y1)\n\n    return crop_transform"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n\n        # Apply EXIF orientation if necessary\n        image = _apply_exif_orientation(image)\n\n        # Convert image to numpy array in the specified format\n        image = convert_PIL_to_numpy(image, format)\n\n    return image"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "    # Transform the bounding box\n    bbox = BoxMode.convert(annotation[\"bbox\"], annotation[\"bbox_mode\"], BoxMode.XYXY_ABS)\n    bbox = transforms.apply_box([bbox])[0]\n    bbox = np.minimum(bbox, list(image_size + image_size)[::-1])\n    annotation[\"bbox\"] = bbox\n    annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    # Transform the segmentation polygons\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], list):\n            # Polygon format\n            polygons = [np.asarray(p).reshape(-1, 2) for p in annotation[\"segmentation\"]]\n            transformed_polygons = [transforms.apply_polygons(p).reshape(-1) for p in polygons]\n            annotation[\"segmentation\"] = transformed_polygons\n        elif isinstance(annotation[\"segmentation\"], dict):\n            # RLE format\n            mask = mask_util.decode(annotation[\"segmentation\"])\n            mask = transforms.apply_segmentation(mask)\n            annotation[\"segmentation\"] = mask_util.encode(np.array(mask, order=\"F\", dtype=\"uint8\"))\n\n    # Transform keypoints\n    if \"keypoints\" in annotation and keypoint_hflip_indices is not None:\n        keypoints = Keypoints(annotation[\"keypoints\"]).tensor\n        if transforms.transforms[-1].__class__.__name__ == \"HFlipTransform\":\n            keypoints = keypoints[:, keypoint_hflip_indices]\n        keypoints = transforms.apply_coords(keypoints)\n        annotation[\"keypoints\"] = keypoints.numpy()\n\n    return annotation"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        coords = np.asarray(coords, dtype=np.float32)\n        # Add an extra dimension for the transformation matrix multiplication\n        coords = np.column_stack((coords, np.ones(coords.shape[0], dtype=np.float32)))\n        # Apply the rotation matrix to the coordinates\n        new_coords = np.dot(coords, self.rm_coords.T)\n        return new_coords[:, :2]"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    target = Instances(image_size)\n\n    classes = [anno[\"category_id\"] for anno in annos]\n    classes = torch.tensor(classes, dtype=torch.int64)\n    target.gt_classes = classes\n\n    if any(\"bbox\" in anno for anno in annos):\n        boxes = [BoxMode.convert(anno[\"bbox\"], anno[\"bbox_mode\"], BoxMode.XYXY_ABS) for anno in annos]\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        target.gt_boxes = Boxes(boxes)\n\n    if mask_format == \"polygon\":\n        masks = [anno[\"segmentation\"] for anno in annos if \"segmentation\" in anno]\n        masks = PolygonMasks(masks)\n        target.gt_masks = masks\n    elif mask_format == \"bitmask\":\n        masks = [mask_util.decode(anno[\"segmentation\"]) for anno in annos if \"segmentation\" in anno]\n        masks = torch.as_tensor(np.stack(masks, axis=0)[:, None], dtype=torch.uint8)\n        target.gt_masks = BitMasks(masks)\n\n    if any(\"keypoints\" in anno for anno in annos):\n        keypoints = [anno[\"keypoints\"] for anno in annos if \"keypoints\" in anno]\n        keypoints = Keypoints(keypoints)\n        target.gt_keypoints = keypoints\n\n    return target"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "    # Create an instance of FlopCountAnalysis\n    analysis = FlopCountAnalysis(model, inputs)\n\n    # Run the analysis to get the flops count\n    analysis.uncached_ops()\n    flops_dict = analysis.by_operator()\n\n    # Convert the flops count to Gflops and store in a defaultdict\n    gflops_dict = defaultdict(float)\n    for op, flops in flops_dict.items():\n        gflops_dict[op] = flops / 1e9  # Convert to Gflops\n\n    return gflops_dict"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "        if img.size == 0 or self.angle % 360 == 0:\n            return img\n\n        if interp is None:\n            interp = self.interp\n\n        # Compute the rotation matrix for the image\n        image_center = tuple(np.array(img.shape[1::-1]) / 2)\n        rot_mat = cv2.getRotationMatrix2D(image_center, self.angle, 1.0)\n\n        # Determine the size of the new image\n        new_w, new_h = self.bound_w, self.bound_h\n\n        # Perform the rotation\n        result = cv2.warpAffine(img, rot_mat, (new_w, new_h), flags=interp)\n        return result"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "        boxes = predictions.pred_boxes if predictions.has(\"pred_boxes\") else None\n        scores = predictions.scores if predictions.has(\"scores\") else None\n        classes = predictions.pred_classes if predictions.has(\"pred_classes\") else None\n        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n\n        if predictions.has(\"pred_masks\"):\n            masks = np.asarray(predictions.pred_masks)\n            for i in range(len(predictions)):\n                color = self._get_color(classes[i]) if classes is not None else None\n                self.draw_binary_mask(masks[i], color=color)\n                if boxes is not None:\n                    self.draw_box(boxes[i], edge_color=color)\n\n        elif predictions.has(\"pred_masks_rle\"):\n            masks_rle = predictions.pred_masks_rle\n            for rle in masks_rle:\n                mask = mask_util.decode(rle)\n                self.draw_binary_mask(mask)\n\n        if boxes is not None:\n            for i, box in enumerate(boxes):\n                color = self._get_color(classes[i]) if classes is not None else None\n                self.draw_box(box, edge_color=color)\n\n        if keypoints is not None:\n            for i, keypoints_per_instance in enumerate(keypoints):\n                self.draw_and_connect_keypoints(keypoints_per_instance)\n\n        # Draw labels last so they are visible on top of other objects\n        if labels is not None:\n            for i, box in enumerate(boxes):\n                # Draw label below the object\n                y = box[1] if boxes is not None else self.output.height // 2\n                self.draw_text(labels[i], (box[0], y), color=_BLACK)\n\n        return self.output"}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "        canvas = self.canvas\n        s, (width, height) = canvas.print_to_buffer()\n\n        # Create an RGBA buffer from the canvas data\n        buffer_rgba = np.frombuffer(s, dtype=np.uint8).reshape(height, width, 4)\n\n        # Convert RGBA to RGB by discarding the alpha channel\n        buffer_rgb = cv2.cvtColor(buffer_rgba, cv2.COLOR_RGBA2RGB)\n\n        return buffer_rgb"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "        annos = dic.get(\"annotations\", None)\n        if annos:\n            if \"segmentation\" in annos[0]:\n                masks = [obj.get(\"segmentation\", None) for obj in annos]\n            else:\n                masks = None\n            boxes = [obj[\"bbox\"] for obj in annos if \"bbox\" in obj]\n            boxes = [BoxMode.convert(box, BoxMode(obj[\"bbox_mode\"]), BoxMode.XYXY_ABS) for box, obj in zip(boxes, annos)]\n            boxes = Boxes(torch.tensor(boxes).to(self.cpu_device))\n            classes = [obj[\"category_id\"] for obj in annos]\n            labels = _create_text_labels(classes, None, self.metadata.get(\"thing_classes\", None))\n            keypoints = [obj.get(\"keypoints\", None) for obj in annos]\n            keypoints = [torch.tensor(kp).to(self.cpu_device) if kp is not None else None for kp in keypoints]\n            self.overlay_instances(masks=masks, boxes=boxes, labels=labels, keypoints=keypoints)\n\n        if \"sem_seg\" in dic:\n            sem_seg = dic[\"sem_seg\"]\n            self.draw_sem_seg(sem_seg=sem_seg)\n\n        if \"panoptic_seg\" in dic and \"segments_info\" in dic:\n            panoptic_seg = dic[\"panoptic_seg\"]\n            segments_info = dic[\"segments_info\"]\n            self.draw_panoptic_seg(panoptic_seg=panoptic_seg, segments_info=segments_info)\n\n        return self.output"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": "    # Save the original module to an in-memory buffer\n    buffer = io.BytesIO()\n    torch.jit.save(module, buffer)\n    \n    # Load the module back from the in-memory buffer\n    buffer.seek(0)  # Move to the start of the buffer\n    reloaded_module = torch.jit.load(buffer)\n    \n    return reloaded_module"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "        if color is None:\n            color = random_color(rgb=True, maximum=1)\n\n        # Find contours of the mask\n        contours, hierarchy = cv2.findContours(\n            binary_mask.astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE\n        )\n\n        # Draw each contour\n        for i, contour in enumerate(contours):\n            # Skip small contours\n            if cv2.contourArea(contour) < area_threshold:\n                continue\n\n            # Draw the contour as a polygon on the image\n            polygon = contour.reshape(-1, 2)\n            self.draw_polygon(polygon, color, alpha=alpha)\n\n            # Optionally draw the edge of the polygon\n            if edge_color is not None:\n                self.draw_polygon(polygon, edge_color, alpha=1, fill=False)\n\n        # Optionally draw text in the center of the mask\n        if text is not None:\n            # Compute the center of the mask\n            M = cv2.moments(binary_mask.astype(np.uint8))\n            if M[\"m00\"] != 0:\n                cx = int(M[\"m10\"] / M[\"m00\"])\n                cy = int(M[\"m01\"] / M[\"m00\"])\n                # Draw the text\n                self.draw_text(text, (cx, cy), color=edge_color or \"w\", horizontal_alignment=\"center\")\n\n        return self.output"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "    assert isinstance(input, Instances), f\"Input must be an Instances object, got {type(input)}.\"\n    assert isinstance(other, Instances), f\"Other must be an Instances object, got {type(other)}.\"\n\n    if size_as_tensor:\n        assert torch.all(input.image_size == other.image_size), \\\n            f\"{msg}Image sizes are different: {input.image_size} vs {other.image_size}\"\n    else:\n        assert input.image_size == other.image_size, \\\n            f\"{msg}Image sizes are different: {input.image_size} vs {other.image_size}\"\n\n    for field in input.get_fields():\n        input_field = input.get(field)\n        other_field = other.get(field)\n\n        assert field in other, f\"{msg}Field '{field}' not found in 'other' instance.\"\n        assert type(input_field) == type(other_field), \\\n            f\"{msg}Field '{field}' types are different: {type(input_field)} vs {type(other_field)}\"\n\n        if isinstance(input_field, torch.Tensor):\n            assert torch.allclose(input_field, other_field, rtol=rtol), \\\n                f\"{msg}Tensors in field '{field}' are not close.\"\n        elif isinstance(input_field, Boxes):\n            assert torch.allclose(input_field.tensor, other_field.tensor, rtol=rtol), \\\n                f\"{msg}Boxes in field '{field}' are not close.\"\n        elif isinstance(input_field, ROIMasks):\n            assert torch.allclose(input_field.tensor, other_field.tensor, rtol=rtol), \\\n                f\"{msg}ROIMasks in field '{field}' are not close.\"\n        else:\n            raise ValueError(f\"{msg}Unsupported field type: {type(input_field)}\")"}
{"namespace": "detectron2.utils.registry.locate", "completion": "    try:\n        # Attempt to locate the object using pydoc.locate\n        obj = pydoc.locate(name)\n        if obj is not None:\n            return obj\n        else:\n            raise ImportError(f\"Cannot locate the object '{name}'\")\n    except Exception as e:\n        # If pydoc.locate fails, raise an exception with a helpful message\n        raise ImportError(f\"Cannot locate the object '{name}'. Error: {e}\")"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "        # The area of a rotated box is the same as the area of the non-rotated box\n        # with the same width and height. The angle does not affect the area.\n        # The format of each box is (x_center, y_center, width, height, angle)\n        # So we extract the width (index 2) and height (index 3) and compute the area.\n        return self.tensor[:, 2] * self.tensor[:, 3]"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": "    # Check if the proposal generator is precomputed, in which case return None\n    if cfg.MODEL.PROPOSAL_GENERATOR.NAME == \"PrecomputedProposals\":\n        return None\n\n    # Get the name of the proposal generator to use\n    name = cfg.MODEL.PROPOSAL_GENERATOR.NAME\n    # Retrieve the proposal generator class from the registry\n    proposal_generator = PROPOSAL_GENERATOR_REGISTRY.get(name)\n    # Initialize the proposal generator with the provided configuration and input shape\n    return proposal_generator(cfg, input_shape)"}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        losses = {}\n        storage = get_event_storage()\n\n        # Classification loss\n        gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n        gt_classes = gt_classes.to(dtype=torch.int64)\n        cls_loss = cross_entropy(scores, gt_classes, reduction=\"mean\")\n\n        # Log the classification stats\n        _log_classification_stats(scores, gt_classes)\n\n        # Box regression loss\n        fg_inds = nonzero_tuple((gt_classes >= 0) & (gt_classes < self.num_classes))[0]\n        if self.box_reg_loss_type == \"smooth_l1\":\n            gt_proposal_deltas = self.box2box_transform.get_deltas(\n                cat([p.proposal_boxes.tensor for p in proposals], dim=0)[fg_inds],\n                cat([p.gt_boxes.tensor for p in proposals], dim=0)[fg_inds],\n            )\n            box_loss = _dense_box_regression_loss(\n                proposal_deltas[fg_inds],\n                gt_proposal_deltas,\n                self.smooth_l1_beta,\n            )\n        else:\n            raise NotImplementedError(f\"Box reg loss type '{self.box_reg_loss_type}' not implemented\")\n\n        # Scale and record the losses\n        losses[\"loss_cls\"] = cls_loss * self.loss_weight[\"loss_cls\"]\n        losses[\"loss_box_reg\"] = box_loss * self.loss_weight[\"loss_box_reg\"]\n        storage.put_scalar(\"loss_cls\", losses[\"loss_cls\"].item())\n        storage.put_scalar(\"loss_box_reg\", losses[\"loss_box_reg\"].item())\n\n        return losses"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": "    # Retrieve the name of the tracker from the configuration\n    tracker_name = cfg.MODEL.TRACKER_HEAD.NAME\n    # Look up the corresponding tracker class from the registry\n    tracker_class = TRACKER_HEADS_REGISTRY.get(tracker_name)\n    # Check if the tracker class was found in the registry\n    if tracker_class is None:\n        raise ValueError(f\"Tracker head '{tracker_name}' is not registered in TRACKER_HEADS_REGISTRY\")\n    # Instantiate and return an object of the tracker class using the configuration\n    return tracker_class.from_config(cfg)"}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n        pred_w = torch.exp(dw) * widths[:, None]\n        pred_h = torch.exp(dh) * heights[:, None]\n\n        pred_boxes = torch.zeros_like(deltas)\n        # x1\n        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n        # y1\n        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n        # x2\n        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n        # y2\n        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n        return pred_boxes"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": "        output = self.general_ins.process(image)\n\n        # If no specific annotation type is requested, return the entire output\n        if anno_type is None:\n            return output\n\n        # If a single annotation type is requested, return only that annotation\n        if isinstance(anno_type, str):\n            return output.get(anno_type)\n\n        # If a list of annotation types is requested, return a dictionary of those annotations\n        if isinstance(anno_type, (list, tuple)):\n            filtered_output = {key: output[key] for key in anno_type if key in output}\n            return filtered_output\n\n        # If the anno_type is not recognized, raise an error\n        raise ValueError(f\"Invalid annotation type: {anno_type}\")"}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "        normalized_query = normalize_string(query)\n        keywords = normalized_query.split()\n        url_scores = {}\n        for kw in keywords:\n            kw_scores = self.bm25(kw)\n            url_scores = update_url_scores(url_scores, kw_scores)\n        return url_scores"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": "        for url, content in documents:\n            self.index(url, content)"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "        self.normalize_angles()\n\n        # Convert to (x1, y1, x2, y2) format\n        boxes = self.tensor\n        angles = boxes[:, 4]\n        ctr_x = boxes[:, 0]\n        ctr_y = boxes[:, 1]\n        widths = boxes[:, 2]\n        heights = boxes[:, 3]\n\n        # Identify nearly horizontal boxes\n        inds = torch.abs(angles) < clip_angle_threshold\n\n        # Clip nearly horizontal boxes\n        x1 = ctr_x[inds] - widths[inds] / 2.0\n        y1 = ctr_y[inds] - heights[inds] / 2.0\n        x2 = ctr_x[inds] + widths[inds] / 2.0\n        y2 = ctr_y[inds] + heights[inds] / 2.0\n\n        # Clamp coordinates to fit within the specified box_size\n        x1.clamp_(min=0, max=box_size[1] - 1)\n        y1.clamp_(min=0, max=box_size[0] - 1)\n        x2.clamp_(min=0, max=box_size[1] - 1)\n        y2.clamp_(min=0, max=box_size[0] - 1)\n\n        # Convert back to (cx, cy, w, h, angle) format\n        ctr_x[inds] = (x1 + x2) / 2.0\n        ctr_y[inds] = (y1 + y2) / 2.0\n        widths[inds] = x2 - x1\n        heights[inds] = y2 - y1\n\n        # Update the tensor in place\n        self.tensor[inds, 0] = ctr_x[inds]\n        self.tensor[inds, 1] = ctr_y[inds]\n        self.tensor[inds, 2] = widths[inds]\n        self.tensor[inds, 3] = heights[inds]\n        # Angles remain unchanged for nearly horizontal boxes\n\n        # Note: The code does not handle clipping for boxes with a significant rotation angle.\n        # Additional logic would be needed to handle such cases properly."}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        # Initialize a dictionary to hold the counts for each type\n        stats = {'doc': 0, 'gen': 0, 'kno': 0, 'num': 0}\n        \n        # Iterate over each item in the data and increment the count for the corresponding type\n        for item in self.data:\n            if 'type' in item and item['type'] in stats:\n                stats[item['type']] += 1\n        \n        return stats"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the texts using jieba\n    continuation_tokens = list(jieba.cut(continuation))\n    reference_tokens = list(jieba.cut(reference))\n\n    # Initialize the BLEU metric from the evaluate library\n    bleu_metric = evaluate.load('bleu')\n\n    # Compute the BLEU score\n    results = bleu_metric.compute(predictions=[continuation_tokens], references=[[reference_tokens]], max_order=4, smooth=False)\n\n    # Extract the BLEU-4 score\n    bleu_score = results['bleu']\n\n    # If with_penalty is False, we need to remove the brevity penalty from the score\n    if not with_penalty:\n        # Calculate the brevity penalty\n        bp = results['brevity_penalty']\n        # Adjust the BLEU score by dividing by the brevity penalty\n        bleu_score /= bp\n\n    return bleu_score"}
{"namespace": "common.rougeL_score", "completion": "    # Define a tokenizer function using jieba\n    f = lambda text: list(jieba.cut(text))\n    # Load the ROUGE metric\n    rouge = evaluate.load('rouge')\n    # Compute the ROUGE-L score using the custom tokenizer\n    results = rouge.compute(predictions=[continuation], references=[reference], tokenizer=f)\n    # Extract the ROUGE-L score from the results\n    score = results['rougeL'].mid.fmeasure\n    return score"}
{"namespace": "chat.utils.run_cmd", "completion": "    print(f\"Executing command: {cmd}\")\n    result = subprocess.run(cmd, shell=True)\n    return result.returncode"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    elif cfg['type'] in MMDET_LOSSES._module_dict.keys():\n        return MMDET_LOSSES.build(cfg)\n    elif cfg['type'] in MMSEG_LOSSES._module_dict.keys():\n        return MMSEG_LOSSES.build(cfg)\n    else:\n        raise NotImplementedError(f\"Loss {cfg['type']} is not recognized.\")"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        raise NotImplementedError(f\"The segmentor type {cfg['type']} is not recognized.\")"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "    assert not (train_cfg and 'train_cfg' in cfg), \\\n        'train_cfg should be either in cfg or as an argument, not both'\n    assert not (test_cfg and 'test_cfg' in cfg), \\\n        'test_cfg should be either in cfg or as an argument, not both'\n\n    if train_cfg or test_cfg:\n        warnings.warn(\n            'The `train_cfg` and `test_cfg` would be deprecated soon. '\n            'Please specify them in the model.', DeprecationWarning)\n\n    if 'train_cfg' in cfg or 'test_cfg' in cfg:\n        warnings.warn(\n            'The model-specific `train_cfg` and `test_cfg` are deprecated. '\n            'Please specify them in the model directly.', DeprecationWarning)\n        if 'train_cfg' in cfg:\n            train_cfg = cfg.pop('train_cfg')\n        if 'test_cfg' in cfg:\n            test_cfg = cfg.pop('test_cfg')\n\n    if cfg['type'] in DETECTORS._module_dict.keys():\n        return DETECTORS.build(cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMDET_DETECTORS.build(cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    # Calculate the adjusted value to fit within the specified period range\n    val = val - torch.floor(val / period + offset) * period\n    return val"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    gt_dict = {label: [] for label in label2cat.keys()}\n    dt_dict = {label: [] for label in label2cat.keys()}\n    for anno in gt_annos:\n        label = anno['label']\n        gt_dict[label].append(anno)\n    for anno in dt_annos:\n        label = anno['label']\n        dt_dict[label].append(anno)\n\n    # Evaluate mAP and recall for each class\n    recalls, precisions, aps = eval_map_recall(dt_dict, gt_dict, metric)\n\n    # Calculate mean AP and mean AR across all classes\n    mean_ap = np.array([np.mean([aps[iou_idx][label] for label in label2cat.keys()]) for iou_idx in range(len(metric))])\n    mean_recall = np.array([np.mean([recalls[iou_idx][label] for label in label2cat.keys()]) for iou_idx in range(len(metric))])\n\n    # Organize results into a dictionary\n    eval_results = {\n        'mAP': mean_ap,\n        'mAR': mean_recall\n    }\n    for iou_idx, iou_thr in enumerate(metric):\n        eval_results[f'AP@{iou_thr}'] = {label2cat[label]: aps[iou_idx][label] for label in label2cat.keys()}\n        eval_results[f'Recall@{iou_thr}'] = {label2cat[label]: recalls[iou_idx][label] for label in label2cat.keys()}\n\n    # Format results for logging\n    if logger is not None:\n        headers = ['Class', 'AP', 'Recall']\n        for iou_idx, iou_thr in enumerate(metric):\n            table_data = [headers]\n            for label in label2cat.keys():\n                row_data = [\n                    label2cat[label],\n                    f'{aps[iou_idx][label]:.3f}',\n                    f'{recalls[iou_idx][label]:.3f}'\n                ]\n                table_data.append(row_data)\n            table = AsciiTable(table_data)\n            print_log(f'\\nIoU threshold: {iou_thr}\\n{table.table}', logger=logger)\n\n    return eval_results"}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        return (LiDARBox, LiDARBox.MODE)\n    elif box_type == 'Camera':\n        return (CameraBox, CameraBox.MODE)\n    elif box_type == 'Depth':\n        return (DepthBox, DepthBox.MODE)\n    else:\n        raise ValueError(f\"Unknown box type: {box_type}\")"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if messages is not None:\n      if not isinstance(messages, Sequence):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      for message in messages:\n        if not isinstance(message, Mapping) or 'role' not in message or 'content' not in message:\n          raise RequestError(\"Each message must be a dict-like object with 'role' and 'content' keys\")\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.pull", "completion": "    # The URL for pulling a model might differ, but for the sake of this example, we'll assume it's '/api/pull'\n    url = '/api/pull'\n\n    # The payload for the POST request\n    payload = {\n      'model': model,\n      'insecure': insecure\n    }\n\n    # Make the request and return the response or an iterator of responses\n    return self._request_stream(\n      'POST',\n      url,\n      json=payload,\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.generate", "completion": "    if not model:\n      raise ValueError(\"Model parameter is required.\")\n\n    payload = {\n      'model': model,\n      'prompt': prompt,\n      'system': system,\n      'template': template,\n      'context': context or [],\n      'raw': raw,\n      'format': format,\n      'images': images,\n      'options': options,\n      'keep_alive': keep_alive,\n    }\n\n    # Remove None values from the payload\n    payload = {k: v for k, v in payload.items() if v is not None}\n\n    # Construct the URL for the request\n    url = f'/v1/models/{urllib.parse.quote(model)}/generate'\n\n    # Make the request and return the response or stream\n    return self._request_stream(method='POST', url=url, json=payload, stream=stream)"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client.create", "completion": "\n    # Check if neither path nor modelfile is provided\n    if not path and not modelfile:\n      raise RequestError('must provide either a path or modelfile')\n\n    # If path is provided, read the file content\n    if path:\n      with open(path, 'r') as file:\n        modelfile = file.read()\n\n    # If modelfile is provided, encode it to base64\n    if modelfile:\n      encoded_modelfile = b64encode(modelfile.encode('utf-8')).decode('utf-8')\n    else:\n      raise RequestError('modelfile content could not be read')\n\n    # Make the request to create the model\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': encoded_modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    # Calculate the SHA-256 checksum of the file\n    with open(path, 'rb') as f:\n      file_content = f.read()\n      digest = sha256(file_content).hexdigest()\n    \n    # Check if the blob already exists on the server\n    digest_prefixed = f'sha256:{digest}'\n    response = self._client.head(f'/api/blobs/{digest_prefixed}')\n    \n    # If the blob does not exist, upload it\n    if response.status_code == 404:\n      response = self._client.post('/api/blobs', content=file_content)\n      response.raise_for_status()  # Ensure the upload was successful\n    \n    # Return the digest of the file\n    return digest_prefixed"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "The provided code is already complete and defines a Python class `Client` and `AsyncClient` for interacting with an API, presumably for a service named \"ollama\". The `generate` function within the `Client` class is fully implemented and is designed to create a response using the requested model. It takes various parameters such as `model`, `prompt`, `system`, `template`, `context`, `stream`, `raw`, `format`, `images`, `options`, and `keep_alive`.\n\nThe `generate` function within the `AsyncClient` class is also fully implemented and is designed to perform the same action as the `Client` class but in an asynchronous manner.\n\nBoth `generate` functions are designed to raise a `RequestError` if a model is not provided and a `ResponseError` if the request could not be fulfilled. They return a `GenerateResponse` if `stream` is `False`, otherwise, they return a `GenerateResponse` generator or an asynchronous generator in the case of `AsyncClient`.\n\nIf you are looking for additional functionality or modifications to the existing code, please provide more specific details on what you need. Otherwise, the code appears to be complete and ready to use as is."}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "import asyncio\n\nasync def main():\n    # Create an instance of the AsyncClient\n    client = AsyncClient(host=\"https://api.example.com\")\n\n    # Define the model and messages for the chat\n    model = \"example-model\"\n    messages = [\n        {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n        {\"role\": \"assistant\", \"content\": \"I'm fine, thank you!\"},\n    ]\n\n    # Call the chat function\n    try:\n        response = await client.chat(model=model, messages=messages, stream=False)\n        print(response)\n    except RequestError as e:\n        print(f\"Request error: {e}\")\n    except ResponseError as e:\n        print(f\"Response error: {e}\")\n\n# Run the main function in the asyncio event loop\nasyncio.run(main())"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "It seems like the `push` function in the `AsyncClient` class is already complete. The function is designed to send an asynchronous POST request to the '/api/push' endpoint with the specified parameters and handle the response accordingly.\n\nThe function takes the following parameters:\n\n- `model`: The name of the model to be pushed.\n- `insecure`: A boolean flag indicating whether the request should be made over an insecure connection (defaults to `False`).\n- `stream`: A boolean flag indicating whether the response should be streamed (defaults to `False`).\n\nThe function returns either a single `ProgressResponse` object or a generator yielding `ProgressResponse` objects, depending on the value of the `stream` parameter.\n\nIf you are looking for a specific change or addition to the function, please provide more details on what you expect the function to do, and I can help you modify it accordingly."}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "        # Create a temporary file to hold the combined user and test code\n        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as temp_file:\n            temp_file_name = temp_file.name\n            temp_file.write(user_code)\n            temp_file.write(test_code)\n        \n        # Run Pyright type checker on the temporary file\n        try:\n            result = subprocess.run(\n                ['pyright', temp_file_name],\n                capture_output=True,\n                text=True,\n                check=False\n            )\n        finally:\n            # Ensure the temporary file is removed after checking\n            Path(temp_file_name).unlink(missing_ok=True)\n\n        # Parse Pyright output for errors\n        errors = re.findall(cls.PYRIGHT_MESSAGE_REGEX, result.stderr, re.MULTILINE)\n\n        # Check for expected type errors\n        expected_errors = defaultdict(list)\n        for line in user_code.splitlines():\n            if cls.EXPECT_ERROR_COMMENT in line:\n                line_number = line.split(':')[0]\n                expected_errors[int(line_number)].append(line)\n\n        # Determine if the type check passed\n        passed = True\n        messages = []\n        for line_number, message in errors:\n            if int(line_number) in expected_errors:\n                expected_error_lines = expected_errors[int(line_number)]\n                if any(expected_error_line in message for expected_error_line in expected_error_lines):\n                    messages.append(f\"Expected type error on line {line_number}: {message}\")\n                else:\n                    passed = False\n                    messages.append(f\"Unexpected type error on line {line_number}: {message}\")\n            else:\n                passed = False\n                messages.append(f\"Unexpected type error on line {line_number}: {message}\")\n\n        # Return the type check result\n        return TypeCheckResult(\n            message='\\n'.join(messages) if messages else \"Type check passed.\",\n            passed=passed,\n            debug_info={\"pyright_output\": result.stderr}\n        )"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "  def _parse_modelfile(self, modelfile: str, base: Optional[Path] = None) -> str:\n    base = Path.cwd() if base is None else base\n\n    out = io.StringIO()\n    for line in io.StringIO(modelfile):\n      command, _, args = line.partition(' ')\n      if command.upper() not in ['FROM', 'ADAPTER']:\n        print(line, end='', file=out)\n        continue\n\n      path = Path(args.strip()).expanduser()\n      path = path if path.is_absolute() else base / path\n      if path.exists():\n        args = f'@{self._create_blob(path)}\\n'\n      print(command, args, end='', file=out)\n\n    return out.getvalue()\n\n  async def _create_blob(self, path: Union[str, Path]) -> str:\n    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    forward_compiler = get_compiler_fn(\"Forward Pass:\")\n    backward_compiler = get_compiler_fn(\"Backward Pass:\")\n\n    with no_fake_tensor():\n        if isinstance(fn, torch.nn.Module):\n            # Compile the module using aot_module\n            compiled_module = aot_module(fn, fw_compiler=forward_compiler, bw_compiler=backward_compiler)\n            return compiled_module\n        else:\n            # Compile the function using aot_function\n            compiled_function = aot_function(fn, fw_compiler=forward_compiler, bw_compiler=backward_compiler)\n            return compiled_function"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    # Load the summary CSV file into a pandas DataFrame\n    summary_file_path = os.path.join(trial_path, 'summary.csv')\n    if not os.path.exists(summary_file_path):\n        raise FileNotFoundError(f\"Summary file not found at {summary_file_path}\")\n    summary_df = pd.read_csv(summary_file_path)\n\n    # Load the configuration YAML file into a dictionary\n    config_file_path = os.path.join(trial_path, 'config.yaml')\n    if not os.path.exists(config_file_path):\n        raise FileNotFoundError(f\"Configuration file not found at {config_file_path}\")\n    with open(config_file_path, 'r') as config_file:\n        config_dict = yaml.safe_load(config_file)\n\n    # Convert the summary DataFrame to a configuration dictionary\n    best_config_dict = summary_df_to_yaml(summary_df, config_dict)\n\n    # Save the configuration dictionary to a YAML file if an output path is provided\n    if output_path:\n        if not output_path.lower().endswith(('.yaml', '.yml')):\n            raise ValueError(\"Output file extension must be .yaml or .yml\")\n        with open(output_path, 'w') as output_file:\n            yaml.dump(best_config_dict, output_file, default_flow_style=False)\n\n    # Return the configuration dictionary\n    return best_config_dict"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "    cache = {}\n    lock = threading.Lock()\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        nonlocal cache\n        key = (args, frozenset(kwargs.items()))\n\n        with lock:\n            if key in cache:\n                traced_func = cache[key]\n            else:\n                if isinstance(func, torch.nn.Module):\n                    # Trace the module's forward method\n                    traced_func, _ = trace_with_kwargs(func.forward, example_inputs=args, example_kwarg_inputs=kwargs, **kwargs_)\n                else:\n                    # Trace the function directly\n                    traced_func, _ = trace_with_kwargs(func, example_inputs=args, example_kwarg_inputs=kwargs, **kwargs_)\n\n                if ts_compiler is not None:\n                    # Apply the compiler to the traced function\n                    traced_func = ts_compiler(traced_func)\n\n                cache[key] = traced_func\n\n        return traced_func(*args, **kwargs)\n\n    return wrapper"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        # Extract the best configuration from the trial folder\n        best_config = extract_best_config(trial_path)\n        \n        # The project directory is assumed to be the parent directory of the trial folder\n        project_dir = os.path.dirname(trial_path)\n        \n        # Initialize and return the Runner instance with the extracted configuration and project directory\n        return cls(best_config, project_dir=project_dir)"}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "    if metadatas is None:\n        metadatas = [None] * len(results)\n\n    filtered_results = []\n    filtered_metadatas = []\n\n    for result, val, metadata in zip(results, value, metadatas):\n        if val <= threshold:\n            filtered_results.append(result)\n            filtered_metadatas.append(metadata)\n\n    return filtered_results, filtered_metadatas"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Initialize variables to store the best module and its results\n    best_module = None\n    best_result = None\n    best_evaluation = None\n\n    # Iterate over modules and their parameters\n    for module, params in zip(modules, module_params):\n        # Run the module with the given parameters and measure its speed\n        result, speed = measure_speed(module, params)\n\n        # Evaluate the module's result\n        evaluation = evaluate_retrieval(result, previous_result)\n\n        # Apply the filter by threshold strategy if specified\n        if 'threshold' in strategies:\n            result = filter_by_threshold(result, strategies['threshold'])\n\n        # Save the result and evaluation to disk\n        result_file = os.path.join(node_line_dir, f\"{module.__name__}_result.csv\")\n        summary_file = os.path.join(node_line_dir, f\"{module.__name__}_summary.json\")\n        result.to_csv(result_file, index=False)\n        load_summary_file(summary_file, evaluation, speed)\n\n        # Select the best module based on the evaluation and speed\n        if best_module is None or select_best_average(evaluation, best_evaluation, speed, strategies):\n            best_module = module\n            best_result = result\n            best_evaluation = evaluation\n\n    # Combine the previous result with the best module's result\n    combined_result = pd.concat([previous_result, best_result], axis=1)\n\n    # Save the combined result to disk\n    combined_result_file = os.path.join(node_line_dir, \"best_combined_result.csv\")\n    combined_result.to_csv(combined_result_file, index=False)\n\n    # Return the best result dataframe\n    return combined_result"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    if len(ids) != len(scores) or len(ids) != len(weights):\n        raise ValueError(\"The length of ids, scores, and weights must be the same.\")\n\n    # Check if the sum of weights is equal to 1\n    if not math.isclose(sum(weights), 1.0):\n        raise ValueError(\"The sum of weights must be equal to 1.\")\n\n    # Normalize scores for each retrieval result\n    normalized_scores = [list(pd.Series(s).rank(pct=True)) for s in scores]\n\n    # Combine scores using the weights\n    combined_scores = {}\n    for i in range(len(ids)):\n        for j in range(len(ids[i])):\n            doc_id = ids[i][j]\n            score = normalized_scores[i][j] * weights[i]\n            if doc_id in combined_scores:\n                combined_scores[doc_id] += score\n            else:\n                combined_scores[doc_id] = score\n\n    # Sort combined results by score in descending order and select top_k results\n    sorted_combined_scores = sorted(combined_scores.items(), key=lambda item: item[1], reverse=True)\n    top_results = sorted_combined_scores[:top_k]\n\n    # Separate ids and scores\n    top_ids, top_scores = zip(*top_results) if top_results else ([], [])\n\n    return list(top_ids), list(top_scores)"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": "    # Check if all lists in ids and scores have the same length\n    assert all(len(id_list) == len(score_list) for id_list, score_list in zip(ids, scores)), \\\n        \"All lists in ids and scores must have the same length.\"\n\n    # Calculate the weighted sum of scores for each ID\n    weighted_scores = {}\n    for id_list, score_list, weight in zip(ids, scores, weights):\n        for id, score in zip(id_list, score_list):\n            if id not in weighted_scores:\n                weighted_scores[id] = 0\n            weighted_scores[id] += score * weight\n\n    # Normalize the weighted scores\n    max_score = max(weighted_scores.values())\n    min_score = min(weighted_scores.values())\n    for id in weighted_scores:\n        weighted_scores[id] = (weighted_scores[id] - min_score) / (max_score - min_score)\n\n    # Sort the IDs by their weighted scores in descending order\n    sorted_ids = sorted(weighted_scores, key=weighted_scores.get, reverse=True)\n\n    # Return the top K IDs and their corresponding normalized weighted scores\n    top_ids = sorted_ids[:top_k]\n    top_scores = [weighted_scores[id] for id in top_ids]\n\n    return top_ids, top_scores"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "\n    # Ensure the output directory exists\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Prepare a list to store the results of each module\n    results = []\n\n    # Iterate over each module and its corresponding parameters\n    for module, params in zip(modules, module_params):\n        # Create a deep copy of the previous result to avoid modifying it\n        input_data = deepcopy(previous_result)\n\n        # Measure the execution time of the module\n        with measure_speed() as timer:\n            # Execute the module with the given parameters\n            output_data = module(input_data, **params)\n\n        # Evaluate the performance of the module\n        evaluation_metrics = evaluate_retrieval_node(output_data, strategies)\n\n        # Store the results along with the execution time and evaluation metrics\n        results.append({\n            'module': module.__name__,\n            'params': params,\n            'output_data': output_data,\n            'execution_time': timer(),\n            'evaluation_metrics': evaluation_metrics\n        })\n\n    # Convert the list of results to a DataFrame\n    results_df = pd.DataFrame(results)\n\n    # Filter the results by the speed threshold if specified\n    if 'speed_threshold' in strategies:\n        results_df = filter_by_threshold(results_df, strategies['speed_threshold'])\n\n    # Select the best result based on the average of the evaluation metrics\n    best_result = select_best_average(results_df, strategies['evaluation_metrics'])\n\n    # Save the results and the best result to the specified directory\n    results_df.to_csv(os.path.join(node_line_dir, 'query_expansion_results.csv'), index=False)\n    best_result['output_data'].to_csv(os.path.join(node_line_dir, 'best_query_expansion_result.csv'), index=False)\n\n    # Log the best module and parameters\n    logger.info(f\"Best query expansion module: {best_result['module']}\")\n    logger.info(f\"Parameters: {best_result['params']}\")\n\n    # Return the DataFrame of the best result\n    return best_result['output_data']"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "\n    # Ensure the node_line_dir exists\n    pathlib.Path(node_line_dir).mkdir(parents=True, exist_ok=True)\n\n    # Validate the input dataset\n    validate_qa_dataset(previous_result)\n\n    # Get the support modules (e.g., generator) if specified in strategies\n    support_modules = get_support_modules(strategies)\n\n    # Prepare the dataframe to collect results\n    results = pd.DataFrame()\n\n    # Iterate over each module and its parameters\n    for module, params in zip(modules, module_params):\n        # Create combinations of parameters for the current module\n        param_combinations = make_combinations(params)\n\n        # Iterate over each combination of parameters\n        for param_set in param_combinations:\n            # Copy the previous result to avoid modifying the original\n            current_result = deepcopy(previous_result)\n\n            # Apply the prompt maker module with the current parameter set\n            current_result = module(current_result, **param_set)\n\n            # Evaluate the generation using the support modules\n            evaluation_metrics = evaluate_generation(current_result, support_modules)\n\n            # Cast metrics to the correct types\n            evaluation_metrics = cast_metrics(evaluation_metrics)\n\n            # Measure the speed of the current prompt maker\n            speed = measure_speed(module, current_result)\n\n            # Add the evaluation metrics and speed to the results\n            result_entry = {**param_set, **evaluation_metrics, 'speed': speed}\n            results = results.append(result_entry, ignore_index=True)\n\n    # Filter results by speed threshold if specified\n    if 'speed_threshold' in strategies:\n        results = filter_by_threshold(results, 'speed', strategies['speed_threshold'])\n\n    # Select the best prompt maker based on average metrics\n    best_prompt_maker = select_best_average(results, strategies.get('evaluation_metrics', []))\n\n    # Combine the best prompt maker's result with the previous result\n    combined_result = pd.concat([previous_result, best_prompt_maker])\n\n    # Save the results and summary to the node_line_dir\n    results.to_csv(os.path.join(node_line_dir, 'results.csv'), index=False)\n    best_prompt_maker.to_csv(os.path.join(node_line_dir, 'best_prompt_maker.csv'), index=False)\n\n    return combined_result"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    # Initialize an empty list to store all extracted values from all nodes\n    all_values = []\n\n    # Iterate over each node and extract values using the extract_values function\n    for node in nodes:\n        node_values = extract_values(node, key)\n        all_values.extend(node_values)\n\n    # Remove duplicates by converting the list to a set and back to a list\n    unique_values = list(set(all_values))\n\n    return unique_values"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    # Set default columns to convert if not provided\n    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    # Read the summary file into a pandas DataFrame\n    df = pd.read_csv(summary_path)\n\n    # Convert specified columns that contain dictionary-like strings into actual dictionaries\n    for column in dict_columns:\n        if column in df.columns:\n            df[column] = df[column].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n\n    return df"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        # Extract the module type from the dictionary\n        module_type = module_dict.get('module_type')\n        if not module_type:\n            raise ValueError(\"The 'module_type' key is missing in the module dictionary.\")\n\n        # Use the remaining dictionary as module parameters\n        module_param = deepcopy(module_dict)\n        del module_param['module_type']\n\n        # Return a new Module instance initialized with these parameters\n        return cls(module_type=module_type, module_param=module_param)"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    metric_names = []\n    metric_params = []\n\n    for metric in metrics:\n        if isinstance(metric, str):\n            # If the metric is a string, add it to the metric_names list and add an empty dict to metric_params.\n            metric_names.append(metric)\n            metric_params.append({})\n        elif isinstance(metric, dict):\n            # If the metric is a dictionary, extract the name and parameters.\n            if 'name' not in metric:\n                raise ValueError(\"Metric dictionary must have a 'name' key.\")\n            name = metric['name']\n            params = deepcopy(metric)\n            del params['name']  # Remove the name key from the params dictionary.\n            metric_names.append(name)\n            metric_params.append(params)\n        else:\n            raise TypeError(\"Metrics must be either a list of strings or a list of dictionaries.\")\n\n    return metric_names, metric_params"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    # If no embedding model is provided, use a default one\n    if embedding_model is None:\n        embedding_model = embedding_models.get_model('all-mpnet-base-v2')\n\n    # Convert the predicted string and ground truth strings to embeddings\n    pred_embedding = embedding_model.encode([pred])[0]\n    gt_embeddings = embedding_model.encode(generation_gt)\n\n    # Calculate cosine similarity between the predicted embedding and each ground truth embedding\n    similarities = [calculate_cosine_similarity(pred_embedding, gt_emb) for gt_emb in gt_embeddings]\n\n    # Return the maximum similarity score\n    return max(similarities)"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "    global gfpgan_face_restorer\n\n    # Check if the GFPGAN face restorer is initialized\n    if gfpgan_face_restorer is None:\n        # Initialize the GFPGAN face restorer\n        try:\n            gfpgan_face_restorer = FaceRestorerGFPGAN()\n            gfpgan_face_restorer.net = gfpgan_face_restorer.load_net()\n        except Exception as e:\n            # Log a warning if the restorer cannot be initialized\n            logger.warning(f\"GFPGAN face restorer could not be initialized: {e}\")\n            return np_image\n\n    # Use the GFPGAN face restorer to restore faces in the image\n    try:\n        restored_image = gfpgan_face_restorer.restore(np_image)\n        return restored_image\n    except Exception as e:\n        # Log an error if face restoration fails\n        logger.error(f\"GFPGAN face restoration failed: {e}\")\n        return np_image"}
{"namespace": "codeformer_model.setup_model", "completion": "    global codeformer\n    try:\n        codeformer = FaceRestorerCodeFormer(model_path=dirname)\n        codeformer.load_net()\n        logger.info(f\"CodeFormer model loaded from {dirname}\")\n    except Exception as e:\n        logger.error(f\"Failed to set up CodeFormer model: {e}\")\n        raise errors.ModelSetupError(f\"Failed to set up CodeFormer model: {e}\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    global gfpgan_face_restorer\n    try:\n        # Patch the facexlib with the given directory\n        shared.face_restoration_model_path = dirname\n\n        # Initialize the GFPGAN face restorer\n        gfpgan_face_restorer = FaceRestorerGFPGAN(model_path=dirname)\n        gfpgan_face_restorer.net = gfpgan_face_restorer.load_net()\n\n    except Exception as e:\n        # Handle any exceptions that occur during the setup process\n        logger.error(f\"Failed to set up GFPGAN model: {e}\")\n        gfpgan_face_restorer = None\n        if isinstance(e, errors.ModelNotFoundError):\n            raise e"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector into a quaternion with zero real part\n  v_quat = jnp.concatenate([v, jnp.array([0.0])], axis=-1)\n  \n  # Apply the rotation: q * v * q^-1\n  q_conj = conjugate(q)\n  rotated_quat = multiply(multiply(q, v_quat), q_conj)\n  \n  # Extract the imaginary part (the rotated vector)\n  return im(rotated_quat)"}
{"namespace": "quaternion.from_axis_angle", "completion": "  angle = linalg.norm(axis_angle, axis=-1, keepdims=True)\n  axis = axis_angle / jnp.maximum(angle, eps)\n  \n  half_angle = angle * 0.5\n  sin_half_angle = jnp.sin(half_angle)\n  \n  w = jnp.cos(half_angle)\n  xyz = sin_half_angle * axis\n  \n  return jnp.concatenate([xyz, w], axis=-1)"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "    topk_words = model.topk(prefix, k=k)\n    num_calls = 1\n\n    # check if idx is already in the top-k\n    if idx in topk_words:\n        return topk_words[idx], num_calls\n\n    # perform bisection search to find the bias that makes idx the most probable\n    low = 0\n    eps = 1e-8\n    while high - low > eps:\n        mid = (high + low) / 2\n        logit_bias = {idx: mid}\n        topk_words = model.topk(prefix, logit_bias, k=k)\n        num_calls += 1\n\n        if idx in topk_words:\n            high = mid\n        else:\n            low = mid\n\n    # calculate the log probability of idx being the top result\n    logit_bias = {idx: high}\n    topk_words = model.topk(prefix, logit_bias, k=k)\n    num_calls += 1\n    log_prob = topk_words.get(idx, float('-inf'))\n\n    return log_prob, num_calls"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "    # Set the current context\n    eglctx.make_current()\n\n    # Set the viewport to match the camera's resolution\n    gl.glViewport(0, 0, camera.width, camera.height)\n\n    # Clear the color and depth buffers\n    gl.glClearColor(0.0, 0.0, 0.0, 0.0)\n    gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)\n\n    # Set up the projection and view matrices\n    projection_matrix = camera.projection_matrix\n    view_matrix = camera.view_matrix\n\n    # Choose the appropriate shader program based on the render type\n    if self.render_type == Mesh.RenderType.POINTS:\n        program = self.point_program\n    else:\n        program = self.mesh_program\n\n    # Use the shader program\n    gl.glUseProgram(program)\n\n    # Set the shader uniforms for projection and view matrices\n    projection_location = gl.glGetUniformLocation(program, \"projection\")\n    view_location = gl.glGetUniformLocation(program, \"view\")\n    gl.glUniformMatrix4fv(projection_location, 1, gl.GL_FALSE, glm.value_ptr(projection_matrix))\n    gl.glUniformMatrix4fv(view_location, 1, gl.GL_FALSE, glm.value_ptr(view_matrix))\n\n    # Bind the vertex array object (VAO) and draw the mesh\n    gl.glBindVertexArray(self.vao)\n    if self.render_type == Mesh.RenderType.POINTS:\n        gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n    elif self.render_type == Mesh.RenderType.TRIS:\n        gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * 3, gl.GL_UNSIGNED_INT, None)\n    # Unbind the VAO\n    gl.glBindVertexArray(0)\n\n    # Unset the shader program\n    gl.glUseProgram(0)\n\n    # Swap buffers if necessary (for double buffering)\n    eglctx.swap_buffers()"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Create a dictionary of the existing BertConfig attributes\n    config_dict = bert_config.to_dict()\n\n    # Add or modify the configuration attributes specific to NomicBertConfig\n    # For example, let's assume 'nomic_specific_attr' is a new attribute for NomicBertConfig\n    # We'll set it to a default value or derive it from existing attributes\n    nomic_specific_attr = 'default_value'  # Replace with actual logic if needed\n\n    # Create a new NomicBertConfig object with the updated configuration\n    nomic_config = NomicBertConfig(**config_dict, nomic_specific_attr=nomic_specific_attr)\n\n    return nomic_config"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        if not self.visible:\n            return\n\n        # Set OpenGL options\n        common_opengl_options()\n\n        # Select and use the appropriate shader program\n        if self.render_type == Mesh.RenderType.POINTS:\n            program = self.point_program\n        else:\n            program = self.mesh_program\n        use_gl_program(program)\n\n        # Upload uniforms to the GPU\n        self.upload_gl_uniforms(camera, program)\n\n        # Bind the vertex array object (VAO)\n        gl.glBindVertexArray(self.vao)\n\n        # Draw the mesh based on the render type\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, len(self.verts))\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glBindBuffer(gl.GL_ELEMENT_ARRAY_BUFFER, self.ebo)\n            gl.glDrawElements(gl.GL_LINES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, None)\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glBindBuffer(gl.GL_ELEMENT_ARRAY_BUFFER, self.ebo)\n            gl.glDrawElements(gl.GL_TRIANGLES, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, None)\n        elif self.render_type == Mesh.RenderType.QUADS:\n            # Note: OpenGL core profile does not support GL_QUADS, you would need to convert quads to triangles\n            pass\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glBindBuffer(gl.GL_ELEMENT_ARRAY_BUFFER, self.ebo)\n            gl.glDrawElements(gl.GL_TRIANGLE_STRIP, len(self.faces) * self.face_size, gl.GL_UNSIGNED_INT, None)\n\n        # Unbind the vertex array object (VAO)\n        gl.glBindVertexArray(0)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "    # Convert PyTorch tensor to numpy array if necessary\n    if isinstance(ptr, torch.Tensor):\n        ptr = ptr.detach().cpu().numpy()\n\n    # Ensure the input is a numpy array with the correct type\n    ptr = np.asarray(ptr, dtype=np.uint8, order='C')\n\n    # Use the object's width and height if w and h are not provided\n    w = w or self.W\n    h = h or self.H\n\n    # Bind the texture\n    gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n    # Set unpack alignment to 1 for tightly packed data\n    gl.glPixelStorei(gl.GL_UNPACK_ALIGNMENT, 1)\n\n    # Update the texture with the new data\n    gl.glTexSubImage2D(gl.GL_TEXTURE_2D, 0, x, y, w, h, gl.GL_RGBA, gl.GL_UNSIGNED_BYTE, ptr)\n\n    # Unbind the texture\n    gl.glBindTexture(gl.GL_TEXTURE_2D, 0)"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "    # Validate input shapes\n    assert R.dim() == 3 and R.shape[1:] == (3, 3), \"R must be a batch of 3x3 rotation matrices\"\n    assert tvec.dim() == 2 and tvec.shape[1] == 3, \"tvec must be a batch of 3D translation vectors\"\n    assert camera_matrix.dim() == 3 and camera_matrix.shape[1:] == (3, 3), \"camera_matrix must be a batch of 3x3 intrinsic matrices\"\n    assert image_size.dim() == 2 and image_size.shape[1] == 2, \"image_size must be a batch of 2D image sizes\"\n\n    # Convert rotation matrix to 6D representation\n    rotation_6d = matrix_to_rotation_6d(R)\n\n    # Compute camera position from translation vector\n    camera_position = -R.transpose(-1, -2) @ tvec.unsqueeze(-1)\n    camera_position = camera_position.squeeze(-1)\n\n    # Compute focal length and sensor width\n    fx, fy, cx, cy = camera_matrix[:, 0, 0], camera_matrix[:, 1, 1], camera_matrix[:, 0, 2], camera_matrix[:, 1, 2]\n    if (fx - fy).abs().max() / fx.mean() > 0.01:\n        warn_once_about_pulsar_fxfy()\n    focal_length = (fx + fy) * 0.5 * znear\n    sensor_width = image_size[:, 0] * znear / focal_length\n\n    # Compute principal point offsets\n    principal_point_offsets = torch.stack([(cx - image_size[:, 0] * 0.5) * znear / focal_length,\n                                           (cy - image_size[:, 1] * 0.5) * znear / focal_length], dim=-1)\n\n    # Combine all parameters into a single tensor\n    camera_params = torch.cat([camera_position, rotation_6d, focal_length.unsqueeze(-1), sensor_width.unsqueeze(-1), principal_point_offsets], dim=-1)\n\n    return camera_params"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            # If not using quad draw, use blit method\n            self.blit(x, y, w, h)\n            return\n\n        # Set the viewport and scissor box for rendering\n        w = w or self.W\n        h = h or self.H\n        old_viewport = gl.glGetIntegerv(gl.GL_VIEWPORT)\n        old_scissor_box = gl.glGetIntegerv(gl.GL_SCISSOR_BOX)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n\n        # Activate the shader program\n        gl.glUseProgram(self.quad_program)\n\n        # Bind the texture\n        gl.glActiveTexture(gl.GL_TEXTURE0)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n\n        # Bind the VAO and draw the quad\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)  # Draw quad as triangle strip\n\n        # Restore the viewport and scissor box\n        gl.glViewport(*old_viewport)\n        gl.glScissor(*old_scissor_box)\n\n        # Unbind the VAO and texture\n        gl.glBindVertexArray(0)\n        gl.glBindTexture(gl.GL_TEXTURE_2D, 0)"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n\n    # Adjust R and T for PyTorch3D's coordinate system\n    R_pytorch3d = R.permute(0, 2, 1)  # Transpose the rotation matrix\n    T_pytorch3d = -R_pytorch3d @ T  # Adjust translation based on the transposed rotation\n\n    # Convert the intrinsic matrix K to NDC\n    # Assuming that the batch contains the same intrinsic matrix for all instances\n    fx, fy = K[0, 0, 0], K[0, 1, 1]\n    cx, cy = K[0, 0, 2], K[0, 1, 2]\n    K_ndc = torch.tensor([[2.0 * fx / W, 0.0, -2.0 * cx / W + 1.0],\n                          [0.0, 2.0 * fy / H, -2.0 * cy / H + 1.0],\n                          [0.0, 0.0, 1.0]], device=K.device)\n\n    # Compute the camera center in the camera's coordinate system\n    C = -R_pytorch3d.transpose(1, 2) @ T_pytorch3d\n\n    return H, W, K_ndc, R_pytorch3d, T_pytorch3d, C.squeeze(2)"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "        w = w or self.W\n        h = h or self.H\n        old_fbo = gl.glGetIntegerv(gl.GL_READ_FRAMEBUFFER_BINDING)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, self.fbo)\n        gl.glBlitFramebuffer(0, 0, self.max_W, self.max_H, x, y, w, h, gl.GL_COLOR_BUFFER_BIT, gl.GL_NEAREST)\n        gl.glBindFramebuffer(gl.GL_READ_FRAMEBUFFER, old_fbo)"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Ensure that t1 is sorted\n    sorted_indices = torch.argsort(t1)\n    t1_sorted = t1[sorted_indices]\n    y1_sorted = y1[sorted_indices]\n\n    # Find the indices of the closest points in t1 that are less than or equal to t0\n    inner_indices = searchsorted(t1_sorted, t0) - 1\n    inner_indices = inner_indices.clamp(min=0)  # Ensure indices are not negative\n\n    # Find the indices of the closest points in t1 that are greater than t0\n    outer_indices = inner_indices + 1\n    outer_indices = outer_indices.clamp(max=t1_sorted.size(0) - 1)  # Ensure indices are within bounds\n\n    # Gather the inner and outer values based on the indices\n    inner_values = torch.gather(y1_sorted, 0, inner_indices)\n    outer_values = torch.gather(y1_sorted, 0, outer_indices)\n\n    return inner_values, outer_values"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # Calculate the inner and outer measures\n    w_inner, w_outer = inner_outer(t, t_env, w_env)\n\n    # Calculate the loss as the sum of the squared differences between the target weights and the outer envelope,\n    # scaled by the outer envelope to form a half-quadratic loss\n    loss = torch.sum((w - w_outer).clamp(min=0) ** 2 / (w_outer + eps), dim=-1)\n\n    return loss"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # Ensure that t and w have the correct dimensions\n    assert t.shape[-1] == w.shape[-1] + 1, \"The last dimension of 't' should be one more than that of 'w'.\"\n\n    # Calculate the differences between adjacent elements in t\n    dt = t[..., 1:] - t[..., :-1]\n\n    # Calculate the inter-interval loss (distortion between intervals)\n    inter_interval_loss = torch.sum(w * dt**2)\n\n    # Calculate the intra-interval loss (distortion within each interval)\n    # This can be done by considering the variance within each interval, weighted by w\n    # For simplicity, we assume uniform distribution within each interval, so the variance is (dt**2) / 12\n    # The factor of 1/12 comes from the variance of a uniform distribution in the interval [a, b]: (b - a)^2 / 12\n    intra_interval_loss = torch.sum(w * (dt**2) / 12)\n\n    # Combine the inter-interval and intra-interval losses\n    total_distortion_loss = inter_interval_loss + intra_interval_loss\n\n    return total_distortion_loss"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Match up the channels of t and w\n    t, w = matchup_channels(t, w)\n\n    # Integrate the weights to get the CDF\n    cw0 = integrate_weights(w)\n\n    # Convert the list of percentiles to a tensor and scale to [0, 1]\n    ps_tensor = torch.tensor(ps, dtype=t.dtype, device=t.device) / 100.0\n\n    # Interpolate the CDF to find the weighted percentiles\n    percentiles = interpolate(ps_tensor, cw0, t)\n\n    return percentiles"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Use torch.searchsorted to find the indices where elements should be inserted\n    idx = torch.searchsorted(a, v, right=False)\n    \n    # Prepare the lower and upper bounds\n    idx_lo = idx - 1\n    idx_hi = idx\n    \n    # Clamp the indices to ensure they are within the bounds of 'a'\n    idx_lo = torch.clamp(idx_lo, 0, a.size(-1) - 1)\n    idx_hi = torch.clamp(idx_hi, 0, a.size(-1) - 1)\n    \n    return idx_lo, idx_hi"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Match up the channels of t and w\n    t, w = matchup_channels(t, w)\n\n    # Compute the cumulative distribution function (CDF) from the weights\n    cdf = integrate_weights(w)\n\n    # Generate uniform samples in the range [0, 1)\n    if single_jitter:\n        u = torch.linspace(0., 1. - 1e-5, num_samples, device=t.device)\n        u = u.expand(*t.shape[:-1], num_samples)\n        if perturb:\n            u += torch.rand_like(u[..., :1]) / num_samples\n    else:\n        u = torch.rand(*t.shape[:-1], num_samples, device=t.device)\n        if perturb:\n            u += torch.rand_like(u) / num_samples\n    u = u.clamp(min=0., max=1. - 1e-5)\n\n    # Invert the CDF to get the samples\n    samples = interpolate(u, cdf, t)\n\n    return samples"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Ensure that t and w have the same number of channels\n    t, w = matchup_channels(t, w)\n\n    # Compute the dilated time steps\n    t_dilated = torch.nn.functional.max_pool1d(t.unsqueeze(0), kernel_size=dilation, stride=1, padding=dilation // 2).squeeze(0)\n\n    # Clip the dilated time steps to the domain\n    t_dilated = torch.clamp(t_dilated, min=domain[0], max=domain[1])\n\n    # Adjust the weights to match the dilated time steps\n    # This can be done by repeating the weights according to the dilation factor\n    # and then taking the maximum weight in each dilated segment\n    w_repeated = w.repeat_interleave(dilation)\n    w_dilated = torch.nn.functional.max_pool1d(w_repeated.unsqueeze(0), kernel_size=dilation, stride=1, padding=dilation // 2).squeeze(0)\n\n    # Return the dilated and clipped time steps and the adjusted weights\n    return t_dilated, w_dilated"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # Ensure that t is sorted\n    assert torch.all(t[..., 1:] >= t[..., :-1]), \"t must be sorted\"\n\n    # Find the indices where tq should be inserted into t to maintain order\n    idx_lo, idx_hi = searchsorted(t, tq)\n\n    # Handle the case where tq matches a step change in t\n    exact_matches = (tq == t.gather(dim=-1, index=idx_lo))\n    # Use outside_value for exact matches, otherwise interpolate using y\n    result = torch.where(\n        exact_matches,\n        torch.full_like(tq, outside_value),\n        y.gather(dim=-1, index=idx_lo)\n    )\n\n    return result"}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "    # Schlick's bias function\n    def schlick_bias(x, k):\n        return x / (x + k * (1 - x) + eps)\n\n    # Compute the annealed weights\n    w_clipped = w.clip(min=eps)  # Prevent log(0)\n    log_w = torch.log(w_clipped)\n    bias = schlick_bias(train_frac, anneal_slope)\n    w_tilde = torch.exp(log_w * bias)\n\n    # Normalize the weights to sum to 1\n    w_tilde_sum = torch.sum(w_tilde, dim=-1, keepdim=True)\n    w_annealed = w_tilde / w_tilde_sum\n\n    return w_annealed"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "    if isinstance(batch, torch.Tensor):\n        return batch.to(device)\n    elif isinstance(batch, dict):\n        return {k: to_cuda(v, device) for k, v in batch.items() if not (ignore_list and k == \"meta\")}\n    elif isinstance(batch, list):\n        return [to_cuda(x, device) for x in batch]\n    elif isinstance(batch, tuple):\n        return tuple(to_cuda(x, device) for x in batch)\n    else:\n        return batch"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "    # Gather the vertices corresponding to each face index\n    gathered_vertices = multi_gather(v, f, dim=dim)  # Shape: [batch_size, num_faces, 3, vertex_dim]\n\n    # Compute the edges of the triangles\n    edge1 = gathered_vertices[:, :, 1] - gathered_vertices[:, :, 0]  # Shape: [batch_size, num_faces, vertex_dim]\n    edge2 = gathered_vertices[:, :, 2] - gathered_vertices[:, :, 0]  # Shape: [batch_size, num_faces, vertex_dim]\n\n    # Compute the normals of the faces using the cross product of the edges\n    normals = torch.cross(edge1, edge2, dim=dim+1)  # Shape: [batch_size, num_faces, vertex_dim]\n\n    # Normalize the normals\n    normals = torch.nn.functional.normalize(normals, p=2, dim=dim+1)\n\n    return normals"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "    if isinstance(batch, (tuple, list)):\n        return [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        return {k: add_batch(v) for k, v in batch.items()}\n    elif isinstance(batch, torch.Tensor):\n        return batch.unsqueeze(0)\n    elif isinstance(batch, np.ndarray):\n        return np.expand_dims(batch, axis=0)\n    else:\n        return torch.as_tensor(batch).unsqueeze(0)"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "        batch = dotdict()\n        batch.H = torch.tensor(self.H, dtype=torch.int32)\n        batch.W = torch.tensor(self.W, dtype=torch.int32)\n        batch.K = self.K.clone().detach()\n        batch.R = self.R.clone().detach()\n        batch.T = self.T.clone().detach()\n        batch.n = torch.tensor(self.n, dtype=torch.float32)\n        batch.f = torch.tensor(self.f, dtype=torch.float32)\n        batch.t = torch.tensor(self.t, dtype=torch.float32)\n        batch.v = torch.tensor(self.v, dtype=torch.float32)\n        batch.bounds = self.bounds.clone().detach()\n\n        # Additional GUI related elements\n        batch.origin = torch.tensor(self.origin, dtype=torch.float32)\n        batch.world_up = torch.tensor(self.world_up, dtype=torch.float32)\n        batch.movement_speed = torch.tensor(self.movement_speed, dtype=torch.float32)\n        batch.movement_force = torch.tensor(self.movement_force, dtype=torch.float32)\n        batch.drag_coeff_mult = torch.tensor(self.drag_coeff_mult, dtype=torch.float32)\n        batch.constant_drag = torch.tensor(self.constant_drag, dtype=torch.float32)\n        batch.mass = torch.tensor(self.mass, dtype=torch.float32)\n        batch.moment_of_inertia = torch.tensor(self.moment_of_inertia, dtype=torch.float32)\n        batch.movement_torque = torch.tensor(self.movement_torque, dtype=torch.float32)\n        batch.angular_friction = torch.tensor(self.angular_friction, dtype=torch.float32)\n        batch.constant_torque = torch.tensor(self.constant_torque, dtype=torch.float32)\n        batch.min_interval = torch.tensor(self.min_interval, dtype=torch.float32)\n        batch.pause_physics = torch.tensor(self.pause_physics, dtype=torch.bool)\n\n        # Nested 'meta' dictionary\n        batch.meta = dotdict({\n            'H': batch.H,\n            'W': batch.W,\n            'K': batch.K,\n            'R': batch.R,\n            'T': batch.T,\n            'n': batch.n,\n            'f': batch.f,\n            't': batch.t,\n            'v': batch.v,\n            'bounds': batch.bounds,\n            'origin': batch.origin,\n            'world_up': batch.world_up,\n            'movement_speed': batch.movement_speed,\n            'movement_force': batch.movement_force,\n            'drag_coeff_mult': batch.drag_coeff_mult,\n            'constant_drag': batch.constant_drag,\n            'mass': batch.mass,\n            'moment_of_inertia': batch.moment_of_inertia,\n            'movement_torque': batch.movement_torque,\n            'angular_friction': batch.angular_friction,\n            'constant_torque': batch.constant_torque,\n            'min_interval': batch.min_interval,\n            'pause_physics': batch.pause_physics,\n        })\n\n        return batch"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        # Check if the agent is a working agent and not a prime agent\n        if agent.is_working_agent() and not agent.is_prime_agent():\n            # Serialize the agent's state\n            serialized_state = AgentSerializer.serialize(agent)\n            # Save the serialized state to the database\n            self.persistence.save_agent(agent.id, serialized_state)"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        closest_agent = None\n        highest_similarity = float('-inf')\n\n        try:\n            for agent in self.agents:\n                if agent.purpose_embedding is None:\n                    agent.purpose_embedding = self.get_embedding(agent.purpose)\n                \n                similarity = cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0]\n                if similarity > highest_similarity:\n                    highest_similarity = similarity\n                    closest_agent = agent\n\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            return None, float('-inf')\n\n        return closest_agent, highest_similarity"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        # Create a new MicroAgent instance with the prime attributes\n        prime_agent = MicroAgent(\n            prompt=PRIME_PROMPT,\n            name=PRIME_NAME,\n            weight=PRIME_AGENT_WEIGHT,\n            is_prime=True,  # Assuming there is a flag for prime status\n            # Assuming there is another unspecified flag, setting it to a default value\n            # Replace `unspecified_flag` with the actual flag name and value\n            unspecified_flag=False\n        )\n        \n        # Add the prime agent to the agents list\n        self.agents.append(prime_agent)\n        \n        # Log the creation of the prime agent\n        logger.info(f\"Prime agent '{PRIME_NAME}' created and added to the agents list.\")"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "        # Retrieve the serialized agent data from the database\n        agent_dict = self.persistence.load_agent(purpose)\n        \n        # If an agent with the given purpose is found, deserialize it\n        if agent_dict is not None:\n            return AgentSerializer.from_dict(agent_dict, agent_lifecycle, openai_wrapper)\n        \n        # If no agent is found, return None\n        return None"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        # Find the start and end indices of the agent invocation\n        start_index = response.find(\"Use Agent[\")\n        end_index = response.find(\"]\", start_index)\n\n        # Extract the agent invocation string\n        agent_invocation = response[start_index + len(\"Use Agent[\"):end_index].strip()\n\n        # Split the agent invocation string into agent name and input text\n        if \":\" in agent_invocation:\n            agent_name, input_text = agent_invocation.split(\":\", 1)\n        else:\n            agent_name = agent_invocation\n            input_text = \"\"\n\n        # Strip any leading/trailing whitespace from the agent name and input text\n        agent_name = agent_name.strip()\n        input_text = input_text.strip()\n\n        return agent_name, input_text"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": "        # Fetch all serialized agents from the database\n        serialized_agents = self.persistence.fetch_all_agents()\n        \n        # Deserialize each agent and add to the list if successful\n        agents = []\n        for serialized_agent in serialized_agents:\n            agent = AgentSerializer.from_dict(serialized_agent, agent_lifecycle, openai_wrapper)\n            if agent:\n                agents.append(agent)\n        \n        return agents"}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.exception(f\"An error occurred while saving the agent: {e}\")\n            raise"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        # Clean up the agents by removing those that have been stopped\n        self.cleanup_agents()\n\n        # Return the current list of agents\n        return self.agent_lifecycle.agents"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        # Assuming that all the mentioned attributes are directly accessible as attributes of the agent object\n        agent_dict = {\n            'dynamic_prompt': agent.dynamic_prompt,\n            'purpose': agent.purpose,\n            'purpose_embedding': agent.purpose_embedding.tolist() if isinstance(agent.purpose_embedding, np.ndarray) else agent.purpose_embedding,\n            'depth': agent.depth,\n            'max_depth': agent.max_depth,\n            'usage_count': agent.usage_count,\n            'id': agent.id,\n            'parent_id': agent.parent_id,\n            'working_agent': agent.working_agent,\n            'is_prime': agent.is_prime,\n            'evolve_count': agent.evolve_count,\n            'number_of_code_executions': agent.number_of_code_executions,\n            'last_input': agent.last_input\n        }\n        return agent_dict"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        try:\n            # Combine the engineering system prompt with the specific goal and sample input\n            prompt = PROMPT_ENGINEERING_TEMPLATE.format(\n                system_prompt=PROMPT_ENGINEERING_SYSTEM_PROMPT,\n                prime_prompt=PRIME_PROMPT,\n                examples=EXAMPLES,\n                goal=goal,\n                sample_input=sample_input\n            )\n            # Use the OpenAI wrapper to get a chat completion based on the generated prompt\n            completion = self.openai_wrapper.get_chat_completion(prompt)\n            return completion\n        except Exception as e:\n            logger.exception(f\"Error in generating LLM prompt: {e}\")\n            return \"\""}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        # Create a new MicroAgent instance with the required lifecycle and OpenAI wrapper\n        agent = MicroAgent(agent_lifecycle, openai_wrapper)\n\n        # Set the attributes of the agent from the dictionary\n        agent.dynamic_prompt = data.get(\"dynamic_prompt\")\n        agent.purpose = data.get(\"purpose\")\n        purpose_embedding = data.get(\"purpose_embedding\")\n        if purpose_embedding is not None:\n            agent.purpose_embedding = np.array(purpose_embedding)\n        agent.depth = data.get(\"depth\")\n        agent.max_depth = data.get(\"max_depth\")\n        agent.usage_count = data.get(\"usage_count\")\n        agent.id = data.get(\"id\")\n        agent.parent_id = data.get(\"parent_id\")\n        agent.working_agent = data.get(\"working_agent\")\n        agent.is_prime = data.get(\"is_prime\")\n        agent.evolve_count = data.get(\"evolve_count\")\n        agent.number_of_code_executions = data.get(\"number_of_code_executions\")\n        agent.last_input = data.get(\"last_input\")\n\n        return agent"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "    # Serialize the agent's data as a JSON string\n    agent_data = json.dumps(agent_dict.get('data', {}))\n    \n    # Prepare the data tuple for the SQL query\n    data_tuple = (agent_dict['id'], agent_dict['purpose'], agent_data)\n    \n    # Connect to the SQLite database and execute the INSERT OR REPLACE statement\n    with sqlite3.connect(self.filename) as conn:\n        conn.execute(\"\"\"\n            INSERT OR REPLACE INTO agents (id, purpose, data)\n            VALUES (?, ?, ?)\n        \"\"\", data_tuple)"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.execute(\"SELECT data FROM agents WHERE purpose = ?\", (purpose,))\n            result = cursor.fetchone()\n            if result:\n                return json.loads(result[0])\n            else:\n                return None"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            rows = cursor.fetchall()\n            return [row[0] for row in rows] if rows else []"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "    # Serialize the function name, arguments, and keyword arguments to ensure they can be hashed\n    serialized_data = json.dumps((func_name, args, kwargs), sort_keys=True)\n    # Create a SHA-256 hash object\n    hash_object = hashlib.sha256(serialized_data.encode('utf-8'))\n    # Return the hexadecimal digest of the hash\n    return hash_object.hexdigest()"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash = ?\", (arg_hash,))\n        row = cursor.fetchone()\n        if row is not None:\n            return json.loads(row[0])\n        return None"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Create a hash key from the function's arguments\n            hash_key = hashlib.md5(json.dumps([args, kwargs], sort_keys=True).encode()).hexdigest()\n            \n            # Connect to the SQLite database\n            with sqlite3.connect(filename) as conn:\n                cursor = conn.cursor()\n                # Create table if it doesn't exist\n                cursor.execute(f'''\n                    CREATE TABLE IF NOT EXISTS memoization (\n                        func_name TEXT,\n                        hash_key TEXT,\n                        result TEXT,\n                        PRIMARY KEY (func_name, hash_key)\n                    )\n                ''')\n                # Check if the result is already in the database\n                cursor.execute('''\n                    SELECT result FROM memoization WHERE func_name = ? AND hash_key = ?\n                ''', (func_name, hash_key))\n                row = cursor.fetchone()\n                if row:\n                    # If the result is in the database, return it\n                    return json.loads(row[0])\n                else:\n                    # If the result is not in the database, call the function and store the result\n                    result = func(*args, **kwargs)\n                    cursor.execute('''\n                        INSERT INTO memoization (func_name, hash_key, result) VALUES (?, ?, ?)\n                    ''', (func_name, hash_key, json.dumps(result)))\n                    conn.commit()\n                    return result\n        return wrapper\n    return decorator"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        serialized_result = json.dumps(result)\n        cursor = self.connection.cursor()\n        cursor.execute(\"INSERT OR REPLACE INTO cache (hash, result) VALUES (?, ?)\", (arg_hash, serialized_result))\n        self.connection.commit()"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    for arg in vars(args):\n        setattr(CONFIG, arg, getattr(args, arg))\n\n    # If quiet mode is enabled, redirect the standard output to a file\n    if quiet_mode or args.quiet:\n        if args.record_dir:\n            os.makedirs(args.record_dir, exist_ok=True)\n            log_file_path = os.path.join(args.record_dir, 'execution_log.txt')\n            with open(log_file_path, 'w') as log_file, redirect_stdout(log_file):\n                run_command_line_process(args)\n        else:\n            raise ValueError(\"Quiet mode is enabled but no record directory is provided.\")\n    else:\n        # Run the command line process normally\n        run_command_line_process(args)"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": "            model_name = get_model_name(\n                kwargs.pop(\"model\", CONFIG.default_completion_kwargs[\"model\"])\n            )\n            logger.debug(\"chatcompletion: using \" + model_name)\n            chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n            if \"azure_endpoint\" in chatcompletion_kwargs:\n                api_base = chatcompletion_kwargs.pop(\"azure_endpoint\", None)\n                chatcompletion_kwargs.update({\"api_base\": api_base})\n            chatcompletion_kwargs.update(kwargs)\n\n            try:\n                response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n                response = json.loads(str(response))\n                if response[\"choices\"][0][\"finish_reason\"] == \"length\":\n                    raise BadRequestError(\"maximum context length exceeded\", None)\n            except BadRequestError as e:\n                if \"maximum context length\" in str(e):\n                    fallback_models = [\"gpt-4-32k\", \"gpt-4-1106-preview\", \"gpt-3.5-turbo-16k\"]\n                    for fallback_model in fallback_models:\n                        if fallback_model in CONFIG.api_keys:\n                            model_name = fallback_model\n                            break\n                    else:\n                        raise e\n                    print(\"max context length reached, retrying with \" + model_name)\n                    chatcompletion_kwargs = get_apiconfig_by_model(model_name)\n                    chatcompletion_kwargs.update(kwargs)\n                    chatcompletion_kwargs.pop(\"schema_error_retry\", None)\n\n                    response = openai.ChatCompletion.create(**chatcompletion_kwargs)\n                    response = json.loads(str(response))\n                else:\n                    raise e\n\n            return response"}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "        current_time = time()\n        if self._client is None or (self._last_time is not None and (current_time - self._last_time) > self._refetch_interval):\n            self._create_client()\n            self._last_time = current_time\n        return self._client"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "        if _is_in_dataloader_worker():\n            raise RuntimeError(\"state_dict cannot be called from within a DataLoader worker process.\")\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir_path\": self.input_dir.path if self.input_dir.path else \"\",\n            \"input_dir_url\": self.input_dir.url if self.input_dir.url else \"\",\n            \"item_loader_state\": self.item_loader.state_dict() if self.item_loader else None,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"shuffle\": self.shuffle,\n        }\n\n        return state"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "        # Validate the keys in the state_dict\n        required_keys = {\n            \"num_samples_yielded\",\n            \"num_workers\",\n            \"batch_size\",\n            \"current_epoch\",\n            \"input_dir_path\",\n            \"input_dir_url\",\n            \"item_loader\",\n            \"drop_last\",\n            \"seed\",\n            \"world_size\",\n            \"shuffle\",\n        }\n        if not required_keys.issubset(state_dict.keys()):\n            missing_keys = required_keys - state_dict.keys()\n            raise KeyError(f\"The provided state_dict is missing the following keys: {missing_keys}\")\n\n        # Restore the state of the StreamingDataset\n        self.current_epoch = state_dict[\"current_epoch\"]\n        self.input_dir.path = state_dict[\"input_dir_path\"]\n        self.input_dir.url = state_dict[\"input_dir_url\"]\n        if self.item_loader and \"item_loader\" in state_dict:\n            self.item_loader.load_state_dict(state_dict[\"item_loader\"])\n        self.drop_last = state_dict[\"drop_last\"]\n        self.seed = state_dict[\"seed\"]\n        self.distributed_env.world_size = state_dict[\"world_size\"]\n        self.shuffle = state_dict[\"shuffle\"]\n\n        # Store the state_dict to be used during iteration\n        self._state_dict = state_dict"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        state = self._state_dict\n        if state is None:\n            raise ValueError(\"No state_dict is available to validate.\")\n\n        # Validate shuffle\n        if state[\"shuffle\"] != self.shuffle:\n            raise ValueError(f\"State shuffle ({state['shuffle']}) does not match current shuffle ({self.shuffle}).\")\n\n        # Validate num_workers\n        if state[\"num_workers\"] != self.worker_env.world_size:\n            raise ValueError(f\"State num_workers ({state['num_workers']}) does not match current num_workers ({self.worker_env.world_size}).\")\n\n        # Validate input_dir_path\n        if state[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(f\"State input_dir_path ({state['input_dir_path']}) does not match current input_dir_path ({self.input_dir.path}).\")\n\n        # Validate input_dir_url\n        if state[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(f\"State input_dir_url ({state['input_dir_url']}) does not match current input_dir_url ({self.input_dir.url}).\")\n\n        # Validate seed\n        if state[\"seed\"] != self.seed:\n            raise ValueError(f\"State seed ({state['seed']}) does not match current seed ({self.seed}).\")\n\n        # Validate item_loader state\n        if self.item_loader and state[\"item_loader\"] != self.item_loader.state_dict():\n            raise ValueError(\"State item_loader does not match current item_loader state.\")\n\n        # Validate drop_last\n        if state[\"drop_last\"] != self.drop_last:\n            raise ValueError(f\"State drop_last ({state['drop_last']}) does not match current drop_last ({self.drop_last}).\")\n\n        # Validate world_size\n        if state[\"world_size\"] != self.distributed_env.world_size:\n            raise ValueError(f\"State world_size ({state['world_size']}) does not match current world_size ({self.distributed_env.world_size}).\")"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description=\"Parse command line arguments for the XAgent.\")\n    \n    # Required arguments\n    parser.add_argument('--task', type=str, required=True, help='The task description, specifying what task should be performed.')\n    \n    # Optional arguments\n    parser.add_argument('--upload-files', nargs='*', help='List of files to upload, allowing multiple files to be specified.')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use.')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs.')\n    parser.add_argument('--mode', type=str, default='auto', choices=['auto', 'manual'], help='Operational mode, which can be \"auto\" or \"manual\", specifying how the task should be executed.')\n    parser.add_argument('--quiet', action='store_true', default=False, help='If set, the program runs in quiet mode with minimal output.')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be.')\n    parser.add_argument('--enable-ask-human-for-help', action='store_true', help='Flag to enable asking for human assistance during task execution.')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans.')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be.')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts, specifying how many times a task can be retried upon failure.')\n    parser.add_argument('--config-file', type=str, default=os.getenv('CONFIG_FILE', 'assets/config.yml'), help=\"Path to the configuration file, specifying where to find the configuration settings.\")\n    \n    return parser.parse_args()"}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "    # Use an empty string if input_dir is None\n    input_dir = input_dir or \"\"\n    \n    # Generate a unique hash for the input directory\n    dir_hash = hashlib.md5(input_dir.encode('utf-8')).hexdigest()\n    \n    # Determine the base directory for the cache\n    base_cache_dir = os.getenv('LITDATA_CACHE_DIR', _DEFAULT_CACHE_DIR)\n    \n    # Create the full path for the cache directory\n    cache_dir = os.path.join(base_cache_dir, dir_hash)\n    \n    # Attempt to create the cache directory\n    try:\n        os.makedirs(cache_dir, exist_ok=True)\n        return cache_dir\n    except OSError as e:\n        logger.error(f\"Failed to create cache directory {cache_dir}: {e}\")\n        return None"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    # Define the prefixes that indicate a path should be replaced\n    special_prefixes = [\"http://\", \"https://\", \"s3://\", \"gs://\", \"hdfs://\"]\n\n    # Check if the path is None or an empty string\n    if not path:\n        return True\n\n    # Check if the path starts with any of the special prefixes\n    for prefix in special_prefixes:\n        if path.startswith(prefix):\n            return True\n\n    # If none of the conditions are met, the path should not be replaced\n    return False"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "    # Calculate the total number of batches processed\n    total_batches = num_samples_yielded // batch_size\n\n    # Calculate the number of batches each worker has processed\n    batches_per_worker = total_batches // num_workers\n\n    # Calculate the number of remaining batches after evenly distributing\n    remaining_batches = total_batches % num_workers\n\n    # Initialize the dictionary to store the number of samples processed by each worker\n    samples_per_worker = {}\n\n    # Calculate the number of samples for each worker\n    for worker_idx in range(num_workers):\n        # Each worker has processed batches_per_worker batches\n        samples = batches_per_worker * batch_size\n\n        # Distribute the remaining batches (if any) among the first few workers\n        if worker_idx < remaining_batches:\n            samples += batch_size\n\n        # Store the number of samples processed by this worker\n        samples_per_worker[worker_idx] = samples\n\n    return samples_per_worker"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "        parsed_url = urlparse(remote_filepath)\n        if parsed_url.scheme != 's3':\n            raise ValueError(f\"Invalid S3 URL scheme: {parsed_url.scheme}\")\n\n        # Ensure the local directory exists\n        os.makedirs(os.path.dirname(local_filepath), exist_ok=True)\n\n        # Use a file lock to prevent multiple processes from downloading the same file\n        lock_file = local_filepath + \".lock\"\n        with FileLock(lock_file, timeout=10):\n            # Check if the file already exists\n            if not os.path.exists(local_filepath):\n                if self._s5cmd_available:\n                    # Use s5cmd if available\n                    cmd = f\"s5cmd cp {remote_filepath} {local_filepath}\"\n                    subprocess.run(cmd, shell=True, check=True)\n                else:\n                    # Use the S3Client to download the file\n                    self._client.download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "    # Initialize dictionaries to hold the mapping of workers to their chunks and intervals\n    workers_chunks = {worker: [] for worker in range(num_workers)}\n    workers_intervals = {worker: [] for worker in range(num_workers)}\n\n    # Distribute chunks and intervals to workers\n    for i, (chunk, interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        # Determine the worker index based on the current index and the number of workers\n        worker_index = i % num_workers\n        # Assign the chunk and interval to the appropriate worker\n        workers_chunks[worker_index].append(chunk)\n        workers_intervals[worker_index].append(interval)\n\n    return workers_chunks, workers_intervals"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "        raise NotImplementedError(\"Subclasses should implement this method.\")"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "    chunks_index = {}  # Dictionary to hold the updated chunk index for each worker\n    updated_indexes = {}  # Dictionary to hold the updated indexes within the chunks for each worker\n\n    for worker_idx, intervals in workers_intervals.items():\n        current_index = indexes[worker_idx]  # Get the current index for the worker\n        chunk_idx = 0  # Initialize the chunk index for the worker\n\n        # Iterate through the intervals for the worker\n        for interval in intervals:\n            interval_size = interval[1] - interval[0]  # Calculate the size of the interval\n\n            # If the current index is within the interval size, break the loop\n            if current_index < interval_size:\n                break\n\n            # Update the current index and chunk index\n            current_index -= interval_size\n            chunk_idx += 1\n\n        # Update the dictionaries with the new chunk index and current index\n        chunks_index[worker_idx] = chunk_idx\n        updated_indexes[worker_idx] = current_index\n\n    return chunks_index, updated_indexes"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "        with io.BytesIO() as buffer:\n            # Save the image to the buffer in PNG format\n            item.save(buffer, format='PNG')\n            # Get the bytes data from the buffer\n            image_bytes = buffer.getvalue()\n        return image_bytes, None"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "        if isinstance(item, JpegImageFile) and item.filename and os.path.exists(item.filename):\n            with open(item.filename, 'rb') as f:\n                return f.read(), None\n        elif isinstance(item, Image.Image):\n            with io.BytesIO() as output:\n                item.save(output, format='JPEG')\n                return output.getvalue(), None\n        else:\n            raise TypeError(\"Unsupported image type for JPEG serialization\")"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "        # Unpack the first 12 bytes to get width, height, and mode length\n        ints = np.frombuffer(data[:12], dtype=np.uint32)\n        width, height, mode_length = ints\n\n        # Extract the mode string\n        mode = data[12:12 + mode_length].decode(\"utf-8\")\n\n        # Extract the raw image data\n        raw_data = data[12 + mode_length:]\n\n        # Create an Image object from the raw data\n        image = Image.frombytes(mode, (width, height), raw_data)\n\n        return image"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "        # Read the dtype index from the first 4 bytes and convert it to dtype\n        dtype_idx = np.frombuffer(data[:4], dtype=np.uint32)[0]\n        dtype = _TORCH_DTYPES_MAPPING[dtype_idx]\n\n        # Read the number of dimensions from the next 4 bytes\n        num_dims = np.frombuffer(data[4:8], dtype=np.uint32)[0]\n\n        # Calculate the offset where the dimension sizes end\n        offset = 8 + num_dims * 4\n\n        # Read the dimension sizes\n        shape = np.frombuffer(data[8:offset], dtype=np.uint32)\n\n        # Read the tensor data starting from the offset\n        tensor_data = np.frombuffer(data[offset:], dtype=dtype)\n\n        # Convert the numpy array to a PyTorch tensor with the correct shape\n        tensor = torch.from_numpy(tensor_data).view(*shape)\n\n        return tensor"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "        # Ensure the tensor is on CPU and is contiguous\n        item = item.cpu().contiguous()\n\n        # Get the dtype index from the mapping\n        dtype_index = self._dtype_to_indices[item.dtype]\n\n        # Get the shape of the tensor\n        shape = item.shape\n\n        # Convert the shape to a bytes object\n        shape_bytes = np.array(shape, dtype=np.uint32).tobytes()\n\n        # Convert the dtype index to a bytes object\n        dtype_bytes = np.array([dtype_index], dtype=np.uint8).tobytes()\n\n        # Get the raw data of the tensor as a bytes object\n        data_bytes = item.numpy().tobytes()\n\n        # Concatenate the dtype, shape, and raw data bytes\n        serialized_tensor = dtype_bytes + shape_bytes + data_bytes\n\n        return serialized_tensor, None"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "        if _TORCH_VISION_AVAILABLE:\n            try:\n                # Attempt to decode the JPEG using torchvision\n                image_tensor = decode_jpeg(data)\n                return image_tensor\n            except RuntimeError:\n                # If decoding fails, fall back to PIL\n                pass\n\n        # If torchvision is not available or decoding failed, use PIL\n        image_pil = Image.open(io.BytesIO(data))\n        image_pil.load()\n\n        if _TORCH_VISION_AVAILABLE:\n            # Convert PIL image to PyTorch tensor\n            return pil_to_tensor(image_pil)\n        else:\n            return image_pil"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "        if self._dtype is None:\n            raise ValueError(\"Data format (dtype) for NoHeaderTensorSerializer is not set up.\")\n        if item.dtype != self._dtype:\n            raise ValueError(f\"Tensor dtype {item.dtype} does not match the expected dtype {self._dtype}.\")\n        data_bytes = item.numpy().tobytes(order=\"C\")\n        dtype_index = self._dtype_to_indices[item.dtype]\n        data_format = f\"no_header_tensor:{dtype_index}\"\n        return data_bytes, data_format"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "        if self._dtype is None:\n            raise ValueError(\"Data type for deserialization is not set. Please call the 'setup' method first.\")\n        tensor = torch.frombuffer(data, dtype=self._dtype)\n        return tensor"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "        # Read the data type index from the first 4 bytes and convert it to numpy dtype\n        dtype_index = np.frombuffer(data[:4], np.uint32)[0]\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_index]\n\n        # Read the shape size (number of dimensions) from the next 4 bytes\n        shape_size = np.frombuffer(data[4:8], np.uint32)[0]\n        shape = []\n\n        # Calculate the starting index for the actual array data\n        data_start_idx = 8 + shape_size * 4\n\n        # Read each dimension size and append to the shape list\n        for i in range(shape_size):\n            dim_start_idx = 8 + i * 4\n            dim_end_idx = dim_start_idx + 4\n            dim_size = np.frombuffer(data[dim_start_idx:dim_end_idx], np.uint32)[0]\n            shape.append(dim_size)\n\n        # Convert the shape list to a tuple\n        shape = tuple(shape)\n\n        # Read the actual array data starting from data_start_idx\n        array_data = data[data_start_idx:]\n\n        # Reconstruct the numpy array using the dtype, shape, and array data\n        return np.frombuffer(array_data, dtype=dtype).reshape(shape)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "        assert self._dtype is not None, \"Data type for deserialization is not set.\"\n        return np.frombuffer(data, dtype=self._dtype)"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "        dtype_indice = self._dtype_to_indices[item.dtype]\n        serialized_data = item.tobytes(order=\"C\")\n        dtype_identifier = f\"no_header_numpy:{dtype_indice}\"\n        return serialized_data, dtype_identifier"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "        dtype_index = self._dtype_to_indices[item.dtype.name]\n        dtype_bytes = np.uint32(dtype_index).tobytes()\n        shape_bytes = np.array(item.shape, dtype=np.uint32).tobytes()\n        data_bytes = item.tobytes(order='C')\n        return dtype_bytes + shape_bytes + data_bytes, None"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "        state = {\n            'current_epoch': self.current_epoch,\n            'latest_worker_idx': self._latest_worker_idx,\n            'rng_state': self.rng_state,\n            'restore': self.restore,\n        }\n\n        if isinstance(self.dataset, StreamingDataset):\n            state['num_samples_yielded_streaming'] = self._num_samples_yielded_streaming\n        else:\n            state['num_samples_yielded_combined'] = self._num_samples_yielded_combined\n\n        # Include the state of the dataset if it has a state_dict method\n        if hasattr(self.dataset, 'state_dict'):\n            state['dataset'] = self.dataset.state_dict()\n\n        return state"}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "        if not _TORCH_VISION_AVAILABLE or not _AV_AVAILABLE:\n            raise ImportError(\"Deserializing video requires torchvision and av to be installed.\")\n\n        from torchvision.io import read_video\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False) as tmpfile:\n            tmpfile.write(data)\n            tmpfile_name = tmpfile.name\n\n        try:\n            video, audio, info = read_video(tmpfile_name)\n        finally:\n            os.unlink(tmpfile_name)\n\n        return video, audio, info"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "        if self.filled:\n            return []\n\n        # Write any remaining chunks to files\n        remaining_chunk_paths = []\n        if self._serialized_items:\n            remaining_chunk_path = self.write_chunk(on_done=True)\n            remaining_chunk_paths.append(remaining_chunk_path)\n\n        # Write the chunks index to a JSON file\n        index_file_path = self.write_chunks_index()\n        if index_file_path:\n            remaining_chunk_paths.append(index_file_path)\n\n        # Mark the writing process as complete\n        self._is_done = True\n\n        return remaining_chunk_paths"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "        self.current_epoch = obj[\"current_epoch\"]\n        self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        self.restore = True\n\n        if isinstance(self.dataset, StreamingDataset):\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        elif isinstance(self.dataset, CombinedStreamingDataset):\n            self._num_samples_yielded_combined = obj[\"num_samples_yielded\"]\n            self.dataset.load_state_dict(obj[\"dataset\"])\n        else:\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {type(self.dataset)}.\"\n            )\n\n        # Adjust the worker index iterator to the latest worker index\n        self._worker_idx = cycle(list(range(self.num_workers if self.num_workers > 0 else 1)))\n        self._worker_idx_iter = iter(self._worker_idx)\n        for _ in range(self._latest_worker_idx):\n            next(self._worker_idx_iter)"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "        if self._iterator is None and num_samples_yielded is None:\n            return {}\n\n        state = {\n            __NUM_SAMPLES_YIELDED_KEY__: num_samples_yielded if num_samples_yielded is not None else self._iterator.num_samples_yielded,\n            __SAMPLES_KEY__: [dataset.state_dict() for dataset in self._datasets]\n        }\n\n        return state"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "        # Load the state of each individual dataset\n        for dataset, dataset_state in zip(self._datasets, state_dict[__SAMPLES_KEY__]):\n            dataset.load_state_dict(dataset_state)\n\n        # Load the number of samples yielded if it's available in the state_dict\n        if __NUM_SAMPLES_YIELDED_KEY__ in state_dict:\n            self._num_samples_yielded = state_dict[__NUM_SAMPLES_YIELDED_KEY__]"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if isinstance(dir_path, str):\n        # Check if the path is an S3 URL\n        if dir_path.startswith(\"s3://\"):\n            if not _BOTO3_AVAILABLE:\n                raise ImportError(\"boto3 is required to handle S3 URLs.\")\n            return Dir(url=dir_path)\n\n        # Check if the path is a remote URL\n        parsed_url = parse.urlparse(dir_path)\n        if parsed_url.scheme and parsed_url.netloc:\n            return Dir(url=dir_path)\n\n        # Otherwise, assume it is a local file path\n        return Dir(path=dir_path)\n\n    raise ValueError(f\"Unsupported type for dir_path: {type(dir_path)}\")"}
