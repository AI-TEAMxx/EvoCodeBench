{"namespace": "chat.utils.run_cmd", "completion": ""}
{"namespace": "skfolio.utils.stats.is_cholesky_dec", "completion": "  # noqa: E501\n\n    if x.ndim != 2:\n        raise ValueError(\"x must be a 2d-array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"x must be a square matrix\")\n    try:\n        np.linalg.cholesky(x)\n        return True\n    except np.linalg.LinAlgError:\n        return False\n\n"}
{"namespace": "coord.inv_contract", "completion": " \n  # Clamping to 1 produces correct scale inside |x| < 1\n  z_mag_sq = jnp.maximum(1, jnp.sum(z**2, axis=-1, keepdims=True))\n  scale = (2 * jnp.sqrt(z_mag_sq) - 1) / z_mag_sq\n  x = scale * z\n  return x\n\n"}
{"namespace": "memoize.memoize_to_sqlite", "completion": "    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            # Get the hash of the function call\n            hash_str = func_name + str(args) + str(kwargs)\n            hash = hashlib.sha256(hash_str.encode('utf-8')).hexdigest()\n\n            # Connect to the SQLite database\n            conn = sqlite3.connect(filename)\n            c = conn.cursor()\n\n            # Check if the result is already stored in the database\n            c.execute(\"SELECT * FROM cache WHERE hash=?\", (hash,))\n            result = c.fetchone()\n\n            # If the result is not stored in the database, compute it, store it, and return it\n            if result is None:\n                result = func(*args, **kwargs)\n                c.execute(\"INSERT INTO cache VALUES (?, ?)\", (hash, result))\n                conn.commit()\n                return result\n            else:\n                return result[1]\n\n        return wrapper\n\n    return decorator"}
{"namespace": "iris.io.validators.is_valid_bbox", "completion": "  # noqa: E501\n    if not (values[\"x_min\"] < values[\"x_max\"]):\n        raise ValueError(f\"{cls.__name__}: x_min must be less than x_max. Received {values}\")\n    if not (values[\"y_min\"] < values[\"y_max\"]):\n        raise ValueError(f\"{cls.__name__}: y_min must be less than y_max. Received {values}\")\n\n    return values\n\n"}
{"namespace": "geopoly.compute_sq_dist", "completion": "  if mat1 is None:\n    mat1 = mat0\n\n  # Compute the squared norms of the columns of mat0 and mat1.\n  norm0 = np.linalg.norm(mat0, axis=0)\n  norm1 = np.linalg.norm(mat1, axis=0)\n\n  # Compute the dot products of the columns of mat0 and mat1.\n  dot0 = np.dot(mat0, mat1.T)\n\n  # Compute the squared Euclidean distances.\n  dist = norm0**2 - 2 * dot0 + norm1**2\n\n  # Set negative distances to zero.\n  dist[dist < 0] = 0\n\n  return dist\n\n"}
{"namespace": "litdata.streaming.dataset._should_replace_path", "completion": "    if path is None:\n        return True\n    if not path:\n        return True\n    if path.startswith(\"/cache/\"):\n        return True\n    if path.startswith(\"/tmp/\"):\n        return True\n    if path.startswith(\"/var/\"):\n        return True\n    if path.startswith(\"/var/lib/\"):\n        return True\n    if path.startswith(\"/var/lib/tensorflow/\"):\n        return True\n    if path.startswith(\"/var/lib/caffe2/\"):\n        return True\n    if path.startswith(\"/var/lib/caffe2/caffe2/\"):\n        return True\n    if path.startswith(\"/var/lib/caffe2/caffe2/caffe2/\"):\n        return True\n    if path.startswith(\"/var/lib/caffe2/caffe2/caffe2/caffe2/\"):\n        return True\n    if path.startswith(\"/var/lib/caffe2/caffe2/caffe2/caffe2/caffe2/\"):\n        return True\n    if path.startswith(\"/var/lib/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2/\"):\n        return True\n    if path.startswith(\"/var/lib/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2/\"):\n        return True\n    if path.startswith(\"/var/lib/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2/\"):\n        return True\n    if path.startswith(\"/var/lib/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2/\"):\n        return True\n    if path.startswith(\"/var/lib/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2/\"):\n        return True\n    if path.startswith(\"/var/lib/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2/\"):\n        return True\n    if path.startswith(\"/var/lib/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2/\"):\n        return True\n    if path.startswith(\"/var/lib/caffe2/caffe2/caffe2/caffe2/caffe2/caffe2"}
{"namespace": "skfolio.utils.tools.input_to_array", "completion": "  # noqa: E501\n\n    if dim == 1:\n        if isinstance(items, dict):\n            if assets_names is None:\n                raise ValueError(\n                    f\"Expected 'assets_names' when 'items' is a dictionary, got None.\"\n                )\n            if len(assets_names) != len(items):\n                raise ValueError(\n                    f\"Expected 'items' to have the same number of assets as 'assets_names', got {len(items)} and {len(assets_names)}.\"\n                )\n            items = {k: v for k, v in items.items() if k in assets_names}\n            if len(items) != len(assets_names):\n                raise ValueError(\n                    f\"Expected 'items' to have the same number of assets as 'assets_names', got {len(items)} and {len(assets_names)}.\"\n                )\n            items = np.array([items[k] for k in assets_names])\n        else:\n            items = np.array(items)\n        if items.ndim != 1:\n            raise ValueError(\n                f\"Expected 'items' to have dimension 1, got {items.ndim}.\"\n            )\n        if items.shape[0] != n_assets:\n            raise ValueError(\n                f\"Expected 'items' to have shape (n_assets,), got {items.shape}.\"\n            )\n        return items\n    elif dim == 2:\n        if isinstance(items, dict):\n            if assets_names is None:\n                raise ValueError(\n                    f\"Expected 'assets_names' when 'items' is a dictionary, got None.\"\n                )\n            if len(assets_names) != len(items):\n                raise ValueError(\n                    f\"Expected 'items' to have the same number of assets as 'assets_names', got {len(items)} and {len(assets_names)}.\"\n                )\n            items = {k: v for k, v in items.items() if k in assets_names}\n            if len(items) != len(assets_names):\n                raise ValueError(\n                    f\"Expected 'items' to have the same number of assets as 'assets_names', got {len(items)} and {len(assets_names)}.\"\n                )\n            items = np.array([items[k] for k in assets_names])\n        else:\n            items = np.array(items)\n        if items.ndim != 2:\n            raise ValueError(\n                f\"Expected 'items' to have dimension 2, got {items.ndim}.\"\n            )\n        if items.shape"}
{"namespace": "agent_serializer.AgentSerializer.from_dict", "completion": "        # Initialize the MicroAgent\n        agent = MicroAgent(agent_lifecycle, openai_wrapper)\n\n        # Set the agent's attributes\n        agent.dynamic_prompt = data[\"dynamic_prompt\"]\n        agent.purpose = data[\"purpose\"]\n        agent.purpose_embedding = data[\"purpose_embedding\"]\n        agent.depth = data[\"depth\"]\n        agent.max_depth = data[\"max_depth\"]\n        agent.usage_count = data[\"usage_count\"]\n        agent.id = data[\"id\"]\n        agent.parent_id = data[\"parent_id\"]\n        agent.working_agent = data[\"working_agent\"]\n        agent.is_prime = data[\"is_prime\"]\n        agent.evolve_count = data[\"evolve_count\"]\n        agent.number_of_code_executions = data[\"number_of_code_executions\"]\n        agent.last_input = data[\"last_input\"]\n\n        return agent"}
{"namespace": "image_utils.srgb_to_linear", "completion": "  # pylint: disable=line-too-long\n  if eps is None:\n    eps = xnp.finfo(xnp.float32).eps\n  srgb0 = 323 / 25 * srgb\n  srgb1 = (211 * xnp.maximum(eps, srgb) ** (5 / 12) - 11) / 200\n  return xnp.where(srgb <= 0.0031308, srgb0, srgb1)\n\n"}
{"namespace": "camera_utils.safe_interpolate_1d", "completion": "  # Check input arguments\n  chex.assert_rank(x, 1)\n  chex.assert_rank(t_input, 1)\n  chex.assert_rank(t_output, 1)\n  chex.assert_shape(x, (-1,))\n  chex.assert_shape(t_input, (-1,))\n  chex.assert_shape(t_output, (-1,))\n  chex.assert_shape(t_output, t_input)\n\n  # Check that the input times are sorted\n  t_input = np.sort(t_input)\n  t_output = np.sort(t_output)\n\n  # Check that the input times are unique\n  if len(t_input) != len(np.unique(t_input)):\n    raise ValueError('Input times are not unique')\n\n  # Check that the output times are unique\n  if len(t_output) != len(np.unique(t_output)):\n    raise ValueError('Output times are not unique')\n\n  # Check that the input and output times are in the same order\n  if not np.all(np.sort(t_input) == t_input):\n    raise ValueError('Input times are not sorted')\n  if not np.all(np.sort(t_output) == t_output):\n    raise ValueError('Output times are not sorted')\n\n  # Check that the input and output times are monotonically increasing\n  if not np.all(np.diff(t_input) >= 0):\n    raise ValueError('Input times are not monotonically increasing')\n  if not np.all(np.diff(t_output) >= 0):\n    raise ValueError('Output times are not monotonically increasing')\n\n  # Check that the input and output times are in the same order\n  if not np.all(np.sort(t_input) == t_input):\n    raise ValueError('Input times are not sorted')\n  if not np.all(np.sort(t_output) == t_output):\n    raise ValueError('Output times are not sorted')\n\n  # Check that the input and output times are monotonically increasing\n  if not np.all(np.diff(t_input) >= 0):\n    raise ValueError('Input times are not monotonically increasing')\n  if not np.all(np.diff(t_output) >= 0):\n    raise ValueError('Output times are not monotonically increasing')\n\n  # Check that the input and output times are in the same order\n  if not np.all(np.sort(t_input) == t_input):\n    raise ValueError('Input times are not sorted')\n  if not np.all(np.sort(t_output) == t_output):\n    raise ValueError('Output times are"}
{"namespace": "nlm_ingestor.ingestor.formatter.fix_mixedcase_words", "completion": "    # if all upper\n    if word.isupper():\n        return word\n    # if all lower\n    if word.islower():\n        return word\n    # if mixed case\n    if word[0].isupper() and word[1].islower():\n        return word[0].upper() + word[1:]\n    if word[0].islower() and word[1].isupper():\n        return word[0].lower() + word[1:]\n    if word[0].isupper() and word[1].isupper():\n        return word[0].upper() + word[1:]\n    if word[0].islower() and word[1].islower():\n        return word[0].lower() + word[1:]\n    if word[0].isupper() and word[1].islower() and word[2].islower():\n        return word[0].upper() + word[1] + word[2]\n    if word[0].islower() and word[1].isupper() and word[2].islower():\n        return word[0].lower() + word[1] + word[2]\n    if word[0].isupper() and word[1].isupper() and word[2].islower():\n        return word[0].upper() + word[1] + word[2]\n    if word[0].islower() and word[1].isupper() and word[2].isupper():\n        return word[0].lower() + word[1] + word[2]\n    if word[0].isupper() and word[1].isupper() and word[2].isupper():\n        return word[0].upper() + word[1] + word[2]\n    if word[0].islower() and word[1].islower() and word[2].isupper():\n        return word[0].lower() + word[1] + word[2]\n    if word[0].isupper() and word[1].islower() and word[2].isupper():\n        return word[0].upper() + word[1] + word[2]\n    if word[0].isupper() and word[1].isupper() and word[2].isupper():\n        return word[0].upper() + word[1] + word"}
{"namespace": "iris.io.validators.is_binary", "completion": "  # noqa: E501\n    if not np.issubdtype(v.dtype, np.bool_):\n        raise ValueError(\n            f\"{cls.__name__}: {field.name} must be a binary array (i.e., contain only True or False values).\"\n        )\n\n    return v\n\n"}
{"namespace": "coord.contract3_isoscale", "completion": "  # Compute the norm of the input array.\n  norm = np.linalg.norm(x, axis=-1, keepdims=True)\n\n  # Compute the scaling factor.\n  scale = 2 * np.sqrt(norm) - norm\n\n  # Apply the scaling factor to the input array.\n  return x * scale\n\n"}
{"namespace": "autorag.utils.util.load_summary_file", "completion": "    if dict_columns is None:\n        dict_columns = ['module_params']\n\n    summary_df = pd.read_csv(summary_path, sep='\\t', header=None, names=['doc_id', 'contents', 'summary', 'summary_id', 'summary_type', 'summary_score', 'summary_score_type', 'summary_score_value', 'summary_score_unit', 'summary_score_source', 'summary_score_source_id', 'summary_score_source_type', 'summary_score_source_value', 'summary_score_source_unit', 'summary_score_source_source', 'summary_score_source_source_id', 'summary_score_source_source_type', 'summary_score_source_source_value', 'summary_score_source_source_unit', 'summary_score_source_source_source', 'summary_score_source_source_source_id', 'summary_score_source_source_source_type', 'summary_score_source_source_source_value', 'summary_score_source_source_source_unit', 'summary_score_source_source_source_source', 'summary_score_source_source_source_source_id', 'summary_score_source_source_source_source_type', 'summary_score_source_source_source_source_value', 'summary_score_source_source_source_source_unit', 'summary_score_source_source_source_source_source', 'summary_score_source_source_source_source_source_id', 'summary_score_source_source_source_source_source_type', 'summary_score_source_source_source_source_source_value', 'summary_score_source_source_source_source_source_unit', 'summary_score_source_source_source_source_source_source', 'summary_score_source_source_source_source_source_source_id', 'summary_score_source_source_source_source_source_source_type', 'summary_score_source_source_source_source_source_source_value', 'summary_score_source_source_source_source_source_source_unit', 'summary_score_source_source_source_source_source_source_source', 'summary_score_source_source_source_source_source_source_source_id', 'summary_score_source_source_source_source_source_source_source_type', 'summary_score_source_source_source_source_source_source_source_value', 'summary_score_source_source_source_source_source_source_source_unit', 'summary_score_source_source_source_source_source_source_source_source', 'summary_score_source_source_source_source_source_source_source_source_id', 'summary_score_source_source_source_source_source_source_source_source_type', 'summary_score_source_source_source_source_source_source_source_source_value', 'summary_score_source_source_source_source_source_source_source_source_unit', 'summary_score_source_source_source_source_source_source_source_source_source', 'summary_score_source_source_source_source_source_source_source_source_source_id', 'summary_score_source_source_source_source_source_source_source_source_source_type', 'summary_score_source_source_source_source_source_source_source_source_source_value', 'summary"}
{"namespace": "coord.isotropize", "completion": "  if mode == 'fast':\n    # Compute the isotropic covariance matrix using the determinant.\n    det = jnp.linalg.det(cov)\n    if det == 0:\n      raise ValueError('Det is zero, cannot isotropize.')\n    return cov / det\n  elif mode == 'accurate':\n    # Compute the isotropic covariance matrix using the logarithm of the determinant.\n    log_det = jnp.log(jnp.linalg.det(cov))\n    if jnp.isinf(log_det):\n      raise ValueError('Det is infinite, cannot isotropize.')\n    return cov * jnp.exp(-log_det)\n  else:\n    raise ValueError('Invalid mode: ' + mode)\n\n"}
{"namespace": "run.parse_args", "completion": "    parser = argparse.ArgumentParser(description='XAgent')\n\n    parser.add_argument('--task', type=str, required=True, help='The task description, specifying what task should be performed.')\n    parser.add_argument('--upload-files', type=str, nargs='+', help='List of files to upload, allowing multiple files to be specified.')\n    parser.add_argument('--model', type=str, help='Model identifier for the task, specifying which model to use.')\n    parser.add_argument('--record-dir', type=str, help='Directory to record task execution logs, specifying where to save the logs.')\n    parser.add_argument('--mode', type=str, default='auto', help='Operational mode, which can be \\'auto\\' or \\'manual\\', specifying how the task should be executed.')\n    parser.add_argument('--quiet', type=bool, default=False, help='If set, the program runs in quiet mode with minimal output.')\n    parser.add_argument('--max-subtask-chain-length', type=int, help='Maximum length of subtask chain, specifying how long a subtask chain can be.')\n    parser.add_argument('--enable-ask-human-for-help', type=bool, help='Flag to enable asking for human assistance during task execution.')\n    parser.add_argument('--max-plan-refine-chain-length', type=int, help='Maximum length of plan refinement chain, specifying the limit for refining plans.')\n    parser.add_argument('--max-plan-tree-depth', type=int, help='Maximum depth of the plan tree, specifying how deep the plan tree can be.')\n    parser.add_argument('--max-plan-tree-width', type=int, help='Maximum width of the plan tree, specifying the maximum number of branches at any level of the tree.')\n    parser.add_argument('--max-retry-times', type=int, help='Maximum number of retry attempts, specifying how many times a task can be retried upon failure.')\n    parser.add_argument('--config-file', type=str, default=os.getenv('CONFIG_FILE', 'assets/config.yml'), help='Path to the configuration file, specifying where to find the configuration settings.')\n\n    args = parser.parse_args()\n\n    return args\n\n"}
{"namespace": "iris.io.validators.is_list_of_points", "completion": "  # noqa: E501\n    if v.shape != (None, 2):\n        raise ValueError(f\"{cls.__name__}: {field.name} must be a list of 2D points. got shape {v.shape}\")\n\n    return v\n\n"}
{"namespace": "tanuki.utils.encode_int", "completion": "    # Define the character set for encoding\n    char_set = string.ascii_lowercase + string.digits + \"_\"\n    # Define the length of the character set\n    char_set_len = len(char_set)\n    # Define the maximum value of the integer\n    max_int = 2**31 - 1\n    # Define the minimum value of the integer\n    min_int = -2**31\n    # Define the range of the integer\n    range_int = max_int - min_int + 1\n    # Define the number of characters in the encoded string\n    num_chars = int(range_int / char_set_len)\n    # Define the encoded string\n    encoded_str = \"\"\n    # Define the index of the character set\n    index = 0\n    # Define the integer to be encoded\n    n = n % range_int\n    # Define the loop to iterate over the integer\n    while n > 0:\n        # Define the character to be added to the encoded string\n        char = char_set[index]\n        # Add the character to the encoded string\n        encoded_str += char\n        # Increment the index of the character set\n        index += 1\n        # Define the integer to be encoded\n        n = n // char_set_len\n    # Return the encoded string\n    return encoded_str\n\n"}
{"namespace": "spin_math.safe_log", "completion": "  safe_x = jnp.where(x > eps, x, jnp.full_like(x, value_at_zero))\n  return jnp.log(safe_x)\n\n"}
{"namespace": "litdata.streaming.dataset._replay_chunks_sampling", "completion": "  # noqa: E501\n    chunk_indexes = {}\n    for worker_idx in range(len(workers_intervals)):\n        chunk_indexes[worker_idx] = 0\n        interval = workers_intervals[worker_idx]\n        current_index = indexes[worker_idx]\n        chunk_size = interval[1] - interval[0]\n        while current_index < interval[1]:\n            chunk_indexes[worker_idx] += 1\n            current_index += chunk_size\n    return chunk_indexes, indexes"}
{"namespace": "grid_utils.trilerp", "completion": "  if datastructure == 'grid':\n    return grid_trilerp(values, coordinates)\n  elif datastructure == 'hash':\n    return hash_trilerp(values, coordinates)\n  else:\n    raise ValueError('Invalid datastructure. Only \"grid\" and \"hash\" are supported.')\n\n"}
{"namespace": "geopoly.compute_tesselation_weights", "completion": "  # Compute the barycentric weights for each vertex of the triangle.\n  # The weights are normalized to sum to 1.\n  # The weights are computed by dividing the area of the triangle by the area of the sub-triangle.\n  # The area of the sub-triangle is computed by multiplying the area of the original triangle by the tessellation factor.\n  # The area of the original triangle is computed by multiplying the length of the base by the height.\n  # The height is computed by taking the difference between the y-coordinates of the two vertices.\n  # The base is computed by taking the difference between the x-coordinates of the two vertices.\n  # The area of the sub-triangle is computed by multiplying the base by the height.\n  # The area of the original triangle is computed by multiplying the area of the sub-triangle by the tessellation factor.\n  # The area of the original triangle is computed by multiplying the length of the base by the height.\n  # The height is computed by taking the difference between the y-coordinates of the two vertices.\n  # The base is computed by taking the difference between the x-coordinates of the two vertices.\n  # The area of the sub-triangle is computed by multiplying the base by the height.\n  # The area of the original triangle is computed by multiplying the area of the sub-triangle by the tessellation factor.\n  # The area of the original triangle is computed by multiplying the length of the base by the height.\n  # The height is computed by taking the difference between the y-coordinates of the two vertices.\n  # The base is computed by taking the difference between the x-coordinates of the two vertices.\n  # The area of the sub-triangle is computed by multiplying the base by the height.\n  # The area of the original triangle is computed by multiplying the area of the sub-triangle by the tessellation factor.\n  # The area of the original triangle is computed by multiplying the length of the base by the height.\n  # The height is computed by taking the difference between the y-coordinates of the two vertices.\n  # The base is computed by taking the difference between the x-coordinates of the two vertices.\n  # The area of the sub-triangle is computed by multiplying the base by the height.\n  # The area of the original triangle is computed by multiplying the area of the sub-triangle by the tessellation factor.\n  # The area"}
{"namespace": "linspline.query", "completion": "  # Check that the spline is valid\n  checkify.check(jnp.all(tq >= 0), 'Query points must be non-negative.')\n  checkify.check(jnp.all(tq <= 1), 'Query points must be in [0, 1].')\n  checkify.check(jnp.all(t >= 0), 'Time points must be non-negative.')\n  checkify.check(jnp.all(t <= 1), 'Time points must be in [0, 1].')\n  checkify.check(jnp.all(t[1:] - t[:-1] > 0), 'Time points must be strictly increasing.')\n  checkify.check(jnp.all(v[1:] - v[:-1] > 0), 'Values must be strictly increasing.')\n  checkify.check(jnp.all(t[0] == 0), 'Time points must start at 0.')\n  checkify.check(jnp.all(t[-1] == 1), 'Time points must end at 1.')\n  checkify.check(jnp.all(v[0] == 0), 'Values must start at 0.')\n  checkify.check(jnp.all(v[-1] == 0), 'Values must end at 0.')\n\n  # Interpolate the spline\n  return jnp.interp(tq, t, v)\n\n"}
{"namespace": "iris.io.validators.are_all_positive", "completion": "  # noqa: E501\n    if isinstance(v, (int, float)):\n        if v <= 0:\n            raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    elif isinstance(v, Iterable):\n        for i in v:\n            if i <= 0:\n                raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n    else:\n        raise ValueError(f\"{cls.__name__}: {field.name} must be positive.\")\n\n    return v\n\n"}
{"namespace": "camera_utils.convert_to_ndc", "completion": "  # Calculate the perspective projection matrix.\n  # The perspective projection matrix is defined as:\n  #   M = K * inv(R) * inv(T)\n  # where K is the intrinsic matrix, R is the extrinsic matrix, and T is the\n  # translation vector.\n  # In this case, we assume an identity extrinsic matrix (R = I) and a translation\n  # vector of (0, 0, -near), so the perspective projection matrix is:\n  #   M = K * inv(T)\n  # The perspective projection matrix is used to transform the ray origins and\n  # directions from world space to NDC.\n  # The perspective projection matrix is defined as:\n  #   M = K * inv(T)\n  # where K is the intrinsic matrix, R is the extrinsic matrix, and T is the\n  # translation vector.\n  # In this case, we assume an identity extrinsic matrix (R = I) and a translation\n  # vector of (0, 0, -near), so the perspective projection matrix is:\n  #   M = K * inv(T)\n  # The perspective projection matrix is used to transform the ray origins and\n  # directions from world space to NDC.\n  # The perspective projection matrix is defined as:\n  #   M = K * inv(T)\n  # where K is the intrinsic matrix, R is the extrinsic matrix, and T is the\n  # translation vector.\n  # In this case, we assume an identity extrinsic matrix (R = I) and a translation\n  # vector of (0, 0, -near), so the perspective projection matrix is:\n  #   M = K * inv(T)\n  # The perspective projection matrix is used to transform the ray origins and\n  # directions from world space to NDC.\n  # The perspective projection matrix is defined as:\n  #   M = K * inv(T)\n  # where K is the intrinsic matrix, R is the extrinsic matrix, and T is the\n  # translation vector.\n  # In this case, we assume an identity extrinsic matrix (R = I) and a translation\n  # vector of (0, 0, -near), so the perspective projection matrix is:\n  #   M = K * inv(T)\n  # The perspective projection matrix is used to transform the ray origins and\n  # directions from world"}
{"namespace": "geometry.are_lines_parallel", "completion": "  return jnp.allclose(jnp.dot(dir1, dir2), 1.0, atol=1e-6)\n\n"}
{"namespace": "common.bleu4_score", "completion": "    # Tokenize the input texts\n    continuation_tokens = jieba.lcut(continuation)\n    reference_tokens = jieba.lcut(reference)\n\n    # Calculate the BLEU score\n    bleu_score = evaluate.bleu_score(continuation_tokens, reference_tokens)\n\n    # Include the brevity penalty if requested\n    if with_penalty:\n        bleu_score = evaluate.brevity_penalty(bleu_score, len(continuation_tokens), len(reference_tokens))\n\n    return bleu_score\n\n"}
{"namespace": "spin_math.safe_sqrt", "completion": "  if x < eps:\n    return value_at_zero\n  else:\n    return jnp.sqrt(x)\n\n"}
{"namespace": "stepfun.weight_to_pdf", "completion": "  # Check input arguments\n  utils.assert_valid_stepfun(t, w)\n  # Check that the weights sum to 1\n  utils.assert_sum_to_one(w)\n  # Check that the weights are non-negative\n  utils.assert_non_negative(w)\n  # Check that the weights are sorted\n  utils.assert_sorted(w)\n  # Check that the weights are not all zero\n  utils.assert_non_zero(w)\n  # Check that the weights are not all the same\n  utils.assert_not_all_equal(w)\n  # Check that the weights are not all positive\n  utils.assert_not_all_positive(w)\n  # Check that the weights are not all negative\n  utils.assert_not_all_negative(w)\n  # Check that the weights are not all zero\n  utils.assert_not_all_zero(w)\n  # Check that the weights are not all the same\n  utils.assert_not_all_equal(w)\n  # Check that the weights are not all positive\n  utils.assert_not_all_positive(w)\n  # Check that the weights are not all negative\n  utils.assert_not_all_negative(w)\n  # Check that the weights are not all zero\n  utils.assert_not_all_zero(w)\n  # Check that the weights are not all the same\n  utils.assert_not_all_equal(w)\n  # Check that the weights are not all positive\n  utils.assert_not_all_positive(w)\n  # Check that the weights are not all negative\n  utils.assert_not_all_negative(w)\n  # Check that the weights are not all zero\n  utils.assert_not_all_zero(w)\n  # Check that the weights are not all the same\n  utils.assert_not_all_equal(w)\n  # Check that the weights are not all positive\n  utils.assert_not_all_positive(w)\n  # Check that the weights are not all negative\n  utils.assert_not_all_negative(w)\n  # Check that the weights are not all zero\n  utils.assert_not_all_zero(w)\n  # Check that the weights are not all the same\n  utils.assert_not_all_equal(w)\n  # Check that the weights are not all positive\n  utils.assert_not_all_positive(w)\n  # Check that the weights are not all negative\n  utils.assert_not_all_negative(w)\n  # Check that the weights are not all zero\n  utils.assert_not_all_zero(w)\n  # Check that the weights are not all the same\n  utils.assert_not_all_equal(w)\n  # Check that the weights are not all"}
{"namespace": "litdata.streaming.reader._get_folder_size", "completion": "  # noqa: E501\n\n    total_size = 0\n    for dirpath, _, filenames in os.walk(path):\n        for f in filenames:\n            fp = os.path.join(dirpath, f)\n            total_size += os.path.getsize(fp)\n    return total_size\n\n"}
{"namespace": "mmdet3d.core.bbox.structures.utils.limit_period", "completion": "    if isinstance(val, torch.Tensor):\n        val = val.detach().cpu().numpy()\n    elif isinstance(val, np.ndarray):\n        pass\n    else:\n        raise TypeError('Unsupported type of input value: {}'.format(type(val)))\n\n    val = np.clip(val, -offset * period, (1 - offset) * period)\n    return val"}
{"namespace": "agent_serializer.AgentSerializer.to_dict", "completion": "        agent_dict = {\n            \"id\": agent.id,\n            \"parent_id\": agent.parent_id,\n            \"working_agent\": agent.working_agent,\n            \"is_prime\": agent.is_prime,\n            \"evolve_count\": agent.evolve_count,\n            \"number_of_code_executions\": agent.number_of_code_executions,\n            \"last_input\": agent.last_input,\n            \"depth\": agent.depth,\n            \"max_depth\": agent.max_depth,\n            \"usage_count\": agent.usage_count,\n            \"purpose\": agent.purpose,\n            \"purpose_embedding\": agent.purpose_embedding.tolist() if agent.purpose_embedding is not None else None\n        }\n\n        return agent_dict\n"}
{"namespace": "litdata.utilities.packing._pack_greedily", "completion": "    # Initialize the dictionaries that will be returned\n    bins = defaultdict(list)\n    bin_weights = defaultdict(int)\n\n    # Sort the items by weight in descending order\n    items = sorted(items, key=lambda x: weights[x], reverse=True)\n\n    # Iterate over the items and place them in the bins\n    for item in items:\n        # Find the bin with the lowest total weight\n        min_bin = 0\n        min_weight = bin_weights[min_bin]\n        for bin_index in range(1, num_bins):\n            if bin_weights[bin_index] < min_weight:\n                min_bin = bin_index\n                min_weight = bin_weights[bin_index]\n\n        # Add the item to the bin with the lowest total weight\n        bins[min_bin].append(item)\n        bin_weights[min_bin] += weights[item]\n\n    return bins, bin_weights\n\n"}
{"namespace": "memoize.SQLiteMemoization._compute_hash", "completion": "        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO: Add support for other hashing algorithms\n        # TODO:"}
{"namespace": "iris.utils.math.polygon_length", "completion": "  # noqa: E501\n\n    if len(polygon.shape) != 2 or polygon.shape[1] != 2:\n        raise ValueError(f\"Unable to determine the length of a polygon with shape {polygon.shape}. Expecting (_, 2).\")\n\n    xs, ys = polygon.T\n    xs = np.array(xs)\n    ys = np.array(ys)\n\n    # Compute the distance between each point and the next one\n    distances = np.sqrt((xs[1:] - xs[:-1]) ** 2 + (ys[1:] - ys[:-1]) ** 2)\n\n    # Compute the maximum distance between two consecutive points\n    max_distance = np.max(distances)\n\n    # Compute the total length of the polygon, considering only the distances between consecutive points that are below the specified maximum distance\n    total_length = np.sum(distances[distances <= max_distance])\n\n    return float(total_length)"}
{"namespace": "iris.nodes.vectorization.contouring.filter_polygon_areas", "completion": "  # noqa: E501\n    if rel_tr < 0 or rel_tr > 1:\n        raise VectorizationError(\"Relative threshold must be between 0 and 1.\")\n    if abs_tr < 0:\n        raise VectorizationError(\"Absolute threshold must be non-negative.\")\n\n    # Get the largest polygon's area\n    largest_area = max([area(p) for p in polygons])\n\n    # Filter out polygons with an area smaller than the absolute threshold\n    if abs_tr > 0:\n        polygons = [p for p in polygons if area(p) >= abs_tr]\n\n    # Filter out polygons with an area smaller than the relative threshold\n    if rel_tr > 0:\n        polygons = [p for p in polygons if area(p) >= rel_tr * largest_area]\n\n    return polygons\n\n"}
{"namespace": "litdata.streaming.dataset._replay_sampling", "completion": "  # noqa: E501\n    assert num_samples_yielded >= 0\n    assert batch_size > 0\n    assert num_workers > 0\n\n    # Calculate the number of samples each worker should process.\n    num_samples_per_worker = num_samples_yielded // num_workers\n    num_samples_remaining = num_samples_yielded % num_workers\n\n    # Calculate the number of samples each worker should process in each batch.\n    num_samples_per_batch = num_samples_per_worker // batch_size\n    num_samples_per_batch_remaining = num_samples_per_worker % batch_size\n\n    # Calculate the number of batches each worker should process.\n    num_batches_per_worker = num_samples_per_worker // batch_size\n\n    # Calculate the number of batches each worker should process in each batch.\n    num_batches_per_batch_remaining = num_samples_per_batch_remaining // batch_size\n\n    # Calculate the number of batches each worker should process in each batch.\n    num_batches_per_batch_remaining = num_samples_per_batch_remaining % batch_size\n\n    # Calculate the number of batches each worker should process in each batch.\n    num_batches_per_batch_remaining = num_samples_per_batch_remaining % batch_size\n\n    # Calculate the number of batches each worker should process in each batch.\n    num_batches_per_batch_remaining = num_samples_per_batch_remaining % batch_size\n\n    # Calculate the number of batches each worker should process in each batch.\n    num_batches_per_batch_remaining = num_samples_per_batch_remaining % batch_size\n\n    # Calculate the number of batches each worker should process in each batch.\n    num_batches_per_batch_remaining = num_samples_per_batch_remaining % batch_size\n\n    # Calculate the number of batches each worker should process in each batch.\n    num_batches_per_batch_remaining = num_samples_per_batch_remaining % batch_size\n\n    # Calculate the number of batches each worker should process in each batch.\n    num_batches_per_batch_remaining = num_samples_per_batch_remaining % batch_size\n\n    # Calculate the number of batches each worker should process in each batch.\n    num_batches_per_batch_remaining = num_samples_per_batch_remaining % batch_size\n\n    # Calculate the number of batches each worker should process in each batch.\n    num_batches_per_batch_remaining = num_samples_per_batch_remaining % batch_size\n\n    # Calculate the number of batches each worker should process in each batch.\n    num_batches_per_batch_remaining = num_samples_per_batch_remaining % batch_size\n\n    # Calculate the number of batches each worker should process in each batch."}
{"namespace": "autorag.strategy.filter_by_threshold", "completion": "@avoid_empty_result([0, 3])\ndef filter_by_threshold_with_index(results, value, threshold, metadatas=None) -> Tuple[List, List]:\n\n    \"\"\"\n    Filters the given lists of results and values by a specified threshold, returning the filtered results and their corresponding metadata. It ensures that only those results (and their metadata) where the associated value is less than or equal to the threshold are returned.\n\n    Input-Output Arguments\n    :param results: List. The list of results that need to be filtered based on the threshold.\n    :param value: List. The list of values corresponding to each result. This list is used to determine whether a result should be filtered based on the threshold. It must have the same length as the results list.\n    :param threshold: Numeric. The threshold value used to filter the results and values. Only items with a value less than or equal to this threshold will be included in the output.\n    :param metadatas: List, optional. A list containing metadata for each result. If not provided, a list of None values will be used instead. This list should have the same length as the results list.\n    :return: A tuple containing two lists: the filtered list of results and the filtered list of metadatas. If no results meet the threshold criteria, empty lists are returned.\n    :rtype: Tuple[List, List]. The first list in the tuple contains the filtered results, and the second list contains the corresponding filtered metadata.\n    \"\"\"\n\n\n@avoid_empty_result([0, 3])\ndef filter_by_threshold_with_index_and_value(results, value, threshold, metadatas=None) -> Tuple[List, List]:\n\n    \"\"\"\n    Filters the given lists of results and values by a specified threshold, returning the filtered results and their corresponding metadata. It ensures that only those results (and their metadata) where the associated value is less than or equal to the threshold are returned.\n\n    Input-Output Arguments\n    :param results: List. The list of results that need to be filtered based on the threshold.\n    :param value: List. The list of values corresponding to each result. This list is used to determine whether a result should be filtered based on the threshold. It must have the same length as the results list.\n    :param threshold: Numeric. The threshold value used to filter the results and values. Only items with a value less than or equal to this threshold will be included in the"}
{"namespace": "iris.utils.math.area", "completion": "  # noqa: E501\n\n    if array.shape[1] != 2:\n        raise ValueError(\"The input array must have the shape (_, 2), where _ can be any number of points.\")\n\n    return 0.5 * np.abs(np.dot(array[:, 0], np.roll(array[:, 1], 1)) - np.dot(array[:, 1], np.roll(array[:, 0], 1)))\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.searchsorted", "completion": "    # Compute the indices where v should be inserted into a to maintain order.\n    # This is done by finding the indices where the difference between v and a is\n    # less than or equal to zero. This is done by using the fact that the\n    # difference between v and a is monotonic increasing, and therefore the\n    # difference between v and a can be computed by taking the cumulative sum\n    # of the difference between v and a.\n    # The cumulative sum of the difference between v and a is computed by\n    # subtracting the cumulative sum of v from the cumulative sum of a. The\n    # cumulative sum of v is computed by taking the cumulative sum of v along\n    # the last dimension. The cumulative sum of a is computed by taking the\n    # cumulative sum of a along the last dimension, and then subtracting the\n    # cumulative sum of a along the last dimension from the cumulative sum of a\n    # along the last dimension.\n    # The cumulative sum of a along the last dimension is computed by taking\n    # the cumulative sum of a along the last dimension, and then subtracting\n    # the cumulative sum of a along the last dimension from the cumulative sum\n    # of a along the last dimension.\n    # The cumulative sum of v along the last dimension is computed by taking\n    # the cumulative sum of v along the last dimension, and then subtracting\n    # the cumulative sum of v along the last dimension from the cumulative sum\n    # of v along the last dimension.\n    # The cumulative sum of a along the last dimension is computed by taking\n    # the cumulative sum of a along the last dimension, and then subtracting\n    # the cumulative sum of a along the last dimension from the cumulative sum\n    # of a along the last dimension.\n    # The cumulative sum of v along the last dimension is computed by taking\n    # the cumulative sum of v along the last dimension, and then subtracting\n    # the cumulative sum of v along the last dimension from the cumulative sum\n    # of v along the last dimension.\n    # The cumulative sum of a along the last dimension is computed by taking\n    # the cumulative sum of a along the last dimension, and then subtracting\n    # the cumulative sum of a along the last dimension from the cumulative sum\n    # of a along the last dimension.\n    # The cumulative sum"}
{"namespace": "camera_utils.intrinsic_matrix", "completion": "  return xnp.array([\n      [fx, 0, cx],\n      [0, fy, cy],\n      [0, 0, 1]\n  ])\n\n"}
{"namespace": "coord.contract", "completion": "  # Calculate the magnitude squared of the points\n  x_mag = jnp.sum(x**2, axis=-1)\n\n  # Calculate the scaling factor\n  scaling_factor = 1 / (1 + x_mag)\n\n  # Scale the points\n  return x * scaling_factor\n\n"}
{"namespace": "litdata.utilities.format._human_readable_bytes", "completion": "  # noqa: E501\n    if num_bytes < 1000:\n        return f\"{num_bytes} B\"\n    for suffix in _FORMAT_TO_RATIO:\n        if num_bytes < _FORMAT_TO_RATIO[suffix]:\n            return f\"{num_bytes / _FORMAT_TO_RATIO[suffix]:.2f} {suffix}\"\n    return f\"{num_bytes / _FORMAT_TO_RATIO['pb']:.2f} PB\""}
{"namespace": "iris.io.validators.is_array_n_dimensions", "completion": "  # noqa: E501\n\n    def validator(cls, v, field):\n        if len(v.shape) != nb_dimensions:\n            raise ValueError(\n                f\"{cls.__name__}: {field.name} must have {nb_dimensions} dimensions.\"\n            )\n\n        return v\n\n    return validator\n\n"}
{"namespace": "geometry.cartesian_to_spherical", "completion": "  # pylint: disable=line-too-long\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable=invalid-name\n  # pylint: disable"}
{"namespace": "common.rougeL_score", "completion": "    rouge = Similarity()\n    rouge.set_model('rougeL')\n    rouge.set_reference(reference)\n    rouge.set_hypothesis(continuation)\n    rouge.set_tokenizer(jieba.cut)\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs')\n    rouge.set_score_type('lcs"}
{"namespace": "detectron2.utils.registry.locate", "completion": "  # noqa: E501\n    try:\n        return pydoc.locate(name)\n    except ImportError:\n        pass\n\n    # Fallback method: try to import the module and locate the object\n    # using the module's `__getattr__` method.\n    try:\n        module = __import__(name, fromlist=[name])\n        return getattr(module, name)\n    except (ImportError, AttributeError):\n        raise ImportError(f\"Cannot locate object {name}\")"}
{"namespace": "detectron2.utils.testing.reload_script_model", "completion": ""}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.hybrid_cc", "completion": "    # Check if the input arguments are valid\n    if len(ids) != len(scores):\n        raise ValueError(\"The length of the ids and scores tuples must be equal.\")\n    if len(weights) != len(ids):\n        raise ValueError(\"The length of the weights tuple must be equal to the length of the ids and scores tuples.\")\n\n    # Normalize the scores\n    scores = [(score / sum(weights)) for score in scores]\n\n    # Combine the scores\n    fused_scores = [sum(score) for score in zip(*scores)]\n\n    # Select the top_k results\n    fused_ids = [ids[i] for i in range(len(fused_scores)) if fused_scores[i] >= fused_scores[i + 1]]\n\n    return fused_ids, fused_scores"}
{"namespace": "skfolio.utils.tools.format_measure", "completion": "  # noqa: E501\n\n    if np.isnan(x):\n        return str(x)\n    if percent:\n        return f\"{x * 100:.2f}%\"\n    return f\"{x:.2f}\"\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_disk_usage_higher_than_threshold", "completion": "    while True:\n        free_space = shutil.disk_usage(input_dir).free / 1024 / 1024 / 1024\n        if free_space < threshold_in_gb:\n            break\n        sleep(sleep_time)\n\n"}
{"namespace": "stepfun.pdf_to_weight", "completion": "  utils.assert_valid_stepfun(t, p)\n  td = jnp.diff(t)\n  return math.safe_div(p, td)\n\n"}
{"namespace": "nlm_ingestor.ingestor.processors.fix_spaced_characters", "completion": "    # Remove all whitespace characters from the input text\n    line_text = re.sub(r\"\\s+\", \" \", line_text)\n\n    # Segment the modified text into smaller parts or tokens\n    line_text = line_text.split(\" \")\n\n    return line_text"}
{"namespace": "skfolio.utils.stats.rand_weights", "completion": "  # noqa: E501\n    weights = np.random.rand(n)\n    weights = weights / np.sum(weights)\n    if zeros > 0:\n        weights[:zeros] = 0\n    return weights\n\n"}
{"namespace": "autorag.schema.module.Module.from_dict", "completion": "        return cls(module_type=module_dict['module_type'], module_param=module_dict)"}
{"namespace": "detectron2.data.detection_utils.gen_crop_transform_with_instance", "completion": "    # Get the bounding box of the instance\n    bbox = instance[\"bbox\"]\n    bbox_mode = instance[\"bbox_mode\"]\n    bbox = BoxMode.convert(bbox, bbox_mode, BoxMode.XYXY_ABS)\n\n    # Get the center of the bounding box\n    center = np.array([bbox[0] + bbox[2] / 2, bbox[1] + bbox[3] / 2])\n\n    # Get the desired crop size\n    crop_size = np.array(crop_size)\n\n    # Calculate the top-left corner of the crop\n    top_left = center - crop_size / 2\n\n    # Ensure that the crop fits within the image boundaries\n    top_left = np.maximum(np.minimum(top_left, image_size - crop_size), 0)\n\n    # Create the CropTransform object\n    crop_transform = T.CropTransform(top_left, crop_size)\n\n    return crop_transform\n\n"}
{"namespace": "ref_utils.l2_normalize", "completion": "  x = x - jnp.mean(x, axis=-1, keepdims=True)\n  x = x / jnp.maximum(jnp.linalg.norm(x, axis=-1, keepdims=True), grad_eps)\n  return x\n\n"}
{"namespace": "agent_response.AgentResponse._parse_agent_info", "completion": "        agent_name = \"\"\n        input_text = \"\"\n        if \"Use Agent[\" in response and \"]]\" in response:\n            agent_name = response.split(\"Use Agent[\")[1].split(\"]\")[0]\n            input_text = response.split(\"Use Agent[\")[1].split(\"]\")[1].strip()\n        return agent_name, input_text\n"}
{"namespace": "detectron2.data.detection_utils.annotations_to_instances", "completion": "    # The following fields are copied from the original annotations to the output\n    fields = {\n        \"image_id\": annos[0].get(\"image_id\", None),\n        \"height\": image_size[0],\n        \"width\": image_size[1],\n    }\n\n    # The following fields are derived from the original annotations\n    fields[\"gt_boxes\"] = Boxes(\n        [BoxMode.convert(b, b[\"bbox_mode\"], BoxMode.XYXY_ABS) for b in annos]\n    )\n    fields[\"gt_classes\"] = [b[\"category_id\"] for b in annos]\n    fields[\"gt_masks\"] = None\n    fields[\"gt_keypoints\"] = None\n    fields[\"iscrowd\"] = [b.get(\"iscrowd\", 0) for b in annos]\n\n    # The following fields are added by this function\n    fields[\"gt_masks\"] = None\n    fields[\"gt_keypoints\"] = None\n\n    # The following fields are added by this function\n    fields[\"gt_masks\"] = None\n    fields[\"gt_keypoints\"] = None\n\n    # The following fields are added by this function\n    fields[\"gt_masks\"] = None\n    fields[\"gt_keypoints\"] = None\n\n    # The following fields are added by this function\n    fields[\"gt_masks\"] = None\n    fields[\"gt_keypoints\"] = None\n\n    # The following fields are added by this function\n    fields[\"gt_masks\"] = None\n    fields[\"gt_keypoints\"] = None\n\n    # The following fields are added by this function\n    fields[\"gt_masks\"] = None\n    fields[\"gt_keypoints\"] = None\n\n    # The following fields are added by this function\n    fields[\"gt_masks\"] = None\n    fields[\"gt_keypoints\"] = None\n\n    # The following fields are added by this function\n    fields[\"gt_masks\"] = None\n    fields[\"gt_keypoints\"] = None\n\n    # The following fields are added by this function\n    fields[\"gt_masks\"] = None\n    fields[\"gt_keypoints\"] = None\n\n    # The following fields are added by this function\n    fields[\"gt_masks\"] = None\n    fields[\"gt_keypoints\"] = None\n\n    # The following fields are added by this function\n    fields[\"gt_masks\"] = None\n    fields[\"gt_keypoints\"] = None\n\n    #"}
{"namespace": "skfolio.datasets._base.get_data_home", "completion": "  # noqa: E501\n\n    if data_home is None:\n        data_home = os.environ.get(\"SKFOLIO_DATA\", os.path.join(os.path.expanduser(\"~\"), \"skfolio_data\"))\n    data_home = os.path.expanduser(data_home)\n    if not os.path.exists(data_home):\n        os.makedirs(data_home)\n    return data_home\n\n"}
{"namespace": "skfolio.utils.stats.cov_to_corr", "completion": "  # noqa: E501\n\n    assert_is_symmetric(cov)\n    assert_is_square(cov)\n    assert_is_distance(cov)\n\n    corr = cov_to_corr_2(cov)\n    std = np.sqrt(np.diag(corr))\n    return corr, std\n\n"}
{"namespace": "detectron2.export.torchscript_patch.freeze_training_mode", "completion": "    def _freeze_training_mode(self):\n        self.training = False\n\n    for name, module in model.named_modules():\n        if hasattr(module, \"training\"):\n            setattr(module, \"training\", torch.jit.Final(False))\n        if hasattr(module, \"training\"):\n            setattr(module, \"training\", torch.jit.Final(False))\n\n    yield\n\n    for name, module in model.named_modules():\n        if hasattr(module, \"training\"):\n            setattr(module, \"training\", torch.jit.Final(False))"}
{"namespace": "iris.io.validators.are_shapes_equal", "completion": "    def __root_validator(cls: type, values: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n        \"\"\"Check if field1 and field2 have the same shape.\"\"\"\n        if values[field1].shape != values[field2].shape:\n            raise ValueError(\n                f\"{cls.__name__}: {field1} and {field2} shape mismatch, \"\n                f\"resp. {values[field1].shape} and {values[field2].shape}\"\n            )\n\n        return values\n\n    return __root_validator\n\n"}
{"namespace": "autorag.evaluate.util.cast_metrics", "completion": "    if isinstance(metrics, list):\n        if all(isinstance(metric, str) for metric in metrics):\n            return metrics, []\n        elif all(isinstance(metric, dict) for metric in metrics):\n            return [], metrics\n        else:\n            raise ValueError(\"The input list contains elements that are neither strings nor dictionaries.\")\n    else:\n        raise TypeError(\"The input is not a list.\")"}
{"namespace": "coord.construct_ray_warps", "completion": "  def t_to_s(t):\n    \"\"\"Maps metric distances to normalized distances.\"\"\"\n    t = jnp.clip(t, t_near, t_far)\n    return (t - t_near) / (t_far - t_near)\n\n  def s_to_t(s):\n    \"\"\"Maps normalized distances to metric distances.\"\"\"\n    s = jnp.clip(s, 0, 1)\n    return s * (t_far - t_near) + t_near\n\n  if fn_inv is None:\n    fn_inv = jax.jit(jax.vmap(jax.jit(fn), in_axes=-1, out_axes=-1))\n\n  return t_to_s, s_to_t\n\n"}
{"namespace": "geometry.spherical_to_cartesian", "completion": "  x = r * jnp.sin(theta) * jnp.cos(phi)\n  y = r * jnp.sin(theta) * jnp.sin(phi)\n  z = r * jnp.cos(theta)\n\n  return x, y, z\n\n"}
{"namespace": "linspline.integrate", "completion": "  utils.assert_valid_linspline(t, w)\n  return jnp.trapz(w, t)\n\n"}
{"namespace": "autorag.nodes.retrieval.hybrid_cc.cc_pure", "completion": ""}
{"namespace": "coord.track_linearize", "completion": "  def linearize_fn(x):\n    \"\"\"Linearizes the function around the mean.\"\"\"\n    return fn(x) - fn(mean)\n\n  def transform_covariance(covariance):\n    \"\"\"Transforms the covariance matrix using the linearized function.\"\"\"\n    return jnp.einsum('...ij,...jk->...ik', linearize_fn, covariance)\n\n  def transform_covariance_2d(covariance):\n    \"\"\"Transforms the covariance matrix using the linearized function.\"\"\"\n    return jnp.einsum('...ij,...jk->...ik', linearize_fn, covariance)\n\n  def transform_covariance_3d(covariance):\n    \"\"\"Transforms the covariance matrix using the linearized function.\"\"\"\n    return jnp.einsum('...ij,...jk->...ik', linearize_fn, covariance)\n\n  def transform_covariance_4d(covariance):\n    \"\"\"Transforms the covariance matrix using the linearized function.\"\"\"\n    return jnp.einsum('...ij,...jk->...ik', linearize_fn, covariance)\n\n  def transform_covariance_5d(covariance):\n    \"\"\"Transforms the covariance matrix using the linearized function.\"\"\"\n    return jnp.einsum('...ij,...jk->...ik', linearize_fn, covariance)\n\n  def transform_covariance_6d(covariance):\n    \"\"\"Transforms the covariance matrix using the linearized function.\"\"\"\n    return jnp.einsum('...ij,...jk->...ik', linearize_fn, covariance)\n\n  def transform_covariance_7d(covariance):\n    \"\"\"Transforms the covariance matrix using the linearized function.\"\"\"\n    return jnp.einsum('...ij,...jk->...ik', linearize_fn, covariance)\n\n  def transform_covariance_8d(covariance):\n    \"\"\"Transforms the covariance matrix using the linearized function.\"\"\"\n    return jnp.einsum('...ij,...jk->...ik', linearize_fn, covariance)\n\n  def transform_covariance_9d(covariance):\n    \"\"\"Transforms the covariance matrix using the linearized function.\"\"\"\n    return jnp.einsum('...ij,...jk->...ik', linearize_fn, covariance)\n\n  def transform_covariance_10d(covariance):\n    \"\"\"Transforms the covariance matrix using the linearized function.\"\"\"\n    return jnp.einsum('...ij,...jk->...ik', linearize_fn, covariance)\n\n  def transform_covariance"}
{"namespace": "skfolio.utils.tools.bisection", "completion": "  # noqa: E501\n\n    for arr in x:\n        if len(arr) > 1:\n            yield arr[: len(arr) // 2], arr[len(arr) // 2 :]\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_square", "completion": "    if x.ndim != 2:\n        raise ValueError(\"The matrix must be a 2d-array\")\n    if x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix must be square\")\n\n"}
{"namespace": "coord.pos_enc", "completion": "  scales = 2.0 ** jnp.arange(min_deg, max_deg)\n  shape = x.shape[:-1] + (-1,)\n  scaled_x = jnp.reshape(x[Ellipsis, None, :] * scales[:, None], shape)\n  scaled_x = jnp.concatenate([scaled_x, x], axis=-1)\n  return scaled_x\n\n"}
{"namespace": "iris.io.validators.are_all_shapes_equal", "completion": "  # noqa: E501\n\n    def __root_validator(cls: type, values: Dict[str, List[Any]]) -> Dict[str, List[Any]]:\n        \"\"\"Check if field1.shape equals field2.shape.\"\"\"\n        if len(values[field1]) != len(values[field2]):\n            raise ValueError(f\"{cls.__name__}: {field1} and {field2} length mismatch, \"\n                             f\"resp. {len(values[field1])} and {len(values[field2])}\")\n\n        for i in range(len(values[field1])):\n            if values[field1][i].shape != values[field2][i].shape:\n                raise ValueError(f\"{cls.__name__}: {field1} and {field2} shape mismatch at index {i}\")\n\n        return values\n\n    return __root_validator\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.offscreen_render", "completion": "  # noqa: E501\n        # Resize the rendering context to match the camera's dimensions\n        eglctx.resize(camera.width, camera.height)\n\n        # Render the Mesh instance using the camera's settings\n        self.render(eglctx, camera)\n\n    def render(self, eglctx: \"eglContextManager\", camera: Camera):\n        \"\"\"\n        The function renders the Mesh instance using a specified camera configuration. It first sets up the OpenGL program and uniforms, then renders the Mesh instance using the camera's settings.\n\n        Input-Output Arguments\n        :param eglctx: eglContextManager, the rendering context manager used for offscreen rendering. It is resized to match the camera's dimensions before rendering.\n        :param camera: Camera, the camera configuration used for rendering. Its width and height are used to resize the rendering context, and its settings are used during the rendering process.\n        :return: No return values.\n        \"\"\"  # noqa: E501\n        # Set up the OpenGL program and uniforms\n        self.setup_gl_program(eglctx)\n\n        # Render the Mesh instance using the camera's settings\n        self.render_mesh(eglctx, camera)\n\n    def render_mesh(self, eglctx: \"eglContextManager\", camera: Camera):\n        \"\"\"\n        The function renders the Mesh instance using a specified camera configuration. It first sets up the OpenGL program and uniforms, then renders the Mesh instance using the camera's settings.\n\n        Input-Output Arguments\n        :param eglctx: eglContextManager, the rendering context manager used for offscreen rendering. It is resized to match the camera's dimensions before rendering.\n        :param camera: Camera, the camera configuration used for rendering. Its width and height are used to resize the rendering context, and its settings are used during the rendering process.\n        :return: No return values.\n        \"\"\"  # noqa: E501\n        # Set up the OpenGL program and uniforms\n        self.setup_gl_program(eglctx)\n\n        # Render the Mesh instance using the camera's settings\n        self.render_meshes(eglctx, camera)\n\n    def render_meshes(self, eglctx: \"eglContextManager\", camera: Camera):\n        \"\"\"\n        The function renders the Mesh instance using a specified camera configuration. It first sets up the OpenGL program and uniforms, then renders the Mesh instance using the camera's settings.\n\n        Input-Output Arguments\n        :"}
{"namespace": "contrastors.models.encoder.bert.bert_config_to_nomic_config", "completion": "    # Copy the BertConfig object\n    nomic_config = NomicBertConfig.from_dict(bert_config.to_dict())\n\n    # Add new arguments\n    nomic_config.add_argument(\"nomic\", type=str, help=\"The name of the Nomic model to use.\")\n    nomic_config.add_argument(\"nomic_model\", type=str, help=\"The path to the Nomic model to use.\")\n    nomic_config.add_argument(\"nomic_model_name\", type=str, help=\"The name of the Nomic model to use.\")\n    nomic_config.add_argument(\"nomic_model_version\", type=str, help=\"The version of the Nomic model to use.\")\n    nomic_config.add_argument(\"nomic_model_type\", type=str, help=\"The type of the Nomic model to use.\")\n    nomic_config.add_argument(\"nomic_model_type_version\", type=str, help=\"The version of the Nomic model type to use.\")\n    nomic_config.add_argument(\"nomic_model_type_name\", type=str, help=\"The name of the Nomic model type to use.\")\n    nomic_config.add_argument(\"nomic_model_type_name_version\", type=str, help=\"The version of the Nomic model type name to use.\")\n    nomic_config.add_argument(\"nomic_model_type_name_version\", type=str, help=\"The version of the Nomic model type name to use.\")\n    nomic_config.add_argument(\"nomic_model_type_name_version\", type=str, help=\"The version of the Nomic model type name to use.\")\n    nomic_config.add_argument(\"nomic_model_type_name_version\", type=str, help=\"The version of the Nomic model type name to use.\")\n    nomic_config.add_argument(\"nomic_model_type_name_version\", type=str, help=\"The version of the Nomic model type name to use.\")\n    nomic_config.add_argument(\"nomic_model_type_name_version\", type=str, help=\"The version of the Nomic model type name to use.\")\n    nomic_config.add_argument(\"nomic_model_type_name_version\", type=str, help=\"The version of the Nomic model type name to use.\")\n    nomic_config.add_argument(\"nomic_model_type_name_version\", type=str, help=\"The version of the Nomic model type name to use.\")\n    nomic_config.add_argument(\"nomic_model_type_name_version\", type=str, help=\"The version of the Nomic model type name to"}
{"namespace": "easyvolcap.utils.gl_utils.Mesh.render", "completion": "        # Check if the mesh is visible\n        if not self.visible:\n            return\n\n        # Set up the OpenGL state\n        common_opengl_options()\n\n        # Set up the OpenGL programs\n        use_gl_program(self.mesh_program)\n        use_gl_program(self.point_program)\n\n        # Upload the uniforms\n        self.upload_gl_uniforms(camera)\n\n        # Bind the vertex array object\n        gl.glBindVertexArray(self.vao)\n\n        # Draw the mesh\n        if self.render_type == Mesh.RenderType.POINTS:\n            gl.glDrawArrays(gl.GL_POINTS, 0, self.n_verts)\n        elif self.render_type == Mesh.RenderType.LINES:\n            gl.glDrawArrays(gl.GL_LINES, 0, self.n_verts)\n        elif self.render_type == Mesh.RenderType.TRIS:\n            gl.glDrawElements(gl.GL_TRIANGLES, self.n_faces, gl.GL_UNSIGNED_INT, None)\n        elif self.render_type == Mesh.RenderType.QUADS:\n            gl.glDrawElements(gl.GL_QUADS, self.n_faces, gl.GL_UNSIGNED_INT, None)\n        elif self.render_type == Mesh.RenderType.STRIPS:\n            gl.glDrawElements(gl.GL_TRIANGLE_STRIP, self.n_faces, gl.GL_UNSIGNED_INT, None)\n        else:\n            raise RuntimeError(f'Unsupported render type: {self.render_type}')\n\n        # Unbind the vertex array object\n        gl.glBindVertexArray(0)\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.upload_to_texture", "completion": "  # TODO: Add a function to update the entire texture\n        if not self.use_quad_cuda:\n            self.upload_to_texture_numpy(ptr, x, y, w, h)\n            return\n\n        if not hasattr(self, 'cu_tex'):\n            self.init_texture()\n\n        # assert self.use_quad_cuda, \"Need to enable cuda-opengl interop to copy from device to device, check creation of this Quad\"\n        w = w or self.W\n        h = h or self.H\n        if isinstance(ptr, torch.Tensor):\n            ptr = ptr.detach().cpu().numpy()\n\n        from cuda import cudart\n        kind = cudart.cudaMemcpyKind.cudaMemcpyDeviceToDevice\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsMapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n        cu_tex_arr = CHECK_CUDART_ERROR(cudart.cudaGraphicsSubResourceGetMappedArray(self.cu_tex, 0, 0))\n\n        CHECK_CUDART_ERROR(cudart.cudaMemcpy2DFromArrayAsync(cu_tex_arr,\n                                                               w * 4 * cu_tex_arr.element_size(),\n                                                               x * 4 * ptr.element_size(),\n                                                               ptr.data_ptr(),\n                                                               w * 4 * ptr.element_size(),  # differently sized\n                                                               h * 4 * ptr.element_size(),  # rgba, should do a composition first\n                                                               h,\n                                                               kind,\n                                                               torch.cuda.current_stream().cuda_stream))\n        CHECK_CUDART_ERROR(cudart.cudaGraphicsUnmapResources(1, self.cu_tex, torch.cuda.current_stream().cuda_stream))\n\n    def upload_to_texture_numpy(self, ptr: np.ndarray, x: int = 0, y: int = 0, w: int = 0, h: int = 0):\n        \"\"\"\n        This function uploads a portion or the entirety of a numpy array to a texture in OpenGL. It is designed to update the texture content starting from a specified position (x, y) and covering a specified width (w) and height (h). If width and height are not provided, it defaults to the object's width and height. The function handles the conversion from a numpy array to a PyTorch tensor before uploading.\n\n        Input-Output Arguments\n        :param self: Quad. An instance of the Quad class, which contains the texture to be updated and the default dimensions (W, H) for the texture update.\n        :param ptr"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pulsar_camera_params", "completion": "  # noqa: E501\n    # Validate input shapes\n    assert R.shape[-2:] == (3, 3), \"R must be a batch of 3x3 rotation matrices\"\n    assert tvec.shape[-2:] == (3, 1), \"tvec must be a batch of 3x1 translation vectors\"\n    assert camera_matrix.shape[-2:] == (3, 3), \"camera_matrix must be a batch of 3x3 camera intrinsic matrices\"\n    assert image_size.shape[-2:] == (2, 1), \"image_size must be a batch of 2x1 image sizes\"\n    assert znear > 0, \"znear must be a positive value\"\n\n    # Compute camera position\n    camera_position = tvec @ R\n\n    # Compute camera rotation\n    camera_rotation = matrix_to_rotation_6d(R)\n\n    # Compute focal lengths\n    focal_lengths = camera_matrix[..., 0, 0] * image_size[..., 0] / 2 + znear\n    sensor_width = camera_matrix[..., 0, 0] * image_size[..., 0] / 2 - znear\n\n    # Compute principal point offsets\n    principal_point_offsets = camera_matrix[..., 2, 2] * image_size[..., 0] / 2\n\n    # Normalize focal lengths\n    focal_lengths = normalize_sum(focal_lengths)\n\n    # Compute camera parameters\n    camera_params = torch.cat([camera_position, camera_rotation, focal_lengths, sensor_width, principal_point_offsets], dim=-1)\n\n    return camera_params\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.draw", "completion": "        if not self.use_quad_draw:\n            gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n            gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, self.fbo)\n            gl.glViewport(x, y, w, h)\n            gl.glScissor(x, y, w, h)\n            gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n            gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, 0)\n            gl.glViewport(0, 0, self.W, self.H)\n            gl.glScissor(0, 0, self.W, self.H)\n            return\n\n        gl.glBindTexture(gl.GL_TEXTURE_2D, self.tex)\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, self.fbo)\n        gl.glViewport(x, y, w, h)\n        gl.glScissor(x, y, w, h)\n        gl.glUseProgram(self.quad_program)\n        gl.glUniform1i(self.uniforms.tex, 0)\n        gl.glBindVertexArray(self.vao)\n        gl.glDrawArrays(gl.GL_TRIANGLE_STRIP, 0, 4)\n        gl.glBindFramebuffer(gl.GL_FRAMEBUFFER, 0)\n        gl.glViewport(0, 0, self.W, self.H)\n        gl.glScissor(0, 0, self.W, self.H)\n"}
{"namespace": "easyvolcap.utils.fcds_utils.get_pytorch3d_camera_params", "completion": "  # noqa: E501\n    H = batch.meta.H[0].item()  # !: BATCH\n    W = batch.meta.W[0].item()  # !: BATCH\n    K = batch.K\n    R = batch.R\n    T = batch.T\n    C = -batch.R.mT @ batch.T  # B, 3, 1\n    # Adjust rotation matrix to match PyTorch3D's coordinate system.\n    R = R.permute(0, 2, 1)\n    # Adjust translation vector to match PyTorch3D's coordinate system.\n    T = -R @ T\n    # Recalculate intrinsic matrix for NDC.\n    K = get_ndc_perspective_matrix(K, H, W, C[..., 0], C[..., 1])\n    return H, W, K, R, T, C\n\n"}
{"namespace": "easyvolcap.utils.gl_utils.Quad.blit", "completion": "class VolumeRenderer:\n    def __init__(self,\n                 H: int = 256, W: int = 256,\n                 use_quad_draw: bool = True,\n                 use_quad_cuda: bool = True,\n                 compose: bool = False,\n                 compose_power: float = 1.0,\n                 ):\n        self.H, self.W = H, W\n        self.use_quad_draw = use_quad_draw\n        self.use_quad_cuda = use_quad_cuda\n        self.compose = compose\n        self.compose_power = compose_power\n\n        self.quad = Quad(H, W, use_quad_draw, use_quad_cuda, compose, compose_power)\n\n    def render(self, camera: Camera, volume: torch.Tensor, alpha: torch.Tensor, depth: torch.Tensor, color: torch.Tensor, depth_curve: str = 'linear'):\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple volumes\n        # TODO: Add support for multiple alpha channels\n        # TODO: Add support for multiple depth curves\n        # TODO: Add support for multiple color channels\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple volumes\n        # TODO: Add support for multiple alpha channels\n        # TODO: Add support for multiple depth curves\n        # TODO: Add support for multiple color channels\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple volumes\n        # TODO: Add support for multiple alpha channels\n        # TODO: Add support for multiple depth curves\n        # TODO: Add support for multiple color channels\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple volumes\n        # TODO: Add support for multiple alpha channels\n        # TODO: Add support for multiple depth curves\n        # TODO: Add support for multiple color channels\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple volumes\n        # TODO: Add support for multiple alpha channels\n        # TODO: Add support for multiple depth curves\n        # TODO: Add support for multiple color channels\n        # TODO: Add support for multiple cameras\n        # TODO: Add support for multiple volumes\n        # TODO: Add support for multiple alpha channels\n        # TODO: Add support for multiple depth curves\n        # TODO: Add support for multiple color channels\n        # TODO: Add support for multiple"}
{"namespace": "easyvolcap.utils.loss_utils.inner_outer", "completion": "    # Compute the cumulative sums of the source times and values\n    t1_cumsum = torch.cumsum(t1, dim=0)\n    y1_cumsum = torch.cumsum(y1, dim=0)\n\n    # Compute the inner measure\n    t0_cumsum = torch.cumsum(t0, dim=0)\n    inner = (t0_cumsum - t1_cumsum) / (t0_cumsum[-1] - t1_cumsum[-1])\n\n    # Compute the outer measure\n    outer = (t0_cumsum - t1_cumsum) / (t0_cumsum[-1] - t1_cumsum[0])\n\n    return inner, outer\n\n"}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_outer", "completion": "    # t.shape[-1] = w.shape[-1] + 1\n    # t_env.shape[-1] = w_env.shape[-1] + 1\n\n    # t = t[..., :-1]\n    # w = w[..., :-1]\n    # t_env = t_env[..., :-1]\n    # w_env = w_env[..., :-1]\n\n    # t = t[..., 1:]\n    # w = w[..., 1:]\n    # t_env = t_env[..., 1:]\n    # w_env = w_env[..., 1:]\n\n    # t = t[..., 1:]\n    # w = w[..., 1:]\n    # t_env = t_env[..., :-1]\n    # w_env = w_env[..., :-1]\n\n    # t = t[..., :-1]\n    # w = w[..., :-1]\n    # t_env = t_env[..., 1:]\n    # w_env = w_env[..., 1:]\n\n    # t = t[..., :-1]\n    # w = w[..., :-1]\n    # t_env = t_env[..., :-1]\n    # w_env = w_env[..., 1:]\n\n    # t = t[..., :-1]\n    # w = w[..., :-1]\n    # t_env = t_env[..., :-1]\n    # w_env = w_env[..., :-1]\n\n    # t = t[..., 1:]\n    # w = w[..., 1:]\n    # t_env = t_env[..., 1:]\n    # w_env = w_env[..., 1:]\n\n    # t = t[..., 1:]\n    # w = w[..., 1:]\n    # t_env = t_env[..., :-1]\n    # w_env = w_env[..., :-1]\n\n    # t = t[..., :-1]\n    # w = w[..., :-1]\n    # t_env = t_env[..., :-1]\n    # w_env = w_env[..., :-1]\n\n    # t = t[..., :-1]\n    # w = w[..., :-1]\n    # t_env = t_env[..., :-1]\n    # w_env = w_env[..., :-1]\n\n    # t = t[..., :-1]\n    # w = w[..., :-1]\n    # t_env = t_env[..., :-1]\n    # w_env = w_env[..., :-1]\n\n    # t = t[..., :-1]\n    # w = w[..., :-1]\n    # t_env ="}
{"namespace": "easyvolcap.utils.loss_utils.lossfun_distortion", "completion": "    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup_channels(t, w)\n    # t, w = matchup"}
{"namespace": "easyvolcap.utils.prop_utils.weighted_percentile", "completion": "    # Match up the channels of the tensors\n    t, w = matchup_channels(t, w)\n\n    # Integrate the weights\n    cw = integrate_weights(w)\n\n    # Interpolate the integrated weights to find the weighted percentiles\n    return interpolate(cw, t, cw)\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.importance_sampling", "completion": "    # Check input arguments\n    if t.ndim != 1:\n        raise ValueError(\"t must be a 1D tensor.\")\n    if w.ndim != 1:\n        raise ValueError(\"w must be a 1D tensor.\")\n    if t.shape[0] != w.shape[0]:\n        raise ValueError(\"t and w must have the same number of bins.\")\n    if t.shape[0] < 2:\n        raise ValueError(\"t must have at least 2 bins.\")\n    if num_samples < 1:\n        raise ValueError(\"num_samples must be a positive integer.\")\n\n    # Compute the CDF of the PDF.\n    cw = integrate_weights(w)\n\n    # Generate samples from the CDF.\n    u = torch.rand(num_samples, device=t.device)\n    t_new = interpolate(u, cw, t)\n\n    # Perturb the samples.\n    if perturb:\n        # Compute the PDF.\n        pdf = w / cw\n        # Compute the PDF at the new samples.\n        pdf_new = pdf.gather(dim=-1, index=t_new)\n        # Compute the perturbation.\n        perturb = pdf_new / pdf\n        # Apply the perturbation.\n        t_new = t_new + perturb * torch.randn_like(t_new)\n\n    # Jitter the samples.\n    if single_jitter:\n        # Compute the jitter.\n        jitter = torch.randn_like(t_new)\n    else:\n        # Compute the jitter.\n        jitter = torch.randn_like(t_new)\n        # Apply the jitter.\n        t_new = t_new + jitter\n\n    return t_new\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.max_dilate", "completion": "    # Dilate the time steps\n    t_dilated = t.unsqueeze(-1) + dilation\n\n    # Clip the dilated time steps\n    t_dilated = t_dilated.clamp(domain[0], domain[1])\n\n    # Compute the weights of the dilated time steps\n    w_dilated = weight_to_pdf(t_dilated, w)\n\n    return t_dilated, w_dilated\n\n"}
{"namespace": "easyvolcap.utils.prop_utils.query", "completion": "    # if tq.ndim == t.ndim + 1:\n    #     tq = tq[..., 0]  # remove last dim\n    # if tq.shape[-1] != t.shape[-1] + 1:\n    #     tq = torch.cat([tq, torch.ones_like(t[..., -1:])], dim=-1)  # 65\n    # if tq.shape[-1] != y.shape[-1]:\n    #     y = torch.cat([y, torch.zeros_like(y[..., -1:])], dim=-1)  # 65\n\n    # if tq.ndim == t.ndim + 1:\n    #     tq = tq[..., 0]  # remove last dim\n    # if tq.shape[-1] != t.shape[-1] + 1:\n    #     tq = torch.cat([tq, torch.ones_like(t[..., -1:])], dim=-1)  # 65\n    # if tq.shape[-1] != y.shape[-1]:\n    #     y = torch.cat([y, torch.zeros_like(y[..., -1:])], dim=-1)  # 65\n\n    # if tq.ndim == t.ndim + 1:\n    #     tq = tq[..., 0]  # remove last dim\n    # if tq.shape[-1] != t.shape[-1] + 1:\n    #     tq = torch.cat([tq, torch.ones_like(t[..., -1:])], dim=-1)  # 65\n    # if tq.shape[-1] != y.shape[-1]:\n    #     y = torch.cat([y, torch.zeros_like(y[..., -1:])], dim=-1)  # 65\n\n    # if tq.ndim == t.ndim + 1:\n    #     tq = tq[..., 0]  # remove last dim\n    # if tq.shape[-1] != t.shape[-1] + 1:\n    #     tq = torch.cat([tq, torch.ones_like(t[..., -1:])], dim=-1)  # 65\n    # if tq.shape[-1] != y.shape[-1]:\n    #     y = torch.cat([y, torch.zeros_like(y[..., -1:])], dim=-1)  # 65\n\n    # if tq.ndim == t.ndim + 1:\n    #     tq = tq[..., 0] "}
{"namespace": "easyvolcap.utils.prop_utils.anneal_weights", "completion": "  # noqa: E501\n    # t.shape[-1] = w.shape[-1] + 1\n    # t, w = matchup_channels(t, w)\n    # t = t[..., 0]  # remove last dim\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1])\n    # w = w.reshape(-1, w.shape[-1"}
{"namespace": "easyvolcap.utils.data_utils.to_cuda", "completion": "  # noqa: E501\n    if isinstance(batch, torch.Tensor):\n        return batch.to(device, non_blocking=True)\n    elif isinstance(batch, dict):\n        if \"meta\" in batch:\n            return batch\n        else:\n            return {k: to_cuda(v, device=device, ignore_list=ignore_list) for k, v in batch.items()}\n    elif isinstance(batch, list):\n        return [to_cuda(v, device=device, ignore_list=ignore_list) for v in batch]\n    elif isinstance(batch, tuple):\n        return tuple([to_cuda(v, device=device, ignore_list=ignore_list) for v in batch])\n    else:\n        return batch\n\n"}
{"namespace": "easyvolcap.utils.chunk_utils.multi_gather_tris", "completion": "  # MARK: SYNC\n    # gather the vertices\n    v = multi_gather(v, f, dim=dim)\n    # compute the normals\n    n = torch.cross(v[:, 1:] - v[:, :-1], v[:, 1:] - v[:, :-1], dim=-1)\n    # reshape the result\n    return n.view(f.shape[:-1] + n.shape[-1:])\n\n"}
{"namespace": "easyvolcap.utils.data_utils.add_batch", "completion": "  # noqa: E501\n    if isinstance(batch, (tuple, list)):\n        batch = [add_batch(b) for b in batch]\n    elif isinstance(batch, dict):\n        batch = dotdict({k: add_batch(v) for k, v in batch.items()})\n    elif isinstance(batch, (torch.Tensor, np.ndarray)):  # numpy and others\n        batch = batch[None]\n    else:\n        batch = torch.as_tensor(batch)[None]\n    return batch\n\n"}
{"namespace": "easyvolcap.utils.viewer_utils.Camera.to_batch", "completion": "  # noqa: E501\n\n        # Initialize the batch dictionary\n        batch = dotdict()\n\n        # Convert camera parameters to tensors\n        batch.H = torch.tensor(self.H, dtype=torch.int32)\n        batch.W = torch.tensor(self.W, dtype=torch.int32)\n        batch.K = torch.tensor(self.K, dtype=torch.float32)\n        batch.R = torch.tensor(self.R, dtype=torch.float32)\n        batch.T = torch.tensor(self.T, dtype=torch.float32)\n        batch.n = torch.tensor(self.n, dtype=torch.float32)\n        batch.f = torch.tensor(self.f, dtype=torch.float32)\n        batch.t = torch.tensor(self.t, dtype=torch.float32)\n        batch.v = torch.tensor(self.v, dtype=torch.float32)\n        batch.bounds = torch.tensor(self.bounds, dtype=torch.float32)\n\n        # Convert GUI related elements to tensors\n        batch.origin = torch.tensor(self.origin, dtype=torch.float32)\n        batch.world_up = torch.tensor(self.world_up, dtype=torch.float32)\n        batch.movement_speed = torch.tensor(self.movement_speed, dtype=torch.float32)\n        batch.movement_force = torch.tensor(self.movement_force, dtype=torch.float32)\n        batch.drag_coeff_mult = torch.tensor(self.drag_coeff_mult, dtype=torch.float32)\n        batch.constant_drag = torch.tensor(self.constant_drag, dtype=torch.float32)\n        batch.mass = torch.tensor(self.mass, dtype=torch.float32)\n        batch.moment_of_inertia = torch.tensor(self.moment_of_inertia, dtype=torch.float32)\n        batch.movement_torque = torch.tensor(self.movement_torque, dtype=torch.float32)\n        batch.angular_friction = torch.tensor(self.angular_friction, dtype=torch.float32)\n        batch.constant_torque = torch.tensor(self.constant_torque, dtype=torch.float32)\n\n        # Add a 'meta' dictionary to the batch\n        batch.meta = dotdict()\n        batch.meta.min_interval = torch.tensor(self.min_interval, dtype=torch.float32)\n        batch.meta.pause_physics = torch.tensor(self.pause_physics, dtype=torch.bool)\n\n        # Return the batch\n        return batch\n\n    def from_batch(self, batch: dotdict):\n        \"\"\"\n        The function converts a batch of camera parameters and GUI related elements into a Camera instance. It uses the tensors in the batch to initialize the camera parameters"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.save_agent", "completion": "        if agent.is_working():\n            if not agent.is_prime():\n                self.persistence.save_agent(agent)\n        else:\n            self.persistence.remove_agent(agent.id)\n"}
{"namespace": "agent_similarity.AgentSimilarity.find_closest_agent", "completion": "        try:\n            similarities = [cosine_similarity([purpose_embedding], [agent.purpose_embedding])[0][0] for agent in self.agents]\n            return self.agents[np.argmax(similarities)], np.max(similarities)\n        except Exception as e:\n            logger.exception(f\"Error finding closest agent: {e}\")\n            raise ValueError(f\"Error finding closest agent: {e}\")"}
{"namespace": "agent_lifecycle.AgentLifecycle.create_prime_agent", "completion": "        \n        self.agents.append(\n            MicroAgent(\n                self.openai_wrapper,\n                self.agent_persistence,\n                PRIME_PROMPT,\n                PRIME_NAME,\n                PRIME_AGENT_WEIGHT,\n                True,\n                True,\n                EXAMPLES\n            )\n        )\n"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_agent", "completion": "    def load_agent(self, purpose, agent_lifecycle, openai_wrapper):\n        \"\"\"\n        Loads an agent with a specified purpose from the database. If an agent with the given purpose is found, it is deserialized and returned; otherwise, None is returned.\n\n        Input-Output Arguments\n        :param self: AgentPersistenceManager. An instance of the AgentPersistenceManager class.\n        :param purpose: str, The purpose of the agent to be loaded. It is used to identify the agent in the database.\n        :param agent_lifecycle: The lifecycle state of the agent. It is passed to the deserializer to properly initialize the agent.\n        :param openai_wrapper: An instance or interface used for interacting with OpenAI services. It is passed to the deserializer for initializing the agent with OpenAI functionalities.\n        :return: An instance of the deserialized agent if found, otherwise None.\n        \"\"\"\n        serialized_agent = self.persistence.load_agent(purpose)\n        if serialized_agent is not None:\n            return AgentSerializer.deserialize(serialized_agent, agent_lifecycle, openai_wrapper)\n\n    def load_all_agents(self):\n        \"\"\"\n        Loads all agents from the database.\n        :return: A list of all agents in the database.\n        \"\"\"\n        return self.persistence.load_all_agents()\n\n    def load_all_working_agents(self):\n        \"\"\"\n        Loads all working agents from the database.\n        :return: A list of all working agents in the database.\n        \"\"\"\n        return self.persistence.load_all_working_agents()\n\n    def load_all_prime_agents(self):\n        \"\"\"\n        Loads all prime agents from the database.\n        :return: A list of all prime agents in the database.\n        \"\"\"\n        return self.persistence.load_all_prime_agents()\n\n    def load_all_agents_by_purpose(self, purpose):\n        \"\"\"\n        Loads all agents with a specified purpose from the database.\n        :param purpose: str, The purpose of the agents to be loaded. It is used to identify the agents in the database.\n        :return: A list of all agents with the specified purpose in the database.\n        \"\"\"\n        return self.persistence.load_all_agents_by_purpose(purpose)\n\n    def load_all_working_agents_by_purpose(self, purpose):\n        \"\"\"\n        Loads all working agents with a specified purpose from the database.\n        :param purpose: str, The purpose of the agents to be loaded. It is used to identify the agents in the database.\n        :return: A list of all working agents"}
{"namespace": "agent_persistence_manager.AgentPersistenceManager.load_all_agents", "completion": ""}
{"namespace": "agent_lifecycle.AgentLifecycle.save_agent", "completion": "        \n        try:\n            self.agent_persistence.save_agent(agent)\n        except Exception as e:\n            logger.error(f\"Error while saving agent {agent.id} with error {e}\")\n            raise e\n"}
{"namespace": "microagent_manager.MicroAgentManager.get_agents", "completion": "        \n        self.cleanup_agents()\n        return self.agent_lifecycle.agents\n"}
{"namespace": "agent_lifecycle.AgentLifecycle._generate_llm_prompt", "completion": "        # Generate prompt\n        prompt = f\"{PROMPT_ENGINEERING_SYSTEM_PROMPT} {PROMPT_ENGINEERING_TEMPLATE} {goal} {sample_input}\"\n        logger.info(f\"Generated prompt: {prompt}\")\n\n        # Get chat completion\n        try:\n            response = self.openai_wrapper.get_chat_completion(prompt)\n        except Exception as e:\n            logger.exception(f\"Error in getting chat completion: {e}\")\n            return \"\"\n\n        # Return prompt\n        return response"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.save_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            conn.execute(\"INSERT OR REPLACE INTO agents VALUES(?, ?, ?)\", (agent_dict['id'], agent_dict['purpose'], json.dumps(agent_dict['data'])))\n            conn.commit()"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.fetch_agent", "completion": "        with sqlite3.connect(self.filename) as conn:\n            result = conn.execute(\"SELECT data FROM agents WHERE id = ?\", (purpose,)).fetchone()\n            if result is None:\n                return None\n            else:\n                return json.loads(result[0])"}
{"namespace": "sqlite_agent_persistence.SQLiteAgentPersistence.load_all_purposes", "completion": "        with sqlite3.connect(self.filename) as conn:\n            cursor = conn.cursor()\n            cursor.execute(\"SELECT purpose FROM agents\")\n            return [row[0] for row in cursor.fetchall()]"}
{"namespace": "memoize.SQLiteMemoization._fetch_from_cache", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\"SELECT result FROM cache WHERE hash=?\", (arg_hash,))\n        result = cursor.fetchone()\n        if result is not None:\n            return json.loads(result[0])\n        else:\n            return None\n"}
{"namespace": "memoize.SQLiteMemoization._cache_result", "completion": "        cursor = self.connection.cursor()\n        cursor.execute(\n            \"INSERT OR REPLACE INTO cache (hash, result) VALUES (?, ?)\",\n            (arg_hash, json.dumps(result)),\n        )\n        self.connection.commit()"}
{"namespace": "run.execute_command_line_process", "completion": "    # Update global configuration parameters with the provided arguments\n    CONFIG.update(args.__dict__)\n\n    # Redirect standard output to a file if quiet mode is enabled\n    if quiet_mode:\n        with open('output.txt', 'w') as f:\n            with redirect_stdout(f):\n                CommandLine().execute()\n    else:\n        CommandLine().execute()\n\n"}
{"namespace": "XAgent.ai_functions.request.openai.chatcompletion_request", "completion": ""}
{"namespace": "litdata.streaming.client.S3Client.client", "completion": "  # noqa: E501\n        if self._client is None:\n            self._create_client()\n        if self._last_time is None or time() - self._last_time > self._refetch_interval:\n            self._create_client()\n        return self._client"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.state_dict", "completion": "  # noqa: E501\n\n        if _is_in_dataloader_worker():\n            raise RuntimeError(\n                \"The `state_dict` method should not be called from a DataLoader worker process. \"\n                \"Please ensure that you are not calling this method from a DataLoader worker process.\"\n            )\n\n        state = {\n            \"num_samples_yielded\": num_samples_yielded,\n            \"num_workers\": num_workers,\n            \"batch_size\": batch_size,\n            \"current_epoch\": self.current_epoch,\n            \"input_dir\": self.input_dir,\n            \"item_loader\": self.item_loader,\n            \"shuffle\": self.shuffle,\n            \"drop_last\": self.drop_last,\n            \"seed\": self.seed,\n            \"world_size\": self.distributed_env.world_size,\n            \"serializers\": self.serializers,\n            \"max_cache_size\": self.max_cache_size,\n        }\n\n        if self._state_dict is not None:\n            state[\"current_epoch\"] = self._state_dict[\"current_epoch\"]\n            state[\"input_dir\"] = self._state_dict[\"input_dir\"]\n            state[\"item_loader\"] = self._state_dict[\"item_loader\"]\n            state[\"shuffle\"] = self._state_dict[\"shuffle\"]\n            state[\"drop_last\"] = self._state_dict[\"drop_last\"]\n            state[\"seed\"] = self._state_dict[\"seed\"]\n            state[\"world_size\"] = self._state_dict[\"world_size\"]\n            state[\"serializers\"] = self._state_dict[\"serializers\"]\n            state[\"max_cache_size\"] = self._state_dict[\"max_cache_size\"]\n\n        return state\n\n    def _validate_state_dict(self) -> None:\n        if self._state_dict is None:\n            return\n\n        state: Dict[str, Any] = self._state_dict\n\n        if state[\"current_epoch\"] != self.current_epoch:\n            raise ValueError(\n                f\"The current epoch of the dataset is {self.current_epoch}, but the state dict has a different value of {state['current_epoch']}.\"\n            )\n\n        if state[\"input_dir\"] != self.input_dir:\n            raise ValueError(\n                f\"The input directory of the dataset is {self.input_dir}, but the state dict has a different value of {state['input_dir']}.\"\n            )\n\n        if state[\"item_loader\"] != self.item_loader:\n            raise ValueError(\n                f\"The item loader of the dataset is {self.item_loader}, but the state dict has a different value of {state['"}
{"namespace": "litdata.streaming.dataset.StreamingDataset.load_state_dict", "completion": "  # noqa: E501\n\n        if _is_in_dataloader_worker():\n            raise RuntimeError(\"The method `load_state_dict` should only be called in the main process.\")\n\n        if self._state_dict is not None:\n            raise RuntimeError(\"The state dictionary has already been loaded.\")\n\n        self._state_dict = state_dict\n\n        self.current_epoch = state_dict[\"current_epoch\"]\n        self.num_workers = state_dict[\"num_workers\"]\n        self.batch_size = state_dict[\"batch_size\"]\n        self.num_samples_yielded = state_dict[\"num_samples_yielded\"]\n        self.input_dir_path = state_dict[\"input_dir_path\"]\n        self.input_dir_url = state_dict[\"input_dir_url\"]\n        self.item_loader = state_dict[\"item_loader\"]\n        self.drop_last = state_dict[\"drop_last\"]\n        self.seed = state_dict[\"seed\"]\n        self.world_size = state_dict[\"world_size\"]\n        self.shuffle = state_dict[\"shuffle\"]\n\n        self.cache = self._create_cache(worker_env=_WorkerEnv.detect())\n        self.shuffler = self._create_shuffler(self.cache)\n\n        self.worker_chunks = []\n        self.worker_intervals = []\n        self.current_indexes = []\n        self.chunk_index = 0\n        self.num_chunks = None\n        self.global_index = 0\n        self.index = 0\n        self.has_triggered_download = False\n        self.min_items_per_replica = None\n\n    def _validate_state_dict(self) -> None:\n        if self._state_dict is None:\n            raise RuntimeError(\"The state dictionary has not been loaded.\")\n\n        if self._state_dict[\"input_dir_path\"] != self.input_dir.path:\n            raise RuntimeError(\n                f\"The state dictionary was loaded with a different input directory. \"\n                f\"Expected: {self.input_dir.path}. Found: {self._state_dict['input_dir_path']}\"\n            )\n\n        if self._state_dict[\"input_dir_url\"] != self.input_dir.url:\n            raise RuntimeError(\n                f\"The state dictionary was loaded with a different input directory. \"\n                f\"Expected: {self.input_dir.url}. Found: {self._state_dict['input_dir_url']}\"\n            )\n\n        if self._state_dict[\"item_loader\"] != self.item_loader.state_dict():\n            raise RuntimeError(\n                f\"The state dictionary was loaded with a different item loader. \"\n                f\"Expected: {self.item_loader.state_dict()}. Found: {self._state_dict['"}
{"namespace": "litdata.streaming.dataset.StreamingDataset._validate_state_dict", "completion": "        if self._state_dict is None:\n            raise ValueError(\"The state dictionary is not set. Please call the `state_dict` method to set the state dictionary.\")\n\n        if self._state_dict[\"input_dir_path\"] != self.input_dir.path:\n            raise ValueError(\n                f\"The input directory path in the state dictionary ({self._state_dict['input_dir_path']}) does not match the current input directory path ({self.input_dir.path}).\"\n            )\n\n        if self._state_dict[\"input_dir_url\"] != self.input_dir.url:\n            raise ValueError(\n                f\"The input directory URL in the state dictionary ({self._state_dict['input_dir_url']}) does not match the current input directory URL ({self.input_dir.url}).\"\n            )\n\n        if self._state_dict[\"item_loader\"] != self.item_loader.state_dict():\n            raise ValueError(\n                f\"The item loader state in the state dictionary ({self._state_dict['item_loader']}) does not match the current item loader state ({self.item_loader.state_dict()}).\"\n            )\n\n        if self._state_dict[\"drop_last\"] != self.drop_last:\n            raise ValueError(\n                f\"The drop_last flag in the state dictionary ({self._state_dict['drop_last']}) does not match the current drop_last flag ({self.drop_last}).\"\n            )\n\n        if self._state_dict[\"seed\"] != self.seed:\n            raise ValueError(\n                f\"The seed in the state dictionary ({self._state_dict['seed']}) does not match the current seed ({self.seed}).\"\n            )\n\n        if self._state_dict[\"shuffle\"] != self.shuffle:\n            raise ValueError(\n                f\"The shuffle flag in the state dictionary ({self._state_dict['shuffle']}) does not match the current shuffle flag ({self.shuffle}).\"\n            )\n\n        if self._state_dict[\"num_workers\"] != self.distributed_env.world_size:\n            raise ValueError(\n                f\"The number of workers in the state dictionary ({self._state_dict['num_workers']}) does not match the current number of workers ({self.distributed_env.world_size}).\"\n            )\n\n        if self._state_dict[\"batch_size\"] != self.distributed_env.batch_size:\n            raise ValueError(\n                f\"The batch size in the state dictionary ({self._state_dict['batch_size']}) does not match the current batch size ({self.distributed_env.batch_size}).\"\n            )\n\n        if self._state_dict[\"current_epoch\"] != self.current_epoch:\n           "}
{"namespace": "litdata.streaming.dataset._try_create_cache_dir", "completion": "  # noqa: E501\n\n    if input_dir is None:\n        input_dir = \"\"\n\n    if not input_dir:\n        return None\n\n    if not os.path.exists(input_dir):\n        raise ValueError(f\"The provided `input_dir` `{input_dir}` doesn't exist.\")\n\n    if not os.path.isdir(input_dir):\n        raise ValueError(f\"The provided `input_dir` `{input_dir}` is not a directory.\")\n\n    # Create a unique directory name by hashing the input directory\n    cache_dir = hashlib.sha256(input_dir.encode(\"utf-8\")).hexdigest()\n\n    # Create the cache directory in a default location\n    cache_dir = os.path.join(_DEFAULT_CACHE_DIR, cache_dir)\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    return cache_dir\n\n"}
{"namespace": "litdata.streaming.downloader.S3Downloader.download_file", "completion": "  # noqa: E501\n        # Parse the S3 URL\n        parsed_url = parse.urlparse(remote_filepath)\n        if parsed_url.scheme != \"s3\":\n            raise ValueError(\"The remote file path must use the 's3' scheme.\")\n        bucket_name = parsed_url.netloc\n        key = parsed_url.path\n\n        # Check if the local file already exists\n        if os.path.exists(local_filepath):\n            return\n\n        # Acquire the file lock\n        lock_path = os.path.join(self._cache_dir, _INDEX_FILENAME)\n        with FileLock(lock_path, timeout=10):\n            # Download the file using s5cmd (if available)\n            if self._s5cmd_available:\n                self._download_file_with_s5cmd(bucket_name, key, local_filepath)\n            # Download the file using boto3 (if s5cmd is not available)\n            else:\n                self._download_file_with_boto3(bucket_name, key, local_filepath)\n\n    def _download_file_with_s5cmd(self, bucket_name: str, key: str, local_filepath: str) -> None:\n        \"\"\"\n        The function downloads a file from an S3 bucket to a local file path using the s5cmd command-line tool. It first checks if the s5cmd command-line tool is available and if the local file already exists. If the local file does not exist, it attempts to download the file using the s5cmd command-line tool. The function uses a file lock to prevent multiple processes from downloading the same file simultaneously.\n\n        Input-Output Arguments\n        :param self: S3Downloader. An instance of the S3Downloader class.\n        :param bucket_name: str, The name of the S3 bucket where the file is stored.\n        :param key: str, The key of the file to be downloaded.\n        :param local_filepath: str, The local file path where the downloaded file will be saved. It is used to check if the file already exists and to specify the download destination.\n        :return: None. There are no return values, but the function may raise a ValueError if the s5cmd command-line tool is not available or a Timeout exception if the file lock cannot be acquired within the specified timeout.\n        \"\"\"  # noqa: E501\n        # Check if s5cmd is available\n        if not self._s5cmd_available:\n            raise ValueError(\"The s"}
{"namespace": "litdata.streaming.dataset._associate_chunks_to_workers", "completion": "  # noqa: E501\n    assert isinstance(num_workers, int)\n    assert isinstance(worker_env, _WorkerEnv)\n    assert isinstance(chunks_replica, list)\n    assert isinstance(intervals_replica, list)\n\n    # Initialize the dictionaries to store the chunks and intervals assigned to each worker.\n    chunks_per_worker = {worker_env.rank: [] for _ in range(num_workers)}\n    intervals_per_worker = {worker_env.rank: [] for _ in range(num_workers)}\n\n    # Distribute the chunks and intervals among the workers based on the worker environment.\n    for i, (chunk_index, interval) in enumerate(zip(chunks_replica, intervals_replica)):\n        # Distribute the chunk to the worker with the lowest index.\n        if i % num_workers == 0:\n            chunks_per_worker[0].append(chunk_index)\n            intervals_per_worker[0].append(interval)\n        else:\n            chunks_per_worker[i % num_workers].append(chunk_index)\n            intervals_per_worker[i % num_workers].append(interval)\n\n    return chunks_per_worker, intervals_per_worker\n\n"}
{"namespace": "litdata.streaming.downloader.LocalDownloaderWithCache.download_file", "completion": "  # noqa: E501\n        if remote_filepath.startswith(\"local:\"):\n            remote_filepath = remote_filepath[6:]\n\n        super().download_file(remote_filepath, local_filepath)"}
{"namespace": "litdata.streaming.serializers.PILSerializer.serialize", "completion": "  # noqa: E501\n\n        if not isinstance(item, Image):\n            raise TypeError(\"The PILSerializer can only serialize PIL Image objects.\")\n\n        if not isinstance(item.mode, str):\n            raise TypeError(\"The PILSerializer can only serialize PIL Image objects with a mode.\")\n\n        if not isinstance(item.size, tuple):\n            raise TypeError(\"The PILSerializer can only serialize PIL Image objects with a size.\")\n\n        if not isinstance(item.mode, str):\n            raise TypeError(\"The PILSerializer can only serialize PIL Image objects with a mode.\")\n\n        if not isinstance(item.mode, str):\n            raise TypeError(\"The PILSerializer can only serialize PIL Image objects with a mode.\")\n\n        if not isinstance(item.mode, str):\n            raise TypeError(\"The PILSerializer can only serialize PIL Image objects with a mode.\")\n\n        if not isinstance(item.mode, str):\n            raise TypeError(\"The PILSerializer can only serialize PIL Image objects with a mode.\")\n\n        if not isinstance(item.mode, str):\n            raise TypeError(\"The PILSerializer can only serialize PIL Image objects with a mode.\")\n\n        if not isinstance(item.mode, str):\n            raise TypeError(\"The PILSerializer can only serialize PIL Image objects with a mode.\")\n\n        if not isinstance(item.mode, str):\n            raise TypeError(\"The PILSerializer can only serialize PIL Image objects with a mode.\")\n\n        if not isinstance(item.mode, str):\n            raise TypeError(\"The PILSerializer can only serialize PIL Image objects with a mode.\")\n\n        if not isinstance(item.mode, str):\n            raise TypeError(\"The PILSerializer can only serialize PIL Image objects with a mode.\")\n\n        if not isinstance(item.mode, str):\n            raise TypeError(\"The PILSerializer can only serialize PIL Image objects with a mode.\")\n\n        if not isinstance(item.mode, str):\n            raise TypeError(\"The PILSerializer can only serialize PIL Image objects with a mode.\")\n\n        if not isinstance(item.mode, str):\n            raise TypeError(\"The PILSerializer can only serialize PIL Image objects with a mode.\")\n\n        if not isinstance(item.mode, str):\n            raise TypeError(\"The PILSerializer can only serialize PIL Image objects with a mode.\")\n\n        if not isinstance(item.mode, str):\n            raise TypeError(\"The PILSerializer can only serialize PIL Image objects with a mode.\")\n\n        if not isinstance(item.mode, str):\n            raise TypeError(\"The PILSerializer can only serialize PIL Image objects with a mode.\")\n\n        if not isinstance(item.mode, str):\n            raise TypeError(\"The PILSerializer can only serialize PIL Image objects with a mode.\")\n\n       "}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.serialize", "completion": "  # noqa: E501\n\n        if isinstance(item, JpegImageFile):\n            return item.tobytes(), None\n\n        if not isinstance(item, Image.Image):\n            raise TypeError(\"JPEGSerializer can only serialize Image.Image objects.\")\n\n        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\"JPEGSerializer requires torchvision to be installed.\")\n\n        if not isinstance(item, (JpegImageFile, PngImageFile, WebPImageFile)):\n            raise TypeError(\"JPEGSerializer can only serialize Image.Image objects.\")\n\n        if isinstance(item, PngImageFile):\n            return decode_jpeg(item).tobytes(), None\n\n        if isinstance(item, WebPImageFile):\n            return decode_jpeg(item).tobytes(), None\n\n        return pil_to_tensor(item).numpy().tobytes(), None\n\n    @classmethod\n    def deserialize(cls, data: bytes) -> Any:\n        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\"JPEGSerializer requires torchvision to be installed.\")\n\n        if not isinstance(data, bytes):\n            raise TypeError(\"JPEGSerializer can only deserialize bytes.\")\n\n        if not isinstance(data, np.ndarray):\n            data = np.frombuffer(data, dtype=np.uint8)\n\n        if data.dtype != np.uint8:\n            raise TypeError(\"JPEGSerializer can only deserialize bytes of dtype np.uint8.\")\n\n        if data.ndim != 3:\n            raise TypeError(\"JPEGSerializer can only deserialize 3-dimensional arrays.\")\n\n        if data.shape[2] != 3:\n            raise TypeError(\"JPEGSerializer can only deserialize 3-dimensional arrays with 3 channels.\")\n\n        return decode_jpeg(io.BytesIO(data))\n\n    def can_serialize(self, item: Any) -> bool:\n        return bool(_TORCH_VISION_AVAILABLE) and isinstance(item, Image.Image) and not isinstance(item, JpegImageFile)\n\n"}
{"namespace": "litdata.streaming.serializers.PILSerializer.deserialize", "completion": "  # noqa: E501\n        ints = np.frombuffer(data[:12], dtype=np.uint32)\n        mode = data[12 : 12 + ints[2]]\n        raw = data[12 + ints[2] :]\n        return Image.frombytes(mode, (ints[0], ints[1]), raw)\n\n    def can_serialize(self, data: Any) -> bool:\n        return isinstance(data, Image)\n\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.deserialize", "completion": "  # noqa: E501\n        dtype_indice = np.frombuffer(data[:4], dtype=np.uint32)[0]\n        shape = np.frombuffer(data[4:8], dtype=np.uint32)\n        tensor = torch.from_numpy(np.frombuffer(data[8:], dtype=np.uint8))\n        return torch.from_numpy(tensor).to(dtype=_TORCH_DTYPES_MAPPING[dtype_indice])\n\n    def can_serialize(self, item: torch.Tensor) -> bool:\n        return isinstance(item, torch.Tensor)\n\n"}
{"namespace": "litdata.streaming.serializers.TensorSerializer.serialize", "completion": "  # noqa: E501\n\n        if not isinstance(item, torch.Tensor):\n            raise TypeError(f\"The provided item should be of type {torch.Tensor}. Found {item}.\")\n\n        dtype = item.dtype\n        if dtype not in _TORCH_DTYPES_MAPPING:\n            raise ValueError(f\"Unsupported dtype {dtype}.\")\n\n        indices = self._dtype_to_indices[dtype]\n        shape = item.shape\n        raw = item.detach().cpu().numpy().tobytes()\n        return indices.tobytes() + shape.tobytes() + raw, None\n\n    def deserialize(self, data: bytes) -> torch.Tensor:\n        \"\"\"\n        Deserializes a bytes object containing the dtype, shape, and raw data of a PyTorch tensor. The function returns a PyTorch tensor with the same dtype, shape, and raw data as the serialized tensor.\n\n        Input-Output Arguments\n        :param data: bytes, the serialized representation of a PyTorch tensor, including its dtype, shape, and raw data.\n        :return: A PyTorch tensor with the same dtype, shape, and raw data as the serialized tensor.\n        \"\"\"  # noqa: E501\n\n        if not isinstance(data, bytes):\n            raise TypeError(f\"The provided data should be of type {bytes}. Found {data}.\")\n\n        dtype = torch.from_numpy(np.frombuffer(data[:4], dtype=np.uint32))\n        shape = torch.from_numpy(np.frombuffer(data[4:], dtype=np.uint32))\n        raw = torch.from_numpy(np.frombuffer(data[4 + shape.numel() * 4 :], dtype=np.uint8))\n        return torch.from_numpy(raw).to(dtype)\n\n    def can_serialize(self, item: Any) -> bool:\n        return isinstance(item, torch.Tensor)\n\n"}
{"namespace": "litdata.streaming.serializers.JPEGSerializer.deserialize", "completion": "  # noqa: E501\n        if _TORCH_VISION_AVAILABLE:\n            try:\n                return decode_jpeg(data)\n            except Exception:\n                pass\n\n        if _PIL_AVAILABLE:\n            return pil_to_tensor(Image.open(io.BytesIO(data)))\n\n        raise TypeError(f\"The provided data should be of type bytes. Found {type(data)}.\")\n\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.serialize", "completion": "  # noqa: E501\n        dtype_indice = self._dtype_to_indices[self._dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.numpy().tobytes(order=\"C\"))\n        return b\"\".join(data), f\"no_header_tensor:{dtype_indice}\"\n\n    def deserialize(self, data: bytes) -> torch.Tensor:\n        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        tensor = torch.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = torch.Size(shape)\n        if tensor.shape == shape:\n            return tensor\n        return torch.reshape(tensor, shape)\n\n    def can_serialize(self, item: torch.Tensor) -> bool:\n        return isinstance(item, torch.Tensor) and type(item) == torch.Tensor and len(item.shape) > 1\n\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderTensorSerializer.deserialize", "completion": "  # noqa: E501\n        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _TORCH_DTYPES_MAPPING[dtype_indice]\n        tensor = torch.frombuffer(data[4:], dtype=dtype)\n        return tensor\n\n    def can_serialize(self, item: torch.Tensor) -> bool:\n        return isinstance(item, torch.Tensor) and type(item) == torch.Tensor and len(item.shape) > 1\n\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.deserialize", "completion": "  # noqa: E501\n        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        data = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if data.shape == shape:\n            return data\n        return np.reshape(data, shape)\n\n    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and len(item.shape) > 1\n\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.deserialize", "completion": "  # noqa: E501\n        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n\n    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and type(item) == np.ndarray and len(item.shape) == 1\n\n"}
{"namespace": "litdata.streaming.serializers.NoHeaderNumpySerializer.serialize", "completion": "  # noqa: E501\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        return item.tobytes(order=\"C\"), f\"no_header_numpy:{dtype_indice}\"\n\n    def deserialize(self, data: bytes) -> np.ndarray:\n        \"\"\"\n        Deserializes a bytes object into a NumPy array and generates a corresponding dtype identifier string. This method is designed for scenarios where header information is not required or desired in the serialized output.\n\n        Input-Output Arguments\n        :param self: NoHeaderNumpySerializer. An instance of the NoHeaderNumpySerializer class.\n        :param data: bytes, The bytes object to be deserialized. It is used to convert the bytes object into a NumPy array and to determine the data type for generating the dtype identifier.\n        :return: np.ndarray, The deserialized NumPy array.\n        \"\"\"  # noqa: E501\n        assert self._dtype\n        return np.frombuffer(data, dtype=self._dtype)\n\n    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and type(item) == np.ndarray and len(item.shape) == 1\n\n"}
{"namespace": "litdata.streaming.serializers.NumpySerializer.serialize", "completion": "  # noqa: E501\n        dtype_indice = self._dtype_to_indices[item.dtype]\n        data = [np.uint32(dtype_indice).tobytes()]\n        data.append(np.uint32(len(item.shape)).tobytes())\n        for dim in item.shape:\n            data.append(np.uint32(dim).tobytes())\n        data.append(item.tobytes(order=\"C\"))\n        return b\"\".join(data), None\n\n    def deserialize(self, data: bytes) -> np.ndarray:\n        \"\"\"\n        Deserializes a bytes object into a NumPy array, including metadata about the array's data type and shape, and returns it.\n        Input-Output Arguments\n        :param data: bytes, the bytes object to be deserialized, used to extract the data type, shape, and the array's binary content.\n        :return: np.ndarray, the deserialized NumPy array.\n        \"\"\"  # noqa: E501\n        dtype_indice = np.frombuffer(data[0:4], np.uint32).item()\n        dtype = _NUMPY_DTYPES_MAPPING[dtype_indice]\n        shape_size = np.frombuffer(data[4:8], np.uint32).item()\n        shape = []\n        for shape_idx in range(shape_size):\n            shape.append(np.frombuffer(data[8 + 4 * shape_idx : 8 + 4 * (shape_idx + 1)], np.uint32).item())\n        array = np.frombuffer(data[8 + 4 * (shape_idx + 1) : len(data)], dtype=dtype)\n        shape = tuple(shape)\n        if array.shape == shape:\n            return array\n        return np.reshape(array, shape)\n\n    def can_serialize(self, item: np.ndarray) -> bool:\n        return isinstance(item, np.ndarray) and len(item.shape) > 1\n\n"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.state_dict", "completion": "  # noqa: E501\n\n        state_dict = {\n            \"dataset\": self.dataset.state_dict(),\n            \"current_epoch\": self.current_epoch,\n            \"num_samples_yielded\": self._num_samples_yielded_streaming,\n            \"latest_worker_idx\": self._latest_worker_idx,\n        }\n\n        if isinstance(self.dataset, StreamingDataset):\n            state_dict[\"num_samples_yielded_combined\"] = self._num_samples_yielded_combined\n\n        return state_dict\n\n    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:\n\n        \"\"\"\n        This function loads the state of the StreamingDataLoader instance from a state dictionary. The state dictionary is generated by the state_dict() function. The structure of the state dictionary varies depending on whether the dataset is an instance of StreamingDataset.\n\n        Input-Output Arguments\n        :param self: StreamingDataLoader. An instance of the StreamingDataLoader class. It is used to access the dataset, batch size, number of workers, current epoch, and other attributes to construct the state dictionary.\n        :param state_dict: Dict[str, Any]. A dictionary representing the state of the StreamingDataLoader instance. The keys include \"dataset\", \"current_epoch\", \"num_samples_yielded\", and \"latest_worker_idx\". The structure of the \"dataset\" part of the dictionary depends on whether the dataset is a StreamingDataset or not.\n        \"\"\"  # noqa: E501\n\n        if isinstance(self.dataset, StreamingDataset):\n            self._num_samples_yielded_combined = state_dict[\"num_samples_yielded_combined\"]\n\n        self.dataset.load_state_dict(state_dict[\"dataset\"])\n        self.current_epoch = state_dict[\"current_epoch\"]\n        self._num_samples_yielded_streaming = state_dict[\"num_samples_yielded\"]\n        self._latest_worker_idx = state_dict[\"latest_worker_idx\"]\n\n    def __len__(self) -> int:\n        return len(self.dataset)\n\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}(dataset={self.dataset}, batch_size={self.batch_size}, num_workers={self.num_workers})\""}
{"namespace": "litdata.streaming.serializers.VideoSerializer.deserialize", "completion": "  # noqa: E501\n        if not _AV_AVAILABLE:\n            raise ImportError(\"The VideoSerializer requires the av library to be installed.\")\n\n        if not _TORCH_VISION_AVAILABLE:\n            raise ImportError(\"The VideoSerializer requires the torchvision library to be installed.\")\n\n        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n            f.write(data)\n            f.flush()\n            video = decode_video(f.name)\n        return video\n\n    def can_serialize(self, data: Any) -> bool:\n        return isinstance(data, str) and os.path.isfile(data) and data.split(\".\")[-1] in self._EXTENSIONS\n\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.done", "completion": "  # noqa: E501\n        if self._is_done:\n            return []\n\n        if self._worker_env is None:\n            self._worker_env = _WorkerEnv.detect()\n\n        if self._worker_env is None:\n            raise RuntimeError(\"The worker environment is not detected. Please make sure to run the script with `--worker_env`.\")\n\n        if self._worker_env.rank == 0:\n            self.write_chunks_index()\n\n        return [os.path.join(self._cache_dir, f\"chunk-{self.rank}-{self._chunk_index}.{self._compression}.bin\")]\n\n    def __del__(self):\n        if self._is_done:\n            return\n        self.done()"}
{"namespace": "litdata.streaming.dataloader.StreamingDataLoader.load_state_dict", "completion": "  # noqa: E501\n\n        if not isinstance(self.dataset, (StreamingDataset, CombinedStreamingDataset)):\n            raise RuntimeError(\n                \"The provided dataset should be either an instance of StreamingDataset or CombinedStreamingDataset.\"\n                f\" Found {self.dataset}.\"\n            )\n\n        if isinstance(self.dataset, StreamingDataset):\n            self.dataset.load_state_dict(obj[\"dataset\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._num_samples_yielded_streaming = obj[\"num_samples_yielded\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n        else:\n            self.dataset.load_state_dict(obj[\"dataset\"], self.num_workers, self.batch_size, obj[\"num_samples_yielded\"])\n            self.current_epoch = obj[\"current_epoch\"]\n            self._latest_worker_idx = obj[\"latest_worker_idx\"]\n\n        self.restore = True\n\n    def __len__(self) -> int:\n        return len(self.dataset)\n\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}(dataset={self.dataset}, batch_size={self.batch_size}, num_workers={self.num_workers})\""}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.state_dict", "completion": "  # noqa: E501\n        if self._iterator is None:\n            if num_samples_yielded is None:\n                return {}\n            else:\n                return self._iterator.state_dict(num_workers, batch_size, num_samples_yielded)\n        else:\n            return self._iterator.state_dict(num_workers, batch_size, num_samples_yielded)\n\n    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:\n        \"\"\"\n        The function `load_state_dict` loads the state dictionary of the CombinedStreamingDataset instance. It either loads the state dictionary from the internal datasets or from the internal iterator's state dictionary, depending on whether the iterator is None.\n\n        Input-Output Arguments\n        :param self: CombinedStreamingDataset. An instance of the CombinedStreamingDataset class.\n        :param state_dict: Dict[str, Any], A dictionary representing the state of the CombinedStreamingDataset instance.\n        :return: None\n        \"\"\"  # noqa: E501\n        if self._iterator is None:\n            self._iterator = _CombinedDatasetIterator(\n                self._datasets,\n                self._seed,\n                self._weights,\n                self._use_streaming_dataloader,\n                self._num_samples_yielded,\n            )\n        self._iterator.load_state_dict(state_dict)\n\n    def __len__(self) -> int:\n        return sum(self._weights)\n\n    def __repr__(self) -> str:\n        return f\"CombinedStreamingDataset({self._datasets})\"\n\n"}
{"namespace": "litdata.streaming.combined.CombinedStreamingDataset.load_state_dict", "completion": "  # noqa: E501\n        if self._iterator is None:\n            self._num_samples_yielded = state_dict.get(__NUM_SAMPLES_YIELDED_KEY__, None)\n            self._datasets = state_dict.get(__SAMPLES_KEY__, None)\n            if self._num_samples_yielded is not None:\n                self._num_samples_yielded = [int(n) for n in self._num_samples_yielded]\n            if self._datasets is not None:\n                self._datasets = [StreamingDataset(d) for d in self._datasets]\n        else:\n            self._iterator.load_state_dict(state_dict)\n\n    def __len__(self) -> int:\n        return len(self._datasets)\n\n    def __repr__(self) -> str:\n        return f\"CombinedStreamingDataset({self._datasets})\"\n\n"}
{"namespace": "litdata.streaming.resolver._resolve_dir", "completion": "  # noqa: E501\n\n    if dir_path is None:\n        return Dir()\n\n    if isinstance(dir_path, Dir):\n        return dir_path\n\n    if isinstance(dir_path, str):\n        if dir_path.startswith(\"s3://\"):\n            return Dir(path=dir_path, url=dir_path)\n        elif dir_path.startswith(\"https://\"):\n            return Dir(path=dir_path, url=dir_path)\n        elif dir_path.startswith(\"http://\"):\n            return Dir(path=dir_path, url=dir_path)\n        elif dir_path.startswith(\"file://\"):\n            return Dir(path=dir_path[7:], url=dir_path)\n        elif dir_path.startswith(\"lightning://\"):\n            return Dir(path=dir_path[11:], url=dir_path)\n        elif dir_path.startswith(\"lightning://project/\"):\n            return Dir(path=dir_path[16:], url=dir_path)\n        elif dir_path.startswith(\"lightning://project/\"):\n            return Dir(path=dir_path[16:], url=dir_path)\n        elif dir_path.startswith(\"lightning://project/\"):\n            return Dir(path=dir_path[16:], url=dir_path)\n        elif dir_path.startswith(\"lightning://project/\"):\n            return Dir(path=dir_path[16:], url=dir_path)\n        elif dir_path.startswith(\"lightning://project/\"):\n            return Dir(path=dir_path[16:], url=dir_path)\n        elif dir_path.startswith(\"lightning://project/\"):\n            return Dir(path=dir_path[16:], url=dir_path)\n        elif dir_path.startswith(\"lightning://project/\"):\n            return Dir(path=dir_path[16:], url=dir_path)\n        elif dir_path.startswith(\"lightning://project/\"):\n            return Dir(path=dir_path[16:], url=dir_path)\n        elif dir_path.startswith(\"lightning://project/\"):\n            return Dir(path=dir_path[16:], url=dir_path)\n        elif dir_path.startswith(\"lightning://project/\"):\n            return Dir(path=dir_path[16:], url=dir_path)\n        elif dir_path.startswith(\"lightning://project/\"):\n            return Dir(path=dir_path[16:], url=dir_path)\n        elif dir_path.startswith(\"lightning://project/\"):\n            return Dir(path=dir_path[16:], url=dir_path)\n        elif dir_path.startswith(\""}
{"namespace": "litdata.streaming.resolver._assert_dir_is_empty", "completion": "  # noqa: E501\n\n    if not isinstance(output_dir, Dir):\n        raise ValueError(f\"`output_dir` must be an instance of the Dir class, got: {output_dir}\")\n\n    if not output_dir.url.startswith(\"s3://\"):\n        raise ValueError(f\"`output_dir` must start with `s3://`, got: {output_dir}\")\n\n    if output_dir.path is not None:\n        if os.path.exists(output_dir.path):\n            raise ValueError(f\"`output_dir` already exists, got: {output_dir}\")\n\n    if output_dir.url is not None:\n        s3 = boto3.resource(\"s3\")\n        bucket = s3.Bucket(output_dir.url.split(\"/\")[2])\n        if bucket.objects.all():\n            raise ValueError(f\"`output_dir` already exists, got: {output_dir}\")\n\n"}
{"namespace": "litdata.streaming.resolver._assert_dir_has_index_file", "completion": "  # noqa: E501\n\n    if not isinstance(output_dir, Dir):\n        raise ValueError(\"The provided output_dir isn't a Dir Object.\")\n\n    if output_dir.url is None:\n        return\n\n    obj = parse.urlparse(output_dir.url)\n\n    if obj.scheme != \"s3\":\n        raise ValueError(f\"The provided folder should start with s3://. Found {output_dir.path}.\")\n\n    s3 = boto3.client(\"s3\")\n\n    objects = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n    )\n\n    # We aren't alloweing to add more data\n    # TODO: Add support for `append` and `overwrite`.\n    if objects[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains data and datasets are meant to be immutable.\"\n            \" HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n    # Check if the directory contains an index file\n    index_file = s3.list_objects_v2(\n        Bucket=obj.netloc,\n        Delimiter=\"/\",\n        Prefix=obj.path.lstrip(\"/\").rstrip(\"/\") + \"/\",\n        MaxKeys=1,\n    )\n\n    if index_file[\"KeyCount\"] > 0:\n        raise RuntimeError(\n            f\"The provided output_dir `{output_dir.path}` already contains an index file. \"\n            \"Datasets are meant to be immutable. \"\n            \"HINT: Did you consider changing the `output_dir` with your own versioning as a suffix?\"\n        )\n\n"}
{"namespace": "litdata.streaming.writer.BinaryWriter.merge", "completion": "  # noqa: E501\n        if node_rank is None:\n            node_rank = self.rank\n\n        if node_rank == 0:\n            # Wait for all index files to be available\n            while True:\n                files = os.listdir(self._cache_dir)\n                index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n                if len(index_files) == num_workers:\n                    break\n                sleep(1)\n\n            # Merge the index files\n            index_files = sorted(index_files)\n            merged_index = os.path.join(self._cache_dir, f\"{self.rank}.{_INDEX_FILENAME}\")\n            with open(merged_index, \"w\") as out:\n                for index_file in index_files:\n                    with open(os.path.join(self._cache_dir, index_file), \"r\") as in_file:\n                        json_data = json.load(in_file)\n                    out.write(json.dumps(json_data, sort_keys=True))\n                    out.write(\"\\n\")\n\n            # Remove the individual index files\n            for index_file in index_files:\n                os.remove(os.path.join(self._cache_dir, index_file))\n\n            # Remove the merged index file if it's not the master node\n            if node_rank != 0:\n                os.remove(merged_index)\n\n        else:\n            # Wait for the merged index file to be available\n            while True:\n                files = os.listdir(self._cache_dir)\n                index_files = [f for f in files if f.endswith(_INDEX_FILENAME)]\n                if len(index_files) == 1:\n                    break\n                sleep(1)\n\n            # Remove the individual index files\n            for index_file in index_files:\n                os.remove(os.path.join(self._cache_dir, index_file))\n\n    def _pretty_serialized_items(self) -> str:\n        \"\"\"Pretty print the serialized items.\"\"\"\n        if self._min_index is None:\n            return \"None\"\n        return f\"Items: {self._min_index} to {self._max_index}\""}
{"namespace": "litdata.streaming.resolver._execute", "completion": "  # noqa: E501\n\n    if not _LIGHTNING_SDK_AVAILABLE:\n        raise RuntimeError(\"The lightning_sdk module is not available.\")\n\n    if machine is None:\n        machine = Machine()\n\n    if command is None:\n        command = f\"cd {os.getcwd()} && {os.environ['LIGHTNING_CLOUD_PROJECT_ID']}\"\n\n    studio = Studio(\n        name=name,\n        num_nodes=num_nodes,\n        machine=machine,\n        command=command,\n    )\n\n    studio.execute()\n\n"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.delete", "completion": "  # noqa: E501\n        for chunk_index in chunk_indexes:\n            self._chunks_index_to_be_deleted.append(chunk_index)\n\n    def run(self) -> None:\n        \"\"\"Runs the thread.\"\"\"\n        while not self._has_exited:\n            try:\n                chunk_index = self._to_download_queue.get(timeout=_DEFAULT_TIMEOUT)\n            except Empty:\n                continue\n\n            if self._pre_download_counter >= self._max_pre_download:\n                self._pre_download_counter = 0\n                self._download_chunks(chunk_index)\n            else:\n                self._pre_download_counter += 1\n\n    def _download_chunks(self, chunk_index: int) -> None:\n        \"\"\"Downloads the chunks associated to the given chunk index.\"\"\"\n        if self._distributed_env.is_master():\n            self._download_chunk(chunk_index)\n        else:\n            self._distributed_env.download_chunk(chunk_index)\n\n    def _download_chunk(self, chunk_index: int) -> None:\n        \"\"\"Downloads the chunk associated to the given chunk index.\"\"\"\n        chunk = self._item_loader.load_chunk(chunk_index)\n        if chunk is None:\n            return\n\n        if self._distributed_env.is_master():\n            self._distributed_env.download_chunk(chunk_index, chunk)\n        else:\n            self._distributed_env.download_chunk(chunk_index, chunk)\n\n    def _delete_chunks(self) -> None:\n        \"\"\"Deletes the chunks associated to the chunk indices in the deletion queue.\"\"\"\n        if self._distributed_env.is_master():\n            for chunk_index in self._chunks_index_to_be_deleted:\n                self._item_loader.delete_chunk(chunk_index)\n        else:\n            self._distributed_env.delete_chunk(chunk_index)\n\n    def _delete_chunks_when_processed(self) -> None:\n        \"\"\"Deletes the chunks associated to the chunk indices in the deletion queue.\"\"\"\n        if self._distributed_env.is_master():\n            for chunk_index in self._chunks_index_to_be_deleted:\n                self._item_loader.delete_chunk(chunk_index)\n        else:\n            self._distributed_env.delete_chunk(chunk_index)\n\n    def _delete_chunks_when_finished(self) -> None:\n        \"\"\"Deletes the chunks associated to the chunk indices in the deletion queue.\"\"\"\n        if self._distributed_env.is_master():\n            for chunk_index in self._chunks_index_to_be_deleted:\n                self._item_loader.delete_chunk(chunk_index)\n        else:\n            self._distributed_env.delete_chunk(chunk_index)\n\n    def _delete_chunks_when_finished(self) -> None:\n        \"\"\"Deletes the chunks associated to the chunk indices in"}
{"namespace": "litdata.streaming.reader.BinaryReader._try_load_config", "completion": "  # noqa: E501\n        if not os.path.exists(self._cache_dir):\n            return None\n\n        # Load the config containing the index\n        config_path = os.path.join(self._cache_dir, \"chunks_config.json\")\n        if not os.path.exists(config_path):\n            return None\n\n        with open(config_path, \"r\") as f:\n            config = ChunksConfig.from_json(f.read())\n\n        # Update the instance's configuration\n        self._config = config\n        self._rank = self._distributed_env.rank\n        self._intervals = config._intervals\n        return config\n\n    def _get_chunk_index_from_index_file(self, index: int) -> int:\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        return self._config._get_chunk_index_from_index_file(index)  # type: ignore\n\n    def _get_chunk_index_from_index_file(self, index: int) -> int:\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        return self._config._get_chunk_index_from_index_file(index)  # type: ignore\n\n    def _get_chunk_index_from_index_file(self, index: int) -> int:\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        return self._config._get_chunk_index_from_index_file(index)  # type: ignore\n\n    def _get_chunk_index_from_index_file(self, index: int) -> int:\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        return self._config._get_chunk_index_from_index_file(index)  # type: ignore\n\n    def _get_chunk_index_from_index_file(self, index: int) -> int:\n        # Load the config containing the index\n        if self._config is None and self._try_load_config() is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        return self._config._get_chunk_index_from_index_file(index)  # type:"}
{"namespace": "litdata.streaming.reader.PrepareChunksThread.download", "completion": "  # noqa: E501\n        for chunk_index in chunk_indexes:\n            self._to_download_queue.put(chunk_index)\n\n    def _delete_chunks(self, chunk_indexes: List[int]) -> None:\n        \"\"\"\n        The delete_chunks function takes a list of chunk indices and enqueues them into a delete queue for processing. This is typically used to delete chunks of data in a multi-threaded or asynchronous environment, where each chunk is identified by its index.\n\n        Input-Output Arguments\n        :param self: PrepareChunksThread. An instance of the PrepareChunksThread class, which should have access to a queue attribute for storing the chunk indices to be deleted.\n        :param chunk_indexes: List[int]. A list of integers where each integer represents the index of a chunk that needs to be deleted. These indices are used to identify and enqueue the specific chunks for deletion.\n        :return: No return values. This method modifies the state of the _to_delete_queue attribute by adding the chunk indices to it but does not return any value.\n        \"\"\"  # noqa: E501\n        for chunk_index in chunk_indexes:\n            self._to_delete_queue.put(chunk_index)\n\n    def _get_chunk(self, chunk_index: int) -> Tuple[bytes, int]:\n        \"\"\"\n        The get_chunk function takes a chunk index and returns the chunk data and its size.\n\n        Input-Output Arguments\n        :param self: PrepareChunksThread. An instance of the PrepareChunksThread class, which should have access to a cache directory attribute for storing the chunk data.\n        :param chunk_index: int. An integer representing the index of the chunk that needs to be retrieved.\n        :return: Tuple[bytes, int]. A tuple containing the chunk data and its size.\n        \"\"\"  # noqa: E501\n        chunk_path = os.path.join(self._parent_cache_dir, f\"chunk_{chunk_index}.bin\")\n        if os.path.exists(chunk_path):\n            with open(chunk_path, \"rb\") as f:\n                chunk_data = f.read()\n                chunk_size = len(chunk_data)\n                return chunk_data, chunk_size\n        else:\n            return None, None\n\n    def _get_chunk_from_cache(self, chunk_index: int) -> Tuple[bytes, int]:\n        \"\"\"\n        The get_chunk_from_cache function takes a chunk index and returns the chunk data and its size.\n\n        Input-Output Arguments\n        :param self: Prepare"}
{"namespace": "litdata.streaming.reader.BinaryReader.config", "completion": "  # noqa: E501\n        if self._config is None:\n            raise RuntimeError(\"The reader index isn't defined.\")\n        return self._config\n\n    @property\n    def rank(self) -> int:\n        \"\"\"The rank of the current worker.\"\"\"\n        if self._rank is None:\n            self._rank = self._distributed_env.rank\n        return self._rank\n\n    @property\n    def num_workers(self) -> int:\n        \"\"\"The number of workers.\"\"\"\n        return self._distributed_env.num_workers\n\n    @property\n    def num_nodes(self) -> int:\n        \"\"\"The number of nodes.\"\"\"\n        return self._distributed_env.num_nodes\n\n    @property\n    def num_chunks(self) -> int:\n        \"\"\"The number of chunks.\"\"\"\n        return self._config.num_chunks\n\n    @property\n    def num_bytes(self) -> int:\n        \"\"\"The number of bytes.\"\"\"\n        return self._config.num_bytes\n\n    @property\n    def num_items(self) -> int:\n        \"\"\"The number of items.\"\"\"\n        return self._config.num_items\n\n    @property\n    def num_items_per_chunk(self) -> int:\n        \"\"\"The number of items per chunk.\"\"\"\n        return self._config.num_items_per_chunk\n\n    @property\n    def num_items_per_node(self) -> int:\n        \"\"\"The number of items per node.\"\"\"\n        return self._config.num_items_per_node\n\n    @property\n    def num_items_per_worker(self) -> int:\n        \"\"\"The number of items per worker.\"\"\"\n        return self._config.num_items_per_worker\n\n    @property\n    def num_items_per_chunk_per_worker(self) -> int:\n        \"\"\"The number of items per chunk per worker.\"\"\"\n        return self._config.num_items_per_chunk_per_worker\n\n    @property\n    def num_items_per_chunk_per_node(self) -> int:\n        \"\"\"The number of items per chunk per node.\"\"\"\n        return self._config.num_items_per_chunk_per_node\n\n    @property\n    def num_items_per_chunk_per_worker_per_node(self) -> int:\n        \"\"\"The number of items per chunk per worker per node.\"\"\"\n        return self._config.num_items_per_chunk_per_worker_per_node\n\n    @property\n    def num_items_per_chunk_per_worker_per_node_per_chunk(self) -> int:\n        \"\"\"The number of items per chunk per worker per node per chunk.\"\"\"\n        return self._config.num_items_per_chunk_per_worker_per_node_per"}
{"namespace": "litdata.streaming.reader.BinaryReader.read", "completion": "  # noqa: E501\n\n        if not isinstance(index, ChunkedIndex):\n            raise ValueError(\"The index should be an instance of ChunkedIndex.\")\n\n        if self._config is None:\n            raise Exception(\"The reader index isn't defined.\")\n\n        chunk_index = self._get_chunk_index_from_index(index.index)\n\n        # Check whether the chunk is available locally\n        if self._config.is_chunk_available(chunk_index):\n            return self._item_loader.load_chunk(chunk_index, self._config[chunk_index])\n\n        # Check whether the chunk is available in memory\n        if self._config.is_chunk_in_memory(chunk_index):\n            return self._item_loader.load_chunk(chunk_index, self._config[chunk_index])\n\n        # Check whether the chunk is available in the cache\n        if self._config.is_chunk_in_cache(chunk_index):\n            return self._item_loader.load_chunk(chunk_index, self._config[chunk_index])\n\n        # Check whether the chunk is available in the remote folder\n        if self._remote_input_dir is not None:\n            return self._item_loader.load_chunk(chunk_index, self._config[chunk_index])\n\n        # Check whether the chunk is available in the cache\n        if self._config.is_chunk_in_cache(chunk_index):\n            return self._item_loader.load_chunk(chunk_index, self._config[chunk_index])\n\n        # Check whether the chunk is available in the remote folder\n        if self._remote_input_dir is not None:\n            return self._item_loader.load_chunk(chunk_index, self._config[chunk_index])\n\n        # Check whether the chunk is available in the cache\n        if self._config.is_chunk_in_cache(chunk_index):\n            return self._item_loader.load_chunk(chunk_index, self._config[chunk_index])\n\n        # Check whether the chunk is available in the remote folder\n        if self._remote_input_dir is not None:\n            return self._item_loader.load_chunk(chunk_index, self._config[chunk_index])\n\n        # Check whether the chunk is available in the cache\n        if self._config.is_chunk_in_cache(chunk_index):\n            return self._item_loader.load_chunk(chunk_index, self._config[chunk_index])\n\n        # Check whether the chunk is available in the remote folder\n        if self._remote_input_dir is not None:\n            return self._item_loader.load_chunk(chunk_index, self._config[chunk_index])\n\n        # Check whether the chunk is available in the cache\n        if self._config"}
{"namespace": "litdata.utilities.broadcast.broadcast_object", "completion": "def _get_token() -> str:\n    \"\"\"Get the token from the environment variable `LIGHTNING_APP_TOKEN`.\"\"\"\n    lightning_app_token = os.getenv(\"LIGHTNING_APP_TOKEN\")\n    if lightning_app_token is None:\n        raise RuntimeError(\"The `LIGHTNING_APP_TOKEN` should be set.\")\n    return lightning_app_token"}
{"namespace": "litdata.utilities.shuffle._intra_node_chunk_shuffle", "completion": "    # Get the number of nodes and the world size\n    num_nodes = distributed_env.num_nodes\n    world_size = distributed_env.world_size\n\n    # Get the number of chunks per node\n    num_chunks_per_node = len(chunks_per_ranks[0])\n\n    # Get the number of chunks per rank\n    num_chunks_per_rank = [len(chunk) for chunk in chunks_per_ranks]\n\n    # Get the total number of chunks\n    total_chunks = sum(num_chunks_per_rank)\n\n    # Get the number of chunks to shuffle\n    num_chunks_to_shuffle = total_chunks - world_size\n\n    # Get the number of chunks to shuffle per node\n    num_chunks_to_shuffle_per_node = num_chunks_to_shuffle // num_nodes\n\n    # Get the number of chunks to shuffle per rank\n    num_chunks_to_shuffle_per_rank = [num_chunks_to_shuffle_per_node] * num_nodes\n\n    # Get the number of chunks to shuffle per rank\n    num_chunks_to_shuffle_per_rank = [num_chunks_to_shuffle_per_node] * num_nodes\n\n    # Get the number of chunks to shuffle per rank\n    num_chunks_to_shuffle_per_rank = [num_chunks_to_shuffle_per_node] * num_nodes\n\n    # Get the number of chunks to shuffle per rank\n    num_chunks_to_shuffle_per_rank = [num_chunks_to_shuffle_per_node] * num_nodes\n\n    # Get the number of chunks to shuffle per rank\n    num_chunks_to_shuffle_per_rank = [num_chunks_to_shuffle_per_node] * num_nodes\n\n    # Get the number of chunks to shuffle per rank\n    num_chunks_to_shuffle_per_rank = [num_chunks_to_shuffle_per_node] * num_nodes\n\n    # Get the number of chunks to shuffle per rank\n    num_chunks_to_shuffle_per_rank = [num_chunks_to_shuffle_per_node] * num_nodes\n\n    # Get the number of chunks to shuffle per rank\n    num_chunks_to_shuffle_per_rank = [num_chunks_to_shuffle_per_node] * num_nodes\n\n    # Get the number of chunks to shuffle per rank\n    num_chunks_to_shuffle_per_rank = [num_chunks_to_shuffle_per_node] * num_nodes\n\n    # Get the number of chunks to shuffle per rank\n    num_chunks_to_shuffle_per_rank = [num_chunks_to_shuffle_per_node] * num_nodes\n\n    # Get the number of chunks to shuffle per rank\n    num_chunks_to_shuffle_per_rank = [num_chunks_to_shuffle_per_node"}
{"namespace": "litdata.processing.functions._get_input_dir", "completion": "  # noqa: E501\n\n    if len(inputs) < 2:\n        return None\n\n    indexed_paths = _get_indexed_paths(inputs)\n\n    if len(indexed_paths) != 1:\n        raise ValueError(\n            \"Expected exactly one indexed path, but found {} indexed paths: {}\".format(\n                len(indexed_paths), indexed_paths\n            )\n        )\n\n    input_path = indexed_paths[0]\n\n    if not os.path.exists(input_path):\n        raise ValueError(\"Input path {} does not exist.\".format(input_path))\n\n    return _resolve_dir(input_path)\n\n"}
{"namespace": "litdata.processing.utilities.optimize_dns_context", "completion": "  # noqa: E501\n\n    if enable:\n        os.environ[\"DNS_OPTIMIZATION\"] = \"1\"\n    else:\n        os.environ[\"DNS_OPTIMIZATION\"] = \"0\"\n\n    try:\n        yield\n    finally:\n        os.environ[\"DNS_OPTIMIZATION\"] = \"0\"\n\n"}
{"namespace": "litdata.utilities.shuffle._associate_chunks_and_internals_to_ranks", "completion": "  # noqa: E501\n\n    # calculate the number of items each rank should process\n    num_items_per_rank = int(np.ceil(len(indexes) / distributed_env.world_size))\n    if drop_last:\n        num_items_per_rank -= 1\n\n    # distribute the chunks and their intervals to each rank\n    chunks_per_rank = [[] for _ in range(distributed_env.world_size)]\n    chunk_intervals_per_rank = [[] for _ in range(distributed_env.world_size)]\n    for i in range(len(indexes)):\n        rank = i % distributed_env.world_size\n        chunks_per_rank[rank].append(indexes[i])\n        chunk_intervals_per_rank[rank].append(chunk_intervals[i])\n\n    return chunks_per_rank, chunk_intervals_per_rank"}
{"namespace": "litdata.processing.functions.LambdaDataTransformRecipe.prepare_item", "completion": "  # noqa: E501\n        if self._contains_device:\n            self._device = item_metadata[\"device\"]\n        if self._contains_is_last:\n            item_metadata[\"is_last\"] = is_last\n\n        self._fn(item_metadata, output_dir)\n\n    def get_num_workers(self) -> int:\n        return _get_default_num_workers()\n\n"}
{"namespace": "litdata.processing.data_processor._wait_for_file_to_exist", "completion": "  # noqa: E501\n    while True:\n        try:\n            return s3.head_object(Bucket=obj.netloc, Key=obj.path[1:])\n        except botocore.exceptions.ClientError as e:\n            if e.response[\"Error\"][\"Code\"] == \"404\":\n                logger.info(f\"File {obj.path[1:]} not found in S3 bucket {obj.netloc}.\")\n                sleep(sleep_time)\n            else:\n                raise e\n\n"}
{"namespace": "litdata.processing.functions.optimize", "completion": "  # noqa: E501\n    if isinstance(inputs, StreamingDataLoader) and batch_size is not None:\n        raise ValueError(\"When providing a streaming dataloader, pass the batch_size to the dataloader directly.\")\n\n    if not isinstance(inputs, (Sequence, StreamingDataLoader)):\n        raise ValueError(f\"The provided inputs should be non empty sequence or a streaming dataloader. Found {inputs}.\")\n\n    if len(inputs) == 0:\n        raise ValueError(f\"The provided inputs should be non empty. Found {inputs}.\")\n\n    if not _IS_IN_STUDIO and (machine is not None or num_nodes is not None):\n        raise ValueError(\n            \"Only https://lightning.ai/ supports multiple nodes or selecting a machine.\"\n            \" Create an account to try it out.\"\n        )\n\n    if not _IS_IN_STUDIO:\n        print(\n            \"Create an account on https://lightning.ai/ to transform your data faster using \"\n            \"multiple nodes and large machines.\"\n        )\n\n    if num_nodes is None or int(os.getenv(\"DATA_OPTIMIZER_NUM_NODES\", 0)) > 0:\n        _output_dir: Dir = _resolve_dir(output_dir)\n\n        if _output_dir.url and \"cloudspaces\" in _output_dir.url:\n            raise ValueError(\n                f\"The provided `output_dir` isn't valid. Found {_output_dir.path if _output_dir else None}.\"\n                \" HINT: You can either use `/teamspace/s3_connections/...` or `/teamspace/datasets/...`.\"\n            )\n\n        if error_when_not_empty:\n            _assert_dir_is_empty(_output_dir)\n\n        if not isinstance(inputs, StreamingDataLoader):\n            input_dir = _resolve_dir(_get_input_dir(inputs))\n\n            if isinstance(batch_size, int) and batch_size > 1:\n                inputs = [inputs[pos : pos + batch_size] for pos in range(0, len(inputs), batch_size)]\n        else:\n            input_dir = Dir()\n\n        data_processor = DataProcessor(\n            input_dir=input_dir,\n            output_dir=_output_dir,\n            num_workers=num_workers or _get_default_num_workers(),\n            fast_dev_run=fast_dev_run,\n            num_downloaders=num_downloaders,\n            num_uploaders=num_uploaders,\n            reorder_files=reorder_files,\n            weights=weights,\n            reader=reader,\n        )\n        with optimize_dns_context(True):\n            return data_processor.run(LambdaData"}
{"namespace": "litdata.processing.functions.map", "completion": "  # noqa: E501\n\n    if not _IS_IN_STUDIO:\n        raise ValueError(\"This function is only available on https://lightning.ai/.\")\n\n    if not isinstance(fn, FunctionType):\n        raise ValueError(f\"The provided {fn} isn't supported.\")\n\n    if not isinstance(inputs, Sequence):\n        raise ValueError(f\"The provided {inputs} isn't supported.\")\n\n    if not isinstance(output_dir, (str, Dir)):\n        raise ValueError(f\"The provided {output_dir} isn't supported.\")\n\n    if not isinstance(fast_dev_run, (bool, int)):\n        raise ValueError(f\"The provided {fast_dev_run} isn't supported.\")\n\n    if not isinstance(num_nodes, int):\n        raise ValueError(f\"The provided {num_nodes} isn't supported.\")\n\n    if not isinstance(machine, str):\n        raise ValueError(f\"The provided {machine} isn't supported.\")\n\n    if not isinstance(num_downloaders, int):\n        raise ValueError(f\"The provided {num_downloaders} isn't supported.\")\n\n    if not isinstance(num_uploaders, int):\n        raise ValueError(f\"The provided {num_uploaders} isn't supported.\")\n\n    if not isinstance(reorder_files, bool):\n        raise ValueError(f\"The provided {reorder_files} isn't supported.\")\n\n    if not isinstance(error_when_not_empty, bool):\n        raise ValueError(f\"The provided {error_when_not_empty} isn't supported.\")\n\n    if not isinstance(reader, BaseReader):\n        raise ValueError(f\"The provided {reader} isn't supported.\")\n\n    if not isinstance(batch_size, int):\n        raise ValueError(f\"The provided {batch_size} isn't supported.\")\n\n    if not isinstance(weights, (list, tuple, set)):\n        raise ValueError(f\"The provided {weights} isn't supported.\")\n\n    if not isinstance(num_workers, int):\n        num_workers = _get_default_num_workers()\n\n    if not isinstance(num_workers, int):\n        raise ValueError(f\"The provided {num_workers} isn't supported.\")\n\n    if num_workers < 1:\n        raise ValueError(f\"The provided {num_workers} isn't supported.\")\n\n    if num_workers > os.cpu_count():\n        raise ValueError(f\"The provided {num_workers} is greater than the number of available CPUs.\")\n\n    if num_workers > torch.cuda.device_count():\n        raise ValueError(f\"The provided {num_workers} is greater than the number of available GPUs.\")\n\n    if num_workers > 1 and not _TORCH_GREATER_EQUAL_2_1_0:\n        raise ValueError(\"This function requires"}
{"namespace": "litdata.processing.data_processor._download_data_target", "completion": "    s3 = S3Client()\n    while True:\n        try:\n            index, paths = queue_in.get(timeout=1)\n        except Empty:\n            continue\n\n        for path in paths:\n            if not os.path.exists(os.path.join(cache_dir, path.lstrip(\"/\"))):\n                try:\n                    _wait_for_file_to_exist(s3, parse.urlparse(input_dir.url + path.lstrip(\"/\")))\n                except Exception as e:\n                    logger.error(f\"Error while downloading file {path}: {e}\")\n                    continue\n\n                try:\n                    s3.client.download_file(input_dir.url, path.lstrip(\"/\"), os.path.join(cache_dir, path.lstrip(\"/\")))\n                except Exception as e:\n                    logger.error(f\"Error while downloading file {path}: {e}\")\n                    continue\n\n        queue_out.put(index)\n\n"}
{"namespace": "litdata.processing.data_processor._upload_fn", "completion": "    s3 = S3Client()\n\n    while True:\n        # 1. Fetch from the queue\n        r: Optional[Tuple[int, List[str]]] = upload_queue.get()\n\n        # 2. Terminate the process if we received a termination signal\n        if r is None:\n            remove_queue.put(None)\n            return\n\n        # 3. Unpack\n        index, paths = r\n\n        # 4. Check whether all the files are already uploaded\n        if output_dir.path and all(\n            os.path.exists(p.replace(output_dir.path, cache_dir) if output_dir else p) for p in paths\n        ):\n            remove_queue.put(index)\n            continue\n\n        # 5. Upload all the required paths to unblock the current index\n        for path in paths:\n            if output_dir.path:\n                local_path = path.replace(output_dir.path, cache_dir)\n\n            if output_dir.url and output_dir.path:\n                path = path.replace(output_dir.path, output_dir.url)\n\n            obj = parse.urlparse(path)\n\n            if obj.scheme == \"s3\":\n                dirpath = os.path.dirname(local_path)\n\n                os.makedirs(dirpath, exist_ok=True)\n\n                with open(local_path, \"rb\") as f:\n                    s3.client.upload_fileobj(f, obj.netloc, obj.path.lstrip(\"/\"))\n\n            elif os.path.isfile(path):\n                if not path.startswith(\"/teamspace/studios/this_studio\"):\n                    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n                    shutil.copyfile(path, local_path)\n            else:\n                raise ValueError(f\"The provided {output_dir.url} isn't supported.\")\n\n        # 6. Inform the worker the current files are available\n        remove_queue.put(index)\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_weighted", "completion": "    # Associate the items to the workers based on number of nodes and node rank.\n    worker_items = [worker_items[i] for i in worker_ids_this_node]\n    worker_weights = [worker_weights[i] for i in worker_ids_this_node]\n\n    # Shuffle the items\n    random.shuffle(worker_items)\n\n    # Calculate the total number of items\n    total_items = sum(worker_weights)\n\n    # Calculate the total number of items per worker\n    items_per_worker = total_items / num_workers\n\n    # Calculate the number of items left over\n    extra_items = total_items % num_workers\n\n    # Calculate the number of items per worker\n    items_per_worker = items_per_worker + 1 if extra_items > 0 else items_per_worker\n\n    # Calculate the number of items per worker\n    start = 0\n    result = []\n    for i in range(num_workers):\n        end = start + items_per_worker\n        result.append(worker_items[start:end])\n        start = end\n\n    if len(result) != num_workers:\n        raise RuntimeError(\"Improper assignment of items to workers\")\n\n    # Print the distribution details for workers on the current node\n    if file_size:\n        print(f\"Items per worker: {items_per_worker} (MB)\")\n    else:\n        print(f\"Items per worker: {items_per_worker}\")\n\n    return result\n\n"}
{"namespace": "litdata.processing.data_processor._map_items_to_workers_sequentially", "completion": "    # Calculate the total number of workers across all nodes\n    total_workers = _get_num_nodes() * num_workers\n\n    # Calculate the number of items each worker should process\n    num_items_per_worker = len(user_items) // total_workers\n\n    # Calculate the number of items each worker should process, accounting for any remainder\n    num_items_per_worker += len(user_items) % total_workers\n\n    # Calculate the start and end indices for each worker's items\n    start_indices = np.cumsum([0] + [num_items_per_worker] * (total_workers - 1))\n    end_indices = start_indices[1:] + num_items_per_worker\n\n    # Ensure the output list has a length equal to the number of workers\n    if len(start_indices) != total_workers:\n        raise RuntimeError(\"Improper assignment of items to workers.\")\n\n    # Distribute items to workers\n    worker_items = []\n    for i in range(total_workers):\n        worker_items.append(user_items[start_indices[i] : end_indices[i]])\n\n    return worker_items\n\n"}
{"namespace": "litdata.processing.data_processor.DataProcessor._cleanup_cache", "completion": ""}
{"namespace": "litdata.processing.data_processor._get_item_filesizes", "completion": "  # noqa: E501\n    for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n        item_sizes.append(future.result())\n\n    return item_sizes\n\n"}
{"namespace": "litdata.processing.data_processor._is_path", "completion": "def _get_item_paths(items: List[Any], base_path: str = \"\") -> List[str]:\n    \"\"\"Computes the total size in bytes of all file paths for every datastructure in the given list.\"\"\"\n    item_paths = []\n\n    cpu_count = os.cpu_count() or 1\n\n    # Parallelize to accelerate retrieving the number of file bytes to read for each item\n    with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count * 2 if cpu_count > 4 else cpu_count) as executor:\n        futures = [executor.submit(_to_path, element) for element in items]\n        for future in futures:\n            item_paths.append(future.result())\n    return item_paths\n\n\ndef _get_item_sizes(items: List[Any], base_path: str = \"\") -> List[int]:\n    \"\"\"Computes the total size in bytes of all file paths for every datastructure in the given list.\"\"\"\n    item_sizes = []\n\n    cpu_count = os.cpu_count() or 1\n\n    # Parallelize to accelerate retrieving the number of file bytes to read for each item\n    with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count * 2 if cpu_count > 4 else cpu_count) as executor:\n        futures = [executor.submit(_get_num_bytes, item, base_path) for item in items]\n        for future in futures:\n            item_sizes.append(future.result())\n    return item_sizes\n\n\ndef _get_item_filepaths(items: List[Any], base_path: str = \"\") -> List[str]:\n    \"\"\"Computes the total size in bytes of all file paths for every datastructure in the given list.\"\"\"\n    item_filepaths = []\n\n    cpu_count = os.cpu_count() or 1\n\n    # Parallelize to accelerate retrieving the number of file bytes to read for each item\n    with concurrent.futures.ThreadPoolExecutor(max_workers=cpu_count * 2 if cpu_count > 4 else cpu_count) as executor:\n        futures = [executor.submit(_to_path, element) for element in items]\n        for future in futures:\n            item_filepaths.append(future.result())\n    return item_filepaths\n\n\ndef _get_item_filepaths_and_sizes(items: List[Any], base_path: str = \"\") -> List[Tuple[str, int]]:\n    \"\"\"Computes the total size in bytes of all file paths for every datastructure in the given list.\"\"\"\n    item_filepaths_and_sizes = []\n\n    cpu_count = os.cpu_count()"}
{"namespace": "internal.utils.network_factory.NetworkFactory.get_network", "completion": "  # noqa: E501\n\n        assert n_layers > 0, \"The number of layers must be greater than 0.\"\n        assert n_neurons > 0, \"The number of neurons must be greater than 0.\"\n\n        if self.tcnn:\n            from tinycudann import create_network\n\n            return create_network(\n                n_input_dims=n_input_dims,\n                n_output_dims=n_output_dims,\n                n_layers=n_layers,\n                n_neurons=n_neurons,\n                activation=activation,\n                output_activation=output_activation,\n            )\n        else:\n            return nn.Sequential(\n                nn.Linear(n_input_dims, n_neurons),\n                nn.ReLU(),\n                *[nn.Linear(n_neurons, n_neurons) for _ in range(n_layers - 1)],\n                nn.ReLU(),\n                nn.Linear(n_neurons, n_output_dims),\n                nn.Sigmoid() if output_activation == \"Sigmoid\" else nn.ReLU(),\n            )"}
{"namespace": "iris.nodes.geometry_refinement.smoothing.Smoothing._rolling_median", "completion": "  # noqa: E501\n        # Trim the signal to remove edge effects\n        signal = signal[kernel_offset:-kernel_offset]\n\n        # Compute the median of the signal shifted by the kernel offset\n        signal_shifted = np.roll(signal, kernel_offset)\n        signal_shifted = signal_shifted[kernel_offset:-kernel_offset]\n        median = np.median(signal_shifted)\n\n        # Compute the rolling median\n        rolling_median = np.zeros(len(signal) - 2 * kernel_offset)\n        for i in range(len(rolling_median)):\n            rolling_median[i] = median\n\n        return rolling_median"}
{"namespace": "iris.nodes.matcher.utils.hamming_distance", "completion": "  # noqa: E501\n\n    if not isinstance(template_probe, IrisTemplate) or not isinstance(template_gallery, IrisTemplate):\n        raise MatcherError(\"The template_probe and template_gallery must be IrisTemplate objects.\")\n\n    if not isinstance(rotation_shift, int):\n        raise MatcherError(\"The rotation_shift must be an integer.\")\n\n    if not isinstance(nm_dist, (int, float)) and nm_dist is not None:\n        raise MatcherError(\"The nm_dist must be an integer or float.\")\n\n    if weights is not None and not isinstance(weights, list):\n        raise MatcherError(\"The weights must be a list of numpy arrays.\")\n\n    if weights is not None and len(weights) != len(template_probe.code):\n        raise MatcherError(\"The weights must be a list of numpy arrays with the same length as the template_probe code.\")\n\n    if weights is not None and len(weights) != len(template_gallery.code):\n        raise MatcherError(\"The weights must be a list of numpy arrays with the same length as the template_gallery code.\")\n\n    if weights is not None and len(weights) != len(template_probe.code):\n        raise MatcherError(\"The weights must be a list of numpy arrays with the same length as the template_probe code.\")\n\n    if weights is not None and len(weights) != len(template_gallery.code):\n        raise MatcherError(\"The weights must be a list of numpy arrays with the same length as the template_gallery code.\")\n\n    if weights is not None and len(weights) != len(template_probe.code):\n        raise MatcherError(\"The weights must be a list of numpy arrays with the same length as the template_probe code.\")\n\n    if weights is not None and len(weights) != len(template_gallery.code):\n        raise MatcherError(\"The weights must be a list of numpy arrays with the same length as the template_gallery code.\")\n\n    if weights is not None and len(weights) != len(template_probe.code):\n        raise MatcherError(\"The weights must be a list of numpy arrays with the same length as the template_probe code.\")\n\n    if weights is not None and len(weights) != len(template_gallery.code):\n        raise MatcherError(\"The weights must be a list of numpy arrays with the same length as the template_gallery code.\")\n\n    if weights is not None and len(weights) != len(template_probe.code):\n        raise MatcherError(\"The weights must be a list of numpy arrays with the same length as the template_probe code.\")\n\n    if weights is not None and len(weights)"}
{"namespace": "iris.nodes.eye_properties_estimation.bisectors_method.BisectorsMethod._calculate_perpendicular_bisectors", "completion": "  # noqa: E501\n        # Initialize the arrays to store the starting and ending points of the perpendicular bisectors\n        first_bisectors_point = np.zeros((self.params.num_bisectors, 2))\n        second_bisectors_point = np.zeros((self.params.num_bisectors, 2))\n\n        # Initialize the counter for the number of iterations\n        iteration_count = 0\n\n        # Iterate until the maximum number of iterations is reached or a sufficient number of point pairs are found\n        while iteration_count < self.params.max_iterations:\n            # Generate random indices for the vertices of the polygon\n            random_indices = np.random.randint(0, polygon.shape[0], size=self.params.num_bisectors)\n\n            # Calculate the perpendicular bisectors for each pair of points\n            for i in range(self.params.num_bisectors):\n                # Calculate the starting point of the bisector\n                first_bisectors_point[i, :] = polygon[random_indices[i], :]\n\n                # Calculate the ending point of the bisector\n                second_bisectors_point[i, :] = self._calculate_bisector_end_point(\n                    first_bisectors_point[i, :], polygon, min_distance_between_sector_points_in_px\n                )\n\n            # Check if the distance between the starting and ending points of the bisectors is greater than the minimum distance\n            if self._check_distance_between_bisectors(first_bisectors_point, second_bisectors_point):\n                # If the distance criterion is met, break the loop and return the bisectors\n                break\n            else:\n                # If the distance criterion is not met, increment the iteration counter and continue the loop\n                iteration_count += 1\n\n        # If the distance criterion is not met after the maximum number of iterations, raise an exception\n        if iteration_count == self.params.max_iterations:\n            raise EyeCentersEstimationError(\n                f\"Failed to find a sufficient number of point pairs that meet the distance criterion within the maximum number of iterations allowed. This indicates that it may not be possible to accurately estimate the center of the shape.\"\n            )\n\n        return first_bisectors_point, second_bisectors_point\n\n    def _calculate_bisector_end_point(\n        self, first_bisector_point: np.ndarray, polygon: np.ndarray, min_distance_between_sector_points_in_px: float\n    ) -> np.ndarray:\n        \"\"\"Calculate the end point of a perpendicular bis"}
{"namespace": "iris.io.class_configs.Algorithm.execute", "completion": ""}
{"namespace": "tanuki.validator.Validator.validate_output", "completion": "        try:\n            output = json.loads(output)\n        except json.JSONDecodeError:\n            return False\n\n        return self.check_type(output, type_definition)\n"}
{"namespace": "tanuki.register.Register.load_function_description", "completion": "        # Get the function's signature and type hints\n        signature = inspect.signature(func_object)\n        type_hints = get_type_hints(func_object)\n\n        # Get the function's name and docstring\n        func_name = func_object.__name__\n        docstring = inspect.getdoc(func_object)\n\n        # Get the input and output type hints\n        input_type_hints = {key: value for key, value in type_hints.items() if key in signature.parameters}\n        output_type_hints = {key: value for key, value in type_hints.items() if key not in signature.parameters}\n\n        # Get the class definitions for the input and output types\n        input_class_definitions = {key: get_class_definition(value) for key, value in input_type_hints.items()}\n        output_class_definitions = {key: get_class_definition(value) for key, value in output_type_hints.items()}\n\n        # Determine the function type based on the output type hint\n        if issubclass(output_type_hints[\"return\"], Embedding):\n            function_type = FunctionType.EMBEDDABLE\n        else:\n            function_type = FunctionType.SYMBOLIC\n\n        # Create the function description\n        function_description = FunctionDescription(\n            name=func_name,\n            docstring=docstring,\n            input_type_hints=input_type_hints,\n            output_type_hints=output_type_hints,\n            input_class_definitions=input_class_definitions,\n            output_class_definitions=output_class_definitions,\n            function_type=function_type\n        )\n\n        return function_description\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.add", "completion": "        hash1, hash2 = self.hash_functions(string)\n        for seed in range(self.hash_count):\n            index = (hash1 + seed * hash2) % self.size\n\n            #print(f\"Add: Seed={seed}, Digest={index}, BitValue={self.bit_array[index]}\")\n            self.bit_array[index] = 1\n            self.indices[index] = seed\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.load", "completion": "        try:\n            self.bit_array = self.persistence.load()\n            if len(self.bit_array) != self.size:\n                logging.warning(\"Bit array length does not match expected length. Reinitializing...\")\n                self.bit_array, self.indices = self.init_bit_array(self.size)\n                self.save()\n        except Exception as e:\n            logging.warning(f\"Failed to load bit array from persistence. Reinitializing...\")\n            self.bit_array, self.indices = self.init_bit_array(self.size)\n            self.save()\n"}
{"namespace": "tanuki.bloom_filter.BloomFilter.lookup", "completion": "        hash1, hash2 = self.hash_functions(string)\n        index1 = hash1 % self.size\n        index2 = hash2 % self.size\n\n        return self.bit_array[index1] and self.bit_array[index2]\n"}
{"namespace": "tanuki.models.function_config.FunctionConfig.load_from_dict", "completion": "        self.distilled_model = config_factory.create_model_config(json_dict[\"distilled_model\"])\n        self.current_model_stats = json_dict[\"current_model_stats\"]\n        self.last_training_run = json_dict[\"last_training_run\"]\n        self.current_training_run = json_dict[\"current_training_run\"]\n        self.nr_of_training_runs = json_dict[\"nr_of_training_runs\"]\n        self.teacher_models = [config_factory.create_model_config(teacher_model_name) for teacher_model_name in json_dict[\"teacher_models\"] if teacher_model_name in DEFAULT_TEACHER_MODEL_NAMES]"}
{"namespace": "tanuki.language_models.openai_api.OpenAI_API.generate", "completion": "        self.check_api_key()\n\n        # check for valid model\n        if model.model_name is None:\n            raise ValueError(\"model.model_name is None\")\n\n        # check for valid model name\n        if model.model_name not in self.get_available_models():\n            raise ValueError(f\"model.model_name {model.model_name} is not a valid model name\")\n\n        # check for valid model name\n        if model.model_name not in self.get_available_models():\n            raise ValueError(f\"model.model_name {model.model_name} is not a valid model name\")\n\n        # check for valid model name\n        if model.model_name not in self.get_available_models():\n            raise ValueError(f\"model.model_name {model.model_name} is not a valid model name\")\n\n        # check for valid model name\n        if model.model_name not in self.get_available_models():\n            raise ValueError(f\"model.model_name {model.model_name} is not a valid model name\")\n\n        # check for valid model name\n        if model.model_name not in self.get_available_models():\n            raise ValueError(f\"model.model_name {model.model_name} is not a valid model name\")\n\n        # check for valid model name\n        if model.model_name not in self.get_available_models():\n            raise ValueError(f\"model.model_name {model.model_name} is not a valid model name\")\n\n        # check for valid model name\n        if model.model_name not in self.get_available_models():\n            raise ValueError(f\"model.model_name {model.model_name} is not a valid model name\")\n\n        # check for valid model name\n        if model.model_name not in self.get_available_models():\n            raise ValueError(f\"model.model_name {model.model_name} is not a valid model name\")\n\n        # check for valid model name\n        if model.model_name not in self.get_available_models():\n            raise ValueError(f\"model.model_name {model.model_name} is not a valid model name\")\n\n        # check for valid model name\n        if model.model_name not in self.get_available_models():\n            raise ValueError(f\"model.model_name {model.model_name} is not a valid model name\")\n\n        # check for valid model name\n        if model.model_name not in self.get_available_models():\n            raise ValueError(f\"model.model_name {model.model_name} is not a valid model name\")\n\n        # check for valid model name\n        if model.model_name not in self.get_available_models():\n            raise ValueError(f\"model.model_name"}
{"namespace": "skfolio.utils.stats.assert_is_symmetric", "completion": "    if x.ndim != 2 or x.shape[0] != x.shape[1]:\n        raise ValueError(\"The matrix must be square\")\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix is not symmetric\")\n\n"}
{"namespace": "skfolio.utils.stats.assert_is_distance", "completion": "    assert_is_square(x)\n    assert_is_symmetric(x)\n    if not np.allclose(x, x.T):\n        raise ValueError(\"The matrix must be symmetric\")\n    if not np.allclose(x.diagonal(), np.zeros(x.shape[0])):\n        raise ValueError(\"The matrix must be a distance matrix\")\n\n"}
{"namespace": "tanuki.language_models.language_model_manager.LanguageModelManager.get_generation_case", "completion": "        # get the model type\n        model_type = function_description.model_type\n        # get the model name\n        model_name = function_description.model_name\n        # get the model\n        model = self.api_provider[model_type].get_model(model_name)\n        # get the model parameters\n        model_parameters = function_description.model_parameters\n        # get the model parameters\n        model_parameters = function_description.model_parameters\n        # get the model parameters\n        model_parameters = function_description.model_parameters\n        # get the model parameters\n        model_parameters = function_description.model_parameters\n        # get the model parameters\n        model_parameters = function_description.model_parameters\n        # get the model parameters\n        model_parameters = function_description.model_parameters\n        # get the model parameters\n        model_parameters = function_description.model_parameters\n        # get the model parameters\n        model_parameters = function_description.model_parameters\n        # get the model parameters\n        model_parameters = function_description.model_parameters\n        # get the model parameters\n        model_parameters = function_description.model_parameters\n        # get the model parameters\n        model_parameters = function_description.model_parameters\n        # get the model parameters\n        model_parameters = function_description.model_parameters\n        # get the model parameters\n        model_parameters = function_description.model_parameters\n        # get the model parameters\n        model_parameters = function_description.model_parameters\n        # get the model parameters\n        model_parameters = function_description.model_parameters\n        # get the model parameters\n        model_parameters = function_description.model_parameters\n        # get the model parameters\n        model_parameters = function_description.model_parameters\n        # get the model parameters\n        model_parameters = function_description.model_parameters\n        # get the model parameters\n        model_parameters = function_description.model_parameters\n        # get the model parameters\n        model_parameters = function_description.model_parameters\n        # get the model parameters\n        model_parameters = function_description.model_parameters\n        # get the model parameters\n        model_parameters = function_description.model_parameters\n        # get the model parameters\n        model_parameters = function_description.model_parameters\n        # get the model parameters\n        model_parameters = function_description.model_parameters\n        # get the model parameters\n        model_parameters = function_description.model_parameters\n        # get the model parameters\n        model_parameters = function_description.model_parameters\n        # get the model parameters\n        model_parameters = function_description.model_parameters\n        # get the model parameters\n        model_parameters = function_description.model_parameters"}
{"namespace": "skfolio.utils.stats.cov_nearest", "completion": "  # noqa: E501\n    if cov.ndim != 2:\n        raise ValueError(f\"`cov` must be a 2D array, got a {cov.ndim}D array\")\n    if not np.allclose(np.diag(cov), np.zeros(cov.shape[0]), atol=_CLIPPING_VALUE):\n        raise ValueError(\n            \"The distance matrix must have diagonal elements close to zeros\"\n        )\n    if higham:\n        cov = _higham(cov)\n    else:\n        cov = _clip(cov)\n    return cov\n\n"}
{"namespace": "skfolio.datasets._base.clear_data_home", "completion": "    data_home = get_data_home(data_home)\n    shutil.rmtree(data_home)\n\n"}
{"namespace": "detectron2.export.flatten.flatten_to_tuple", "completion": "    if isinstance(obj, str):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, bytes):\n        return (obj,), IdentitySchema()\n    elif isinstance(obj, list):\n        return flatten_to_tuple(obj), ListSchema([IdentitySchema() for _ in range(len(obj))])\n    elif isinstance(obj, tuple):\n        return flatten_to_tuple(obj), TupleSchema([IdentitySchema() for _ in range(len(obj))])\n    elif isinstance(obj, dict):\n        return flatten_to_tuple(obj), DictSchema([IdentitySchema() for _ in range(len(obj))])\n    elif isinstance(obj, Instances):\n        return flatten_to_tuple(obj), InstancesSchema()\n    elif isinstance(obj, Boxes):\n        return flatten_to_tuple(obj), TensorWrapSchema(\"Boxes\")\n    elif isinstance(obj, ROIMasks):\n        return flatten_to_tuple(obj), TensorWrapSchema(\"ROIMasks\")\n    else:\n        raise ValueError(f\"Unsupported type {type(obj)} for flattening.\")"}
{"namespace": "skfolio.utils.equations.equations_to_matrix", "completion": "  # noqa: E501\n\n    # Check input arguments\n    if not isinstance(groups, np.ndarray):\n        groups = np.array(groups)\n    if not isinstance(equations, np.ndarray):\n        equations = np.array(equations)\n    if not isinstance(sum_to_one, bool):\n        raise TypeError(f\"sum_to_one must be a bool, not {type(sum_to_one)}\")\n    if not isinstance(raise_if_group_missing, bool):\n        raise TypeError(f\"raise_if_group_missing must be a bool, not {type(raise_if_group_missing)}\")\n    if not isinstance(names, tuple):\n        raise TypeError(f\"names must be a tuple, not {type(names)}\")\n    if len(names) != 2:\n        raise ValueError(f\"names must be a tuple of length 2, not {len(names)}\")\n    if not all(isinstance(name, str) for name in names):\n        raise TypeError(f\"names must be a tuple of strings, not {type(names)}\")\n    if not all(name in (\"groups\", \"equations\") for name in names):\n        raise ValueError(f\"names must be a tuple of ('groups', 'equations'), not {names}\")\n\n    # Check that the groups and equations are of the same length\n    if len(groups) != len(equations):\n        raise ValueError(\n            f\"The number of groups ({len(groups)}) and equations ({len(equations)}) must be the same.\"\n        )\n\n    # Check that the groups are unique\n    if len(np.unique(groups, axis=0)) != len(groups):\n        raise ValueError(\n            f\"The groups must be unique. The following groups are not unique: {np.unique(groups, axis=0)}\"\n        )\n\n    # Check that the equations are unique\n    if len(np.unique(equations, axis=0)) != len(equations):\n        raise ValueError(\n            f\"The equations must be unique. The following equations are not unique: {np.unique(equations, axis=0)}\"\n        )\n\n    # Check that the equations are valid\n    for equation in equations:\n        if not isinstance(equation, str):\n            raise TypeError(f\"equations must be a list of strings, not {type(equations)}\")\n        if not re.match(r\"^\\s*([+-])?\\s*([0-9.]+)\\s*([+-])?\\s*([0-9.]+)\\s"}
{"namespace": "detectron2.export.torchscript_patch.patch_instances", "completion": "  # noqa: E501\n    global _counter\n    _counter += 1\n    cls_name = f\"Instances_{_counter}\"\n    cls = type(cls_name, (Instances,), fields)\n    cls.__module__ = \"detectron2.structures\"\n    cls.__qualname__ = cls_name\n    cls.__qualname__ = cls.__qualname__.replace(\".\", \"_\")\n    cls.__name__ = cls_name\n    cls.__doc__ = \"A class for storing instances of objects in an image.\"\n    cls.__module__ = \"detectron2.structures\"\n    cls.__qualname__ = cls.__qualname__.replace(\".\", \"_\")\n    cls.__name__ = cls.__name__.replace(\".\", \"_\")\n    cls.__doc__ = cls.__doc__.replace(\".\", \"_\")\n    cls.__module__ = \"detectron2.structures\"\n    cls.__qualname__ = cls.__qualname__.replace(\".\", \"_\")\n    cls.__name__ = cls.__name__.replace(\".\", \"_\")\n    cls.__doc__ = cls.__doc__.replace(\".\", \"_\")\n    cls.__module__ = \"detectron2.structures\"\n    cls.__qualname__ = cls.__qualname__.replace(\".\", \"_\")\n    cls.__name__ = cls.__name__.replace(\".\", \"_\")\n    cls.__doc__ = cls.__doc__.replace(\".\", \"_\")\n    cls.__module__ = \"detectron2.structures\"\n    cls.__qualname__ = cls.__qualname__.replace(\".\", \"_\")\n    cls.__name__ = cls.__name__.replace(\".\", \"_\")\n    cls.__doc__ = cls.__doc__.replace(\".\", \"_\")\n    cls.__module__ = \"detectron2.structures\"\n    cls.__qualname__ = cls.__qualname__.replace(\".\", \"_\")\n    cls.__name__ = cls.__name__.replace(\".\", \"_\")\n    cls.__doc__ = cls.__doc__.replace(\".\", \"_\")\n    cls.__module__ = \"detectron2.structures\"\n    cls.__qualname__ = cls.__qualname__.replace(\".\", \"_\")\n    cls.__name__ = cls.__name__.replace(\".\", \"_\")\n    cls.__doc__ = cls.__doc__.replace(\".\", \"_\")\n    cls.__module__ = \"detectron2.structures\"\n    cls.__qualname__ = cls.__qualname__.replace(\".\", \"_\")\n    cls.__name__ = cls.__name__.replace(\".\", \"_\")\n    cls.__doc__"}
{"namespace": "detectron2.data.detection_utils.read_image", "completion": "  # noqa: E501\n    with PathManager.open(file_name, \"rb\") as f:\n        image = Image.open(f)\n        image = _apply_exif_orientation(image)\n        image = convert_PIL_to_numpy(image, format)\n    return image\n\n"}
{"namespace": "detectron2.data.detection_utils.transform_instance_annotations", "completion": "  # noqa: E501\n    if \"bbox\" in annotation:\n        annotation[\"bbox\"] = transforms.apply_box(\n            BoxMode.convert(\n                annotation[\"bbox\"],\n                annotation[\"bbox_mode\"],\n                BoxMode.XYXY_ABS,\n            )\n        )\n        annotation[\"bbox\"] = annotation[\"bbox\"].clip(image_size)\n        annotation[\"bbox_mode\"] = BoxMode.XYXY_ABS\n\n    if \"segmentation\" in annotation:\n        if isinstance(annotation[\"segmentation\"], PolygonMasks):\n            annotation[\"segmentation\"] = transforms.apply_polygons(\n                annotation[\"segmentation\"]\n            )\n        elif isinstance(annotation[\"segmentation\"], BitMasks):\n            annotation[\"segmentation\"] = transforms.apply_bitmask(\n                annotation[\"segmentation\"]\n            )\n        elif isinstance(annotation[\"segmentation\"], list):\n            annotation[\"segmentation\"] = transforms.apply_rle(\n                annotation[\"segmentation\"]\n            )\n        else:\n            raise ValueError(\n                \"Unknown segmentation type: {}\".format(type(annotation[\"segmentation\"]))\n            )\n\n    if \"keypoints\" in annotation:\n        if keypoint_hflip_indices is not None:\n            annotation[\"keypoints\"] = transforms.apply_keypoints(\n                annotation[\"keypoints\"], keypoint_hflip_indices\n            )\n        annotation[\"keypoints\"] = transforms.apply_keypoints(\n            annotation[\"keypoints\"], image_size\n        )\n\n"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_coords", "completion": "  # noqa\n        if len(coords) == 0 or self.angle % 360 == 0:\n            return coords\n        return np.dot(self.rm_coords, coords.T).T\n\n    def apply_segmentation(self, segmentation):\n        segmentation = self.apply_image(segmentation, interp=Image.NEAREST)\n        return segmentation\n\n    def inverse(self):\n        return RotationTransform(\n            self.bound_h, self.bound_w, -self.angle, self.expand, self.center, self.interp\n        )\n\n    def create_rotation_matrix(self, offset=0.5):\n        \"\"\"\n        Creates a rotation matrix that rotates the image by the given angle.\n        The rotation matrix is centered at the given offset.\n        \"\"\"\n        angle = np.deg2rad(self.angle)\n        cos = np.cos(angle)\n        sin = np.sin(angle)\n        return np.array(\n            [\n                [cos, -sin, offset],\n                [sin, cos, offset],\n                [0, 0, 1],\n            ]\n        )\n\n"}
{"namespace": "detectron2.utils.analysis.flop_count_operators", "completion": "  # noqa: E501\n\n    # TODO: add support for other types of inputs\n    assert isinstance(inputs, list)\n    assert all(isinstance(i, dict) for i in inputs)\n    assert \"image\" in inputs[0]\n\n    # TODO: add support for other types of models\n    assert isinstance(model, nn.Module)\n\n    # TODO: add support for other types of models\n    assert isinstance(model, nn.Module)\n\n    # TODO: add support for other types of models\n    assert isinstance(model, nn.Module)\n\n    # TODO: add support for other types of models\n    assert isinstance(model, nn.Module)\n\n    # TODO: add support for other types of models\n    assert isinstance(model, nn.Module)\n\n    # TODO: add support for other types of models\n    assert isinstance(model, nn.Module)\n\n    # TODO: add support for other types of models\n    assert isinstance(model, nn.Module)\n\n    # TODO: add support for other types of models\n    assert isinstance(model, nn.Module)\n\n    # TODO: add support for other types of models\n    assert isinstance(model, nn.Module)\n\n    # TODO: add support for other types of models\n    assert isinstance(model, nn.Module)\n\n    # TODO: add support for other types of models\n    assert isinstance(model, nn.Module)\n\n    # TODO: add support for other types of models\n    assert isinstance(model, nn.Module)\n\n    # TODO: add support for other types of models\n    assert isinstance(model, nn.Module)\n\n    # TODO: add support for other types of models\n    assert isinstance(model, nn.Module)\n\n    # TODO: add support for other types of models\n    assert isinstance(model, nn.Module)\n\n    # TODO: add support for other types of models\n    assert isinstance(model, nn.Module)\n\n    # TODO: add support for other types of models\n    assert isinstance(model, nn.Module)\n\n    # TODO: add support for other types of models\n    assert isinstance(model, nn.Module)\n\n    # TODO: add support for other types of models\n    assert isinstance(model, nn.Module)\n\n    # TODO: add support for other types of models\n    assert isinstance(model, nn.Module)\n\n    # TODO: add support for other types of models\n    assert isinstance(model, nn.Module)\n\n    # TODO: add support for other types of models\n    assert isinstance(model, nn.Module)\n\n    # TODO: add support for other types"}
{"namespace": "detectron2.data.transforms.transform.RotationTransform.apply_image", "completion": "  # noqa\n        if interp is None:\n            interp = self.interp\n        if img.shape[0] == 0 or img.shape[1] == 0:\n            return img\n        if img.shape[0] == 1 or img.shape[1] == 1:\n            img = np.expand_dims(img, 0)\n        if img.shape[0] == 3 or img.shape[1] == 3:\n            img = np.expand_dims(img, 0)\n        if img.shape[0] == 4 or img.shape[1] == 4:\n            img = np.expand_dims(img, 0)\n        if img.shape[0] == 5 or img.shape[1] == 5:\n            img = np.expand_dims(img, 0)\n        if img.shape[0] == 6 or img.shape[1] == 6:\n            img = np.expand_dims(img, 0)\n        if img.shape[0] == 7 or img.shape[1] == 7:\n            img = np.expand_dims(img, 0)\n        if img.shape[0] == 8 or img.shape[1] == 8:\n            img = np.expand_dims(img, 0)\n        if img.shape[0] == 9 or img.shape[1] == 9:\n            img = np.expand_dims(img, 0)\n        if img.shape[0] == 10 or img.shape[1] == 10:\n            img = np.expand_dims(img, 0)\n        if img.shape[0] == 11 or img.shape[1] == 11:\n            img = np.expand_dims(img, 0)\n        if img.shape[0] == 12 or img.shape[1] == 12:\n            img = np.expand_dims(img, 0)\n        if img.shape[0] == 13 or img.shape[1] == 13:\n            img = np.expand_dims(img, 0)\n        if img.shape[0] == 14 or img.shape[1] == 14:\n            img = np.expand_dims(img, 0)\n        if img.shape[0] == 15 or img.shape[1] == 15:\n            img = np.expand_dims(img, 0)\n        if img.shape[0] == 16 or img"}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_instance_predictions", "completion": "  # noqa: E501\n        if not isinstance(predictions, dict):\n            raise ValueError(\"predictions must be a dict, but got {}\".format(type(predictions)))\n\n        if \"pred_boxes\" in predictions:\n            self.draw_boxes(predictions[\"pred_boxes\"], predictions[\"scores\"])\n        if \"pred_classes\" in predictions:\n            self.draw_classes(predictions[\"pred_classes\"], predictions[\"scores\"])\n        if \"pred_masks\" in predictions:\n            self.draw_masks(predictions[\"pred_masks\"], predictions[\"scores\"])\n        if \"pred_keypoints\" in predictions:\n            self.draw_keypoints(predictions[\"pred_keypoints\"], predictions[\"scores\"])\n        if \"pred_masks_rle\" in predictions:\n            self.draw_masks_rle(predictions[\"pred_masks_rle\"], predictions[\"scores\"])\n        if \"pred_panoptic_seg\" in predictions:\n            self.draw_panoptic_seg_predictions(predictions[\"pred_panoptic_seg\"])\n        if \"pred_sem_seg\" in predictions:\n            self.draw_sem_seg_predictions(predictions[\"pred_sem_seg\"])\n        if \"pred_panoptic_seg_masks\" in predictions:\n            self.draw_panoptic_seg_masks(predictions[\"pred_panoptic_seg_masks\"])\n        if \"pred_sem_seg_masks\" in predictions:\n            self.draw_sem_seg_masks(predictions[\"pred_sem_seg_masks\"])\n        if \"pred_panoptic_seg_info\" in predictions:\n            self.draw_panoptic_seg_info(predictions[\"pred_panoptic_seg_info\"])\n        if \"pred_sem_seg_info\" in predictions:\n            self.draw_sem_seg_info(predictions[\"pred_sem_seg_info\"])\n\n        return self.output\n\n    def draw_sem_seg_info(self, sem_seg_info):\n        \"\"\"\n        This function visualizes semantic segmentation information on an image. It processes the predictions to extract and visualize semantic segmentation masks, semantic segmentation classes, and semantic segmentation scores. Depending on the instance mode, it may also adjust the visualization style (e.g., segmentation colors, grayscale image).\n\n        Input-Output Arguments\n        :param self: Visualizer. An instance of the Visualizer class, which contains methods and properties for visualizing prediction results.\n        :param sem_seg_info: list[dict]. The output of a semantic segmentation model. It uses fields like \"sem_seg_masks\", \"sem_seg_classes\", \"sem_seg_scores\" to draw the visualizations on the image.\n        :return: VisImage. An image object with the visualizations drawn on it.\n        \"\"\"  # noqa: E501\n        if not isinstance(sem_seg_info, list):\n            raise ValueError(\""}
{"namespace": "detectron2.utils.visualizer.VisImage.get_image", "completion": "class Visualizer:\n    \"\"\"\n    A class for visualizing instance segmentation results.\n\n    Args:\n        metadata (Metadata): the metadata of the dataset\n        scale (float): scale the input image\n    \"\"\"\n\n    def __init__(self, metadata, scale=1.0):\n        self.metadata = metadata\n        self._mode = ColorMode.IMAGE\n        self._scale = scale\n        self._colors = None\n        self._init_colors()\n\n    def _init_colors(self):\n        \"\"\"\n        Initialize a list of colors for visualizing instances.\n        The colors are picked from a default palette of web-safe colors in\n        matplotlib. The label with index i has the color at position\n        i in this list.\n        \"\"\"\n        # The default palette of web-safe colors in matplotlib has 6 x 3 = 18 colors.\n        # We only use 16 of them so that two classes (background and stuff) don't\n        # have the same color.\n        self._colors = [\n            random_color() for _ in range(16)\n        ]  # 16 colors for 16 classes\n        self._colors[0] = _OFF_WHITE\n        self._colors[1] = _BLACK\n\n    def draw_instance_predictions(self, img, predictions, show_mask=False):\n        \"\"\"\n        Draw instance segmentation predictions on the image.\n\n        Args:\n            img (VisImage): the image to draw on\n            predictions (dict): the output of an instance segmentation model.\n                It should have the following keys:\n                \"instances\" (Instances): the output of the model, containing\n                    fields \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\"\n                \"panoptic_seg\" (Tensor): the panoptic segmentation output of the model.\n                \"segments_info\" (list[dict]): the panoptic segmentation metadata.\n        \"\"\"\n        if \"panoptic_seg\" in predictions:\n            panoptic_seg = predictions[\"panoptic_seg\"]\n            segments_info = predictions[\"segments_info\"]\n            panoptic_prediction = _PanopticPrediction(panoptic_seg, segments_info)\n        else:\n            panoptic_prediction = None\n\n        if \"instances\" in predictions:\n            instances = predictions[\"instances\"]\n            if \"pred_masks\" in instances:\n                masks = instances.pred_masks\n            else:\n                masks = None\n            if \"pred_boxes\" in instances:\n                boxes = instances.pred_boxes\n           "}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_dataset_dict", "completion": "  # noqa: E501\n\n        if \"sem_seg\" in dic:\n            self.draw_sem_seg(dic[\"sem_seg\"], area_threshold=_SMALL_OBJECT_AREA_THRESH)\n        if \"panoptic_seg\" in dic:\n            self.draw_panoptic_seg(dic[\"panoptic_seg\"], dic[\"segments_info\"])\n        if \"instances\" in dic:\n            self.draw_instance_predictions(dic[\"instances\"])\n        if \"keypoints\" in dic:\n            self.draw_keypoints(dic[\"keypoints\"], dic[\"keypoint_scores\"], dic[\"keypoint_indices\"])\n        if \"bbox\" in dic:\n            self.draw_bboxes(dic[\"bbox\"], dic[\"bbox_mode\"])\n        if \"mask\" in dic:\n            self.draw_binary_mask(dic[\"mask\"], alpha=0.5)\n        return self.output\n\n    def draw_bboxes(self, bboxes, mode=BoxMode.XYXY_ABS):\n        \"\"\"\n        Draw bounding boxes on an image.\n\n        Args:\n            bboxes (list[Boxes] or RotatedBoxes): bounding boxes to be drawn.\n            mode (BoxMode): the format of the bounding boxes.\n        \"\"\"\n        if isinstance(bboxes, Boxes):\n            bboxes = [bboxes]\n        if isinstance(bboxes, RotatedBoxes):\n            bboxes = [bboxes]\n        for bbox in bboxes:\n            self.draw_box(bbox, mode=mode)\n\n    def draw_box(self, bbox, mode=BoxMode.XYXY_ABS):\n        \"\"\"\n        Draw a bounding box on an image.\n\n        Args:\n            bbox (Boxes or RotatedBoxes): bounding box to be drawn.\n            mode (BoxMode): the format of the bounding box.\n        \"\"\"\n        if isinstance(bbox, Boxes):\n            bbox = bbox.tensor\n        if isinstance(bbox, RotatedBoxes):\n            bbox = bbox.tensor\n        bbox = bbox.astype(np.int32)\n        self.draw_polygon(bbox, edge_color=_BLACK, fill_color=_BLACK)\n\n    def draw_keypoints(self, keypoints, scores, indices):\n        \"\"\"\n        Draw keypoints on an image.\n\n        Args:\n            keypoints (Keypoints): keypoints to be drawn.\n            scores (list[float]): scores for each keypoint.\n            indices (list[int]): indices of keypoints to be drawn.\n        \"\"\"\n        if isinstance(keypoints, Keypoints):\n            keypoints = keypoints.tensor\n        if isinstance(keypoints, torch.Tensor):\n            keypoints = keypoints.cpu().numpy()\n        if isinstance(scores, torch.Tensor):\n           "}
{"namespace": "detectron2.utils.visualizer.Visualizer.draw_binary_mask", "completion": "  # noqa: E501\n        if color is None:\n            color = random_color(rgb=True, maximum=1)\n        if edge_color is None:\n            edge_color = color\n        if text is not None:\n            font_size = self._default_font_size\n            text_color = self._change_color_brightness(color, brightness_factor=0.7)\n            self.draw_text(text, (0, 0), color=text_color, font_size=font_size)\n\n        # draw polygon\n        if binary_mask.sum() > area_threshold:\n            polygons = mask_util.to_polygon(binary_mask)\n            for polygon in polygons:\n                self.draw_polygon(polygon, color, alpha=alpha, edge_color=edge_color)\n\n        # draw holes\n        if binary_mask.sum() > 0:\n            polygons, has_holes = mask_util.to_polygon(binary_mask)\n            if has_holes:\n                self.draw_polygon(polygons, color, alpha=alpha, edge_color=edge_color)\n\n        return self.output\n\n    def draw_polygon(self, polygon, color, alpha=0.5, edge_color=None):\n        \"\"\"\n        This function draws a polygon on an image. It supports drawing regular polygons and polygons with holes using a different approach. It can also draw text on the polygon if specified. The function allows for customization of the polygon's color, edge color, transparency, and the minimum area of components to be drawn.\n\n        Input-Output Arguments\n        :param self: Visualizer. An instance of the Visualizer class.\n        :param polygon: ndarray. A numpy array of shape (N, 2) representing the polygon, where N is the number of vertices. Each value in the array is either 0 or 1 of uint8 type.\n        :param color: Optional. The color of the polygon. If None, a random color is chosen. Accepts formats listed in `matplotlib.colors`.\n        :param alpha: float, Optional. The blending efficiency for the polygon, with smaller values leading to more transparent polygons. Defaults to 0.5.\n        :param edge_color: Optional. The color of the polygon edges. Accepts formats listed in `matplotlib.colors`.\n        :return: VisImage. The image object with the polygon drawn on it.\n        \"\"\"  # noqa: E501\n        if edge_color is None:\n            edge_color = color\n        if polygon.shape[0] < 3:\n            return self.output"}
{"namespace": "detectron2.utils.testing.assert_instances_allclose", "completion": "  # noqa: E501\n    if not isinstance(input, Instances):\n        raise ValueError(f\"Input is not an Instances object: {type(input)}\")\n    if not isinstance(other, Instances):\n        raise ValueError(f\"Other is not an Instances object: {type(other)}\")\n\n    if size_as_tensor:\n        assert input.image_size == other.image_size, (\n            f\"Image sizes are not equal: {input.image_size} vs. {other.image_size}\"\n        )\n    else:\n        assert input.image_size == other.image_size, (\n            f\"Image sizes are not equal: {input.image_size} vs. {other.image_size}\"\n        )\n\n    for name in input._field_names:\n        val = getattr(input, \"_\" + name, None)\n        if val is not None:\n            if isinstance(val, Boxes):\n                assert val == other.get(name), (\n                    f\"Boxes are not equal: {val} vs. {other.get(name)}\"\n                )\n            elif isinstance(val, ROIMasks):\n                assert val == other.get(name), (\n                    f\"ROIMasks are not equal: {val} vs. {other.get(name)}\"\n                )\n            elif isinstance(val, torch.Tensor):\n                assert torch.allclose(\n                    val, other.get(name), rtol=rtol, atol=0, equal_nan=True\n                ), (\n                    f\"Tensor fields are not equal: {val} vs. {other.get(name)}\"\n                )\n            else:\n                assert val == other.get(name), (\n                    f\"Fields are not equal: {val} vs. {other.get(name)}\"\n                )"}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.area", "completion": "  # noqa: E501\n        return self.tensor[:, 2] * self.tensor[:, 3]\n\n    def clip(self, image_shape: Tuple[int, int]) -> \"RotatedBoxes\":\n        \"\"\"\n        Clips the rotated boxes to the image boundaries.\n\n        Input-Output Arguments\n        :param self: RotatedBoxes. An instance of the RotatedBoxes class, which contains the boxes' data in a tensor.\n        :param image_shape: Tuple[int, int]. A tuple of two integers, representing the height and width of the image.\n        :return: RotatedBoxes. An instance of the RotatedBoxes class, which contains the clipped boxes' data in a tensor.\n        \"\"\"  # noqa: E501\n        image_height, image_width = image_shape\n        tensor = self.tensor\n        tensor[:, 0] = torch.clamp(tensor[:, 0], min=0, max=image_width)\n        tensor[:, 1] = torch.clamp(tensor[:, 1], min=0, max=image_height)\n        tensor[:, 2] = torch.clamp(tensor[:, 2], min=0, max=image_width - tensor[:, 0])\n        tensor[:, 3] = torch.clamp(tensor[:, 3], min=0, max=image_height - tensor[:, 1])\n        return RotatedBoxes(tensor)\n\n    def nonempty(self) -> bool:\n        \"\"\"\n        Returns whether the RotatedBoxes contains any boxes.\n\n        Input-Output Arguments\n        :param self: RotatedBoxes. An instance of the RotatedBoxes class, which contains the boxes' data in a tensor.\n        :return: bool. A boolean value, indicating whether the RotatedBoxes contains any boxes.\n        \"\"\"  # noqa: E501\n        return self.tensor.numel() > 0\n\n    def __getitem__(self, item):\n        \"\"\"\n        Returns a view of the RotatedBoxes with the specified indices.\n\n        Input-Output Arguments\n        :param self: RotatedBoxes. An instance of the RotatedBoxes class, which contains the boxes' data in a tensor.\n        :param item: int or tuple[int]. An integer or a tuple of integers, representing the indices of the boxes to be returned.\n        :return: RotatedBoxes. A view of the RotatedBoxes with the specified indices.\n        \"\"\"  # noqa: E501\n        if isinstance(item,"}
{"namespace": "detectron2.modeling.proposal_generator.build.build_proposal_generator", "completion": ""}
{"namespace": "detectron2.modeling.roi_heads.fast_rcnn.FastRCNNOutputLayers.losses", "completion": "        scores, proposal_deltas = predictions\n        gt_classes = [x.gt_classes for x in proposals]\n        gt_boxes = [x.gt_boxes for x in proposals]\n        box2box_transform = self.box2box_transform\n        num_classes = self.num_classes\n        cls_agnostic_bbox_reg = self.cls_agnostic_bbox_reg\n        smooth_l1_beta = self.smooth_l1_beta\n        box_reg_loss_type = self.box_reg_loss_type\n        loss_weight = self.loss_weight\n\n        # classification loss\n        loss_cls = cross_entropy(scores, gt_classes, weight=loss_weight[\"loss_cls\"])\n        # box regression loss\n        loss_box_reg = _dense_box_regression_loss(\n            proposal_deltas,\n            gt_boxes,\n            box2box_transform,\n            num_classes,\n            cls_agnostic_bbox_reg,\n            box_reg_loss_type,\n            smooth_l1_beta,\n            loss_weight[\"loss_box_reg\"],\n        )\n\n        # log classification stats\n        _log_classification_stats(scores, gt_classes)\n\n        return {\"loss_cls\": loss_cls, \"loss_box_reg\": loss_box_reg}"}
{"namespace": "detectron2.tracking.base_tracker.build_tracker_head", "completion": ""}
{"namespace": "detectron2.modeling.box_regression.Box2BoxTransform.apply_deltas", "completion": "        assert isinstance(deltas, torch.Tensor), type(deltas)\n        assert isinstance(boxes, torch.Tensor), type(boxes)\n\n        widths = boxes[:, 2] - boxes[:, 0]\n        heights = boxes[:, 3] - boxes[:, 1]\n        ctr_x = boxes[:, 0] + 0.5 * widths\n        ctr_y = boxes[:, 1] + 0.5 * heights\n\n        wx, wy, ww, wh = self.weights\n        dx = deltas[:, 0::4] / wx\n        dy = deltas[:, 1::4] / wy\n        dw = deltas[:, 2::4] / ww\n        dh = deltas[:, 3::4] / wh\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh = torch.clamp(dh, max=self.scale_clamp)\n\n        # Prevent sending too large values into torch.exp()\n        dw = torch.clamp(dw, max=self.scale_clamp)\n        dh"}
{"namespace": "scepter.scepter.modules.annotator.utils.AnnotatorProcessor.run", "completion": ""}
{"namespace": "microsearch.engine.SearchEngine.search", "completion": "  # noqa: E501\n        query = normalize_string(query)\n        keywords = query.split()\n        scores = {}\n        for kw in keywords:\n            scores = update_url_scores(scores, self.bm25(kw))\n        return scores\n\n    def get_urls(self, kw: str) -> dict[str, int]:\n        \"\"\"\n        The function returns a dictionary where each key is a URL and its value is the number of times the keyword appears in the URL.\n\n        Input-Output Arguments\n        :param self: SearchEngine. An instance of the SearchEngine class.\n        :param kw: str, The keyword to search for in the URLs.\n        :return: dict[str, int], A dictionary where each key is a URL and its value is the number of times the keyword appears in the URL.\n        \"\"\"  # noqa: E501\n        return {url: kw in url for url in self._documents}"}
{"namespace": "microsearch.engine.SearchEngine.bulk_index", "completion": ""}
{"namespace": "detectron2.structures.rotated_boxes.RotatedBoxes.clip", "completion": "  # noqa: E501\n        box = self.tensor\n        box[:, 4] = (box[:, 4] + 180.0) % 360.0 - 180.0\n        box[:, 2] = box[:, 2] * math.cos(math.radians(box[:, 4])) + box[:, 3] * math.sin(math.radians(box[:, 4]))\n        box[:, 3] = box[:, 3] * math.cos(math.radians(box[:, 4])) - box[:, 2] * math.sin(math.radians(box[:, 4]))\n        box[:, 0] = box[:, 0] * math.cos(math.radians(box[:, 4])) + box[:, 1] * math.sin(math.radians(box[:, 4]))\n        box[:, 1] = box[:, 1] * math.cos(math.radians(box[:, 4])) - box[:, 0] * math.sin(math.radians(box[:, 4]))\n        box[:, 0] = torch.clamp(box[:, 0], min=0, max=box_size[1])\n        box[:, 1] = torch.clamp(box[:, 1], min=0, max=box_size[0])\n        box[:, 2] = torch.clamp(box[:, 2], min=0, max=box_size[1])\n        box[:, 3] = torch.clamp(box[:, 3], min=0, max=box_size[0])\n        box[:, 0] = box[:, 0] + box[:, 0] * math.sin(math.radians(box[:, 4])) - box[:, 1] * math.cos(math.radians(box[:, 4]))\n        box[:, 1] = box[:, 1] + box[:, 1] * math.sin(math.radians(box[:, 4])) + box[:, 0] * math.cos(math.radians(box[:, 4]))\n        box[:, 2] = box[:, 2] + box[:, 2] * math.sin(math.radians(box[:, 4])) - box[:, 3] * math.cos(math.radians(box[:, 4]))\n        box[:, 3] = box[:, 3] + box[:, 3] * math.sin(math.radians(box[:, 4])) + box[:, 2] * math.cos(math.radians(box[:, 4]))\n        box[:, 0] = torch.clamp(box[:, 0], min="}
{"namespace": "xinhua.XinhuaHallucinations.statistics", "completion": "        stats = {\n            'doc': 0,\n            'gen': 0,\n            'kno': 0,\n            'num': 0\n        }\n\n        for item in self.data:\n            if item['type'] == 'doc':\n                stats['doc'] += 1\n            elif item['type'] == 'gen':\n                stats['gen'] += 1\n            elif item['type'] == 'kno':\n                stats['kno'] += 1\n            elif item['type'] == 'num':\n                stats['num'] += 1\n\n        return stats"}
{"namespace": "mmdet3d.models.builder.build_neck", "completion": "    if cfg['type'] in NECKS._module_dict.keys():\n        return NECKS.build(cfg)\n    else:\n        return MMDET_NECKS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_loss", "completion": "    if cfg['type'] in LOSSES._module_dict.keys():\n        return LOSSES.build(cfg)\n    else:\n        return MMDET_LOSSES.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_head", "completion": "    if cfg['type'] in HEADS._module_dict.keys():\n        return HEADS.build(cfg)\n    else:\n        return MMDET_HEADS.build(cfg)\n\n"}
{"namespace": "mmdet3d.models.builder.build_segmentor", "completion": "    if train_cfg is not None or test_cfg is not None:\n        warnings.warn(\n            'train_cfg and test_cfg is deprecated, '\n            'please specify them in model', UserWarning)\n    assert cfg.get('train_cfg') is None or train_cfg is None, \\\n        'train_cfg specified in both outer field and model field '\n    assert cfg.get('test_cfg') is None or test_cfg is None, \\\n        'test_cfg specified in both outer field and model field '\n    if cfg['type'] in SEGMENTORS._module_dict.keys():\n        return SEGMENTORS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))\n    else:\n        return MMSEG_MODELS.build(\n            cfg, default_args=dict(train_cfg=train_cfg, test_cfg=test_cfg))"}
{"namespace": "mmdet3d.models.builder.build_detector", "completion": "  # noqa: E501\n    if 'train_cfg' in cfg:\n        warnings.warn('train_cfg is deprecated, please use model.train_cfg instead')\n    if 'test_cfg' in cfg:\n        warnings.warn('test_cfg is deprecated, please use model.test_cfg instead')\n    if 'type' not in cfg:\n        raise KeyError('cfg must contain type of the detector')\n    if 'train_cfg' in cfg and 'test_cfg' in cfg:\n        warnings.warn('train_cfg and test_cfg are deprecated, please use model.train_cfg and model.test_cfg instead')\n    if 'train_cfg' in cfg or 'test_cfg' in cfg:\n        warnings.warn('train_cfg and test_cfg are deprecated, please use model.train_cfg and model.test_cfg instead')\n    if 'train_cfg' in cfg and 'test_cfg' in cfg:\n        warnings.warn('train_cfg and test_cfg are deprecated, please use model.train_cfg and model.test_cfg instead')\n    if 'train_cfg' in cfg:\n        warnings.warn('train_cfg is deprecated, please use model.train_cfg instead')\n    if 'test_cfg' in cfg:\n        warnings.warn('test_cfg is deprecated, please use model.test_cfg instead')\n    if 'train_cfg' in cfg and 'test_cfg' in cfg:\n        warnings.warn('train_cfg and test_cfg are deprecated, please use model.train_cfg and model.test_cfg instead')\n    if 'train_cfg' in cfg:\n        warnings.warn('train_cfg is deprecated, please use model.train_cfg instead')\n    if 'test_cfg' in cfg:\n        warnings.warn('test_cfg is deprecated, please use model.test_cfg instead')\n    if 'train_cfg' in cfg:\n        warnings.warn('train_cfg is deprecated, please use model.train_cfg instead')\n    if 'test_cfg' in cfg:\n        warnings.warn('test_cfg is deprecated, please use model.test_cfg instead')\n    if 'train_cfg' in cfg:\n        warnings.warn('train_cfg is deprecated, please use model.train_cfg instead')\n    if 'test_cfg' in cfg:\n        warnings.warn('test_cfg is deprecated, please use model.test_cfg instead')\n    if 'train_cfg' in cfg:\n        warnings.warn('train_cfg is deprecated, please use model.train_cfg instead')\n    if 'test_cfg' in cfg:\n        warnings.warn('test_cfg is deprecated, please use model.test_cfg instead')\n    if 'train_cfg' in cfg:\n        warnings.warn('train"}
{"namespace": "mmdet3d.core.evaluation.indoor_eval.indoor_eval", "completion": "    # TODO: add support for 3D bounding boxes\n    if box_type_3d is not None:\n        raise NotImplementedError\n\n    if box_mode_3d is not None:\n        raise NotImplementedError\n\n    # TODO: add support for 3D bounding boxes\n    if box_type_3d is not None:\n        raise NotImplementedError\n\n    # TODO: add support for 3D bounding boxes\n    if box_mode_3d is not None:\n        raise NotImplementedError\n\n    # TODO: add support for 3D bounding boxes\n    if box_type_3d is not None:\n        raise NotImplementedError\n\n    # TODO: add support for 3D bounding boxes\n    if box_mode_3d is not None:\n        raise NotImplementedError\n\n    # TODO: add support for 3D bounding boxes\n    if box_type_3d is not None:\n        raise NotImplementedError\n\n    # TODO: add support for 3D bounding boxes\n    if box_mode_3d is not None:\n        raise NotImplementedError\n\n    # TODO: add support for 3D bounding boxes\n    if box_type_3d is not None:\n        raise NotImplementedError\n\n    # TODO: add support for 3D bounding boxes\n    if box_mode_3d is not None:\n        raise NotImplementedError\n\n    # TODO: add support for 3D bounding boxes\n    if box_type_3d is not None:\n        raise NotImplementedError\n\n    # TODO: add support for 3D bounding boxes\n    if box_mode_3d is not None:\n        raise NotImplementedError\n\n    # TODO: add support for 3D bounding boxes\n    if box_type_3d is not None:\n        raise NotImplementedError\n\n    # TODO: add support for 3D bounding boxes\n    if box_mode_3d is not None:\n        raise NotImplementedError\n\n    # TODO: add support for 3D bounding boxes\n    if box_type_3d is not None:\n        raise NotImplementedError\n\n    # TODO: add support for 3D bounding boxes\n    if box_mode_3d is not None:\n        raise NotImplementedError\n\n    # TODO: add support for 3D bounding boxes\n    if box_type_3d is not None:\n        raise NotImplementedError\n\n    # TODO: add support for 3D bounding boxes\n    if box_mode_3d is not None:\n        raise NotImplementedError\n\n    # TODO: add support for "}
{"namespace": "mmdet3d.core.bbox.structures.utils.get_box_type", "completion": "    if box_type == 'LiDAR':\n        return 'LiDAR', 'box3d'\n    elif box_type == 'Camera':\n        return 'Camera', 'box3d'\n    elif box_type == 'Depth':\n        return 'Depth', 'box3d'\n    else:\n        raise ValueError('box_type should be one of \"LiDAR\", \"Camera\", or \"Depth\"')\n\n"}
{"namespace": "ollama._client.Client.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    if not messages:\n      raise RequestError('must provide at least one message')\n\n    if not isinstance(messages, (list, tuple)):\n      raise TypeError('messages must be a list of Message or dict-like objects')\n\n    for message in messages:\n      if not isinstance(message, (dict, Message)):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n\n      if not message.get('role'):\n        raise RequestError('each message must contain a role')\n\n      if not message.get('content'):\n        raise RequestError('each message must contain a content')\n\n      if message.get('images'):\n        message['images'] = [_encode_image(image) for image in message['images']]\n\n    return self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.pull", "completion": "    return self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'model': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.generate", "completion": "  def generate_async(\n    self,\n    model: str = '',\n    prompt: str = '',\n    system: str = '',\n    template: str = '',\n    context: Optional[Sequence[int]] = None,\n    stream: bool = False,\n    raw: bool = False,\n    format: Literal['', 'json'] = '',\n    images: Optional[Sequence[AnyStr]] = None,\n    options: Optional[Options] = None,\n    keep_alive: Optional[Union[float, str]] = None,\n  ) -> Union[Mapping[str, Any], AsyncIterator[Mapping[str, Any]]]:\n\n    \"\"\"\n    The function generates a response based on the specified model and parameters. It raises exceptions if the model is not provided or if the request cannot be fulfilled. Depending on the 'stream' parameter, it either returns a single response or a generator for streaming responses.\n\n    Input-Output Arguments\n    :param self: Client. An instance of the Client class.\n    :param model: str, optional, default is an empty string. The model to use for generating a response. It is required, and an exception is raised if not provided.\n    :param prompt: str, optional, default is an empty string. The prompt to pass to the model for generating a response.\n    :param system: str, optional, default is an empty string. The system identifier to use for the request.\n    :param template: str, optional, default is an empty string. The template identifier to use for formatting the response.\n    :param context: Optional[Sequence[int]], optional. A sequence of integers representing the context for the request. Defaults to None, which is treated as an empty list.\n    :param stream: bool, optional, default is False. Determines whether the response should be streamed. If True, the function returns a generator.\n    :param raw: bool, optional, default is False. Determines whether the response should include raw data.\n    :param format: Literal['', 'json'], optional, default is an empty string. Specifies the format of the response. Can be either an empty string or 'json'.\n    :param images: Optional[Sequence[AnyStr]], optional. A sequence of images to include in the request. Defaults to None.\n    :param options: Optional[Options], optional. Additional options for the request. Defaults to None.\n    :param keep_alive: Optional[Union[float, str]], optional. Specifies the keep"}
{"namespace": "ollama._client.Client.push", "completion": "    return self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client.create", "completion": "    if not (path or modelfile):\n      raise RequestError('must provide either a path or a modelfile')\n\n    if path:\n      with open(path, 'rb') as f:\n        modelfile = f.read()\n\n    return self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.Client._create_blob", "completion": "    if isinstance(path, str):\n      path = Path(path)\n\n    if not path.exists():\n      raise FileNotFoundError(f'file {path} does not exist')\n\n    with open(path, 'rb') as f:\n      data = f.read()\n\n    digest = sha256(data).hexdigest()\n\n    response = self._client.head(f'/api/blob/{digest}')\n\n    if response.status_code == 404:\n      response = self._client.post(f'/api/blob/{digest}', data=data)\n\n    return digest\n"}
{"namespace": "ollama._client.AsyncClient.generate", "completion": "def _parse_host(host: Optional[str]) -> str:\n  if host is None:\n    return 'https://ollama.com'\n  elif host.startswith('http://') or host.startswith('https://'):\n    return host\n  else:\n    return f'https://{host}'\n\n\ndef _as_path(path: Union[str, PathLike]) -> Path:\n  if isinstance(path, str):\n    return Path(path).expanduser()\n  else:\n    return path\n\n\ndef _encode_image(image: AnyStr) -> str:\n  if isinstance(image, str):\n    return image\n  elif isinstance(image, bytes):\n    return b64encode(image).decode('utf-8')\n  else:\n    raise TypeError('images must be a string or bytes')"}
{"namespace": "ollama._client.AsyncClient.pull", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/pull',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.chat", "completion": "    if not model:\n      raise RequestError('must provide a model')\n\n    for message in messages or []:\n      if not isinstance(message, dict):\n        raise TypeError('messages must be a list of Message or dict-like objects')\n      if not (role := message.get('role')) or role not in ['system', 'user', 'assistant']:\n        raise RequestError('messages must contain a role and it must be one of \"system\", \"user\", or \"assistant\"')\n      if not message.get('content'):\n        raise RequestError('messages must contain content')\n      if images := message.get('images'):\n        message['images'] = [_encode_image(image) for image in images]\n\n    return await self._request_stream(\n      'POST',\n      '/api/chat',\n      json={\n        'model': model,\n        'messages': messages,\n        'stream': stream,\n        'format': format,\n        'options': options or {},\n        'keep_alive': keep_alive,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient.push", "completion": "    return await self._request_stream(\n      'POST',\n      '/api/push',\n      json={\n        'name': model,\n        'insecure': insecure,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "ollama._client.AsyncClient._create_blob", "completion": "    sha256sum = sha256()\n    with open(path, 'rb') as r:\n      while True:\n        chunk = r.read(32 * 1024)\n        if not chunk:\n          break\n        sha256sum.update(chunk)\n\n    digest = f'sha256:{sha256sum.hexdigest()}'\n\n    try:\n      response = await self._request('HEAD', f'/api/blobs/{digest}')\n    except ResponseError as e:\n      if e.status_code != 404:\n        raise\n\n      with open(path, 'rb') as r:\n        await self._request('POST', f'/api/blobs/{digest}', content=r)\n\n    return digest\n"}
{"namespace": "challenge.ChallengeManager._type_check_with_pyright", "completion": "  # noqa: E501\n\n        # Create a temporary file to write the user code to\n        with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as f:\n            f.write(user_code)\n            user_code_file = f.name\n\n        # Create a temporary file to write the test code to\n        with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as f:\n            f.write(test_code)\n            test_code_file = f.name\n\n        # Run pyright on the user code and test code\n        pyright_output = subprocess.run(\n            [\n                \"pyright\",\n                \"--no-color\",\n                \"--no-watch\",\n                \"--no-verify\",\n                \"--no-strict\",\n                \"--no-strict-expressions\",\n                \"--no-strict-optional\",\n                \"--no-strict-classes\",\n                \"--no-strict-enum-members\",\n                \"--no-strict-annotations\",\n                \"--no-strict-boolean-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-strict-bool-expr\",\n                \"--no-"}
{"namespace": "ollama._client.AsyncClient.create", "completion": "    if (realpath := _as_path(path)) and realpath.exists():\n      modelfile = self._parse_modelfile(realpath.read_text(), base=realpath.parent)\n    elif modelfile:\n      modelfile = self._parse_modelfile(modelfile)\n    else:\n      raise RequestError('must provide either path or modelfile')\n\n    return await self._request_stream(\n      'POST',\n      '/api/create',\n      json={\n        'name': model,\n        'modelfile': modelfile,\n        'stream': stream,\n      },\n      stream=stream,\n    )\n"}
{"namespace": "sfast.utils.aot_printer.aot_printer", "completion": "    def wrapper(*args, **kwargs):\n        if isinstance(fn, torch.nn.Module):\n            return aot_module(fn, get_compiler_fn(\"Module\"))\n        else:\n            return aot_function(fn, get_compiler_fn(\"Function\"))\n\n    return wrapper"}
{"namespace": "autorag.deploy.extract_best_config", "completion": "    summary_df = load_summary_file(trial_path)\n    config_dict = yaml.safe_load(open(os.path.join(trial_path, 'config.yaml'), 'r'))\n    config_dict['node_lines'] = [node_line for node_line in config_dict['node_lines'] if node_line['node_line_name'] in summary_df['node_line_name'].unique()]\n    config_dict['node_lines'] = [node_line for node_line in config_dict['node_lines'] if node_line['node_line_name'] in summary_df['categorical_node_line_name'].unique()]\n    config_dict['node_lines'] = [node_line for node_line in config_dict['node_lines'] if node_line['node_line_name'] in summary_df['categorical_node_line_name'].unique()]\n    config_dict['node_lines'] = [node_line for node_line in config_dict['node_lines'] if node_line['node_line_name'] in summary_df['categorical_node_line_name'].unique()]\n    config_dict['node_lines'] = [node_line for node_line in config_dict['node_lines'] if node_line['node_line_name'] in summary_df['categorical_node_line_name'].unique()]\n    config_dict['node_lines'] = [node_line for node_line in config_dict['node_lines'] if node_line['node_line_name'] in summary_df['categorical_node_line_name'].unique()]\n    config_dict['node_lines'] = [node_line for node_line in config_dict['node_lines'] if node_line['node_line_name'] in summary_df['categorical_node_line_name'].unique()]\n    config_dict['node_lines'] = [node_line for node_line in config_dict['node_lines'] if node_line['node_line_name'] in summary_df['categorical_node_line_name'].unique()]\n    config_dict['node_lines'] = [node_line for node_line in config_dict['node_lines'] if node_line['node_line_name'] in summary_df['categorical_node_line_name'].unique()]\n    config_dict['node_lines'] = [node_line for node_line in config_dict['node_lines'] if node_line['node_line_name'] in summary_df['categorical_node_line_name'].unique()]\n    config_dict['node_lines'] = [node_line for node_line in config_dict['node_lines'] if node_line['node_line_name'] in summary_df['categorical_node_line_name'].unique()]\n    config_dict['node_lines'] = [node_line for node_line in config_dict"}
{"namespace": "sfast.jit.trace_helper.lazy_trace", "completion": "  # noqa: E501\n\n    if ts_compiler is None:\n        ts_compiler = lambda m: m\n\n    def _lazy_trace(func, **kwargs):\n        if not hasattr(func, 'forward'):\n            raise ValueError(\n                'The function or module must have a forward method to be traced.')\n        if not callable(func.forward):\n            raise ValueError(\n                'The function or module must have a callable forward method.')\n\n        # Get the original function or module's forward method.\n        forward = func.forward\n\n        # Get the function's name.\n        name = inspect.getsourcefile(func) or inspect.getfile(func)\n\n        # Get the function's line number.\n        line = inspect.getsourcelines(func)[1]\n\n        # Get the function's source code.\n        source = inspect.getsource(func)\n\n        # Get the function's signature.\n        signature = inspect.signature(forward)\n\n        # Get the function's docstring.\n        docstring = inspect.getdoc(forward)\n\n        # Get the function's arguments.\n        args = list(signature.parameters.keys())\n\n        # Get the function's return type.\n        return_type = signature.return_annotation\n\n        # Get the function's annotations.\n        annotations = signature.parameters\n\n        # Get the function's annotations.\n        annotations = signature.parameters\n\n        # Get the function's annotations.\n        annotations = signature.parameters\n\n        # Get the function's annotations.\n        annotations = signature.parameters\n\n        # Get the function's annotations.\n        annotations = signature.parameters\n\n        # Get the function's annotations.\n        annotations = signature.parameters\n\n        # Get the function's annotations.\n        annotations = signature.parameters\n\n        # Get the function's annotations.\n        annotations = signature.parameters\n\n        # Get the function's annotations.\n        annotations = signature.parameters\n\n        # Get the function's annotations.\n        annotations = signature.parameters\n\n        # Get the function's annotations.\n        annotations = signature.parameters\n\n        # Get the function's annotations.\n        annotations = signature.parameters\n\n        # Get the function's annotations.\n        annotations = signature.parameters\n\n        # Get the function's annotations.\n        annotations = signature.parameters\n\n        # Get the function's annotations.\n        annotations = signature.parameters\n\n        # Get the function's annotations.\n        annotations = signature.parameters\n\n        # Get the function's annotations.\n        annotations = signature.parameters\n\n        # Get the function's annotations.\n        annotations = signature.parameters\n\n        # Get the function's annotations.\n        annotations = signature.parameters"}
{"namespace": "autorag.deploy.Runner.from_trial_folder", "completion": "        config_path = os.path.join(trial_path, 'config.yaml')\n        with open(config_path, 'r') as f:\n            try:\n                config = yaml.safe_load(f)\n            except yaml.YAMLError as exc:\n                logger.error(exc)\n                raise exc\n        return cls(config, project_dir=os.path.dirname(trial_path))\n"}
{"namespace": "autorag.nodes.retrieval.run.run_retrieval_node", "completion": "    # Load the previous result\n    previous_result = load_summary_file(previous_result, node_line_dir)\n\n    # Evaluate the retrieval node results\n    results = evaluate_retrieval(modules, module_params, previous_result)\n\n    # Filter the results by speed threshold\n    results = filter_by_threshold(results, strategies[\"speed_threshold\"])\n\n    # Select the best result\n    results = select_best_average(results, strategies[\"average\"])\n\n    # Save the results and summary to disk\n    results.to_csv(os.path.join(node_line_dir, \"results.csv\"), index=False)\n    measure_speed(results, strategies[\"speed_threshold\"], strategies[\"average\"], node_line_dir)\n\n    return results\n\n"}
{"namespace": "autorag.nodes.queryexpansion.run.run_query_expansion_node", "completion": "    # save the previous result to the node line directory\n    previous_result.to_csv(os.path.join(node_line_dir, \"previous_result.csv\"), index=False)\n\n    # save the module parameters to the node line directory\n    module_params.to_csv(os.path.join(node_line_dir, \"module_params.csv\"), index=False)\n\n    # save the strategies to the node line directory\n    strategies.to_csv(os.path.join(node_line_dir, \"strategies.csv\"), index=False)\n\n    # save the modules to the node line directory\n    modules = [str(module) for module in modules]\n    modules = \"\\n\".join(modules)\n    with open(os.path.join(node_line_dir, \"modules.txt\"), \"w\") as f:\n        f.write(modules)\n\n    # run each module with the given parameters\n    results = []\n    for module, params in zip(modules, module_params):\n        result = run_query_expansion_module(module, params, previous_result)\n        results.append(result)\n\n    # save the results to the node line directory\n    results = pd.concat(results)\n    results.to_csv(os.path.join(node_line_dir, \"results.csv\"), index=False)\n\n    # save the execution times to the node line directory\n    execution_times = measure_speed(results)\n    execution_times.to_csv(os.path.join(node_line_dir, \"execution_times.csv\"), index=False)\n\n    # save the evaluation metrics to the node line directory\n    evaluation_metrics = evaluate_retrieval_node(results, strategies)\n    evaluation_metrics.to_csv(os.path.join(node_line_dir, \"evaluation_metrics.csv\"), index=False)\n\n    # select the best result based on the evaluation metrics\n    best_result = select_best_average(evaluation_metrics, strategies)\n\n    # save the best result to the node line directory\n    best_result.to_csv(os.path.join(node_line_dir, \"best_result.csv\"), index=False)\n\n    return best_result\n\n"}
{"namespace": "autorag.nodes.promptmaker.run.run_prompt_maker_node", "completion": "    # Create directories\n    node_dir = pathlib.Path(node_line_dir)\n    node_dir.mkdir(parents=True, exist_ok=True)\n    node_dir = node_dir / \"results\"\n    node_dir.mkdir(parents=True, exist_ok=True)\n    node_dir = node_dir / \"summary\"\n    node_dir.mkdir(parents=True, exist_ok=True)\n\n    # Create a dataframe to store the results\n    results = pd.DataFrame(columns=[\"module\", \"module_params\", \"result\"])\n\n    # Run each prompt maker module\n    for i, module in enumerate(modules):\n        # Get the module's parameters\n        module_params = deepcopy(module_params[i])\n\n        # Run the module\n        result = module(**module_params)\n\n        # Save the result\n        results = results.append({\"module\": module.__name__,\n                                  \"module_params\": module_params,\n                                  \"result\": result},\n                                 ignore_index=True)\n\n    # Evaluate the results\n    results = evaluate_generation(results, strategies)\n\n    # Save the results\n    results.to_csv(node_dir / \"results.csv\", index=False)\n\n    # Save the summary\n    summary = pd.DataFrame(columns=[\"module\", \"module_params\", \"result\", \"metrics\", \"speed\", \"generator\"])\n    for i, row in results.iterrows():\n        module = row[\"module\"]\n        module_params = row[\"module_params\"]\n        result = row[\"result\"]\n        metrics = cast_metrics(row[\"metrics\"])\n        speed = row[\"speed\"]\n        generator = row[\"generator\"]\n\n        # Add the module's name and parameters to the summary\n        summary = summary.append({\"module\": module.__name__,\n                                  \"module_params\": module_params,\n                                  \"result\": result,\n                                  \"metrics\": metrics,\n                                  \"speed\": speed,\n                                  \"generator\": generator},\n                                 ignore_index=True)\n\n    summary.to_csv(node_dir / \"summary.csv\", index=False)\n\n    return results\n\n"}
{"namespace": "autorag.schema.node.extract_values_from_nodes", "completion": "    def extract_values_from_node(node: Node):\n        return extract_values(node=node, key=key)\n\n    values = list(map(extract_values_from_node, nodes))\n    return list(set(list(itertools.chain.from_iterable(values))))"}
{"namespace": "autorag.evaluate.metric.generation.sem_score", "completion": "    def compute_score(gt: str, pred: str) -> float:\n        if embedding_model is None:\n            embedding_model = embedding_models.get_default_model()\n        return calculate_cosine_similarity(embedding_model, gt, pred)\n\n    result = list(map(lambda x: compute_score(x[0], x[1]), zip(generation_gt, generations)))\n    return result\n\n"}
{"namespace": "gfpgan_model.gfpgan_fix_faces", "completion": "  # noqa: E501\n    global gfpgan_face_restorer\n    if gfpgan_face_restorer is None:\n        logger.warning(\"GFPGAN face restorer not set up. Returning original image.\")\n        return np_image\n    else:\n        return gfpgan_face_restorer.restore(np_image)\n\n"}
{"namespace": "codeformer_model.setup_model", "completion": "  # noqa: E501\n    try:\n        global codeformer\n        codeformer = FaceRestorerCodeFormer(dirname)\n    except Exception as e:\n        logger.error(f\"Error setting up CodeFormer: {e}\")\n        raise errors.FailedToSetupModelError(f\"Error setting up CodeFormer: {e}\")"}
{"namespace": "gfpgan_model.setup_model", "completion": "    try:\n        gfpgan_face_restorer = FaceRestorerGFPGAN()\n        gfpgan_face_restorer.model_path = dirname\n        gfpgan_face_restorer.load_net()\n        logger.info(\"GFPGAN face restorer successfully set up\")\n    except Exception as e:\n        logger.error(f\"Failed to set up GFPGAN face restorer: {e}\")\n        raise errors.GFPGANSetupError(f\"Failed to set up GFPGAN face restorer: {e}\")"}
{"namespace": "quaternion.rotate", "completion": "  # Convert the vector to a quaternion\n  v = jnp.concatenate([v, jnp.zeros_like(v)], axis=-1)\n  v = normalize(v)\n\n  # Multiply the quaternion by the vector\n  v = multiply(q, v)\n\n  # Extract the real and imaginary parts of the quaternion\n  w = re(v)\n  v = im(v)\n\n  # Convert the quaternion back to a vector\n  v = jnp.concatenate([w, v], axis=-1)\n\n  return v\n\n"}
{"namespace": "quaternion.from_axis_angle", "completion": "  axis = jnp.asarray(axis_angle[Ellipsis, :3])\n  angle = jnp.asarray(axis_angle[Ellipsis, 3])\n  axis = axis / jnp.linalg.norm(axis, axis=-1, keepdims=True)\n  axis = jnp.expand_dims(axis, -1)\n  axis = jnp.tile(axis, [1] * (len(axis_angle.shape) - 1) + [4])\n  angle = jnp.expand_dims(angle, -1)\n  angle = jnp.tile(angle, [1] * (len(axis_angle.shape) - 1) + [4])\n  return jnp.concatenate([jnp.cos(angle / 2), jnp.sin(angle / 2) * axis], axis=-1)\n\n"}
{"namespace": "openlogprobs.extract.topk_search", "completion": "def topk_search(\n    model: Model,\n    prefix: str,\n    idx: int,\n    k=1,\n    high=40,\n    top_logprob: Optional[float] = None,\n    max_calls: int = 1000,\n    max_workers: int = 10,\n):\n    \"\"\"Parallel top-k search based on https://mattf1n.github.io/openlogprobs.html\"\"\"\n    logit_bias = {idx: high}\n    topk_words = model.topk(prefix, logit_bias)\n    if idx in topk_words:\n        biased_logprobs = np.array([topk_words[idx]])\n        log_biased_prob = logsumexp(biased_logprobs)\n        logprobs = biased_logprobs - np.logaddexp(\n            high + np.log1p(-np.exp(log_biased_prob)), log_biased_prob\n        )\n        return logprobs, 1\n    else:\n        if top_logprob is None:\n            raise TypeError(\n                f\"Token {idx} not in top-k with bias {high}.\"\n                \"Either increase bias or provide top unbiased logprob (top_logprob)\"\n            )\n        success_idxs = list(i for i in topk_words if i == idx)\n        fail_idxs = set(topk_words) - set(success_idxs)\n        biased_top_logprob = max(\n            logprob for i, logprob in topk_words.items() if i not in idx\n        )\n        biased_logprobs = np.array([topk_words[i] for i in success_idxs])\n        logprobs = biased_logprobs - biased_top_logprob + top_logprob - high\n        return logprobs, 1\n\n\ndef topk_search_batch(\n    model: Model,\n    prefix: str,\n    idx: list[int],\n    k=1,\n    high=40,\n    top_logprob: Optional[float] = None,\n    max_calls: int = 1000,\n    max_workers: int = 10,\n):\n    \"\"\"Parallel top-k search based on https://mattf1n.github.io/openlogprobs.html\"\"\"\n    logit_bias = {i: high for i in idx}\n    topk_words = model.topk(prefix, logit_bias)\n    if all(i in topk_words for i in idx):\n        biased_logprobs = np.array([topk"}
{"namespace": "resample.resample_3d", "completion": "  # Resample the data at the specified locations.\n  if method == 'TRILINEAR':\n    # Resample the data at the specified locations.\n    return jax.lax.dynamic_slice(data, locations, (1, 1, 1, 1), 'F')\n  elif method == 'NEAREST':\n    # Resample the data at the specified locations.\n    return jax.lax.dynamic_slice(data, locations, (1, 1, 1, 1), 'F')\n  else:\n    raise ValueError('Invalid interpolation method: {}'.format(method))\n\n"}
{"namespace": "math.plus_eps", "completion": "  def plus_eps_fwd(x):\n    return jnp.where(x < tiny_val, tiny_val, x + 1)\n\n  def plus_eps_bwd(x, g):\n    return jnp.where(x < tiny_val, 0, g)\n\n  return jax.jvp(plus_eps_fwd, (x,), (plus_eps_bwd,))\n\n"}
{"namespace": "math.minus_eps", "completion": "  return jnp.where(\n      jnp.abs(x) < tiny_val, -tiny_val, jnp.nextafter(jnp.float32(x), jnp.neginf)\n  )\n\n"}
{"namespace": "math.safe_exp", "completion": "  return generate_safe_fn(\n      jnp.exp,\n      lambda x, _, x_dot: x_dot * x,\n      (tiny_val, max_val),\n  )(x)\n\n"}
{"namespace": "math.safe_log", "completion": "def safe_log1p(x):\n\n  \"\"\"\n  This function creates a safe version of the logarithm function that can handle edge cases or specific conditions defined by `generate_safe_fn`. It wraps the JAX numpy logarithm function (`jnp.log1p`) with additional logic to manage derivatives and bounds, ensuring stability and safety in computations.\n\n  Input-Output Arguments\n  :param x: The input value or array for which the logarithm is to be calculated. It is used as the input to the safe logarithm function created by `generate_safe_fn`.\n  :return: The result of applying the safe logarithm function to `x`. This could be a single value or an array of values, depending on the input `x`.\n  \"\"\"\n\n\ndef safe_log2(x):\n\n  \"\"\"\n  This function creates a safe version of the logarithm function that can handle edge cases or specific conditions defined by `generate_safe_fn`. It wraps the JAX numpy logarithm function (`jnp.log2`) with additional logic to manage derivatives and bounds, ensuring stability and safety in computations.\n\n  Input-Output Arguments\n  :param x: The input value or array for which the logarithm is to be calculated. It is used as the input to the safe logarithm function created by `generate_safe_fn`.\n  :return: The result of applying the safe logarithm function to `x`. This could be a single value or an array of values, depending on the input `x`.\n  \"\"\"\n\n\ndef safe_log10(x):\n\n  \"\"\"\n  This function creates a safe version of the logarithm function that can handle edge cases or specific conditions defined by `generate_safe_fn`. It wraps the JAX numpy logarithm function (`jnp.log10`) with additional logic to manage derivatives and bounds, ensuring stability and safety in computations.\n\n  Input-Output Arguments\n  :param x: The input value or array for which the logarithm is to be calculated. It is used as the input to the safe logarithm function created by `generate_safe_fn`.\n  :return: The result of applying the safe logarithm function to `x`. This could be a single value or an array of values, depending on the input `x`.\n  \"\"\"\n\n\ndef safe_logit(x):\n\n  \"\"\"\n  This function creates a safe version of the logarithm function that can handle edge cases or specific conditions defined by `generate_safe_fn`. It wraps the JAX numpy logarithm function (`"}
{"namespace": "math.safe_sqrt", "completion": "  return generate_safe_fn(\n      jnp.sqrt,\n      lambda _, y, x_dot: 0.5 * y * x_dot,\n      (0, max_val),\n  )(x)\n\n"}
{"namespace": "math.power_ladder_max_output", "completion": "  if p == 0:\n    return 1\n  elif p == 1:\n    return 2\n  elif p == 2:\n    return 3\n  elif p == 3:\n    return 4\n  elif p == 4:\n    return 5\n  elif p == 5:\n    return 6\n  elif p == 6:\n    return 7\n  elif p == 7:\n    return 8\n  elif p == 8:\n    return 9\n  elif p == 9:\n    return 10\n  elif p == 10:\n    return 11\n  elif p == 11:\n    return 12\n  elif p == 12:\n    return 13\n  elif p == 13:\n    return 14\n  elif p == 14:\n    return 15\n  elif p == 15:\n    return 16\n  elif p == 16:\n    return 17\n  elif p == 17:\n    return 18\n  elif p == 18:\n    return 19\n  elif p == 19:\n    return 20\n  elif p == 20:\n    return 21\n  elif p == 21:\n    return 22\n  elif p == 22:\n    return 23\n  elif p == 23:\n    return 24\n  elif p == 24:\n    return 25\n  elif p == 25:\n    return 26\n  elif p == 26:\n    return 27\n  elif p == 27:\n    return 28\n  elif p == 28:\n    return 29\n  elif p == 29:\n    return 30\n  elif p == 30:\n    return 31\n  elif p == 31:\n    return 32\n  elif p == 32:\n    return 33\n  elif p == 33:\n    return 34\n  elif p == 34:\n    return 35\n  elif p == 35:\n    return 36\n  elif p == 36:\n    return 37"}
{"namespace": "geopoly.generate_basis", "completion": "  if base_shape not in ['tetrahedron', 'icosahedron', 'octahedron']:\n    raise ValueError(f'base_shape must be either \"tetrahedron\", \"icosahedron\", or \"octahedron\"')\n\n  if angular_tesselation < 1:\n    raise ValueError(f'angular_tesselation must be >= 1')\n\n  if remove_symmetries and angular_tesselation > 1:\n    raise ValueError(f'angular_tesselation > 1 is not supported when remove_symmetries is True')\n\n  if remove_symmetries:\n    # Remove symmetries\n    base_verts = np.array([\n        [0, 0, 0],\n        [1, 0, 0],\n        [0, 1, 0],\n        [0, 0, 1],\n    ])\n    base_faces = np.array([\n        [0, 1, 2],\n        [0, 2, 3],\n        [0, 3, 1],\n        [1, 2, 3],\n    ])\n  else:\n    if base_shape == 'tetrahedron':\n      base_verts = np.array([\n          [0, 0, 0],\n          [1, 0, 0],\n          [0, 1, 0],\n          [0, 0, 1],\n      ])\n      base_faces = np.array([\n          [0, 1, 2],\n          [0, 2, 3],\n          [0, 3, 1],\n          [1, 2, 3],\n      ])\n    elif base_shape == 'icosahedron':\n      base_verts = np.array([\n          [0, 0, 0],\n          [1, 0, 0],\n          [0, 1, 0],\n          [0, 0, 1],\n          [1, 1, 0],\n          [1, 0, 1],\n          [0, 1, 1],\n          [1, 1, 1],\n          [0.5, 0.5, 0.5],\n      ])\n      base_faces = np.array([\n          [0, 1, 2],\n          [0, 2, 3],\n          [0, 3, 4],\n          [0, "}
{"namespace": "math.safe_log1p", "completion": "  return generate_safe_fn(\n      jnp.log1p,\n      lambda x, _, x_dot: x_dot / (1 + x),\n      (0, max_val),\n  )(x)\n\n"}
{"namespace": "math.power_ladder", "completion": "  # pylint: disable=line-too-long\n  if premult is not None:\n    x = premult * x\n  if p == 1:\n    return x\n  elif p == 0:\n    return jnp.zeros_like(x)\n  elif p == -jnp.inf:\n    return jnp.sign(x)\n  elif p == jnp.inf:\n    return jnp.sign(x) * jnp.abs(x)\n  elif p == jnp.nan:\n    return jnp.nan\n  elif p == jnp.inf:\n    return jnp.sign(x) * jnp.abs(x)\n  elif p == -jnp.inf:\n    return jnp.sign(x)\n  elif p == 0:\n    return jnp.zeros_like(x)\n  else:\n    return jnp.sign(x) * jnp.abs(x) ** (1 / jnp.abs(p))\n\n"}
{"namespace": "math.inv_power_ladder", "completion": "  # pylint: disable=line-too-long\n  # Compute sign(y) * |p - 1|/p * ((|y|/|p-1| + 1)^p - 1)\n  if premult is not None:\n    y = y * premult\n  xp = jnp.abs(y)\n  xs = xp / jnp.maximum(tiny_val, jnp.abs(p - 1))\n  p_safe = clip_finite_nograd(remove_zero(p))\n  y = safe_sign(y) * select(\n      [\n          (p == 1, xp),\n          (p == 0, safe_log1p(xp)),\n          (p == -jnp.inf, -safe_expm1(-xp)),\n          (p == jnp.inf, safe_expm1(xp)),\n      ],\n      clip_finite_nograd(\n          jnp.abs(p_safe - 1) / p_safe * ((xs + 1) ** p_safe - 1)\n      ),\n  )\n  if postmult is not None:\n    y = y * postmult\n  return y\n\n"}
{"namespace": "math.learning_rate_decay", "completion": "  # pylint: disable=line-too-long\n  if lr_delay_steps > 0:\n    lr_delay = lr_init * lr_delay_mult\n    if step < lr_delay_steps:\n      return lr_delay\n    else:\n      return lr_init\n  else:\n    return lr_init\n\n"}
{"namespace": "utils.dummy_rays", "completion": "def get_rays(\n    rng,\n    n,\n    origin_lo,\n    origin_hi,\n    radius_lo,\n    radius_hi,\n    near_lo,\n    near_hi,\n    far_lo,\n    far_hi,\n    include_exposure_idx = False,\n    include_exposure_values = False,\n    include_device_idx = False,\n):\n  \"\"\"Generate a Rays datastructure.\"\"\"\n  return generate_random_rays(\n      rng,\n      n,\n      origin_lo,\n      origin_hi,\n      radius_lo,\n      radius_hi,\n      near_lo,\n      near_hi,\n      far_lo,\n      far_hi,\n      include_exposure_idx,\n      include_exposure_values,\n      include_device_idx,\n  )\n\n\ndef get_rays_from_file(\n    filename,\n    include_exposure_idx = False,\n    include_exposure_values = False,\n    include_device_idx = False,\n):\n  \"\"\"Generate a Rays datastructure from a file.\"\"\"\n  with open(filename, 'rb') as f:\n    rays = np.load(f)\n  return Rays(\n      origins=rays['origins'],\n      directions=rays['directions'],\n      viewdirs=rays['viewdirs'],\n      radii=rays['radii'],\n      imageplane=rays['imageplane'],\n      pixels=rays['pixels'],\n      lossmult=rays['lossmult'],\n      near=rays['near'],\n      far=rays['far'],\n      cam_idx=rays['cam_idx'],\n      exposure_idx=rays['exposure_idx'],\n      exposure_values=rays['exposure_values'],\n      device_idx=rays['device_idx'],\n  )\n\n\ndef get_rays_from_file_batch(\n    filenames,\n    include_exposure_idx = False,\n    include_exposure_values = False,\n    include_device_idx = False,\n):\n  \"\"\"Generate a Rays datastructure from a file.\"\"\"\n  rays = []\n  for filename in filenames:\n    with open(filename, 'rb') as f:\n      rays.append(np.load(f))\n  return Rays(\n      origins=rays[0]['origins'],\n      directions=rays[0]['directions'],\n      viewdirs=rays[0]['viewdirs'],\n      radii=rays[0]['radii'],\n      imageplane=rays[0]['imageplane'],\n      pixels=rays[0]['pixels'],\n      lossmult=rays[0]['lossmult'],\n      near=rays[0]['near'],\n      far=rays[0]['far'],\n      cam_idx=rays[0]['cam"}
{"namespace": "camera_utils.points_to_pixels", "completion": "  # Compute the camera ray directions.\n  origins, directions, viewdirs, radii, imageplane = pixels_to_rays(\n      points,\n      points,\n      pixtocams,\n      camtoworlds,\n      distortion_params=distortion_params,\n      camtype=camtype,\n      xnp=xnp,\n  )\n\n  # Compute the pixel coordinates.\n  pixel_coords = imageplane + directions * radii\n\n  # Compute the depth values.\n  depth = origins - pixel_coords\n\n  return pixel_coords, depth\n\n"}
{"namespace": "rigid_body.exp_se3", "completion": "  # Compute the rotation matrix R and translation vector t from the screw axis.\n  R, t = se3_to_rp(screw_axis)\n\n  # Compute the angle theta from the magnitude of the screw axis.\n  theta = jnp.linalg.norm(screw_axis[0:3])\n\n  # Compute the rotation matrix R_taylor from the first order Taylor expansion.\n  R_taylor = jnp.eye(3) + skew(screw_axis[0:3]) * theta\n\n  # Prevent bad gradients from propagating back when theta is small.\n  theta_safe = jnp.where(theta > eps, theta, 1.0)\n  R = jnp.where(theta > eps, R, R_taylor)\n\n  # Compute the translation vector t_taylor from the first order Taylor expansion.\n  t_taylor = jnp.where(theta > eps, t, 0.0)\n\n  # Compute the homogeneous transformation matrix X from the rotation matrix R and translation vector t.\n  X = rp_to_se3(R, t)\n\n  # Compute the homogeneous transformation matrix X_taylor from the rotation matrix R_taylor and translation vector t_taylor.\n  X_taylor = rp_to_se3(R_taylor, t_taylor)\n\n  # Prevent bad gradients from propagating back when theta is small.\n  X = jnp.where(theta > eps, X, X_taylor)\n\n  return X\n\n"}
{"namespace": "rigid_body.exp_so3", "completion": "  axis_angle = jnp.reshape(axis_angle, (3, 1))\n  axis = jnp.reshape(axis_angle, (3))\n  angle = jnp.linalg.norm(axis)\n  if angle < eps:\n    return jnp.eye(3)\n  axis = axis / angle\n  axis = jnp.reshape(axis, (3, 1))\n  axis = jnp.where(angle < 0, -axis, axis)\n  axis = jnp.where(angle > 0, axis, -axis)\n  axis = jnp.where(angle < eps, jnp.zeros_like(axis), axis)\n  axis = jnp.where(angle > eps, axis, jnp.zeros_like(axis))\n  axis = jnp.where(angle < eps, jnp.zeros_like(axis), axis)\n  axis = jnp.where(angle > eps, axis, jnp.zeros_like(axis))\n  axis = jnp.where(angle < eps, jnp.zeros_like(axis), axis)\n  axis = jnp.where(angle > eps, axis, jnp.zeros_like(axis))\n  axis = jnp.where(angle < eps, jnp.zeros_like(axis), axis)\n  axis = jnp.where(angle > eps, axis, jnp.zeros_like(axis))\n  axis = jnp.where(angle < eps, jnp.zeros_like(axis), axis)\n  axis = jnp.where(angle > eps, axis, jnp.zeros_like(axis))\n  axis = jnp.where(angle < eps, jnp.zeros_like(axis), axis)\n  axis = jnp.where(angle > eps, axis, jnp.zeros_like(axis))\n  axis = jnp.where(angle < eps, jnp.zeros_like(axis), axis)\n  axis = jnp.where(angle > eps, axis, jnp.zeros_like(axis))\n  axis = jnp.where(angle < eps, jnp.zeros_like(axis), axis)\n  axis = jnp.where(angle > eps, axis, jnp.zeros_like(axis))\n  axis = jnp.where(angle < eps, jnp.zeros_like(axis), axis)\n  axis = jnp.where(angle > eps, axis, jnp.zeros_like(axis))\n  axis = jnp.where(angle < eps, jnp.zeros_like(axis), axis)\n  axis = jnp.where(angle > eps, axis, jnp.zeros_like(axis))\n  axis = jnp.where(angle < eps, jnp.zeros_like(axis), axis)\n  axis = jnp.where(angle > eps, axis, jnp.zeros_like(axis"}
{"namespace": "render.conical_frustum_to_gaussian", "completion": "  # Calculate the mean and variance of the Gaussian distribution.\n  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n\n  # Calculate the covariance matrix of the Gaussian distribution.\n  if diag:\n    cov_diag = t_var[Ellipsis, None] + r_var[Ellipsis, None]\n  else:\n    d_outer = d[Ellipsis, :, None] * d[Ellipsis, None, :]\n    eye = jnp.eye(d.shape[-1])\n    null_outer = eye - d[Ellipsis, :, None] * (d / jnp.maximum(1e-10, jnp.sum(d**2, axis=-1, keepdims=True)))[Ellipsis, None, :]\n    t_cov = t_var[Ellipsis, None, None] * d_outer[Ellipsis, None, :, :]\n    xy_cov = r_var[Ellipsis, None, None] * null_outer[Ellipsis, None, :, :]\n    cov = t_cov + xy_cov\n\n  # Calculate the mean of the Gaussian distribution.\n  mean = d[Ellipsis, None, :] * t_mean[Ellipsis, None]\n\n  # Calculate the covariance of the Gaussian distribution.\n  return mean, cov\n\n"}
{"namespace": "render.cylinder_to_gaussian", "completion": "  t_mean, t_var, r_var = gaussianize_frustum(t0, t1)\n  r_var *= radius**2\n  mean, cov = lift_gaussian(d, t_mean, t_var, r_var, diag)\n  return mean, cov\n\n"}
{"namespace": "camera_utils.pixels_to_rays", "completion": "  # pylint: disable=line-too-long\n  # pylint: disable=too-many-locals\n  # pylint: disable=too-many-arguments\n  # pylint: disable=too-many-branches\n  # pylint: disable=too-many-statements\n  # pylint: disable=too-many-locals\n  # pylint: disable=too-many-arguments\n  # pylint: disable=too-many-branches\n  # pylint: disable=too-many-statements\n  # pylint: disable=too-many-locals\n  # pylint: disable=too-many-arguments\n  # pylint: disable=too-many-branches\n  # pylint: disable=too-many-statements\n  # pylint: disable=too-many-locals\n  # pylint: disable=too-many-arguments\n  # pylint: disable=too-many-branches\n  # pylint: disable=too-many-statements\n  # pylint: disable=too-many-locals\n  # pylint: disable=too-many-arguments\n  # pylint: disable=too-many-branches\n  # pylint: disable=too-many-statements\n  # pylint: disable=too-many-locals\n  # pylint: disable=too-many-arguments\n  # pylint: disable=too-many-branches\n  # pylint: disable=too-many-statements\n  # pylint: disable=too-many-locals\n  # pylint: disable=too-many-arguments\n  # pylint: disable=too-many-branches\n  # pylint: disable=too-many-statements\n  # pylint: disable=too-many-locals\n  # pylint: disable=too-many-arguments\n  # pylint: disable=too-many-branches\n  # pylint: disable=too-many-statements\n  # pylint: disable=too-many-locals\n  # pylint: disable=too-many-arguments\n  # pylint: disable=too-many-branches\n  # pylint: disable=too-many-statements\n  # pylint: disable=too-many-locals\n  # pylint: disable=too-many-arguments\n  # pylint: disable=too-many-branches\n  # pylint: disable=too-many-statements\n  # pylint: disable=too-many-locals\n  # pylint: disable=too-many-arguments\n  # pylint: disable=too-many-branches\n  # pylint: disable=too-many-statements\n  # pylint: disable=too"}
{"namespace": "render.compute_alpha_weights", "completion": "  density_delta = jnp.diff(density, axis=-1)\n  weights = compute_alpha_weights_helper(density_delta)\n  weights = jnp.concatenate(\n      [weights[Ellipsis, :1], jnp.cumsum(weights[Ellipsis, :-1], axis=-1)],\n      axis=-1,\n  )\n  weights = weights * jnp.sqrt(jnp.sum(dirs**2, axis=-1, keepdims=True))\n  weights = weights / jnp.maximum(1e-10, jnp.sum(weights, axis=-1, keepdims=True))\n  return weights\n\n"}
{"namespace": "stepfun.sample", "completion": "  # pylint: disable=line-too-long\n  utils.assert_valid_stepfun(t, w_logits)\n  # Compute the PDF and CDF for each weight vector.\n  w = jax.nn.softmax(w_logits, axis=-1)\n  cw = integrate_weights(w)\n  # Compute the inverse CDF.\n  t_new = invert_cdf(jnp.linspace(0, 1, num_samples), t, w_logits)\n  # Sample from the inverse CDF.\n  if rng is None:\n    if deterministic_center:\n      t_new = math.select(\n          [(i == 0, t[0] + eps), (i == t.shape[-1] - 1, t[-1] - eps)], t_new)\n    else:\n      t_new = math.select(\n          [(i == 0, t[0] - eps), (i == t.shape[-1] - 1, t[-1] + eps)], t_new)\n    return t_new\n  else:\n    if single_jitter:\n      jitter = jnp.zeros_like(t_new)\n    else:\n      jitter = jnp.random.uniform(\n          shape=t_new.shape, minval=0, maxval=1, dtype=jnp.float32, seed=rng)\n    return t_new + jitter\n\n"}
{"namespace": "stepfun.sample_intervals", "completion": "  # Draw uniform samples.\n  if rng is None:\n    # Match the behavior of jax.random.uniform() by spanning [0, 1-eps].\n    u = jnp.linspace(0, 1 - eps, num_samples)\n  else:\n    # `u` is in [0, 1) --- it can be zero, but it can never be 1.\n    u_max = eps + (1 - eps) / num_samples\n    max_jitter = (1 - u_max) / (num_samples - 1) - eps\n    d = 1 if single_jitter else num_samples\n    u = jnp.linspace(0, 1 - u_max, num_samples) + jax.random.uniform(\n        rng, t.shape[:-1] + (d,), maxval=max_jitter\n    )\n\n  # Invert the CDF.\n  t_samples = invert_cdf(u, t, w_logits)\n\n  # Adjust the first and last intervals to fit within the specified domain.\n  t_samples = jnp.clip(t_samples, domain[0], domain[1])\n\n  return t_samples\n\n"}
{"namespace": "stepfun.weighted_percentile", "completion": "  utils.assert_valid_stepfun(t, w)\n  # Compute the integrated weights.\n  cw = integrate_weights(w)\n  # Interpolate into the integrated weights.\n  t_new = math.sorted_interp(ps, cw, t, utils.device_is_tpu())\n  # Compute the weighted percentiles.\n  return math.percentile(t_new, ps, axis=-1)\n\n"}
{"namespace": "stepfun.blur_and_resample_weights", "completion": "  # Convert the histogram to a PDF.\n  pdf = weight_to_pdf(t, w)\n\n  # Blur the PDF.\n  pdf = linspline.linspline(pdf, blur_halfwidth)\n\n  # Resample the blurred PDF to match the new time points.\n  resampled_pdf = resample(tq, t, pdf)\n\n  # Convert the resampled PDF back to weights.\n  resampled_w = pdf_to_weight(tq, resampled_pdf)\n\n  return resampled_w"}
{"namespace": "spin_math.apply_homogeneous_transform", "completion": "  # Apply the transformation to each point in the collection.\n  transformed_vectors = matmul(transform, vectors)\n\n  # Extract the transformed points from the transformed vectors.\n  transformed_points = from_homogeneous(transformed_vectors)\n\n  return transformed_points\n\n"}
{"namespace": "stepfun.resample", "completion": "  utils.assert_valid_stepfun(t, vp)\n  # Compute the width of each interval in t.\n  dt = jnp.diff(t)\n  # Compute the cumulative sum of the values of the step function.\n  v = jnp.cumsum(vp, axis=-1)\n  # Compute the cumulative sum of the width of each interval in t.\n  w = jnp.cumsum(dt, axis=-1)\n  # Compute the cumulative sum of the width of each interval in t.\n  w = jnp.where(w < np.finfo(np.float32).tiny, 0, w)\n  # Compute the cumulative sum of the width of each interval in t.\n  w = jnp.where(w > np.finfo(np.float32).max, np.finfo(np.float32).max, w)\n  # Compute the cumulative sum of the width of each interval in t.\n  w = jnp.where(w < np.finfo(np.float32).tiny, 0, w)\n  # Compute the cumulative sum of the width of each interval in t.\n  w = jnp.where(w > np.finfo(np.float32).max, np.finfo(np.float32).max, w)\n  # Compute the cumulative sum of the width of each interval in t.\n  w = jnp.where(w < np.finfo(np.float32).tiny, 0, w)\n  # Compute the cumulative sum of the width of each interval in t.\n  w = jnp.where(w > np.finfo(np.float32).max, np.finfo(np.float32).max, w)\n  # Compute the cumulative sum of the width of each interval in t.\n  w = jnp.where(w < np.finfo(np.float32).tiny, 0, w)\n  # Compute the cumulative sum of the width of each interval in t.\n  w = jnp.where(w > np.finfo(np.float32).max, np.finfo(np.float32).max, w)\n  # Compute the cumulative sum of the width of each interval in t.\n  w = jnp.where(w < np.finfo(np.float32).tiny, 0, w)\n  # Compute the cumulative sum of the width of each interval in t.\n  w = jnp.where(w > np.finfo(np.float32).max, np.finfo(np.float32).max, w)"}
{"namespace": "coord.integrated_pos_enc", "completion": "  # Scale the mean and variance to the range of the encoding.\n  mean_scaled = mean * (max_deg - min_deg) + min_deg\n  var_scaled = var * (max_deg - min_deg) ** 2\n\n  # Concatenate the scaled mean and variance.\n  concat = jnp.concatenate([mean_scaled, var_scaled], axis=-1)\n\n  # Apply the sinusoidal encoding.\n  pos_enc = jnp.sin(2 * jnp.pi * concat / max_deg)\n\n  return pos_enc\n\n"}
{"namespace": "ref_utils.generate_dir_enc_fn", "completion": "  if deg_view > 5:\n    raise ValueError('Only deg_view of at most 5 is numerically stable.')\n\n  ml_array = get_ml_array(deg_view)\n  l_max = 2 ** (deg_view - 1)\n\n  # Create a matrix corresponding to ml_array holding all coefficients, which,\n  # when multiplied (from the right) by the z coordinate Vandermonde matrix,\n  # results in the z component of the encoding.\n  mat = np.zeros((l_max + 1, ml_array.shape[1]))\n  for i, (m, l) in enumerate(ml_array.T):\n    for k in range(l - m + 1):\n      mat[k, i] = sph_harm_coeff(l, m, k)\n\n  def dir_enc_fn(xyz, kappa_inv):\n    \"\"\"Function returning directional encoding (DE).\n\n    Args:\n      xyz: [..., 3] array of Cartesian coordinates of directions to evaluate at.\n      kappa_inv: [..., 1] reciprocal of the concentration parameter of the von\n        Mises-Fisher distribution.\n\n    Returns:\n      An array with the resulting directional encoding.\n    \"\"\"\n    x = xyz[Ellipsis, 0:1]\n    y = xyz[Ellipsis, 1:2]\n    z = xyz[Ellipsis, 2:3]\n\n    # Compute z Vandermonde matrix.\n    vmz = jnp.concatenate([z**i for i in range(mat.shape[0])], axis=-1)\n\n    # Compute x+iy Vandermonde matrix.\n    vmxy = jnp.concatenate([(x + 1j * y) ** m for m in ml_array[0, :]], axis=-1)\n\n    # Get spherical harmonics.\n    sph_harms = vmxy * math_lib.matmul(vmz, mat)\n\n    # Apply attenuation function using the von Mises-Fisher distribution\n    # concentration parameter, kappa.\n    sigma = 0.5 * ml_array[1, :] * (ml_array[1, :] + 1)\n    dir_enc = sph_harms * jnp.exp(-sigma * kappa_inv)\n\n    # Split into real and imaginary parts and return\n    return jnp.concatenate([jnp.real(dir_enc), jnp.imag(dir_enc)], axis=-1)\n\n  return dir_enc_fn"}
{"namespace": "nlm_ingestor.ingestor.processors.clean_lines", "completion": "    # remove duplicate lines\n    lines = list(set(lines))\n\n    # remove empty lines\n    lines = [line for line in lines if line.strip()]\n\n    # remove lines with only numbers\n    lines = [line for line in lines if not line.isdigit()]\n\n    # remove lines with only whitespace\n    lines = [line for line in lines if line.strip()]\n\n    # remove lines with only punctuation\n    lines = [line for line in lines if not line.isalpha()]\n\n    # remove lines with only whitespace\n    lines = [line for line in lines if not line.isspace()]\n\n    # remove lines with only whitespace\n    lines = [line for line in lines if not line.isspace()]\n\n    # remove lines with only whitespace\n    lines = [line for line in lines if not line.isspace()]\n\n    # remove lines with only whitespace\n    lines = [line for line in lines if not line.isspace()]\n\n    # remove lines with only whitespace\n    lines = [line for line in lines if not line.isspace()]\n\n    # remove lines with only whitespace\n    lines = [line for line in lines if not line.isspace()]\n\n    # remove lines with only whitespace\n    lines = [line for line in lines if not line.isspace()]\n\n    # remove lines with only whitespace\n    lines = [line for line in lines if not line.isspace()]\n\n    # remove lines with only whitespace\n    lines = [line for line in lines if not line.isspace()]\n\n    # remove lines with only whitespace\n    lines = [line for line in lines if not line.isspace()]\n\n    # remove lines with only whitespace\n    lines = [line for line in lines if not line.isspace()]\n\n    # remove lines with only whitespace\n    lines = [line for line in lines if not line.isspace()]\n\n    # remove lines with only whitespace\n    lines = [line for line in lines if not line.isspace()]\n\n    # remove lines with only whitespace\n    lines = [line for line in lines if not line.isspace()]\n\n    # remove lines with only whitespace\n    lines = [line for line in lines if not line.isspace()]\n\n    # remove lines with only whitespace\n    lines = [line for line in lines if not line.isspace()]\n\n    # remove lines with only whitespace\n    lines = [line for line in lines if not line.isspace()]\n\n    # remove lines with only whitespace\n    lines"}
{"namespace": "nlm_ingestor.ingestor_utils.utils.sent_tokenize", "completion": "    if org_texts is None or len(org_texts) == 0:\n        return org_texts\n\n    # remove new lines\n    org_texts = org_texts.replace(\"\\n\", \" \")\n\n    # remove any space between punctuations (.')\n    org_texts = space_rule.sub(r\"\\1\", org_texts)\n\n    # remove any space between punctuations (.')\n    org_texts = quotation_pattern.sub(r\"\\1\", org_texts)\n\n    # remove any space between punctuations (.')\n    org_texts = space_rule.sub(r\"\\1\", org_texts)\n\n    # remove any space between punctuations (.')\n    org_texts = quotation_pattern.sub(r\"\\1\", org_texts)\n\n    # remove any space between punctuations (.')\n    org_texts = space_rule.sub(r\"\\1\", org_texts)\n\n    # remove any space between punctuations (.')\n    org_texts = quotation_pattern.sub(r\"\\1\", org_texts)\n\n    # remove any space between punctuations (.')\n    org_texts = space_rule.sub(r\"\\1\", org_texts)\n\n    # remove any space between punctuations (.')\n    org_texts = quotation_pattern.sub(r\"\\1\", org_texts)\n\n    # remove any space between punctuations (.')\n    org_texts = space_rule.sub(r\"\\1\", org_texts)\n\n    # remove any space between punctuations (.')\n    org_texts = quotation_pattern.sub(r\"\\1\", org_texts)\n\n    # remove any space between punctuations (.')\n    org_texts = space_rule.sub(r\"\\1\", org_texts)\n\n    # remove any space between punctuations (.')\n    org_texts = quotation_pattern.sub(r\"\\1\", org_texts)\n\n    # remove any space between punctuations (.')\n    org_texts = space_rule.sub(r\"\\1\", org_texts)\n\n    # remove any space between punctuations (.')\n    org_texts = quotation_pattern.sub(r\"\\1\", org_texts)\n\n    # remove any space between punctuations (.')\n    org_texts = space_rule.sub(r\"\\1\", org_texts)\n\n    # remove any space between punctuations (.')\n    org_texts = quotation_pattern.sub(r\"\\1\", org_texts)\n\n    # remove any space between punctuations (.')\n    org_texts = space_rule.sub(r\"\\1\", org_texts)\n\n    # remove any space between punctuations (.')\n    org_texts = quotation_pattern.sub(r\"\\1\", org_texts)\n\n    # remove any space between punctuations (.')\n    org_texts = space_rule.sub(r\"\\1\", org_texts)\n\n    # remove any space"}
{"namespace": "searcharray.postings.SearchArray.positions", "completion": "        token = self._check_token_arg(token)\n        if key is None:\n            # Get all positions\n            positions = self.posns.positions(token)\n            return positions\n        else:\n            # Get positions for a single document\n            positions = self.posns.positions(token, key=key)\n            return positions\n"}
{"namespace": "searcharray.solr.parse_min_should_match", "completion": "    # Remove any leading or trailing whitespace\n    spec = spec.strip()\n\n    # Remove any leading or trailing commas\n    spec = spec.replace(',', '')\n\n    # Remove any leading or trailing spaces\n    spec = spec.replace(' ', '')\n\n    # Remove any leading or trailing quotes\n    spec = spec.replace('\"', '')\n\n    # Remove any leading or trailing brackets\n    spec = spec.replace('[', '')\n    spec = spec.replace(']', '')\n\n    # Remove any leading or trailing parentheses\n    spec = spec.replace('(', '')\n    spec = spec.replace(')', '')\n\n    # Remove any leading or trailing colons\n    spec = spec.replace(':', '')\n\n    # Remove any leading or trailing semicolons\n    spec = spec.replace(';', '')\n\n    # Remove any leading or trailing commas\n    spec = spec.replace(',', '')\n\n    # Remove any leading or trailing spaces\n    spec = spec.replace(' ', '')\n\n    # Remove any leading or trailing quotes\n    spec = spec.replace('\"', '')\n\n    # Remove any leading or trailing brackets\n    spec = spec.replace('[', '')\n    spec = spec.replace(']', '')\n\n    # Remove any leading or trailing parentheses\n    spec = spec.replace('(', '')\n    spec = spec.replace(')', '')\n\n    # Remove any leading or trailing colons\n    spec = spec.replace(':', '')\n\n    # Remove any leading or trailing semicolons\n    spec = spec.replace(';', '')\n\n    # Remove any leading or trailing commas\n    spec = spec.replace(',', '')\n\n    # Remove any leading or trailing spaces\n    spec = spec.replace(' ', '')\n\n    # Remove any leading or trailing quotes\n    spec = spec.replace('\"', '')\n\n    # Remove any leading or trailing brackets\n    spec = spec.replace('[', '')\n    spec = spec.replace(']', '')\n\n    # Remove any leading or trailing parentheses\n    spec = spec.replace('(', '')\n    spec = spec.replace(')', '')\n\n    # Remove any leading or trailing colons\n    spec = spec.replace(':', '')\n\n    # Remove any leading or trailing semicolons\n    spec = spec.replace(';', '')\n\n    # Remove any leading or trailing commas\n    spec = spec.replace(',', '')\n\n    # Remove any leading or trailing spaces\n    spec = spec.replace(' ', '')\n\n    # Remove any leading or trailing quotes\n    spec = spec.replace('\"', '')\n\n    # Remove any leading or trailing brackets\n    spec = spec.replace('[', '')\n    spec = spec.replace(']', '')\n\n   "}
{"namespace": "searcharray.postings.SearchArray.phrase_freq", "completion": "        # Check if the tokens are unique\n        if len(tokens) != len(set(tokens)):\n            raise ValueError(\"Tokens must be unique\")\n\n        # Check if the slop is valid\n        if slop < 0:\n            raise ValueError(\"Slop must be a positive integer\")\n\n        # Check if the tokens are in the SearchArray\n        if not all([token in self.term_dict for token in tokens]):\n            raise ValueError(\"Tokens must be in the SearchArray\")\n\n        # Check if the tokens are in the SearchArray\n        if not all([token in self.term_dict for token in tokens]):\n            raise ValueError(\"Tokens must be in the SearchArray\")\n\n        # Check if the tokens are in the SearchArray\n        if not all([token in self.term_dict for token in tokens]):\n            raise ValueError(\"Tokens must be in the SearchArray\")\n\n        # Check if the tokens are in the SearchArray\n        if not all([token in self.term_dict for token in tokens]):\n            raise ValueError(\"Tokens must be in the SearchArray\")\n\n        # Check if the tokens are in the SearchArray\n        if not all([token in self.term_dict for token in tokens]):\n            raise ValueError(\"Tokens must be in the SearchArray\")\n\n        # Check if the tokens are in the SearchArray\n        if not all([token in self.term_dict for token in tokens]):\n            raise ValueError(\"Tokens must be in the SearchArray\")\n\n        # Check if the tokens are in the SearchArray\n        if not all([token in self.term_dict for token in tokens]):\n            raise ValueError(\"Tokens must be in the SearchArray\")\n\n        # Check if the tokens are in the SearchArray\n        if not all([token in self.term_dict for token in tokens]):\n            raise ValueError(\"Tokens must be in the SearchArray\")\n\n        # Check if the tokens are in the SearchArray\n        if not all([token in self.term_dict for token in tokens]):\n            raise ValueError(\"Tokens must be in the SearchArray\")\n\n        # Check if the tokens are in the SearchArray\n        if not all([token in self.term_dict for token in tokens]):\n            raise ValueError(\"Tokens must be in the SearchArray\")\n\n        # Check if the tokens are in the SearchArray\n        if not all([token in self.term_dict for token in tokens]):\n            raise ValueError(\"Tokens must be in the SearchArray\")\n\n        # Check if the tokens are in the SearchArray"}
{"namespace": "searcharray.postings.SearchArray.index", "completion": "        if not is_list_like(array):\n            raise TypeError(\"Expected list-like object, got {}\".format(type(array)))\n\n        if truncate:\n            array = array[:batch_size]\n\n        if avoid_copies:\n            array = array.copy()\n\n        postings = []\n        for doc in array:\n            if not isinstance(doc, str):\n                raise TypeError(\"Expected a string, got {}\".format(type(doc)))\n            postings.append(doc.split())\n\n        return cls(postings, tokenizer=tokenizer, avoid_copies=avoid_copies)\n"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.init", "completion": "        self.server = Server(self.config['serverHost'], self.config['serverPort'], self.logger)\n        self.connections = {}\n        self.lock = threading.Lock()\n\n        self.server.start()\n"}
{"namespace": "searcharray.utils.bitcount.bit_count64", "completion": "    # Initialize the array to 0\n    arr[:] = 0\n\n    # Iterate over the array\n    for i in range(len(arr)):\n\n        # Iterate over the bits of the current element\n        for j in range(num_bytes_64):\n\n            # If the current bit is set to 1, increment the bit count\n            if (arr[i] & (1 << j)) != 0:\n                arr[i] += all_but_one_bit\n\n    return arr\n\n"}
{"namespace": "searcharray.solr.edismax", "completion": "    if mm is None:\n        mm = \"1\"\n\n    if pf is None:\n        pf = []\n    if pf2 is None:\n        pf2 = []\n    if pf3 is None:\n        pf3 = []\n\n    num_search_terms, search_terms, term_centric = parse_query_terms(frame, q, qf)\n\n    if term_centric:\n        return _edismax_term_centric(frame, parse_field_boosts(qf), num_search_terms, search_terms, mm, similarity)\n    else:\n        return _edismax_field_centric(frame, parse_field_boosts(qf), num_search_terms, search_terms, mm, similarity)"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.intercept", "completion": "        if message.type == MessageType.SendMessage:\n            with self.lock:\n                if self.config['multipleConnections']:\n                    connection = self.connections.get(message.connectionId, None)\n                    if connection is None:\n                        connection = Connection(\n                            connection_id=message.connectionId,\n                            process=process,\n                            server=self.server,\n                            logger=self.logger\n                        )\n                        self.connections[message.connectionId] = connection\n                    message.data = connection.c2s(message.data)\n                else:\n                    message.data = self.server.s2c(message.data)\n        elif message.type == MessageType.RecvMessage:\n            with self.lock:\n                if self.config['multipleConnections']:\n                    connection = self.connections.get(message.connectionId, None)\n                    if connection is None:\n                        connection = Connection(\n                            connection_id=message.connectionId,\n                            process=process,\n                            server=self.server,\n                            logger=self.logger\n                        )\n                        self.connections[message.connectionId] = connection\n                    message.data = connection.s2c(message.data)\n                else:\n                    message.data = self.server.c2s(message.data)\n        elif message.type == MessageType.CloseMessage:\n            with self.lock:\n                if self.config['multipleConnections']:\n                    connection = self.connections.get(message.connectionId, None)\n                    if connection is not None:\n                        connection.close()\n                        del self.connections[message.connectionId]\n                else:\n                    self.server.close()\n        else:\n            raise Exception('Invalid message type')"}
{"namespace": "deluder.interceptors.proxifier.interceptor.ProxifierMessageInterceptor.destroy", "completion": "        \n        with self.lock:\n            for connection in self.connections.values():\n                connection.close()\n            if self.server:\n                self.server.stop()\n"}
